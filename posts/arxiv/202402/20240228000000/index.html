<!doctype html><html><head><title>arXiv @ 2024.02.28</title>
<meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/bootstrap.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/layouts/main.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/navigators/navbar.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/plyr.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/flag-icon.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/custom/style.css><link href=https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css rel=stylesheet integrity=sha384-9ndCyUaIbzAi2FUVXJi0CjmCapSmO7SnpJef0486qhLnuZ2cdeRhO02iuK6FUUVM crossorigin=anonymous><script async src=https://cdn.jsdelivr.net/npm/es-module-shims@1/dist/es-module-shims.min.js crossorigin=anonymous></script><script type=importmap>
{
  "imports": {
    "@popperjs/core": "https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.8/dist/esm/popper.min.js",
    "bootstrap": "https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.esm.min.js"
  }
}
</script><script src=https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js integrity=sha384-geWF76RCwLtnZ8qwWowPQNguL3RmwHVBC9FhGdlKrxdiJJigb/j/68SIy3Te4Bkz crossorigin=anonymous></script><script type=module src=https://akitenkrad.github.io/akitenkrad-blog/js/custom/custom.js></script><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Muli:wght@300;400;500;600"><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/fontawesome/css/all.min.css><link rel=icon type=image/png href=https://akitenkrad.github.io/akitenkrad-blog/images/favicons/favicon-96x96_huf1ee13f0caf27d1547f91fb46207d708_13005_42x0_resize_box_3.png><meta property="og:title" content="arXiv @ 2024.02.28"><meta property="og:description" content="Primary Categories astro-ph.SR (1) cond-mat.supr-con (1) cs.AI (10) cs.AR (1) cs.CE (1) cs.CL (62) cs.CR (8) cs.CV (40) cs.CY (3) cs.DM (1) cs.DS (2) cs.FL (2) cs.GR (2) cs.HC (5) cs.IR (11) cs.IT (9) cs.LG (51) cs.LO (1) cs.MA (2) cs.NE (3) cs.NI (1) cs.RO (13) cs.SD (2) cs.SE (7) cs.SI (1) eess.AS (3) eess.IV (3) eess.SP (1) eess.SY (8) hep-ex (1) math.NA (7) math.OC (1) math.ST (1) physics.chem-ph (1) physics."><meta property="og:type" content="article"><meta property="og:url" content="https://akitenkrad.github.io/akitenkrad-blog/posts/arxiv/202402/20240228000000/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-02-28T00:00:00+00:00"><meta property="article:modified_time" content="2024-02-28T00:00:00+00:00"><meta name=description content="arXiv @ 2024.02.28"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/atom-one-dark.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/layouts/single.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/navigators/sidebar.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/style.css><script type=text/javascript src=//ajax.googleapis.com/ajax/libs/jquery/3.1.1/jquery.min.js></script><script language=JavaScript>$(document).ready(function(){$("a[href^='http']:not([href*='"+location.hostname+"'])").attr("target","_blank")})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-1MYYZQG0WE"></script><script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-1MYYZQG0WE",{anonymize_ip:!1})}</script></head><body data-spy=scroll data-target=#TableOfContents data-offset=80><div class="container-fluid bg-dimmed wrapper"><nav class="navbar navbar-expand-xl top-navbar final-navbar shadow" style=position:fixed><div class=container><button class="navbar-toggler navbar-light navbar-for-sidebar" id=sidebar-toggler type=button onclick=toggleSidebar()>
<span class=navbar-toggler-icon></span>
</button>
<button class="navbar-toggler navbar-light" id=toc-toggler type=button onclick=toggleTOC()>
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse lang-selector" id=top-nav-items><ul class="navbar-nav mr-auto"><li class=nav-item><a class=navbar-brand href=/akitenkrad-blog><img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png alt=Logo>
Akitenkrad's Blog</a></li></ul><ul class="navbar-nav ml-auto"><li class=nav-item><a class=nav-link href=/akitenkrad-blog#home>Home</a></li><li class=nav-item><a class=nav-link href=/akitenkrad-blog#about>About</a></li><li class=nav-item><a class=nav-link href=/akitenkrad-blog#recent-posts>Recent Posts</a></li><li class=nav-item><a class=nav-link id=blog-link href=https://akitenkrad.github.io/akitenkrad-blog/posts>Posts</a></li><li class=nav-item><a class=nav-link id=tag-link href=https://akitenkrad.github.io/akitenkrad-blog/tags>Tags</a></li></ul></div></div><img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png class=d-none id=main-logo alt=Logo>
<img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png class=d-none id=inverted-logo alt="Inverted Logo"></nav><section class=sidebar-section id=sidebar-section><div class=sidebar-holder><div class=sidebar id=sidebar><form class=mx-auto method=get action=/akitenkrad-blog/search><input type=text name=keyword placeholder=Search data-search id=search-box></form><div class=sidebar-tree><ul class=tree id=tree><li id=list-heading><a href=/posts data-filter=all>Posts</a></li><div class=subtree><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/>Papers</a><ul><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202205/>2022.05</a><ul><li><a href=/akitenkrad-blog/posts/papers/202205/20220518224923/ title="A Context-Aware Citation Recommendation Model with BERT and Graph Convolutional Networks">A Context-Aware Citation Recommendation Model with BERT and Graph Convolutional Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220508162318/ title="A Deep Cascade Model for Multi-Document Reading Comprehension">A Deep Cascade Model for Multi-Document Reading Comprehension</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220506021208/ title="A Primer in BERTology: What We Know About How BERT Works">A Primer in BERTology: What We Know About How BERT Works</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220529131339/ title="Attention Is All You Need">Attention Is All You Need</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220503010000/ title="DeBERTa: Decoding-Enhanced BERT with Disentangled Attention">DeBERTa: Decoding-Enhanced BERT with Disentangled Attention</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220505222900/ title="Dense Passage Retrieval for Open-Domain Question Answering">Dense Passage Retrieval for Open-Domain Question Answering</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220511010217/ title="Multi-Style Generative Reading Comprehension">Multi-Style Generative Reading Comprehension</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220530102936/ title="Neural Machine Translation of Rare Words with Subword Units">Neural Machine Translation of Rare Words with Subword Units</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220523223206/ title="RoBERTa: A Robustly Optimized BERT Pretraining Approach">RoBERTa: A Robustly Optimized BERT Pretraining Approach</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220520124748/ title="Semi-Supervised Classification with Graph Convolutional Networks">Semi-Supervised Classification with Graph Convolutional Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220509110738/ title="Survey on graph embeddings and their applications to machine learning problems on graphs">Survey on graph embeddings and their applications to machine learning problems on graphs</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220514151839/ title="UnitedQA: A Hybrid Approach for Open Domain Question Answering">UnitedQA: A Hybrid Approach for Open Domain Question Answering</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202206/>2022.06</a><ul><li><a href=/akitenkrad-blog/posts/papers/202206/20220612105422/ title="Attributed Network Embedding for Learning in a Dynamic Environment">Attributed Network Embedding for Learning in a Dynamic Environment</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220608085622/ title="CodeBERT: A Pre-Trained Model for Programming and Natural Languages">CodeBERT: A Pre-Trained Model for Programming and Natural Languages</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220618223844/ title="High-order Proximity Preserved Embedding for Dynamic Networks">High-order Proximity Preserved Embedding for Dynamic Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220602171700/ title="S-Net: From Answer Extraction to Answer Generation for Machine Reading Comprehension">S-Net: From Answer Extraction to Answer Generation for Machine Reading Comprehension</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202207/>2022.07</a><ul><li><a href=/akitenkrad-blog/posts/papers/202207/20220727145036/ title="Dynamic Heterogeneous Graph Embedding Using Hierarchical Attentions">Dynamic Heterogeneous Graph Embedding Using Hierarchical Attentions</a></li><li><a href=/akitenkrad-blog/posts/papers/202207/20220726163444/ title="Dynamic Network Embedding Survey">Dynamic Network Embedding Survey</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202208/>2022.08</a><ul><li><a href=/akitenkrad-blog/posts/papers/202208/20220802103319/ title="Modeling Dynamic Heterogeneous Network for Link Prediction using Hierarchical Attention with Temporal RNN">Modeling Dynamic Heterogeneous Network for Link Prediction using Hierarchical Attention with Temporal RNN</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202209/>2022.09</a><ul><li><a href=/akitenkrad-blog/posts/papers/202209/20220909180827/ title="Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting">Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202305/>2023.05</a><ul><li><a href=/akitenkrad-blog/posts/papers/202305/20230513094804/ title="Improving Language Understanding by Generative Pre-Training">Improving Language Understanding by Generative Pre-Training</a></li><li><a href=/akitenkrad-blog/posts/papers/202305/20230514200241/ title="Language Models are Unsupervised Multitask Learners">Language Models are Unsupervised Multitask Learners</a></li></ul></li><li><a href=/akitenkrad-blog/posts/papers/202306/ title=2023.06>2023.06</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/akitenkrad-blog/posts/arxiv/>arXiv</a><ul class=active><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202307/>2023.07</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230701000000/ title="arXiv @ 2023.07.01">arXiv @ 2023.07.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230702000000/ title="arXiv @ 2023.07.02">arXiv @ 2023.07.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230703000000/ title="arXiv @ 2023.07.03">arXiv @ 2023.07.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230704000000/ title="arXiv @ 2023.07.04">arXiv @ 2023.07.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230705000000/ title="arXiv @ 2023.07.05">arXiv @ 2023.07.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230706000000/ title="arXiv @ 2023.07.06">arXiv @ 2023.07.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230707000000/ title="arXiv @ 2023.07.07">arXiv @ 2023.07.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230708000000/ title="arXiv @ 2023.07.08">arXiv @ 2023.07.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230709000000/ title="arXiv @ 2023.07.09">arXiv @ 2023.07.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230710000000/ title="arXiv @ 2023.07.10">arXiv @ 2023.07.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230711000000/ title="arXiv @ 2023.07.11">arXiv @ 2023.07.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230712000000/ title="arXiv @ 2023.07.12">arXiv @ 2023.07.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230713000000/ title="arXiv @ 2023.07.13">arXiv @ 2023.07.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230714000000/ title="arXiv @ 2023.07.14">arXiv @ 2023.07.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230715000000/ title="arXiv @ 2023.07.15">arXiv @ 2023.07.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230716000000/ title="arXiv @ 2023.07.16">arXiv @ 2023.07.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230717000000/ title="arXiv @ 2023.07.17">arXiv @ 2023.07.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230718000000/ title="arXiv @ 2023.07.18">arXiv @ 2023.07.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230719000000/ title="arXiv @ 2023.07.19">arXiv @ 2023.07.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230720000000/ title="arXiv @ 2023.07.20">arXiv @ 2023.07.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230721000000/ title="arXiv @ 2023.07.21">arXiv @ 2023.07.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230722000000/ title="arXiv @ 2023.07.22">arXiv @ 2023.07.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230723000000/ title="arXiv @ 2023.07.23">arXiv @ 2023.07.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230724000000/ title="arXiv @ 2023.07.24">arXiv @ 2023.07.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230725000000/ title="arXiv @ 2023.07.25">arXiv @ 2023.07.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230726000000/ title="arXiv @ 2023.07.26">arXiv @ 2023.07.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230727000000/ title="arXiv @ 2023.07.27">arXiv @ 2023.07.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230728000000/ title="arXiv @ 2023.07.28">arXiv @ 2023.07.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230729000000/ title="arXiv @ 2023.07.29">arXiv @ 2023.07.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230730000000/ title="arXiv @ 2023.07.30">arXiv @ 2023.07.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230731000000/ title="arXiv @ 2023.07.31">arXiv @ 2023.07.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202308/>2023.08</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230801000000/ title="arXiv @ 2023.08.01">arXiv @ 2023.08.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230802000000/ title="arXiv @ 2023.08.02">arXiv @ 2023.08.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230803000000/ title="arXiv @ 2023.08.03">arXiv @ 2023.08.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230804000000/ title="arXiv @ 2023.08.04">arXiv @ 2023.08.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230805000000/ title="arXiv @ 2023.08.05">arXiv @ 2023.08.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230806000000/ title="arXiv @ 2023.08.06">arXiv @ 2023.08.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230807000000/ title="arXiv @ 2023.08.07">arXiv @ 2023.08.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230808000000/ title="arXiv @ 2023.08.08">arXiv @ 2023.08.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230809000000/ title="arXiv @ 2023.08.09">arXiv @ 2023.08.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230810000000/ title="arXiv @ 2023.08.10">arXiv @ 2023.08.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230811000000/ title="arXiv @ 2023.08.11">arXiv @ 2023.08.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230812000000/ title="arXiv @ 2023.08.12">arXiv @ 2023.08.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230813000000/ title="arXiv @ 2023.08.13">arXiv @ 2023.08.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230814000000/ title="arXiv @ 2023.08.14">arXiv @ 2023.08.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230815000000/ title="arXiv @ 2023.08.15">arXiv @ 2023.08.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230816000000/ title="arXiv @ 2023.08.16">arXiv @ 2023.08.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230817000000/ title="arXiv @ 2023.08.17">arXiv @ 2023.08.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230818000000/ title="arXiv @ 2023.08.18">arXiv @ 2023.08.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230819000000/ title="arXiv @ 2023.08.19">arXiv @ 2023.08.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230820000000/ title="arXiv @ 2023.08.20">arXiv @ 2023.08.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230821000000/ title="arXiv @ 2023.08.21">arXiv @ 2023.08.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230822000000/ title="arXiv @ 2023.08.22">arXiv @ 2023.08.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230823000000/ title="arXiv @ 2023.08.23">arXiv @ 2023.08.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230824000000/ title="arXiv @ 2023.08.24">arXiv @ 2023.08.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230825000000/ title="arXiv @ 2023.08.25">arXiv @ 2023.08.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230826000000/ title="arXiv @ 2023.08.26">arXiv @ 2023.08.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230827000000/ title="arXiv @ 2023.08.27">arXiv @ 2023.08.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230828000000/ title="arXiv @ 2023.08.28">arXiv @ 2023.08.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230829000000/ title="arXiv @ 2023.08.29">arXiv @ 2023.08.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230830000000/ title="arXiv @ 2023.08.30">arXiv @ 2023.08.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230831000000/ title="arXiv @ 2023.08.31">arXiv @ 2023.08.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202309/>2023.09</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230901000000/ title="arXiv @ 2023.09.01">arXiv @ 2023.09.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230902000000/ title="arXiv @ 2023.09.02">arXiv @ 2023.09.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230903000000/ title="arXiv @ 2023.09.03">arXiv @ 2023.09.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230904000000/ title="arXiv @ 2023.09.04">arXiv @ 2023.09.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230905000000/ title="arXiv @ 2023.09.05">arXiv @ 2023.09.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230906000000/ title="arXiv @ 2023.09.06">arXiv @ 2023.09.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230907000000/ title="arXiv @ 2023.09.07">arXiv @ 2023.09.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230908000000/ title="arXiv @ 2023.09.08">arXiv @ 2023.09.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230909000000/ title="arXiv @ 2023.09.09">arXiv @ 2023.09.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230910000000/ title="arXiv @ 2023.09.10">arXiv @ 2023.09.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230911000000/ title="arXiv @ 2023.09.11">arXiv @ 2023.09.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230912000000/ title="arXiv @ 2023.09.12">arXiv @ 2023.09.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230913000000/ title="arXiv @ 2023.09.13">arXiv @ 2023.09.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230914000000/ title="arXiv @ 2023.09.14">arXiv @ 2023.09.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230915000000/ title="arXiv @ 2023.09.15">arXiv @ 2023.09.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230916000000/ title="arXiv @ 2023.09.16">arXiv @ 2023.09.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230917000000/ title="arXiv @ 2023.09.17">arXiv @ 2023.09.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230918000000/ title="arXiv @ 2023.09.18">arXiv @ 2023.09.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230919000000/ title="arXiv @ 2023.09.19">arXiv @ 2023.09.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230920000000/ title="arXiv @ 2023.09.20">arXiv @ 2023.09.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230921000000/ title="arXiv @ 2023.09.21">arXiv @ 2023.09.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230922000000/ title="arXiv @ 2023.09.22">arXiv @ 2023.09.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230923000000/ title="arXiv @ 2023.09.23">arXiv @ 2023.09.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230924000000/ title="arXiv @ 2023.09.24">arXiv @ 2023.09.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230925000000/ title="arXiv @ 2023.09.25">arXiv @ 2023.09.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230926000000/ title="arXiv @ 2023.09.26">arXiv @ 2023.09.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230927000000/ title="arXiv @ 2023.09.27">arXiv @ 2023.09.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230928000000/ title="arXiv @ 2023.09.28">arXiv @ 2023.09.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230929000000/ title="arXiv @ 2023.09.29">arXiv @ 2023.09.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230930000000/ title="arXiv @ 2023.09.30">arXiv @ 2023.09.30</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202310/>2023.1</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231001000000/ title="arXiv @ 2023.10.01">arXiv @ 2023.10.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231002000000/ title="arXiv @ 2023.10.02">arXiv @ 2023.10.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231003000000/ title="arXiv @ 2023.10.03">arXiv @ 2023.10.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231004000000/ title="arXiv @ 2023.10.04">arXiv @ 2023.10.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231005000000/ title="arXiv @ 2023.10.05">arXiv @ 2023.10.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231006000000/ title="arXiv @ 2023.10.06">arXiv @ 2023.10.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231007000000/ title="arXiv @ 2023.10.07">arXiv @ 2023.10.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231008000000/ title="arXiv @ 2023.10.08">arXiv @ 2023.10.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231009000000/ title="arXiv @ 2023.10.09">arXiv @ 2023.10.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231010000000/ title="arXiv @ 2023.10.10">arXiv @ 2023.10.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231011000000/ title="arXiv @ 2023.10.11">arXiv @ 2023.10.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231012000000/ title="arXiv @ 2023.10.12">arXiv @ 2023.10.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231013000000/ title="arXiv @ 2023.10.13">arXiv @ 2023.10.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231014000000/ title="arXiv @ 2023.10.14">arXiv @ 2023.10.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231015000000/ title="arXiv @ 2023.10.15">arXiv @ 2023.10.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231016000000/ title="arXiv @ 2023.10.16">arXiv @ 2023.10.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231017000000/ title="arXiv @ 2023.10.17">arXiv @ 2023.10.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231018000000/ title="arXiv @ 2023.10.18">arXiv @ 2023.10.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231019000000/ title="arXiv @ 2023.10.19">arXiv @ 2023.10.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231020000000/ title="arXiv @ 2023.10.20">arXiv @ 2023.10.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231021000000/ title="arXiv @ 2023.10.21">arXiv @ 2023.10.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231022000000/ title="arXiv @ 2023.10.22">arXiv @ 2023.10.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231023000000/ title="arXiv @ 2023.10.23">arXiv @ 2023.10.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231024000000/ title="arXiv @ 2023.10.24">arXiv @ 2023.10.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231025000000/ title="arXiv @ 2023.10.25">arXiv @ 2023.10.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231026000000/ title="arXiv @ 2023.10.26">arXiv @ 2023.10.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231027000000/ title="arXiv @ 2023.10.27">arXiv @ 2023.10.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231028000000/ title="arXiv @ 2023.10.28">arXiv @ 2023.10.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231029000000/ title="arXiv @ 2023.10.29">arXiv @ 2023.10.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231030000000/ title="arXiv @ 2023.10.30">arXiv @ 2023.10.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231031000000/ title="arXiv @ 2023.10.31">arXiv @ 2023.10.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202311/>2023.11</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231101000000/ title="arXiv @ 2023.11.01">arXiv @ 2023.11.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231102000000/ title="arXiv @ 2023.11.02">arXiv @ 2023.11.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231103000000/ title="arXiv @ 2023.11.03">arXiv @ 2023.11.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231104000000/ title="arXiv @ 2023.11.04">arXiv @ 2023.11.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231105000000/ title="arXiv @ 2023.11.05">arXiv @ 2023.11.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231106000000/ title="arXiv @ 2023.11.06">arXiv @ 2023.11.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231107000000/ title="arXiv @ 2023.11.07">arXiv @ 2023.11.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231108000000/ title="arXiv @ 2023.11.08">arXiv @ 2023.11.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231109000000/ title="arXiv @ 2023.11.09">arXiv @ 2023.11.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231110000000/ title="arXiv @ 2023.11.10">arXiv @ 2023.11.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231111000000/ title="arXiv @ 2023.11.11">arXiv @ 2023.11.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231112000000/ title="arXiv @ 2023.11.12">arXiv @ 2023.11.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231113000000/ title="arXiv @ 2023.11.13">arXiv @ 2023.11.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231114000000/ title="arXiv @ 2023.11.14">arXiv @ 2023.11.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231115000000/ title="arXiv @ 2023.11.15">arXiv @ 2023.11.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231116000000/ title="arXiv @ 2023.11.16">arXiv @ 2023.11.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231117000000/ title="arXiv @ 2023.11.17">arXiv @ 2023.11.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231118000000/ title="arXiv @ 2023.11.18">arXiv @ 2023.11.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231119000000/ title="arXiv @ 2023.11.19">arXiv @ 2023.11.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231120000000/ title="arXiv @ 2023.11.20">arXiv @ 2023.11.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231121000000/ title="arXiv @ 2023.11.21">arXiv @ 2023.11.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231122000000/ title="arXiv @ 2023.11.22">arXiv @ 2023.11.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231123000000/ title="arXiv @ 2023.11.23">arXiv @ 2023.11.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231124000000/ title="arXiv @ 2023.11.24">arXiv @ 2023.11.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231125000000/ title="arXiv @ 2023.11.25">arXiv @ 2023.11.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231126000000/ title="arXiv @ 2023.11.26">arXiv @ 2023.11.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231127000000/ title="arXiv @ 2023.11.27">arXiv @ 2023.11.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231128000000/ title="arXiv @ 2023.11.28">arXiv @ 2023.11.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231129000000/ title="arXiv @ 2023.11.29">arXiv @ 2023.11.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231130000000/ title="arXiv @ 2023.11.30">arXiv @ 2023.11.30</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202312/>2023.12</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231201000000/ title="arXiv @ 2023.12.01">arXiv @ 2023.12.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231202000000/ title="arXiv @ 2023.12.02">arXiv @ 2023.12.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231203000000/ title="arXiv @ 2023.12.03">arXiv @ 2023.12.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231204000000/ title="arXiv @ 2023.12.04">arXiv @ 2023.12.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231205000000/ title="arXiv @ 2023.12.05">arXiv @ 2023.12.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231206000000/ title="arXiv @ 2023.12.06">arXiv @ 2023.12.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231207000000/ title="arXiv @ 2023.12.07">arXiv @ 2023.12.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231208000000/ title="arXiv @ 2023.12.08">arXiv @ 2023.12.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231209000000/ title="arXiv @ 2023.12.09">arXiv @ 2023.12.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231210000000/ title="arXiv @ 2023.12.10">arXiv @ 2023.12.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231211000000/ title="arXiv @ 2023.12.11">arXiv @ 2023.12.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231212000000/ title="arXiv @ 2023.12.12">arXiv @ 2023.12.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231213000000/ title="arXiv @ 2023.12.13">arXiv @ 2023.12.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231214000000/ title="arXiv @ 2023.12.14">arXiv @ 2023.12.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231215000000/ title="arXiv @ 2023.12.15">arXiv @ 2023.12.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231216000000/ title="arXiv @ 2023.12.16">arXiv @ 2023.12.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231217000000/ title="arXiv @ 2023.12.17">arXiv @ 2023.12.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231218000000/ title="arXiv @ 2023.12.18">arXiv @ 2023.12.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231219000000/ title="arXiv @ 2023.12.19">arXiv @ 2023.12.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231220000000/ title="arXiv @ 2023.12.20">arXiv @ 2023.12.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231221000000/ title="arXiv @ 2023.12.21">arXiv @ 2023.12.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231222000000/ title="arXiv @ 2023.12.22">arXiv @ 2023.12.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231223000000/ title="arXiv @ 2023.12.23">arXiv @ 2023.12.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231224000000/ title="arXiv @ 2023.12.24">arXiv @ 2023.12.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231225000000/ title="arXiv @ 2023.12.25">arXiv @ 2023.12.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231226000000/ title="arXiv @ 2023.12.26">arXiv @ 2023.12.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231227000000/ title="arXiv @ 2023.12.27">arXiv @ 2023.12.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231228000000/ title="arXiv @ 2023.12.28">arXiv @ 2023.12.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231229000000/ title="arXiv @ 2023.12.29">arXiv @ 2023.12.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231230000000/ title="arXiv @ 2023.12.30">arXiv @ 2023.12.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231231000000/ title="arXiv @ 2023.12.31">arXiv @ 2023.12.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202401/>2024.01</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240101000000/ title="arXiv @ 2024.01.01">arXiv @ 2024.01.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240102000000/ title="arXiv @ 2024.01.02">arXiv @ 2024.01.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240103000000/ title="arXiv @ 2024.01.03">arXiv @ 2024.01.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240104000000/ title="arXiv @ 2024.01.04">arXiv @ 2024.01.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240105000000/ title="arXiv @ 2024.01.05">arXiv @ 2024.01.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240106000000/ title="arXiv @ 2024.01.06">arXiv @ 2024.01.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240107000000/ title="arXiv @ 2024.01.07">arXiv @ 2024.01.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240108000000/ title="arXiv @ 2024.01.08">arXiv @ 2024.01.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240109000000/ title="arXiv @ 2024.01.09">arXiv @ 2024.01.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240110000000/ title="arXiv @ 2024.01.10">arXiv @ 2024.01.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240111000000/ title="arXiv @ 2024.01.11">arXiv @ 2024.01.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240112000000/ title="arXiv @ 2024.01.12">arXiv @ 2024.01.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240113000000/ title="arXiv @ 2024.01.13">arXiv @ 2024.01.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240114000000/ title="arXiv @ 2024.01.14">arXiv @ 2024.01.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240115000000/ title="arXiv @ 2024.01.15">arXiv @ 2024.01.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240116000000/ title="arXiv @ 2024.01.16">arXiv @ 2024.01.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240117000000/ title="arXiv @ 2024.01.17">arXiv @ 2024.01.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240118000000/ title="arXiv @ 2024.01.18">arXiv @ 2024.01.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240119000000/ title="arXiv @ 2024.01.19">arXiv @ 2024.01.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240120000000/ title="arXiv @ 2024.01.20">arXiv @ 2024.01.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240121000000/ title="arXiv @ 2024.01.21">arXiv @ 2024.01.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240122000000/ title="arXiv @ 2024.01.22">arXiv @ 2024.01.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240123000000/ title="arXiv @ 2024.01.23">arXiv @ 2024.01.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240124000000/ title="arXiv @ 2024.01.24">arXiv @ 2024.01.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240125000000/ title="arXiv @ 2024.01.25">arXiv @ 2024.01.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240126000000/ title="arXiv @ 2024.01.26">arXiv @ 2024.01.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240127000000/ title="arXiv @ 2024.01.27">arXiv @ 2024.01.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240128000000/ title="arXiv @ 2024.01.28">arXiv @ 2024.01.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240129000000/ title="arXiv @ 2024.01.29">arXiv @ 2024.01.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240130000000/ title="arXiv @ 2024.01.30">arXiv @ 2024.01.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240131000000/ title="arXiv @ 2024.01.31">arXiv @ 2024.01.31</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/akitenkrad-blog/posts/arxiv/202402/>2024.02</a><ul class=active><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240201000000/ title="arXiv @ 2024.02.01">arXiv @ 2024.02.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240202000000/ title="arXiv @ 2024.02.02">arXiv @ 2024.02.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240203000000/ title="arXiv @ 2024.02.03">arXiv @ 2024.02.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240204000000/ title="arXiv @ 2024.02.04">arXiv @ 2024.02.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240205000000/ title="arXiv @ 2024.02.05">arXiv @ 2024.02.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240206000000/ title="arXiv @ 2024.02.06">arXiv @ 2024.02.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240207000000/ title="arXiv @ 2024.02.07">arXiv @ 2024.02.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240208000000/ title="arXiv @ 2024.02.08">arXiv @ 2024.02.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240209000000/ title="arXiv @ 2024.02.09">arXiv @ 2024.02.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240210000000/ title="arXiv @ 2024.02.10">arXiv @ 2024.02.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240211000000/ title="arXiv @ 2024.02.11">arXiv @ 2024.02.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240212000000/ title="arXiv @ 2024.02.12">arXiv @ 2024.02.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240213000000/ title="arXiv @ 2024.02.13">arXiv @ 2024.02.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240214000000/ title="arXiv @ 2024.02.14">arXiv @ 2024.02.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240215000000/ title="arXiv @ 2024.02.15">arXiv @ 2024.02.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240216000000/ title="arXiv @ 2024.02.16">arXiv @ 2024.02.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240217000000/ title="arXiv @ 2024.02.17">arXiv @ 2024.02.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240218000000/ title="arXiv @ 2024.02.18">arXiv @ 2024.02.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240219000000/ title="arXiv @ 2024.02.19">arXiv @ 2024.02.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240220000000/ title="arXiv @ 2024.02.20">arXiv @ 2024.02.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240221000000/ title="arXiv @ 2024.02.21">arXiv @ 2024.02.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240222000000/ title="arXiv @ 2024.02.22">arXiv @ 2024.02.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240223000000/ title="arXiv @ 2024.02.23">arXiv @ 2024.02.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240224000000/ title="arXiv @ 2024.02.24">arXiv @ 2024.02.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240225000000/ title="arXiv @ 2024.02.25">arXiv @ 2024.02.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240226000000/ title="arXiv @ 2024.02.26">arXiv @ 2024.02.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240227000000/ title="arXiv @ 2024.02.27">arXiv @ 2024.02.27</a></li><li><a class=active href=/akitenkrad-blog/posts/arxiv/202402/20240228000000/ title="arXiv @ 2024.02.28">arXiv @ 2024.02.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240229000000/ title="arXiv @ 2024.02.29">arXiv @ 2024.02.29</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202403/>2024.03</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240301000000/ title="arXiv @ 2024.03.01">arXiv @ 2024.03.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240302000000/ title="arXiv @ 2024.03.02">arXiv @ 2024.03.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240303000000/ title="arXiv @ 2024.03.03">arXiv @ 2024.03.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240304000000/ title="arXiv @ 2024.03.04">arXiv @ 2024.03.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240305000000/ title="arXiv @ 2024.03.05">arXiv @ 2024.03.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240306000000/ title="arXiv @ 2024.03.06">arXiv @ 2024.03.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240307000000/ title="arXiv @ 2024.03.07">arXiv @ 2024.03.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240308000000/ title="arXiv @ 2024.03.08">arXiv @ 2024.03.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240309000000/ title="arXiv @ 2024.03.09">arXiv @ 2024.03.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240310000000/ title="arXiv @ 2024.03.10">arXiv @ 2024.03.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240311000000/ title="arXiv @ 2024.03.11">arXiv @ 2024.03.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240312000000/ title="arXiv @ 2024.03.12">arXiv @ 2024.03.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240313000000/ title="arXiv @ 2024.03.13">arXiv @ 2024.03.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240314000000/ title="arXiv @ 2024.03.14">arXiv @ 2024.03.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240315000000/ title="arXiv @ 2024.03.15">arXiv @ 2024.03.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240316000000/ title="arXiv @ 2024.03.16">arXiv @ 2024.03.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240317000000/ title="arXiv @ 2024.03.17">arXiv @ 2024.03.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240318000000/ title="arXiv @ 2024.03.18">arXiv @ 2024.03.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240319000000/ title="arXiv @ 2024.03.19">arXiv @ 2024.03.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240320000000/ title="arXiv @ 2024.03.20">arXiv @ 2024.03.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240321000000/ title="arXiv @ 2024.03.21">arXiv @ 2024.03.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240322000000/ title="arXiv @ 2024.03.22">arXiv @ 2024.03.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240323000000/ title="arXiv @ 2024.03.23">arXiv @ 2024.03.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240324000000/ title="arXiv @ 2024.03.24">arXiv @ 2024.03.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240325000000/ title="arXiv @ 2024.03.25">arXiv @ 2024.03.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240326000000/ title="arXiv @ 2024.03.26">arXiv @ 2024.03.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240327000000/ title="arXiv @ 2024.03.27">arXiv @ 2024.03.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240328000000/ title="arXiv @ 2024.03.28">arXiv @ 2024.03.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240329000000/ title="arXiv @ 2024.03.29">arXiv @ 2024.03.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240330000000/ title="arXiv @ 2024.03.30">arXiv @ 2024.03.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240331000000/ title="arXiv @ 2024.03.31">arXiv @ 2024.03.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202404/>2024.04</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240401000000/ title="arXiv @ 2024.04.01">arXiv @ 2024.04.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240402000000/ title="arXiv @ 2024.04.02">arXiv @ 2024.04.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240403000000/ title="arXiv @ 2024.04.03">arXiv @ 2024.04.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240404000000/ title="arXiv @ 2024.04.04">arXiv @ 2024.04.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240405000000/ title="arXiv @ 2024.04.05">arXiv @ 2024.04.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240406000000/ title="arXiv @ 2024.04.06">arXiv @ 2024.04.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240407000000/ title="arXiv @ 2024.04.07">arXiv @ 2024.04.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240408000000/ title="arXiv @ 2024.04.08">arXiv @ 2024.04.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240409000000/ title="arXiv @ 2024.04.09">arXiv @ 2024.04.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240410000000/ title="arXiv @ 2024.04.10">arXiv @ 2024.04.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240411000000/ title="arXiv @ 2024.04.11">arXiv @ 2024.04.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240412000000/ title="arXiv @ 2024.04.12">arXiv @ 2024.04.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202404/20240413000000/ title="arXiv @ 2024.04.13">arXiv @ 2024.04.13</a></li></ul></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/algorithms/>Algorithms</a><ul><li><a href=/akitenkrad-blog/posts/algorithms/bandit-algorithm-basic/ title="Bandit Algorithm Basic">Bandit Algorithm Basic</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/conference/>Conference</a><ul><li><a href=/akitenkrad-blog/posts/conference/acl/ title="Annual Meeting of the Association for Computational Linguistics">Annual Meeting of the Association for Computational Linguistics</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/>Figures</a><ul><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/202208/>2022.08</a><ul><li><a href=/akitenkrad-blog/posts/figures/202208/20220822092748/ title=1ヶ月当たりの実労働時間の推移>1ヶ月当たりの実労働時間の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220813115200/ title=コンビニエンスストアの店舗数の推移>コンビニエンスストアの店舗数の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220821113322/ title=地域別1世帯当たり1ヶ月間の支出額の推移>地域別1世帯当たり1ヶ月間の支出額の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220828111836/ title="夫婦別子供の有無別生活時間 (2016)">夫婦別子供の有無別生活時間 (2016)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220827103615/ title=家族類型の変遷>家族類型の変遷</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220815131016/ title="年齢別大学院入学者数 (2021)">年齢別大学院入学者数 (2021)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220818122628/ title=情報サービス業における企業特殊的人的資本（名目）の推移>情報サービス業における企業特殊的人的資本（名目）の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220824095127/ title=有業者の社会生活における活動の変化>有業者の社会生活における活動の変化</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220814175022/ title=東京23区の乗用車保有台数>東京23区の乗用車保有台数</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220817121117/ title="業種別企業特殊的人的資本 (2018)">業種別企業特殊的人的資本 (2018)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220825094111/ title=無業者の社会生活における活動の変化>無業者の社会生活における活動の変化</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220823104100/ title=睡眠時間の推移>睡眠時間の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220826102652/ title=総人口の推移>総人口の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220820225528/ title=都道府県別国公立別学校数・学生数>都道府県別国公立別学校数・学生数</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220816120845/ title="都道府県別消費者物価指数/教育（全国平均=100）">都道府県別消費者物価指数/教育（全国平均=100）</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220812173814/ title=金沢市の夏の月平均気温の遷移>金沢市の夏の月平均気温の遷移</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/202209/>2022.09</a><ul><li><a href=/akitenkrad-blog/posts/figures/202209/20220907122906/ title="ACL-2022 WordCloud">ACL-2022 WordCloud</a></li></ul></li></ul></li><li><a href=/akitenkrad-blog/posts/latex/ title="Latex Mathematics Syntax Guide">Latex Mathematics Syntax Guide</a></li><li><a href=/akitenkrad-blog/posts/markdown/ title="Markdown Sample">Markdown Sample</a></li></div></ul></div></div></div></section><section class=content-section id=content-section><div class=content><div class="container p-0 read-area"><div class="hero-area col-sm-12" id=hero-area style=background-image:url(/akitenkrad-blog/posts/arxiv/202402/20240228000000/hero.png)></div><div class=page-content><div class="author-profile ml-auto align-self-lg-center"><img class=rounded-circle src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_120x120_fit_box_3.png alt="Author Image"><h5 class=author-name></h5><p>Wednesday, Feb 28, 2024</p></div><div class=title><h1>arXiv @ 2024.02.28</h1></div><div class=taxonomy-terms><ul><li class=rounded><a href=/akitenkrad-blog/tags/arxiv class="btn, btn-sm">arXiv</a></li><li class=rounded><a href=/akitenkrad-blog/tags/published2024 class="btn, btn-sm">2024</a></li></ul></div><div class=post-content id=post-content><figure style=border:none;width:100%;display:flex;justify-content:center><iframe src=pie.html width=900 height=620 style=border:none></iframe></figure><h2 id=primary-categories>Primary Categories</h2><ul><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240228000000/#astro-phsr-1>astro-ph.SR (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240228000000/#cond-matsupr-con-1>cond-mat.supr-con (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240228000000/#csai-10>cs.AI (10)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240228000000/#csar-1>cs.AR (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240228000000/#csce-1>cs.CE (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240228000000/#cscl-62>cs.CL (62)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240228000000/#cscr-8>cs.CR (8)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240228000000/#cscv-40>cs.CV (40)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240228000000/#cscy-3>cs.CY (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240228000000/#csdm-1>cs.DM (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240228000000/#csds-2>cs.DS (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240228000000/#csfl-2>cs.FL (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240228000000/#csgr-2>cs.GR (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240228000000/#cshc-5>cs.HC (5)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240228000000/#csir-11>cs.IR (11)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240228000000/#csit-9>cs.IT (9)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240228000000/#cslg-51>cs.LG (51)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240228000000/#cslo-1>cs.LO (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240228000000/#csma-2>cs.MA (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240228000000/#csne-3>cs.NE (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240228000000/#csni-1>cs.NI (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240228000000/#csro-13>cs.RO (13)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240228000000/#cssd-2>cs.SD (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240228000000/#csse-7>cs.SE (7)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240228000000/#cssi-1>cs.SI (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240228000000/#eessas-3>eess.AS (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240228000000/#eessiv-3>eess.IV (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240228000000/#eesssp-1>eess.SP (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240228000000/#eesssy-8>eess.SY (8)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240228000000/#hep-ex-1>hep-ex (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240228000000/#mathna-7>math.NA (7)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240228000000/#mathoc-1>math.OC (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240228000000/#mathst-1>math.ST (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240228000000/#physicschem-ph-1>physics.chem-ph (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240228000000/#physicsflu-dyn-1>physics.flu-dyn (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240228000000/#physicsoptics-1>physics.optics (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240228000000/#q-bionc-1>q-bio.NC (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240228000000/#q-bioqm-1>q-bio.QM (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240228000000/#quant-ph-1>quant-ph (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240228000000/#statml-8>stat.ML (8)</a></li></ul><h2 id=keywords>Keywords</h2><table border=1 class=dataframe><thead><tr style=text-align:right><th>keyword</th><th>cs.AI</th><th>cs.CL</th><th>cs.CV</th><th>cs.IR</th><th>cs.LG</th><th>cs.RO</th></tr></thead><tbody><tr><td>Adversarial Attack</td><td></td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Adversarial Detection</td><td></td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Adversarial Learning</td><td></td><td>2</td><td>1</td><td></td><td>1</td><td></td></tr><tr><td>Alpaca</td><td></td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Automatic Speech Recognition</td><td></td><td>2</td><td></td><td></td><td></td><td></td></tr><tr><td>BART</td><td></td><td>1</td><td></td><td></td><td>1</td><td></td></tr><tr><td>BERT</td><td></td><td>3</td><td></td><td></td><td></td><td></td></tr><tr><td>BERTScore</td><td></td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>BLOOM</td><td></td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Bandit Algorithm</td><td></td><td></td><td></td><td></td><td>3</td><td></td></tr><tr><td>Benchmarking</td><td></td><td>17</td><td>11</td><td>2</td><td>11</td><td>2</td></tr><tr><td>Black Box</td><td></td><td></td><td></td><td></td><td>3</td><td>1</td></tr><tr><td>ChatGPT</td><td>1</td><td>7</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Chatbot</td><td></td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Clustering</td><td></td><td>1</td><td></td><td></td><td>2</td><td>2</td></tr><tr><td>Code Generation</td><td></td><td>2</td><td></td><td></td><td></td><td></td></tr><tr><td>Common-sense Reasoning</td><td></td><td></td><td></td><td></td><td></td><td>1</td></tr><tr><td>Continual Learning</td><td></td><td></td><td></td><td>1</td><td>1</td><td></td></tr><tr><td>Continuous Time</td><td></td><td></td><td></td><td></td><td></td><td>2</td></tr><tr><td>Contrastive Learning</td><td>1</td><td>1</td><td>1</td><td>1</td><td></td><td></td></tr><tr><td>Convolution</td><td></td><td></td><td>7</td><td></td><td>4</td><td>1</td></tr><tr><td>Convolutional Neural Network</td><td></td><td></td><td>5</td><td></td><td>3</td><td>1</td></tr><tr><td>Data Augmentation</td><td></td><td>3</td><td>1</td><td></td><td>1</td><td></td></tr><tr><td>Differential Privacy</td><td></td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Diffusion Model</td><td></td><td></td><td>5</td><td></td><td>2</td><td></td></tr><tr><td>Disambiguation</td><td>1</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Distribution Shift</td><td></td><td></td><td></td><td></td><td>4</td><td></td></tr><tr><td>Domain Adaptation</td><td></td><td>1</td><td></td><td></td><td>2</td><td></td></tr><tr><td>Event Detection</td><td></td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Face Recognition</td><td></td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Fairness</td><td></td><td>1</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Federated Learning</td><td></td><td></td><td></td><td></td><td>2</td><td></td></tr><tr><td>Few-shot</td><td></td><td>2</td><td>3</td><td></td><td></td><td></td></tr><tr><td>Few-shot Learning</td><td></td><td></td><td>2</td><td></td><td></td><td></td></tr><tr><td>Fine-tuning</td><td></td><td>12</td><td>5</td><td>2</td><td>10</td><td></td></tr><tr><td>Foundation Model</td><td></td><td>2</td><td>1</td><td></td><td>3</td><td></td></tr><tr><td>GPT</td><td>1</td><td>8</td><td>4</td><td></td><td>1</td><td></td></tr><tr><td>GPT-2</td><td></td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>GPT-3</td><td></td><td>2</td><td></td><td></td><td></td><td></td></tr><tr><td>GPT-3.5</td><td></td><td>2</td><td></td><td></td><td></td><td></td></tr><tr><td>GPT-4</td><td></td><td>5</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Gaussian Process</td><td></td><td></td><td></td><td></td><td></td><td>1</td></tr><tr><td>Gemini</td><td>1</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Generative AI</td><td></td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Generative Adversarial Network</td><td></td><td></td><td>2</td><td></td><td>3</td><td></td></tr><tr><td>Geometry</td><td></td><td></td><td>2</td><td></td><td></td><td></td></tr><tr><td>Grammatical Error Correction</td><td></td><td>2</td><td></td><td></td><td></td><td></td></tr><tr><td>Graph</td><td>2</td><td>3</td><td>1</td><td>2</td><td>11</td><td></td></tr><tr><td>Graph Attention Networks</td><td>1</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Graph Neural Network</td><td></td><td>1</td><td></td><td>4</td><td>7</td><td></td></tr><tr><td>GraphSAGE</td><td></td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Grounding</td><td></td><td>2</td><td>2</td><td></td><td></td><td></td></tr><tr><td>Hate Speech Detection</td><td></td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>High-Resource</td><td></td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>In-context Learning</td><td></td><td>6</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Information Retrieval</td><td></td><td>2</td><td></td><td></td><td></td><td></td></tr><tr><td>Instruction Following</td><td></td><td>1</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Instruction Tuning</td><td></td><td>5</td><td>1</td><td>1</td><td>1</td><td></td></tr><tr><td>Knowledge Distillation</td><td>1</td><td>5</td><td>2</td><td></td><td>3</td><td></td></tr><tr><td>Knowledge Graph</td><td>1</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Knowledge Transfer</td><td>1</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>LLaMA</td><td></td><td>5</td><td></td><td></td><td>1</td><td></td></tr><tr><td>LSTM</td><td></td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Language Generation</td><td></td><td>1</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Large Language Model</td><td>7</td><td>82</td><td>3</td><td>6</td><td>6</td><td></td></tr><tr><td>Low-Resource</td><td></td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Machine Unlearning</td><td></td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Markov Decision Process</td><td>1</td><td></td><td></td><td></td><td>2</td><td></td></tr><tr><td>Mathematical Reasoning</td><td></td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Message-Passing</td><td></td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Meta Learning</td><td></td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Mistral</td><td></td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Model Compression</td><td></td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Model Pruning</td><td></td><td>2</td><td></td><td></td><td></td><td></td></tr><tr><td>Multi-modal</td><td>7</td><td>2</td><td>9</td><td>2</td><td>1</td><td>3</td></tr><tr><td>Named Entity Recognition</td><td></td><td>3</td><td></td><td></td><td></td><td></td></tr><tr><td>Natural Language Generation</td><td></td><td>1</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Natural Language Understanding</td><td></td><td>1</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Neural Machine Translation</td><td></td><td>5</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Node Embedding</td><td>1</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Object Detection</td><td></td><td></td><td>3</td><td></td><td></td><td></td></tr><tr><td>Online Reinforcement Learning</td><td></td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Open-Domain Question Answering</td><td></td><td>2</td><td></td><td></td><td></td><td></td></tr><tr><td>Out-of-distribution</td><td></td><td></td><td>1</td><td></td><td>2</td><td></td></tr><tr><td>PaLM</td><td></td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Parameter Sharing</td><td></td><td>1</td><td>1</td><td></td><td>1</td><td></td></tr><tr><td>Perplexity</td><td></td><td>1</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Pre-trained Language Model</td><td></td><td>6</td><td></td><td>2</td><td></td><td></td></tr><tr><td>Probabilistic Model</td><td></td><td></td><td>2</td><td></td><td>1</td><td></td></tr><tr><td>Prompt</td><td>1</td><td>12</td><td>1</td><td>1</td><td>3</td><td></td></tr><tr><td>Pruning</td><td></td><td>2</td><td>1</td><td></td><td>1</td><td></td></tr><tr><td>Quantization</td><td></td><td>4</td><td>1</td><td>1</td><td></td><td></td></tr><tr><td>Question Answering</td><td>1</td><td>10</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Reasoning</td><td>2</td><td>5</td><td>1</td><td></td><td></td><td>1</td></tr><tr><td>Recommendation</td><td></td><td>1</td><td></td><td>6</td><td>4</td><td></td></tr><tr><td>Recommender System</td><td></td><td></td><td></td><td>4</td><td></td><td></td></tr><tr><td>Recurrent Neural Network</td><td></td><td></td><td></td><td></td><td>5</td><td></td></tr><tr><td>Reinforcement Learning</td><td></td><td></td><td></td><td></td><td>9</td><td>2</td></tr><tr><td>Representation Learning</td><td></td><td></td><td></td><td>1</td><td>1</td><td>1</td></tr><tr><td>Retrieval-Augmented Generation</td><td></td><td>5</td><td></td><td>3</td><td>2</td><td></td></tr><tr><td>RoBERTa</td><td></td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Rouge</td><td></td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Sample Size</td><td></td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Self-Attention</td><td></td><td>1</td><td>2</td><td></td><td></td><td></td></tr><tr><td>Self-Distillation</td><td></td><td>1</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Self-supervised Learning</td><td></td><td>1</td><td></td><td>1</td><td>3</td><td>1</td></tr><tr><td>Self-supervised Pre-training</td><td></td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Semi-Supervised Learning</td><td></td><td></td><td>1</td><td></td><td>1</td><td></td></tr><tr><td>Sentiment Analysis</td><td></td><td>2</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Simulation</td><td>1</td><td></td><td></td><td></td><td>4</td><td>8</td></tr><tr><td>Simulator</td><td>1</td><td></td><td></td><td></td><td>4</td><td>8</td></tr><tr><td>Stance Detection</td><td></td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Stochastic Gradient Descent</td><td></td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Summarization</td><td></td><td>4</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Supervised Learning</td><td></td><td>2</td><td>4</td><td></td><td>2</td><td></td></tr><tr><td>T5</td><td></td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>TF-IDF</td><td></td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Temporal Knowledge Graph</td><td></td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Tensor Decomposition</td><td></td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Text Classification</td><td></td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Text Embedding</td><td></td><td>1</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Text2SQL</td><td></td><td>2</td><td></td><td></td><td></td><td></td></tr><tr><td>Text2image</td><td></td><td></td><td>5</td><td></td><td>2</td><td></td></tr><tr><td>Transfer Learning</td><td></td><td></td><td>1</td><td></td><td>1</td><td></td></tr><tr><td>Transformer</td><td></td><td>7</td><td>3</td><td></td><td>7</td><td>1</td></tr><tr><td>Unsupervised Learning</td><td></td><td>1</td><td>2</td><td></td><td>3</td><td></td></tr><tr><td>Vision Transformer</td><td></td><td></td><td>2</td><td></td><td>2</td><td></td></tr><tr><td>Vision-and-Language</td><td></td><td></td><td>1</td><td></td><td>1</td><td></td></tr><tr><td>Visual Question Answering</td><td></td><td></td><td></td><td>2</td><td></td><td></td></tr><tr><td>Yolo</td><td></td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Zero-shot</td><td></td><td>2</td><td>2</td><td></td><td>1</td><td></td></tr></tbody></table><script>$(function(){$("table").addClass("keyword-table table-bordered border-success"),$("table thead").addClass("sticky-top"),$("table tbody td").css("text-align","")})</script><h2 id=csir-11>cs.IR (11)</h2><h3 id=111--1279-integrating-large-language-models-with-graphical-session-based-recommendation-naicheng-guo-et-al-2024>(1/11 | 1/279) Integrating Large Language Models with Graphical Session-Based Recommendation (Naicheng Guo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Naicheng Guo, Hongwei Cheng, Qianqiao Liang, Linxun Chen, Bing Han. (2024)<br><strong>Integrating Large Language Models with Graphical Session-Based Recommendation</strong><br><button class=copy-to-clipboard title="Integrating Large Language Models with Graphical Session-Based Recommendation" index=1>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-1 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-CL, cs-IR, cs-LG, cs.IR<br>Keyword Score: 113<br>Keywords: Graph, Graph Neural Network, Graph Neural Network, Recommendation, Recommender System, Language Generation, Natural Language Generation, Natural Language Understanding, Instruction Tuning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16539v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16539v1.pdf filename=2402.16539v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the rapid development of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs),</b> various explorations have arisen to utilize <b>LLMs</b> capability of context understanding on <b>recommender</b> <b>systems.</b> While pioneering strategies have primarily transformed traditional <b>recommendation</b> tasks into challenges of <b>natural</b> <b>language</b> <b>generation,</b> there has been a relative scarcity of exploration in the domain of session-based <b>recommendation</b> (SBR) due to its specificity. SBR has been primarily dominated by <b>Graph</b> <b>Neural</b> <b>Networks,</b> which have achieved many successful outcomes due to their ability to capture both the implicit and explicit relationships between adjacent behaviors. The structural nature of <b>graphs</b> <b>contrasts</b> <b>with</b> the essence of <b>natural</b> <b>language,</b> <b>posing</b> a significant adaptation gap for <b>LLMs.</b> In this paper, we introduce <b>large</b> <b>language</b> <b>models</b> with graphical Session-Based <b>recommendation,</b> named LLMGR, an effective framework that bridges the aforementioned gap by harmoniously integrating <b>LLMs</b> with <b>Graph</b> <b>Neural</b> <b>Networks</b> <b>(GNNs)</b> for SBR tasks. This integration seeks to leverage the complementary strengths of <b>LLMs</b> in <b>natural</b> <b>language</b> <b>understanding</b> and <b>GNNs</b> in relational data processing, leading to a more powerful session-based <b>recommender</b> <b>system</b> that can understand and recommend items within a session. Moreover, to endow the <b>LLM</b> with the capability to empower SBR tasks, we design a series of <b>prompts</b> for both auxiliary and major <b>instruction</b> <b>tuning</b> tasks. These <b>prompts</b> are crafted to assist the <b>LLM</b> in understanding <b>graph-structured</b> <b>data</b> <b>and</b> align textual information with nodes, effectively translating nuanced user interactions into a format that can be understood and utilized by <b>LLM</b> architectures. Extensive experiments on three real-world datasets demonstrate that LLMGR outperforms several competitive baselines, indicating its effectiveness in enhancing SBR tasks and its potential as a research direction for future exploration.</p></p class="citation"></blockquote><h3 id=211--2279-a-fine-tuning-enhanced-rag-system-with-quantized-influence-measure-as-ai-judge-keshav-rangan-et-al-2024>(2/11 | 2/279) A Fine-tuning Enhanced RAG System with Quantized Influence Measure as AI Judge (Keshav Rangan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Keshav Rangan, Yiqiao Yin. (2024)<br><strong>A Fine-tuning Enhanced RAG System with Quantized Influence Measure as AI Judge</strong><br><button class=copy-to-clipboard title="A Fine-tuning Enhanced RAG System with Quantized Influence Measure as AI Judge" index=2>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-2 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 90<br>Keywords: Fine-tuning, Fine-tuning, Quantization, Retrieval-Augmented Generation, Retrieval-Augmented Generation, Retrieval-Augmented Generation, Chatbot, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17081v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17081v1.pdf filename=2402.17081v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This study presents an innovative enhancement to <b>retrieval-augmented</b> <b>generation</b> <b>(RAG)</b> systems by seamlessly integrating <b>fine-tuned</b> <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> with vector databases. This integration capitalizes on the combined strengths of structured data <b>retrieval</b> <b>and</b> <b>the</b> nuanced comprehension provided by advanced <b>LLMs.</b> Central to our approach are the LoRA and QLoRA methodologies, which stand at the forefront of model refinement through parameter-efficient <b>fine-tuning</b> and memory optimization. A novel feature of our research is the incorporation of user feedback directly into the training process, ensuring the model&rsquo;s continuous adaptation to user expectations and thus, improving its performance and applicability. Additionally, we introduce a <b>Quantized</b> Influence Measure (QIM) as an innovative &ldquo;AI Judge&rdquo; mechanism to enhance the precision of result selection, further refining the system&rsquo;s accuracy. Accompanied by an executive diagram and a detailed algorithm for <b>fine-tuning</b> QLoRA, our work provides a comprehensive framework for implementing these advancements within <b>chatbot</b> technologies. This research contributes significant insights into <b>LLM</b> optimization for specific uses and heralds new directions for further development in <b>retrieval-augmented</b> <b>models.</b> <b>Through</b> extensive experimentation and analysis, our findings lay a robust foundation for future advancements in <b>chatbot</b> technology and <b>retrieval</b> <b>systems,</b> <b>marking</b> a significant step forward in the creation of more sophisticated, precise, and user-centric conversational AI systems.</p></p class="citation"></blockquote><h3 id=311--3279-high-frequency-aware-hierarchical-contrastive-selective-coding-for-representation-learning-on-text-attributed-graphs-peiyan-zhang-et-al-2024>(3/11 | 3/279) High-Frequency-aware Hierarchical Contrastive Selective Coding for Representation Learning on Text-attributed Graphs (Peiyan Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Peiyan Zhang, Chaozhuo Li, Liying Kang, Feiran Huang, Senzhang Wang, Xing Xie, Sunghun Kim. (2024)<br><strong>High-Frequency-aware Hierarchical Contrastive Selective Coding for Representation Learning on Text-attributed Graphs</strong><br><button class=copy-to-clipboard title="High-Frequency-aware Hierarchical Contrastive Selective Coding for Representation Learning on Text-attributed Graphs" index=3>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-3 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: H-3-3, cs-IR, cs-SI, cs.IR<br>Keyword Score: 71<br>Keywords: Graph, Graph Neural Network, Graph Neural Network, Benchmarking, Contrastive Learning, Representation Learning, Self-supervised Learning, Pre-trained Language Model, Pre-trained Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16240v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16240v1.pdf filename=2402.16240v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We investigate node <b>representation</b> <b>learning</b> on text-attributed <b>graphs</b> <b>(TAGs),</b> <b>where</b> nodes are associated with text information. Although recent studies on <b>graph</b> <b>neural</b> <b>networks</b> <b>(GNNs)</b> and <b>pretrained</b> <b>language</b> <b>models</b> <b>(PLMs)</b> have exhibited their power in encoding network and text signals, respectively, less attention has been paid to delicately coupling these two types of models on TAGs. Specifically, existing <b>GNNs</b> rarely model text in each node in a contextualized way; existing <b>PLMs</b> can hardly be applied to characterize <b>graph</b> <b>structures</b> <b>due</b> to their sequence architecture. To address these challenges, we propose HASH-CODE, a High-frequency Aware Spectral Hierarchical <b>Contrastive</b> <b>Selective</b> Coding method that integrates <b>GNNs</b> and <b>PLMs</b> into a unified model. Different from previous &ldquo;cascaded architectures&rdquo; that directly add <b>GNN</b> layers upon a <b>PLM,</b> our HASH-CODE relies on five <b>self-supervised</b> optimization objectives to facilitate thorough mutual enhancement between network and text signals in diverse granularities. Moreover, we show that existing <b>contrastive</b> <b>objective</b> learns the low-frequency component of the augmentation <b>graph</b> <b>and</b> <b>propose</b> a high-frequency component (HFC)-aware <b>contrastive</b> <b>learning</b> objective that makes the learned embeddings more distinctive. Extensive experiments on six real-world <b>benchmarks</b> substantiate the efficacy of our proposed approach. In addition, theoretical analysis and item embedding visualization provide insights into our model interoperability.</p></p class="citation"></blockquote><h3 id=411--4279-llm-assisted-multi-teacher-continual-learning-for-visual-question-answering-in-robotic-surgery-kexin-chen-et-al-2024>(4/11 | 4/279) LLM-Assisted Multi-Teacher Continual Learning for Visual Question Answering in Robotic Surgery (Kexin Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kexin Chen, Yuyang Du, Tao You, Mobarakol Islam, Ziyu Guo, Yueming Jin, Guangyong Chen, Pheng-Ann Heng. (2024)<br><strong>LLM-Assisted Multi-Teacher Continual Learning for Visual Question Answering in Robotic Surgery</strong><br><button class=copy-to-clipboard title="LLM-Assisted Multi-Teacher Continual Learning for Visual Question Answering in Robotic Surgery" index=4>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-4 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 66<br>Keywords: Continual Learning, Multi-modal, Multi-modal, Question Answering, Visual Question Answering, Visual Question Answering, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16664v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16664v1.pdf filename=2402.16664v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Visual</b> <b>question</b> <b>answering</b> <b>(VQA)</b> can be fundamentally crucial for promoting robotic-assisted surgical education. In practice, the needs of trainees are constantly evolving, such as learning more surgical types, adapting to different robots, and learning new surgical instruments and techniques for one surgery. Therefore, continually updating the <b>VQA</b> system by a sequential data stream from multiple resources is demanded in robotic surgery to address new tasks. In surgical scenarios, the storage cost and patient data privacy often restrict the availability of old data when updating the model, necessitating an exemplar-free <b>continual</b> <b>learning</b> (CL) setup. However, prior studies overlooked two vital problems of the surgical domain: i) <b>large</b> <b>domain</b> <b>shifts</b> from diverse surgical operations collected from multiple departments or clinical centers, and ii) severe data imbalance arising from the uneven presence of surgical instruments or activities during surgical procedures. This paper proposes to address these two problems with a <b>multimodal</b> <b>large</b> <b>language</b> <b>model</b> <b>(LLM)</b> and an adaptive weight assignment methodology. We first develop a new multi-teacher CL framework that leverages a <b>multimodal</b> <b>LLM</b> as the additional teacher. The strong generalization ability of the <b>LLM</b> can bridge the knowledge gap when domain shifts and data imbalances occur. We then put forth a novel data processing method that transforms complex <b>LLM</b> embeddings into logits compatible with our CL framework. We further design an adaptive weight assignment approach that balances the generalization ability of the <b>LLM</b> and the domain expertise of the old CL model. We construct a new dataset for surgical <b>VQA</b> tasks, providing valuable data resources for future research. Extensive experimental results on three datasets demonstrate the superiority of our method to other advanced CL models.</p></p class="citation"></blockquote><h3 id=511--5279-deep-rating-elicitation-for-new-users-in-collaborative-filtering-wonbin-kweon-et-al-2024>(5/11 | 5/279) Deep Rating Elicitation for New Users in Collaborative Filtering (Wonbin Kweon et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wonbin Kweon, SeongKu Kang, Junyoung Hwang, Hwanjo Yu. (2024)<br><strong>Deep Rating Elicitation for New Users in Collaborative Filtering</strong><br><button class=copy-to-clipboard title="Deep Rating Elicitation for New Users in Collaborative Filtering" index=5>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-5 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 20<br>Keywords: Recommendation, Recommender System<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16327v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16327v1.pdf filename=2402.16327v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent <b>recommender</b> <b>systems</b> started to use rating elicitation, which asks new users to rate a small seed itemset for inferring their preferences, to improve the quality of initial <b>recommendations.</b> The key challenge of the rating elicitation is to choose the seed items which can best infer the new users&rsquo; preference. This paper proposes a novel end-to-end Deep learning framework for Rating Elicitation (DRE), that chooses all the seed items at a time with consideration of the non-linear interactions. To this end, it first defines categorical distributions to sample seed items from the entire itemset, then it trains both the categorical distributions and a neural reconstruction network to infer users&rsquo; preferences on the remaining items from CF information of the sampled seed items. Through the end-to-end training, the categorical distributions are learned to select the most representative seed items while reflecting the complex non-linear interactions. Experimental results show that DRE outperforms the state-of-the-art approaches in the <b>recommendation</b> quality by accurately inferring the new users&rsquo; preferences and its seed itemset better represents the latent space than the seed itemset obtained by the other methods.</p></p class="citation"></blockquote><h3 id=611--6279-confidence-calibration-for-recommender-systems-and-its-applications-wonbin-kweon-2024>(6/11 | 6/279) Confidence Calibration for Recommender Systems and Its Applications (Wonbin Kweon, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wonbin Kweon. (2024)<br><strong>Confidence Calibration for Recommender Systems and Its Applications</strong><br><button class=copy-to-clipboard title="Confidence Calibration for Recommender Systems and Its Applications" index=6>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-6 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 20<br>Keywords: Recommendation, Recommender System<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16325v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16325v1.pdf filename=2402.16325v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Despite the importance of having a measure of confidence in <b>recommendation</b> results, it has been surprisingly overlooked in the literature compared to the accuracy of the <b>recommendation.</b> In this dissertation, I propose a model calibration framework for <b>recommender</b> <b>systems</b> for estimating accurate confidence in <b>recommendation</b> results based on the learned ranking scores. Moreover, I subsequently introduce two real-world applications of confidence on <b>recommendations:</b> (1) Training a small student model by treating the confidence of a big teacher model as additional learning guidance, (2) Adjusting the number of presented items based on the expected user utility estimated with calibrated probability.</p></p class="citation"></blockquote><h3 id=711--7279-against-filter-bubbles-diversified-music-recommendation-via-weighted-hypergraph-embedding-learning-chaoguang-luo-et-al-2024>(7/11 | 7/279) Against Filter Bubbles: Diversified Music Recommendation via Weighted Hypergraph Embedding Learning (Chaoguang Luo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chaoguang Luo, Liuying Wen, Yong Qin, Liangwei Yang, Zhineng Hu, Philip S. Yu. (2024)<br><strong>Against Filter Bubbles: Diversified Music Recommendation via Weighted Hypergraph Embedding Learning</strong><br><button class=copy-to-clipboard title="Against Filter Bubbles: Diversified Music Recommendation via Weighted Hypergraph Embedding Learning" index=7>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-7 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs-LG, cs.IR<br>Keyword Score: 20<br>Keywords: Recommendation, Recommender System<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16299v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16299v1.pdf filename=2402.16299v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Recommender</b> <b>systems</b> serve a dual purpose for users: sifting out inappropriate or mismatched information while accurately identifying items that align with their preferences. Numerous <b>recommendation</b> algorithms are designed to provide users with a personalized array of information tailored to their preferences. Nevertheless, excessive personalization can confine users within a &ldquo;filter bubble&rdquo;. Consequently, achieving the right balance between accuracy and diversity in <b>recommendations</b> is a pressing concern. To address this challenge, exemplified by music <b>recommendation,</b> we introduce the Diversified Weighted Hypergraph music <b>Recommendation</b> algorithm (DWHRec). In the DWHRec algorithm, the initial connections between users and listened tracks are represented by a weighted hypergraph. Simultaneously, associations between artists, albums and tags with tracks are also appended to the hypergraph. To explore users&rsquo; latent preferences, a hypergraph-based random walk embedding method is applied to the constructed hypergraph. In our investigation, accuracy is gauged by the alignment between the user and the track, whereas the array of recommended track types measures diversity. We rigorously compared DWHRec against seven state-of-the-art <b>recommendation</b> algorithms using two real-world music datasets. The experimental results validate DWHRec as a solution that adeptly harmonizes accuracy and diversity, delivering a more enriched musical experience. Beyond music <b>recommendation,</b> DWHRec can be extended to cater to other scenarios with similar data structures.</p></p class="citation"></blockquote><h3 id=811--8279-boxrec-recommending-a-box-of-preferred-outfits-in-online-shopping-debopriyo-banerjee-et-al-2024>(8/11 | 8/279) BOXREC: Recommending a Box of Preferred Outfits in Online Shopping (Debopriyo Banerjee et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Debopriyo Banerjee, Krothapalli Sreenivasa Rao, Shamik Sural, Niloy Ganguly. (2024)<br><strong>BOXREC: Recommending a Box of Preferred Outfits in Online Shopping</strong><br><button class=copy-to-clipboard title="BOXREC: Recommending a Box of Preferred Outfits in Online Shopping" index=8>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-8 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 10<br>Keywords: Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16660v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16660v1.pdf filename=2402.16660v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Over the past few years, automation of outfit composition has gained much attention from the research community. Most of the existing outfit <b>recommendation</b> systems focus on pairwise item compatibility prediction (using visual and text features) to score an outfit combination having several items, followed by <b>recommendation</b> of top-n outfits or a capsule wardrobe having a collection of outfits based on user&rsquo;s fashion taste. However, none of these consider user&rsquo;s preference of price-range for individual clothing types or an overall shopping budget for a set of items. In this paper, we propose a box <b>recommendation</b> framework - BOXREC - which at first, collects user preferences across different item types (namely, top-wear, bottom-wear and foot-wear) including price-range of each type and a maximum shopping budget for a particular shopping session. It then generates a set of preferred outfits by retrieving all types of preferred items from the database (according to user specified preferences including price-ranges), creates all possible combinations of three preferred items (belonging to distinct item types) and verifies each combination using an outfit scoring framework - BOXREC-OSF. Finally, it provides a box full of fashion items, such that different combinations of the items maximize the number of outfits suitable for an occasion while satisfying maximum shopping budget. Empirical results show superior performance of BOXREC-OSF over the baseline methods.</p></p class="citation"></blockquote><h3 id=911--9279-retrouver-linventeur-auteur--la-levée-dhomonymies-dautorat-entre-les-brevets-et-les-publications-scientifiques-david-reymond-et-al-2024>(9/11 | 9/279) Retrouver l&rsquo;inventeur-auteur : la lev{é}e d&rsquo;homonymies d&rsquo;autorat entre les brevets et les publications scientifiques (David Reymond et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>David Reymond, Heman Khouilla, Sandrine Wolff, Manuel Durand-Barthez. (2024)<br><strong>Retrouver l&rsquo;inventeur-auteur : la lev{é}e d&rsquo;homonymies d&rsquo;autorat entre les brevets et les publications scientifiques</strong><br><button class=copy-to-clipboard title="Retrouver l'inventeur-auteur : la lev{é}e d'homonymies d'autorat entre les brevets et les publications scientifiques" index=9>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-9 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 10<br>Keywords: Disambiguation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16440v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16440v1.pdf filename=2402.16440v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Patents and scientific papers provide an essential source for measuring science and technology output, to be used as a basis for the most varied scientometric analyzes. Authors&rsquo; and inventors&rsquo; names are the key identifiers to carry out these analyses, which however, run up against the issue of <b>disambiguation.</b> By extension identifying inventors who are also academic authors is a non-trivial challenge. We propose a method using the International Patent Classification (IPC) and the IPCCAT API to assess the degree of similarity of patents and papers abstracts of a given inventor, in order to match both types of documents. The method is developed and manually qualified based on three corpora of patents extracted from the international EPO database Espacenet. Among a set of 4679 patents and 7720 inventors, we obtain 2501 authors. The proposed algorithm solves the general problem of <b>disambiguation</b> with an error rate lower than 5%.</p></p class="citation"></blockquote><h3 id=1011--10279-top-personalized-k-recommendation-wonbin-kweon-et-al-2024>(10/11 | 10/279) Top-Personalized-K Recommendation (Wonbin Kweon et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wonbin Kweon, SeongKu Kang, Sanghwan Jang, Hwanjo Yu. (2024)<br><strong>Top-Personalized-K Recommendation</strong><br><button class=copy-to-clipboard title="Top-Personalized-K Recommendation" index=10>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-10 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 10<br>Keywords: Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16304v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16304v1.pdf filename=2402.16304v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The conventional top-K <b>recommendation,</b> which presents the top-K items with the highest ranking scores, is a common practice for generating personalized ranking lists. However, is this fixed-size top-K <b>recommendation</b> the optimal approach for every user&rsquo;s satisfaction? Not necessarily. We point out that providing fixed-size <b>recommendations</b> without taking into account user utility can be suboptimal, as it may unavoidably include irrelevant items or limit the exposure to relevant ones. To address this issue, we introduce Top-Personalized-K <b>Recommendation,</b> a new <b>recommendation</b> task aimed at generating a personalized-sized ranking list to maximize individual user satisfaction. As a solution to the proposed task, we develop a model-agnostic framework named PerK. PerK estimates the expected user utility by leveraging calibrated interaction probabilities, subsequently selecting the <b>recommendation</b> size that maximizes this expected utility. Through extensive experiments on real-world datasets, we demonstrate the superiority of PerK in Top-Personalized-K <b>recommendation</b> task. We expect that Top-Personalized-K <b>recommendation</b> has the potential to offer enhanced solutions for various real-world <b>recommendation</b> scenarios, based on its great compatibility with existing models.</p></p class="citation"></blockquote><h3 id=1111--11279-corpusbrain-a-continual-generative-pre-training-framework-for-knowledge-intensive-language-tasks-jiafeng-guo-et-al-2024>(11/11 | 11/279) CorpusBrain++: A Continual Generative Pre-Training Framework for Knowledge-Intensive Language Tasks (Jiafeng Guo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiafeng Guo, Changjiang Zhou, Ruqing Zhang, Jiangui Chen, Maarten de Rijke, Yixing Fan, Xueqi Cheng. (2024)<br><strong>CorpusBrain++: A Continual Generative Pre-Training Framework for Knowledge-Intensive Language Tasks</strong><br><button class=copy-to-clipboard title="CorpusBrain++: A Continual Generative Pre-Training Framework for Knowledge-Intensive Language Tasks" index=11>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-11 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-CL, cs-IR, cs.IR<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16767v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16767v1.pdf filename=2402.16767v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Knowledge-intensive language tasks (KILTs) typically require retrieving relevant documents from trustworthy corpora, e.g., Wikipedia, to produce specific answers. Very recently, a pre-trained generative retrieval model for KILTs, named CorpusBrain, was proposed and reached new state-of-the-art retrieval performance. However, most existing research on KILTs, including CorpusBrain, has predominantly focused on a static document collection, overlooking the dynamic nature of real-world scenarios, where new documents are continuously being incorporated into the source corpus. To address this gap, it is crucial to explore the capability of retrieval models to effectively handle the dynamic retrieval scenario inherent in KILTs. In this work, we first introduce the continual document learning (CDL) task for KILTs and build a novel <b>benchmark</b> dataset named KILT++ based on the original KILT dataset for evaluation. Then, we conduct a comprehensive study over the use of pre-trained CorpusBrain on KILT++. Unlike the promising results in the stationary scenario, CorpusBrain is prone to catastrophic forgetting in the dynamic scenario, hence hampering the retrieval performance. To alleviate this issue, we propose CorpusBrain++, a continual generative pre-training framework. Empirical results demonstrate the significant effectiveness and remarkable efficiency of CorpusBrain++ in comparison to both traditional and generative IR methods.</p></p class="citation"></blockquote><h2 id=cscr-8>cs.CR (8)</h2><h3 id=18--12279-deep-learning-algorithms-used-in-intrusion-detection-systems----a-review-richard-kimanzi-et-al-2024>(1/8 | 12/279) Deep Learning Algorithms Used in Intrusion Detection Systems &ndash; A Review (Richard Kimanzi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Richard Kimanzi, Peter Kimanga, Dedan Cherori, Patrick K. Gikunda. (2024)<br><strong>Deep Learning Algorithms Used in Intrusion Detection Systems &ndash; A Review</strong><br><button class=copy-to-clipboard title="Deep Learning Algorithms Used in Intrusion Detection Systems -- A Review" index=12>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-12 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs.CR<br>Keyword Score: 106<br>Keywords: Anomaly Detection, Autoencoder, Benchmarking, Benchmarking, Convolution, Convolutional Neural Network, Convolutional Neural Network, LSTM, LSTM, LSTM, Recurrent Neural Network, Recurrent Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17020v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17020v1.pdf filename=2402.17020v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The increase in network attacks has necessitated the development of robust and efficient intrusion detection systems (IDS) capable of identifying malicious activities in real-time. In the last five years, deep learning algorithms have emerged as powerful tools in this domain, offering enhanced detection capabilities compared to traditional methods. This review paper studies recent advancements in the application of deep learning techniques, including <b>Convolutional</b> <b>Neural</b> <b>Networks</b> <b>(CNN),</b> <b>Recurrent</b> <b>Neural</b> <b>Networks</b> <b>(RNN),</b> Deep Belief Networks (DBN), Deep Neural Networks (DNN), <b>Long</b> <b>Short-Term</b> <b>Memory</b> <b>(LSTM),</b> <b>autoencoders</b> (AE), Multi-Layer Perceptrons (MLP), Self-Normalizing Networks (SNN) and hybrid models, within network intrusion detection systems. we delve into the unique architectures, training models, and classification methodologies tailored for network traffic analysis and <b>anomaly</b> <b>detection.</b> Furthermore, we analyze the strengths and limitations of each deep learning approach in terms of detection accuracy, computational efficiency, scalability, and adaptability to evolving threats. Additionally, this paper highlights prominent datasets and <b>benchmarking</b> frameworks commonly utilized for evaluating the performance of deep learning-based IDS. This review will provide researchers and industry practitioners with valuable insights into the state-of-the-art deep learning algorithms for enhancing the security framework of network environments through intrusion detection.</p></p class="citation"></blockquote><h3 id=28--13279-pandoras-white-box-increased-training-data-leakage-in-open-llms-jeffrey-g-wang-et-al-2024>(2/8 | 13/279) Pandora&rsquo;s White-Box: Increased Training Data Leakage in Open LLMs (Jeffrey G. Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jeffrey G. Wang, Jason Wang, Marvin Li, Seth Neel. (2024)<br><strong>Pandora&rsquo;s White-Box: Increased Training Data Leakage in Open LLMs</strong><br><button class=copy-to-clipboard title="Pandora's White-Box: Increased Training Data Leakage in Open LLMs" index=13>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-13 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-AI, cs-CR, cs-LG, cs.CR<br>Keyword Score: 65<br>Keywords: Black Box, Fine-tuning, Fine-tuning, Supervised Learning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17012v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17012v1.pdf filename=2402.17012v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper we undertake a systematic study of privacy attacks against open source <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs),</b> where an adversary has access to either the model weights, gradients, or losses, and tries to exploit them to learn something about the underlying training data. Our headline results are the first membership inference attacks (MIAs) against pre-trained <b>LLMs</b> that are able to simultaneously achieve high TPRs and low FPRs, and a pipeline showing that over $50%$ (!) of the <b>fine-tuning</b> dataset can be extracted from a <b>fine-tuned</b> <b>LLM</b> in natural settings. We consider varying degrees of access to the underlying model, customization of the language model, and resources available to the attacker. In the pre-trained setting, we propose three new white-box MIAs: an attack based on the gradient norm, a <b>supervised</b> neural network classifier, and a single step loss ratio attack. All outperform existing <b>black-box</b> <b>baselines,</b> and our <b>supervised</b> attack closes the gap between MIA attack success against <b>LLMs</b> and other types of models. In <b>fine-tuning,</b> we find that given access to the loss of the <b>fine-tuned</b> and base models, a <b>fine-tuned</b> loss ratio attack FLoRA is able to achieve near perfect MIA peformance. We then leverage these MIAs to extract <b>fine-tuning</b> data from <b>fine-tuned</b> language models. We find that the pipeline of generating from <b>fine-tuned</b> models <b>prompted</b> with a small snippet of the prefix of each training example, followed by using FLoRa to select the most likely training sample, succeeds the majority of the <b>fine-tuning</b> dataset after only $3$ epochs of <b>fine-tuning.</b> Taken together, these findings show that highly effective MIAs are available in almost all <b>LLM</b> training settings, and highlight that great care must be taken before <b>LLMs</b> are <b>fine-tuned</b> on highly sensitive data and then deployed.</p></p class="citation"></blockquote><h3 id=38--14279-wipi-a-new-web-threat-for-llm-driven-web-agents-fangzhou-wu-et-al-2024>(3/8 | 14/279) WIPI: A New Web Threat for LLM-Driven Web Agents (Fangzhou Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fangzhou Wu, Shutong Wu, Yulong Cao, Chaowei Xiao. (2024)<br><strong>WIPI: A New Web Threat for LLM-Driven Web Agents</strong><br><button class=copy-to-clipboard title="WIPI: A New Web Threat for LLM-Driven Web Agents" index=14>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-14 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-AI, cs-CR, cs.CR<br>Keyword Score: 55<br>Keywords: Black Box, ChatGPT, GPT, Automatic Speech Recognition, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16965v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16965v1.pdf filename=2402.16965v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the fast development of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs),</b> <b>LLM-driven</b> Web Agents (Web Agents for short) have obtained tons of attention due to their superior capability where <b>LLMs</b> serve as the core part of making decisions like the human brain equipped with multiple web tools to actively interact with external deployed websites. As uncountable Web Agents have been released and such <b>LLM</b> systems are experiencing rapid development and drawing closer to widespread deployment in our daily lives, an essential and pressing question arises: &ldquo;Are these Web Agents secure?&rdquo;. In this paper, we introduce a novel threat, WIPI, that indirectly controls Web Agent to execute malicious instructions embedded in publicly accessible webpages. To launch a successful WIPI works in a <b>black-box</b> <b>environment.</b> This methodology focuses on the form and content of indirect instructions within external webpages, enhancing the efficiency and stealthiness of the attack. To evaluate the effectiveness of the proposed methodology, we conducted extensive experiments using 7 plugin-based <b>ChatGPT</b> Web Agents, 8 Web <b>GPTs,</b> and 3 different open-source Web Agents. The results reveal that our methodology achieves an average attack success rate <b>(ASR)</b> exceeding 90% even in pure <b>black-box</b> <b>scenarios.</b> Moreover, through an ablation study examining various user prefix instructions, we demonstrated that the WIPI exhibits strong robustness, maintaining high performance across diverse prefix instructions.</p></p class="citation"></blockquote><h3 id=48--15279-synthesizing-tight-privacy-and-accuracy-bounds-via-weighted-model-counting-lisa-oakley-et-al-2024>(4/8 | 15/279) Synthesizing Tight Privacy and Accuracy Bounds via Weighted Model Counting (Lisa Oakley et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lisa Oakley, Steven Holtzen, Alina Oprea. (2024)<br><strong>Synthesizing Tight Privacy and Accuracy Bounds via Weighted Model Counting</strong><br><button class=copy-to-clipboard title="Synthesizing Tight Privacy and Accuracy Bounds via Weighted Model Counting" index=15>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-15 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs-PL, cs.CR<br>Keyword Score: 40<br>Keywords: Probabilistic Model, Reasoning, Stemming, Differential Privacy<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16982v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16982v2.pdf filename=2402.16982v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Programmatically generating tight <b>differential</b> <b>privacy</b> (DP) bounds is a hard problem. Two core challenges are (1) finding expressive, compact, and efficient encodings of the distributions of DP algorithms, and (2) state space explosion <b>stemming</b> from the multiple quantifiers and relational properties of the DP definition. We address the first challenge by developing a method for tight privacy and accuracy bound synthesis using weighted model counting on binary decision diagrams, a state of the art technique from the artificial intelligence and automated <b>reasoning</b> communities for exactly computing probability distributions. We address the second challenge by developing a framework for leveraging inherent symmetries in DP algorithms. Our solution benefits from ongoing research in <b>probabilistic</b> <b>programming</b> languages, allowing us to succinctly and expressively represent different DP algorithms with approachable language syntax that can be used by non-experts. We provide a detailed case study of our solution on the binary randomized response algorithm. We also evaluate an implementation of our solution using the Dice <b>probabilistic</b> <b>programming</b> language for the randomized response and truncated geometric above threshold algorithms. We compare to prior work on exact DP verification using Markov chain <b>probabilistic</b> <b>model</b> checking. Very few existing works consider mechanized analysis of accuracy guarantees for DP algorithms. We additionally provide a detailed analysis using our technique for finding tight accuracy bounds for DP algorithms.</p></p class="citation"></blockquote><h3 id=58--16279-improving-behavior-based-authentication-against-adversarial-attack-using-xai-dong-qin-et-al-2024>(5/8 | 16/279) Improving behavior based authentication against adversarial attack using XAI (Dong Qin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dong Qin, George Amariucai, Daji Qiao, Yong Guan. (2024)<br><strong>Improving behavior based authentication against adversarial attack using XAI</strong><br><button class=copy-to-clipboard title="Improving behavior based authentication against adversarial attack using XAI" index=16>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-16 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs-HC, cs.CR<br>Keyword Score: 40<br>Keywords: Adversarial Learning, Explainable AI, Knowledge Distillation, Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16430v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16430v1.pdf filename=2402.16430v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In recent years, machine learning models, especially deep neural networks, have been widely used for classification tasks in the security domain. However, these models have been shown to be vulnerable to <b>adversarial</b> <b>manipulation:</b> small changes learned by an <b>adversarial</b> <b>attack</b> model, when applied to the input, can cause significant changes in the output. Most research on <b>adversarial</b> <b>attacks</b> and corresponding defense methods focuses only on scenarios where <b>adversarial</b> <b>samples</b> are directly generated by the attack model. In this study, we explore a more practical scenario in behavior-based authentication, where <b>adversarial</b> <b>samples</b> are collected from the attacker. The generated <b>adversarial</b> <b>samples</b> from the model are replicated by attackers with a certain level of discrepancy. We propose an <b>eXplainable</b> <b>AI</b> (XAI) based defense strategy against <b>adversarial</b> <b>attacks</b> in such scenarios. A feature selector, trained with our method, can be used as a filter in front of the original authenticator. It filters out features that are more vulnerable to <b>adversarial</b> <b>attacks</b> or irrelevant to authentication, while retaining features that are more robust. Through comprehensive experiments, we demonstrate that our XAI based defense strategy is effective against <b>adversarial</b> <b>attacks</b> and outperforms other defense strategies, such as <b>adversarial</b> <b>training</b> and defensive <b>distillation.</b></p></p class="citation"></blockquote><h3 id=68--17279-a-survey-of-large-language-models-in-cybersecurity-gabriel-de-jesus-coelho-da-silva-et-al-2024>(6/8 | 17/279) A Survey of Large Language Models in Cybersecurity (Gabriel de Jesus Coelho da Silva et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Gabriel de Jesus Coelho da Silva, Carlos Becker Westphall. (2024)<br><strong>A Survey of Large Language Models in Cybersecurity</strong><br><button class=copy-to-clipboard title="A Survey of Large Language Models in Cybersecurity" index=17>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-17 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-AI, cs-CR, cs.CR<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16968v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16968v1.pdf filename=2402.16968v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have quickly risen to prominence due to their ability to perform at or close to the state-of-the-art in a variety of fields while handling natural language. An important field of research is the application of such models at the cybersecurity context. This survey aims to identify where in the field of cybersecurity <b>LLMs</b> have already been applied, the ways in which they are being used and their limitations in the field. Finally, suggestions are made on how to improve such limitations and what can be expected from these systems once these limitations are overcome.</p></p class="citation"></blockquote><h3 id=78--18279-decentralized-federated-unlearning-on-blockchain-xiao-liu-et-al-2024>(7/8 | 18/279) Decentralized Federated Unlearning on Blockchain (Xiao Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiao Liu, Mingyuan Li, Xu Wang, Guangsheng Yu, Wei Ni, Lixiang Li, Haipeng Peng, Renping Liu. (2024)<br><strong>Decentralized Federated Unlearning on Blockchain</strong><br><button class=copy-to-clipboard title="Decentralized Federated Unlearning on Blockchain" index=18>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-18 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-AI, cs-CR, cs.CR<br>Keyword Score: 13<br>Keywords: Graph, Federated Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16294v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16294v1.pdf filename=2402.16294v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Blockchained <b>Federated</b> <b>Learning</b> (FL) has been gaining traction for ensuring the integrity and traceability of FL processes. Blockchained FL involves participants training models locally with their data and subsequently publishing the models on the blockchain, forming a Directed Acyclic <b>Graph</b> (DAG)-like inheritance structure that represents the model relationship. However, this particular DAG-based structure presents challenges in updating models with sensitive data, due to the complexity and overhead involved. To address this, we propose Blockchained <b>Federated</b> <b>Unlearning</b> (BlockFUL), a generic framework that redesigns the blockchain structure using Chameleon Hash (CH) technology to mitigate the complexity of model updating, thereby reducing the computational and consensus costs of unlearning tasks.Furthermore, BlockFUL supports various <b>federated</b> <b>unlearning</b> methods, ensuring the integrity and traceability of model updates, whether conducted in parallel or serial. We conduct a comprehensive study of two typical unlearning methods, gradient ascent and re-training, demonstrating the efficient unlearning workflow in these two categories with minimal CH and block update operations. Additionally, we compare the computation and communication costs of these methods.</p></p class="citation"></blockquote><h3 id=88--19279-on-the-infeasibility-of-ml-backdoor-detection-as-an-hypothesis-testing-problem-georg-pichler-et-al-2024>(8/8 | 19/279) On the (In)feasibility of ML Backdoor Detection as an Hypothesis Testing Problem (Georg Pichler et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Georg Pichler, Marco Romanelli, Divya Prakash Manivannan, Prashanth Krishnamurthy, Farshad Khorrami, Siddharth Garg. (2024)<br><strong>On the (In)feasibility of ML Backdoor Detection as an Hypothesis Testing Problem</strong><br><button class=copy-to-clipboard title="On the (In)feasibility of ML Backdoor Detection as an Hypothesis Testing Problem" index=19>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-19 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-AI, cs-CR, cs-LG, cs.CR, stat-ML<br>Keyword Score: 10<br>Keywords: Out-of-distribution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16926v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16926v1.pdf filename=2402.16926v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce a formal statistical definition for the problem of backdoor detection in machine learning systems and use it to analyze the feasibility of such problems, providing evidence for the utility and applicability of our definition. The main contributions of this work are an impossibility result and an achievability result for backdoor detection. We show a no-free-lunch theorem, proving that universal (adversary-unaware) backdoor detection is impossible, except for very small alphabet sizes. Thus, we argue, that backdoor detection methods need to be either explicitly, or implicitly adversary-aware. However, our work does not imply that backdoor detection cannot work in specific scenarios, as evidenced by successful backdoor detection methods in the scientific literature. Furthermore, we connect our definition to the probably approximately correct (PAC) learnability of the <b>out-of-distribution</b> detection problem.</p></p class="citation"></blockquote><h2 id=cscl-62>cs.CL (62)</h2><h3 id=162--20279-codes-towards-building-open-source-language-models-for-text-to-sql-haoyang-li-et-al-2024>(1/62 | 20/279) CodeS: Towards Building Open-source Language Models for Text-to-SQL (Haoyang Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haoyang Li, Jing Zhang, Hanbing Liu, Ju Fan, Xiaokang Zhang, Jun Zhu, Renjie Wei, Hongyan Pan, Cuiping Li, Hong Chen. (2024)<br><strong>CodeS: Towards Building Open-source Language Models for Text-to-SQL</strong><br><button class=copy-to-clipboard title="CodeS: Towards Building Open-source Language Models for Text-to-SQL" index=20>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-20 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-DB, cs.CL<br>Keyword Score: 103<br>Keywords: Benchmarking, Data Augmentation, ChatGPT, GPT, GPT-4, Text2SQL, Domain Adaptation, Large Language Model, Large Language Model, Pre-trained Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16347v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16347v1.pdf filename=2402.16347v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Language models have shown promising performance on the task of translating natural language questions into SQL queries <b>(Text-to-SQL).</b> However, most of the state-of-the-art (SOTA) approaches rely on powerful yet closed-source <b>large</b> <b>language</b> <b>models</b> <b>(LLMs),</b> such as <b>ChatGPT</b> and <b>GPT-4,</b> which may have the limitations of unclear model architectures, <b>data</b> <b>privacy</b> risks, and expensive inference overheads. To address the limitations, we introduce CodeS, a series of <b>pre-trained</b> <b>language</b> <b>models</b> with parameters ranging from 1B to 15B, specifically designed for the <b>text-to-SQL</b> task. CodeS is a fully open-source language model, which achieves superior accuracy with much smaller parameter sizes. This paper studies the research challenges in building CodeS. To enhance the SQL generation abilities of CodeS, we adopt an incremental pre-training approach using a specifically curated SQL-centric corpus. Based on this, we address the challenges of schema linking and rapid <b>domain</b> <b>adaptation</b> through strategic <b>prompt</b> construction and a bi-directional <b>data</b> <b>augmentation</b> technique. We conduct comprehensive evaluations on multiple datasets, including the widely used Spider <b>benchmark,</b> the newly released BIRD <b>benchmark,</b> robustness-diagnostic <b>benchmarks</b> such as Spider-DK, Spider-Syn, Spider-Realistic, and Dr.Spider, as well as two real-world datasets created for financial and academic applications. The experimental results show that our CodeS achieves new SOTA accuracy and robustness on nearly all challenging <b>text-to-SQL</b> <b>benchmarks.</b></p></p class="citation"></blockquote><h3 id=262--21279-rocoins-enhancing-robustness-of-large-language-models-through-code-style-instructions-yuansen-zhang-et-al-2024>(2/62 | 21/279) RoCoIns: Enhancing Robustness of Large Language Models through Code-Style Instructions (Yuansen Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuansen Zhang, Xiao Wang, Zhiheng Xi, Han Xia, Tao Gui, Qi Zhang, Xuanjing Huang. (2024)<br><strong>RoCoIns: Enhancing Robustness of Large Language Models through Code-Style Instructions</strong><br><button class=copy-to-clipboard title="RoCoIns: Enhancing Robustness of Large Language Models through Code-Style Instructions" index=21>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-21 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 90<br>Keywords: Few-shot, GPT, GPT-3, GPT-3.5, Automatic Speech Recognition, In-context Learning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16431v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16431v1.pdf filename=2402.16431v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have showcased remarkable capabilities in following human instructions. However, recent studies have raised concerns about the robustness of <b>LLMs</b> when <b>prompted</b> with instructions combining textual adversarial samples. In this paper, drawing inspiration from recent works that <b>LLMs</b> are sensitive to the design of the instructions, we utilize instructions in code style, which are more structural and less ambiguous, to replace typically natural language instructions. Through this conversion, we provide <b>LLMs</b> with more precise instructions and strengthen the robustness of <b>LLMs.</b> Moreover, under <b>few-shot</b> scenarios, we propose a novel method to compose <b>in-context</b> demonstrations using both clean and adversarial samples (\textit{adversarial context method}) to further boost the robustness of the <b>LLMs.</b> Experiments on eight robustness datasets show that our method consistently outperforms <b>prompting</b> <b>LLMs</b> with natural language instructions. For example, with <b>gpt-3.5-turbo,</b> our method achieves an improvement of 5.68% in test set accuracy and a reduction of 5.66 points in Attack Success Rate <b>(ASR).</b></p></p class="citation"></blockquote><h3 id=362--22279-two-stage-generative-question-answering-on-temporal-knowledge-graph-using-large-language-models-yifu-gao-et-al-2024>(3/62 | 22/279) Two-stage Generative Question Answering on Temporal Knowledge Graph Using Large Language Models (Yifu Gao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yifu Gao, Linbo Qiao, Zhigang Kan, Zhihua Wen, Yongquan He, Dongsheng Li. (2024)<br><strong>Two-stage Generative Question Answering on Temporal Knowledge Graph Using Large Language Models</strong><br><button class=copy-to-clipboard title="Two-stage Generative Question Answering on Temporal Knowledge Graph Using Large Language Models" index=22>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-22 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 78<br>Keywords: Graph, Graph Neural Network, Knowledge Graph, Question Answering, Reasoning, Instruction Tuning, Large Language Model, Large Language Model, Temporal Knowledge Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16568v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16568v1.pdf filename=2402.16568v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Temporal</b> <b>knowledge</b> <b>graph</b> <b>question</b> <b>answering</b> (TKGQA) poses a significant challenge task, due to the <b>temporal</b> <b>constraints</b> <b>hidden</b> in <b>questions</b> <b>and</b> the answers sought from dynamic structured <b>knowledge.</b> <b>Although</b> <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have made considerable progress in their <b>reasoning</b> ability over structured data, their application to the TKGQA task is a relatively unexplored area. This paper first proposes a novel generative <b>temporal</b> <b>knowledge</b> <b>graph</b> <b>question</b> <b>answering</b> framework, GenTKGQA, which guides <b>LLMs</b> to answer <b>temporal</b> <b>questions</b> <b>through</b> two phases: Subgraph Retrieval and Answer Generation. First, we exploit <b>LLM&rsquo;s</b> intrinsic <b>knowledge</b> <b>to</b> mine <b>temporal</b> <b>constraints</b> <b>and</b> structural links in the <b>questions</b> <b>without</b> extra training, thus narrowing down the subgraph search space in both <b>temporal</b> <b>and</b> <b>structural</b> dimensions. Next, we design virtual <b>knowledge</b> <b>indicators</b> to fuse the <b>graph</b> <b>neural</b> <b>network</b> signals of the subgraph and the text representations of the <b>LLM</b> in a non-shallow way, which helps the open-source <b>LLM</b> deeply understand the <b>temporal</b> <b>order</b> <b>and</b> structural dependencies among the retrieved facts through <b>instruction</b> <b>tuning.</b> Experimental results demonstrate that our model outperforms state-of-the-art baselines, even achieving 100% on the metrics for the simple <b>question</b> <b>type.</b></p></p class="citation"></blockquote><h3 id=462--23279-benchmarking-llms-on-the-semantic-overlap-summarization-task-john-salvador-et-al-2024>(4/62 | 23/279) Benchmarking LLMs on the Semantic Overlap Summarization Task (John Salvador et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>John Salvador, Naman Bansal, Mousumi Akter, Souvika Sarkar, Anupam Das, Shubhra Kanti Karmaker. (2024)<br><strong>Benchmarking LLMs on the Semantic Overlap Summarization Task</strong><br><button class=copy-to-clipboard title="Benchmarking LLMs on the Semantic Overlap Summarization Task" index=23>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-23 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 76<br>Keywords: Benchmarking, Benchmarking, BERTScore, Large Language Model, Large Language Model, Prompt, Rouge, Summarization, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17008v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17008v1.pdf filename=2402.17008v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Semantic Overlap <b>Summarization</b> (SOS) is a constrained multi-document <b>summarization</b> task, where the constraint is to capture the common/overlapping information between two alternative narratives. While recent advancements in <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have achieved superior performance in numerous <b>summarization</b> tasks, a <b>benchmarking</b> study of the SOS task using <b>LLMs</b> is yet to be performed. As <b>LLMs&rsquo;</b> responses are sensitive to slight variations in <b>prompt</b> design, a major challenge in conducting such a <b>benchmarking</b> study is to systematically explore a variety of <b>prompts</b> before drawing a reliable conclusion. Fortunately, very recently, the TELeR taxonomy has been proposed which can be used to design and explore various <b>prompts</b> for <b>LLMs.</b> Using this TELeR taxonomy and 15 popular <b>LLMs,</b> this paper comprehensively evaluates <b>LLMs</b> on the SOS Task, assessing their ability to <b>summarize</b> overlapping information from multiple alternative narratives. For evaluation, we report well-established metrics like <b>ROUGE,</b> <b>BERTscore,</b> and SEM-F1$ on two different datasets of alternative narratives. We conclude the paper by analyzing the strengths and limitations of various <b>LLMs</b> in terms of their capabilities in capturing overlapping information The code and datasets used to conduct this study are available at <a href=https://anonymous.4open.science/r/llm_eval-E16D>https://anonymous.4open.science/r/llm_eval-E16D</a>.</p></p class="citation"></blockquote><h3 id=562--24279-look-before-you-leap-towards-decision-aware-and-generalizable-tool-usage-for-large-language-models-anchun-gui-et-al-2024>(5/62 | 24/279) Look Before You Leap: Towards Decision-Aware and Generalizable Tool-Usage for Large Language Models (Anchun Gui et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Anchun Gui, Jian Li, Yong Dai, Nan Du, Han Xiao. (2024)<br><strong>Look Before You Leap: Towards Decision-Aware and Generalizable Tool-Usage for Large Language Models</strong><br><button class=copy-to-clipboard title="Look Before You Leap: Towards Decision-Aware and Generalizable Tool-Usage for Large Language Models" index=24>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-24 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 70<br>Keywords: ChatGPT, LLaMA, In-context Learning, In-context Learning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16696v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16696v2.pdf filename=2402.16696v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Tool-augmented <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> are attracting widespread attention when accessing up-to-date knowledge and alleviating hallucination issues. Nowadays, advanced closed-source <b>LLMs</b> (e.g., <b>ChatGPT)</b> have demonstrated surprising tool-usage capabilities through <b>prompting</b> and <b>in-context</b> <b>learning</b> techniques. To empower the capabilities of open-source <b>LLMs</b> (e.g., <b>LLaMA)</b> in manipulating tools, current efforts focus on either template-driven or token-triggered tool-usage. However, the former hampers <b>LLMs&rsquo;</b> flexibility to address diverse user&rsquo;s queries due to constrained tool interactions, while the latter limits the generalizability when engaging with new tools, since tool-usage learning is based on task- and tool-specific datasets. To alleviate these concerns, in this paper, we propose a decision-aware and generalizable tool-usage framework (DEER). Specifically, we first construct the tool-usage samples with multiple decision branches via an automatic generation pipeline, thereby inspiring the decision-making awareness of <b>LLMs</b> under diverse scenarios. Meanwhile, we propose a novel tool sampling strategy to enhance the generalizability of <b>LLMs</b> over unseen tools. Extensive experiments demonstrate that our proposed DEER is effective and significantly outperforms baselines across various datasets.</p></p class="citation"></blockquote><h3 id=662--25279-esg-sentiment-analysis-comparing-human-and-language-model-performance-including-gpt-karim-derrick-2024>(6/62 | 25/279) ESG Sentiment Analysis: comparing human and language model performance including GPT (Karim Derrick, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Karim Derrick. (2024)<br><strong>ESG Sentiment Analysis: comparing human and language model performance including GPT</strong><br><button class=copy-to-clipboard title="ESG Sentiment Analysis: comparing human and language model performance including GPT" index=25>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-25 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CE, cs-CL, cs-CY, cs.CL<br>Keyword Score: 70<br>Keywords: GPT, GPT-3, GPT-3.5, GPT-4, Mistral, T5, Sentiment Analysis<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16650v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16650v1.pdf filename=2402.16650v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper we explore the challenges of measuring <b>sentiment</b> <b>in</b> relation to Environmental, Social and Governance (ESG) social media. ESG has grown in importance in recent years with a surge in interest from the financial sector and the performance of many businesses has become based in part on their ESG related reputations. The use of <b>sentiment</b> <b>analysis</b> to measure ESG related reputation has developed and with it interest in the use of machines to do so. The era of digital media has created an explosion of new media sources, driven by the growth of social media platforms. This growing data environment has become an excellent source for behavioural insight studies across many disciplines that includes politics, healthcare and market research. Our study seeks to compare human performance with the cutting edge in machine performance in the measurement of ESG related <b>sentiment.</b> <b>To</b> this end researchers classify the <b>sentiment</b> <b>of</b> 150 tweets and a reliability measure is made. A gold standard data set is then established based on the consensus of 3 researchers and this data set is then used to measure the performance of different machine approaches: one based on the VADER dictionary approach to <b>sentiment</b> <b>classification</b> and then multiple language model approaches, including Llama2, <b>T5,</b> <b>Mistral,</b> Mixtral, FINBERT, <b>GPT3.5</b> and <b>GPT4.</b></p></p class="citation"></blockquote><h3 id=762--26279-long-context-language-modeling-with-parallel-context-encoding-howard-yen-et-al-2024>(7/62 | 26/279) Long-Context Language Modeling with Parallel Context Encoding (Howard Yen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Howard Yen, Tianyu Gao, Danqi Chen. (2024)<br><strong>Long-Context Language Modeling with Parallel Context Encoding</strong><br><button class=copy-to-clipboard title="Long-Context Language Modeling with Parallel Context Encoding" index=26>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-26 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 70<br>Keywords: LLaMA, Transformer, Instruction Following, In-context Learning, In-context Learning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16617v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16617v1.pdf filename=2402.16617v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Extending <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> to process longer inputs is crucial for numerous applications. However, the considerable computational cost of <b>transformers,</b> coupled with limited generalization of positional encoding, restricts the size of their context window. We introduce Context Expansion with Parallel Encoding (CEPE), a framework that can be applied to any existing decoder-only <b>LLMs</b> to extend their context window. CEPE adopts a small encoder to process long inputs chunk by chunk and enables the frozen decoder to leverage additional contexts via cross-attention. CEPE is efficient, generalizable, and versatile: trained with 8K-token documents, CEPE extends the context window of <b>LLAMA-2</b> to 128K tokens, offering 10x the throughput with only 1/6 of the memory. CEPE yields strong performance on language modeling and <b>in-context</b> <b>learning.</b> CEPE also excels in retrieval-augmented applications, while existing long-context models degenerate with retrieved contexts. We further introduce a CEPE variant that can extend the context window of <b>instruction-tuned</b> <b>models</b> with only unlabeled data, and showcase its effectiveness on <b>LLAMA-2-CHAT,</b> leading to a strong <b>instruction-following</b> <b>model</b> that can leverage very long context on downstream tasks.</p></p class="citation"></blockquote><h3 id=862--27279-medit-multilingual-text-editing-via-instruction-tuning-vipul-raheja-et-al-2024>(8/62 | 27/279) mEdIT: Multilingual Text Editing via Instruction Tuning (Vipul Raheja et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Vipul Raheja, Dimitris Alikaniotis, Vivek Kulkarni, Bashar Alhafni, Dhruv Kumar. (2024)<br><strong>mEdIT: Multilingual Text Editing via Instruction Tuning</strong><br><button class=copy-to-clipboard title="mEdIT: Multilingual Text Editing via Instruction Tuning" index=27>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-27 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: I-2-7, cs-AI, cs-CL, cs.CL<br>Keyword Score: 63<br>Keywords: Benchmarking, Fine-tuning, Grammatical Error Correction, Grammatical Error Correction, Instruction Tuning, Large Language Model, Pre-trained Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16472v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16472v1.pdf filename=2402.16472v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce mEdIT, a multi-lingual extension to CoEdIT &ndash; the recent state-of-the-art text editing models for writing assistance. mEdIT models are trained by <b>fine-tuning</b> multi-lingual large, <b>pre-trained</b> <b>language</b> <b>models</b> <b>(LLMs)</b> via <b>instruction</b> <b>tuning.</b> They are designed to take <b>instructions</b> <b>from</b> the user specifying the attributes of the desired text in the form of natural language <b>instructions,</b> <b>such</b> as Grammatik korrigieren (German) or Parafrasee la oraci'on (Spanish). We build mEdIT by curating data from multiple publicly available human-annotated text editing datasets for three text editing tasks <b>(Grammatical</b> <b>Error</b> <b>Correction</b> <b>(GEC),</b> Text Simplification, and Paraphrasing) across diverse languages belonging to six different language families. We detail the design and training of mEdIT models and demonstrate their strong performance on many multi-lingual text editing <b>benchmarks</b> against other multilingual <b>LLMs.</b> We also find that mEdIT generalizes effectively to new languages over multilingual baselines. We publicly release our data, code, and trained models at <a href=https://github.com/vipulraheja/medit>https://github.com/vipulraheja/medit</a>.</p></p class="citation"></blockquote><h3 id=962--28279-retrievalqa-assessing-adaptive-retrieval-augmented-generation-for-short-form-open-domain-question-answering-zihan-zhang-et-al-2024>(9/62 | 28/279) RetrievalQA: Assessing Adaptive Retrieval-Augmented Generation for Short-form Open-Domain Question Answering (Zihan Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zihan Zhang, Meng Fang, Ling Chen. (2024)<br><strong>RetrievalQA: Assessing Adaptive Retrieval-Augmented Generation for Short-form Open-Domain Question Answering</strong><br><button class=copy-to-clipboard title="RetrievalQA: Assessing Adaptive Retrieval-Augmented Generation for Short-form Open-Domain Question Answering" index=28>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-28 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 63<br>Keywords: Benchmarking, Retrieval-Augmented Generation, Retrieval-Augmented Generation, Open-Domain Question Answering, Question Answering, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16457v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16457v1.pdf filename=2402.16457v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Adaptive <b>retrieval-augmented</b> <b>generation</b> <b>(ARAG)</b> aims to dynamically determine the necessity of <b>retrieval</b> <b>for</b> <b>queries</b> instead of retrieving indiscriminately to enhance the efficiency and relevance of the sourced information. However, previous works largely overlook the evaluation of ARAG approaches, leading to their effectiveness being understudied. This work presents a <b>benchmark,</b> RetrievalQA, comprising 1,271 short-form <b>questions</b> <b>covering</b> new world and long-tail knowledge. The knowledge necessary to answer the <b>questions</b> <b>is</b> absent from <b>LLMs;</b> therefore, external information must be retrieved to answer correctly. This makes RetrievalQA a suitable testbed to evaluate existing ARAG methods. We observe that calibration-based methods heavily rely on threshold tuning, while vanilla <b>prompting</b> is inadequate for guiding <b>LLMs</b> to make reliable <b>retrieval</b> <b>decisions.</b> <b>Based</b> on our findings, we propose Time-Aware Adaptive <b>Retrieval</b> <b>(TA-ARE),</b> <b>a</b> simple yet effective method that helps <b>LLMs</b> assess the necessity of <b>retrieval</b> <b>without</b> <b>calibration</b> or additional training. The dataset and code will be available at \url{https://github.com/hyintell/RetrievalQA}</p></p class="citation"></blockquote><h3 id=1062--29279-mozip-a-multilingual-benchmark-to-evaluate-large-language-models-in-intellectual-property-shiwen-ni-et-al-2024>(10/62 | 29/279) MoZIP: A Multilingual Benchmark to Evaluate Large Language Models in Intellectual Property (Shiwen Ni et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shiwen Ni, Minghuan Tan, Yuelin Bai, Fuqiang Niu, Min Yang, Bowen Zhang, Ruifeng Xu, Xiaojun Chen, Chengming Li, Xiping Hu, Ye Li, Jianping Fan. (2024)<br><strong>MoZIP: A Multilingual Benchmark to Evaluate Large Language Models in Intellectual Property</strong><br><button class=copy-to-clipboard title="MoZIP: A Multilingual Benchmark to Evaluate Large Language Models in Intellectual Property" index=29>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-29 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 63<br>Keywords: Benchmarking, Fine-tuning, Supervised Learning, ChatGPT, Question Answering, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16389v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16389v1.pdf filename=2402.16389v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have demonstrated impressive performance in various natural language processing (NLP) tasks. However, there is limited understanding of how well <b>LLMs</b> perform in specific domains (e.g, the intellectual property (IP) domain). In this paper, we contribute a new <b>benchmark,</b> the first Multilingual-oriented quiZ on Intellectual Property (MoZIP), for the evaluation of <b>LLMs</b> in the IP domain. The MoZIP <b>benchmark</b> includes three challenging tasks: IP multiple-choice quiz (IPQuiz), IP <b>question</b> <b>answering</b> (IPQA), and patent matching (PatentMatch). In addition, we also develop a new IP-oriented multilingual <b>large</b> <b>language</b> <b>model</b> (called MoZi), which is a BLOOMZ-based model that has been <b>supervised</b> <b>fine-tuned</b> with multilingual IP-related text data. We evaluate our proposed MoZi model and four well-known <b>LLMs</b> (i.e., BLOOMZ, BELLE, ChatGLM and <b>ChatGPT)</b> on the MoZIP <b>benchmark.</b> Experimental results demonstrate that MoZi outperforms BLOOMZ, BELLE and ChatGLM by a noticeable margin, while it had lower scores compared with <b>ChatGPT.</b> Notably, the performance of current <b>LLMs</b> on the MoZIP <b>benchmark</b> has much room for improvement, and even the most powerful <b>ChatGPT</b> does not reach the passing level. Our source code, data, and models are available at \url{https://github.com/AI-for-Science/MoZi}.</p></p class="citation"></blockquote><h3 id=1162--30279-perltqa-a-personal-long-term-memory-dataset-for-memory-classification-retrieval-and-synthesis-in-question-answering-yiming-du-et-al-2024>(11/62 | 30/279) PerLTQA: A Personal Long-Term Memory Dataset for Memory Classification, Retrieval, and Synthesis in Question Answering (Yiming Du et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yiming Du, Hongru Wang, Zhengyi Zhao, Bin Liang, Baojun Wang, Wanjun Zhong, Zezhong Wang, Kam-Fai Wong. (2024)<br><strong>PerLTQA: A Personal Long-Term Memory Dataset for Memory Classification, Retrieval, and Synthesis in Question Answering</strong><br><button class=copy-to-clipboard title="PerLTQA: A Personal Long-Term Memory Dataset for Memory Classification, Retrieval, and Synthesis in Question Answering" index=30>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-30 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-IR, cs.CL<br>Keyword Score: 63<br>Keywords: Benchmarking, BERT, ChatGPT, Question Answering, Question Answering, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16288v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16288v1.pdf filename=2402.16288v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Long-term memory plays a critical role in personal interaction, considering long-term memory can better leverage world knowledge, historical information, and preferences in dialogues. Our research introduces PerLTQA, an innovative <b>QA</b> dataset that combines semantic and episodic memories, including world knowledge, profiles, social relationships, events, and dialogues. This dataset is collected to investigate the use of personalized memories, focusing on social interactions and events in the <b>QA</b> task. PerLTQA features two types of memory and a comprehensive <b>benchmark</b> of 8,593 <b>questions</b> <b>for</b> 30 characters, facilitating the exploration and application of personalized memories in <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs).</b> Based on PerLTQA, we propose a novel framework for memory integration and generation, consisting of three main components: Memory Classification, Memory Retrieval, and Memory Synthesis. We evaluate this framework using five <b>LLMs</b> and three retrievers. Experimental results demonstrate that <b>BERT-based</b> classification models significantly outperform <b>LLMs</b> such as ChatGLM3 and <b>ChatGPT</b> in the memory classification task. Furthermore, our study highlights the importance of effective memory integration in the <b>QA</b> task.</p></p class="citation"></blockquote><h3 id=1262--31279-rethinking-negative-instances-for-generative-named-entity-recognition-yuyang-ding-et-al-2024>(12/62 | 31/279) Rethinking Negative Instances for Generative Named Entity Recognition (Yuyang Ding et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuyang Ding, Juntao Li, Pinzheng Wang, Zecheng Tang, Bowen Yan, Min Zhang. (2024)<br><strong>Rethinking Negative Instances for Generative Named Entity Recognition</strong><br><button class=copy-to-clipboard title="Rethinking Negative Instances for Generative Named Entity Recognition" index=31>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-31 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 60<br>Keywords: Zero-shot, Named Entity Recognition, Named Entity Recognition, Instruction Tuning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16602v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16602v1.pdf filename=2402.16602v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have demonstrated impressive capabilities for generalizing in unseen tasks. In the <b>Named</b> <b>Entity</b> <b>Recognition</b> <b>(NER)</b> task, recent advancements have seen the remarkable improvement of <b>LLMs</b> in a broad range of entity domains via <b>instruction</b> <b>tuning,</b> by adopting entity-centric schema. In this work, we explore the potential enhancement of the existing methods by incorporating negative instances into training. Our experiments reveal that negative instances contribute to remarkable improvements by (1) introducing contextual information, and (2) clearly delineating label boundaries. Furthermore, we introduce a novel and efficient algorithm <b>named</b> <b>Hierarchical</b> <b>Matching,</b> which is tailored to transform unstructured predictions into structured entities. By integrating these components, we present GNER, a Generative <b>NER</b> system that shows improved <b>zero-shot</b> performance across unseen entity domains. Our comprehensive evaluation illustrates our system&rsquo;s superiority, surpassing state-of-the-art (SoTA) methods by 11 $F_1$ score in <b>zero-shot</b> evaluation.</p></p class="citation"></blockquote><h3 id=1362--32279-llm-based-privacy-data-augmentation-guided-by-knowledge-distillation-with-a-distribution-tutor-for-medical-text-classification-yiping-song-et-al-2024>(13/62 | 32/279) LLM-based Privacy Data Augmentation Guided by Knowledge Distillation with a Distribution Tutor for Medical Text Classification (Yiping Song et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yiping Song, Juhua Zhang, Zhiliang Tian, Yuxin Yang, Minlie Huang, Dongsheng Li. (2024)<br><strong>LLM-based Privacy Data Augmentation Guided by Knowledge Distillation with a Distribution Tutor for Medical Text Classification</strong><br><button class=copy-to-clipboard title="LLM-based Privacy Data Augmentation Guided by Knowledge Distillation with a Distribution Tutor for Medical Text Classification" index=32>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-32 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-CR, cs.CL<br>Keyword Score: 60<br>Keywords: Data Augmentation, Knowledge Distillation, Knowledge Distillation, Text Classification, Large Language Model, Differential Privacy<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16515v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16515v1.pdf filename=2402.16515v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>As sufficient <b>data</b> <b>are</b> not always publically accessible for model training, researchers exploit limited <b>data</b> <b>with</b> advanced learning algorithms or expand the dataset via <b>data</b> <b>augmentation</b> (DA). Conducting DA in private domain requires private protection approaches (i.e. anonymization and perturbation), but those methods cannot provide protection guarantees. <b>Differential</b> <b>privacy</b> (DP) learning methods theoretically bound the protection but are not skilled at generating pseudo <b>text</b> <b>samples</b> with large models. In this paper, we transfer DP-based pseudo sample generation task to DP-based generated samples discrimination task, where we propose a DP-based DA method with a <b>LLM</b> and a DP-based discriminator for <b>text</b> <b>classification</b> on private domains. We construct a <b>knowledge</b> <b>distillation</b> model as the DP-based discriminator: teacher models, accessing private <b>data,</b> <b>teaches</b> students how to select private samples with calibrated noise to achieve DP. To constrain the distribution of DA&rsquo;s generation, we propose a DP-based tutor that models the noised private distribution and controls samples&rsquo; generation with a low privacy cost. We theoretically analyze our model&rsquo;s privacy protection and empirically verify our model.</p></p class="citation"></blockquote><h3 id=1462--33279-pre-training-cross-lingual-open-domain-question-answering-with-large-scale-synthetic-supervision-fan-jiang-et-al-2024>(14/62 | 33/279) Pre-training Cross-lingual Open Domain Question Answering with Large-scale Synthetic Supervision (Fan Jiang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fan Jiang, Tom Drummond, Trevor Cohn. (2024)<br><strong>Pre-training Cross-lingual Open Domain Question Answering with Large-scale Synthetic Supervision</strong><br><button class=copy-to-clipboard title="Pre-training Cross-lingual Open Domain Question Answering with Large-scale Synthetic Supervision" index=33>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-33 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-IR, cs.CL<br>Keyword Score: 60<br>Keywords: Self-supervised Learning, Supervised Learning, Zero-shot, Neural Machine Translation, Open-Domain Question Answering, Question Answering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16508v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16508v1.pdf filename=2402.16508v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Cross-lingual <b>question</b> <b>answering</b> (CLQA) is a complex problem, comprising cross-lingual retrieval from a multilingual knowledge base, followed by answer generation either in English or the query language. Both steps are usually tackled by separate models, requiring substantial annotated datasets, and typically auxiliary resources, like <b>machine</b> <b>translation</b> systems to bridge between languages. In this paper, we show that CLQA can be addressed using a single encoder-decoder model. To effectively train this model, we propose a <b>self-supervised</b> method based on exploiting the cross-lingual link structure within Wikipedia. We demonstrate how linked Wikipedia pages can be used to synthesise supervisory signals for cross-lingual retrieval, through a form of cloze query, and generate more natural queries to supervise answer generation. Together, we show our approach, \texttt{CLASS}, outperforms comparable methods on both <b>supervised</b> and <b>zero-shot</b> language adaptation settings, including those using <b>machine</b> <b>translation.</b></p></p class="citation"></blockquote><h3 id=1562--34279-from-rags-to-riches-using-large-language-models-to-write-documents-for-clinical-trials-nigel-markey-et-al-2024>(15/62 | 34/279) From RAGs to riches: Using large language models to write documents for clinical trials (Nigel Markey et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nigel Markey, Ilyass El-Mansouri, Gaetan Rensonnet, Casper van Langen, Christoph Meier. (2024)<br><strong>From RAGs to riches: Using large language models to write documents for clinical trials</strong><br><button class=copy-to-clipboard title="From RAGs to riches: Using large language models to write documents for clinical trials" index=34>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-34 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 60<br>Keywords: Retrieval-Augmented Generation, Retrieval-Augmented Generation, Retrieval-Augmented Generation, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16406v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16406v1.pdf filename=2402.16406v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Clinical trials require numerous documents to be written &ndash; protocols, consent forms, clinical study reports and others. <b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> offer the potential to rapidly generate first versions of these documents, however there are concerns about the quality of their output Here we report an evaluation of <b>LLMs</b> in generating parts of one such document, clinical trial protocols. We find that an offthe-shelf <b>LLM</b> delivers reasonable results, especially when assessing content relevance and the correct use of terminology. However, deficiencies remain: specifically clinical thinking and logic, and appropriate use of references. To improve performance, we used <b>retrieval-augmented</b> <b>generation</b> <b>(RAG)</b> to <b>prompt</b> an <b>LLM</b> with accurate up-to-date information. As a result of using <b>RAG,</b> the writing quality of the <b>LLM</b> improves substantially, which has implications for the practical useability of <b>LLMs</b> in clinical trial-related writing.</p></p class="citation"></blockquote><h3 id=1662--35279-improving-llm-based-machine-translation-with-systematic-self-correction-zhaopeng-feng-et-al-2024>(16/62 | 35/279) Improving LLM-based Machine Translation with Systematic Self-Correction (Zhaopeng Feng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhaopeng Feng, Yan Zhang, Hao Li, Wenqiang Liu, Jun Lang, Yang Feng, Jian Wu, Zuozhu Liu. (2024)<br><strong>Improving LLM-based Machine Translation with Systematic Self-Correction</strong><br><button class=copy-to-clipboard title="Improving LLM-based Machine Translation with Systematic Self-Correction" index=35>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-35 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 60<br>Keywords: High-Resource, Low-Resource, Neural Machine Translation, Neural Machine Translation, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16379v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16379v1.pdf filename=2402.16379v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have achieved impressive results in <b>Machine</b> <b>Translation</b> <b>(MT).</b> However, careful evaluations by human reveal that the translations produced by <b>LLMs</b> still contain multiple errors. Importantly, feeding back such error information into the <b>LLMs</b> can lead to self-correction and result in improved translation performance. Motivated by these insights, we introduce a systematic <b>LLM-based</b> self-correcting translation framework, named TER, which stands for Translate, Estimate, and Refine, marking a significant step forward in this direction. Our findings demonstrate that 1) our self-correction framework successfully assists <b>LLMs</b> in improving their translation quality across a wide range of languages, whether it&rsquo;s from <b>high-resource</b> languages to <b>low-resource</b> ones or whether it&rsquo;s English-centric or centered around other languages; 2) TER exhibits superior systematicity and interpretability compared to previous methods; 3) different estimation strategies yield varied impacts on AI feedback, directly affecting the effectiveness of the final corrections. We further compare different <b>LLMs</b> and conduct various experiments involving self-correction and cross-model correction to investigate the potential relationship between the translation and evaluation capabilities of <b>LLMs.</b></p></p class="citation"></blockquote><h3 id=1762--36279-llm-inference-unveiled-survey-and-roofline-model-insights-zhihang-yuan-et-al-2024>(17/62 | 36/279) LLM Inference Unveiled: Survey and Roofline Model Insights (Zhihang Yuan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhihang Yuan, Yuzhang Shang, Yang Zhou, Zhen Dong, Chenhao Xue, Bingzhe Wu, Zhikai Li, Qingyi Gu, Yong Jae Lee, Yan Yan, Beidi Chen, Guangyu Sun, Kurt Keutzer. (2024)<br><strong>LLM Inference Unveiled: Survey and Roofline Model Insights</strong><br><button class=copy-to-clipboard title="LLM Inference Unveiled: Survey and Roofline Model Insights" index=36>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-36 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 60<br>Keywords: Knowledge Distillation, Knowledge Distillation, Model Compression, Quantization, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16363v3 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16363v3.pdf filename=2402.16363v3.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The field of efficient <b>Large</b> <b>Language</b> <b>Model</b> <b>(LLM)</b> inference is rapidly evolving, presenting a unique blend of opportunities and challenges. Although the field has expanded and is vibrant, there hasn&rsquo;t been a concise framework that analyzes the various methods of <b>LLM</b> Inference to provide a clear understanding of this domain. Our survey stands out from traditional literature reviews by not only summarizing the current state of research but also by introducing a framework based on roofline <b>model</b> <b>for</b> systematic analysis of <b>LLM</b> inference techniques. This framework identifies the bottlenecks when deploying <b>LLMs</b> on hardware devices and provides a clear understanding of practical problems, such as why <b>LLMs</b> are memory-bound, how much memory and computation they need, and how to choose the right hardware. We systematically collate the latest advancements in efficient <b>LLM</b> inference, covering crucial areas such as <b>model</b> <b>compression</b> (e.g., <b>Knowledge</b> <b>Distillation</b> and <b>Quantization),</b> algorithm improvements (e.g., Early Exit and Mixture-of-Expert), and both hardware and system-level enhancements. Our survey stands out by analyzing these methods with roofline <b>model,</b> <b>helping</b> us understand their impact on memory access and computation. This distinctive approach not only showcases the current research landscape but also delivers valuable insights for practical implementation, positioning our work as an indispensable resource for researchers new to the field as well as for those seeking to deepen their understanding of efficient <b>LLM</b> deployment. The analyze tool, <b>LLM-Viewer,</b> is open-sourced.</p></p class="citation"></blockquote><h3 id=1862--37279-layer-wise-regularized-dropout-for-neural-language-models-shiwen-ni-et-al-2024>(18/62 | 37/279) Layer-wise Regularized Dropout for Neural Language Models (Shiwen Ni et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shiwen Ni, Min Yang, Ruifeng Xu, Chengming Li, Xiping Hu. (2024)<br><strong>Layer-wise Regularized Dropout for Neural Language Models</strong><br><button class=copy-to-clipboard title="Layer-wise Regularized Dropout for Neural Language Models" index=37>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-37 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 60<br>Keywords: Self-Distillation, Transformer, Natural Language Understanding, Neural Machine Translation, Neural Machine Translation, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16361v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16361v1.pdf filename=2402.16361v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Among the various pre-trained <b>neural</b> <b>language</b> <b>models</b> that are popular today, dropout is already an indispensable regularization technique. To solve the inconsistency between training and inference caused by the randomness of dropout, some studies use consistency training to regularize dropout at the output layer. In this paper, we propose a novel Layer-wise Regularized Dropout (LR-Drop), which is specially designed for <b>Transformer-based</b> Language models. Specifically, LR-Drop layer-wise regularizes each <b>Transformer</b> layer using the consistency training strategy. Each training sample passes through the two siamese sub-models sampled by dropout, and then LR-Drop forces the hidden states, multi-head attention matrices, and output distribution of the two siamese sub-models to be consistent. The proposed LR-Drop can be regarded as a <b>&ldquo;self-distillation&rdquo;</b> framework, in which each sub-model generated by dropout is the other&rsquo;s &ldquo;teacher&rdquo; model and &ldquo;student&rdquo; model. Through extensive experiments on 8 <b>natural</b> <b>language</b> <b>understanding</b> datasets, 6 <b>neural</b> <b>machine</b> <b>translation</b> datasets, and 1 abstractive <b>summarization</b> dataset (a total of 15 datasets), we show that LR-Drop achieves superior performances, including state-of-the-art results.</p></p class="citation"></blockquote><h3 id=1962--38279-mathgenie-generating-synthetic-data-with-question-back-translation-for-enhancing-mathematical-reasoning-of-llms-zimu-lu-et-al-2024>(19/62 | 38/279) MathGenie: Generating Synthetic Data with Question Back-translation for Enhancing Mathematical Reasoning of LLMs (Zimu Lu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zimu Lu, Aojun Zhou, Houxing Ren, Ke Wang, Weikang Shi, Junting Pan, Mingjie Zhan, Hongsheng Li. (2024)<br><strong>MathGenie: Generating Synthetic Data with Question Back-translation for Enhancing Mathematical Reasoning of LLMs</strong><br><button class=copy-to-clipboard title="MathGenie: Generating Synthetic Data with Question Back-translation for Enhancing Mathematical Reasoning of LLMs" index=38>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-38 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 60<br>Keywords: GPT, GPT-4, Mathematical Reasoning, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16352v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16352v1.pdf filename=2402.16352v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have exhibited great potential in <b>mathematical</b> <b>reasoning.</b> However, there remains a performance gap in this area between existing open-source models and closed-source models such as <b>GPT-4.</b> In this paper, we introduce MathGenie, a novel method for generating diverse and reliable math problems from a small-scale problem-solution dataset (denoted as seed data). We augment the ground-truth solutions of our seed data and train a back-translation model to translate the augmented solutions back into new questions. Subsequently, we generate code-integrated solutions for the new questions. To ensure the correctness of the code-integrated solutions, we employ rationale-based strategy for solution verification. Various pretrained models, ranging from 7B to 70B, are trained on the newly curated data to test the effectiveness of the proposed augmentation technique, resulting in a family of models known as MathGenieLM. These models consistently outperform previous open-source models across five representative <b>mathematical</b> <b>reasoning</b> datasets, achieving state-of-the-art performance. In particular, MathGenieLM-InternLM2 achieves an accuracy of 87.7% on GSM8K and 55.7% on MATH, securing the best overall score among open-source language models.</p></p class="citation"></blockquote><h3 id=2062--39279-mysterious-projections-multimodal-llms-gain-domain-specific-visual-capabilities-without-richer-cross-modal-projections-gaurav-verma-et-al-2024>(20/62 | 39/279) Mysterious Projections: Multimodal LLMs Gain Domain-Specific Visual Capabilities Without Richer Cross-Modal Projections (Gaurav Verma et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Gaurav Verma, Minje Choi, Kartik Sharma, Jamelle Watson-Daniels, Sejoon Oh, Srijan Kumar. (2024)<br><strong>Mysterious Projections: Multimodal LLMs Gain Domain-Specific Visual Capabilities Without Richer Cross-Modal Projections</strong><br><button class=copy-to-clipboard title="Mysterious Projections: Multimodal LLMs Gain Domain-Specific Visual Capabilities Without Richer Cross-Modal Projections" index=39>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-39 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-CV, cs.CL<br>Keyword Score: 56<br>Keywords: Fine-tuning, Fine-tuning, Multi-modal, Multi-modal, GPT, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16832v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16832v1.pdf filename=2402.16832v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Multimodal</b> <b>large</b> <b>language</b> <b>models</b> (MLLMs) like LLaVA and <b>GPT-4(V)</b> enable general-purpose conversations about images with the language modality. As off-the-shelf MLLMs may have limited capabilities on images from domains like dermatology and agriculture, they must be <b>fine-tuned</b> to unlock domain-specific applications. The prevalent architecture of current open-source MLLMs comprises two major modules: an image-language (cross-modal) projection network and a <b>large</b> <b>language</b> <b>model.</b> It is desirable to understand the roles of these two modules in modeling domain-specific visual attributes to inform the design of future models and streamline the interpretability efforts on the current models. To this end, via experiments on 4 datasets and under 2 <b>fine-tuning</b> settings, we find that as the MLLM is <b>fine-tuned,</b> it indeed gains domain-specific visual capabilities, but the updates do not lead to the projection extracting relevant domain-specific visual attributes. Our results indicate that the domain-specific visual attributes are modeled by the <b>LLM,</b> even when only the projection is <b>fine-tuned.</b> Through this study, we offer a potential reinterpretation of the role of cross-modal projections in MLLM architectures. Projection webpage: <a href=https://claws-lab.github.io/projection-in-MLLMs/>https://claws-lab.github.io/projection-in-MLLMs/</a></p></p class="citation"></blockquote><h3 id=2162--40279-a-comprehensive-evaluation-of-quantization-strategies-for-large-language-models-renren-jin-et-al-2024>(21/62 | 40/279) A Comprehensive Evaluation of Quantization Strategies for Large Language Models (Renren Jin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Renren Jin, Jiangcun Du, Wuwei Huang, Wei Liu, Jian Luan, Bin Wang, Deyi Xiong. (2024)<br><strong>A Comprehensive Evaluation of Quantization Strategies for Large Language Models</strong><br><button class=copy-to-clipboard title="A Comprehensive Evaluation of Quantization Strategies for Large Language Models" index=40>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-40 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 53<br>Keywords: Benchmarking, Quantization, Quantization, Large Language Model, Large Language Model, Perplexity<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16775v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16775v1.pdf filename=2402.16775v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Increasing the number of parameters in <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> usually improves performance in downstream tasks but raises compute and memory costs, making deployment difficult in resource-limited settings. <b>Quantization</b> techniques, which reduce the bits needed for model weights or activations with minimal performance loss, have become popular due to the rise of <b>LLMs.</b> However, most <b>quantization</b> studies use pre-trained <b>LLMs,</b> and the impact of <b>quantization</b> on instruction-tuned <b>LLMs</b> and the relationship between <b>perplexity</b> and <b>benchmark</b> performance of <b>quantized</b> <b>LLMs</b> are not well understood. Evaluation of <b>quantized</b> <b>LLMs</b> is often limited to language modeling and a few classification tasks, leaving their performance on other <b>benchmarks</b> unclear. To address these gaps, we propose a structured evaluation framework consisting of three critical dimensions: (1) knowledge & capacity, (2) alignment, and (3) efficiency, and conduct extensive experiments across ten diverse <b>benchmarks.</b> Our experimental results indicate that <b>LLMs</b> with 4-bit <b>quantization</b> can retain performance comparable to their non-quantized counterparts, and <b>perplexity</b> can serve as a proxy metric for <b>quantized</b> <b>LLMs</b> on most <b>benchmarks.</b> Furthermore, <b>quantized</b> <b>LLMs</b> with larger parameter scales can outperform smaller <b>LLMs.</b> Despite the memory savings achieved through <b>quantization,</b> it can also slow down the inference speed of <b>LLMs.</b> Consequently, substantial engineering efforts and hardware support are imperative to achieve a balanced optimization of decoding speed and memory consumption in the context of <b>quantized</b> <b>LLMs.</b></p></p class="citation"></blockquote><h3 id=2262--41279-structlm-towards-building-generalist-models-for-structured-knowledge-grounding-alex-zhuang-et-al-2024>(22/62 | 41/279) StructLM: Towards Building Generalist Models for Structured Knowledge Grounding (Alex Zhuang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Alex Zhuang, Ge Zhang, Tianyu Zheng, Xinrun Du, Junjie Wang, Weiming Ren, Stephen W. Huang, Jie Fu, Xiang Yue, Wenhu Chen. (2024)<br><strong>StructLM: Towards Building Generalist Models for Structured Knowledge Grounding</strong><br><button class=copy-to-clipboard title="StructLM: Towards Building Generalist Models for Structured Knowledge Grounding" index=41>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-41 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 53<br>Keywords: Graph, ChatGPT, Grounding, Instruction Tuning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16671v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16671v2.pdf filename=2402.16671v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Structured data sources, such as tables, <b>graphs,</b> and databases, are ubiquitous knowledge sources. Despite the demonstrated capabilities of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> on plain text, their proficiency in interpreting and utilizing structured data remains limited. Our investigation reveals a notable deficiency in <b>LLMs&rsquo;</b> ability to process structured data, e.g., <b>ChatGPT</b> lags behind state-of-the-art (SoTA) model by an average of 35%. To augment the Structured Knowledge <b>Grounding</b> (SKG) capabilities in <b>LLMs,</b> we have developed a comprehensive <b>instruction</b> <b>tuning</b> dataset comprising 1.1 million examples. Utilizing this dataset, we train a series of models, referred to as StructLM, based on the Code-LLaMA architecture, ranging from 7B to 34B parameters. Our StructLM series surpasses task-specific models on 14 out of 18 evaluated datasets and establishes new SoTA achievements on 7 SKG tasks. Furthermore, StructLM demonstrates exceptional generalization across 6 novel SKG tasks. Contrary to expectations, we observe that scaling model size offers marginal benefits, with StructLM-34B showing only slight improvements over StructLM-7B. This suggests that structured knowledge <b>grounding</b> is still a challenging task and requires more innovative design to push to a new level.</p></p class="citation"></blockquote><h3 id=2362--42279-aligning-large-language-models-to-a-domain-specific-graph-database-yuanyuan-liang-et-al-2024>(23/62 | 42/279) Aligning Large Language Models to a Domain-specific Graph Database (Yuanyuan Liang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuanyuan Liang, Keren Tan, Tingyu Xie, Wenbiao Tao, Siyuan Wang, Yunshi Lan, Weining Qian. (2024)<br><strong>Aligning Large Language Models to a Domain-specific Graph Database</strong><br><button class=copy-to-clipboard title="Aligning Large Language Models to a Domain-specific Graph Database" index=42>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-42 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-DB, cs.CL<br>Keyword Score: 53<br>Keywords: Graph, Fine-tuning, ChatGPT, Text2SQL, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16567v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16567v2.pdf filename=2402.16567v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Graph</b> Databases <b>(Graph</b> DB) are widely applied in various fields, including finance, social networks, and medicine. However, translating Natural Language (NL) into the <b>Graph</b> Query Language (GQL), commonly known as NL2GQL, proves to be challenging due to its inherent complexity and specialized nature. Some approaches have sought to utilize <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> to address analogous tasks like <b>text2SQL.</b> Nevertheless, when it comes to NL2GQL taskson a particular domain, the absence of domain-specific NL-GQL data pairs makes it difficult to establish alignment between <b>LLMs</b> and the <b>graph</b> DB. To address this challenge, we propose a well-defined pipeline. Specifically, we utilize <b>ChatGPT</b> to create NL-GQL data pairs based on the given <b>graph</b> DB with self-instruct. Then, we use the created data to <b>fine-tune</b> <b>LLMs,</b> thereby achieving alignment between <b>LLMs</b> and the <b>graph</b> DB. Additionally, during inference, we propose a method that extracts relevant schema to the queried NL as the input context to guide <b>LLMs</b> for generating accurate GQLs.We evaluate our method on two constructed datasets deriving from <b>graph</b> DBs in finance domain and medicine domain, namely FinGQL and MediGQL. Experimental results demonstrate that our method significantly outperforms a set of baseline methods, with improvements of 5.90 and 6.36 absolute points on EM, and 6.00 and 7.09 absolute points on EX, respectively.</p></p class="citation"></blockquote><h3 id=2462--43279-z-agi-labs-at-climateactivism-2024-stance-and-hate-event-detection-on-social-media-nikhil-narayan-et-al-2024>(24/62 | 43/279) Z-AGI Labs at ClimateActivism 2024: Stance and Hate Event Detection on Social Media (Nikhil Narayan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nikhil Narayan, Mrutyunjay Biswal. (2024)<br><strong>Z-AGI Labs at ClimateActivism 2024: Stance and Hate Event Detection on Social Media</strong><br><button class=copy-to-clipboard title="Z-AGI Labs at ClimateActivism 2024: Stance and Hate Event Detection on Social Media" index=43>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-43 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: LSTM, Event Detection, Hate Speech Detection, Stance Detection, TF-IDF<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17014v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17014v1.pdf filename=2402.17014v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the digital realm, rich data serves as a crucial source of insights into the complexities of social, political, and economic landscapes. Addressing the growing need for high-quality information on <b>events</b> <b>and</b> the imperative to combat <b>hate</b> <b>speech,</b> <b>this</b> research led to the establishment of the Shared Task on Climate Activism <b>Stance</b> <b>and</b> <b>Hate</b> <b>Event</b> <b>Detection</b> at CASE 2024. Focused on climate activists contending with <b>hate</b> <b>speech</b> <b>on</b> social media, our study contributes to <b>hate</b> <b>speech</b> <b>identification</b> from tweets. Analyzing three sub-tasks - <b>Hate</b> <b>Speech</b> <b>Detection</b> (Sub-task A), Targets of <b>Hate</b> <b>Speech</b> <b>Identification</b> (Sub-task B), and <b>Stance</b> <b>Detection</b> (Sub-task C) - Team Z-AGI Labs evaluated various models, including <b>LSTM,</b> Xgboost, and LGBM based on <b>Tf-Idf.</b> Results unveiled intriguing variations, with Catboost excelling in Subtask-B (F1: 0.5604) and Subtask-C (F1: 0.7081), while LGBM emerged as the top-performing model for Subtask-A (F1: 0.8684). This research provides valuable insights into the suitability of classical machine learning models for climate <b>hate</b> <b>speech</b> <b>and</b> <b>stance</b> <b>detection,</b> aiding informed model selection for robust mechanisms.</p></p class="citation"></blockquote><h3 id=2562--44279-rainbow-teaming-open-ended-generation-of-diverse-adversarial-prompts-mikayel-samvelyan-et-al-2024>(25/62 | 44/279) Rainbow Teaming: Open-Ended Generation of Diverse Adversarial Prompts (Mikayel Samvelyan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mikayel Samvelyan, Sharath Chandra Raparthy, Andrei Lupu, Eric Hambro, Aram H. Markosyan, Manish Bhatt, Yuning Mao, Minqi Jiang, Jack Parker-Holder, Jakob Foerster, Tim Rocktäschel, Roberta Raileanu. (2024)<br><strong>Rainbow Teaming: Open-Ended Generation of Diverse Adversarial Prompts</strong><br><button class=copy-to-clipboard title="Rainbow Teaming: Open-Ended Generation of Diverse Adversarial Prompts" index=44>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-44 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 50<br>Keywords: Fine-tuning, Question Answering, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16822v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16822v1.pdf filename=2402.16822v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>As <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> become increasingly prevalent across many real-world applications, understanding and enhancing their robustness to user inputs is of paramount importance. Existing methods for identifying adversarial <b>prompts</b> tend to focus on specific domains, lack diversity, or require extensive human annotations. To address these limitations, we present Rainbow Teaming, a novel approach for producing a diverse collection of adversarial <b>prompts.</b> Rainbow Teaming casts adversarial <b>prompt</b> generation as a quality-diversity problem, and uses open-ended search to generate <b>prompts</b> that are both effective and diverse. It can uncover a model&rsquo;s vulnerabilities across a broad range of domains including, in this paper, safety, <b>question</b> <b>answering,</b> and cybersecurity. We also demonstrate that <b>fine-tuning</b> on synthetic data generated by Rainbow Teaming improves the safety of state-of-the-art <b>LLMs</b> without hurting their general capabilities and helpfulness, paving the path to open-ended self-improvement.</p></p class="citation"></blockquote><h3 id=2662--45279-investigating-the-effectiveness-of-hypertuning-via-gisting-jason-phang-2024>(26/62 | 45/279) Investigating the Effectiveness of HyperTuning via Gisting (Jason Phang, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jason Phang. (2024)<br><strong>Investigating the Effectiveness of HyperTuning via Gisting</strong><br><button class=copy-to-clipboard title="Investigating the Effectiveness of HyperTuning via Gisting" index=45>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-45 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: Few-shot, Fine-tuning, LLaMA, Transformer, In-context Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16817v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16817v1.pdf filename=2402.16817v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Gisting (Mu et al., 2023) is a simple method for training models to compress information into fewer token representations using a modified attention mask, and can serve as an economical approach to training <b>Transformer-based</b> hypernetworks. We introduce HyperLlama, a set of Gisting-based hypernetworks built on <b>Llama-2</b> models that generates task-specific soft prefixes based on <b>few-shot</b> inputs. In experiments across P3, Super-NaturalInstructions and Symbol Tuning datasets, we show that HyperLlama models can effectively compress information from <b>few-shot</b> examples into soft prefixes. However, they still underperform multi-task <b>fine-tuned</b> language models with full attention over <b>few-shot</b> <b>in-context</b> examples. We also show that HyperLlama-generated soft prefixes can serve as better initializations for further prefix tuning. Overall, Gisting-based hypernetworks are economical and easy to implement, but have mixed empirical performance.</p></p class="citation"></blockquote><h3 id=2762--46279-oncogpt-a-medical-conversational-model-tailored-with-oncology-domain-expertise-on-a-large-language-model-meta-ai-llama-fujian-jia-et-al-2024>(27/62 | 46/279) OncoGPT: A Medical Conversational Model Tailored with Oncology Domain Expertise on a Large Language Model Meta-AI (LLaMA) (Fujian Jia et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fujian Jia, Xin Liu, Lixi Deng, Jiwen Gu, Chunchao Pu, Tunan Bai, Mengjiang Huang, Yuanzhi Lu, Kang Liu. (2024)<br><strong>OncoGPT: A Medical Conversational Model Tailored with Oncology Domain Expertise on a Large Language Model Meta-AI (LLaMA)</strong><br><button class=copy-to-clipboard title="OncoGPT: A Medical Conversational Model Tailored with Oncology Domain Expertise on a Large Language Model Meta-AI (LLaMA)" index=46>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-46 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: Fine-tuning, ChatGPT, LLaMA, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16810v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16810v1.pdf filename=2402.16810v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the past year, there has been a growing trend in applying <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> to the field of medicine, particularly with the advent of advanced language models such as <b>ChatGPT</b> developed by OpenAI. However, there is limited research on <b>LLMs</b> specifically addressing oncology-related queries. The primary aim of this research was to develop a specialized language model that demonstrates improved accuracy in providing advice related to oncology. We performed an extensive data collection of online question-answer interactions centered around oncology, sourced from reputable doctor-patient platforms. Following data cleaning and anonymization, a dataset comprising over 180K+ oncology-related conversations was established. The conversations were categorized and meticulously reviewed by field specialists and clinicians to ensure precision. Employing the <b>LLaMA</b> model and other selected open-source datasets, we conducted iterative <b>fine-tuning</b> to enhance the model&rsquo;s proficiency in basic medical conversation and specialized oncology knowledge. We observed a substantial enhancement in the model&rsquo;s understanding of genuine patient inquiries and its reliability in offering oncology-related advice through the utilization of real online question-answer interactions in the <b>fine-tuning</b> process. We release database and models to the research community (<a href=https://github.com/OncoGPT1)>https://github.com/OncoGPT1)</a>.</p></p class="citation"></blockquote><h3 id=2862--47279-codechameleon-personalized-encryption-framework-for-jailbreaking-large-language-models-huijie-lv-et-al-2024>(28/62 | 47/279) CodeChameleon: Personalized Encryption Framework for Jailbreaking Large Language Models (Huijie Lv et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Huijie Lv, Xiao Wang, Yuansen Zhang, Caishuang Huang, Shihan Dou, Junjie Ye, Tao Gui, Qi Zhang, Xuanjing Huang. (2024)<br><strong>CodeChameleon: Personalized Encryption Framework for Jailbreaking Large Language Models</strong><br><button class=copy-to-clipboard title="CodeChameleon: Personalized Encryption Framework for Jailbreaking Large Language Models" index=47>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-47 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-CR, cs.CL<br>Keyword Score: 50<br>Keywords: GPT, GPT-4, Automatic Speech Recognition, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16717v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16717v1.pdf filename=2402.16717v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Adversarial misuse, particularly through `jailbreaking&rsquo; that circumvents a model&rsquo;s safety and ethical protocols, poses a significant challenge for <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs).</b> This paper delves into the mechanisms behind such successful attacks, introducing a hypothesis for the safety mechanism of aligned <b>LLMs:</b> intent security recognition followed by response generation. Grounded in this hypothesis, we propose CodeChameleon, a novel jailbreak framework based on personalized encryption tactics. To elude the intent security recognition phase, we reformulate tasks into a code completion format, enabling users to encrypt queries using personalized encryption functions. To guarantee response generation functionality, we embed a decryption function within the instructions, which allows the <b>LLM</b> to decrypt and execute the encrypted queries successfully. We conduct extensive experiments on 7 <b>LLMs,</b> achieving state-of-the-art average Attack Success Rate <b>(ASR).</b> Remarkably, our method achieves an 86.6% <b>ASR</b> on <b>GPT-4-1106.</b></p></p class="citation"></blockquote><h3 id=2962--48279-selectit-selective-instruction-tuning-for-large-language-models-via-uncertainty-aware-self-reflection-liangxin-liu-et-al-2024>(29/62 | 48/279) SelectIT: Selective Instruction Tuning for Large Language Models via Uncertainty-Aware Self-Reflection (Liangxin Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Liangxin Liu, Xuebo Liu, Derek F. Wong, Dongfang Li, Ziyi Wang, Baotian Hu, Min Zhang. (2024)<br><strong>SelectIT: Selective Instruction Tuning for Large Language Models via Uncertainty-Aware Self-Reflection</strong><br><button class=copy-to-clipboard title="SelectIT: Selective Instruction Tuning for Large Language Models via Uncertainty-Aware Self-Reflection" index=48>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-48 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 50<br>Keywords: Foundation Model, Alpaca, Instruction Tuning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16705v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16705v1.pdf filename=2402.16705v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Instruction</b> <b>tuning</b> (IT) is crucial to tailoring <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> towards human-centric interactions. Recent advancements have shown that the careful selection of a small, high-quality subset of IT data can significantly enhance the performance of <b>LLMs.</b> Despite this, common approaches often rely on additional models or data sets, which increases costs and limits widespread adoption. In this work, we propose a novel approach, termed SelectIT, that capitalizes on the <b>foundational</b> <b>capabilities</b> of the <b>LLM</b> itself. Specifically, we exploit the intrinsic uncertainty present in <b>LLMs</b> to more effectively select high-quality IT data, without the need for extra resources. Furthermore, we introduce a novel IT dataset, the Selective <b>Alpaca,</b> created by applying SelectIT to the <b>Alpaca-GPT4</b> dataset. Empirical results demonstrate that IT using Selective <b>Alpaca</b> leads to substantial model ability enhancement. The robustness of SelectIT has also been corroborated in various <b>foundation</b> <b>models</b> and domain-specific tasks. Our findings suggest that longer and more computationally intensive IT data may serve as superior sources of IT, offering valuable insights for future research in this area. Data, code, and scripts are freely available at <a href=https://github.com/Blue-Raincoat/SelectIT>https://github.com/Blue-Raincoat/SelectIT</a>.</p></p class="citation"></blockquote><h3 id=3062--49279-unveiling-vulnerability-of-self-attention-khai-jiet-liong-et-al-2024>(30/62 | 49/279) Unveiling Vulnerability of Self-Attention (Khai Jiet Liong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Khai Jiet Liong, Hongqiu Wu, Hai Zhao. (2024)<br><strong>Unveiling Vulnerability of Self-Attention</strong><br><button class=copy-to-clipboard title="Unveiling Vulnerability of Self-Attention" index=49>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-49 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: Adversarial Learning, Transformer, Pre-trained Language Model, Pre-trained Language Model, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16470v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16470v1.pdf filename=2402.16470v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Pre-trained</b> <b>language</b> <b>models</b> <b>(PLMs)</b> are shown to be vulnerable to minor word changes, which poses a big threat to real-world systems. While previous studies directly focus on manipulating word inputs, they are limited by their means of generating <b>adversarial</b> <b>samples,</b> lacking generalization to versatile real-world attack. This paper studies the basic structure of <b>transformer-based</b> <b>PLMs,</b> the <b>self-attention</b> (SA) mechanism. (1) We propose a powerful perturbation technique \textit{HackAttend}, which perturbs the attention scores within the SA matrices via meticulously crafted attention masks. We show that state-of-the-art <b>PLMs</b> fall into heavy vulnerability that minor attention perturbations $(1%)$ can produce a very high attack success rate $(98%)$. Our paper expands the conventional text attack of word perturbations to more general structural perturbations. (2) We introduce \textit{S-Attend}, a novel smoothing technique that effectively makes SA robust via structural perturbations. We empirically demonstrate that this simple yet effective technique achieves robust performance on par with <b>adversarial</b> <b>training</b> when facing various text attackers. Code is publicly available at \url{github.com/liongkj/HackAttend}.</p></p class="citation"></blockquote><h3 id=3162--50279-language-specific-neurons-the-key-to-multilingual-capabilities-in-large-language-models-tianyi-tang-et-al-2024>(31/62 | 50/279) Language-Specific Neurons: The Key to Multilingual Capabilities in Large Language Models (Tianyi Tang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tianyi Tang, Wenyang Luo, Haoyang Huang, Dongdong Zhang, Xiaolei Wang, Xin Zhao, Furu Wei, Ji-Rong Wen. (2024)<br><strong>Language-Specific Neurons: The Key to Multilingual Capabilities in Large Language Models</strong><br><button class=copy-to-clipboard title="Language-Specific Neurons: The Key to Multilingual Capabilities in Large Language Models" index=50>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-50 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: BLOOM, LLaMA, Transformer, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16438v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16438v1.pdf filename=2402.16438v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> demonstrate remarkable multilingual capabilities without being pre-trained on specially curated multilingual parallel corpora. It remains a challenging problem to explain the underlying mechanisms by which <b>LLMs</b> process multilingual texts. In this paper, we delve into the composition of <b>Transformer</b> architectures in <b>LLMs</b> to pinpoint language-specific regions. Specially, we propose a novel detection method, language activation probability entropy (LAPE), to identify language-specific neurons within <b>LLMs.</b> Based on LAPE, we conduct comprehensive experiments on two representative <b>LLMs,</b> namely <b>LLaMA-2</b> and <b>BLOOM.</b> Our findings indicate that <b>LLMs&rsquo;</b> proficiency in processing a particular language is predominantly due to a small subset of neurons, primarily situated in the models&rsquo; top and bottom layers. Furthermore, we showcase the feasibility to &ldquo;steer&rdquo; the output language of <b>LLMs</b> by selectively activating or deactivating language-specific neurons. Our research provides important evidence to the understanding and exploration of the multilingual capabilities of <b>LLMs.</b></p></p class="citation"></blockquote><h3 id=3262--51279-data-freeweight-compress-and-denoise-for-large-language-models-runyu-peng-et-al-2024>(32/62 | 51/279) Data-freeWeight Compress and Denoise for Large Language Models (Runyu Peng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Runyu Peng, Yunhua Zhou, Qipeng Guo, Yang Gao, Hang Yan, Xipeng Qiu, Dahua Lin. (2024)<br><strong>Data-freeWeight Compress and Denoise for Large Language Models</strong><br><button class=copy-to-clipboard title="Data-freeWeight Compress and Denoise for Large Language Models" index=51>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-51 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: Model Pruning, Pruning, Quantization, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16319v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16319v1.pdf filename=2402.16319v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> are reshaping the research landscape in artificial intelligence, particularly as <b>model</b> <b>parameters</b> scale up significantly, unlocking remarkable capabilities across various domains. Nevertheless, the scalability of <b>model</b> <b>parameters</b> faces constraints due to limitations in GPU memory and computational speed. To address these constraints, various weight compression methods have emerged, such as <b>Pruning</b> and <b>Quantization.</b> Given the low-rank nature of weight matrices in language <b>models,</b> <b>the</b> reduction of weights through matrix decomposition undoubtedly holds significant potential and promise. In this paper, drawing upon the intrinsic structure of <b>LLMs,</b> we propose a novel approach termed Data-free Joint Rank-k Approximation for compressing the parameter matrices. Significantly, our method is characterized by without necessitating additional involvement of any corpus, while simultaneously preserving orthogonality in conjunction with <b>pruning</b> and <b>quantization</b> methods. We achieve a <b>model</b> <b>pruning</b> of 80% parameters while retaining 93.43% of the original performance without any calibration data. Additionally, we explore the fundamental properties of the weight matrix of <b>LLMs</b> undergone Rank-k Approximation and conduct comprehensive experiments to elucidate our hypothesis.</p></p class="citation"></blockquote><h3 id=3362--52279-generating-effective-ensembles-for-sentiment-analysis-itay-etelis-et-al-2024>(33/62 | 52/279) Generating Effective Ensembles for Sentiment Analysis (Itay Etelis et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Itay Etelis, Avi Rosenfeld, Abraham Itzhak Weinberg, David Sarne. (2024)<br><strong>Generating Effective Ensembles for Sentiment Analysis</strong><br><button class=copy-to-clipboard title="Generating Effective Ensembles for Sentiment Analysis" index=52>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-52 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 43<br>Keywords: Benchmarking, GPT, GPT-4, Transformer, Sentiment Analysis<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16700v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16700v1.pdf filename=2402.16700v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In recent years, <b>transformer</b> models have revolutionized Natural Language Processing (NLP), achieving exceptional results across various tasks, including <b>Sentiment</b> <b>Analysis</b> (SA). As such, current state-of-the-art approaches for SA predominantly rely on <b>transformer</b> models alone, achieving impressive accuracy levels on <b>benchmark</b> datasets. In this paper, we show that the key for further improving the accuracy of such ensembles for SA is to include not only <b>transformers,</b> but also traditional NLP models, despite the inferiority of the latter compared to <b>transformer</b> models. However, as we empirically show, this necessitates a change in how the ensemble is constructed, specifically relying on the Hierarchical Ensemble Construction (HEC) algorithm we present. Our empirical studies across eight canonical SA datasets reveal that ensembles incorporating a mix of model types, structured via HEC, significantly outperform traditional ensembles. Finally, we provide a comparative analysis of the performance of the HEC and <b>GPT-4,</b> demonstrating that while <b>GPT-4</b> closely approaches state-of-the-art SA methods, it remains outperformed by our proposed ensemble strategy.</p></p class="citation"></blockquote><h3 id=3462--53279-humaneval-xl-a-multilingual-code-generation-benchmark-for-cross-lingual-natural-language-generalization-qiwei-peng-et-al-2024>(34/62 | 53/279) HumanEval-XL: A Multilingual Code Generation Benchmark for Cross-lingual Natural Language Generalization (Qiwei Peng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qiwei Peng, Yekun Chai, Xuhong Li. (2024)<br><strong>HumanEval-XL: A Multilingual Code Generation Benchmark for Cross-lingual Natural Language Generalization</strong><br><button class=copy-to-clipboard title="HumanEval-XL: A Multilingual Code Generation Benchmark for Cross-lingual Natural Language Generalization" index=53>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-53 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-PL, cs-SE, cs.CL<br>Keyword Score: 43<br>Keywords: Benchmarking, Code Generation, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16694v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16694v1.pdf filename=2402.16694v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have made significant progress in generating <b>codes</b> <b>from</b> textual <b>prompts.</b> However, existing <b>benchmarks</b> have mainly concentrated on translating English <b>prompts</b> to multilingual <b>codes</b> <b>or</b> have been constrained to very limited natural languages (NLs). These <b>benchmarks</b> have overlooked the vast landscape of massively multilingual NL to multilingual <b>code,</b> <b>leaving</b> a critical gap in the evaluation of multilingual <b>LLMs.</b> In response, we introduce HumanEval-XL, a massively multilingual <b>code</b> <b>generation</b> <b>benchmark</b> specifically crafted to address this deficiency. HumanEval-XL establishes connections between 23 NLs and 12 programming languages (PLs), and comprises of a collection of 22,080 <b>prompts</b> with an average of 8.33 test cases. By ensuring parallel data across multiple NLs and PLs, HumanEval-XL offers a comprehensive evaluation platform for multilingual <b>LLMs,</b> allowing the assessment of the understanding of different NLs. Our work serves as a pioneering step towards filling the void in evaluating NL generalization in the area of multilingual <b>code</b> <b>generation.</b> We make our evaluation <b>code</b> <b>and</b> data publicly available at \url{https://github.com/FloatAI/HumanEval-XL}.</p></p class="citation"></blockquote><h3 id=3562--54279-mobillama-towards-accurate-and-lightweight-fully-transparent-gpt-omkar-thawakar-et-al-2024>(35/62 | 54/279) MobiLlama: Towards Accurate and Lightweight Fully Transparent GPT (Omkar Thawakar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Omkar Thawakar, Ashmal Vayani, Salman Khan, Hisham Cholakal, Rao M. Anwer, Michael Felsberg, Tim Baldwin, Eric P. Xing, Fahad Shahbaz Khan. (2024)<br><strong>MobiLlama: Towards Accurate and Lightweight Fully Transparent GPT</strong><br><button class=copy-to-clipboard title="MobiLlama: Towards Accurate and Lightweight Fully Transparent GPT" index=54>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-54 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Parameter Sharing, GPT, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16840v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16840v1.pdf filename=2402.16840v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>&ldquo;Bigger the better&rdquo; has been the predominant trend in recent <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> development. However, <b>LLMs</b> do not suit well for scenarios that require on-device processing, energy efficiency, low memory footprint, and response efficiency. These requisites are crucial for privacy, security, and sustainable deployment. This paper explores the &ldquo;less is more&rdquo; paradigm by addressing the challenge of designing accurate yet efficient Small Language Models (SLMs) for resource constrained devices. Our primary contribution is the introduction of an accurate and fully transparent open-source 0.5 billion (0.5B) <b>parameter</b> <b>SLM,</b> named MobiLlama, catering to the specific needs of resource-constrained computing with an emphasis on enhanced performance with reduced resource demands. MobiLlama is a SLM design that initiates from a larger model and applies a careful <b>parameter</b> <b>sharing</b> scheme to reduce both the pre-training and the deployment cost. Our work strives to not only bridge the gap in open-source SLMs but also ensures full transparency, where complete training data pipeline, training code, model weights, and over 300 checkpoints along with evaluation codes is available at : <a href=https://github.com/mbzuai-oryx/MobiLlama>https://github.com/mbzuai-oryx/MobiLlama</a>.</p></p class="citation"></blockquote><h3 id=3662--55279-do-large-language-models-latently-perform-multi-hop-reasoning-sohee-yang-et-al-2024>(36/62 | 55/279) Do Large Language Models Latently Perform Multi-Hop Reasoning? (Sohee Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sohee Yang, Elena Gribovskaya, Nora Kassner, Mor Geva, Sebastian Riedel. (2024)<br><strong>Do Large Language Models Latently Perform Multi-Hop Reasoning?</strong><br><button class=copy-to-clipboard title="Do Large Language Models Latently Perform Multi-Hop Reasoning?" index=55>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-55 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Reasoning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16837v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16837v1.pdf filename=2402.16837v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We study whether <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> latently perform multi-hop <b>reasoning</b> with complex <b>prompts</b> such as &ldquo;The mother of the singer of &lsquo;Superstition&rsquo; is&rdquo;. We look for evidence of a latent <b>reasoning</b> pathway where an <b>LLM</b> (1) latently identifies &ldquo;the singer of &lsquo;Superstition&rsquo;&rdquo; as Stevie Wonder, the bridge entity, and (2) uses its knowledge of Stevie Wonder&rsquo;s mother to complete the <b>prompt.</b> We analyze these two hops individually and consider their co-occurrence as indicative of latent multi-hop <b>reasoning.</b> For the first hop, we test if changing the <b>prompt</b> to indirectly mention the bridge entity instead of any other entity increases the <b>LLM&rsquo;s</b> internal recall of the bridge entity. For the second hop, we test if increasing this recall causes the <b>LLM</b> to better utilize what it knows about the bridge entity. We find strong evidence of latent multi-hop <b>reasoning</b> for the <b>prompts</b> of certain relation types, with the <b>reasoning</b> pathway used in more than 80% of the <b>prompts.</b> However, the utilization is highly contextual, varying across different types of <b>prompts.</b> Also, on average, the evidence for the second hop and the full multi-hop traversal is rather moderate and only substantial for the first hop. Moreover, we find a clear scaling trend with increasing model size for the first hop of <b>reasoning</b> but not for the second hop. Our experimental findings suggest potential challenges and opportunities for future development and applications of <b>LLMs.</b></p></p class="citation"></blockquote><h3 id=3762--56279-set-the-clock-temporal-alignment-of-pretrained-language-models-bowen-zhao-et-al-2024>(37/62 | 56/279) Set the Clock: Temporal Alignment of Pretrained Language Models (Bowen Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bowen Zhao, Zander Brumbaugh, Yizhong Wang, Hannaneh Hajishirzi, Noah A. Smith. (2024)<br><strong>Set the Clock: Temporal Alignment of Pretrained Language Models</strong><br><button class=copy-to-clipboard title="Set the Clock: Temporal Alignment of Pretrained Language Models" index=56>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-56 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Fine-tuning, Grounding, Pre-trained Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16797v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16797v1.pdf filename=2402.16797v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Language models (LMs) are trained on web text originating from many points in time and, in general, without any explicit temporal <b>grounding.</b> This work investigates the temporal chaos of <b>pretrained</b> <b>LMs</b> <b>and</b> explores various methods to align their internal knowledge to a target time, which we call &ldquo;temporal alignment.&rdquo; To do this, we first automatically construct a dataset containing 20K time-sensitive questions and their answers for each year from 2000 to 2023. Based on this dataset, we empirically show that <b>pretrained</b> <b>LMs</b> <b>(e.g.,</b> LLaMa2), despite having a recent pretraining cutoff (e.g., 2022), mostly answer questions using earlier knowledge (e.g., in 2019). We then develop several methods, from <b>prompting</b> to <b>finetuning,</b> to align LMs to use their most recent knowledge when answering questions, and investigate various factors in this alignment. Our experiments show that aligning LLaMa2 to the year 2022 can boost its performance by up to 62% relatively as measured by that year, even without mentioning time information explicitly, indicating the possibility of aligning models&rsquo; internal sense of time after pretraining. Finally, we find that alignment to a historical time is also possible, with up to 2.8$\times$ the performance of the unaligned LM in 2010 if <b>finetuning</b> models to that year. These findings hint at the sophistication of LMs&rsquo; internal knowledge organization and the necessity of tuning them properly.</p></p class="citation"></blockquote><h3 id=3862--57279-political-compass-or-spinning-arrow-towards-more-meaningful-evaluations-for-values-and-opinions-in-large-language-models-paul-röttger-et-al-2024>(38/62 | 57/279) Political Compass or Spinning Arrow? Towards More Meaningful Evaluations for Values and Opinions in Large Language Models (Paul Röttger et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Paul Röttger, Valentin Hofmann, Valentina Pyatkin, Musashi Hinck, Hannah Rose Kirk, Hinrich Schütze, Dirk Hovy. (2024)<br><strong>Political Compass or Spinning Arrow? Towards More Meaningful Evaluations for Values and Opinions in Large Language Models</strong><br><button class=copy-to-clipboard title="Political Compass or Spinning Arrow? Towards More Meaningful Evaluations for Values and Opinions in Large Language Models" index=57>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-57 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Knowledge Distillation, Recommendation, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16786v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16786v1.pdf filename=2402.16786v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Much recent work seeks to evaluate values and opinions in <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> using multiple-choice surveys and questionnaires. Most of this work is motivated by concerns around real-world <b>LLM</b> applications. For example, politically-biased <b>LLMs</b> may subtly influence society when they are used by millions of people. Such real-world concerns, however, stand in stark contrast to the artificiality of current evaluations: real users do not typically ask <b>LLMs</b> survey questions. Motivated by this discrepancy, we challenge the prevailing constrained evaluation paradigm for values and opinions in <b>LLMs</b> and explore more realistic unconstrained evaluations. As a case study, we focus on the popular Political Compass Test (PCT). In a systematic review, we find that most prior work using the PCT forces models to comply with the PCT&rsquo;s multiple-choice format. We show that models give substantively different answers when not forced; that answers change depending on how models are forced; and that answers lack paraphrase robustness. Then, we demonstrate that models give different answers yet again in a more realistic open-ended answer setting. We <b>distill</b> these findings into <b>recommendations</b> and open challenges in evaluating values and opinions in <b>LLMs.</b></p></p class="citation"></blockquote><h3 id=3962--58279-predicting-sustainable-development-goals-using-course-descriptions----from-llms-to-conventional-foundation-models-lev-kharlashkin-et-al-2024>(39/62 | 58/279) Predicting Sustainable Development Goals Using Course Descriptions &ndash; from LLMs to Conventional Foundation Models (Lev Kharlashkin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lev Kharlashkin, Melany Macias, Leo Huovinen, Mika Hämäläinen. (2024)<br><strong>Predicting Sustainable Development Goals Using Course Descriptions &ndash; from LLMs to Conventional Foundation Models</strong><br><button class=copy-to-clipboard title="Predicting Sustainable Development Goals Using Course Descriptions -- from LLMs to Conventional Foundation Models" index=58>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-58 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Foundation Model, BART, PaLM, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16420v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16420v1.pdf filename=2402.16420v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present our work on predicting United Nations sustainable development goals (SDG) for university courses. We use an <b>LLM</b> named <b>PaLM</b> 2 to generate training data given a noisy human-authored course description input as input. We use this data to train several different smaller language models to predict SDGs for university courses. This work contributes to better university level adaptation of SDGs. The best performing model in our experiments was <b>BART</b> with an F1-score of 0.786.</p></p class="citation"></blockquote><h3 id=4062--59279-immunization-against-harmful-fine-tuning-attacks-domenic-rosati-et-al-2024>(40/62 | 59/279) Immunization against harmful fine-tuning attacks (Domenic Rosati et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Domenic Rosati, Jan Wehner, Kai Williams, Łukasz Bartoszcze, Jan Batzner, Hassan Sajjad, Frank Rudzicz. (2024)<br><strong>Immunization against harmful fine-tuning attacks</strong><br><button class=copy-to-clipboard title="Immunization against harmful fine-tuning attacks" index=59>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-59 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Fine-tuning, Fine-tuning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16382v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16382v1.pdf filename=2402.16382v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Approaches to aligning <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> with human values has focused on correcting misalignment that emerges from pretraining. However, this focus overlooks another source of misalignment: bad actors might purposely <b>fine-tune</b> <b>LLMs</b> to achieve harmful goals. In this paper, we present an emerging threat model that has arisen from alignment circumvention and <b>fine-tuning</b> attacks. However, lacking in previous works is a clear presentation of the conditions for effective defence. We propose a set of conditions for effective defence against harmful <b>fine-tuning</b> in <b>LLMs</b> called &ldquo;Immunization conditions,&rdquo; which help us understand how we would construct and measure future defences. Using this formal framework for defence, we offer a synthesis of different research directions that might be persued to prevent harmful <b>fine-tuning</b> attacks and provide a demonstration of how to use these conditions experimentally showing early results of using an adversarial loss to immunize LLama2-7b-chat.</p></p class="citation"></blockquote><h3 id=4162--60279-unraveling-babel-exploring-multilingual-activation-patterns-within-large-language-models-weize-liu-et-al-2024>(41/62 | 60/279) Unraveling Babel: Exploring Multilingual Activation Patterns within Large Language Models (Weize Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Weize Liu, Yinlong Xu, Hongxia Xu, Jintai Chen, Xuming Hu, Jian Wu. (2024)<br><strong>Unraveling Babel: Exploring Multilingual Activation Patterns within Large Language Models</strong><br><button class=copy-to-clipboard title="Unraveling Babel: Exploring Multilingual Activation Patterns within Large Language Models" index=60>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-60 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Model Pruning, Pruning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16367v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16367v1.pdf filename=2402.16367v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recently, <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have achieved tremendous breakthroughs in the field of language processing, yet their mechanisms in processing multiple languages remain agnostic. Therefore, in this work we study the multilingual activation patterns of <b>LLMs.</b> By transforming the original <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> into a Mixture of Experts (MoE) architecture, we analyze the expert activation patterns when processing various languages and demonstrate the connections of these activation patterns at the level of language families. We discover the existence of non-language-specific neurons as well as language-specific activation neurons. Further exploration even showcases that merely leveraging high-frequency activation neurons can accelerate inference while maintaining comparable performance. These findings shed light on the <b>LLMs&rsquo;</b> multilingual processing mechanism, and are of significant importance in guiding the multilingual training and <b>model</b> <b>pruning</b> of <b>LLMs.</b></p></p class="citation"></blockquote><h3 id=4262--61279-chain-of-discussion-a-multi-model-framework-for-complex-evidence-based-question-answering-mingxu-tao-et-al-2024>(42/62 | 61/279) Chain-of-Discussion: A Multi-Model Framework for Complex Evidence-Based Question Answering (Mingxu Tao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mingxu Tao, Dongyan Zhao, Yansong Feng. (2024)<br><strong>Chain-of-Discussion: A Multi-Model Framework for Complex Evidence-Based Question Answering</strong><br><button class=copy-to-clipboard title="Chain-of-Discussion: A Multi-Model Framework for Complex Evidence-Based Question Answering" index=61>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-61 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Question Answering, Question Answering, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16313v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16313v1.pdf filename=2402.16313v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Open-ended <b>question</b> <b>answering</b> requires models to find appropriate evidence to form well-reasoned, comprehensive and helpful answers. In practical applications, models also need to engage in extended discussions on potential scenarios closely relevant to the <b>question.</b> <b>With</b> augmentation of retrieval module, open-source <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> can produce coherent answers often with different focuses, but are still sub-optimal in terms of reliable evidence selection and in-depth <b>question</b> <b>analysis.</b> In this paper, we propose a novel Chain-of-Discussion framework to leverage the synergy among multiple open-source <b>LLMs</b> aiming to provide \textbf{more correct} and \textbf{more comprehensive} answers for open-ended <b>QA,</b> although they are not strong enough individually. Our experiments show that discussions among multiple <b>LLMs</b> play a vital role in enhancing the quality of answers. We release our data and code at \url{https://github.com/kobayashikanna01/Chain-of-Discussion}.</p></p class="citation"></blockquote><h3 id=4362--62279-towards-explainability-and-fairness-in-swiss-judgement-prediction-benchmarking-on-a-multilingual-dataset-santosh-t-y-s-s-et-al-2024>(43/62 | 62/279) Towards Explainability and Fairness in Swiss Judgement Prediction: Benchmarking on a Multilingual Dataset (Santosh T. Y. S. S et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Santosh T. Y. S. S, Nina Baumgartner, Matthias Stürmer, Matthias Grabmair, Joel Niklaus. (2024)<br><strong>Towards Explainability and Fairness in Swiss Judgement Prediction: Benchmarking on a Multilingual Dataset</strong><br><button class=copy-to-clipboard title="Towards Explainability and Fairness in Swiss Judgement Prediction: Benchmarking on a Multilingual Dataset" index=62>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-62 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: 68T50, I-2, cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 36<br>Keywords: Benchmarking, Benchmarking, Data Augmentation, Fairness, BERT<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17013v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17013v1.pdf filename=2402.17013v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The assessment of explainability in Legal Judgement Prediction (LJP) systems is of paramount importance in building trustworthy and transparent systems, particularly considering the reliance of these systems on factors that may lack legal relevance or involve sensitive attributes. This study delves into the realm of explainability and <b>fairness</b> in LJP models, utilizing Swiss Judgement Prediction (SJP), the only available multilingual LJP dataset. We curate a comprehensive collection of rationales that <code>support' and </code>oppose&rsquo; judgement from legal experts for 108 cases in German, French, and Italian. By employing an occlusion-based explainability approach, we evaluate the explainability performance of state-of-the-art monolingual and multilingual <b>BERT-based</b> LJP models, as well as models developed with techniques such as <b>data</b> <b>augmentation</b> and cross-lingual transfer, which demonstrated prediction performance improvement. Notably, our findings reveal that improved prediction performance does not necessarily correspond to enhanced explainability performance, underscoring the significance of evaluating models from an explainability perspective. Additionally, we introduce a novel evaluation framework, Lower Court Insertion (LCI), which allows us to quantify the influence of lower court information on model predictions, exposing current models&rsquo; biases.</p></p class="citation"></blockquote><h3 id=4462--63279-llmarena-assessing-capabilities-of-large-language-models-in-dynamic-multi-agent-environments-junzhe-chen-et-al-2024>(44/62 | 63/279) LLMArena: Assessing Capabilities of Large Language Models in Dynamic Multi-Agent Environments (Junzhe Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Junzhe Chen, Xuming Hu, Shuodi Liu, Shiyu Huang, Wei-Wei Tu, Zhaofeng He, Lijie Wen. (2024)<br><strong>LLMArena: Assessing Capabilities of Large Language Models in Dynamic Multi-Agent Environments</strong><br><button class=copy-to-clipboard title="LLMArena: Assessing Capabilities of Large Language Models in Dynamic Multi-Agent Environments" index=63>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-63 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 33<br>Keywords: Benchmarking, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16499v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16499v1.pdf filename=2402.16499v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advancements in <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have revealed their potential for achieving autonomous agents possessing human-level intelligence. However, existing <b>benchmarks</b> for evaluating <b>LLM</b> Agents either use static datasets, potentially leading to data leakage or focus only on single-agent scenarios, overlooking the complexities of multi-agent interactions. There is a lack of a <b>benchmark</b> that evaluates the diverse capabilities of <b>LLM</b> agents in multi-agent, dynamic environments. To this end, we introduce LLMArena, a novel and easily extensible framework for evaluating the diverse capabilities of <b>LLM</b> in multi-agent dynamic environments. LLMArena encompasses seven distinct gaming environments, employing Trueskill scoring to assess crucial abilities in <b>LLM</b> agents, including spatial <b>reasoning,</b> strategic planning, numerical <b>reasoning,</b> risk assessment, communication, opponent modeling, and team collaboration. We conduct an extensive experiment and human evaluation among different sizes and types of <b>LLMs,</b> showing that <b>LLMs</b> still have a significant journey ahead in their development towards becoming fully autonomous agents, especially in opponent modeling and team collaboration. We hope LLMArena could guide future research towards enhancing these capabilities in <b>LLMs,</b> ultimately leading to more sophisticated and practical applications in dynamic, multi-agent settings. The code and data will be available.</p></p class="citation"></blockquote><h3 id=4562--64279-can-large-language-models-recall-reference-location-like-humans-ye-wang-et-al-2024>(45/62 | 64/279) Can Large Language Models Recall Reference Location Like Humans? (Ye Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ye Wang, Xinrun Xu, Rui Xie, Wenxin Hu, Wei Ye. (2024)<br><strong>Can Large Language Models Recall Reference Location Like Humans?</strong><br><button class=copy-to-clipboard title="Can Large Language Models Recall Reference Location Like Humans?" index=64>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-64 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17010v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17010v1.pdf filename=2402.17010v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>When completing knowledge-intensive tasks, humans sometimes need not just an answer but also a corresponding reference passage for auxiliary reading. Previous methods required obtaining pre-segmented article chunks through additional retrieval models. This paper explores leveraging the parameterized knowledge stored during the pre-training phase of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> to independently recall reference passage from any starting position. We propose a two-stage framework that simulates the scenario of humans recalling easily forgotten references. Initially, the <b>LLM</b> is <b>prompted</b> to recall document title identifiers to obtain a coarse-grained document set. Then, based on the acquired coarse-grained document set, it recalls fine-grained passage. In the two-stage recall process, we use constrained decoding to ensure that content outside of the stored documents is not generated. To increase speed, we only recall a short prefix in the second stage, then locate its position to retrieve a complete passage. Experiments on KILT knowledge-sensitive tasks have verified that <b>LLMs</b> can independently recall reference passage location in various task forms, and the obtained reference significantly assist downstream tasks.</p></p class="citation"></blockquote><h3 id=4662--65279-eight-methods-to-evaluate-robust-unlearning-in-llms-aengus-lynch-et-al-2024>(46/62 | 65/279) Eight Methods to Evaluate Robust Unlearning in LLMs (Aengus Lynch et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Aengus Lynch, Phillip Guo, Aidan Ewart, Stephen Casper, Dylan Hadfield-Menell. (2024)<br><strong>Eight Methods to Evaluate Robust Unlearning in LLMs</strong><br><button class=copy-to-clipboard title="Eight Methods to Evaluate Robust Unlearning in LLMs" index=65>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-65 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Machine Unlearning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16835v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16835v1.pdf filename=2402.16835v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Machine</b> <b>unlearning</b> can be useful for removing harmful capabilities and memorized text from <b>large</b> <b>language</b> <b>models</b> <b>(LLMs),</b> but there are not yet standardized methods for rigorously evaluating it. In this paper, we first survey techniques and limitations of existing unlearning evaluations. Second, we apply a comprehensive set of tests for the robustness and competitiveness of unlearning in the &ldquo;Who&rsquo;s Harry Potter&rdquo; (WHP) model from Eldan and Russinovich (2023). While WHP&rsquo;s unlearning generalizes well when evaluated with the &ldquo;Familiarity&rdquo; metric from Eldan and Russinovich, we find i) higher-than-baseline amounts of knowledge can reliably be extracted, ii) WHP performs on par with the original model on Harry Potter Q&amp;A tasks, iii) it represents latent knowledge comparably to the original model, and iv) there is collateral unlearning in related domains. Overall, our results highlight the importance of comprehensive unlearning evaluation that avoids ad-hoc metrics.</p></p class="citation"></blockquote><h3 id=4762--66279-adaptation-of-biomedical-and-clinical-pretrained-models-to-french-long-documents-a-comparative-study-adrien-bazoge-et-al-2024>(47/62 | 66/279) Adaptation of Biomedical and Clinical Pretrained Models to French Long Documents: A Comparative Study (Adrien Bazoge et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Adrien Bazoge, Emmanuel Morin, Beatrice Daille, Pierre-Antoine Gourraud. (2024)<br><strong>Adaptation of Biomedical and Clinical Pretrained Models to French Long Documents: A Comparative Study</strong><br><button class=copy-to-clipboard title="Adaptation of Biomedical and Clinical Pretrained Models to French Long Documents: A Comparative Study" index=66>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-66 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: BERT, Named Entity Recognition, Pre-trained Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16689v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16689v1.pdf filename=2402.16689v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recently, <b>pretrained</b> <b>language</b> <b>models</b> based on <b>BERT</b> have been introduced for the French biomedical domain. Although these models have achieved state-of-the-art results on biomedical and clinical NLP tasks, they are constrained by a limited input sequence length of 512 tokens, which poses challenges when applied to clinical notes. In this paper, we present a comparative study of three adaptation strategies for long-sequence models, leveraging the Longformer architecture. We conducted evaluations of these models on 16 downstream tasks spanning both biomedical and clinical domains. Our findings reveal that further pre-training an English clinical model with French biomedical texts can outperform both converting a French biomedical <b>BERT</b> to the Longformer architecture and pre-training a French biomedical Longformer from scratch. The results underscore that long-sequence French biomedical models improve performance across most downstream tasks regardless of sequence length, but <b>BERT</b> based models remain the most efficient for <b>named</b> <b>entity</b> <b>recognition</b> tasks.</p></p class="citation"></blockquote><h3 id=4862--67279-repoagent-an-llm-powered-open-source-framework-for-repository-level-code-documentation-generation-qinyu-luo-et-al-2024>(48/62 | 67/279) RepoAgent: An LLM-Powered Open-Source Framework for Repository-level Code Documentation Generation (Qinyu Luo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qinyu Luo, Yining Ye, Shihao Liang, Zhong Zhang, Yujia Qin, Yaxi Lu, Yesai Wu, Xin Cong, Yankai Lin, Yingli Zhang, Xiaoyin Che, Zhiyuan Liu, Maosong Sun. (2024)<br><strong>RepoAgent: An LLM-Powered Open-Source Framework for Repository-level Code Documentation Generation</strong><br><button class=copy-to-clipboard title="RepoAgent: An LLM-Powered Open-Source Framework for Repository-level Code Documentation Generation" index=67>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-67 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: I-2-7; F-2-2, cs-AI, cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Code Generation, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16667v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16667v1.pdf filename=2402.16667v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Generative models have demonstrated considerable potential in software engineering, particularly in tasks such as <b>code</b> <b>generation</b> and debugging. However, their utilization in the domain of <b>code</b> <b>documentation</b> generation remains underexplored. To this end, we introduce RepoAgent, a <b>large</b> <b>language</b> <b>model</b> powered open-source framework aimed at proactively generating, maintaining, and updating <b>code</b> <b>documentation.</b> Through both qualitative and quantitative evaluations, we have validated the effectiveness of our approach, showing that RepoAgent excels in generating high-quality repository-level documentation. The <b>code</b> <b>and</b> results are publicly accessible at <a href=https://github.com/OpenBMB/RepoAgent>https://github.com/OpenBMB/RepoAgent</a>.</p></p class="citation"></blockquote><h3 id=4962--68279-defending-llms-against-jailbreaking-attacks-via-backtranslation-yihan-wang-et-al-2024>(49/62 | 68/279) Defending LLMs against Jailbreaking Attacks via Backtranslation (Yihan Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yihan Wang, Zhouxing Shi, Andrew Bai, Cho-Jui Hsieh. (2024)<br><strong>Defending LLMs against Jailbreaking Attacks via Backtranslation</strong><br><button class=copy-to-clipboard title="Defending LLMs against Jailbreaking Attacks via Backtranslation" index=68>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-68 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16459v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16459v2.pdf filename=2402.16459v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Although many <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have been trained to refuse harmful requests, they are still vulnerable to jailbreaking attacks, which rewrite the original <b>prompt</b> to conceal its harmful intent. In this paper, we propose a new method for defending <b>LLMs</b> against jailbreaking attacks by ``backtranslation&rsquo;&rsquo;. Specifically, given an initial response generated by the target <b>LLM</b> from an input <b>prompt,</b> our backtranslation <b>prompts</b> a language model to infer an input <b>prompt</b> that can lead to the response. The inferred <b>prompt</b> is called the backtranslated <b>prompt</b> which tends to reveal the actual intent of the original <b>prompt,</b> since it is generated based on the <b>LLM&rsquo;s</b> response and is not directly manipulated by the attacker. We then run the target <b>LLM</b> again on the backtranslated <b>prompt,</b> and we refuse the original <b>prompt</b> if the model refuses the backtranslated <b>prompt.</b> We explain that the proposed defense provides several benefits on its effectiveness and efficiency. We empirically demonstrate that our defense significantly outperforms the baselines, in the cases that are hard for the baselines, and our defense also has little impact on the generation quality for benign input <b>prompts.</b></p></p class="citation"></blockquote><h3 id=5062--69279-id-xcb-data-independent-debiasing-for-fair-and-accurate-transformer-based-cyberbullying-detection-peiling-yi-et-al-2024>(50/62 | 69/279) ID-XCB: Data-independent Debiasing for Fair and Accurate Transformer-based Cyberbullying Detection (Peiling Yi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Peiling Yi, Arkaitz Zubiaga. (2024)<br><strong>ID-XCB: Data-independent Debiasing for Fair and Accurate Transformer-based Cyberbullying Detection</strong><br><button class=copy-to-clipboard title="ID-XCB: Data-independent Debiasing for Fair and Accurate Transformer-based Cyberbullying Detection" index=69>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-69 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Adversarial Learning, Fine-tuning, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16458v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16458v2.pdf filename=2402.16458v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Swear words are a common proxy to collect datasets with cyberbullying incidents. Our focus is on measuring and mitigating biases derived from spurious associations between swear words and incidents occurring as a result of such data collection strategies. After demonstrating and quantifying these biases, we introduce ID-XCB, the first data-independent debiasing technique that combines <b>adversarial</b> <b>training,</b> bias constraints and debias <b>fine-tuning</b> approach aimed at alleviating model attention to bias-inducing words without impacting overall model performance. We explore ID-XCB on two popular session-based cyberbullying datasets along with comprehensive ablation and generalisation studies. We show that ID-XCB learns robust cyberbullying detection capabilities while mitigating biases, outperforming state-of-the-art debiasing methods in both performance and bias mitigation. Our quantitative and qualitative analyses demonstrate its generalisability to unseen data.</p></p class="citation"></blockquote><h3 id=5162--70279-multi-task-contrastive-learning-for-8192-token-bilingual-text-embeddings-isabelle-mohr-et-al-2024>(51/62 | 70/279) Multi-Task Contrastive Learning for 8192-Token Bilingual Text Embeddings (Isabelle Mohr et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Isabelle Mohr, Markus Krimmel, Saba Sturua, Mohammad Kalim Akram, Andreas Koukounas, Michael Günther, Georgios Mastrapas, Vinit Ravishankar, Joan Fontanals Martínez, Feng Wang, Qi Liu, Ziniu Yu, Jie Fu, Saahil Ognawala, Susana Guzman, Bo Wang, Maximilian Werk, Nan Wang, Han Xiao. (2024)<br><strong>Multi-Task Contrastive Learning for 8192-Token Bilingual Text Embeddings</strong><br><button class=copy-to-clipboard title="Multi-Task Contrastive Learning for 8192-Token Bilingual Text Embeddings" index=70>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-70 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: 68T50, I-2-7, cs-AI, cs-CL, cs-IR, cs.CL<br>Keyword Score: 26<br>Keywords: Benchmarking, Clustering, Contrastive Learning, Text Embedding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17016v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17016v1.pdf filename=2402.17016v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce a novel suite of state-of-the-art bilingual <b>text</b> <b>embedding</b> models that are designed to support English and another target language. These models are capable of processing lengthy <b>text</b> <b>inputs</b> with up to 8192 tokens, making them highly versatile for a range of natural language processing tasks such as <b>text</b> <b>retrieval,</b> <b>clustering,</b> and semantic textual similarity (STS) calculations. By focusing on bilingual models and introducing a unique multi-task learning objective, we have significantly improved the model performance on STS tasks, which outperforms the capabilities of existing multilingual models in both target language understanding and cross-lingual evaluation tasks. Moreover, our bilingual models are more efficient, requiring fewer parameters and less memory due to their smaller vocabulary needs. Furthermore, we have expanded the Massive <b>Text</b> <b>Embedding</b> <b>Benchmark</b> (MTEB) to include <b>benchmarks</b> for German and Spanish embedding models. This integration aims to stimulate further research and advancement in <b>text</b> <b>embedding</b> technologies for these languages.</p></p class="citation"></blockquote><h3 id=5262--71279-where-do-we-go-from-here-multi-scale-allocentric-relational-inference-from-natural-spatial-descriptions-tzuf-paz-argaman-et-al-2024>(52/62 | 71/279) Where Do We Go from Here? Multi-scale Allocentric Relational Inference from Natural Spatial Descriptions (Tzuf Paz-Argaman et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tzuf Paz-Argaman, Sayali Kulkarni, John Palowitch, Jason Baldridge, Reut Tsarfaty. (2024)<br><strong>Where Do We Go from Here? Multi-scale Allocentric Relational Inference from Natural Spatial Descriptions</strong><br><button class=copy-to-clipboard title="Where Do We Go from Here? Multi-scale Allocentric Relational Inference from Natural Spatial Descriptions" index=71>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-71 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs-MM, cs.CL<br>Keyword Score: 23<br>Keywords: Benchmarking, Information Retrieval, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16364v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16364v1.pdf filename=2402.16364v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>When communicating routes in natural language, the concept of {\em acquired spatial knowledge} is crucial for geographic <b>information</b> <b>retrieval</b> (GIR) and in spatial cognitive research. However, NLP navigation studies often overlook the impact of such acquired knowledge on textual descriptions. Current navigation studies concentrate on egocentric local descriptions (e.g., <code>it will be on your right') that require &lt;b>reasoning&lt;/b> over the agent's local perception. These instructions are typically given as a sequence of steps, with each action-step explicitly mentioning and being followed by a landmark that the agent can use to verify they are on the right path (e.g., </code>turn right and then you will see&mldr;&rsquo;). In contrast, descriptions based on knowledge acquired through a map provide a complete view of the environment and capture its overall structure. These instructions (e.g., `it is south of Central Park and a block north of a police station&rsquo;) are typically non-sequential, contain allocentric relations, with multiple spatial relations and implicit actions, without any explicit verification. This paper introduces the Rendezvous (RVS) task and dataset, which includes 10,404 examples of English geospatial instructions for reaching a target location using map-knowledge. Our analysis reveals that RVS exhibits a richer use of spatial allocentric relations, and requires resolving more spatial relations simultaneously compared to previous text-based navigation <b>benchmarks.</b></p></p class="citation"></blockquote><h3 id=5362--72279-leveraging-large-language-models-for-learning-complex-legal-concepts-through-storytelling-hang-jiang-et-al-2024>(53/62 | 72/279) Leveraging Large Language Models for Learning Complex Legal Concepts through Storytelling (Hang Jiang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hang Jiang, Xiajie Zhang, Robert Mahari, Daniel Kessler, Eric Ma, Tal August, Irene Li, Alex &lsquo;Sandy&rsquo; Pentland, Yoon Kim, Jad Kabbara, Deb Roy. (2024)<br><strong>Leveraging Large Language Models for Learning Complex Legal Concepts through Storytelling</strong><br><button class=copy-to-clipboard title="Leveraging Large Language Models for Learning Complex Legal Concepts through Storytelling" index=72>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-72 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-HC, cs.CL<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17019v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17019v1.pdf filename=2402.17019v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Making legal knowledge accessible to non-experts is crucial for enhancing general legal literacy and encouraging civic participation in democracy. However, legal documents are often challenging to understand for people without legal backgrounds. In this paper, we present a novel application of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> in legal education to help non-experts learn intricate legal concepts through storytelling, an effective pedagogical tool in conveying complex and abstract concepts. We also introduce a new dataset LegalStories, which consists of 295 complex legal doctrines, each accompanied by a story and a set of multiple-choice questions generated by <b>LLMs.</b> To construct the dataset, we experiment with various <b>LLMs</b> to generate legal stories explaining these concepts. Furthermore, we use an expert-in-the-loop method to iteratively design multiple-choice questions. Then, we evaluate the effectiveness of storytelling with <b>LLMs</b> through an RCT experiment with legal novices on 10 samples from the dataset. We find that <b>LLM-generated</b> stories enhance comprehension of legal concepts and interest in law among non-native speakers compared to only definitions. Moreover, stories consistently help participants relate legal concepts to their lives. Finally, we find that learning with stories shows a higher retention rate for non-native speakers in the follow-up assessment. Our work has strong implications for using <b>LLMs</b> in promoting teaching and learning in the legal field and beyond.</p></p class="citation"></blockquote><h3 id=5462--73279-a-survey-on-data-selection-for-language-models-alon-albalak-et-al-2024>(54/62 | 73/279) A Survey on Data Selection for Language Models (Alon Albalak et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Alon Albalak, Yanai Elazar, Sang Michael Xie, Shayne Longpre, Nathan Lambert, Xinyi Wang, Niklas Muennighoff, Bairu Hou, Liangming Pan, Haewon Jeong, Colin Raffel, Shiyu Chang, Tatsunori Hashimoto, William Yang Wang. (2024)<br><strong>A Survey on Data Selection for Language Models</strong><br><button class=copy-to-clipboard title="A Survey on Data Selection for Language Models" index=73>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-73 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keyword Score: 20<br>Keywords: Unsupervised Learning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16827v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16827v1.pdf filename=2402.16827v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A major factor in the recent success of <b>large</b> <b>language</b> <b>models</b> is the use of enormous and ever-growing text datasets for <b>unsupervised</b> pre-training. However, naively training a model on all available data may not be optimal (or feasible), as the quality of available text data can vary. Filtering out data can also decrease the carbon footprint and financial costs of training models by reducing the amount of training required. Data selection methods aim to determine which candidate data points to include in the training dataset and how to appropriately sample from the selected data points. The promise of improved data selection methods has caused the volume of research in the area to rapidly expand. However, because deep learning is mostly driven by empirical evidence and experimentation on <b>large-scale</b> <b>data</b> <b>is</b> expensive, few organizations have the resources for extensive data selection research. Consequently, knowledge of effective data selection practices has become concentrated within a few organizations, many of which do not openly share their findings and methodologies. To narrow this gap in knowledge, we present a comprehensive review of existing literature on data selection methods and related research areas, providing a taxonomy of existing approaches. By describing the current landscape of research, this work aims to accelerate progress in data selection by establishing an entry point for new and established researchers. Additionally, throughout this review we draw attention to noticeable holes in the literature and conclude the paper by proposing promising avenues for future research.</p></p class="citation"></blockquote><h3 id=5562--74279-multi-bit-distortion-free-watermarking-for-large-language-models-massieh-kordi-boroujeny-et-al-2024>(55/62 | 74/279) Multi-Bit Distortion-Free Watermarking for Large Language Models (Massieh Kordi Boroujeny et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Massieh Kordi Boroujeny, Ya Jiang, Kai Zeng, Brian Mark. (2024)<br><strong>Multi-Bit Distortion-Free Watermarking for Large Language Models</strong><br><button class=copy-to-clipboard title="Multi-Bit Distortion-Free Watermarking for Large Language Models" index=74>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-74 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keyword Score: 20<br>Keywords: Adversarial Detection, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16578v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16578v1.pdf filename=2402.16578v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Methods for watermarking <b>large</b> <b>language</b> <b>models</b> have been proposed that distinguish AI-generated text from human-generated text by slightly altering the model output distribution, but they also distort the quality of the text, exposing the watermark to <b>adversarial</b> <b>detection.</b> More recently, distortion-free watermarking methods were proposed that require a secret key to detect the watermark. The prior methods generally embed zero-bit watermarks that do not provide additional information beyond tagging a text as being AI-generated. We extend an existing zero-bit distortion-free watermarking method by embedding multiple bits of meta-information as part of the watermark. We also develop a computationally efficient decoder that extracts the embedded information from the watermark with low bit error rate.</p></p class="citation"></blockquote><h3 id=5662--75279-shieldlm-empowering-llms-as-aligned-customizable-and-explainable-safety-detectors-zhexin-zhang-et-al-2024>(56/62 | 75/279) ShieldLM: Empowering LLMs as Aligned, Customizable and Explainable Safety Detectors (Zhexin Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhexin Zhang, Yida Lu, Jingyuan Ma, Di Zhang, Rui Li, Pei Ke, Hao Sun, Lei Sha, Zhifang Sui, Hongning Wang, Minlie Huang. (2024)<br><strong>ShieldLM: Empowering LLMs as Aligned, Customizable and Explainable Safety Detectors</strong><br><button class=copy-to-clipboard title="ShieldLM: Empowering LLMs as Aligned, Customizable and Explainable Safety Detectors" index=75>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-75 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16444v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16444v1.pdf filename=2402.16444v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The safety of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> has gained increasing attention in recent years, but there still lacks a comprehensive approach for detecting safety issues within <b>LLMs&rsquo;</b> responses in an aligned, customizable and explainable manner. In this paper, we propose ShieldLM, an <b>LLM-based</b> safety detector, which aligns with general human safety standards, supports customizable detection rules, and provides explanations for its decisions. To train ShieldLM, we compile a <b>large</b> <b>bilingual</b> <b>dataset</b> comprising 14,387 query-response pairs, annotating the safety of responses based on various safety standards. Through extensive experiments, we demonstrate that ShieldLM surpasses strong baselines across four test sets, showcasing remarkable customizability and explainability. Besides performing well on standard detection datasets, ShieldLM has also been shown to be effective in real-world situations as a safety evaluator for advanced <b>LLMs.</b> We release ShieldLM at \url{https://github.com/thu-coai/ShieldLM} to support accurate and explainable safety detection under various safety standards, contributing to the ongoing efforts to enhance the safety of <b>LLMs.</b></p></p class="citation"></blockquote><h3 id=5762--76279-cross-domain-chinese-sentence-pattern-parsing-jingsi-yu-et-al-2024>(57/62 | 76/279) Cross-domain Chinese Sentence Pattern Parsing (Jingsi Yu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jingsi Yu, Cunliang Kong, Liner Yang, Meishan Zhang, Lin Zhu, Yujie Wang, Haozhe Lin, Maosong Sun, Erhong Yang. (2024)<br><strong>Cross-domain Chinese Sentence Pattern Parsing</strong><br><button class=copy-to-clipboard title="Cross-domain Chinese Sentence Pattern Parsing" index=76>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-76 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16311v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16311v2.pdf filename=2402.16311v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Sentence Pattern Structure (SPS) parsing is a syntactic analysis method primarily employed in language teaching.Existing SPS parsers rely heavily on textbook corpora for training, lacking cross-domain capability.To overcome this constraint, this paper proposes an innovative approach leveraging <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> within a self-training framework. Partial syntactic rules from a source domain are combined with target domain sentences to dynamically generate training data, enhancing the adaptability of the parser to diverse domains.Experiments conducted on textbook and news domains demonstrate the effectiveness of the proposed method, outperforming rule-based baselines by 1.68 points on F1 metrics.</p></p class="citation"></blockquote><h3 id=5862--77279-topic-to-essay-generation-with-knowledge-based-content-selection-jieyong-wang-et-al-2024>(58/62 | 77/279) Topic-to-essay generation with knowledge-based content selection (Jieyong Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jieyong Wang, Chunyao Song, Yihao Wu. (2024)<br><strong>Topic-to-essay generation with knowledge-based content selection</strong><br><button class=copy-to-clipboard title="Topic-to-essay generation with knowledge-based content selection" index=77>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-77 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Language Generation, Natural Language Generation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16248v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16248v1.pdf filename=2402.16248v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The topic-to-essay generation task is a challenging <b>natural</b> <b>language</b> <b>generation</b> task that aims to generate paragraph-level text with high semantic coherence based on a given set of topic words. Previous work has focused on the introduction of external knowledge, ignoring the insufficient generated text diversity. In order to improve the generation diversity, we propose a novel copy mechanism model with a content selection module that integrates rich semantic knowledge from the <b>language</b> <b>model</b> into the decoder. Furthermore, we introduce the improved prefix tuning method to train the model, enabling it to adapt to varying input complexities. In addition, we have contributed a new Chinese dataset for TEG tasks. Experimental results demonstrate that the proposed model can improve the generated text diversity by 35% to 59% compared to the state-of-the-art method, while maintaining a high level of topic consistency.</p></p class="citation"></blockquote><h3 id=5962--78279-long-dialog-summarization-an-analysis-ankan-mullick-et-al-2024>(59/62 | 78/279) Long Dialog Summarization: An Analysis (Ankan Mullick et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ankan Mullick, Ayan Kumar Bhowmick, Raghav R, Ravi Kokku, Prasenjit Dey, Pawan Goyal, Niloy Ganguly. (2024)<br><strong>Long Dialog Summarization: An Analysis</strong><br><button class=copy-to-clipboard title="Long Dialog Summarization: An Analysis" index=78>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-78 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-IR, cs.CL<br>Keyword Score: 13<br>Keywords: Benchmarking, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16986v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16986v1.pdf filename=2402.16986v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Dialog <b>summarization</b> has become increasingly important in managing and comprehending large-scale conversations across various domains. This task presents unique challenges in capturing the key points, context, and nuances of multi-turn long conversations for <b>summarization.</b> It is worth noting that the <b>summarization</b> techniques may vary based on specific requirements such as in a shopping-chatbot scenario, the dialog summary helps to learn user preferences, whereas in the case of a customer call center, the summary may involve the problem attributes that a user specified, and the final resolution provided. This work emphasizes the significance of creating coherent and contextually rich summaries for effective communication in various applications. We explore current state-of-the-art approaches for long dialog <b>summarization</b> in different domains and <b>benchmark</b> metrics based evaluations show that one single model does not perform well across various areas for distinct <b>summarization</b> tasks.</p></p class="citation"></blockquote><h3 id=6062--79279-paqa-toward-proactive-open-retrieval-question-answering-pierre-erbacher-et-al-2024>(60/62 | 79/279) PAQA: Toward ProActive Open-Retrieval Question Answering (Pierre Erbacher et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Pierre Erbacher, Jian-Yun Nie, Philippe Preux, Laure Soulier. (2024)<br><strong>PAQA: Toward ProActive Open-Retrieval Question Answering</strong><br><button class=copy-to-clipboard title="PAQA: Toward ProActive Open-Retrieval Question Answering" index=79>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-79 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-IR, cs.CL<br>Keyword Score: 10<br>Keywords: Question Answering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16608v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16608v1.pdf filename=2402.16608v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Conversational systems have made significant progress in generating natural language responses. However, their potential as conversational search systems is currently limited due to their passive role in the information-seeking process. One major limitation is the scarcity of datasets that provide labelled ambiguous <b>questions</b> <b>along</b> with a supporting corpus of documents and relevant clarifying <b>questions.</b> <b>This</b> work aims to tackle the challenge of generating relevant clarifying <b>questions</b> <b>by</b> taking into account the inherent ambiguities present in both user queries and documents. To achieve this, we propose PAQA, an extension to the existing AmbiNQ dataset, incorporating clarifying <b>questions.</b> <b>We</b> then evaluate various models and assess how passage retrieval impacts ambiguity detection and the generation of clarifying <b>questions.</b> <b>By</b> addressing this gap in conversational search systems, we aim to provide additional supervision to enhance their active participation in the information-seeking process and provide users with more accurate results.</p></p class="citation"></blockquote><h3 id=6162--80279-uniretriever-multi-task-candidates-selection-for-various-context-adaptive-conversational-retrieval-hongru-wang-et-al-2024>(61/62 | 80/279) UniRetriever: Multi-task Candidates Selection for Various Context-Adaptive Conversational Retrieval (Hongru Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hongru Wang, Boyang Xue, Baohang Zhou, Rui Wang, Fei Mi, Weichao Wang, Yasheng Wang, Kam-Fai Wong. (2024)<br><strong>UniRetriever: Multi-task Candidates Selection for Various Context-Adaptive Conversational Retrieval</strong><br><button class=copy-to-clipboard title="UniRetriever: Multi-task Candidates Selection for Various Context-Adaptive Conversational Retrieval" index=80>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-80 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-IR, cs.CL<br>Keyword Score: 10<br>Keywords: Information Retrieval<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16261v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16261v2.pdf filename=2402.16261v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Conversational retrieval refers to an <b>information</b> <b>retrieval</b> system that operates in an iterative and interactive manner, requiring the retrieval of various external resources, such as persona, knowledge, and even response, to effectively engage with the user and successfully complete the dialogue. However, most previous work trained independent retrievers for each specific resource, resulting in sub-optimal performance and low efficiency. Thus, we propose a multi-task framework function as a universal retriever for three dominant retrieval tasks during the conversation: persona selection, knowledge selection, and response selection. To this end, we design a dual-encoder architecture consisting of a context-adaptive dialogue encoder and a candidate encoder, aiming to attention to the relevant context from the long dialogue and retrieve suitable candidates by simply a dot product. Furthermore, we introduce two loss constraints to capture the subtle relationship between dialogue context and different candidates by regarding historically selected candidates as hard negatives. Extensive experiments and analysis establish state-of-the-art retrieval quality both within and outside its training domain, revealing the promising potential and generalization capability of our model to serve as a universal retriever for different candidate selection tasks simultaneously.</p></p class="citation"></blockquote><h3 id=6262--81279-diffucomet-contextual-commonsense-knowledge-diffusion-silin-gao-et-al-2024>(62/62 | 81/279) DiffuCOMET: Contextual Commonsense Knowledge Diffusion (Silin Gao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Silin Gao, Mete Ismayilzada, Mengjie Zhao, Hiromi Wakaki, Yuki Mitsufuji, Antoine Bosselut. (2024)<br><strong>DiffuCOMET: Contextual Commonsense Knowledge Diffusion</strong><br><button class=copy-to-clipboard title="DiffuCOMET: Contextual Commonsense Knowledge Diffusion" index=81>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-81 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17011v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17011v1.pdf filename=2402.17011v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Inferring contextually-relevant and diverse commonsense to understand narratives remains challenging for knowledge models. In this work, we develop a series of knowledge models, DiffuCOMET, that leverage diffusion to learn to reconstruct the implicit semantic connections between narrative contexts and relevant commonsense knowledge. Across multiple diffusion steps, our method progressively refines a representation of commonsense facts that is anchored to a narrative, producing contextually-relevant and diverse commonsense inferences for an input context. To evaluate DiffuCOMET, we introduce new metrics for commonsense inference that more closely measure knowledge diversity and contextual relevance. Our results on two different <b>benchmarks,</b> ComFact and WebNLG+, show that knowledge generated by DiffuCOMET achieves a better trade-off between commonsense diversity, contextual relevance and alignment to known gold references, compared to baseline knowledge models.</p></p class="citation"></blockquote><h2 id=eessas-3>eess.AS (3)</h2><h3 id=13--82279-skill-similarity-aware-knowledge-distillation-for-speech-self-supervised-learning-luca-zampierin-et-al-2024>(1/3 | 82/279) SKILL: Similarity-aware Knowledge distILLation for Speech Self-Supervised Learning (Luca Zampierin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Luca Zampierin, Ghouthi Boukli Hacene, Bac Nguyen, Mirco Ravanelli. (2024)<br><strong>SKILL: Similarity-aware Knowledge distILLation for Speech Self-Supervised Learning</strong><br><button class=copy-to-clipboard title="SKILL: Similarity-aware Knowledge distILLation for Speech Self-Supervised Learning" index=82>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-82 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.AS<br>Categories: cs-CL, cs-LG, cs-SD, eess-AS, eess.AS<br>Keyword Score: 83<br>Keywords: Clustering, Hierarchical Clustering, Knowledge Distillation, Knowledge Distillation, Knowledge Distillation, Knowledge Distillation, Pruning, Self-supervised Learning, Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16830v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16830v1.pdf filename=2402.16830v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Self-supervised</b> <b>learning</b> (SSL) has achieved remarkable success across various speech-processing tasks. To enhance its efficiency, previous works often leverage the use of compression techniques. A notable recent attempt is DPHuBERT, which applies joint <b>knowledge</b> <b>distillation</b> <b>(KD)</b> and structured <b>pruning</b> to learn a significantly smaller SSL model. In this paper, we contribute to this research domain by introducing SKILL, a novel method that conducts <b>distillation</b> across groups of layers instead of <b>distilling</b> individual arbitrarily selected layers within the teacher network. The identification of the layers to <b>distill</b> is achieved through a <b>hierarchical</b> <b>clustering</b> procedure applied to layer similarity measures. Extensive experiments demonstrate that our <b>distilled</b> version of WavLM Base+ not only outperforms DPHuBERT but also achieves state-of-the-art results in the 30M parameters model class across several SUPERB tasks.</p></p class="citation"></blockquote><h3 id=23--83279-an-automated-end-to-end-open-source-software-for-high-quality-text-to-speech-dataset-generation-ahmet-gunduz-et-al-2024>(2/3 | 83/279) An Automated End-to-End Open-Source Software for High-Quality Text-to-Speech Dataset Generation (Ahmet Gunduz et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ahmet Gunduz, Kamer Ali Yuksel, Kareem Darwish, Golara Javadi, Fabio Minazzi, Nicola Sobieski, Sebastien Bratieres. (2024)<br><strong>An Automated End-to-End Open-Source Software for High-Quality Text-to-Speech Dataset Generation</strong><br><button class=copy-to-clipboard title="An Automated End-to-End Open-Source Software for High-Quality Text-to-Speech Dataset Generation" index=83>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-83 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.AS<br>Categories: cs-AI, cs-CL, cs-LG, eess-AS, eess.AS<br>Keyword Score: 30<br>Keywords: human-in-the-loop, Text-to-speech, Text-to-speech<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16380v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16380v1.pdf filename=2402.16380v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Data availability is crucial for advancing artificial intelligence applications, including voice-based technologies. As content creation, particularly in social media, experiences increasing demand, translation and <b>text-to-speech</b> <b>(TTS)</b> technologies have become essential tools. Notably, the performance of these <b>TTS</b> technologies is highly dependent on the quality of the training data, emphasizing the mutual dependence of data availability and technological progress. This paper introduces an end-to-end tool to generate high-quality datasets for <b>text-to-speech</b> <b>(TTS)</b> models to address this critical need for high-quality data. The contributions of this work are manifold and include: the integration of language-specific phoneme distribution into sample selection, automation of the recording process, automated and <b>human-in-the-loop</b> quality assurance of recordings, and processing of recordings to meet specified formats. The proposed application aims to streamline the dataset creation process for <b>TTS</b> models through these features, thereby facilitating advancements in voice-based technologies.</p></p class="citation"></blockquote><h3 id=33--84279-audio-visual-speech-enhancement-in-noisy-environments-via-emotion-based-contextual-cues-tassadaq-hussain-et-al-2024>(3/3 | 84/279) Audio-Visual Speech Enhancement in Noisy Environments via Emotion-Based Contextual Cues (Tassadaq Hussain et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tassadaq Hussain, Kia Dashtipour, Yu Tsao, Amir Hussain. (2024)<br><strong>Audio-Visual Speech Enhancement in Noisy Environments via Emotion-Based Contextual Cues</strong><br><button class=copy-to-clipboard title="Audio-Visual Speech Enhancement in Noisy Environments via Emotion-Based Contextual Cues" index=84>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-84 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.AS<br>Categories: cs-SD, eess-AS, eess.AS<br>Keyword Score: 6<br>Keywords: Multi-modal, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16394v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16394v1.pdf filename=2402.16394v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In real-world environments, background noise significantly degrades the intelligibility and clarity of human speech. Audio-visual speech enhancement (AVSE) attempts to restore speech quality, but existing methods often fall short, particularly in dynamic noise conditions. This study investigates the inclusion of emotion as a novel contextual cue within AVSE, hypothesizing that incorporating emotional understanding can improve speech enhancement performance. We propose a novel emotion-aware AVSE system that leverages both auditory and visual information. It extracts emotional features from the facial landmarks of the speaker and fuses them with corresponding audio and visual modalities. This enriched data serves as input to a deep UNet-based encoder-decoder network, specifically designed to orchestrate the fusion of <b>multimodal</b> information enhanced with emotion. The network iteratively refines the enhanced speech representation through an encoder-decoder architecture, guided by perceptually-inspired loss functions for joint learning and optimization. We train and evaluate the model on the CMU <b>Multimodal</b> Opinion Sentiment and Emotion Intensity (CMU-MOSEI) dataset, a rich repository of audio-visual recordings with annotated emotions. Our comprehensive evaluation demonstrates the effectiveness of emotion as a contextual cue for AVSE. By integrating emotional features, the proposed system achieves significant improvements in both objective and subjective assessments of speech quality and intelligibility, especially in challenging noise environments. Compared to baseline AVSE and audio-only speech enhancement systems, our approach exhibits a noticeable increase in PESQ and STOI, indicating higher perceptual quality and intelligibility. Large-scale listening tests corroborate these findings, suggesting improved human understanding of enhanced speech.</p></p class="citation"></blockquote><h2 id=cslg-51>cs.LG (51)</h2><h3 id=151--85279-gistembed-guided-in-sample-selection-of-training-negatives-for-text-embedding-fine-tuning-aivin-v-solatorio-2024>(1/51 | 85/279) GISTEmbed: Guided In-sample Selection of Training Negatives for Text Embedding Fine-tuning (Aivin V. Solatorio, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Aivin V. Solatorio. (2024)<br><strong>GISTEmbed: Guided In-sample Selection of Training Negatives for Text Embedding Fine-tuning</strong><br><button class=copy-to-clipboard title="GISTEmbed: Guided In-sample Selection of Training Negatives for Text Embedding Fine-tuning" index=85>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-85 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CL, cs-LG, cs.LG<br>Keyword Score: 83<br>Keywords: Benchmarking, Fine-tuning, Recommendation, Retrieval-Augmented Generation, Retrieval-Augmented Generation, Unsupervised Learning, Large Language Model, Prompt, Text Embedding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16829v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16829v1.pdf filename=2402.16829v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Embedding models are integral to AI applications like semantic search, personalized <b>recommendations,</b> and <b>retrieval</b> <b>augmented</b> <b>generation</b> for <b>LLMs,</b> necessitating high-quality training data. However, the limited scalability of manual data curation <b>prompts</b> the need for automated methods to ensure data integrity. Traditional <b>unsupervised</b> triplet mining automates training data generation, crucial for embedding model training, yet inadvertently injects biases and noise, thereby degrading model performance. Addressing this, we introduce GISTEmbed, a novel strategy that enhances in-batch negative selection during contrastive training through a guide model. This approach departs from reliance on random sampling and equal utility assumption of batch negatives, significantly reducing noise from data quality issues and improving model <b>fine-tuning.</b> <b>Benchmarked</b> against the Massive <b>Text</b> <b>Embedding</b> <b>Benchmark</b> (MTEB), GISTEmbed showcases consistent performance improvements across various model sizes and achieves state-of-the-art results in select categories. This framework enables significant enhancements for smaller models by leveraging the capabilities of powerful yet resource-intensive large models. GISTEmbed can potentially revolutionize the creation of highly efficient, smaller models, democratizing access to advanced AI technologies. Making these technologies more accessible and cost-effective, especially for applications constrained by resources, significantly expands the impact and accessibility of state-of-the-art AI solutions across diverse sectors.</p></p class="citation"></blockquote><h3 id=251--86279-think-big-generate-quick-llm-to-slm-for-fast-autoregressive-decoding-benjamin-bergner-et-al-2024>(2/51 | 86/279) Think Big, Generate Quick: LLM-to-SLM for Fast Autoregressive Decoding (Benjamin Bergner et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Benjamin Bergner, Andrii Skliar, Amelie Royer, Tijmen Blankevoort, Yuki Asano, Babak Ehteshami Bejnordi. (2024)<br><strong>Think Big, Generate Quick: LLM-to-SLM for Fast Autoregressive Decoding</strong><br><button class=copy-to-clipboard title="Think Big, Generate Quick: LLM-to-SLM for Fast Autoregressive Decoding" index=86>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-86 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CL, cs-LG, cs.LG<br>Keyword Score: 63<br>Keywords: Benchmarking, Fine-tuning, Instruction Following, Large Language Model, Large Language Model, Prompt, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16844v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16844v1.pdf filename=2402.16844v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have become ubiquitous in practice and are widely used for generation tasks such as translation, <b>summarization</b> and <b>instruction</b> <b>following.</b> However, their enormous size and reliance on autoregressive decoding increase deployment costs and complicate their use in latency-critical applications. In this work, we propose a hybrid approach that combines language models of different sizes to increase the efficiency of autoregressive decoding while maintaining high performance. Our method utilizes a pretrained frozen <b>LLM</b> that encodes all <b>prompt</b> tokens once in parallel, and uses the resulting representations to condition and guide a small language model (SLM), which then generates the response more efficiently. We investigate the combination of encoder-decoder <b>LLMs</b> with both encoder-decoder and decoder-only SLMs from different model families and only require <b>fine-tuning</b> of the SLM. Experiments with various <b>benchmarks</b> show substantial speedups of up to $4\times$, with minor performance penalties of $1-2%$ for translation and <b>summarization</b> tasks compared to the <b>LLM.</b></p></p class="citation"></blockquote><h3 id=351--87279-asymmetry-in-low-rank-adapters-of-foundation-models-jiacheng-zhu-et-al-2024>(3/51 | 87/279) Asymmetry in Low-Rank Adapters of Foundation Models (Jiacheng Zhu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiacheng Zhu, Kristjan Greenewald, Kimia Nadjahi, Haitz Sáez de Ocáriz Borde, Rickard Brüel Gabrielsson, Leshem Choshen, Marzyeh Ghassemi, Mikhail Yurochkin, Justin Solomon. (2024)<br><strong>Asymmetry in Low-Rank Adapters of Foundation Models</strong><br><button class=copy-to-clipboard title="Asymmetry in Low-Rank Adapters of Foundation Models" index=87>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-87 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 60<br>Keywords: Fine-tuning, Fine-tuning, Foundation Model, BART, LLaMA, RoBERTa<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16842v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16842v2.pdf filename=2402.16842v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Parameter-efficient <b>fine-tuning</b> optimizes large, pre-trained <b>foundation</b> <b>models</b> by updating a subset of parameters; in this class, Low-Rank Adaptation (LoRA) is particularly effective. Inspired by an effort to investigate the different roles of LoRA matrices during <b>fine-tuning,</b> this paper characterizes and leverages unexpected asymmetry in the importance of low-rank adapter matrices. Specifically, when updating the parameter matrices of a neural network by adding a product $BA$, we observe that the $B$ and $A$ matrices have distinct functions: $A$ extracts features from the input, while $B$ uses these features to create the desired output. Based on this observation, we demonstrate that <b>fine-tuning</b> $B$ is inherently more effective than <b>fine-tuning</b> $A$, and that a random untrained $A$ should perform nearly as well as a <b>fine-tuned</b> one. Using an information-theoretic lens, we also bound the generalization of low-rank adapters, showing that the parameter savings of exclusively training $B$ improves the bound. We support our conclusions with experiments on <b>RoBERTa,</b> <b>BART-Large,</b> <b>LLaMA-2,</b> and ViTs.</p></p class="citation"></blockquote><h3 id=451--88279-graph-learning-under-distribution-shifts-a-comprehensive-survey-on-domain-adaptation-out-of-distribution-and-continual-learning-man-wu-et-al-2024>(4/51 | 88/279) Graph Learning under Distribution Shifts: A Comprehensive Survey on Domain Adaptation, Out-of-distribution, and Continual Learning (Man Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Man Wu, Xin Zheng, Qin Zhang, Xiao Shen, Xiong Luo, Xingquan Zhu, Shirui Pan. (2024)<br><strong>Graph Learning under Distribution Shifts: A Comprehensive Survey on Domain Adaptation, Out-of-distribution, and Continual Learning</strong><br><button class=copy-to-clipboard title="Graph Learning under Distribution Shifts: A Comprehensive Survey on Domain Adaptation, Out-of-distribution, and Continual Learning" index=88>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-88 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs-SI, cs.LG<br>Keyword Score: 53<br>Keywords: Graph, Continual Learning, Distribution Shift, Distribution Shift, Out-of-distribution, Recommendation, Domain Adaptation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16374v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16374v1.pdf filename=2402.16374v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Graph</b> learning plays a pivotal role and has gained significant attention in various application scenarios, from social network analysis to <b>recommendation</b> systems, for its effectiveness in modeling complex data relations represented by <b>graph</b> structural data. In reality, the real-world <b>graph</b> data typically show dynamics over time, with changing node attributes and edge structure, leading to the severe <b>graph</b> data <b>distribution</b> <b>shift</b> issue. This issue is compounded by the diverse and complex nature of <b>distribution</b> <b>shifts,</b> which can significantly impact the performance of <b>graph</b> learning methods in degraded generalization and adaptation capabilities, posing a substantial challenge to their effectiveness. In this survey, we provide a comprehensive review and summary of the latest approaches, strategies, and insights that address <b>distribution</b> <b>shifts</b> within the context of <b>graph</b> learning. Concretely, according to the observability of <b>distributions</b> <b>in</b> the inference stage and the availability of sufficient supervision information in the training stage, we categorize existing <b>graph</b> learning methods into several essential scenarios, including <b>graph</b> <b>domain</b> <b>adaptation</b> learning, <b>graph</b> <b>out-of-distribution</b> learning, and <b>graph</b> <b>continual</b> <b>learning.</b> For each scenario, a detailed taxonomy is proposed, with specific descriptions and discussions of existing progress made in <b>distribution-shifted</b> <b>graph</b> learning. Additionally, we discuss the potential applications and future directions for <b>graph</b> learning under <b>distribution</b> <b>shifts</b> with a systematic analysis of the current state in this field. The survey is positioned to provide general guidance for the development of effective <b>graph</b> learning algorithms in handling <b>graph</b> <b>distribution</b> <b>shifts,</b> and to stimulate future research and advancements in this area.</p></p class="citation"></blockquote><h3 id=551--89279-generative-pretrained-hierarchical-transformer-for-time-series-forecasting-zhiding-liu-et-al-2024>(5/51 | 89/279) Generative Pretrained Hierarchical Transformer for Time Series Forecasting (Zhiding Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhiding Liu, Jiqian Yang, Mingyue Cheng, Yucong Luo, Zhi Li. (2024)<br><strong>Generative Pretrained Hierarchical Transformer for Time Series Forecasting</strong><br><button class=copy-to-clipboard title="Generative Pretrained Hierarchical Transformer for Time Series Forecasting" index=89>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-89 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 50<br>Keywords: Fine-tuning, Self-supervised Learning, Self-supervised Pre-training, Supervised Learning, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16516v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16516v1.pdf filename=2402.16516v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent efforts have been dedicated to enhancing time series forecasting accuracy by introducing advanced network architectures and <b>self-supervised</b> <b>pretraining</b> strategies. Nevertheless, existing approaches still exhibit two critical drawbacks. Firstly, these methods often rely on a single dataset for training, limiting the model&rsquo;s generalizability due to the restricted scale of the training data. Secondly, the one-step generation schema is widely followed, which necessitates a customized forecasting head and overlooks the temporal dependencies in the output series, and also leads to increased training costs under different horizon length settings. To address these issues, we propose a novel generative pretrained hierarchical <b>transformer</b> architecture for forecasting, named GPHT. There are two aspects of key designs in GPHT. On the one hand, we advocate for constructing a mixed dataset for pretraining our model, comprising various datasets from diverse data scenarios. This approach significantly expands the scale of training data, allowing our model to uncover commonalities in time series data and facilitating improved transfer to specific datasets. On the other hand, GPHT employs an auto-regressive forecasting approach under the channel-independent assumption, effectively modeling temporal dependencies in the output series. Importantly, no customized forecasting head is required, enabling a single model to forecast at arbitrary horizon settings. We conduct sufficient experiments on eight datasets with mainstream <b>self-supervised</b> <b>pretraining</b> models and <b>supervised</b> models. The results demonstrated that GPHT surpasses the baseline models across various <b>fine-tuning</b> and zero/few-shot learning settings in the traditional long-term forecasting task, providing support for verifying the feasibility of pretrained time series large models.</p></p class="citation"></blockquote><h3 id=651--90279-personalized-federated-instruction-tuning-via-neural-architecture-search-pengyu-zhang-et-al-2024>(6/51 | 90/279) Personalized Federated Instruction Tuning via Neural Architecture Search (Pengyu Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Pengyu Zhang, Yingbo Zhou, Ming Hu, Junxian Feng, Jiawen Weng, Mingsong Chen. (2024)<br><strong>Personalized Federated Instruction Tuning via Neural Architecture Search</strong><br><button class=copy-to-clipboard title="Personalized Federated Instruction Tuning via Neural Architecture Search" index=90>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-90 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 50<br>Keywords: Fine-tuning, Pruning, Instruction Tuning, Large Language Model, Perplexity<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16919v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16919v1.pdf filename=2402.16919v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Federated <b>Instruction</b> <b>Tuning</b> (FIT) has shown the ability to achieve collaborative model <b>instruction</b> <b>tuning</b> among massive data owners without sharing private data. However, it still faces two key challenges, i.e., data and resource heterogeneity. Due to the varying data distribution and preferences among data owners, FIT cannot adapt to the personalized data of individual owners. Moreover, clients with superior computational abilities are constrained since they need to maintain the same <b>fine-tuning</b> architecture as the weaker clients. To address these issues, we propose a novel Personalized Federated <b>Instruction</b> <b>Tuning</b> (PerFIT) framework based on architecture search. Specifically, PerFIT allows each client to search for a personalized architecture by expanding the trainable parameter space of the global model followed by <b>pruning</b> the parameters to the original state. This procedure allows personalized <b>instruction</b> <b>fine-tuning</b> within expanded parameter spaces, concurrently preserving the same number of trainable parameters. Furthermore, to release the abilities of heterogeneous computational resources and enhance the performance of personalization on local data, we exploit personalized parameter-wise aggregation. The evaluation with multiple <b>LLMs</b> non-IID scenarios demonstrates that compared to the state-of-the-art FIT methods, our approach can achieve up to a 23% decrease in <b>perplexity.</b></p></p class="citation"></blockquote><h3 id=751--91279-referee-can-play-an-alternative-approach-to-conditional-generation-via-model-inversion-xuantong-liu-et-al-2024>(7/51 | 91/279) Referee Can Play: An Alternative Approach to Conditional Generation via Model Inversion (Xuantong Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xuantong Liu, Tianyang Hu, Wenjia Wang, Kenji Kawaguchi, Yuan Yao. (2024)<br><strong>Referee Can Play: An Alternative Approach to Conditional Generation via Model Inversion</strong><br><button class=copy-to-clipboard title="Referee Can Play: An Alternative Approach to Conditional Generation via Model Inversion" index=91>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-91 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 50<br>Keywords: Knowledge Distillation, Probabilistic Model, Text2image, Text2image, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16305v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16305v1.pdf filename=2402.16305v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>As a dominant force in <b>text-to-image</b> generation tasks, Diffusion <b>Probabilistic</b> <b>Models</b> (DPMs) face a critical challenge in controllability, struggling to adhere strictly to complex, multi-faceted instructions. In this work, we aim to address this alignment challenge for conditional generation tasks. First, we provide an alternative view of state-of-the-art DPMs as a way of inverting advanced <b>Vision-Language</b> Models (VLMs). With this formulation, we naturally propose a training-free approach that bypasses the conventional sampling process associated with DPMs. By directly optimizing images with the supervision of discriminative VLMs, the proposed method can potentially achieve a better <b>text-image</b> alignment. As proof of concept, we demonstrate the pipeline with the pre-trained BLIP-2 model and identify several key designs for improved image generation. To further enhance the image fidelity, a Score <b>Distillation</b> Sampling module of Stable Diffusion is incorporated. By carefully balancing the two components during optimization, our method can produce high-quality images with near state-of-the-art performance on T2I-Compbench.</p></p class="citation"></blockquote><h3 id=851--92279-interrogate-learning-to-share-specialize-and-prune-representations-for-multi-task-learning-babak-ehteshami-bejnordi-et-al-2024>(8/51 | 92/279) InterroGate: Learning to Share, Specialize, and Prune Representations for Multi-task Learning (Babak Ehteshami Bejnordi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Babak Ehteshami Bejnordi, Gaurav Kumar, Amelie Royer, Christos Louizos, Tijmen Blankevoort, Mohsen Ghafoorian. (2024)<br><strong>InterroGate: Learning to Share, Specialize, and Prune Representations for Multi-task Learning</strong><br><button class=copy-to-clipboard title="InterroGate: Learning to Share, Specialize, and Prune Representations for Multi-task Learning" index=92>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-92 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 43<br>Keywords: Graph Attention Networks, Benchmarking, Convolution, Parameter Sharing, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16848v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16848v1.pdf filename=2402.16848v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Jointly learning multiple tasks with a unified model can improve accuracy and data efficiency, but it faces the challenge of task interference, where optimizing one task objective may inadvertently compromise the performance of another. A solution to mitigate this issue is to allocate task-specific <b>parameters,</b> <b>free</b> from interference, on top of shared features. However, manually designing such architectures is cumbersome, as practitioners need to balance between the overall performance across all tasks and the higher computational cost induced by the newly added <b>parameters.</b> <b>In</b> this work, we propose \textit{InterroGate}, a novel multi-task learning (MTL) architecture designed to mitigate task interference while optimizing inference computational efficiency. We employ a learnable <b>gating</b> mechanism to automatically balance the shared and task-specific representations while preserving the performance of all tasks. Crucially, the patterns of <b>parameter</b> <b>sharing</b> and specialization dynamically learned during training, become fixed at inference, resulting in a static, optimized MTL architecture. Through extensive empirical evaluations, we demonstrate SoTA results on three MTL <b>benchmarks</b> using <b>convolutional</b> as well as <b>transformer-based</b> backbones on CelebA, NYUD-v2, and PASCAL-Context.</p></p class="citation"></blockquote><h3 id=951--93279-training-neural-networks-from-scratch-with-parallel-low-rank-adapters-minyoung-huh-et-al-2024>(9/51 | 93/279) Training Neural Networks from Scratch with Parallel Low-Rank Adapters (Minyoung Huh et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Minyoung Huh, Brian Cheung, Jeremy Bernstein, Phillip Isola, Pulkit Agrawal. (2024)<br><strong>Training Neural Networks from Scratch with Parallel Low-Rank Adapters</strong><br><button class=copy-to-clipboard title="Training Neural Networks from Scratch with Parallel Low-Rank Adapters" index=93>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-93 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CV, cs-LG, cs.LG<br>Keyword Score: 40<br>Keywords: Vision Transformer, Fine-tuning, Transformer, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16828v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16828v1.pdf filename=2402.16828v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The scalability of deep learning models is fundamentally limited by computing resources, memory, and communication. Although methods like low-rank adaptation (LoRA) have reduced the cost of model <b>finetuning,</b> its application in model pre-training remains largely unexplored. This paper explores extending LoRA to model pre-training, identifying the inherent constraints and limitations of standard LoRA in this context. We introduce LoRA-the-Explorer (LTE), a novel bi-level optimization algorithm designed to enable parallel training of multiple low-rank heads across computing nodes, thereby reducing the need for frequent synchronization. Our approach includes extensive experimentation on <b>vision</b> <b>transformers</b> using various <b>vision</b> <b>datasets,</b> demonstrating that LTE is competitive with standard pre-training.</p></p class="citation"></blockquote><h3 id=1051--94279-training-implicit-generative-models-via-an-invariant-statistical-loss-josé-manuel-de-frutos-et-al-2024>(10/51 | 94/279) Training Implicit Generative Models via an Invariant Statistical Loss (José Manuel de Frutos et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>José Manuel de Frutos, Pablo M. Olmos, Manuel A. Vázquez, Joaquín Míguez. (2024)<br><strong>Training Implicit Generative Models via an Invariant Statistical Loss</strong><br><button class=copy-to-clipboard title="Training Implicit Generative Models via an Invariant Statistical Loss" index=94>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-94 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG, math-ST, stat-ML, stat-TH<br>Keyword Score: 40<br>Keywords: Generative Adversarial Network, Generative Adversarial Network, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16435v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16435v1.pdf filename=2402.16435v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Implicit <b>generative</b> <b>models</b> <b>have</b> the capability to learn arbitrary complex data distributions. On the downside, training requires telling apart real data from artificially-generated ones using adversarial discriminators, leading to unstable training and mode-dropping issues. As reported by Zahee et al. (2017), even in the one-dimensional (1D) case, training a <b>generative</b> <b>adversarial</b> <b>network</b> <b>(GAN)</b> is challenging and often suboptimal. In this work, we develop a discriminator-free method for training one-dimensional (1D) <b>generative</b> <b>implicit</b> <b>models</b> and subsequently expand this method to accommodate multivariate cases. Our loss function is a discrepancy measure between a suitably chosen transformation of the model samples and a uniform distribution; hence, it is invariant with respect to the true distribution of the data. We first formulate our method for 1D random variables, providing an effective solution for approximate reparameterization of arbitrary complex distributions. Then, we consider the temporal setting (both univariate and multivariate), in which we model the conditional distribution of each sample given the history of the process. We demonstrate through numerical <b>simulations</b> that this new method yields promising results, successfully learning true distributions in a variety of scenarios and mitigating some of the well-known problems that state-of-the-art implicit methods present.</p></p class="citation"></blockquote><h3 id=1151--95279-feedback-efficient-online-fine-tuning-of-diffusion-models-masatoshi-uehara-et-al-2024>(11/51 | 95/279) Feedback Efficient Online Fine-Tuning of Diffusion Models (Masatoshi Uehara et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Masatoshi Uehara, Yulai Zhao, Kevin Black, Ehsan Hajiramezanali, Gabriele Scalia, Nathaniel Lee Diamant, Alex M Tseng, Sergey Levine, Tommaso Biancalani. (2024)<br><strong>Feedback Efficient Online Fine-Tuning of Diffusion Models</strong><br><button class=copy-to-clipboard title="Feedback Efficient Online Fine-Tuning of Diffusion Models" index=95>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-95 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG, q-bio-QM, stat-ML<br>Keyword Score: 40<br>Keywords: Diffusion Model, Fine-tuning, Fine-tuning, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16359v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16359v2.pdf filename=2402.16359v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Diffusion</b> <b>models</b> excel at modeling complex data distributions, including those of images, proteins, and small molecules. However, in many cases, our goal is to model parts of the distribution that maximize certain properties: for example, we may want to generate images with high aesthetic quality, or molecules with high bioactivity. It is natural to frame this as a <b>reinforcement</b> <b>learning</b> (RL) problem, in which the objective is to <b>fine-tune</b> a <b>diffusion</b> <b>model</b> to maximize a reward function that corresponds to some property. Even with access to online queries of the ground-truth reward function, efficiently discovering high-reward samples can be challenging: they might have a low probability in the initial distribution, and there might be many infeasible samples that do not even have a well-defined reward (e.g., unnatural images or physically impossible molecules). In this work, we propose a novel <b>reinforcement</b> <b>learning</b> procedure that efficiently explores on the manifold of feasible samples. We present a theoretical analysis providing a regret guarantee, as well as empirical validation across three domains: images, biological sequences, and molecules.</p></p class="citation"></blockquote><h3 id=1251--96279-an-integrated-data-processing-framework-for-pretraining-foundation-models-yiding-sun-et-al-2024>(12/51 | 96/279) An Integrated Data Processing Framework for Pretraining Foundation Models (Yiding Sun et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yiding Sun, Feng Wang, Yutao Zhu, Wayne Xin Zhao, Jiaxin Mao. (2024)<br><strong>An Integrated Data Processing Framework for Pretraining Foundation Models</strong><br><button class=copy-to-clipboard title="An Integrated Data Processing Framework for Pretraining Foundation Models" index=96>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-96 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CL, cs-IR, cs-LG, cs.LG<br>Keyword Score: 40<br>Keywords: Foundation Model, ChatGPT, GPT, GPT-2<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16358v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16358v1.pdf filename=2402.16358v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The ability of the <b>foundation</b> <b>models</b> heavily relies on large-scale, diverse, and high-quality pretraining data. In order to improve data quality, researchers and practitioners often have to manually curate datasets from difference sources and develop dedicated data cleansing pipeline for each data repository. Lacking a unified data processing framework, this process is repetitive and cumbersome. To mitigate this issue, we propose a data processing framework that integrates a Processing Module which consists of a series of operators at different granularity levels, and an Analyzing Module which supports probing and evaluation of the refined data. The proposed framework is easy to use and highly flexible. In this demo paper, we first introduce how to use this framework with some example use cases and then demonstrate its effectiveness in improving the data quality with an automated evaluation with <b>ChatGPT</b> and an end-to-end evaluation in pretraining the <b>GPT-2</b> model. The code and demonstration videos are accessible on GitHub.</p></p class="citation"></blockquote><h3 id=1351--97279-language-guided-skill-learning-with-temporal-variational-inference-haotian-fu-et-al-2024>(13/51 | 97/279) Language-guided Skill Learning with Temporal Variational Inference (Haotian Fu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haotian Fu, Pratyusha Sharma, Elias Stengel-Eskin, George Konidaris, Nicolas Le Roux, Marc-Alexandre Côté, Xingdi Yuan. (2024)<br><strong>Language-guided Skill Learning with Temporal Variational Inference</strong><br><button class=copy-to-clipboard title="Language-guided Skill Learning with Temporal Variational Inference" index=97>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-97 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CL, cs-LG, cs.LG<br>Keyword Score: 40<br>Keywords: Simulation, Simulator, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16354v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16354v1.pdf filename=2402.16354v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present an algorithm for skill discovery from expert demonstrations. The algorithm first utilizes <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> to propose an initial segmentation of the trajectories. Following that, a hierarchical variational inference framework incorporates the <b>LLM-generated</b> segmentation information to discover reusable skills by merging trajectory segments. To further control the trade-off between compression and reusability, we introduce a novel auxiliary objective based on the Minimum Description Length principle that helps guide this skill discovery process. Our results demonstrate that agents equipped with our method are able to discover skills that help accelerate learning and outperform baseline skill learning approaches on new long-horizon tasks in BabyAI, a grid world navigation environment, as well as ALFRED, a household <b>simulation</b> environment.</p></p class="citation"></blockquote><h3 id=1451--98279-minimize-control-inputs-for-strong-structural-controllability-using-reinforcement-learning-with-graph-neural-network-mengbang-zou-et-al-2024>(14/51 | 98/279) Minimize Control Inputs for Strong Structural Controllability Using Reinforcement Learning with Graph Neural Network (Mengbang Zou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mengbang Zou, Weisi Guo, Bailu Jin. (2024)<br><strong>Minimize Control Inputs for Strong Structural Controllability Using Reinforcement Learning with Graph Neural Network</strong><br><button class=copy-to-clipboard title="Minimize Control Inputs for Strong Structural Controllability Using Reinforcement Learning with Graph Neural Network" index=98>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-98 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 33<br>Keywords: Graph, Graph Neural Network, Markov Decision Process, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16925v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16925v1.pdf filename=2402.16925v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Strong structural controllability (SSC) guarantees networked system with linear-invariant dynamics controllable for all numerical realizations of parameters. Current research has established algebraic and <b>graph-theoretic</b> <b>conditions</b> <b>of</b> SSC for zero/nonzero or zero/nonzero/arbitrary structure. One relevant practical problem is how to fully control the system with the minimal number of input signals and identify which nodes must be imposed signals. Previous work shows that this optimization problem is NP-hard and it is difficult to find the solution. To solve this problem, we formulate the <b>graph</b> <b>coloring</b> <b>process</b> as a <b>Markov</b> <b>decision</b> <b>process</b> (MDP) according to the <b>graph-theoretical</b> <b>condition</b> <b>of</b> SSC for both zero/nonzero and zero/nonzero/arbitrary structure. We use Actor-critic method with Directed <b>graph</b> <b>neural</b> <b>network</b> which represents the color information of <b>graph</b> <b>to</b> <b>optimize</b> MDP. Our method is validated in a social influence network with real data and different complex network models. We find that the number of input nodes is determined by the average degree of the network and the input nodes tend to select nodes with low in-degree and avoid high-degree nodes.</p></p class="citation"></blockquote><h3 id=1551--99279-graph-learning-with-distributional-edge-layouts-xinjian-zhao-et-al-2024>(15/51 | 99/279) Graph Learning with Distributional Edge Layouts (Xinjian Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xinjian Zhao, Chaolong Ying, Tianshu Yu. (2024)<br><strong>Graph Learning with Distributional Edge Layouts</strong><br><button class=copy-to-clipboard title="Graph Learning with Distributional Edge Layouts" index=99>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-99 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 33<br>Keywords: GraphSAGE, Graph, Graph Neural Network, Graph Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16402v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16402v1.pdf filename=2402.16402v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Graph</b> <b>Neural</b> <b>Networks</b> <b>(GNNs)</b> learn from <b>graph-structured</b> <b>data</b> <b>by</b> passing local messages between neighboring nodes along edges on certain topological layouts. Typically, these topological layouts in modern <b>GNNs</b> are deterministically computed (e.g., attention-based <b>GNNs)</b> or locally sampled (e.g., <b>GraphSage)</b> under heuristic assumptions. In this paper, we for the first time pose that these layouts can be globally sampled via Langevin dynamics following Boltzmann distribution equipped with explicit physical energy, leading to higher feasibility in the physical world. We argue that such a collection of sampled/optimized layouts can capture the wide energy distribution and bring extra expressivity on top of WL-test, therefore easing downstream tasks. As such, we propose Distributional Edge Layouts (DELs) to serve as a complement to a variety of <b>GNNs.</b> DEL is a pre-processing strategy independent of subsequent <b>GNN</b> variants, thus being highly flexible. Experimental results demonstrate that DELs consistently and substantially improve a series of <b>GNN</b> baselines, achieving state-of-the-art performance on multiple datasets.</p></p class="citation"></blockquote><h3 id=1651--100279-boosting-graph-pooling-with-persistent-homology-chaolong-ying-et-al-2024>(16/51 | 100/279) Boosting Graph Pooling with Persistent Homology (Chaolong Ying et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chaolong Ying, Xinjian Zhao, Tianshu Yu. (2024)<br><strong>Boosting Graph Pooling with Persistent Homology</strong><br><button class=copy-to-clipboard title="Boosting Graph Pooling with Persistent Homology" index=100>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-100 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, math-AT<br>Keyword Score: 33<br>Keywords: Message-Passing, Graph, Graph Neural Network, Graph Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16346v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16346v1.pdf filename=2402.16346v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recently, there has been an emerging trend to integrate persistent homology (PH) into <b>graph</b> <b>neural</b> <b>networks</b> <b>(GNNs)</b> to enrich expressive power. However, naively plugging PH features into <b>GNN</b> layers always results in marginal improvement with low interpretability. In this paper, we investigate a novel mechanism for injecting global topological invariance into pooling layers using PH, motivated by the observation that filtration operation in PH naturally aligns <b>graph</b> <b>pooling</b> <b>in</b> a cut-off manner. In this fashion, message passing in the coarsened <b>graph</b> <b>acts</b> <b>along</b> persistent pooled topology, leading to improved performance. Experimentally, we apply our mechanism to a collection of <b>graph</b> <b>pooling</b> <b>methods</b> and observe consistent and substantial performance gain over several popular datasets, demonstrating its wide applicability and flexibility.</p></p class="citation"></blockquote><h3 id=1751--101279-watch-your-head-assembling-projection-heads-to-save-the-reliability-of-federated-models-jinqian-chen-et-al-2024>(17/51 | 101/279) Watch Your Head: Assembling Projection Heads to Save the Reliability of Federated Models (Jinqian Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jinqian Chen, Jihua Zhu, Qinghai Zheng, Zhongyu Li, Zhiqiang Tian. (2024)<br><strong>Watch Your Head: Assembling Projection Heads to Save the Reliability of Federated Models</strong><br><button class=copy-to-clipboard title="Watch Your Head: Assembling Projection Heads to Save the Reliability of Federated Models" index=101>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-101 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 33<br>Keywords: Benchmarking, Federated Learning, Fine-tuning, Out-of-distribution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16255v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16255v1.pdf filename=2402.16255v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Federated</b> <b>learning</b> encounters substantial challenges with heterogeneous data, leading to performance degradation and convergence issues. While considerable progress has been achieved in mitigating such an impact, the reliability aspect of <b>federated</b> <b>models</b> has been largely disregarded. In this study, we conduct extensive experiments to investigate the reliability of both generic and personalized <b>federated</b> <b>models.</b> Our exploration uncovers a significant finding: \textbf{federated models exhibit unreliability when faced with heterogeneous data}, demonstrating poor calibration on in-distribution test data and low uncertainty levels on <b>out-of-distribution</b> data. This unreliability is primarily attributed to the presence of biased projection heads, which introduce miscalibration into the <b>federated</b> <b>models.</b> Inspired by this observation, we propose the &ldquo;Assembled Projection Heads&rdquo; (APH) method for enhancing the reliability of <b>federated</b> <b>models.</b> By treating the existing projection head parameters as priors, APH randomly samples multiple initialized parameters of projection heads from the prior and further performs targeted <b>fine-tuning</b> on locally available data under varying learning rates. Such a head ensemble introduces parameter diversity into the deterministic model, eliminating the bias and producing reliable predictions via head averaging. We evaluate the effectiveness of the proposed APH method across three prominent <b>federated</b> <b>benchmarks.</b> Experimental results validate the efficacy of APH in model calibration and uncertainty estimation. Notably, APH can be seamlessly integrated into various <b>federated</b> <b>approaches</b> but only requires less than 30% additional computation cost for 100$\times$ inferences within large models.</p></p class="citation"></blockquote><h3 id=1851--102279-one-shot-graph-representation-learning-using-hyperdimensional-computing-abhishek-dalvi-et-al-2024>(18/51 | 102/279) One-Shot Graph Representation Learning Using Hyperdimensional Computing (Abhishek Dalvi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Abhishek Dalvi, Vasant Honavar. (2024)<br><strong>One-Shot Graph Representation Learning Using Hyperdimensional Computing</strong><br><button class=copy-to-clipboard title="One-Shot Graph Representation Learning Using Hyperdimensional Computing" index=102>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-102 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs-SI, cs.LG<br>Keyword Score: 31<br>Keywords: Graph, Graph Neural Network, Benchmarking, Representation Learning, Semi-Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17073v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17073v1.pdf filename=2402.17073v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present a novel, simple, fast, and efficient approach for <b>semi-supervised</b> <b>learning</b> on <b>graphs.</b> <b>The</b> <b>proposed</b> approach takes advantage of hyper-dimensional computing which encodes data samples using random projections into a high dimensional space (HD space for short). Specifically, we propose a Hyper-dimensional <b>Graph</b> <b>Learning</b> <b>(HDGL)</b> algorithm that leverages the injectivity property of the node <b>representations</b> <b>of</b> a family of <b>graph</b> <b>neural</b> <b>networks.</b> HDGL maps node features to the HD space and then uses HD operators such as bundling and binding to aggregate information from the local neighborhood of each node. Results of experiments with widely used <b>benchmark</b> data sets show that HDGL achieves predictive performance that is competitive with the state-of-the-art deep learning methods, without the need for computationally expensive training.</p></p class="citation"></blockquote><h3 id=1951--103279-neural-operators-with-localized-integral-and-differential-kernels-miguel-liu-schiaffini-et-al-2024>(19/51 | 103/279) Neural Operators with Localized Integral and Differential Kernels (Miguel Liu-Schiaffini et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Miguel Liu-Schiaffini, Julius Berner, Boris Bonev, Thorsten Kurth, Kamyar Azizzadenesheli, Anima Anandkumar. (2024)<br><strong>Neural Operators with Localized Integral and Differential Kernels</strong><br><button class=copy-to-clipboard title="Neural Operators with Localized Integral and Differential Kernels" index=103>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-103 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs-NA, cs.LG, math-NA<br>Keyword Score: 30<br>Keywords: Convolution, Convolutional Neural Network, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16845v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16845v1.pdf filename=2402.16845v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Neural operators learn mappings between function spaces, which is practical for learning solution operators of PDEs and other scientific modeling applications. Among them, the Fourier neural operator (FNO) is a popular architecture that performs global <b>convolutions</b> in the Fourier space. However, such global operations are often prone to over-smoothing and may fail to capture local details. In contrast, <b>convolutional</b> <b>neural</b> <b>networks</b> <b>(CNN)</b> can capture local features but are limited to training and inference at a single resolution. In this work, we present a principled approach to operator learning that can capture local features under two frameworks by learning differential operators and integral operators with locally supported kernels. Specifically, inspired by stencil methods, we prove that we obtain differential operators under an appropriate scaling of the kernel values of <b>CNNs.</b> To obtain local integral operators, we utilize suitable basis representations for the kernels based on discrete-continuous <b>convolutions.</b> Both these approaches preserve the properties of operator learning and, hence, the ability to predict at any resolution. Adding our layers to FNOs significantly improves their performance, reducing the relative L2-error by 34-72% in our experiments on turbulent 2D Navier-Stokes fluid flow and the spherical shallow water equations.</p></p class="citation"></blockquote><h3 id=2051--104279-why-transformers-need-adam-a-hessian-perspective-yushun-zhang-et-al-2024>(20/51 | 104/279) Why Transformers Need Adam: A Hessian Perspective (Yushun Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yushun Zhang, Congliang Chen, Tian Ding, Ziniu Li, Ruoyu Sun, Zhi-Quan Luo. (2024)<br><strong>Why Transformers Need Adam: A Hessian Perspective</strong><br><button class=copy-to-clipboard title="Why Transformers Need Adam: A Hessian Perspective" index=104>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-104 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Convolutional Neural Network, Stochastic Gradient Descent, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16788v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16788v1.pdf filename=2402.16788v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>SGD</b> performs worse than Adam by a significant margin on <b>Transformers,</b> but the reason remains unclear. In this work, we provide an explanation of <b>SGD&rsquo;s</b> failure on <b>Transformers</b> through the lens of Hessian: (i) <b>Transformers</b> are <code>heterogeneous'': the Hessian spectrum across parameter blocks vary dramatically, a phenomenon we call </code>block heterogeneity"; (ii) Heterogeneity hampers <b>SGD:</b> <b>SGD</b> performs badly on problems with block heterogeneity. To validate that heterogeneity hampers <b>SGD,</b> we check various <b>Transformers,</b> <b>CNNs,</b> MLPs, and quadratic problems, and find that <b>SGD</b> works well on problems without block heterogeneity but performs badly when the heterogeneity exists. Our initial theoretical analysis indicates that <b>SGD</b> fails because it applies one single learning rate for all blocks, which cannot handle the heterogeneity among blocks. The failure could be rescued if we could assign different learning rates across blocks, as designed in Adam.</p></p class="citation"></blockquote><h3 id=2151--105279-q-fox-learning-breaking-tradition-in-reinforcement-learning-mahmood-alqaseer-et-al-2024>(21/51 | 105/279) Q-FOX Learning: Breaking Tradition in Reinforcement Learning (Mahmood Alqaseer et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mahmood Alqaseer, Yossra H. Ali, Tarik A. Rashid. (2024)<br><strong>Q-FOX Learning: Breaking Tradition in Reinforcement Learning</strong><br><button class=copy-to-clipboard title="Q-FOX Learning: Breaking Tradition in Reinforcement Learning" index=105>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-105 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs-NE, cs.LG<br>Keyword Score: 30<br>Keywords: Reinforcement Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16562v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16562v1.pdf filename=2402.16562v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Reinforcement</b> <b>learning</b> (RL) is a subset of artificial intelligence (AI) where agents learn the best action by interacting with the environment, making it suitable for tasks that do not require labeled data or direct supervision. Hyperparameters (HP) tuning refers to choosing the best parameter that leads to optimal solutions in RL algorithms. Manual or random tuning of the HP may be a crucial process because variations in this parameter lead to changes in the overall learning aspects and different rewards. In this paper, a novel and automatic HP-tuning method called Q-FOX is proposed. This uses both the FOX optimizer, a new optimization method inspired by nature that mimics red foxes&rsquo; hunting behavior, and the commonly used, easy-to-implement RL Q-learning algorithm to solve the problem of HP tuning. Moreover, a new objective function is proposed which prioritizes the reward over the mean squared error (MSE) and learning time (steps). Q-FOX has been evaluated on two OpenAI Gym environment control tasks: Cart Pole and Frozen Lake. It exposed greater cumulative rewards than HP tuning with other optimizers, such as PSO, GA, Bee, or randomly selected HP. The cumulative reward for the Cart Pole task was 32.08, and for the Frozen Lake task was 0.95. Despite the robustness of Q-FOX, it has limitations. It cannot be used directly in real-word problems before choosing the HP in a <b>simulation</b> environment because its processes work iteratively, making it time-consuming. The results indicate that Q-FOX has played an essential role in HP tuning for RL algorithms to effectively solve different control tasks.</p></p class="citation"></blockquote><h3 id=2251--106279-c-gail-stabilizing-generative-adversarial-imitation-learning-with-control-theory-tianjiao-luo-et-al-2024>(22/51 | 106/279) C-GAIL: Stabilizing Generative Adversarial Imitation Learning with Control Theory (Tianjiao Luo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tianjiao Luo, Tim Pearce, Huayu Chen, Jianfei Chen, Jun Zhu. (2024)<br><strong>C-GAIL: Stabilizing Generative Adversarial Imitation Learning with Control Theory</strong><br><button class=copy-to-clipboard title="C-GAIL: Stabilizing Generative Adversarial Imitation Learning with Control Theory" index=106>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-106 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs-SY, cs.LG, eess-SY<br>Keyword Score: 30<br>Keywords: Distribution Shift, Distribution Shift, Generative Adversarial Network, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16349v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16349v1.pdf filename=2402.16349v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Generative Adversarial Imitation Learning (GAIL) trains a generative policy to mimic a demonstrator. It uses on-policy <b>Reinforcement</b> <b>Learning</b> (RL) to optimize a reward signal derived from a <b>GAN-like</b> discriminator. A major drawback of GAIL is its training instability - it inherits the complex training dynamics of <b>GANs,</b> and the <b>distribution</b> <b>shift</b> introduced by RL. This can cause oscillations during training, harming its sample efficiency and final policy performance. Recent work has shown that control theory can help with the convergence of a <b>GAN&rsquo;s</b> training. This paper extends this line of work, conducting a control-theoretic analysis of GAIL and deriving a novel controller that not only pushes GAIL to the desired equilibrium but also achieves asymptotic stability in a &lsquo;one-step&rsquo; setting. Based on this, we propose a practical algorithm &lsquo;Controlled-GAIL&rsquo; (C-GAIL). On MuJoCo tasks, our controlled variant is able to speed up the rate of convergence, reduce the range of oscillation and match the expert&rsquo;s <b>distribution</b> <b>more</b> closely both for vanilla GAIL and GAIL-DAC.</p></p class="citation"></blockquote><h3 id=2351--107279-m2mkd-module-to-module-knowledge-distillation-for-modular-transformers-ka-man-lo-et-al-2024>(23/51 | 107/279) m2mKD: Module-to-Module Knowledge Distillation for Modular Transformers (Ka Man Lo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ka Man Lo, Yiming Liang, Wenyu Du, Yuantao Fan, Zili Wang, Wenhao Huang, Lei Ma, Jie Fu. (2024)<br><strong>m2mKD: Module-to-Module Knowledge Distillation for Modular Transformers</strong><br><button class=copy-to-clipboard title="m2mKD: Module-to-Module Knowledge Distillation for Modular Transformers" index=107>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-107 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CV, cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Knowledge Distillation, Knowledge Distillation, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16918v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16918v1.pdf filename=2402.16918v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Modular neural architectures are gaining increasing attention due to their powerful capability for generalization and sample-efficient adaptation to new domains. However, training modular models, particularly in the early stages, poses challenges due to the optimization difficulties arising from their intrinsic sparse connectivity. Leveraging the <b>knowledge</b> <b>from</b> monolithic models, using techniques such as <b>knowledge</b> <b>distillation,</b> is likely to facilitate the training of modular models and enable them to integrate <b>knowledge</b> <b>from</b> multiple models pretrained on diverse sources. Nevertheless, conventional <b>knowledge</b> <b>distillation</b> approaches are not tailored to modular models and can fail when directly applied due to the unique architectures and the enormous number of parameters involved. Motivated by these challenges, we propose a general module-to-module <b>knowledge</b> <b>distillation</b> (m2mKD) method for transferring <b>knowledge</b> <b>between</b> modules. Our approach involves teacher modules split from a pretrained monolithic model, and student modules of a modular model. m2mKD separately combines these modules with a shared meta model and encourages the student module to mimic the behaviour of the teacher module. We evaluate the effectiveness of m2mKD on two distinct modular neural architectures: Neural Attentive Circuits (NACs) and Vision Mixture-of-Experts (V-MoE). By applying m2mKD to NACs, we achieve significant improvements in IID accuracy on Tiny-ImageNet (up to 5.6%) and OOD robustness on Tiny-ImageNet-R (up to 4.2%). On average, we observe a 1% gain in both ImageNet and ImageNet-R. The V-MoE-Base model trained using m2mKD also achieves 3.5% higher accuracy than end-to-end training on ImageNet. The experimental results demonstrate that our method offers a promising solution for connecting modular networks with pretrained monolithic models. Code is available at <a href=https://github.com/kamanphoebe/m2mKD>https://github.com/kamanphoebe/m2mKD</a>.</p></p class="citation"></blockquote><h3 id=2451--108279-poisson-gamma-dynamical-systems-with-non-stationary-transition-dynamics-jiahao-wang-et-al-2024>(24/51 | 108/279) Poisson-Gamma Dynamical Systems with Non-Stationary Transition Dynamics (Jiahao Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiahao Wang, Sikun Yang, Heinz Koeppl, Xiuzhen Cheng, Pengfei Hu, Guoming Zhang. (2024)<br><strong>Poisson-Gamma Dynamical Systems with Non-Stationary Transition Dynamics</strong><br><button class=copy-to-clipboard title="Poisson-Gamma Dynamical Systems with Non-Stationary Transition Dynamics" index=108>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-108 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Data Augmentation, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16297v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16297v1.pdf filename=2402.16297v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Bayesian methodologies for handling count-valued time series have gained prominence due to their ability to infer interpretable latent structures and to estimate uncertainties, and thus are especially suitable for dealing with noisy and incomplete count <b>data.</b> <b>Among</b> these Bayesian models, Poisson-Gamma Dynamical Systems (PGDSs) are proven to be effective in capturing the evolving dynamics underlying observed count sequences. However, the state-of-the-art PGDS still falls short in capturing the time-varying transition dynamics that are commonly observed in real-world count time series. To mitigate this limitation, a non-stationary PGDS is proposed to allow the underlying transition matrices to evolve over time, and the evolving transition matrices are modeled by sophisticatedly-designed Dirichlet Markov chains. Leveraging Dirichlet-Multinomial-Beta <b>data</b> <b>augmentation</b> techniques, a fully-conjugate and efficient Gibbs sampler is developed to perform posterior <b>simulation.</b> Experiments show that, in comparison with related models, the proposed non-stationary PGDS achieves improved predictive performance due to its capacity to learn non-stationary dependency structure captured by the time-evolving transition matrices.</p></p class="citation"></blockquote><h3 id=2551--109279-foundation-model-transparency-reports-rishi-bommasani-et-al-2024>(25/51 | 109/279) Foundation Model Transparency Reports (Rishi Bommasani et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rishi Bommasani, Kevin Klyman, Shayne Longpre, Betty Xiong, Sayash Kapoor, Nestor Maslej, Arvind Narayanan, Percy Liang. (2024)<br><strong>Foundation Model Transparency Reports</strong><br><button class=copy-to-clipboard title="Foundation Model Transparency Reports" index=109>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-109 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CY, cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Foundation Model, Recommendation, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16268v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16268v1.pdf filename=2402.16268v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Foundation</b> <b>models</b> are critical digital technologies with sweeping societal impact that necessitates transparency. To codify how <b>foundation</b> <b>model</b> developers should provide transparency about the development and deployment of their models, we propose <b>Foundation</b> <b>Model</b> Transparency Reports, drawing upon the transparency reporting practices in social media. While external documentation of societal harms <b>prompted</b> social media transparency reports, our objective is to institutionalize transparency reporting for <b>foundation</b> <b>models</b> while the industry is still nascent. To design our reports, we identify 6 design principles given the successes and shortcomings of social media transparency reporting. To further schematize our reports, we draw upon the 100 transparency indicators from the <b>Foundation</b> <b>Model</b> Transparency Index. Given these indicators, we measure the extent to which they overlap with the transparency requirements included in six prominent government policies (e.g., the EU AI Act, the US Executive Order on Safe, Secure, and Trustworthy AI). Well-designed transparency reports could reduce compliance costs, in part due to overlapping regulatory requirements across different jurisdictions. We encourage <b>foundation</b> <b>model</b> developers to regularly publish transparency reports, building upon <b>recommendations</b> from the G7 and the White House.</p></p class="citation"></blockquote><h3 id=2651--110279-self-supervised-correlation-based-permutations-for-multi-view-clustering-ran-eisenberg-et-al-2024>(26/51 | 110/279) Self Supervised Correlation-based Permutations for Multi-View Clustering (Ran Eisenberg et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ran Eisenberg, Jonathan Svirsky, Ofir Lindenbaum. (2024)<br><strong>Self Supervised Correlation-based Permutations for Multi-View Clustering</strong><br><button class=copy-to-clipboard title="Self Supervised Correlation-based Permutations for Multi-View Clustering" index=110>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-110 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-ML<br>Keyword Score: 26<br>Keywords: Benchmarking, Clustering, Self-supervised Learning, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16383v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16383v1.pdf filename=2402.16383v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Fusing information from different modalities can enhance data analysis tasks, including <b>clustering.</b> However, existing multi-view <b>clustering</b> (MVC) solutions are limited to specific domains or rely on a suboptimal and computationally demanding two-stage procedure of representation and <b>clustering.</b> We propose an end-to-end deep learning-based MVC framework for general data (image, tabular, etc.). Our approach involves learning meaningful fused data representations with a novel permutation-based canonical correlation objective. Concurrently, we learn cluster assignments by identifying consistent pseudo-labels across multiple views. We demonstrate the effectiveness of our model using ten MVC <b>benchmark</b> datasets. Theoretically, we show that our model approximates the <b>supervised</b> linear discrimination analysis (LDA) representation. Additionally, we provide an error bound induced by false-pseudo label annotations.</p></p class="citation"></blockquote><h3 id=2751--111279-garnn-an-interpretable-graph-attentive-recurrent-neural-network-for-predicting-blood-glucose-levels-via-multivariate-time-series-chengzhe-piao-et-al-2024>(27/51 | 111/279) GARNN: An Interpretable Graph Attentive Recurrent Neural Network for Predicting Blood Glucose Levels via Multivariate Time Series (Chengzhe Piao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chengzhe Piao, Taiyu Zhu, Stephanie E Baldeweg, Paul Taylor, Pantelis Georgiou, Jiahao Sun, Jun Wang, Kezhi Li. (2024)<br><strong>GARNN: An Interpretable Graph Attentive Recurrent Neural Network for Predicting Blood Glucose Levels via Multivariate Time Series</strong><br><button class=copy-to-clipboard title="GARNN: An Interpretable Graph Attentive Recurrent Neural Network for Predicting Blood Glucose Levels via Multivariate Time Series" index=111>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-111 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 26<br>Keywords: Graph, Multi-modal, Recurrent Neural Network, Neural Machine Translation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16230v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16230v1.pdf filename=2402.16230v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Accurate prediction of future blood glucose (BG) levels can effectively improve BG management for people living with diabetes, thereby reducing complications and improving quality of life. The state of the art of BG prediction has been achieved by leveraging advanced deep learning methods to model <b>multi-modal</b> data, i.e., sensor data and self-reported event data, organised as multi-variate time series <b>(MTS).</b> However, these methods are mostly regarded as ``black boxes&rsquo;&rsquo; and not entirely trusted by clinicians and patients. In this paper, we propose interpretable <b>graph</b> attentive <b>recurrent</b> <b>neural</b> <b>networks</b> (GARNNs) to model <b>MTS,</b> explaining variable contributions via summarizing variable importance and generating feature maps by <b>graph</b> attention mechanisms instead of post-hoc analysis. We evaluate GARNNs on four datasets, representing diverse clinical scenarios. Upon comparison with twelve well-established baseline methods, GARNNs not only achieve the best prediction accuracy but also provide high-quality temporal interpretability, in particular for postprandial glucose levels as a result of corresponding meal intake and insulin injection. These findings underline the potential of GARNN as a robust tool for improving diabetes care, bridging the gap between deep learning technology and real-world healthcare solutions.</p></p class="citation"></blockquote><h3 id=2851--112279-a-curious-case-of-remarkable-resilience-to-gradient-attacks-via-fully-convolutional-and-differentiable-front-end-with-a-skip-connection-leonid-boytsov-et-al-2024>(28/51 | 112/279) A Curious Case of Remarkable Resilience to Gradient Attacks via Fully Convolutional and Differentiable Front End with a Skip Connection (Leonid Boytsov et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Leonid Boytsov, Ameya Joshi, Filipe Condessa. (2024)<br><strong>A Curious Case of Remarkable Resilience to Gradient Attacks via Fully Convolutional and Differentiable Front End with a Skip Connection</strong><br><button class=copy-to-clipboard title="A Curious Case of Remarkable Resilience to Gradient Attacks via Fully Convolutional and Differentiable Front End with a Skip Connection" index=112>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-112 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CV, cs-LG, cs.LG<br>Keyword Score: 25<br>Keywords: Adversarial Learning, Black Box, Convolution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17018v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17018v1.pdf filename=2402.17018v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We tested front-end enhanced neural models where a frozen classifier was prepended by a differentiable and fully <b>convolutional</b> model with a skip connection. By training them using a small learning rate for about one epoch, we obtained models that retained the accuracy of the backbone classifier while being unusually resistant to gradient attacks including APGD and FAB-T attacks from the AutoAttack package, which we attributed to gradient masking. The gradient masking phenomenon is not new, but the degree of masking was quite remarkable for fully differentiable models that did not have gradient-shattering components such as JPEG compression or components that are expected to cause diminishing gradients. Though <b>black</b> <b>box</b> attacks can be partially effective against gradient masking, they are easily defeated by combining models into randomized ensembles. We estimate that such ensembles achieve near-SOTA AutoAttack accuracy on CIFAR10, CIFAR100, and ImageNet despite having virtually zero accuracy under adaptive attacks. <b>Adversarial</b> <b>training</b> of the backbone classifier can further increase resistance of the front-end enhanced model to gradient attacks. On CIFAR10, the respective randomized ensemble achieved 90.8$\pm 2.5$% (99% CI) accuracy under AutoAttack while having only 18.2$\pm 3.6$% accuracy under the adaptive attack. We do not establish SOTA in <b>adversarial</b> <b>robustness.</b> Instead, we make methodological contributions and further supports the thesis that adaptive attacks designed with the complete knowledge of model architecture are crucial in demonstrating model robustness and that even the so-called white-box gradient attacks can have limited applicability. Although gradient attacks can be complemented with <b>black-box</b> <b>attack</b> such as the SQUARE attack or the zero-order PGD, <b>black-box</b> <b>attacks</b> can be weak against randomized ensembles, e.g., when ensemble models mask gradients.</p></p class="citation"></blockquote><h3 id=2951--113279-craftax-a-lightning-fast-benchmark-for-open-ended-reinforcement-learning-michael-matthews-et-al-2024>(29/51 | 113/279) Craftax: A Lightning-Fast Benchmark for Open-Ended Reinforcement Learning (Michael Matthews et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Michael Matthews, Michael Beukman, Benjamin Ellis, Mikayel Samvelyan, Matthew Jackson, Samuel Coward, Jakob Foerster. (2024)<br><strong>Craftax: A Lightning-Fast Benchmark for Open-Ended Reinforcement Learning</strong><br><button class=copy-to-clipboard title="Craftax: A Lightning-Fast Benchmark for Open-Ended Reinforcement Learning" index=113>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-113 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 23<br>Keywords: Benchmarking, Reinforcement Learning, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16801v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16801v1.pdf filename=2402.16801v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Benchmarks</b> play a crucial role in the development and analysis of <b>reinforcement</b> <b>learning</b> (RL) algorithms. We identify that existing <b>benchmarks</b> used for research into open-ended learning fall into one of two categories. Either they are too slow for meaningful research to be performed without enormous computational resources, like Crafter, NetHack and Minecraft, or they are not complex enough to pose a significant challenge, like Minigrid and Procgen. To remedy this, we first present Craftax-Classic: a ground-up rewrite of Crafter in JAX that runs up to 250x faster than the Python-native original. A run of PPO using 1 billion environment interactions finishes in under an hour using only a single GPU and averages 90% of the optimal reward. To provide a more compelling challenge we present the main Craftax <b>benchmark,</b> a significant extension of the Crafter mechanics with elements inspired from NetHack. Solving Craftax requires deep exploration, long term planning and memory, as well as continual adaptation to novel situations as more of the world is discovered. We show that existing methods including global and episodic exploration, as well as <b>unsupervised</b> environment design fail to make material progress on the <b>benchmark.</b> We believe that Craftax can for the first time allow researchers to experiment in a complex, open-ended environment with limited computational resources.</p></p class="citation"></blockquote><h3 id=3051--114279-on-the-generalization-capability-of-temporal-graph-learning-algorithms-theoretical-insights-and-a-simpler-method-weilin-cong-et-al-2024>(30/51 | 114/279) On the Generalization Capability of Temporal Graph Learning Algorithms: Theoretical Insights and a Simpler Method (Weilin Cong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Weilin Cong, Jian Kang, Hanghang Tong, Mehrdad Mahdavi. (2024)<br><strong>On the Generalization Capability of Temporal Graph Learning Algorithms: Theoretical Insights and a Simpler Method</strong><br><button class=copy-to-clipboard title="On the Generalization Capability of Temporal Graph Learning Algorithms: Theoretical Insights and a Simpler Method" index=114>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-114 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 23<br>Keywords: Graph, Graph Neural Network, Recurrent Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16387v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16387v1.pdf filename=2402.16387v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Temporal <b>Graph</b> Learning (TGL) has become a prevalent technique across diverse real-world applications, especially in domains where data can be represented as a <b>graph</b> and evolves over time. Although TGL has recently seen notable progress in algorithmic solutions, its theoretical foundations remain largely unexplored. This paper aims at bridging this gap by investigating the generalization ability of different TGL algorithms (e.g., <b>GNN-based,</b> <b>RNN-based,</b> and memory-based methods) under the finite-wide over-parameterized regime. We establish the connection between the generalization error of TGL algorithms and &ldquo;the number of layers/steps&rdquo; in the <b>GNN-/RNN-based</b> TGL methods and &ldquo;the feature-label alignment (FLA) score&rdquo;, where FLA can be used as a proxy for the expressive power and explains the performance of memory-based methods. Guided by our theoretical analysis, we propose Simplified-Temporal-Graph-Network, which enjoys a small generalization error, improved overall performance, and lower model complexity. Extensive experiments on real-world datasets demonstrate the effectiveness of our method. Our theoretical findings and proposed algorithm offer essential insights into TGL from a theoretical standpoint, laying the groundwork for the designing practical TGL algorithms in future studies.</p></p class="citation"></blockquote><h3 id=3151--115279-graph-diffusion-policy-optimization-yijing-liu-et-al-2024>(31/51 | 115/279) Graph Diffusion Policy Optimization (Yijing Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yijing Liu, Chao Du, Tianyu Pang, Chongxuan Li, Wei Chen, Min Lin. (2024)<br><strong>Graph Diffusion Policy Optimization</strong><br><button class=copy-to-clipboard title="Graph Diffusion Policy Optimization" index=115>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-115 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CE, cs-LG, cs.LG<br>Keyword Score: 23<br>Keywords: Diffusion Model, Graph, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16302v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16302v1.pdf filename=2402.16302v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent research has made significant progress in optimizing <b>diffusion</b> <b>models</b> for specific downstream objectives, which is an important pursuit in fields such as <b>graph</b> generation for drug design. However, directly applying these models to <b>graph</b> <b>diffusion</b> <b>presents</b> challenges, resulting in suboptimal performance. This paper introduces <b>graph</b> <b>diffusion</b> <b>policy</b> optimization (GDPO), a novel approach to optimize <b>graph</b> <b>diffusion</b> <b>models</b> for arbitrary (e.g., non-differentiable) objectives using <b>reinforcement</b> <b>learning.</b> GDPO is based on an eager policy gradient tailored for <b>graph</b> <b>diffusion</b> <b>models,</b> developed through meticulous analysis and promising improved performance. Experimental results show that GDPO achieves state-of-the-art performance in various <b>graph</b> generation tasks with complex and diverse objectives. Code is available at <a href=https://github.com/sail-sg/GDPO>https://github.com/sail-sg/GDPO</a>.</p></p class="citation"></blockquote><h3 id=3251--116279-parallelized-spatiotemporal-binding-gautam-singh-et-al-2024>(32/51 | 116/279) Parallelized Spatiotemporal Binding (Gautam Singh et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Gautam Singh, Yue Wang, Jiawei Yang, Boris Ivanovic, Sungjin Ahn, Marco Pavone, Tong Che. (2024)<br><strong>Parallelized Spatiotemporal Binding</strong><br><button class=copy-to-clipboard title="Parallelized Spatiotemporal Binding" index=116>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-116 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CV, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Unsupervised Learning, Recurrent Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17077v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17077v1.pdf filename=2402.17077v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>While modern best practices advocate for scalable architectures that support long-range interactions, object-centric models are yet to fully embrace these architectures. In particular, existing object-centric models for handling sequential inputs, due to their reliance on <b>RNN-based</b> implementation, show poor stability and capacity and are slow to train on long sequences. We introduce Parallelizable Spatiotemporal Binder or PSB, the first temporally-parallelizable slot learning architecture for sequential inputs. Unlike conventional <b>RNN-based</b> approaches, PSB produces object-centric representations, known as slots, for all time-steps in parallel. This is achieved by refining the initial slots across all time-steps through a fixed number of layers equipped with causal attention. By capitalizing on the parallelism induced by our architecture, the proposed model exhibits a significant boost in efficiency. In experiments, we test PSB extensively as an encoder within an auto-encoding framework paired with a wide variety of decoder options. Compared to the state-of-the-art, our architecture demonstrates stable training on longer sequences, achieves parallelization that results in a 60% increase in training speed, and yields performance that is on par with or better on <b>unsupervised</b> 2D and 3D object-centric scene decomposition and understanding.</p></p class="citation"></blockquote><h3 id=3351--117279-monitoring-fidelity-of-online-reinforcement-learning-algorithms-in-clinical-trials-anna-l-trella-et-al-2024>(33/51 | 117/279) Monitoring Fidelity of Online Reinforcement Learning Algorithms in Clinical Trials (Anna L. Trella et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Anna L. Trella, Kelly W. Zhang, Inbal Nahum-Shani, Vivek Shetty, Iris Yan, Finale Doshi-Velez, Susan A. Murphy. (2024)<br><strong>Monitoring Fidelity of Online Reinforcement Learning Algorithms in Clinical Trials</strong><br><button class=copy-to-clipboard title="Monitoring Fidelity of Online Reinforcement Learning Algorithms in Clinical Trials" index=117>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-117 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CY, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Online Reinforcement Learning, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17003v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17003v1.pdf filename=2402.17003v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Online</b> <b>reinforcement</b> <b>learning</b> (RL) algorithms offer great potential for personalizing treatment for participants in clinical trials. However, deploying an <b>online,</b> <b>autonomous</b> <b>algorithm</b> in the high-stakes healthcare setting makes quality control and data quality especially difficult to achieve. This paper proposes algorithm fidelity as a critical requirement for deploying <b>online</b> <b>RL</b> <b>algorithms</b> in clinical trials. It emphasizes the responsibility of the algorithm to (1) safeguard participants and (2) preserve the scientific utility of the data for post-trial analyses. We also present a framework for pre-deployment planning and real-time monitoring to help algorithm developers and clinical researchers ensure algorithm fidelity. To illustrate our framework&rsquo;s practical application, we present real-world examples from the Oralytics clinical trial. Since Spring 2023, this trial successfully deployed an autonomous, <b>online</b> <b>RL</b> <b>algorithm</b> to personalize behavioral interventions for participants at risk for dental disease.</p></p class="citation"></blockquote><h3 id=3451--118279-discovering-symmetry-group-structures-via-implicit-orthogonality-bias-dongsung-huh-2024>(34/51 | 118/279) Discovering Symmetry Group Structures via Implicit Orthogonality Bias (Dongsung Huh, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dongsung Huh. (2024)<br><strong>Discovering Symmetry Group Structures via Implicit Orthogonality Bias</strong><br><button class=copy-to-clipboard title="Discovering Symmetry Group Structures via Implicit Orthogonality Bias" index=118>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-118 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, math-GR, math-RT<br>Keyword Score: 20<br>Keywords: Convolution, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17002v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17002v2.pdf filename=2402.17002v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce the HyperCube network, a novel approach for autonomously discovering symmetry group structures within data. The key innovation is a unique factorization architecture coupled with a novel regularizer that instills a powerful inductive bias towards learning orthogonal representations. This leverages a fundamental theorem of representation theory that all compact/finite groups can be represented by orthogonal matrices. HyperCube efficiently learns general group operations from partially observed data, successfully recovering complete operation tables. Remarkably, the learned factors correspond directly to exact matrix representations of the underlying group. Moreover, these factors capture the group&rsquo;s complete set of irreducible representations, forming the generalized Fourier basis for performing group <b>convolutions.</b> In extensive experiments with both group and non-group symbolic operations, HyperCube demonstrates a dramatic 100-1000x improvement in training speed and 2-10x greater sample efficiency compared to the <b>Transformer</b> baseline. These results suggest that our approach unlocks a new class of deep learning models capable of harnessing inherent symmetries within data, leading to significant improvements in performance and broader applicability.</p></p class="citation"></blockquote><h3 id=3551--119279-enhancing-continuous-domain-adaptation-with-multi-path-transfer-curriculum-hanbing-liu-et-al-2024>(35/51 | 119/279) Enhancing Continuous Domain Adaptation with Multi-Path Transfer Curriculum (Hanbing Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hanbing Liu, Jingge Wang, Xuan Zhang, Ye Guo, Yang Li. (2024)<br><strong>Enhancing Continuous Domain Adaptation with Multi-Path Transfer Curriculum</strong><br><button class=copy-to-clipboard title="Enhancing Continuous Domain Adaptation with Multi-Path Transfer Curriculum" index=119>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-119 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Transfer Learning, Domain Adaptation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16681v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16681v1.pdf filename=2402.16681v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Addressing the large distribution gap between training and testing data has long been a challenge in machine learning, giving rise to fields such as <b>transfer</b> <b>learning</b> and <b>domain</b> <b>adaptation.</b> Recently, Continuous <b>Domain</b> <b>Adaptation</b> (CDA) has emerged as an effective technique, closing this gap by utilizing a series of intermediate <b>domains.</b> <b>This</b> paper contributes a novel CDA method, W-MPOT, which rigorously addresses the <b>domain</b> <b>ordering</b> and error accumulation problems overlooked by previous studies. Specifically, we construct a <b>transfer</b> <b>curriculum</b> over the source and intermediate <b>domains</b> <b>based</b> on Wasserstein distance, motivated by theoretical analysis of CDA. Then we <b>transfer</b> <b>the</b> source model to the target <b>domain</b> <b>through</b> multiple valid paths in the curriculum using a modified version of continuous optimal transport. A bidirectional path consistency constraint is introduced to mitigate the impact of accumulated mapping errors during continuous <b>transfer.</b> <b>We</b> extensively evaluate W-MPOT on multiple datasets, achieving up to 54.1% accuracy improvement on multi-session Alzheimer MR image classification and 94.7% MSE reduction on battery capacity estimation.</p></p class="citation"></blockquote><h3 id=3651--120279-program-based-strategy-induction-for-reinforcement-learning-carlos-g-correa-et-al-2024>(36/51 | 120/279) Program-Based Strategy Induction for Reinforcement Learning (Carlos G. Correa et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Carlos G. Correa, Thomas L. Griffiths, Nathaniel D. Daw. (2024)<br><strong>Program-Based Strategy Induction for Reinforcement Learning</strong><br><button class=copy-to-clipboard title="Program-Based Strategy Induction for Reinforcement Learning" index=120>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-120 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Bandit Algorithm, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16668v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16668v1.pdf filename=2402.16668v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Typical models of learning assume incremental estimation of continuously-varying decision variables like expected rewards. However, this class of models fails to capture more idiosyncratic, discrete heuristics and strategies that people and animals appear to exhibit. Despite recent advances in strategy discovery using tools like recurrent networks that generalize the classic models, the resulting strategies are often onerous to interpret, making connections to cognition difficult to establish. We use Bayesian program induction to discover strategies implemented by programs, letting the simplicity of strategies trade off against their effectiveness. Focusing on <b>bandit</b> tasks, we find strategies that are difficult or unexpected with classical incremental learning, like asymmetric learning from rewarded and unrewarded trials, adaptive horizon-dependent random exploration, and discrete state switching.</p></p class="citation"></blockquote><h3 id=3751--121279-achieving-tildeo1ε-sample-complexity-for-constrained-markov-decision-process-jiashuo-jiang-et-al-2024>(37/51 | 121/279) Achieving $\tilde{O}(1/ε)$ Sample Complexity for Constrained Markov Decision Process (Jiashuo Jiang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiashuo Jiang, Yinyu Ye. (2024)<br><strong>Achieving $\tilde{O}(1/ε)$ Sample Complexity for Constrained Markov Decision Process</strong><br><button class=copy-to-clipboard title="Achieving $\tilde{O}(1/ε)$ Sample Complexity for Constrained Markov Decision Process" index=121>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-121 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, math-OC<br>Keyword Score: 20<br>Keywords: Markov Decision Process, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16324v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16324v1.pdf filename=2402.16324v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We consider the <b>reinforcement</b> <b>learning</b> problem for the constrained <b>Markov</b> <b>decision</b> <b>process</b> (CMDP), which plays a central role in satisfying safety or resource constraints in sequential learning and decision-making. In this problem, we are given finite resources and a MDP with unknown transition probabilities. At each stage, we take an action, collecting a reward and consuming some resources, all assumed to be unknown and need to be learned over time. In this work, we take the first step towards deriving optimal problem-dependent guarantees for the CMDP problems. We derive a logarithmic regret bound, which translates into a $O(\frac{\kappa}{\epsilon}\cdot\log^2(1/\epsilon))$ sample complexity bound, with $\kappa$ being a problem-dependent parameter, yet independent of $\epsilon$. Our sample complexity bound improves upon the state-of-art $O(1/\epsilon^2)$ sample complexity for CMDP problems established in the previous literature, in terms of the dependency on $\epsilon$. To achieve this advance, we develop a new framework for analyzing CMDP problems. To be specific, our algorithm operates in the primal space and we resolve the primal LP for the CMDP problem at each period in an online manner, with \textit{adaptive} remaining resource capacities. The key elements of our algorithm are: i). an eliminating procedure that characterizes one optimal basis of the primal LP, and; ii) a resolving procedure that is adaptive to the remaining resources and sticks to the characterized optimal basis.</p></p class="citation"></blockquote><h3 id=3851--122279-federated-contextual-cascading-bandits-with-asynchronous-communication-and-heterogeneous-users-hantao-yang-et-al-2024>(38/51 | 122/279) Federated Contextual Cascading Bandits with Asynchronous Communication and Heterogeneous Users (Hantao Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hantao Yang, Xutong Liu, Zhiyong Wang, Hong Xie, John C. S. Lui, Defu Lian, Enhong Chen. (2024)<br><strong>Federated Contextual Cascading Bandits with Asynchronous Communication and Heterogeneous Users</strong><br><button class=copy-to-clipboard title="Federated Contextual Cascading Bandits with Asynchronous Communication and Heterogeneous Users" index=122>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-122 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Bandit Algorithm, Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16312v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16312v1.pdf filename=2402.16312v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We study the problem of federated contextual combinatorial cascading <b>bandits,</b> where $|\mathcal{U}|$ agents collaborate under the coordination of a central server to provide tailored <b>recommendations</b> to the $|\mathcal{U}|$ corresponding users. Existing works consider either a synchronous framework, necessitating full agent participation and global synchronization, or assume user homogeneity with identical behaviors. We overcome these limitations by considering (1) federated agents operating in an asynchronous communication paradigm, where no mandatory synchronization is required and all agents communicate independently with the server, (2) heterogeneous user behaviors, where users can be stratified into $J \le |\mathcal{U}|$ latent user clusters, each exhibiting distinct preferences. For this setting, we propose a UCB-type algorithm with delicate communication protocols. Through theoretical analysis, we give sub-linear regret bounds on par with those achieved in the synchronous framework, while incurring only logarithmic communication costs. Empirical evaluation on synthetic and real-world datasets validates our algorithm&rsquo;s superior performance in terms of regrets and communication costs.</p></p class="citation"></blockquote><h3 id=3951--123279-replay-modeling-time-varying-temporal-regularities-of-human-mobility-for-location-prediction-over-sparse-trajectories-bangchao-deng-et-al-2024>(39/51 | 123/279) REPLAY: Modeling Time-Varying Temporal Regularities of Human Mobility for Location Prediction over Sparse Trajectories (Bangchao Deng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bangchao Deng, Bingqing Qu, Pengyang Wang, Dingqi Yang. (2024)<br><strong>REPLAY: Modeling Time-Varying Temporal Regularities of Human Mobility for Location Prediction over Sparse Trajectories</strong><br><button class=copy-to-clipboard title="REPLAY: Modeling Time-Varying Temporal Regularities of Human Mobility for Location Prediction over Sparse Trajectories" index=123>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-123 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Recurrent Neural Network, Recurrent Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16310v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16310v1.pdf filename=2402.16310v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Location prediction forecasts a user&rsquo;s location based on historical user mobility traces. To tackle the intrinsic sparsity issue of real-world user mobility traces, spatiotemporal contexts have been shown as significantly useful. Existing solutions mostly incorporate spatiotemporal distances between locations in mobility traces, either by feeding them as additional inputs to <b>Recurrent</b> <b>Neural</b> <b>Networks</b> <b>(RNNs)</b> or by using them to search for informative past hidden states for prediction. However, such distance-based methods fail to capture the time-varying temporal regularities of human mobility, where human mobility is often more regular in the morning than in other periods, for example; this suggests the usefulness of the actual timestamps besides the temporal distances. Against this background, we propose REPLAY, a general <b>RNN</b> architecture learning to capture the time-varying temporal regularities for location prediction. Specifically, REPLAY not only resorts to the spatiotemporal distances in sparse trajectories to search for the informative past hidden states, but also accommodates the time-varying temporal regularities by incorporating smoothed timestamp embeddings using Gaussian weighted averaging with timestamp-specific learnable bandwidths, which can flexibly adapt to the temporal regularities of different strengths across different timestamps. Our extensive evaluation compares REPLAY against a sizable collection of state-of-the-art techniques on two real-world datasets. Results show that REPLAY consistently and significantly outperforms state-of-the-art methods by 7.7%-10.9% in the location prediction task, and the bandwidths reveal interesting patterns of the time-varying temporal regularities.</p></p class="citation"></blockquote><h3 id=4051--124279-learning-to-schedule-online-tasks-with-bandit-feedback-yongxin-xu-et-al-2024>(40/51 | 124/279) Learning to Schedule Online Tasks with Bandit Feedback (Yongxin Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yongxin Xu, Shangshang Wang, Hengquan Guo, Xin Liu, Ziyu Shao. (2024)<br><strong>Learning to Schedule Online Tasks with Bandit Feedback</strong><br><button class=copy-to-clipboard title="Learning to Schedule Online Tasks with Bandit Feedback" index=124>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-124 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-DC, cs-LG, cs.LG<br>Keyword Score: 15<br>Keywords: Bandit Algorithm, Black Box<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16463v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16463v1.pdf filename=2402.16463v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Online task scheduling serves an integral role for task-intensive applications in cloud computing and crowdsourcing. Optimal scheduling can enhance system performance, typically measured by the reward-to-cost ratio, under some task arrival distribution. On one hand, both reward and cost are dependent on task context (e.g., evaluation metric) and remain <b>black-box</b> <b>in</b> practice. These render reward and cost hard to model thus unknown before decision making. On the other hand, task arrival behaviors remain sensitive to factors like unpredictable system fluctuation whereby a prior estimation or the conventional assumption of arrival distribution (e.g., Poisson) may fail. This implies another practical yet often neglected challenge, i.e., uncertain task arrival distribution. Towards effective scheduling under a stationary environment with various uncertainties, we propose a double-optimistic learning based Robbins-Monro (DOL-RM) algorithm. Specifically, DOL-RM integrates a learning module that incorporates optimistic estimation for reward-to-cost ratio and a decision module that utilizes the Robbins-Monro method to implicitly learn task arrival distribution while making scheduling decisions. Theoretically, DOL-RM achieves convergence gap and no regret learning with a sub-linear regret of $O(T^{3/4})$, which is the first result for online task scheduling under uncertain task arrival distribution and unknown reward and cost. Our numerical results in a synthetic experiment and a real-world application demonstrate the effectiveness of DOL-RM in achieving the best cumulative reward-to-cost ratio compared with other state-of-the-art baselines.</p></p class="citation"></blockquote><h3 id=4151--125279-totem-tokenized-time-series-embeddings-for-general-time-series-analysis-sabera-talukder-et-al-2024>(41/51 | 125/279) TOTEM: TOkenized Time Series EMbeddings for General Time Series Analysis (Sabera Talukder et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sabera Talukder, Yisong Yue, Georgia Gkioxari. (2024)<br><strong>TOTEM: TOkenized Time Series EMbeddings for General Time Series Analysis</strong><br><button class=copy-to-clipboard title="TOTEM: TOkenized Time Series EMbeddings for General Time Series Analysis" index=125>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-125 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 13<br>Keywords: Benchmarking, Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16412v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16412v1.pdf filename=2402.16412v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The field of general time series analysis has recently begun to explore unified modeling, where a common architectural backbone can be retrained on a specific task for a specific dataset. In this work, we approach unification from a complementary vantage point: unification across tasks and domains. To this end, we explore the impact of discrete, learnt, time series data representations that enable generalist, cross-domain training. Our method, TOTEM, or TOkenized Time Series EMbeddings, proposes a simple tokenizer architecture that embeds time series data from varying domains using a discrete vectorized representation learned in a <b>self-supervised</b> manner. TOTEM works across multiple tasks and domains with minimal to no tuning. We study the efficacy of TOTEM with an extensive evaluation on 17 real world time series datasets across 3 tasks. We evaluate both the specialist (i.e., training a model on each domain) and generalist (i.e., training a single model on many domains) settings, and show that TOTEM matches or outperforms previous best methods on several popular <b>benchmarks.</b> The code can be found at: <a href=https://github.com/SaberaTalukder/TOTEM>https://github.com/SaberaTalukder/TOTEM</a>.</p></p class="citation"></blockquote><h3 id=4251--126279-fedreview-a-review-mechanism-for-rejecting-poisoned-updates-in-federated-learning-tianhang-zheng-et-al-2024>(42/51 | 126/279) FedReview: A Review Mechanism for Rejecting Poisoned Updates in Federated Learning (Tianhang Zheng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tianhang Zheng, Baochun Li. (2024)<br><strong>FedReview: A Review Mechanism for Rejecting Poisoned Updates in Federated Learning</strong><br><button class=copy-to-clipboard title="FedReview: A Review Mechanism for Rejecting Poisoned Updates in Federated Learning" index=126>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-126 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CR, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Federated Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16934v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16934v1.pdf filename=2402.16934v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Federated</b> <b>learning</b> has recently emerged as a decentralized approach to learn a high-performance model without access to user data. Despite its effectiveness, <b>federated</b> <b>learning</b> gives malicious users opportunities to manipulate the model by uploading poisoned model updates to the server. In this paper, we propose a review mechanism called FedReview to identify and decline the potential poisoned updates in <b>federated</b> <b>learning.</b> Under our mechanism, the server randomly assigns a subset of clients as reviewers to evaluate the model updates on their training datasets in each round. The reviewers rank the model updates based on the evaluation results and count the number of the updates with relatively low quality as the estimated number of poisoned updates. Based on review reports, the server employs a majority voting mechanism to integrate the rankings and remove the potential poisoned updates in the model aggregation process. Extensive evaluation on multiple datasets demonstrate that FedReview can assist the server to learn a well-performed global model in an adversarial environment.</p></p class="citation"></blockquote><h3 id=4351--127279-interpreting-grokked-transformers-in-complex-modular-arithmetic-hiroki-furuta-et-al-2024>(43/51 | 127/279) Interpreting Grokked Transformers in Complex Modular Arithmetic (Hiroki Furuta et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hiroki Furuta, Gouki Minegishi, Yusuke Iwasawa, Yutaka Matsuo. (2024)<br><strong>Interpreting Grokked Transformers in Complex Modular Arithmetic</strong><br><button class=copy-to-clipboard title="Interpreting Grokked Transformers in Complex Modular Arithmetic" index=127>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-127 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16726v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16726v2.pdf filename=2402.16726v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Grokking has been actively explored to reveal the mystery of delayed generalization. Identifying interpretable algorithms inside the grokked models is a suggestive hint to understanding its mechanism. In this work, beyond the simplest and well-studied modular addition, we observe the internal circuits learned through grokking in complex modular arithmetic via interpretable reverse engineering, which highlights the significant difference in their dynamics: subtraction poses a strong asymmetry on <b>Transformer;</b> multiplication requires cosine-biased components at all the frequencies in a Fourier domain; polynomials often result in the superposition of the patterns from elementary arithmetic, but clear patterns do not emerge in challenging cases; grokking can easily occur even in higher-degree formulas with basic symmetric and alternating expressions. We also introduce the novel progress measure for modular arithmetic; Fourier Frequency Sparsity and Fourier Coefficient Ratio, which not only indicate the late generalization but also characterize distinctive internal representations of grokked models per modular operation. Our empirical analysis emphasizes the importance of holistic evaluation among various combinations.</p></p class="citation"></blockquote><h3 id=4451--128279-conformalized-selective-regression-anna-sokol-et-al-2024>(44/51 | 128/279) Conformalized Selective Regression (Anna Sokol et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Anna Sokol, Nuno Moniz, Nitesh Chawla. (2024)<br><strong>Conformalized Selective Regression</strong><br><button class=copy-to-clipboard title="Conformalized Selective Regression" index=128>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-128 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-ML<br>Keyword Score: 10<br>Keywords: Fairness<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16300v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16300v1.pdf filename=2402.16300v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Should prediction models always deliver a prediction? In the pursuit of maximum predictive performance, critical considerations of reliability and <b>fairness</b> are often overshadowed, particularly when it comes to the role of uncertainty. Selective regression, also known as the &ldquo;reject option,&rdquo; allows models to abstain from predictions in cases of considerable uncertainty. Initially proposed seven decades ago, approaches to selective regression have mostly focused on distribution-based proxies for measuring uncertainty, particularly conditional variance. However, this focus neglects the significant influence of model-specific biases on a model&rsquo;s performance. In this paper, we propose a novel approach to selective regression by leveraging conformal prediction, which provides grounded confidence measures for individual predictions based on model-specific biases. In addition, we propose a standardized evaluation framework to allow proper comparison of selective regression approaches. Via an extensive experimental approach, we demonstrate how our proposed approach, conformalized selective regression, demonstrates an advantage over multiple state-of-the-art baselines.</p></p class="citation"></blockquote><h3 id=4551--129279-learning-translations-emergent-communication-pretraining-for-cooperative-language-acquisition-dylan-cope-et-al-2024>(45/51 | 129/279) Learning Translations: Emergent Communication Pretraining for Cooperative Language Acquisition (Dylan Cope et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dylan Cope, Peter McBurney. (2024)<br><strong>Learning Translations: Emergent Communication Pretraining for Cooperative Language Acquisition</strong><br><button class=copy-to-clipboard title="Learning Translations: Emergent Communication Pretraining for Cooperative Language Acquisition" index=129>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-129 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CL, cs-LG, cs-MA, cs.LG<br>Keyword Score: 10<br>Keywords: Zero-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16247v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16247v1.pdf filename=2402.16247v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In Emergent Communication (EC) agents learn to communicate with one another, but the protocols that they develop are specialised to their training community. This observation led to research into <b>Zero-Shot</b> Coordination (ZSC) for learning communication strategies that are robust to agents not encountered during training. However, ZSC typically assumes that no prior data is available about the agents that will be encountered in the <b>zero-shot</b> setting. In many cases, this presents an unnecessarily hard problem and rules out communication via preestablished conventions. We propose a novel AI challenge called a Cooperative Language Acquisition Problem (CLAP) in which the ZSC assumptions are relaxed by allowing a &lsquo;joiner&rsquo; agent to learn from a dataset of interactions between agents in a target community. We propose and compare two methods for solving CLAPs: Imitation Learning (IL), and Emergent Communication pretraining and Translation Learning (ECTL), in which an agent is trained in self-play with EC and then learns from the data to translate between the emergent protocol and the target community&rsquo;s protocol.</p></p class="citation"></blockquote><h3 id=4651--130279-carte-pretraining-and-transfer-for-tabular-learning-myung-jun-kim-et-al-2024>(46/51 | 130/279) CARTE: pretraining and transfer for tabular learning (Myung Jun Kim et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Myung Jun Kim, Léo Grinsztajn, Gaël Varoquaux. (2024)<br><strong>CARTE: pretraining and transfer for tabular learning</strong><br><button class=copy-to-clipboard title="CARTE: pretraining and transfer for tabular learning" index=130>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-130 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 6<br>Keywords: Graph, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16785v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16785v1.pdf filename=2402.16785v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Pretrained deep-learning models are the go-to solution for images or text. However, for tabular data the standard is still to train tree-based models. Pre-training or transfer is a huge challenge as in general tables have columns about different quantities and naming conventions that vary vastly across sources. Data integration tackles correspondences across multiple sources: schema matching for columns, and entity matching for entries. We propose a neural architecture that does not need such matches. As a result, we can pretrain it on background data that has not been matched. The architecture - CARTE for Context Aware Representation of Table Entries - uses a <b>graph</b> representation of tabular (or relational) data to process tables with different columns, string embeddings of entries and columns names to model an open vocabulary, and a <b>graph-attentional</b> network to contextualize entries with column names and neighboring entries. An extensive <b>benchmark</b> shows that CARTE facilitates learning, outperforming a solid set of baselines including the best tree-based models. CARTE also enables joint learning across tables with unmatched columns, enhancing a small table with bigger ones. CARTE opens the door to large pretrained models embarking information for tabular data.</p></p class="citation"></blockquote><h3 id=4751--131279-partial-rankings-of-optimizers-julian-rodemann-et-al-2024>(47/51 | 131/279) Partial Rankings of Optimizers (Julian Rodemann et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Julian Rodemann, Hannah Blocher. (2024)<br><strong>Partial Rankings of Optimizers</strong><br><button class=copy-to-clipboard title="Partial Rankings of Optimizers" index=131>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-131 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-ML<br>Keyword Score: 6<br>Keywords: Benchmarking, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16565v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16565v1.pdf filename=2402.16565v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce a framework for <b>benchmarking</b> optimizers according to multiple criteria over various test functions. Based on a recently introduced union-free generic depth function for partial orders/rankings, it fully exploits the ordinal information and allows for incomparability. Our method describes the distribution of all partial orders/rankings, avoiding the notorious shortcomings of aggregation. This permits to identify test functions that produce central or outlying rankings of optimizers and to assess the quality of <b>benchmarking</b> suites.</p></p class="citation"></blockquote><h3 id=4851--132279-label-learning-method-based-on-tensor-projection-jing-li-et-al-2024>(48/51 | 132/279) Label Learning Method Based on Tensor Projection (Jing Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jing Li, Quanxue Gao, Qianqian Wang, Cheng Deng, Deyan Xie. (2024)<br><strong>Label Learning Method Based on Tensor Projection</strong><br><button class=copy-to-clipboard title="Label Learning Method Based on Tensor Projection" index=132>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-132 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 6<br>Keywords: Graph, Clustering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16544v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16544v1.pdf filename=2402.16544v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Multi-view <b>clustering</b> method based on anchor <b>graph</b> has been widely concerned due to its high efficiency and effectiveness. In order to avoid post-processing, most of the existing anchor <b>graph-based</b> methods learn bipartite <b>graphs</b> with connected components. However, such methods have high requirements on parameters, and in some cases it may not be possible to obtain bipartite <b>graphs</b> with clear connected components. To end this, we propose a label learning method based on tensor projection (LLMTP). Specifically, we project anchor <b>graph</b> into the label space through an orthogonal projection matrix to obtain cluster labels directly. Considering that the spatial structure information of multi-view data may be ignored to a certain extent when projected in different views separately, we extend the matrix projection transformation to tensor projection, so that the spatial structure information between views can be fully utilized. In addition, we introduce the tensor Schatten $p$-norm regularization to make the <b>clustering</b> label matrices of different views as consistent as possible. Extensive experiments have proved the effectiveness of the proposed method.</p></p class="citation"></blockquote><h3 id=4951--133279-active-level-set-estimation-for-continuous-search-space-with-theoretical-guarantee-giang-ngo-et-al-2024>(49/51 | 133/279) Active Level Set Estimation for Continuous Search Space with Theoretical Guarantee (Giang Ngo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Giang Ngo, Dang Nguyen, Dat Phan-Trong, Sunil Gupta. (2024)<br><strong>Active Level Set Estimation for Continuous Search Space with Theoretical Guarantee</strong><br><button class=copy-to-clipboard title="Active Level Set Estimation for Continuous Search Space with Theoretical Guarantee" index=133>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-133 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 5<br>Keywords: Black Box<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16237v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16237v1.pdf filename=2402.16237v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A common problem encountered in many real-world applications is level set estimation where the goal is to determine the region in the function domain where the function is above or below a given threshold. When the function is <b>black-box</b> <b>and</b> expensive to evaluate, the level sets need to be found in a minimum set of function evaluations. Existing methods often assume a discrete search space with a finite set of data points for function evaluations and estimating the level sets. When applied to a continuous search space, these methods often need to first discretize the space which leads to poor results while needing high computational time. While some methods cater for the continuous setting, they still lack a proper guarantee for theoretical convergence. To address this problem, we propose a novel algorithm that does not need any discretization and can directly work in continuous search spaces. Our method suggests points by constructing an acquisition function that is defined as a measure of confidence of the function being higher or lower than the given threshold. A theoretical analysis for the convergence of the algorithm to an accurate solution is provided. On multiple synthetic and real-world datasets, our algorithm successfully outperforms state-of-the-art methods.</p></p class="citation"></blockquote><h3 id=5051--134279-a-multi-fidelity-methodology-for-reduced-order-models-with-high-dimensional-inputs-bilal-mufti-et-al-2024>(50/51 | 134/279) A Multi-Fidelity Methodology for Reduced Order Models with High-Dimensional Inputs (Bilal Mufti et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bilal Mufti, Christian Perron, Dimitri N. Mavris. (2024)<br><strong>A Multi-Fidelity Methodology for Reduced Order Models with High-Dimensional Inputs</strong><br><button class=copy-to-clipboard title="A Multi-Fidelity Methodology for Reduced Order Models with High-Dimensional Inputs" index=134>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-134 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 3<br>Keywords: Sample Size<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17061v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17061v1.pdf filename=2402.17061v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the early stages of aerospace design, reduced order models (ROMs) are crucial for minimizing computational costs associated with using physics-rich field information in many-query scenarios requiring multiple evaluations. The intricacy of aerospace design demands the use of high-dimensional design spaces to capture detailed features and design variability accurately. However, these spaces introduce significant challenges, including the curse of dimensionality, which stems from both high-dimensional inputs and outputs necessitating substantial training data and computational effort. To address these complexities, this study introduces a novel multi-fidelity, parametric, and non-intrusive ROM framework designed for high-dimensional contexts. It integrates machine learning techniques for manifold alignment and dimension reduction employing Proper Orthogonal Decomposition (POD) and Model-based Active Subspace with multi-fidelity regression for ROM construction. Our approach is validated through two test cases: the 2D RAE~2822 airfoil and the 3D NASA CRM wing, assessing combinations of various fidelity levels, training data ratios, and <b>sample</b> <b>sizes.</b> Compared to the single-fidelity PCAS method, our multi-fidelity solution offers improved cost-accuracy benefits and achieves better predictive accuracy with reduced computational demands. Moreover, our methodology outperforms the manifold-aligned ROM (MA-ROM) method by 50% in handling scenarios with large input dimensions, underscoring its efficacy in addressing the complex challenges of aerospace design.</p></p class="citation"></blockquote><h3 id=5151--135279-dagnosis-localized-identification-of-data-inconsistencies-using-structures-nicolas-huynh-et-al-2024>(51/51 | 135/279) DAGnosis: Localized Identification of Data Inconsistencies using Structures (Nicolas Huynh et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nicolas Huynh, Jeroen Berrevoets, Nabeel Seedat, Jonathan Crabbé, Zhaozhi Qian, Mihaela van der Schaar. (2024)<br><strong>DAGnosis: Localized Identification of Data Inconsistencies using Structures</strong><br><button class=copy-to-clipboard title="DAGnosis: Localized Identification of Data Inconsistencies using Structures" index=135>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-135 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG, stat-ML<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17599v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17599v2.pdf filename=2402.17599v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Identification and appropriate handling of inconsistencies in data at deployment time is crucial to reliably use machine learning models. While recent data-centric methods are able to identify such inconsistencies with respect to the training set, they suffer from two key limitations: (1) suboptimality in settings where features exhibit statistical independencies, due to their usage of compressive representations and (2) lack of localization to pin-point why a sample might be flagged as inconsistent, which is important to guide future data collection. We solve these two fundamental limitations using directed acyclic <b>graphs</b> (DAGs) to encode the training set&rsquo;s features probability distribution and independencies as a structure. Our method, called DAGnosis, leverages these structural interactions to bring valuable and insightful data-centric conclusions. DAGnosis unlocks the localization of the causes of inconsistencies on a DAG, an aspect overlooked by previous approaches. Moreover, we show empirically that leveraging these interactions (1) leads to more accurate conclusions in detecting inconsistencies, as well as (2) provides more detailed insights into why some samples are flagged.</p></p class="citation"></blockquote><h2 id=cscv-40>cs.CV (40)</h2><h3 id=140--136279-few-shot-learning-for-annotation-efficient-nucleus-instance-segmentation-yu-ming-et-al-2024>(1/40 | 136/279) Few-Shot Learning for Annotation-Efficient Nucleus Instance Segmentation (Yu Ming et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yu Ming, Zihao Wu, Jie Yang, Danyi Li, Yuan Gao, Changxin Gao, Gui-Song Xia, Yuanqing Li, Li Liang, Jin-Gang Yu. (2024)<br><strong>Few-Shot Learning for Annotation-Efficient Nucleus Instance Segmentation</strong><br><button class=copy-to-clipboard title="Few-Shot Learning for Annotation-Efficient Nucleus Instance Segmentation" index=136>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-136 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 80<br>Keywords: Adversarial Learning, Few-shot, Few-shot Learning, Meta Learning, Semi-Supervised Learning, Supervised Learning, Supervised Learning, Transfer Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16280v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16280v2.pdf filename=2402.16280v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Nucleus instance segmentation from histopathology images suffers from the extremely laborious and expert-dependent annotation of nucleus instances. As a promising solution to this task, annotation-efficient deep learning paradigms have recently attracted much research interest, such as weakly-/semi-supervised learning, generative <b>adversarial</b> <b>learning,</b> etc. In this paper, we propose to formulate annotation-efficient nucleus instance segmentation from the perspective of <b>few-shot</b> <b>learning</b> (FSL). Our work was motivated by that, with the prosperity of computational pathology, an increasing number of fully-annotated datasets are publicly accessible, and we hope to leverage these external datasets to assist nucleus instance segmentation on the target dataset which only has very limited annotation. To achieve this goal, we adopt the <b>meta-learning</b> <b>based</b> FSL paradigm, which however has to be tailored in two substantial aspects before adapting to our task. First, since the novel classes may be inconsistent with those of the external dataset, we extend the basic definition of <b>few-shot</b> <b>instance</b> segmentation (FSIS) to generalized <b>few-shot</b> <b>instance</b> segmentation (GFSIS). Second, to cope with the intrinsic challenges of nucleus segmentation, including touching between adjacent cells, cellular heterogeneity, etc., we further introduce a structural guidance mechanism into the GFSIS network, finally leading to a unified Structurally-Guided Generalized <b>Few-Shot</b> <b>Instance</b> Segmentation (SGFSIS) framework. Extensive experiments on a couple of publicly accessible datasets demonstrate that, SGFSIS can outperform other annotation-efficient learning baselines, including <b>semi-supervised</b> <b>learning,</b> simple <b>transfer</b> <b>learning,</b> etc., with comparable performance to fully <b>supervised</b> <b>learning</b> with less than 5% annotations.</p></p class="citation"></blockquote><h3 id=240--137279-consept-continual-semantic-segmentation-via-adapter-based-vision-transformer-bowen-dong-et-al-2024>(2/40 | 137/279) ConSept: Continual Semantic Segmentation via Adapter-based Vision Transformer (Bowen Dong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bowen Dong, Guanglei Yang, Wangmeng Zuo, Lei Zhang. (2024)<br><strong>ConSept: Continual Semantic Segmentation via Adapter-based Vision Transformer</strong><br><button class=copy-to-clipboard title="ConSept: Continual Semantic Segmentation via Adapter-based Vision Transformer" index=137>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-137 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 53<br>Keywords: Vision Transformer, Benchmarking, Fine-tuning, Knowledge Distillation, Transformer, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16674v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16674v1.pdf filename=2402.16674v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we delve into the realm of <b>vision</b> <b>transformers</b> for continual semantic segmentation, a problem that has not been sufficiently explored in previous literature. Empirical investigations on the adaptation of existing frameworks to vanilla ViT reveal that incorporating visual adapters into ViTs or <b>fine-tuning</b> ViTs with <b>distillation</b> terms is advantageous for enhancing the segmentation capability of novel classes. These findings motivate us to propose Continual semantic Segmentation via Adapter-based ViT, namely ConSept. Within the simplified architecture of ViT with linear segmentation head, ConSept integrates lightweight attention-based adapters into vanilla ViTs. Capitalizing on the feature adaptation abilities of these adapters, ConSept not only retains superior segmentation ability for old classes, but also attains promising segmentation quality for novel classes. To further harness the intrinsic anti-catastrophic forgetting ability of ConSept and concurrently enhance the segmentation capabilities for both old and new classes, we propose two key strategies: <b>distillation</b> with a deterministic old-classes boundary for improved anti-catastrophic forgetting, and dual dice losses to regularize segmentation maps, thereby improving overall segmentation performance. Extensive experiments show the effectiveness of ConSept on multiple continual semantic segmentation <b>benchmarks</b> under overlapped or disjoint settings. Code will be publicly available at \url{https://github.com/DongSky/ConSept}.</p></p class="citation"></blockquote><h3 id=340--138279-generative-ai-in-vision-a-survey-on-models-metrics-and-applications-gaurav-raut-et-al-2024>(3/40 | 138/279) Generative AI in Vision: A Survey on Models, Metrics and Applications (Gaurav Raut et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Gaurav Raut, Apoorv Singh. (2024)<br><strong>Generative AI in Vision: A Survey on Models, Metrics and Applications</strong><br><button class=copy-to-clipboard title="Generative AI in Vision: A Survey on Models, Metrics and Applications" index=138>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-138 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-LG, cs.CV<br>Keyword Score: 50<br>Keywords: Diffusion Model, Data Augmentation, Generative AI, Probabilistic Model, Text2image<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16369v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16369v1.pdf filename=2402.16369v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Generative</b> <b>AI</b> models have revolutionized various fields by enabling the creation of realistic and diverse <b>data</b> <b>samples.</b> Among these models, <b>diffusion</b> <b>models</b> have emerged as a powerful approach for generating high-quality images, text, and audio. This survey paper provides a comprehensive overview of <b>generative</b> <b>AI</b> <b>diffusion</b> <b>and</b> legacy models, focusing on their underlying techniques, applications across different domains, and their challenges. We delve into the theoretical foundations of <b>diffusion</b> <b>models,</b> including concepts such as denoising <b>diffusion</b> <b>probabilistic</b> <b>models</b> (DDPM) and score-based <b>generative</b> <b>modeling.</b> Furthermore, we explore the diverse applications of these models in <b>text-to-image,</b> image inpainting, and image super-resolution, along with others, showcasing their potential in creative tasks and <b>data</b> <b>augmentation.</b> By synthesizing existing research and highlighting critical advancements in this field, this survey aims to provide researchers and practitioners with a comprehensive understanding of <b>generative</b> <b>AI</b> <b>diffusion</b> <b>and</b> legacy models and inspire future innovations in this exciting area of artificial intelligence.</p></p class="citation"></blockquote><h3 id=440--139279-groundhog-grounding-large-language-models-to-holistic-segmentation-yichi-zhang-et-al-2024>(4/40 | 139/279) GROUNDHOG: Grounding Large Language Models to Holistic Segmentation (Yichi Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yichi Zhang, Ziqiao Ma, Xiaofeng Gao, Suhaila Shakiah, Qiaozi Gao, Joyce Chai. (2024)<br><strong>GROUNDHOG: Grounding Large Language Models to Holistic Segmentation</strong><br><button class=copy-to-clipboard title="GROUNDHOG: Grounding Large Language Models to Holistic Segmentation" index=139>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-139 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CL, cs-CV, cs.CV<br>Keyword Score: 46<br>Keywords: Fine-tuning, Multi-modal, Multi-modal, Grounding, Instruction Tuning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16846v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16846v1.pdf filename=2402.16846v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Most <b>multimodal</b> <b>large</b> <b>language</b> <b>models</b> (MLLMs) learn language-to-object <b>grounding</b> through causal language modeling where grounded objects are captured by bounding boxes as sequences of location tokens. This paradigm lacks pixel-level representations that are important for fine-grained visual understanding and diagnosis. In this work, we introduce GROUNDHOG, an MLLM developed by <b>grounding</b> <b>Large</b> <b>Language</b> <b>Models</b> to holistic segmentation. GROUNDHOG incorporates a masked feature extractor and converts extracted features into visual entity tokens for the MLLM backbone, which then connects groundable phrases to unified <b>grounding</b> masks by retrieving and merging the entity masks. To train GROUNDHOG, we carefully curated M3G2, a grounded visual <b>instruction</b> <b>tuning</b> dataset with <b>Multi-Modal</b> Multi-Grained <b>Grounding,</b> by harvesting a collection of segmentation-grounded datasets with rich annotations. Our experimental results show that GROUNDHOG achieves superior performance on various language <b>grounding</b> tasks without task-specific <b>fine-tuning,</b> and significantly reduces object hallucination. GROUNDHOG also demonstrates better <b>grounding</b> towards complex forms of visual input and provides easy-to-understand diagnosis in failure cases.</p></p class="citation"></blockquote><h3 id=540--140279-placing-objects-in-context-via-inpainting-for-out-of-distribution-segmentation-pau-de-jorge-et-al-2024>(5/40 | 140/279) Placing Objects in Context via Inpainting for Out-of-distribution Segmentation (Pau de Jorge et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Pau de Jorge, Riccardo Volpi, Puneet K. Dokania, Philip H. S. Torr, Gregory Rogez. (2024)<br><strong>Placing Objects in Context via Inpainting for Out-of-distribution Segmentation</strong><br><button class=copy-to-clipboard title="Placing Objects in Context via Inpainting for Out-of-distribution Segmentation" index=140>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-140 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 43<br>Keywords: Diffusion Model, Benchmarking, Fine-tuning, Out-of-distribution, In-context Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16392v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16392v1.pdf filename=2402.16392v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>When deploying a semantic segmentation model into the real world, it will inevitably be confronted with semantic classes unseen during training. Thus, to safely deploy such systems, it is crucial to accurately evaluate and improve their anomaly segmentation capabilities. However, acquiring and labelling semantic segmentation data is expensive and unanticipated conditions are long-tail and potentially hazardous. Indeed, existing anomaly segmentation datasets capture a limited number of anomalies, lack realism or have strong domain shifts. In this paper, we propose the Placing Objects in Context (POC) pipeline to realistically add any object into any image via <b>diffusion</b> <b>models.</b> POC can be used to easily extend any dataset with an arbitrary number of objects. In our experiments, we present different anomaly segmentation datasets based on POC-generated data and show that POC can improve the performance of recent state-of-the-art anomaly <b>fine-tuning</b> methods in several standardized <b>benchmarks.</b> POC is also effective to learn new classes. For example, we use it to edit Cityscapes samples by adding a subset of Pascal classes and show that models trained on such data achieve comparable performance to the Pascal-trained baseline. This corroborates the low sim-to-real gap of models trained on POC-generated images.</p></p class="citation"></blockquote><h3 id=640--141279-finer-investigating-and-enhancing-fine-grained-visual-concept-recognition-in-large-vision-language-models-jeonghwan-kim-et-al-2024>(6/40 | 141/279) Finer: Investigating and Enhancing Fine-Grained Visual Concept Recognition in Large Vision Language Models (Jeonghwan Kim et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jeonghwan Kim, Heng Ji. (2024)<br><strong>Finer: Investigating and Enhancing Fine-Grained Visual Concept Recognition in Large Vision Language Models</strong><br><button class=copy-to-clipboard title="Finer: Investigating and Enhancing Fine-Grained Visual Concept Recognition in Large Vision Language Models" index=141>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-141 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CL, cs-CV, cs.CV<br>Keyword Score: 43<br>Keywords: Benchmarking, GPT, Large Language Model, Large Language Model, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16315v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16315v1.pdf filename=2402.16315v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advances in instruction-tuned <b>Large</b> <b>Vision-Language</b> <b>Models</b> (LVLMs) have imbued the models with the ability to generate high-level, image-grounded explanations with ease. While such capability is largely attributed to the rich world knowledge contained within the <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs),</b> our work reveals their shortcomings in fine-grained visual categorization (FGVC) across six different <b>benchmark</b> settings. Most recent state-of-the-art LVLMs like LLaVa-1.5, InstructBLIP and <b>GPT-4V</b> not only severely deteriorate in terms of classification performance, e.g., average drop of 65.58 in EM for Stanford Dogs for LLaVA-1.5, but also struggle to generate an accurate explanation with detailed attributes based on the concept that appears within an input image despite their capability to generate holistic image-level descriptions. In-depth analyses show that instruction-tuned LVLMs exhibit modality gap, showing discrepancy when given textual and visual inputs that correspond to the same concept, preventing the image modality from leveraging the rich parametric knowledge within the <b>LLMs.</b> In an effort to further the community&rsquo;s endeavor in this direction, we propose a multiple granularity attribute-centric evaluation <b>benchmark,</b> Finer, which aims to establish a ground to evaluate LVLMs&rsquo; fine-grained visual comprehension ability and provide significantly improved explainability.</p></p class="citation"></blockquote><h3 id=740--142279-blo-sam-bi-level-optimization-based-overfitting-preventing-finetuning-of-sam-li-zhang-et-al-2024>(7/40 | 142/279) BLO-SAM: Bi-level Optimization Based Overfitting-Preventing Finetuning of SAM (Li Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Li Zhang, Youwei Liang, Ruiyi Zhang, Pengtao Xie. (2024)<br><strong>BLO-SAM: Bi-level Optimization Based Overfitting-Preventing Finetuning of SAM</strong><br><button class=copy-to-clipboard title="BLO-SAM: Bi-level Optimization Based Overfitting-Preventing Finetuning of SAM" index=142>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-142 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Fine-tuning, Fine-tuning, Foundation Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16338v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16338v2.pdf filename=2402.16338v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The Segment Anything Model (SAM), a <b>foundation</b> <b>model</b> pretrained on millions of images and segmentation masks, has significantly advanced semantic segmentation, a fundamental task in computer vision. Despite its strengths, SAM encounters two major challenges. Firstly, it struggles with segmenting specific objects autonomously, as it relies on users to manually input <b>prompts</b> like points or bounding boxes to identify targeted objects. Secondly, SAM faces challenges in excelling at specific downstream tasks, like medical imaging, due to a disparity between the distribution of its pretraining data, which predominantly consists of general-domain images, and the data used in downstream tasks. Current solutions to these problems, which involve <b>finetuning</b> SAM, often lead to overfitting, a notable issue in scenarios with very limited data, like in medical imaging. To overcome these limitations, we introduce BLO-SAM, which <b>finetunes</b> SAM based on bi-level optimization (BLO). Our approach allows for automatic image segmentation without the need for manual <b>prompts,</b> by optimizing a learnable <b>prompt</b> embedding. Furthermore, it significantly reduces the risk of overfitting by training the model&rsquo;s weight parameters and the <b>prompt</b> embedding on two separate subsets of the training dataset, each at a different level of optimization. We apply BLO-SAM to diverse semantic segmentation tasks in general and medical domains. The results demonstrate BLO-SAM&rsquo;s superior performance over various state-of-the-art image semantic segmentation methods.</p></p class="citation"></blockquote><h3 id=840--143279-offline-writer-identification-using-convolutional-neural-network-activation-features-vincent-christlein-et-al-2024>(8/40 | 143/279) Offline Writer Identification Using Convolutional Neural Network Activation Features (Vincent Christlein et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Vincent Christlein, David Bernecker, Andreas Maier, Elli Angelopoulou. (2024)<br><strong>Offline Writer Identification Using Convolutional Neural Network Activation Features</strong><br><button class=copy-to-clipboard title="Offline Writer Identification Using Convolutional Neural Network Activation Features" index=143>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-143 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 33<br>Keywords: Benchmarking, Convolution, Convolutional Neural Network, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17029v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17029v1.pdf filename=2402.17029v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Convolutional</b> <b>neural</b> <b>networks</b> <b>(CNNs)</b> have recently become the state-of-the-art tool for large-scale image classification. In this work we propose the use of activation features from <b>CNNs</b> as local descriptors for writer identification. A global descriptor is then formed by means of GMM supervector encoding, which is further improved by normalization with the KL-Kernel. We evaluate our method on two publicly available datasets: the ICDAR 2013 <b>benchmark</b> database and the CVL dataset. While we perform comparably to the state of the art on CVL, our proposed method yields about 0.21 absolute improvement in terms of mAP on the challenging bilingual ICDAR dataset.</p></p class="citation"></blockquote><h3 id=940--144279-saliency-aware-automatic-buddhas-statue-recognition-yong-qi-et-al-2024>(9/40 | 144/279) Saliency-Aware Automatic Buddhas Statue Recognition (Yong Qi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yong Qi, Fanghan Zhao. (2024)<br><strong>Saliency-Aware Automatic Buddhas Statue Recognition</strong><br><button class=copy-to-clipboard title="Saliency-Aware Automatic Buddhas Statue Recognition" index=144>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-144 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Convolution, Convolutional Neural Network, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16980v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16980v1.pdf filename=2402.16980v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Buddha statues, as a symbol of many religions, have significant cultural implications that are crucial for understanding the culture and history of different regions, and the recognition of Buddha statues is therefore the pivotal link in the field of Buddha study. However, the Buddha statue recognition requires extensive time and effort from knowledgeable professionals, making it a costly task to perform. <b>Convolution</b> neural networks <b>(CNNs)</b> are inherently efficient at processing visual information, but <b>CNNs</b> alone are likely to make inaccurate classification decisions when subjected to the class imbalance problem. Therefore, this paper proposes an end-to-end automatic Buddha statue recognition model based on saliency map sampling. The proposed Grid-Wise Local <b>Self-Attention</b> Module (GLSA) provides extra salient features which can serve to enrich the dataset and allow <b>CNNs</b> to observe in a much more comprehensive way. Eventually, our model is evaluated on a Buddha dataset collected with the aid of Buddha experts and outperforms state-of-the-art networks in terms of Top-1 accuracy by 4.63% on average, while only marginally increasing MUL-ADD.</p></p class="citation"></blockquote><h3 id=1040--145279-weighted-monte-carlo-augmented-spherical-fourier-bessel-convolutional-layers-for-3d-abdominal-organ-segmentation-wenzhao-zhao-et-al-2024>(10/40 | 145/279) Weighted Monte Carlo augmented spherical Fourier-Bessel convolutional layers for 3D abdominal organ segmentation (Wenzhao Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wenzhao Zhao, Steffen Albert, Barbara D. Wichtmann, Angelika Maurer, Ulrike Attenberger, Frank G. Zöllner, Jürgen Hesser. (2024)<br><strong>Weighted Monte Carlo augmented spherical Fourier-Bessel convolutional layers for 3D abdominal organ segmentation</strong><br><button class=copy-to-clipboard title="Weighted Monte Carlo augmented spherical Fourier-Bessel convolutional layers for 3D abdominal organ segmentation" index=145>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-145 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Convolution, Convolutional Neural Network, Parameter Sharing<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16825v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16825v2.pdf filename=2402.16825v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Filter-decomposition-based group equivariant <b>convolutional</b> <b>neural</b> <b>networks</b> show promising stability and data efficiency for 3D image feature extraction. However, the existing filter-decomposition-based 3D group equivariant neural networks rely on <b>parameter-sharing</b> <b>designs</b> and are mostly limited to rotation transform groups, where the chosen spherical harmonic filter bases consider only angular orthogonality. These limitations hamper its application to deep neural network architectures for medical image segmentation. To address these issues, this paper describes a non-parameter-sharing affine group equivariant neural network for 3D medical image segmentation based on an adaptive aggregation of Monte Carlo augmented spherical Fourier Bessel filter bases. The efficiency and flexibility of the adopted non-parameter strategy enable for the first time an efficient implementation of 3D affine group equivariant <b>convolutional</b> <b>neural</b> <b>networks</b> for volumetric data. The introduced spherical Bessel Fourier filter basis combines both angular and radial orthogonality for better feature extraction. The 3D image segmentation experiments on two abdominal image sets, BTCV and the NIH Pancreas datasets, show that the proposed methods excel the state-of-the-art 3D neural networks with high training stability and data efficiency. The code will be available at <a href=https://github.com/ZhaoWenzhao/WVMS>https://github.com/ZhaoWenzhao/WVMS</a>.</p></p class="citation"></blockquote><h3 id=1140--146279-intelligent-known-and-novel-aircraft-recognition----a-shift-from-classification-to-similarity-learning-for-combat-identification-ahmad-saeed-et-al-2024>(11/40 | 146/279) Intelligent Known and Novel Aircraft Recognition &ndash; A Shift from Classification to Similarity Learning for Combat Identification (Ahmad Saeed et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ahmad Saeed, Haasha Bin Atif, Usman Habib, Mohsin Bilal. (2024)<br><strong>Intelligent Known and Novel Aircraft Recognition &ndash; A Shift from Classification to Similarity Learning for Combat Identification</strong><br><button class=copy-to-clipboard title="Intelligent Known and Novel Aircraft Recognition -- A Shift from Classification to Similarity Learning for Combat Identification" index=146>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-146 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Few-shot, Few-shot Learning, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16486v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16486v1.pdf filename=2402.16486v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Precise aircraft recognition in low-resolution remote sensing imagery is a challenging yet crucial task in aviation, especially combat identification. This research addresses this problem with a novel, scalable, and AI-driven solution. The primary hurdle in combat identification in remote sensing imagery is the accurate recognition of Novel/Unknown types of aircraft in addition to Known types. Traditional methods, human expert-driven combat identification and image classification, fall short in identifying Novel classes. Our methodology employs similarity learning to discern features of a broad spectrum of military and civilian aircraft. It discerns both Known and Novel aircraft types, leveraging metric learning for the identification and <b>supervised</b> <b>few-shot</b> <b>learning</b> for aircraft type classification. To counter the challenge of limited low-resolution remote sensing data, we propose an end-to-end framework that adapts to the diverse and versatile process of military aircraft recognition by training a generalized embedder in fully <b>supervised</b> manner. Comparative analysis with earlier aircraft image classification methods shows that our approach is effective for aircraft image classification (F1-score Aircraft Type of 0.861) and pioneering for quantifying the identification of Novel types (F1-score Bipartitioning of 0.936). The proposed methodology effectively addresses inherent challenges in remote sensing data, thereby setting new standards in dataset quality. The research opens new avenues for domain experts and demonstrates unique capabilities in distinguishing various aircraft types, contributing to a more robust, domain-adapted potential for real-time aircraft recognition.</p></p class="citation"></blockquote><h3 id=1240--147279-deyo-detr-with-yolo-for-end-to-end-object-detection-haodong-ouyang-2024>(12/40 | 147/279) DEYO: DETR with YOLO for End-to-End Object Detection (Haodong Ouyang, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haodong Ouyang. (2024)<br><strong>DEYO: DETR with YOLO for End-to-End Object Detection</strong><br><button class=copy-to-clipboard title="DEYO: DETR with YOLO for End-to-End Object Detection" index=147>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-147 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Yolo, Object Detection, Convolution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16370v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16370v1.pdf filename=2402.16370v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The training paradigm of DETRs is heavily contingent upon pre-training their backbone on the ImageNet dataset. However, the limited supervisory signals provided by the image classification task and one-to-one matching strategy result in an inadequately pre-trained neck for DETRs. Additionally, the instability of matching in the early stages of training engenders inconsistencies in the optimization objectives of DETRs. To address these issues, we have devised an innovative training methodology termed step-by-step training. Specifically, in the first stage of training, we employ a classic detector, pre-trained with a one-to-many matching strategy, to initialize the backbone and neck of the end-to-end detector. In the second stage of training, we froze the backbone and neck of the end-to-end detector, necessitating the training of the decoder from scratch. Through the application of step-by-step training, we have introduced the first real-time end-to-end <b>object</b> <b>detection</b> model that utilizes a purely <b>convolutional</b> structure encoder, DETR with <b>YOLO</b> (DEYO). Without reliance on any supplementary training data, DEYO surpasses all existing real-time <b>object</b> <b>detectors</b> in both speed and accuracy. Moreover, the comprehensive DEYO series can complete its second-phase training on the COCO dataset using a single 8GB RTX 4060 GPU, significantly reducing the training expenditure. Source code and pre-trained models are available at <a href=https://github.com/ouyanghaodong/DEYO>https://github.com/ouyanghaodong/DEYO</a>.</p></p class="citation"></blockquote><h3 id=1340--148279-mapm-multi-scale-attention-pyramid-module-for-enhanced-scale-variation-in-rld-detection-yunusa-haruna-et-al-2024>(13/40 | 148/279) mAPm: multi-scale Attention Pyramid module for Enhanced scale-variation in RLD detection (Yunusa Haruna et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yunusa Haruna, Shiyin Qin, Abdulrahman Hamman Adama Chukkol, Isah Bello, Adamu Lawan. (2024)<br><strong>mAPm: multi-scale Attention Pyramid module for Enhanced scale-variation in RLD detection</strong><br><button class=copy-to-clipboard title="mAPm: multi-scale Attention Pyramid module for Enhanced scale-variation in RLD detection" index=148>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-148 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Object Detection, Convolution, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16291v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16291v1.pdf filename=2402.16291v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Detecting <b>objects</b> <b>across</b> various scales remains a significant challenge in computer vision, particularly in tasks such as Rice Leaf Disease (RLD) detection, where <b>objects</b> <b>exhibit</b> considerable scale variations. Traditional <b>object</b> <b>detection</b> methods often struggle to address these variations, resulting in missed detections or reduced accuracy. In this study, we propose the multi-scale Attention Pyramid module (mAPm), a novel approach that integrates dilated <b>convolutions</b> into the Feature Pyramid Network (FPN) to enhance multi-scale information ex-traction. Additionally, we incorporate a global Multi-Head <b>Self-Attention</b> (MHSA) mechanism and a deconvolutional layer to refine the up-sampling process. We evaluate mAPm on YOLOv7 using the MRLD and COCO datasets. Compared to vanilla FPN, BiFPN, NAS-FPN, PANET, and ACFPN, mAPm achieved a significant improvement in Average Precision (AP), with a +2.61% increase on the MRLD dataset compared to the baseline FPN method in YOLOv7. This demonstrates its effectiveness in handling scale variations. Furthermore, the versatility of mAPm allows its integration into various FPN-based <b>object</b> <b>detection</b> models, showcasing its potential to advance <b>object</b> <b>detection</b> techniques.</p></p class="citation"></blockquote><h3 id=1440--149279-automated-floodwater-depth-estimation-using-large-multimodal-model-for-rapid-flood-mapping-temitope-akinboyewa-et-al-2024>(14/40 | 149/279) Automated Floodwater Depth Estimation Using Large Multimodal Model for Rapid Flood Mapping (Temitope Akinboyewa et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Temitope Akinboyewa, Huan Ning, M. Naser Lessani, Zhenlong Li. (2024)<br><strong>Automated Floodwater Depth Estimation Using Large Multimodal Model for Rapid Flood Mapping</strong><br><button class=copy-to-clipboard title="Automated Floodwater Depth Estimation Using Large Multimodal Model for Rapid Flood Mapping" index=149>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-149 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 26<br>Keywords: Multi-modal, Multi-modal, GPT, GPT-4<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16684v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16684v1.pdf filename=2402.16684v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Information on the depth of floodwater is crucial for rapid mapping of areas affected by floods. However, previous approaches for estimating floodwater depth, including field surveys, remote sensing, and machine learning techniques, can be time-consuming and resource-intensive. This paper presents an automated and fast approach for estimating floodwater depth from on-site flood photos. A pre-trained large <b>multimodal</b> model, <b>GPT-4</b> Vision, was used specifically for estimating floodwater. The input data were flooding photos that contained referenced objects, such as street signs, cars, people, and buildings. Using the heights of the common objects as references, the model returned the floodwater depth as the output. Results show that the proposed approach can rapidly provide a consistent and reliable estimation of floodwater depth from flood photos. Such rapid estimation is transformative in flood inundation mapping and assessing the severity of the flood in near-real time, which is essential for effective flood response strategies.</p></p class="citation"></blockquote><h3 id=1540--150279-cursor-scalable-mixed-order-hypergraph-matching-with-cur-decomposition-qixuan-zheng-et-al-2024>(15/40 | 150/279) CURSOR: Scalable Mixed-Order Hypergraph Matching with CUR Decomposition (Qixuan Zheng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qixuan Zheng, Ming Zhang, Hong Yan. (2024)<br><strong>CURSOR: Scalable Mixed-Order Hypergraph Matching with CUR Decomposition</strong><br><button class=copy-to-clipboard title="CURSOR: Scalable Mixed-Order Hypergraph Matching with CUR Decomposition" index=150>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-150 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 26<br>Keywords: Graph, Benchmarking, Knowledge Distillation, Tensor Decomposition<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16594v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16594v1.pdf filename=2402.16594v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>To achieve greater accuracy, hypergraph matching algorithms require exponential increases in computational resources. Recent <b>kd-tree-based</b> approximate nearest neighbor (ANN) methods, despite the sparsity of their compatibility <b>tensor,</b> <b>still</b> require exhaustive calculations for large-scale <b>graph</b> matching. This work utilizes CUR <b>tensor</b> <b>decomposition</b> and introduces a novel cascaded second and third-order hypergraph matching framework (CURSOR) for efficient hypergraph matching. A CUR-based second-order <b>graph</b> matching algorithm is used to provide a rough match, and then the core of CURSOR, a fiber-CUR-based <b>tensor</b> <b>generation</b> method, directly calculates entries of the compatibility <b>tensor</b> <b>by</b> leveraging the initial second-order match result. This significantly decreases the time complexity and <b>tensor</b> <b>density.</b> A probability relaxation labeling (PRL)-based matching algorithm, specifically suitable for sparse <b>tensors,</b> <b>is</b> developed. Experiment results on large-scale synthetic datasets and widely-adopted <b>benchmark</b> sets demonstrate the superiority of CURSOR over existing methods. The <b>tensor</b> <b>generation</b> method in CURSOR can be integrated seamlessly into existing hypergraph matching methods to improve their performance and lower their computational costs.</p></p class="citation"></blockquote><h3 id=1640--151279-taming-the-tail-in-class-conditional-gans-knowledge-sharing-via-unconditional-training-at-lower-resolutions-saeed-khorram-et-al-2024>(16/40 | 151/279) Taming the Tail in Class-Conditional GANs: Knowledge Sharing via Unconditional Training at Lower Resolutions (Saeed Khorram et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Saeed Khorram, Mingqi Jiang, Mohamad Shahbazi, Mohamad H. Danesh, Li Fuxin. (2024)<br><strong>Taming the Tail in Class-Conditional GANs: Knowledge Sharing via Unconditional Training at Lower Resolutions</strong><br><button class=copy-to-clipboard title="Taming the Tail in Class-Conditional GANs: Knowledge Sharing via Unconditional Training at Lower Resolutions" index=151>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-151 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-LG, cs.CV<br>Keyword Score: 23<br>Keywords: Benchmarking, Generative Adversarial Network, Generative Adversarial Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17065v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17065v1.pdf filename=2402.17065v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Despite the extensive research on training <b>generative</b> <b>adversarial</b> <b>networks</b> <b>(GANs)</b> with limited training data, learning to generate images from long-tailed training distributions remains fairly unexplored. In the presence of imbalanced multi-class training data, <b>GANs</b> tend to favor classes with more samples, leading to the generation of low-quality and less diverse samples in tail classes. In this study, we aim to improve the training of class-conditional <b>GANs</b> with long-tailed data. We propose a straightforward yet effective method for knowledge sharing, allowing tail classes to borrow from the rich information from classes with more abundant training data. More concretely, we propose modifications to existing class-conditional <b>GAN</b> architectures to ensure that the lower-resolution layers of the generator are trained entirely unconditionally while reserving class-conditional generation for the higher-resolution layers. Experiments on several long-tail <b>benchmarks</b> and <b>GAN</b> architectures demonstrate a significant improvement over existing methods in both the diversity and fidelity of the generated images. The code is available at <a href=https://github.com/khorrams/utlo>https://github.com/khorrams/utlo</a>.</p></p class="citation"></blockquote><h3 id=1740--152279-towards-open-ended-visual-quality-comparison-haoning-wu-et-al-2024>(17/40 | 152/279) Towards Open-ended Visual Quality Comparison (Haoning Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haoning Wu, Hanwei Zhu, Zicheng Zhang, Erli Zhang, Chaofeng Chen, Liang Liao, Chunyi Li, Annan Wang, Wenxiu Sun, Qiong Yan, Xiaohong Liu, Guangtao Zhai, Shiqi Wang, Weisi Lin. (2024)<br><strong>Towards Open-ended Visual Quality Comparison</strong><br><button class=copy-to-clipboard title="Towards Open-ended Visual Quality Comparison" index=152>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-152 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 23<br>Keywords: Benchmarking, GPT, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16641v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16641v1.pdf filename=2402.16641v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Comparative settings (e.g. pairwise choice, listwise ranking) have been adopted by a wide range of subjective studies for image quality assessment (IQA), as it inherently standardizes the evaluation criteria across different observers and offer more clear-cut responses. In this work, we extend the edge of emerging large multi-modality models (LMMs) to further advance visual quality comparison into open-ended settings, that 1) can respond to open-range questions on quality comparison; 2) can provide detailed <b>reasonings</b> beyond direct answers. To this end, we propose the Co-Instruct. To train this first-of-its-kind open-source open-ended visual quality comparer, we collect the Co-Instruct-562K dataset, from two sources: (a) LMM-merged single image quality description, (b) <b>GPT-4V</b> &ldquo;teacher&rdquo; responses on unlabeled data. Furthermore, to better evaluate this setting, we propose the MICBench, the first <b>benchmark</b> on multi-image comparison for LMMs. We demonstrate that Co-Instruct not only achieves 30% higher superior accuracy than state-of-the-art open-source LMMs, but also outperforms <b>GPT-4V</b> (its teacher), on both existing related <b>benchmarks</b> and the proposed MICBench. Our model is published at <a href=https://huggingface.co/q-future/co-instruct>https://huggingface.co/q-future/co-instruct</a>.</p></p class="citation"></blockquote><h3 id=1840--153279-multi-lora-composition-for-image-generation-ming-zhong-et-al-2024>(18/40 | 153/279) Multi-LoRA Composition for Image Generation (Ming Zhong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ming Zhong, Yelong Shen, Shuohang Wang, Yadong Lu, Yizhu Jiao, Siru Ouyang, Donghan Yu, Jiawei Han, Weizhu Chen. (2024)<br><strong>Multi-LoRA Composition for Image Generation</strong><br><button class=copy-to-clipboard title="Multi-LoRA Composition for Image Generation" index=153>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-153 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CL, cs-CV, cs-GR, cs-LG, cs.CV<br>Keyword Score: 20<br>Keywords: GPT, Text2image<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16843v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16843v1.pdf filename=2402.16843v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Low-Rank Adaptation (LoRA) is extensively utilized in <b>text-to-image</b> models for the accurate rendition of specific elements like distinct characters or unique styles in generated images. Nonetheless, existing methods face challenges in effectively composing multiple LoRAs, especially as the number of LoRAs to be integrated grows, thus hindering the creation of complex imagery. In this paper, we study multi-LoRA composition through a decoding-centric perspective. We present two training-free methods: LoRA Switch, which alternates between different LoRAs at each denoising step, and LoRA Composite, which simultaneously incorporates all LoRAs to guide more cohesive image synthesis. To evaluate the proposed approaches, we establish ComposLoRA, a new comprehensive testbed as part of this research. It features a diverse range of LoRA categories with 480 composition sets. Utilizing an evaluation framework based on <b>GPT-4V,</b> our findings demonstrate a clear improvement in performance with our methods over the prevalent baseline, particularly evident when increasing the number of LoRAs in a composition.</p></p class="citation"></blockquote><h3 id=1940--154279-disentangled-3d-scene-generation-with-layout-learning-dave-epstein-et-al-2024>(19/40 | 154/279) Disentangled 3D Scene Generation with Layout Learning (Dave Epstein et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dave Epstein, Ben Poole, Ben Mildenhall, Alexei A. Efros, Aleksander Holynski. (2024)<br><strong>Disentangled 3D Scene Generation with Layout Learning</strong><br><button class=copy-to-clipboard title="Disentangled 3D Scene Generation with Layout Learning" index=154>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-154 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 20<br>Keywords: Unsupervised Learning, Text2image<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16936v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16936v1.pdf filename=2402.16936v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce a method to generate 3D scenes that are disentangled into their component objects. This disentanglement is <b>unsupervised,</b> relying only on the knowledge of a large pretrained <b>text-to-image</b> model. Our key insight is that objects can be discovered by finding parts of a 3D scene that, when rearranged spatially, still produce valid configurations of the same scene. Concretely, our method jointly optimizes multiple NeRFs from scratch - each representing its own object - along with a set of layouts that composite these objects into scenes. We then encourage these composited scenes to be in-distribution according to the image generator. We show that despite its simplicity, our approach successfully generates 3D scenes decomposed into individual objects, enabling new capabilities in text-to-3D content creation. For results and an interactive demo, see our project page at <a href=https://dave.ml/layoutlearning/>https://dave.ml/layoutlearning/</a></p></p class="citation"></blockquote><h3 id=2040--155279-unifying-latent-and-lexicon-representations-for-effective-video-text-retrieval-haowei-liu-et-al-2024>(20/40 | 155/279) Unifying Latent and Lexicon Representations for Effective Video-Text Retrieval (Haowei Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haowei Liu, Yaya Shi, Haiyang Xu, Chunfeng Yuan, Qinghao Ye, Chenliang Li, Ming Yan, Ji Zhang, Fei Huang, Bing Li, Weiming Hu. (2024)<br><strong>Unifying Latent and Lexicon Representations for Effective Video-Text Retrieval</strong><br><button class=copy-to-clipboard title="Unifying Latent and Lexicon Representations for Effective Video-Text Retrieval" index=155>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-155 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Self-Distillation, Grounding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16769v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16769v1.pdf filename=2402.16769v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In video-text retrieval, most existing methods adopt the dual-encoder architecture for fast retrieval, which employs two individual encoders to extract global latent representations for videos and texts. However, they face challenges in capturing fine-grained semantic concepts. In this work, we propose the UNIFY framework, which learns lexicon representations to capture fine-grained semantics and combines the strengths of latent and lexicon representations for video-text retrieval. Specifically, we map videos and texts into a pre-defined lexicon space, where each dimension corresponds to a semantic concept. A two-stage semantics <b>grounding</b> approach is proposed to activate semantically relevant dimensions and suppress irrelevant dimensions. The learned lexicon representations can thus reflect fine-grained semantics of videos and texts. Furthermore, to leverage the complementarity between latent and lexicon representations, we propose a unified learning scheme to facilitate mutual learning via structure sharing and <b>self-distillation.</b> Experimental results show our UNIFY framework largely outperforms previous video-text retrieval methods, with 4.8% and 8.2% Recall@1 improvement on MSR-VTT and DiDeMo respectively.</p></p class="citation"></blockquote><h3 id=2140--156279-cross-modal-contextualized-diffusion-models-for-text-guided-visual-generation-and-editing-ling-yang-et-al-2024>(21/40 | 156/279) Cross-Modal Contextualized Diffusion Models for Text-Guided Visual Generation and Editing (Ling Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ling Yang, Zhilong Zhang, Zhaochen Yu, Jingwei Liu, Minkai Xu, Stefano Ermon, Bin Cui. (2024)<br><strong>Cross-Modal Contextualized Diffusion Models for Text-Guided Visual Generation and Editing</strong><br><button class=copy-to-clipboard title="Cross-Modal Contextualized Diffusion Models for Text-Guided Visual Generation and Editing" index=156>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-156 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Diffusion Model, Text2image<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16627v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16627v1.pdf filename=2402.16627v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Conditional <b>diffusion</b> <b>models</b> have exhibited superior performance in high-fidelity text-guided visual generation and editing. Nevertheless, prevailing text-guided visual <b>diffusion</b> <b>models</b> primarily focus on incorporating text-visual relationships exclusively into the reverse process, often disregarding their relevance in the forward process. This inconsistency between forward and reverse processes may limit the precise conveyance of textual semantics in visual synthesis results. To address this issue, we propose a novel and general contextualized <b>diffusion</b> <b>model</b> (ContextDiff) by incorporating the cross-modal context encompassing interactions and alignments between text condition and visual sample into forward and reverse processes. We propagate this context to all timesteps in the two processes to adapt their trajectories, thereby facilitating cross-modal conditional modeling. We generalize our contextualized <b>diffusion</b> <b>to</b> both DDPMs and DDIMs with theoretical derivations, and demonstrate the effectiveness of our model in evaluations with two challenging tasks: <b>text-to-image</b> generation, and text-to-video editing. In each task, our ContextDiff achieves new state-of-the-art performance, significantly enhancing the semantic alignment between text condition and generated samples, as evidenced by quantitative and qualitative evaluations. Our code is available at <a href=https://github.com/YangLing0818/ContextDiff>https://github.com/YangLing0818/ContextDiff</a></p></p class="citation"></blockquote><h3 id=2240--157279-improving-the-jpeg-resistance-of-adversarial-attacks-on-face-recognition-by-interpolation-smoothing-kefu-guo-et-al-2024>(22/40 | 157/279) Improving the JPEG-resistance of Adversarial Attacks on Face Recognition by Interpolation Smoothing (Kefu Guo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kefu Guo, Fengfan Zhou, Hefei Ling, Ping Li, Hui Liu. (2024)<br><strong>Improving the JPEG-resistance of Adversarial Attacks on Face Recognition by Interpolation Smoothing</strong><br><button class=copy-to-clipboard title="Improving the JPEG-resistance of Adversarial Attacks on Face Recognition by Interpolation Smoothing" index=157>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-157 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Face Recognition, Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16586v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16586v1.pdf filename=2402.16586v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>JPEG compression can significantly impair the performance of <b>adversarial</b> <b>face</b> <b>examples,</b> which previous <b>adversarial</b> <b>attacks</b> on <b>face</b> <b>recognition</b> (FR) have not adequately addressed. Considering this challenge, we propose a novel <b>adversarial</b> <b>attack</b> on FR that aims to improve the resistance of <b>adversarial</b> <b>examples</b> against JPEG compression. Specifically, during the iterative process of generating <b>adversarial</b> <b>face</b> <b>examples,</b> we interpolate the <b>adversarial</b> <b>face</b> <b>examples</b> into a smaller size. Then we utilize these interpolated <b>adversarial</b> <b>face</b> <b>examples</b> to create the <b>adversarial</b> <b>examples</b> in the next iteration. Subsequently, we restore the <b>adversarial</b> <b>face</b> <b>examples</b> to their original size by interpolating. Throughout the entire process, our proposed method can smooth the <b>adversarial</b> <b>perturbations,</b> effectively mitigating the presence of high-frequency signals in the crafted <b>adversarial</b> <b>face</b> <b>examples</b> that are typically eliminated by JPEG compression. Our experimental results demonstrate the effectiveness of our proposed method in improving the JPEG-resistance of <b>adversarial</b> <b>face</b> <b>examples.</b></p></p class="citation"></blockquote><h3 id=2340--158279-edge-detectors-can-make-deep-convolutional-neural-networks-more-robust-jin-ding-et-al-2024>(23/40 | 158/279) Edge Detectors Can Make Deep Convolutional Neural Networks More Robust (Jin Ding et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jin Ding, Jie-Chao Zhao, Yong-Zhi Sun, Ping Tan, Jia-Wei Wang, Ji-En Ma, You-Tong Fang. (2024)<br><strong>Edge Detectors Can Make Deep Convolutional Neural Networks More Robust</strong><br><button class=copy-to-clipboard title="Edge Detectors Can Make Deep Convolutional Neural Networks More Robust" index=158>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-158 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Convolution, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16479v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16479v1.pdf filename=2402.16479v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep <b>convolutional</b> <b>neural</b> <b>networks</b> (DCNN for short) are vulnerable to examples with small perturbations. Improving DCNN&rsquo;s robustness is of great significance to the safety-critical applications, such as autonomous driving and industry automation. Inspired by the principal way that human eyes recognize objects, i.e., largely relying on the shape features, this paper first employs the edge detectors as layer kernels and designs a binary edge feature branch (BEFB for short) to learn the binary edge features, which can be easily integrated into any popular backbone. The four edge detectors can learn the horizontal, vertical, positive diagonal, and negative diagonal edge features, respectively, and the branch is stacked by multiple Sobel layers (using edge detectors as kernels) and one threshold layer. The binary edge features learned by the branch, concatenated with the texture features learned by the backbone, are fed into the fully connected layers for classification. We integrate the proposed branch into VGG16 and ResNet34, respectively, and conduct experiments on multiple datasets. Experimental results demonstrate the BEFB is lightweight and has no side effects on training. And the accuracy of the BEFB integrated models is better than the original ones on all datasets when facing FGSM, PGD, and C&amp;W attacks. Besides, BEFB integrated models equipped with the robustness enhancing techniques can achieve better classification accuracy compared to the original models. The work in this paper for the first time shows it is feasible to enhance the robustness of DCNNs through combining both shape-like features and texture features.</p></p class="citation"></blockquote><h3 id=2440--159279-comae-comprehensive-attribute-exploration-for-zero-shot-hashing-yihang-zhou-et-al-2024>(24/40 | 159/279) COMAE: COMprehensive Attribute Exploration for Zero-shot Hashing (Yihang Zhou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yihang Zhou, Qingqing Long, Yuchen Yan, Xiao Luo, Zeyu Dong, Xuezhi Wang, Zhen Meng, Pengfei Wang, Yuanchun Zhou. (2024)<br><strong>COMAE: COMprehensive Attribute Exploration for Zero-shot Hashing</strong><br><button class=copy-to-clipboard title="COMAE: COMprehensive Attribute Exploration for Zero-shot Hashing" index=159>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-159 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Contrastive Learning, Zero-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16424v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16424v1.pdf filename=2402.16424v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Zero-shot</b> hashing (ZSH) has shown excellent success owing to its efficiency and generalization in large-scale retrieval scenarios. While considerable success has been achieved, there still exist urgent limitations. Existing works ignore the locality relationships of representations and attributes, which have effective transferability between seeable classes and unseeable classes. Also, the continuous-value attributes are not fully harnessed. In response, we conduct a COMprehensive Attribute Exploration for ZSH, named COMAE, which depicts the relationships from seen classes to unseen ones through three meticulously designed explorations, i.e., point-wise, pair-wise and class-wise consistency constraints. By regressing attributes from the proposed attribute prototype network, COMAE learns the local features that are relevant to the visual attributes. Then COMAE utilizes <b>contrastive</b> <b>learning</b> to comprehensively depict the context of attributes, rather than instance-independent optimization. Finally, the class-wise constraint is designed to cohesively learn the hash code, image representation, and visual attributes more effectively. Experimental results on the popular ZSH datasets demonstrate that COMAE outperforms state-of-the-art hashing techniques, especially in scenarios with a larger number of unseen label classes.</p></p class="citation"></blockquote><h3 id=2540--160279-spc-nerf-spatial-predictive-compression-for-voxel-based-radiance-field-zetian-song-et-al-2024>(25/40 | 160/279) SPC-NeRF: Spatial Predictive Compression for Voxel Based Radiance Field (Zetian Song et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zetian Song, Wenhong Duan, Yuhuai Zhang, Shiqi Wang, Siwei Ma, Wen Gao. (2024)<br><strong>SPC-NeRF: Spatial Predictive Compression for Voxel Based Radiance Field</strong><br><button class=copy-to-clipboard title="SPC-NeRF: Spatial Predictive Compression for Voxel Based Radiance Field" index=160>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-160 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-MM, cs.CV<br>Keyword Score: 20<br>Keywords: Pruning, Quantization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16366v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16366v1.pdf filename=2402.16366v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Representing the Neural Radiance Field (NeRF) with the explicit voxel grid (EVG) is a promising direction for improving NeRFs. However, the EVG representation is not efficient for storage and transmission because of the terrific memory cost. Current methods for compressing EVG mainly inherit the methods designed for neural network compression, such as <b>pruning</b> and <b>quantization,</b> which do not take full advantage of the spatial correlation of voxels. Inspired by prosperous digital image compression techniques, this paper proposes SPC-NeRF, a novel framework applying spatial predictive coding in EVG compression. The proposed framework can remove spatial redundancy efficiently for better compression performance.Moreover, we model the bitrate and design a novel form of the loss function, where we can jointly optimize compression ratio and distortion to achieve higher coding efficiency. Extensive experiments demonstrate that our method can achieve 32% bit saving compared to the state-of-the-art method VQRF on multiple representative test datasets, with comparable training time.</p></p class="citation"></blockquote><h3 id=2640--161279-mv-swin-t-mammogram-classification-with-multi-view-swin-transformer-sushmita-sarker-et-al-2024>(26/40 | 161/279) MV-Swin-T: Mammogram Classification with Multi-view Swin Transformer (Sushmita Sarker et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sushmita Sarker, Prithul Sarker, George Bebis, Alireza Tavakkoli. (2024)<br><strong>MV-Swin-T: Mammogram Classification with Multi-view Swin Transformer</strong><br><button class=copy-to-clipboard title="MV-Swin-T: Mammogram Classification with Multi-view Swin Transformer" index=161>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-161 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Convolution, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16298v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16298v1.pdf filename=2402.16298v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Traditional deep learning approaches for breast cancer classification has predominantly concentrated on single-view analysis. In clinical practice, however, radiologists concurrently examine all views within a mammography exam, leveraging the inherent correlations in these views to effectively detect tumors. Acknowledging the significance of multi-view analysis, some studies have introduced methods that independently process mammogram views, either through distinct <b>convolutional</b> branches or simple fusion strategies, inadvertently leading to a loss of crucial inter-view correlations. In this paper, we propose an innovative multi-view network exclusively based on <b>transformers</b> to address challenges in mammographic image classification. Our approach introduces a novel shifted window-based dynamic attention block, facilitating the effective integration of multi-view information and promoting the coherent transfer of this information between views at the spatial feature map level. Furthermore, we conduct a comprehensive comparative analysis of the performance and effectiveness of <b>transformer-based</b> models under diverse settings, employing the CBIS-DDSM and Vin-Dr Mammo datasets. Our code is publicly available at <a href=https://github.com/prithuls/MV-Swin-T>https://github.com/prithuls/MV-Swin-T</a></p></p class="citation"></blockquote><h3 id=2740--162279-gradient-guided-modality-decoupling-for-missing-modality-robustness-hao-wang-et-al-2024>(27/40 | 162/279) Gradient-Guided Modality Decoupling for Missing-Modality Robustness (Hao Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hao Wang, Shengda Luo, Guosheng Hu, Jianguo Zhang. (2024)<br><strong>Gradient-Guided Modality Decoupling for Missing-Modality Robustness</strong><br><button class=copy-to-clipboard title="Gradient-Guided Modality Decoupling for Missing-Modality Robustness" index=162>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-162 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-MM, cs.CV<br>Keyword Score: 19<br>Keywords: Benchmarking, Multi-modal, Multi-modal, Sentiment Analysis<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16318v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16318v1.pdf filename=2402.16318v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Multimodal</b> learning with incomplete input data (missing modality) is practical and challenging. In this work, we conduct an in-depth analysis of this challenge and find that modality dominance has a significant negative impact on the model training, greatly degrading the missing modality performance. Motivated by Grad-CAM, we introduce a novel indicator, gradients, to monitor and reduce modality dominance which widely exists in the missing-modality scenario. In aid of this indicator, we present a novel Gradient-guided Modality Decoupling (GMD) method to decouple the dependency on dominating modalities. Specifically, GMD removes the conflicted gradient components from different modalities to achieve this decoupling, significantly improving the performance. In addition, to flexibly handle modal-incomplete data, we design a parameter-efficient Dynamic Sharing (DS) framework which can adaptively switch on/off the network parameters based on whether one modality is available. We conduct extensive experiments on three popular <b>multimodal</b> <b>benchmarks,</b> including BraTS 2018 for medical segmentation, CMU-MOSI, and CMU-MOSEI for <b>sentiment</b> <b>analysis.</b> The results show that our method can significantly outperform the competitors, showing the effectiveness of the proposed solutions. Our code is released here: <a href=https://github.com/HaoWang420/Gradient-guided-Modality-Decoupling>https://github.com/HaoWang420/Gradient-guided-Modality-Decoupling</a>.</p></p class="citation"></blockquote><h3 id=2840--163279-gem3d-generative-medial-abstractions-for-3d-shape-synthesis-dmitry-petrov-et-al-2024>(28/40 | 163/279) GEM3D: GEnerative Medial Abstractions for 3D Shape Synthesis (Dmitry Petrov et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dmitry Petrov, Pradyumn Goyal, Vikas Thamizharasan, Vladimir G. Kim, Matheus Gadelha, Melinos Averkiou, Siddhartha Chaudhuri, Evangelos Kalogerakis. (2024)<br><strong>GEM3D: GEnerative Medial Abstractions for 3D Shape Synthesis</strong><br><button class=copy-to-clipboard title="GEM3D: GEnerative Medial Abstractions for 3D Shape Synthesis" index=163>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-163 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-GR, cs-LG, cs.CV<br>Keyword Score: 15<br>Keywords: Geometry, Probabilistic Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16994v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16994v1.pdf filename=2402.16994v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce GEM3D &ndash; a new deep, topology-aware generative model of 3D shapes. The key ingredient of our method is a neural skeleton-based representation encoding information on both shape topology and <b>geometry.</b> Through a denoising diffusion <b>probabilistic</b> <b>model,</b> our method first generates skeleton-based representations following the Medial Axis Transform (MAT), then generates surfaces through a skeleton-driven neural implicit formulation. The neural implicit takes into account the topological and geometric information stored in the generated skeleton representations to yield surfaces that are more topologically and geometrically accurate compared to previous neural field formulations. We discuss applications of our method in shape synthesis and point cloud reconstruction tasks, and evaluate our method both qualitatively and quantitatively. We demonstrate significantly more faithful surface reconstruction and diverse shape generation results compared to the state-of-the-art, also involving challenging scenarios of reconstructing and synthesizing structurally complex, high-genus shape surfaces from Thingi10K and ShapeNet.</p></p class="citation"></blockquote><h3 id=2940--164279-multi-human-mesh-recovery-with-transformers-zeyu-wang-et-al-2024>(29/40 | 164/279) Multi-Human Mesh Recovery with Transformers (Zeyu Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zeyu Wang, Zhenzhen Weng, Serena Yeung-Levy. (2024)<br><strong>Multi-Human Mesh Recovery with Transformers</strong><br><button class=copy-to-clipboard title="Multi-Human Mesh Recovery with Transformers" index=164>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-164 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 13<br>Keywords: Benchmarking, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16806v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16806v1.pdf filename=2402.16806v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Conventional approaches to human mesh recovery predominantly employ a region-based strategy. This involves initially cropping out a human-centered region as a preprocessing step, with subsequent modeling focused on this zoomed-in image. While effective for single figures, this pipeline poses challenges when dealing with images featuring multiple individuals, as different people are processed separately, often leading to inaccuracies in relative positioning. Despite the advantages of adopting a whole-image-based approach to address this limitation, early efforts in this direction have fallen short in performance compared to recent region-based methods. In this work, we advocate for this under-explored area of modeling all people at once, emphasizing its potential for improved accuracy in multi-person scenarios through considering all individuals simultaneously and leveraging the overall context and interactions. We introduce a new model with a streamlined <b>transformer-based</b> design, featuring three critical design choices: multi-scale feature incorporation, focused attention mechanisms, and relative joint supervision. Our proposed model demonstrates a significant performance improvement, surpassing state-of-the-art region-based and whole-image-based methods on various <b>benchmarks</b> involving multiple individuals.</p></p class="citation"></blockquote><h3 id=3040--165279-stochastic-conditional-diffusion-models-for-semantic-image-synthesis-juyeon-ko-et-al-2024>(30/40 | 165/279) Stochastic Conditional Diffusion Models for Semantic Image Synthesis (Juyeon Ko et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Juyeon Ko, Inho Kong, Dogyun Park, Hyunwoo J. Kim. (2024)<br><strong>Stochastic Conditional Diffusion Models for Semantic Image Synthesis</strong><br><button class=copy-to-clipboard title="Stochastic Conditional Diffusion Models for Semantic Image Synthesis" index=165>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-165 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 13<br>Keywords: Diffusion Model, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16506v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16506v2.pdf filename=2402.16506v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Semantic image synthesis (SIS) is a task to generate realistic images corresponding to semantic maps (labels). It can be applied to diverse real-world practices such as photo editing or content creation. However, in real-world applications, SIS often encounters noisy user inputs. To address this, we propose Stochastic Conditional <b>Diffusion</b> <b>Model</b> (SCDM), which is a robust conditional <b>diffusion</b> <b>model</b> that features novel forward and generation processes tailored for SIS with noisy labels. It enhances robustness by stochastically perturbing the semantic label maps through Label <b>Diffusion,</b> <b>which</b> diffuses the labels with discrete <b>diffusion.</b> <b>Through</b> the <b>diffusion</b> <b>of</b> labels, the noisy and clean semantic maps become similar as the timestep increases, eventually becoming identical at $t=T$. This facilitates the generation of an image close to a clean image, enabling robust generation. Furthermore, we propose a class-wise noise schedule to differentially diffuse the labels depending on the class. We demonstrate that the proposed method generates high-quality samples through extensive experiments and analyses on <b>benchmark</b> datasets, including a novel experimental setup simulating human errors during real-world applications.</p></p class="citation"></blockquote><h3 id=3140--166279-infrared-and-visible-image-fusion-with-language-driven-loss-in-clip-embedding-space-yuhao-wang-et-al-2024>(31/40 | 166/279) Infrared and visible Image Fusion with Language-driven Loss in CLIP Embedding Space (Yuhao Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuhao Wang, Lingjuan Miao, Zhiqiang Zhou, Lei Zhang, Yajun Qiao. (2024)<br><strong>Infrared and visible Image Fusion with Language-driven Loss in CLIP Embedding Space</strong><br><button class=copy-to-clipboard title="Infrared and visible Image Fusion with Language-driven Loss in CLIP Embedding Space" index=166>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-166 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 13<br>Keywords: Multi-modal, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16267v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16267v1.pdf filename=2402.16267v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Infrared-visible image fusion (IVIF) has attracted much attention owing to the highly-complementary properties of the two image modalities. Due to the lack of ground-truth fused images, the fusion output of current deep-learning based methods heavily depends on the loss functions defined mathematically. As it is hard to well mathematically define the fused image without ground truth, the performance of existing fusion methods is limited. In this paper, we first propose to use natural language to express the objective of IVIF, which can avoid the explicit mathematical modeling of fusion output in current losses, and make full use of the advantage of language expression to improve the fusion performance. For this purpose, we present a comprehensive language-expressed fusion objective, and encode relevant texts into the <b>multi-modal</b> embedding space using CLIP. A language-driven fusion model is then constructed in the embedding space, by establishing the relationship among the embedded vectors to represent the fusion objective and input image modalities. Finally, a language-driven loss is derived to make the actual IVIF aligned with the embedded language-driven fusion model via <b>supervised</b> training. Experiments show that our method can obtain much better fusion results than existing techniques.</p></p class="citation"></blockquote><h3 id=3240--167279-neural-mesh-fusion-unsupervised-3d-planar-surface-understanding-farhad-g-zanjani-et-al-2024>(32/40 | 167/279) Neural Mesh Fusion: Unsupervised 3D Planar Surface Understanding (Farhad G. Zanjani et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Farhad G. Zanjani, Hong Cai, Yinhao Zhu, Leyla Mirvakhabova, Fatih Porikli. (2024)<br><strong>Neural Mesh Fusion: Unsupervised 3D Planar Surface Understanding</strong><br><button class=copy-to-clipboard title="Neural Mesh Fusion: Unsupervised 3D Planar Surface Understanding" index=167>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-167 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16739v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16739v1.pdf filename=2402.16739v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents Neural Mesh Fusion (NMF), an efficient approach for joint optimization of polygon mesh from multi-view image observations and <b>unsupervised</b> 3D planar-surface parsing of the scene. In contrast to implicit neural representations, NMF directly learns to deform surface triangle mesh and generate an embedding for <b>unsupervised</b> 3D planar segmentation through gradient-based optimization directly on the surface mesh. The conducted experiments show that NMF obtains competitive results compared to state-of-the-art multi-view planar reconstruction, while not requiring any ground-truth 3D or planar supervision. Moreover, NMF is significantly more computationally efficient compared to implicit neural rendering-based scene reconstruction approaches.</p></p class="citation"></blockquote><h3 id=3340--168279-pretrained-visual-uncertainties-michael-kirchhof-et-al-2024>(33/40 | 168/279) Pretrained Visual Uncertainties (Michael Kirchhof et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Michael Kirchhof, Mark Collier, Seong Joon Oh, Enkelejda Kasneci. (2024)<br><strong>Pretrained Visual Uncertainties</strong><br><button class=copy-to-clipboard title="Pretrained Visual Uncertainties" index=168>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-168 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 10<br>Keywords: Zero-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16569v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16569v2.pdf filename=2402.16569v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Accurate uncertainty estimation is vital to trustworthy machine learning, yet uncertainties typically have to be learned for each task anew. This work introduces the first pretrained uncertainty modules for vision models. Similar to standard pretraining this enables the <b>zero-shot</b> transfer of uncertainties learned on a large pretraining dataset to specialized downstream datasets. We enable our large-scale pretraining on ImageNet-21k by solving a gradient conflict in previous uncertainty modules and accelerating the training by up to 180x. We find that the pretrained uncertainties generalize to unseen datasets. In scrutinizing the learned uncertainties, we find that they capture aleatoric uncertainty, disentangled from epistemic components. We demonstrate that this enables safe retrieval and uncertainty-aware dataset visualization. To encourage applications to further problems and domains, we release all pretrained checkpoints and code under <a href=https://github.com/mkirchhof/url>https://github.com/mkirchhof/url</a> .</p></p class="citation"></blockquote><h3 id=3440--169279-outline-guided-object-inpainting-with-diffusion-models-markus-pobitzer-et-al-2024>(34/40 | 169/279) Outline-Guided Object Inpainting with Diffusion Models (Markus Pobitzer et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Markus Pobitzer, Filip Janicki, Mattia Rigotti, Cristiano Malossi. (2024)<br><strong>Outline-Guided Object Inpainting with Diffusion Models</strong><br><button class=copy-to-clipboard title="Outline-Guided Object Inpainting with Diffusion Models" index=169>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-169 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: I-4; I-5, cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Diffusion Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16421v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16421v1.pdf filename=2402.16421v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Instance segmentation datasets play a crucial role in training accurate and robust computer vision models. However, obtaining accurate mask annotations to produce high-quality segmentation datasets is a costly and labor-intensive process. In this work, we show how this issue can be mitigated by starting with small annotated instance segmentation datasets and augmenting them to effectively obtain a sizeable annotated dataset. We achieve that by creating variations of the available annotated object instances in a way that preserves the provided mask annotations, thereby resulting in new image-mask pairs to be added to the set of annotated images. Specifically, we generate new images using a <b>diffusion-based</b> <b>inpainting</b> model to fill out the masked area with a desired object class by guiding the <b>diffusion</b> <b>through</b> the object outline. We show that the object outline provides a simple, but also reliable and convenient training-free guidance signal for the underlying inpainting model that is often sufficient to fill out the mask with an object of the correct class without further text guidance and preserve the correspondence between generated images and the mask annotations with high precision. Our experimental results reveal that our method successfully generates realistic variations of object instances, preserving their shape characteristics while introducing diversity within the augmented area. We also show that the proposed method can naturally be combined with text guidance and other image augmentation techniques.</p></p class="citation"></blockquote><h3 id=3540--170279-cmc-few-shot-novel-view-synthesis-via-cross-view-multiplane-consistency-hanxin-zhu-et-al-2024>(35/40 | 170/279) CMC: Few-shot Novel View Synthesis via Cross-view Multiplane Consistency (Hanxin Zhu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hanxin Zhu, Tianyu He, Zhibo Chen. (2024)<br><strong>CMC: Few-shot Novel View Synthesis via Cross-view Multiplane Consistency</strong><br><button class=copy-to-clipboard title="CMC: Few-shot Novel View Synthesis via Cross-view Multiplane Consistency" index=170>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-170 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-GR, cs.CV<br>Keyword Score: 10<br>Keywords: Few-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16407v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16407v1.pdf filename=2402.16407v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Neural Radiance Field (NeRF) has shown impressive results in novel view synthesis, particularly in Virtual Reality (VR) and Augmented Reality (AR), thanks to its ability to represent scenes continuously. However, when just a few input view images are available, NeRF tends to overfit the given views and thus make the estimated depths of pixels share almost the same value. Unlike previous methods that conduct regularization by introducing complex priors or additional supervisions, we propose a simple yet effective method that explicitly builds depth-aware consistency across input views to tackle this challenge. Our key insight is that by forcing the same spatial points to be sampled repeatedly in different input views, we are able to strengthen the interactions between views and therefore alleviate the overfitting problem. To achieve this, we build the neural networks on layered representations (\textit{i.e.}, multiplane images), and the sampling point can thus be resampled on multiple discrete planes. Furthermore, to regularize the unseen target views, we constrain the rendered colors and depths from different input views to be the same. Although simple, extensive experiments demonstrate that our proposed method can achieve better synthesis quality over state-of-the-art methods.</p></p class="citation"></blockquote><h3 id=3640--171279-what-text-design-characterizes-book-genres-daichi-haraguchi-et-al-2024>(36/40 | 171/279) What Text Design Characterizes Book Genres? (Daichi Haraguchi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Daichi Haraguchi, Brian Kenji Iwana, Seiichi Uchida. (2024)<br><strong>What Text Design Characterizes Book Genres?</strong><br><button class=copy-to-clipboard title="What Text Design Characterizes Book Genres?" index=171>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-171 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Text2image<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16356v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16356v1.pdf filename=2402.16356v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This study analyzes the relationship between non-verbal information (e.g., genres) and text design (e.g., font style, character color, etc.) through the classification of book genres using text design on book covers. Text images have both semantic information about the word itself and other information (non-semantic information or visual design), such as font style, character color, etc. When we read a word printed on some materials, we receive impressions or other information from both the word itself and the visual design. Basically, we can understand verbal information only from semantic information, i.e., the words themselves; however, we can consider that text design is helpful for understanding other additional information (i.e., non-verbal information), such as impressions, genre, etc. To investigate the effect of text design, we analyze text design using words printed on book covers and their genres in two scenarios. First, we attempted to understand the importance of visual design for determining the genre (i.e., non-verbal information) of books by analyzing the differences in the relationship between semantic information/visual design and genres. In the experiment, we found that semantic information is sufficient to determine the genre; however, text design is helpful in adding more discriminative features for book genres. Second, we investigated the effect of each text design on book genres. As a result, we found that each text design characterizes some book genres. For example, font style is useful to add more discriminative features for genres of <code>Mystery, Thriller \& Suspense'' and </code>Christian books & Bibles.''</p></p class="citation"></blockquote><h3 id=3740--172279-real-time-vehicle-detection-and-urban-traffic-behavior-analysis-based-on-uav-traffic-videos-on-mobile-devices-yuan-zhu-et-al-2024>(37/40 | 172/279) Real-Time Vehicle Detection and Urban Traffic Behavior Analysis Based on UAV Traffic Videos on Mobile Devices (Yuan Zhu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuan Zhu, Yanqiang Wang, Yadong An, Hong Yang, Yiming Pan. (2024)<br><strong>Real-Time Vehicle Detection and Urban Traffic Behavior Analysis Based on UAV Traffic Videos on Mobile Devices</strong><br><button class=copy-to-clipboard title="Real-Time Vehicle Detection and Urban Traffic Behavior Analysis Based on UAV Traffic Videos on Mobile Devices" index=172>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-172 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Object Detection<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16246v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16246v1.pdf filename=2402.16246v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper focuses on a real-time vehicle detection and urban traffic behavior analysis system based on Unmanned Aerial Vehicle (UAV) traffic video. By using UAV to collect traffic data and combining the YOLOv8 model and SORT tracking algorithm, the <b>object</b> <b>detection</b> and tracking functions are implemented on the iOS mobile platform. For the problem of traffic data acquisition and analysis, the dynamic computing method is used to process the performance in real time and calculate the micro and macro traffic parameters of the vehicles, and real-time traffic behavior analysis is conducted and visualized. The experiment results reveals that the vehicle <b>object</b> <b>detection</b> can reach 98.27% precision rate and 87.93% recall rate, and the real-time processing capacity is stable at 30 frames per seconds. This work integrates drone technology, iOS development, and deep learning techniques to integrate traffic video acquisition, <b>object</b> <b>detection,</b> <b>object</b> <b>tracking,</b> and traffic behavior analysis functions on mobile devices. It provides new possibilities for lightweight traffic information collection and data analysis, and offers innovative solutions to improve the efficiency of analyzing road traffic conditions and addressing transportation issues for transportation authorities.</p></p class="citation"></blockquote><h3 id=3840--173279-misc-ultra-low-bitrate-image-semantic-compression-driven-by-large-multimodal-model-chunyi-li-et-al-2024>(38/40 | 173/279) MISC: Ultra-low Bitrate Image Semantic Compression Driven by Large Multimodal Model (Chunyi Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chunyi Li, Guo Lu, Donghui Feng, Haoning Wu, Zicheng Zhang, Xiaohong Liu, Guangtao Zhai, Weisi Lin, Wenjun Zhang. (2024)<br><strong>MISC: Ultra-low Bitrate Image Semantic Compression Driven by Large Multimodal Model</strong><br><button class=copy-to-clipboard title="MISC: Ultra-low Bitrate Image Semantic Compression Driven by Large Multimodal Model" index=173>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-173 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV, eess-IV<br>Keyword Score: 6<br>Keywords: Multi-modal, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16749v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16749v2.pdf filename=2402.16749v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the evolution of storage and communication protocols, ultra-low bitrate image compression has become a highly demanding topic. However, existing compression algorithms must sacrifice either consistency with the ground truth or perceptual quality at ultra-low bitrate. In recent years, the rapid development of the Large <b>Multimodal</b> Model (LMM) has made it possible to balance these two goals. To solve this problem, this paper proposes a method called <b>Multimodal</b> Image Semantic Compression (MISC), which consists of an LMM encoder for extracting the semantic information of the image, a map encoder to locate the region corresponding to the semantic, an image encoder generates an extremely compressed bitstream, and a decoder reconstructs the image based on the above information. Experimental results show that our proposed MISC is suitable for compressing both traditional Natural Sense Images (NSIs) and emerging AI-Generated Images (AIGIs) content. It can achieve optimal consistency and perception results while saving 50% bitrate, which has strong potential applications in the next generation of storage and communication. The code will be released on <a href=https://github.com/lcysyzxdxc/MISC>https://github.com/lcysyzxdxc/MISC</a>.</p></p class="citation"></blockquote><h3 id=3940--174279-dcvsmnet-double-cost-volume-stereo-matching-network-mahmoud-tahmasebi-et-al-2024>(39/40 | 174/279) DCVSMNet: Double Cost Volume Stereo Matching Network (Mahmoud Tahmasebi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mahmoud Tahmasebi, Saif Huq, Kevin Meehan, Marion McAfee. (2024)<br><strong>DCVSMNet: Double Cost Volume Stereo Matching Network</strong><br><button class=copy-to-clipboard title="DCVSMNet: Double Cost Volume Stereo Matching Network" index=174>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-174 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16473v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16473v1.pdf filename=2402.16473v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce Double Cost Volume Stereo Matching Network(DCVSMNet) which is a novel architecture characterised by by two small upper (group-wise) and lower (norm correlation) cost volumes. Each cost volume is processed separately, and a coupling module is proposed to fuse the <b>geometry</b> information extracted from the upper and lower cost volumes. DCVSMNet is a fast stereo matching network with a 67 ms inference time and strong generalization ability which can produce competitive results compared to state-of-the-art methods. The results on several bench mark datasets show that DCVSMNet achieves better accuracy than methods such as CGI-Stereo and BGNet at the cost of greater inference time.</p></p class="citation"></blockquote><h3 id=4040--175279-hoisdf-constraining-3d-hand-object-pose-estimation-with-global-signed-distance-fields-haozhe-qi-et-al-2024>(40/40 | 175/279) HOISDF: Constraining 3D Hand-Object Pose Estimation with Global Signed Distance Fields (Haozhe Qi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haozhe Qi, Chen Zhao, Mathieu Salzmann, Alexander Mathis. (2024)<br><strong>HOISDF: Constraining 3D Hand-Object Pose Estimation with Global Signed Distance Fields</strong><br><button class=copy-to-clipboard title="HOISDF: Constraining 3D Hand-Object Pose Estimation with Global Signed Distance Fields" index=175>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-175 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17062v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17062v1.pdf filename=2402.17062v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Human hands are highly articulated and versatile at handling objects. Jointly estimating the 3D poses of a hand and the object it manipulates from a monocular camera is challenging due to frequent occlusions. Thus, existing methods often rely on intermediate 3D shape representations to increase performance. These representations are typically explicit, such as 3D point clouds or meshes, and thus provide information in the direct surroundings of the intermediate hand pose estimate. To address this, we introduce HOISDF, a Signed Distance Field (SDF) guided hand-object pose estimation network, which jointly exploits hand and object SDFs to provide a global, implicit representation over the complete reconstruction volume. Specifically, the role of the SDFs is threefold: equip the visual encoder with implicit shape information, help to encode hand-object interactions, and guide the hand and object pose regression via SDF-based sampling and by augmenting the feature representations. We show that HOISDF achieves state-of-the-art results on hand-object pose estimation <b>benchmarks</b> (DexYCB and HO3Dv2). Code is available at <a href=https://github.com/amathislab/HOISDF>https://github.com/amathislab/HOISDF</a></p></p class="citation"></blockquote><h2 id=cssd-2>cs.SD (2)</h2><h3 id=12--176279-self-supervised-speech-quality-estimation-and-enhancement-using-only-clean-speech-szu-wei-fu-et-al-2024>(1/2 | 176/279) Self-Supervised Speech Quality Estimation and Enhancement Using Only Clean Speech (Szu-Wei Fu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Szu-Wei Fu, Kuo-Hsuan Hung, Yu Tsao, Yu-Chiang Frank Wang. (2024)<br><strong>Self-Supervised Speech Quality Estimation and Enhancement Using Only Clean Speech</strong><br><button class=copy-to-clipboard title="Self-Supervised Speech Quality Estimation and Enhancement Using Only Clean Speech" index=176>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-176 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SD<br>Categories: cs-AI, cs-LG, cs-SD, cs.SD, eess-AS<br>Keyword Score: 70<br>Keywords: Adversarial Learning, Autoencoder, Quantization, Self-Distillation, Self-supervised Learning, Supervised Learning, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16321v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16321v1.pdf filename=2402.16321v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Speech quality estimation has recently undergone a paradigm shift from human-hearing expert designs to machine-learning models. However, current models rely mainly on <b>supervised</b> <b>learning,</b> which is time-consuming and expensive for label collection. To solve this problem, we propose VQScore, a <b>self-supervised</b> metric for evaluating speech based on the <b>quantization</b> error of a vector-quantized-variational <b>autoencoder</b> (VQ-VAE). The training of VQ-VAE relies on clean speech; hence, large <b>quantization</b> errors can be expected when the speech is distorted. To further improve correlation with real quality scores, domain knowledge of speech processing is incorporated into the model design. We found that the vector <b>quantization</b> mechanism could also be used for <b>self-supervised</b> speech enhancement (SE) model training. To improve the robustness of the encoder for SE, a novel <b>self-distillation</b> mechanism combined with <b>adversarial</b> <b>training</b> is introduced. In summary, the proposed speech quality estimation method and enhancement models require only clean speech for training without any label requirements. Experimental results show that the proposed VQScore and enhancement model are competitive with <b>supervised</b> <b>baselines.</b> The code will be released after publication.</p></p class="citation"></blockquote><h3 id=22--177279-towards-environmental-preference-based-speech-enhancement-for-individualised-multi-modal-hearing-aids-jasper-kirton-wingate-et-al-2024>(2/2 | 177/279) Towards Environmental Preference Based Speech Enhancement For Individualised Multi-Modal Hearing Aids (Jasper Kirton-Wingate et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jasper Kirton-Wingate, Shafique Ahmed, Adeel Hussain, Mandar Gogate, Kia Dashtipour, Jen-Cheng Hou, Tassadaq Hussain, Yu Tsao, Amir Hussain. (2024)<br><strong>Towards Environmental Preference Based Speech Enhancement For Individualised Multi-Modal Hearing Aids</strong><br><button class=copy-to-clipboard title="Towards Environmental Preference Based Speech Enhancement For Individualised Multi-Modal Hearing Aids" index=177>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-177 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SD<br>Categories: cs-SD, cs.SD, eess-AS<br>Keyword Score: 3<br>Keywords: Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16757v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16757v1.pdf filename=2402.16757v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Since the advent of Deep Learning (DL), Speech Enhancement (SE) models have performed well under a variety of noise conditions. However, such systems may still introduce sonic artefacts, sound unnatural, and restrict the ability for a user to hear ambient sound which may be of importance. Hearing Aid (HA) users may wish to customise their SE systems to suit their personal preferences and day-to-day lifestyle. In this paper, we introduce a preference learning based SE (PLSE) model for future <b>multi-modal</b> HAs that can contextually exploit audio information to improve listening comfort, based upon the preferences of the user. The proposed system estimates the Signal-to-noise ratio (SNR) as a basic objective speech quality measure which quantifies the relative amount of background noise present in speech, and directly correlates to the intelligibility of the signal. Additionally, to provide contextual information we predict the acoustic scene in which the user is situated. These tasks are achieved via a multi-task DL model, which surpasses the performance of inferring the acoustic scene or SNR separately, by jointly leveraging a shared encoded feature space. These environmental inferences are exploited in a preference elicitation framework, which linearly learns a set of predictive functions to determine the target SNR of an AV (Audio-Visual) SE system. By greatly reducing noise in challenging listening conditions, and by novelly scaling the output of the SE model, we are able to provide HA users with contextually individualised SE. Preliminary results suggest an improvement over the non-individualised baseline model in some participants.</p></p class="citation"></blockquote><h2 id=csse-7>cs.SE (7)</h2><h3 id=17--178279-clap-learning-transferable-binary-code-representations-with-natural-language-supervision-hao-wang-et-al-2024>(1/7 | 178/279) CLAP: Learning Transferable Binary Code Representations with Natural Language Supervision (Hao Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hao Wang, Zeyu Gao, Chao Zhang, Zihan Sha, Mingyang Sun, Yuchen Zhou, Wenyu Zhu, Wenju Sun, Han Qiu, Xi Xiao. (2024)<br><strong>CLAP: Learning Transferable Binary Code Representations with Natural Language Supervision</strong><br><button class=copy-to-clipboard title="CLAP: Learning Transferable Binary Code Representations with Natural Language Supervision" index=178>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-178 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-AI, cs-SE, cs.SE<br>Keyword Score: 55<br>Keywords: Few-shot, Representation Learning, Supervised Learning, Transfer Learning, Zero-shot, Natural Language Explanation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16928v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16928v1.pdf filename=2402.16928v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Binary code <b>representation</b> <b>learning</b> has shown significant performance in binary analysis tasks. But existing solutions often have poor transferability, particularly in <b>few-shot</b> and <b>zero-shot</b> scenarios where few or no training samples are available for the tasks. To address this problem, we present CLAP (Contrastive Language-Assembly Pre-training), which employs <b>natural</b> <b>language</b> <b>supervision</b> to learn better <b>representations</b> <b>of</b> binary code (i.e., assembly code) and get better transferability. At the core, our approach boosts superior <b>transfer</b> <b>learning</b> capabilities by effectively aligning binary code with their semantics explanations (in <b>natural</b> <b>language),</b> <b>resulting</b> a model able to generate better embeddings for binary code. To enable this alignment training, we then propose an efficient dataset engine that could automatically generate a large and diverse dataset comprising of binary code and corresponding <b>natural</b> <b>language</b> <b>explanations.</b> We have generated 195 million pairs of binary code and explanations and trained a prototype of CLAP. The evaluations of CLAP across various downstream tasks in binary analysis all demonstrate exceptional performance. Notably, without any task-specific training, CLAP is often competitive with a fully <b>supervised</b> baseline, showing excellent transferability. We release our pre-trained model and code at <a href=https://github.com/Hustcw/CLAP>https://github.com/Hustcw/CLAP</a>.</p></p class="citation"></blockquote><h3 id=27--179279-beyond-self-learned-attention-mitigating-attention-bias-in-transformer-based-models-using-attention-guidance-jiri-gesi-et-al-2024>(2/7 | 179/279) Beyond Self-learned Attention: Mitigating Attention Bias in Transformer-based Models Using Attention Guidance (Jiri Gesi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiri Gesi, Iftekhar Ahmed. (2024)<br><strong>Beyond Self-learned Attention: Mitigating Attention Bias in Transformer-based Models Using Attention Guidance</strong><br><button class=copy-to-clipboard title="Beyond Self-learned Attention: Mitigating Attention Bias in Transformer-based Models Using Attention Guidance" index=179>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-179 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-AI, cs-SE, cs.SE<br>Keyword Score: 40<br>Keywords: Fine-tuning, Fine-tuning, Transformer, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16790v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16790v1.pdf filename=2402.16790v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Transformer-based</b> models have demonstrated considerable potential for source code modeling tasks in software engineering. However, they are limited by their dependence solely on automatic <b>self-attention</b> weight learning mechanisms. Previous studies have shown that these models overemphasize delimiters added by tokenizers (e.g., [CLS], [SEP]), which may lead to overlooking essential information in the original input source code. To address this challenge, we introduce SyntaGuid, a novel approach that utilizes the observation that attention weights tend to be biased towards specific source code syntax tokens and abstract syntax tree (AST) elements in <b>fine-tuned</b> language models when they make correct predictions. SyntaGuid facilitates the guidance of attention-weight learning, leading to improved model performance on various software engineering tasks. We evaluate the effectiveness of SyntaGuid on multiple tasks and demonstrate that it outperforms existing state-of-the-art models in overall performance without requiring additional data. Experimental result shows that SyntaGuid can improve overall performance up to 3.25% and fix up to 28.3% wrong predictions. Our work represents the first attempt to guide the attention of <b>Transformer-based</b> models towards critical source code tokens during <b>fine-tuning,</b> highlighting the potential for enhancing <b>Transformer-based</b> models in software engineering.</p></p class="citation"></blockquote><h3 id=37--180279-dealing-with-data-for-re-mitigating-challenges-while-using-nlp-and-generative-ai-smita-ghaisas-et-al-2024>(3/7 | 180/279) Dealing with Data for RE: Mitigating Challenges while using NLP and Generative AI (Smita Ghaisas et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Smita Ghaisas, Anmol Singhal. (2024)<br><strong>Dealing with Data for RE: Mitigating Challenges while using NLP and Generative AI</strong><br><button class=copy-to-clipboard title="Dealing with Data for RE: Mitigating Challenges while using NLP and Generative AI" index=180>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-180 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-CL, cs-SE, cs.SE<br>Keyword Score: 33<br>Keywords: Benchmarking, Generative AI, Supervised Learning, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16977v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16977v2.pdf filename=2402.16977v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Across the dynamic business landscape today, enterprises face an ever-increasing range of challenges. These include the constantly evolving regulatory environment, the growing demand for personalization within software applications, and the heightened emphasis on governance. In response to these multifaceted demands, large enterprises have been adopting automation that spans from the optimization of core business processes to the enhancement of customer experiences. Indeed, Artificial Intelligence (AI) has emerged as a pivotal element of modern software systems. In this context, data plays an indispensable role. AI-centric software systems based on <b>supervised</b> <b>learning</b> and operating at an industrial scale require large volumes of training data to perform effectively. Moreover, the incorporation of <b>generative</b> <b>AI</b> has led to a growing demand for adequate evaluation <b>benchmarks.</b> Our experience in this field has revealed that the requirement for large datasets for training and evaluation introduces a host of intricate challenges. This book chapter explores the evolving landscape of Software Engineering (SE) in general, and Requirements Engineering (RE) in particular, in this era marked by AI integration. We discuss challenges that arise while integrating Natural Language Processing (NLP) and <b>generative</b> <b>AI</b> into enterprise-critical software systems. The chapter provides practical insights, solutions, and examples to equip readers with the knowledge and tools necessary for effectively building solutions with NLP at their cores. We also reflect on how these text data-centric tasks sit together with the traditional RE process. We also highlight new RE tasks that may be necessary for handling the increasingly important text data-centricity involved in developing software systems.</p></p class="citation"></blockquote><h3 id=47--181279-unveiling-chatgpts-usage-in-open-source-projects-a-mining-based-study-rosalia-tufano-et-al-2024>(4/7 | 181/279) Unveiling ChatGPT&rsquo;s Usage in Open Source Projects: A Mining-based Study (Rosalia Tufano et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rosalia Tufano, Antonio Mastropaolo, Federica Pepe, Ozren Dabić, Massimiliano Di Penta, Gabriele Bavota. (2024)<br><strong>Unveiling ChatGPT&rsquo;s Usage in Open Source Projects: A Mining-based Study</strong><br><button class=copy-to-clipboard title="Unveiling ChatGPT's Usage in Open Source Projects: A Mining-based Study" index=181>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-181 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-SE, cs.SE<br>Keyword Score: 30<br>Keywords: ChatGPT, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16480v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16480v1.pdf filename=2402.16480v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have gained significant attention in the software engineering community. Nowadays developers have the possibility to exploit these models through industrial-grade tools providing a handy interface toward <b>LLMs,</b> such as OpenAI&rsquo;s <b>ChatGPT.</b> While the potential of <b>LLMs</b> in assisting developers across several tasks has been documented in the literature, there is a lack of empirical evidence mapping the actual usage of <b>LLMs</b> in software projects. In this work, we aim at filling such a gap. First, we mine 1,501 commits, pull requests (PRs), and issues from open-source projects by matching regular expressions likely to indicate the usage of <b>ChatGPT</b> to accomplish the task. Then, we manually analyze these instances, discarding false positives (i.e., instances in which <b>ChatGPT</b> was mentioned but not actually used) and categorizing the task automated in the 467 true positive instances (165 commits, 159 PRs, 143 issues). This resulted in a taxonomy of 45 tasks which developers automate via <b>ChatGPT.</b> The taxonomy, accompanied with representative examples, provides (i) developers with valuable insights on how to exploit <b>LLMs</b> in their workflow and (ii) researchers with a clear overview of tasks that, according to developers, could benefit from automated solutions.</p></p class="citation"></blockquote><h3 id=57--182279-quality-assurance-for-artificial-intelligence-a-study-of-industrial-concerns-challenges-and-best-practices-chenyu-wang-et-al-2024>(5/7 | 182/279) Quality Assurance for Artificial Intelligence: A Study of Industrial Concerns, Challenges and Best Practices (Chenyu Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chenyu Wang, Zhou Yang, Ze Shi Li, Daniela Damian, David Lo. (2024)<br><strong>Quality Assurance for Artificial Intelligence: A Study of Industrial Concerns, Challenges and Best Practices</strong><br><button class=copy-to-clipboard title="Quality Assurance for Artificial Intelligence: A Study of Industrial Concerns, Challenges and Best Practices" index=182>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-182 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-SE, cs.SE<br>Keyword Score: 30<br>Keywords: Fairness, Model Compression, Question Answering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16391v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16391v1.pdf filename=2402.16391v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Quality Assurance <b>(QA)</b> aims to prevent mistakes and defects in manufactured products and avoid problems when delivering products or services to customers. <b>QA</b> for AI systems, however, poses particular challenges, given their data-driven and non-deterministic nature as well as more complex architectures and algorithms. While there is growing empirical evidence about practices of machine learning in industrial contexts, little is known about the challenges and best practices of quality assurance for AI systems (QA4AI). In this paper, we report on a mixed-method study of QA4AI in industry practice from various countries and companies. Through interviews with fifteen industry practitioners and a validation survey with 50 practitioner responses, we studied the concerns as well as challenges and best practices in ensuring the QA4AI properties reported in the literature, such as correctness, <b>fairness,</b> interpretability and others. Our findings suggest correctness as the most important property, followed by <b>model</b> <b>relevance,</b> efficiency and deployability. In contrast, transferability (applying knowledge learned in one task to another task), security and <b>fairness</b> are not paid much attention by practitioners compared to other properties. Challenges and solutions are identified for each QA4AI property. For example, interviewees highlighted the trade-off challenge among latency, cost and accuracy for efficiency (latency and cost are parts of efficiency concern). Solutions like <b>model</b> <b>compression</b> are proposed. We identified 21 QA4AI practices across each stage of AI development, with 10 practices being well recognized and another 8 practices being marginally agreed by the survey practitioners.</p></p class="citation"></blockquote><h3 id=67--183279-promptset-a-programmers-prompting-dataset-kaiser-pister-et-al-2024>(6/7 | 183/279) PromptSet: A Programmer&rsquo;s Prompting Dataset (Kaiser Pister et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kaiser Pister, Dhruba Jyoti Paul, Patrick Brophy, Ishan Joshi. (2024)<br><strong>PromptSet: A Programmer&rsquo;s Prompting Dataset</strong><br><button class=copy-to-clipboard title="PromptSet: A Programmer's Prompting Dataset" index=183>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-183 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-SE, cs.SE<br>Keyword Score: 25<br>Keywords: Black Box, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16932v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16932v1.pdf filename=2402.16932v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The rise of capabilities expressed by <b>large</b> <b>language</b> <b>models</b> has been quickly followed by the integration of the same complex systems into application level logic. Algorithms, programs, systems, and companies are built around structured <b>prompting</b> to <b>black</b> <b>box</b> models where the majority of the design and implementation lies in capturing and quantifying the `agent mode&rsquo;. The standard way to shape a closed language model is to prime it for a specific task with a tailored <b>prompt,</b> often initially handwritten by a human. The textual <b>prompts</b> co-evolve with the codebase, taking shape over the course of project life as artifacts which must be reviewed and maintained, just as the traditional code files might be. Unlike traditional code, we find that <b>prompts</b> do not receive effective static testing and linting to prevent runtime issues. In this work, we present a novel dataset called PromptSet, with more than 61,000 unique developer <b>prompts</b> used in open source Python programs. We perform analysis on this dataset and introduce the notion of a static linter for <b>prompts.</b> Released with this publication is a HuggingFace dataset and a Github repository to recreate collection and processing efforts, both under the name \texttt{pisterlabs/promptset}.</p></p class="citation"></blockquote><h3 id=77--184279-langgpt-rethinking-structured-reusable-prompt-design-framework-for-llms-from-the-programming-language-ming-wang-et-al-2024>(7/7 | 184/279) LangGPT: Rethinking Structured Reusable Prompt Design Framework for LLMs from the Programming Language (Ming Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ming Wang, Yuanzhong Liu, Xiaoming Zhang, Songlian Li, Yijie Huang, Chi Zhang, Daling Wang, Shi Feng, Jigang Li. (2024)<br><strong>LangGPT: Rethinking Structured Reusable Prompt Design Framework for LLMs from the Programming Language</strong><br><button class=copy-to-clipboard title="LangGPT: Rethinking Structured Reusable Prompt Design Framework for LLMs from the Programming Language" index=184>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-184 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-AI, cs-CL, cs-PL, cs-SE, cs.SE<br>Keyword Score: 20<br>Keywords: Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16929v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16929v1.pdf filename=2402.16929v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>LLMs</b> have demonstrated commendable performance across diverse domains. Nevertheless, formulating high-quality <b>prompts</b> to effectively instruct <b>LLMs</b> poses a challenge for non-AI experts. Existing research in <b>prompt</b> engineering suggests somewhat fragmented optimization principles and designs empirically dependent <b>prompt</b> optimizers. Unfortunately, these endeavors lack a structured design template, incurring high learning costs and resulting in low reusability. Inspired by structured reusable programming languages, we propose LangGPT, a dual-layer <b>prompt</b> design framework as the programming language for <b>LLMs.</b> LangGPT has an easy-to-learn normative structure and provides an extended structure for migration and reuse. Experiments illustrate that LangGPT significantly enhances the capacity of <b>LLMs</b> to produce responses of superior quality compared to baselines. Moreover, LangGPT has proven effective in guiding <b>LLMs</b> to generate high-quality <b>prompts.</b> We have built a community on LangGPT to facilitate the tuition and sharing of <b>prompt</b> design. We also analyzed the ease of use and reusability of LangGPT through a community user survey.</p></p class="citation"></blockquote><h2 id=eessiv-3>eess.IV (3)</h2><h3 id=13--185279-investigating-the-robustness-of-vision-transformers-against-label-noise-in-medical-image-classification-bidur-khanal-et-al-2024>(1/3 | 185/279) Investigating the Robustness of Vision Transformers against Label Noise in Medical Image Classification (Bidur Khanal et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bidur Khanal, Prashant Shrestha, Sanskar Amgain, Bishesh Khanal, Binod Bhattarai, Cristian A. Linte. (2024)<br><strong>Investigating the Robustness of Vision Transformers against Label Noise in Medical Image Classification</strong><br><button class=copy-to-clipboard title="Investigating the Robustness of Vision Transformers against Label Noise in Medical Image Classification" index=185>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-185 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, cs-LG, eess-IV, eess.IV<br>Keyword Score: 50<br>Keywords: Vision Transformer, Convolutional Neural Network, Supervised Learning, Transformer, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16734v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16734v1.pdf filename=2402.16734v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Label noise in medical image classification datasets significantly hampers the training of <b>supervised</b> deep learning methods, undermining their generalizability. The test performance of a model tends to decrease as the label noise rate increases. Over recent years, several methods have been proposed to mitigate the impact of label noise in medical image classification and enhance the robustness of the model. Predominantly, these works have employed <b>CNN-based</b> architectures as the backbone of their classifiers for feature extraction. However, in recent years, <b>Vision</b> <b>Transformer</b> (ViT)-based backbones have replaced <b>CNNs,</b> demonstrating improved performance and a greater ability to learn more generalizable features, especially when the dataset is large. Nevertheless, no prior work has rigorously investigated how <b>transformer-based</b> backbones handle the impact of label noise in medical image classification. In this paper, we investigate the architectural robustness of ViT against label noise and compare it to that of <b>CNNs.</b> We use two medical image classification datasets &ndash; COVID-DU-Ex, and NCT-CRC-HE-100K &ndash; both corrupted by injecting label noise at various rates. Additionally, we show that pretraining is crucial for ensuring ViT&rsquo;s improved robustness against label noise in <b>supervised</b> training.</p></p class="citation"></blockquote><h3 id=23--186279-un-sam-universal-prompt-free-segmentation-for-generalized-nuclei-images-zhen-chen-et-al-2024>(2/3 | 186/279) UN-SAM: Universal Prompt-Free Segmentation for Generalized Nuclei Images (Zhen Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhen Chen, Qing Xu, Xinyu Liu, Yixuan Yuan. (2024)<br><strong>UN-SAM: Universal Prompt-Free Segmentation for Generalized Nuclei Images</strong><br><button class=copy-to-clipboard title="UN-SAM: Universal Prompt-Free Segmentation for Generalized Nuclei Images" index=186>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-186 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 20<br>Keywords: Zero-shot, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16663v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16663v1.pdf filename=2402.16663v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In digital pathology, precise nuclei segmentation is pivotal yet challenged by the diversity of tissue types, staining protocols, and imaging conditions. Recently, the segment anything model (SAM) revealed overwhelming performance in natural scenarios and impressive adaptation to medical imaging. Despite these advantages, the reliance of labor-intensive manual annotation as segmentation <b>prompts</b> severely hinders their clinical applicability, especially for nuclei image analysis containing massive cells where dense manual <b>prompts</b> are impractical. To overcome the limitations of current SAM methods while retaining the advantages, we propose the Universal <b>prompt-free</b> SAM framework for Nuclei segmentation (UN-SAM), by providing a fully automated solution with remarkable generalization capabilities. Specifically, to eliminate the labor-intensive requirement of per-nuclei annotations for <b>prompt,</b> we devise a multi-scale Self-Prompt Generation (SPGen) module to revolutionize clinical workflow by automatically generating high-quality mask hints to guide the segmentation tasks. Moreover, to unleash the generalization capability of SAM across a variety of nuclei images, we devise a Domain-adaptive Tuning Encoder (DT-Encoder) to seamlessly harmonize visual features with domain-common and domain-specific knowledge, and further devise a Domain Query-enhanced Decoder (DQ-Decoder) by leveraging learnable domain queries for segmentation decoding in different nuclei domains. Extensive experiments prove that UN-SAM with exceptional performance surpasses state-of-the-arts in nuclei instance and semantic segmentation, especially the generalization capability in <b>zero-shot</b> scenarios. The source code is available at <a href=https://github.com/CUHK-AIM-Group/UN-SAM>https://github.com/CUHK-AIM-Group/UN-SAM</a>.</p></p class="citation"></blockquote><h3 id=33--187279-spineps----automatic-whole-spine-segmentation-of-t2-weighted-mr-images-using-a-two-phase-approach-to-multi-class-semantic-and-instance-segmentation-hendrik-möller-et-al-2024>(3/3 | 187/279) SPINEPS &ndash; Automatic Whole Spine Segmentation of T2-weighted MR images using a Two-Phase Approach to Multi-class Semantic and Instance Segmentation (Hendrik Möller et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hendrik Möller, Robert Graf, Joachim Schmitt, Benjamin Keinert, Matan Atad, Anjany Sekuboyina, Felix Streckenbach, Hanna Schön, Florian Kofler, Thomas Kroencke, Stefanie Bette, Stefan Willich, Thomas Keil, Thoralf Niendorf, Tobias Pischon, Beate Endemann, Bjoern Menze, Daniel Rueckert, Jan S. Kirschke. (2024)<br><strong>SPINEPS &ndash; Automatic Whole Spine Segmentation of T2-weighted MR images using a Two-Phase Approach to Multi-class Semantic and Instance Segmentation</strong><br><button class=copy-to-clipboard title="SPINEPS -- Automatic Whole Spine Segmentation of T2-weighted MR images using a Two-Phase Approach to Multi-class Semantic and Instance Segmentation" index=187>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-187 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 10<br>Keywords: Out-of-distribution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16368v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16368v1.pdf filename=2402.16368v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Purpose. To present SPINEPS, an open-source deep learning approach for semantic and instance segmentation of 14 spinal structures (ten vertebra substructures, intervertebral discs, spinal cord, spinal canal, and sacrum) in whole body T2w MRI. Methods. During this HIPPA-compliant, retrospective study, we utilized the public SPIDER dataset (218 subjects, 63% female) and a subset of the German National Cohort (1423 subjects, mean age 53, 49% female) for training and evaluation. We combined CT and T2w segmentations to train models that segment 14 spinal structures in T2w sagittal scans both semantically and instance-wise. Performance evaluation metrics included Dice similarity coefficient, average symmetrical surface distance, panoptic quality, segmentation quality, and recognition quality. Statistical significance was assessed using the Wilcoxon signed-rank test. An in-house dataset was used to qualitatively evaluate <b>out-of-distribution</b> samples. Results. On the public dataset, our approach outperformed the baseline (instance-wise vertebra dice score 0.929 vs. 0.907, p-value&lt;0.001). Training on auto-generated annotations and evaluating on manually corrected test data from the GNC yielded global dice scores of 0.900 for vertebrae, 0.960 for intervertebral discs, and 0.947 for the spinal canal. Incorporating the SPIDER dataset during training increased these scores to 0.920, 0.967, 0.958, respectively. Conclusions. The proposed segmentation approach offers robust segmentation of 14 spinal structures in T2w sagittal images, including the spinal cord, spinal canal, intervertebral discs, endplate, sacrum, and vertebrae. The approach yields both a semantic and instance mask as output, thus being easy to utilize. This marks the first publicly available algorithm for whole spine segmentation in sagittal T2w MR imaging.</p></p class="citation"></blockquote><h2 id=hep-ex-1>hep-ex (1)</h2><h3 id=11--188279-a-comparison-of-deep-learning-models-for-proton-background-rejection-with-the-ams-electromagnetic-calorimeter-raheem-karim-hashmani-et-al-2024>(1/1 | 188/279) A Comparison of Deep Learning Models for Proton Background Rejection with the AMS Electromagnetic Calorimeter (Raheem Karim Hashmani et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Raheem Karim Hashmani, Emre Akbaş, Melahat Bilge Demirköz. (2024)<br><strong>A Comparison of Deep Learning Models for Proton Background Rejection with the AMS Electromagnetic Calorimeter</strong><br><button class=copy-to-clipboard title="A Comparison of Deep Learning Models for Proton Background Rejection with the AMS Electromagnetic Calorimeter" index=188>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-188 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: hep-ex<br>Categories: cs-LG, hep-ex, hep-ex<br>Keyword Score: 50<br>Keywords: Vision Transformer, Convolution, Convolutional Neural Network, Transformer, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16285v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16285v1.pdf filename=2402.16285v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The Alpha Magnetic Spectrometer (AMS) is a high-precision particle detector onboard the International Space Station containing six different subdetectors. The Transition Radiation Detector and Electromagnetic Calorimeter (ECAL) are used to separate electrons/positrons from the abundant cosmic-ray proton background. The positron flux measured in space by AMS falls with a power law which unexpectedly softens above 25 GeV and then hardens above 280 GeV. Several theoretical models try to explain these phenomena, and a purer measurement of positrons at higher energies is needed to help test them. The currently used methods to reject the proton background at high energies involve extrapolating shower features from the ECAL to use as inputs for boosted decision tree and likelihood classifiers. We present a new approach for particle identification with the AMS ECAL using deep learning (DL). By taking the energy deposition within all the ECAL cells as an input and treating them as pixels in an image-like format, we train an MLP, a <b>CNN,</b> and multiple ResNets and <b>Convolutional</b> <b>vision</b> <b>Transformers</b> (CvTs) as shower classifiers. Proton rejection performance is evaluated using Monte Carlo (MC) events and ISS data separately. For MC, using events with a reconstructed energy between 0.2 - 2 TeV, at 90% electron accuracy, the proton rejection power of our CvT model is more than 5 times that of the other DL models. Similarly, for ISS data with a reconstructed energy between 50 - 70 GeV, the proton rejection power of our CvT model is more than 2.5 times that of the other DL models.</p></p class="citation"></blockquote><h2 id=csro-13>cs.RO (13)</h2><h3 id=113--189279-phygrasp-generalizing-robotic-grasping-with-physics-informed-large-multimodal-models-dingkun-guo-et-al-2024>(1/13 | 189/279) PhyGrasp: Generalizing Robotic Grasping with Physics-informed Large Multimodal Models (Dingkun Guo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dingkun Guo, Yuqi Xiang, Shuqi Zhao, Xinghao Zhu, Masayoshi Tomizuka, Mingyu Ding, Wei Zhan. (2024)<br><strong>PhyGrasp: Generalizing Robotic Grasping with Physics-informed Large Multimodal Models</strong><br><button class=copy-to-clipboard title="PhyGrasp: Generalizing Robotic Grasping with Physics-informed Large Multimodal Models" index=189>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-189 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-CL, cs-CV, cs-RO, cs.RO<br>Keyword Score: 46<br>Keywords: Multi-modal, Multi-modal, Simulation, Simulator, Common-sense Reasoning, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16836v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16836v1.pdf filename=2402.16836v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Robotic grasping is a fundamental aspect of robot functionality, defining how robots interact with objects. Despite substantial progress, its generalizability to counter-intuitive or long-tailed scenarios, such as objects with uncommon materials or shapes, remains a challenge. In contrast, humans can easily apply their intuitive physics to grasp skillfully and change grasps efficiently, even for objects they have never seen before. This work delves into infusing such physical <b>commonsense</b> <b>reasoning</b> into robotic manipulation. We introduce PhyGrasp, a <b>multimodal</b> large model that leverages inputs from two modalities: natural language and 3D point clouds, seamlessly integrated through a bridge module. The language modality exhibits robust <b>reasoning</b> capabilities concerning the impacts of diverse physical properties on grasping, while the 3D modality comprehends object shapes and parts. With these two capabilities, PhyGrasp is able to accurately assess the physical properties of object parts and determine optimal grasping poses. Additionally, the model&rsquo;s language comprehension enables human instruction interpretation, generating grasping poses that align with human preferences. To train PhyGrasp, we construct a dataset PhyPartNet with 195K object instances with varying physical properties and human preferences, alongside their corresponding language descriptions. Extensive experiments conducted in the <b>simulation</b> and on the real robots demonstrate that PhyGrasp achieves state-of-the-art performance, particularly in long-tailed cases, e.g., about 10% improvement in success rate over GraspNet. Project page: <a href=https://sites.google.com/view/phygrasp>https://sites.google.com/view/phygrasp</a></p></p class="citation"></blockquote><h3 id=213--190279-trajectory-prediction-for-autonomous-driving-using-a-transformer-network-zhenning-li-et-al-2024>(2/13 | 190/279) Trajectory Prediction for Autonomous Driving Using a Transformer Network (Zhenning Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhenning Li, Hao Yu. (2024)<br><strong>Trajectory Prediction for Autonomous Driving Using a Transformer Network</strong><br><button class=copy-to-clipboard title="Trajectory Prediction for Autonomous Driving Using a Transformer Network" index=190>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-190 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 33<br>Keywords: Convolution, Convolutional Neural Network, Multi-modal, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16501v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16501v1.pdf filename=2402.16501v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Predicting the trajectories of surrounding agents is still considered one of the most challenging tasks for autonomous driving. In this paper, we introduce a <b>multi-modal</b> trajectory prediction framework based on the <b>transformer</b> network. The semantic maps of each agent are used as inputs to <b>convolutional</b> <b>networks</b> to automatically derive relevant contextual information. A novel auxiliary loss that penalizes unfeasible off-road predictions is also proposed in this study. Experiments on the Lyft l5kit dataset show that the proposed model achieves state-of-the-art performance, substantially improving the accuracy and feasibility of the prediction outcomes.</p></p class="citation"></blockquote><h3 id=313--191279-expressive-whole-body-control-for-humanoid-robots-xuxin-cheng-et-al-2024>(3/13 | 191/279) Expressive Whole-Body Control for Humanoid Robots (Xuxin Cheng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xuxin Cheng, Yandong Ji, Junming Chen, Ruihan Yang, Ge Yang, Xiaolong Wang. (2024)<br><strong>Expressive Whole-Body Control for Humanoid Robots</strong><br><button class=copy-to-clipboard title="Expressive Whole-Body Control for Humanoid Robots" index=191>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-191 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-LG, cs-RO, cs.RO<br>Keyword Score: 30<br>Keywords: Reinforcement Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16796v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16796v1.pdf filename=2402.16796v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Can we enable humanoid robots to generate rich, diverse, and expressive motions in the real world? We propose to learn a whole-body control policy on a human-sized robot to mimic human motions as realistic as possible. To train such a policy, we leverage the large-scale human motion capture data from the graphics community in a <b>Reinforcement</b> <b>Learning</b> framework. However, directly performing imitation learning with the motion capture dataset would not work on the real humanoid robot, given the large gap in degrees of freedom and physical capabilities. Our method Expressive Whole-Body Control (Exbody) tackles this problem by encouraging the upper humanoid body to imitate a reference motion, while relaxing the imitation constraint on its two legs and only requiring them to follow a given velocity robustly. With training in <b>simulation</b> and Sim2Real transfer, our policy can control a humanoid robot to walk in different styles, shake hands with humans, and even dance with a human in the real world. We conduct extensive studies and comparisons on diverse motions in both <b>simulation</b> and the real world to show the effectiveness of our approach.</p></p class="citation"></blockquote><h3 id=413--192279-learning-based-nmpc-adaptation-for-autonomous-driving-using-parallelized-digital-twin-jean-pierre-allamaa-et-al-2024>(4/13 | 192/279) Learning Based NMPC Adaptation for Autonomous Driving using Parallelized Digital Twin (Jean Pierre Allamaa et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jean Pierre Allamaa, Panagiotis Patrinos, Herman Van der Auweraer, Tong Duy Son. (2024)<br><strong>Learning Based NMPC Adaptation for Autonomous Driving using Parallelized Digital Twin</strong><br><button class=copy-to-clipboard title="Learning Based NMPC Adaptation for Autonomous Driving using Parallelized Digital Twin" index=192>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-192 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs-SY, cs.RO, eess-SY<br>Keyword Score: 25<br>Keywords: Black Box, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16645v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16645v1.pdf filename=2402.16645v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this work, we address the problem of transferring an autonomous driving (AD) module from one domain to another, in particular from <b>simulation</b> to the real world (Sim2Real). We propose a data-efficient method for online and on-the-fly learning based adaptation for parametrizable control architectures such that the target closed-loop performance is optimized under several uncertainty sources such as model mismatches, environment changes and task choice. The novelty of the work resides in leveraging <b>black-box</b> <b>optimization</b> enabled by executable digital twins, with data-driven hyper-parameter tuning through derivative-free methods to directly adapt in real-time the AD module. Our proposed method requires a minimal amount of interaction with the real-world in the randomization and online training phase. Specifically, we validate our approach in real-world experiments and show the ability to transfer and safely tune a nonlinear model predictive controller in less than 10 minutes, eliminating the need of day-long manual tuning and hours-long machine learning training phases. Our results show that the online adapted NMPC directly compensates for disturbances, avoids overtuning in <b>simulation</b> and for one specific task, and it generalizes for less than 15cm of tracking accuracy over a multitude of trajectories, and leads to 83% tracking improvement.</p></p class="citation"></blockquote><h3 id=513--193279-the-door-and-drawer-reset-mechanisms-automated-mechanisms-for-testing-and-data-collection-kyle-dufrene-et-al-2024>(5/13 | 193/279) The Door and Drawer Reset Mechanisms: Automated Mechanisms for Testing and Data Collection (Kyle DuFrene et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kyle DuFrene, Luke Strohbehn, Keegan Nave, Ravi Balasubramanian, Cindy Grimm. (2024)<br><strong>The Door and Drawer Reset Mechanisms: Automated Mechanisms for Testing and Data Collection</strong><br><button class=copy-to-clipboard title="The Door and Drawer Reset Mechanisms: Automated Mechanisms for Testing and Data Collection" index=193>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-193 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 23<br>Keywords: Benchmarking, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16759v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16759v1.pdf filename=2402.16759v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Robotic manipulation in human environments is a challenging problem for researchers and industry alike. In particular, opening doors/drawers can be challenging for robots, as the size, shape, actuation and required force is variable. Because of this, it can be difficult to collect large real-world datasets and to <b>benchmark</b> different control algorithms on the same hardware. In this paper we present two automated testbeds, the Door Reset Mechanism (DORM) and Drawer Reset Mechanism (DWRM), for the purpose of real world testing and data collection. These devices are low-cost, are sensorized, operate with customized variable resistance, and come with open source software. Additionally, we provide a dataset of over 600 grasps using the DORM and DWRM. We use this dataset to highlight how much variability can exist even with the same trial on the same hardware. This data can also serve as a source for real-world noise in <b>simulation</b> environments.</p></p class="citation"></blockquote><h3 id=613--194279-star-searcher-a-complete-and-efficient-aerial-system-for-autonomous-target-search-in-complex-unknown-environments-yiming-luo-et-al-2024>(6/13 | 194/279) Star-Searcher: A Complete and Efficient Aerial System for Autonomous Target Search in Complex Unknown Environments (Yiming Luo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yiming Luo, Zixuan Zhuang, Neng Pan, Chen Feng, Shaojie Shen, Fei Gao, Hui Cheng, Boyu Zhou. (2024)<br><strong>Star-Searcher: A Complete and Efficient Aerial System for Autonomous Target Search in Complex Unknown Environments</strong><br><button class=copy-to-clipboard title="Star-Searcher: A Complete and Efficient Aerial System for Autonomous Target Search in Complex Unknown Environments" index=194>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-194 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 23<br>Keywords: Clustering, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16348v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16348v1.pdf filename=2402.16348v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper tackles the challenge of autonomous target search using unmanned aerial vehicles (UAVs) in complex unknown environments. To fill the gap in systematic approaches for this task, we introduce Star-Searcher, an aerial system featuring specialized sensor suites, mapping, and planning modules to optimize searching. Path planning challenges due to increased inspection requirements are addressed through a hierarchical planner with a visibility-based viewpoint <b>clustering</b> method. This simplifies planning by breaking it into global and local sub-problems, ensuring efficient global and local path coverage in real-time. Furthermore, our global path planning employs a history-aware mechanism to reduce motion inconsistency from frequent map changes, significantly enhancing search efficiency. We conduct comparisons with state-of-the-art methods in both <b>simulation</b> and the real world, demonstrating shorter flight paths, reduced time, and higher target search completeness. Our approach will be open-sourced for community benefit at <a href=https://github.com/SYSU-STAR/STAR-Searcher>https://github.com/SYSU-STAR/STAR-Searcher</a>.</p></p class="citation"></blockquote><h3 id=713--195279-swarmprm-probabilistic-roadmap-motion-planning-for-swarm-robotic-systems-yunze-hu-et-al-2024>(7/13 | 195/279) SwarmPRM: Probabilistic Roadmap Motion Planning for Swarm Robotic Systems (Yunze Hu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yunze Hu, Xuru Yang, Kangjie Zhou, Qinghang Liu, Kang Ding, Han Gao, Pingping Zhu, Chang Liu. (2024)<br><strong>SwarmPRM: Probabilistic Roadmap Motion Planning for Swarm Robotic Systems</strong><br><button class=copy-to-clipboard title="SwarmPRM: Probabilistic Roadmap Motion Planning for Swarm Robotic Systems" index=195>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-195 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16699v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16699v1.pdf filename=2402.16699v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Swarm robotic systems consisting of large-scale cooperative agents hold promise for performing autonomous tasks in diverse fields. However, existing planning strategies for swarm robotic systems often encounter a trade-off between scalability and solution quality. We introduce here SwarmPRM, a hierarchical, highly scalable, computationally efficient, and risk-aware sampling-based motion planning approach for swarm robotic systems, which is asymptotically optimal under mild assumptions. We employ probability density functions (PDFs) to represent the swarm&rsquo;s macroscopic state and utilize optimal mass transport (OMT) theory to measure the swarm&rsquo;s cost to go. A risk-aware Gaussian roadmap is constructed wherein each node encapsulates a distinct PDF and conditional-value-at-risk (CVaR) is employed to assess the collision risk, facilitating the generation of macroscopic PDFs in Wasserstein-GMM space. Extensive <b>simulations</b> demonstrate that the proposed approach outperforms state-of-the-art methods in terms of computational efficiency and the average travelling distance.</p></p class="citation"></blockquote><h3 id=813--196279-rover-risk-aware-swarm-robotics-motion-planner-using-conditional-value-at-risk-xuru-yang-et-al-2024>(8/13 | 196/279) ROVER: Risk-Aware Swarm Robotics MOtion Planner Using Conditional ValuE at Risk (Xuru Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xuru Yang, Yunze Hu, Han Gao, Kang Ding, Pingping Zhu, Ying Sun, Chang Liu. (2024)<br><strong>ROVER: Risk-Aware Swarm Robotics MOtion Planner Using Conditional ValuE at Risk</strong><br><button class=copy-to-clipboard title="ROVER: Risk-Aware Swarm Robotics MOtion Planner Using Conditional ValuE at Risk" index=196>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-196 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16690v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16690v1.pdf filename=2402.16690v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The field of swarm robotics has attracted considerable interest for its capacity to complete intricate and synchronized tasks. Existing methodologies for motion planning within swarm robotic systems mainly encounter difficulties in scalability and safety guarantee. To address these two limitations, we propose a Risk-aware swarm mOtion planner using conditional ValuE at Risk (ROVER) that systematically modulates the safety and conservativeness and navigates the swarm to the target area through cluttered environments. Our approach formulates a finite-time model predictive control (FTMPC) problem predicated upon the macroscopic state of the robot swarm represented by Gaussian Mixture Model (GMM) and integrates conditional value-at-risk (CVaR) to avoid collision. We leverage the linearized Signed Distance Function for the efficient computation of CVaR concerning the proximity between the robot swarm to obstacles. The key component of this method is implementing CVaR constraint under GMM uncertainty in the FTMPC to measure the collision risk that a robot swarm faces. However, the non-convex constrained FTMPC is nontrival to solve. To navigate this complexity, we develop a computationally tractable strategy through 1) an explicit linear approximation of the CVaR constraint; and 2) a sequential quadratic programming formulation. <b>Simulations</b> and comparisons with other approaches demonstrate the effectiveness of the proposed method in flexibility, scalability, and risk mitigation.</p></p class="citation"></blockquote><h3 id=913--197279-efficient-continuous-time-ego-motion-estimation-for-asynchronous-event-based-data-associations-zhixiang-wang-et-al-2024>(9/13 | 197/279) Efficient Continuous-Time Ego-Motion Estimation for Asynchronous Event-based Data Associations (Zhixiang Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhixiang Wang, Xudong Li, Tianle Liu, Yizhai Zhang, Panfeng Huang. (2024)<br><strong>Efficient Continuous-Time Ego-Motion Estimation for Asynchronous Event-based Data Associations</strong><br><button class=copy-to-clipboard title="Efficient Continuous-Time Ego-Motion Estimation for Asynchronous Event-based Data Associations" index=197>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-197 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Continuous Time, Continuous Time, Gaussian Process<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16398v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16398v1.pdf filename=2402.16398v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Event cameras are bio-inspired vision sensors that asynchronously measure per-pixel brightness changes. The high temporal resolution and asynchronicity of event cameras offer great potential for estimating the robot motion state. Recent works have adopted the <b>continuous-time</b> <b>ego-motion</b> estimation methods to exploit the inherent nature of event cameras. However, most of the adopted methods have poor real-time performance. To alleviate it, a lightweight <b>Gaussian</b> <b>Process</b> (GP)-based estimation framework is proposed to efficiently estimate motion trajectory from asynchronous event-driven data associations. Concretely, an asynchronous front-end pipeline is designed to adapt event-driven feature trackers and generate feature trajectories from event streams; a parallel dynamic sliding-window back-end is presented within the framework of sparse GP regression on SE(3). Notably, a specially designed state marginalization strategy is employed to ensure the consistency and sparsity of this GP regression. Experiments conducted on synthetic and real-world datasets demonstrate that the proposed method achieves competitive precision and superior robustness compared to the state-of-the-art. Furthermore, the evaluations on three 60 s trajectories show that the proposal outperforms the ISAM2-based method in terms of computational efficiency by 2.64, 4.22, and 11.70 times, respectively.</p></p class="citation"></blockquote><h3 id=1013--198279-scaling-robust-optimization-for-multi-agent-robotic-systems-a-distributed-perspective-arshiya-taj-abdul-et-al-2024>(10/13 | 198/279) Scaling Robust Optimization for Multi-Agent Robotic Systems: A Distributed Perspective (Arshiya Taj Abdul et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Arshiya Taj Abdul, Augustinos D. Saravanos, Evangelos A. Theodorou. (2024)<br><strong>Scaling Robust Optimization for Multi-Agent Robotic Systems: A Distributed Perspective</strong><br><button class=copy-to-clipboard title="Scaling Robust Optimization for Multi-Agent Robotic Systems: A Distributed Perspective" index=198>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-198 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs-SY, cs.RO, eess-SY, math-OC<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16227v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16227v1.pdf filename=2402.16227v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents a novel distributed robust optimization scheme for steering distributions of multi-agent systems under stochastic and deterministic uncertainty. Robust optimization is a subfield of optimization which aims in discovering an optimal solution that remains robustly feasible for all possible realizations of the problem parameters within a given uncertainty set. Such approaches would naturally constitute an ideal candidate for multi-robot control, where in addition to stochastic noise, there might be exogenous deterministic disturbances. Nevertheless, as these methods are usually associated with significantly high computational demands, their application to multi-agent robotics has remained limited. The scope of this work is to propose a scalable robust optimization framework that effectively addresses both types of uncertainties, while retaining computational efficiency and scalability. In this direction, we provide tractable approximations for robust constraints that are relevant in multi-robot settings. Subsequently, we demonstrate how computations can be distributed through an Alternating Direction Method of Multipliers (ADMM) approach towards achieving scalability and communication efficiency. <b>Simulation</b> results highlight the performance of the proposed algorithm in effectively handling both stochastic and deterministic uncertainty in multi-robot systems. The scalability of the method is also emphasized by showcasing tasks with up to 100 agents. The results of this work indicate the promise of blending robust optimization, distribution steering and distributed optimization towards achieving scalable, safe and robust multi-robot control.</p></p class="citation"></blockquote><h3 id=1113--199279-dreamup3d-object-centric-generative-models-for-single-view-3d-scene-understanding-and-real-to-sim-transfer-yizhe-wu-et-al-2024>(11/13 | 199/279) DreamUp3D: Object-Centric Generative Models for Single-View 3D Scene Understanding and Real-to-Sim Transfer (Yizhe Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yizhe Wu, Haitz Sáez de Ocáriz Borde, Jack Collins, Oiwi Parker Jones, Ingmar Posner. (2024)<br><strong>DreamUp3D: Object-Centric Generative Models for Single-View 3D Scene Understanding and Real-to-Sim Transfer</strong><br><button class=copy-to-clipboard title="DreamUp3D: Object-Centric Generative Models for Single-View 3D Scene Understanding and Real-to-Sim Transfer" index=199>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-199 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 15<br>Keywords: Representation Learning, Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16308v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16308v1.pdf filename=2402.16308v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>3D scene understanding for robotic applications exhibits a unique set of requirements including real-time inference, object-centric latent <b>representation</b> <b>learning,</b> accurate 6D pose estimation and 3D reconstruction of objects. Current methods for scene understanding typically rely on a combination of trained models paired with either an explicit or learnt volumetric <b>representation,</b> <b>all</b> of which have their own drawbacks and limitations. We introduce DreamUp3D, a novel Object-Centric Generative Model (OCGM) designed explicitly to perform inference on a 3D scene informed only by a single RGB-D image. DreamUp3D is a <b>self-supervised</b> model, trained end-to-end, and is capable of segmenting objects, providing 3D object reconstructions, generating object-centric latent <b>representations</b> <b>and</b> accurate per-object 6D pose estimates. We compare DreamUp3D to baselines including NeRFs, pre-trained CLIP-features, ObSurf, and ObPose, in a range of tasks including 3D scene reconstruction, object matching and object pose estimation. Our experiments show that our model outperforms all baselines by a significant margin in real-world scenarios displaying its applicability for 3D scene understanding tasks while meeting the strict demands exhibited in robotics applications.</p></p class="citation"></blockquote><h3 id=1213--200279-think2drive-efficient-reinforcement-learning-by-thinking-in-latent-world-model-for-quasi-realistic-autonomous-driving-in-carla-v2-qifeng-li-et-al-2024>(12/13 | 200/279) Think2Drive: Efficient Reinforcement Learning by Thinking in Latent World Model for Quasi-Realistic Autonomous Driving (in CARLA-v2) (Qifeng Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qifeng Li, Xiaosong Jia, Shaobo Wang, Junchi Yan. (2024)<br><strong>Think2Drive: Efficient Reinforcement Learning by Thinking in Latent World Model for Quasi-Realistic Autonomous Driving (in CARLA-v2)</strong><br><button class=copy-to-clipboard title="Think2Drive: Efficient Reinforcement Learning by Thinking in Latent World Model for Quasi-Realistic Autonomous Driving (in CARLA-v2)" index=200>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-200 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 13<br>Keywords: Benchmarking, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16720v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16720v1.pdf filename=2402.16720v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Real-world autonomous driving (AD) especially urban driving involves many corner cases. The lately released AD simulator CARLA v2 adds 39 common events in the driving scene, and provide more quasi-realistic testbed compared to CARLA v1. It poses new challenge to the community and so far no literature has reported any success on the new scenarios in V2 as existing works mostly have to rely on specific rules for planning yet they cannot cover the more complex cases in CARLA v2. In this work, we take the initiative of directly training a planner and the hope is to handle the corner cases flexibly and effectively, which we believe is also the future of AD. To our best knowledge, we develop the first model-based RL method named Think2Drive for AD, with a world model to learn the transitions of the environment, and then it acts as a neural simulator to train the planner. This paradigm significantly boosts the training efficiency due to the low dimensional state space and parallel computing of tensors in the world model. As a result, Think2Drive is able to run in an expert-level proficiency in CARLA v2 within 3 days of training on a single A6000 GPU, and to our best knowledge, so far there is no reported success (100% route completion)on CARLA v2. We also propose CornerCase-Repository, a <b>benchmark</b> that supports the evaluation of driving models by scenarios. Additionally, we propose a new and balanced metric to evaluate the performance by route completion, infraction number, and scenario density, so that the driving score could give more information about the actual driving performance.</p></p class="citation"></blockquote><h3 id=1313--201279-online-efficient-safety-critical-control-for-mobile-robots-in-unknown-dynamic-multi-obstacle-environments-yu-zhang-et-al-2024>(13/13 | 201/279) Online Efficient Safety-Critical Control for Mobile Robots in Unknown Dynamic Multi-Obstacle Environments (Yu Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yu Zhang, Guangyao Tian, Long Wen, Xiangtong Yao, Liding Zhang, Zhenshan Bing, Wei He, Alois Knoll. (2024)<br><strong>Online Efficient Safety-Critical Control for Mobile Robots in Unknown Dynamic Multi-Obstacle Environments</strong><br><button class=copy-to-clipboard title="Online Efficient Safety-Critical Control for Mobile Robots in Unknown Dynamic Multi-Obstacle Environments" index=201>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-201 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-RO, cs.RO<br>Keyword Score: 3<br>Keywords: Clustering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16449v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16449v1.pdf filename=2402.16449v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper proposes a LiDAR-based goal-seeking and exploration framework, addressing the efficiency of online obstacle avoidance in unstructured environments populated with static and moving obstacles. This framework addresses two significant challenges associated with traditional dynamic control barrier functions (D-CBFs): their online construction and the diminished real-time performance caused by utilizing multiple D-CBFs. To tackle the first challenge, the framework&rsquo;s perception component begins with <b>clustering</b> point clouds via the DBSCAN algorithm, followed by encapsulating these clusters with the minimum bounding ellipses (MBEs) algorithm to create elliptical representations. By comparing the current state of MBEs with those stored from previous moments, the differentiation between static and dynamic obstacles is realized, and the Kalman filter is utilized to predict the movements of the latter. Such analysis facilitates the D-CBF&rsquo;s online construction for each MBE. To tackle the second challenge, we introduce buffer zones, generating Type-II D-CBFs online for each identified obstacle. Utilizing these buffer zones as activation areas substantially reduces the number of D-CBFs that need to be activated. Upon entering these buffer zones, the system prioritizes safety, autonomously navigating safe paths, and hence referred to as the exploration mode. Exiting these buffer zones triggers the system&rsquo;s transition to goal-seeking mode. We demonstrate that the system&rsquo;s states under this framework achieve safety and asymptotic stabilization. Experimental results in simulated and real-world environments have validated our framework&rsquo;s capability, allowing a LiDAR-equipped mobile robot to efficiently and safely reach the desired location within dynamic environments containing multiple obstacles.</p></p class="citation"></blockquote><h2 id=csai-10>cs.AI (10)</h2><h3 id=110--202279-a-surprising-failure-multimodal-llms-and-the-nlvr-challenge-anne-wu-et-al-2024>(1/10 | 202/279) A Surprising Failure? Multimodal LLMs and the NLVR Challenge (Anne Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Anne Wu, Kianté Brantley, Yoav Artzi. (2024)<br><strong>A Surprising Failure? Multimodal LLMs and the NLVR Challenge</strong><br><button class=copy-to-clipboard title="A Surprising Failure? Multimodal LLMs and the NLVR Challenge" index=202>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-202 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CL, cs-LG, cs.AI<br>Keyword Score: 46<br>Keywords: Multi-modal, Multi-modal, GPT, Gemini, Reasoning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17793v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17793v1.pdf filename=2402.17793v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This study evaluates three state-of-the-art MLLMs &ndash; <b>GPT-4V,</b> <b>Gemini</b> Pro, and the open-source model IDEFICS &ndash; on the compositional natural language vision <b>reasoning</b> task NLVR. Given a human-written sentence paired with a synthetic image, this task requires the model to determine the truth value of the sentence with respect to the image. Despite the strong performance demonstrated by these models, we observe they perform poorly on NLVR, which was constructed to require compositional and spatial <b>reasoning,</b> and to be robust for semantic and systematic biases.</p></p class="citation"></blockquote><h3 id=210--203279-language-agents-as-optimizable-graphs-mingchen-zhuge-et-al-2024>(2/10 | 203/279) Language Agents as Optimizable Graphs (Mingchen Zhuge et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mingchen Zhuge, Wenyi Wang, Louis Kirsch, Francesco Faccio, Dmitrii Khizbullin, Jürgen Schmidhuber. (2024)<br><strong>Language Agents as Optimizable Graphs</strong><br><button class=copy-to-clipboard title="Language Agents as Optimizable Graphs" index=203>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-203 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CL, cs-LG, cs-MA, cs.AI<br>Keyword Score: 39<br>Keywords: Graph, Multi-modal, Multi-modal, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16823v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16823v2.pdf filename=2402.16823v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Various human-designed <b>prompt</b> engineering techniques have been proposed to improve problem solvers based on <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs),</b> yielding many disparate code bases. We unify these approaches by describing <b>LLM-based</b> agents as computational <b>graphs.</b> The nodes implement functions to process <b>multimodal</b> data or query <b>LLMs,</b> and the edges describe the information flow between operations. <b>Graphs</b> can be recursively combined into larger composite <b>graphs</b> representing hierarchies of inter-agent collaboration (where edges connect operations of different agents). Our novel automatic <b>graph</b> optimizers (1) refine node-level <b>LLM</b> <b>prompts</b> (node optimization) and (2) improve agent orchestration by changing <b>graph</b> connectivity (edge optimization). Experiments demonstrate that our framework can be used to efficiently develop, integrate, and automatically improve various <b>LLM</b> agents. The code can be found at <a href=https://github.com/metauto-ai/gptswarm>https://github.com/metauto-ai/gptswarm</a>.</p></p class="citation"></blockquote><h3 id=310--204279-label-informed-contrastive-pretraining-for-node-importance-estimation-on-knowledge-graphs-tianyu-zhang-et-al-2024>(3/10 | 204/279) Label Informed Contrastive Pretraining for Node Importance Estimation on Knowledge Graphs (Tianyu Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tianyu Zhang, Chengbin Hou, Rui Jiang, Xuegong Zhang, Chenghu Zhou, Ke Tang, Hairong Lv. (2024)<br><strong>Label Informed Contrastive Pretraining for Node Importance Estimation on Knowledge Graphs</strong><br><button class=copy-to-clipboard title="Label Informed Contrastive Pretraining for Node Importance Estimation on Knowledge Graphs" index=204>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-204 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-LG, cs-SI, cs.AI<br>Keyword Score: 38<br>Keywords: Graph Attention Networks, Graph, Node Embedding, Contrastive Learning, Knowledge Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17791v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17791v1.pdf filename=2402.17791v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Node</b> <b>Importance</b> Estimation (NIE) is a task of inferring importance scores of the <b>nodes</b> <b>in</b> a <b>graph.</b> <b>Due</b> <b>to</b> the availability of richer data and <b>knowledge,</b> <b>recent</b> research interests of NIE have been dedicating to <b>knowledge</b> <b>graphs</b> <b>for</b> <b>predicting</b> future or missing <b>node</b> <b>importance</b> scores. Existing state-of-the-art NIE methods train the model by available labels, and they consider every interested <b>node</b> <b>equally</b> before training. However, the <b>nodes</b> <b>with</b> higher importance often require or receive more attention in real-world scenarios, e.g., people may care more about the movies or webpages with higher importance. To this end, we introduce Label Informed <b>ContrAstive</b> <b>Pretraining</b> (LICAP) to the NIE problem for being better aware of the <b>nodes</b> <b>with</b> high importance scores. Specifically, LICAP is a novel type of <b>contrastive</b> <b>learning</b> framework that aims to fully utilize the continuous labels to generate <b>contrastive</b> <b>samples</b> for pretraining embeddings. Considering the NIE problem, LICAP adopts a novel sampling strategy called top <b>nodes</b> <b>preferred</b> hierarchical sampling to first group all interested <b>nodes</b> <b>into</b> a top bin and a non-top bin based on <b>node</b> <b>importance</b> scores, and then divide the <b>nodes</b> <b>within</b> top bin into several finer bins also based on the scores. The <b>contrastive</b> <b>samples</b> are generated from those bins, and are then used to pretrain <b>node</b> <b>embeddings</b> of <b>knowledge</b> <b>graphs</b> <b>via</b> <b>a</b> newly proposed Predicate-aware <b>Graph</b> <b>Attention</b> <b>Networks</b> (PreGAT), so as to better separate the top <b>nodes</b> <b>from</b> non-top <b>nodes,</b> <b>and</b> distinguish the top <b>nodes</b> <b>within</b> top bin by keeping the relative order among finer bins. Extensive experiments demonstrate that the LICAP pretrained embeddings can further boost the performance of existing NIE methods and achieve the new state-of-the-art performance regarding both regression and ranking metrics. The source code for reproducibility is available at <a href=https://github.com/zhangtia16/LICAP>https://github.com/zhangtia16/LICAP</a></p></p class="citation"></blockquote><h3 id=410--205279-on-languaging-a-simulation-engine-han-liu-et-al-2024>(4/10 | 205/279) On Languaging a Simulation Engine (Han Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Han Liu, Liantang Li. (2024)<br><strong>On Languaging a Simulation Engine</strong><br><button class=copy-to-clipboard title="On Languaging a Simulation Engine" index=205>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-205 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CE, cs-CL, cs.AI<br>Keyword Score: 30<br>Keywords: Knowledge Distillation, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16482v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16482v1.pdf filename=2402.16482v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Language model intelligence is revolutionizing the way we program materials <b>simulations.</b> However, the diversity of <b>simulation</b> scenarios renders it challenging to precisely transform human language into a tailored simulator. Here, using three functionalized types of language model, we propose a language-to-simulation (Lang2Sim) framework that enables interactive navigation on languaging a <b>simulation</b> engine, by taking a scenario instance of water sorption in porous matrices. Unlike line-by-line coding of a target simulator, the language models interpret each simulator as an assembly of invariant tool function and its variant input-output pair. Lang2Sim enables the precise transform of textual description by functionalizing and sequentializing the language models of, respectively, rationalizing the tool categorization, customizing its input-output combinations, and <b>distilling</b> the simulator input into executable format. Importantly, depending on its functionalized type, each language model features a distinct processing of chat history to best balance its memory limit and information completeness, thus leveraging the model intelligence to unstructured nature of human request. Overall, this work establishes language model as an intelligent platform to unlock the era of languaging a <b>simulation</b> engine.</p></p class="citation"></blockquote><h3 id=510--206279-from-large-language-models-and-optimization-to-decision-optimization-copilot-a-research-manifesto-segev-wasserkrug-et-al-2024>(5/10 | 206/279) From Large Language Models and Optimization to Decision Optimization CoPilot: A Research Manifesto (Segev Wasserkrug et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Segev Wasserkrug, Leonard Boussioux, Dick den Hertog, Farzaneh Mirzazadeh, Ilker Birbil, Jannis Kurtz, Donato Maragno. (2024)<br><strong>From Large Language Models and Optimization to Decision Optimization CoPilot: A Research Manifesto</strong><br><button class=copy-to-clipboard title="From Large Language Models and Optimization to Decision Optimization CoPilot: A Research Manifesto" index=206>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-206 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-LG, cs.AI, math-OC<br>Keyword Score: 30<br>Keywords: ChatGPT, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16269v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16269v1.pdf filename=2402.16269v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Significantly simplifying the creation of optimization models for real-world business problems has long been a major goal in applying mathematical optimization more widely to important business and societal decisions. The recent capabilities of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> present a timely opportunity to achieve this goal. Therefore, we propose research at the intersection of <b>LLMs</b> and optimization to create a Decision Optimization CoPilot (DOCP) - an AI tool designed to assist any decision maker, interacting in natural language to grasp the business problem, subsequently formulating and solving the corresponding optimization model. This paper outlines our DOCP vision and identifies several fundamental requirements for its implementation. We describe the state of the art through a literature survey and experiments using <b>ChatGPT.</b> We show that a) <b>LLMs</b> already provide substantial novel capabilities relevant to a DOCP, and b) major research challenges remain to be addressed. We also propose possible research directions to overcome these gaps. We also see this work as a call to action to bring together the <b>LLM</b> and optimization communities to pursue our vision, thereby enabling much more widespread improved decision-making.</p></p class="citation"></blockquote><h3 id=610--207279-gigapevt-multimodal-medical-assistant-pavel-blinov-et-al-2024>(6/10 | 207/279) GigaPevt: Multimodal Medical Assistant (Pavel Blinov et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Pavel Blinov, Konstantin Egorov, Ivan Sviridov, Nikolay Ivanov, Stepan Botman, Evgeniy Tagin, Stepan Kudin, Galina Zubkova, Andrey Savchenko. (2024)<br><strong>GigaPevt: Multimodal Medical Assistant</strong><br><button class=copy-to-clipboard title="GigaPevt: Multimodal Medical Assistant" index=207>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-207 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: 68T07, I-2-1, cs-AI, cs-CL, cs-HC, cs.AI<br>Keyword Score: 26<br>Keywords: Multi-modal, Multi-modal, Question Answering, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16654v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16654v1.pdf filename=2402.16654v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Building an intelligent and efficient medical assistant is still a challenging AI problem. The major limitation comes from the data modality scarceness, which reduces comprehensive patient perception. This demo paper presents the GigaPevt, the first <b>multimodal</b> medical assistant that combines the dialog capabilities of <b>large</b> <b>language</b> <b>models</b> with specialized medical models. Such an approach shows immediate advantages in dialog quality and metric performance, with a 1.18% accuracy improvement in the <b>question-answering</b> <b>task.</b></p></p class="citation"></blockquote><h3 id=710--208279-genainet-enabling-wireless-collective-intelligence-via-knowledge-transfer-and-reasoning-hang-zou-et-al-2024>(7/10 | 208/279) GenAINet: Enabling Wireless Collective Intelligence via Knowledge Transfer and Reasoning (Hang Zou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hang Zou, Qiyang Zhao, Lina Bariah, Yu Tian, Mehdi Bennis, Samson Lasaulce, Merouane Debbah, Faouzi Bader. (2024)<br><strong>GenAINet: Enabling Wireless Collective Intelligence via Knowledge Transfer and Reasoning</strong><br><button class=copy-to-clipboard title="GenAINet: Enabling Wireless Collective Intelligence via Knowledge Transfer and Reasoning" index=208>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-208 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-NI, cs.AI, eess-SP<br>Keyword Score: 23<br>Keywords: Knowledge Transfer, Multi-modal, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16631v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16631v2.pdf filename=2402.16631v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Generative artificial intelligence (GenAI) and communication networks are expected to have groundbreaking synergies in 6G. Connecting GenAI agents over a wireless network can potentially unleash the power of collective intelligence and pave the way for artificial general intelligence (AGI). However, current wireless networks are designed as a &ldquo;data pipe&rdquo; and are not suited to accommodate and leverage the power of GenAI. In this paper, we propose the GenAINet framework in which distributed GenAI agents communicate <b>knowledge</b> <b>(high-level</b> concepts or abstracts) to accomplish arbitrary tasks. We first provide a network architecture integrating GenAI capabilities to manage both network protocols and applications. Building on this, we investigate effective communication and <b>reasoning</b> problems by proposing a semantic-native GenAINet. Specifically, GenAI agents extract semantic concepts from <b>multi-modal</b> raw data, build a knowledgebase representing their semantic relations, which is retrieved by GenAI models for planning and <b>reasoning.</b> Under this paradigm, an agent can learn fast from other agents&rsquo; experience for making better decisions with efficient communications. Furthermore, we conduct two case studies where in wireless device query, we show that extracting and transferring <b>knowledge</b> <b>can</b> improve query accuracy with reduced communication; and in wireless power control, we show that distributed agents can improve decisions via collaborative <b>reasoning.</b> Finally, we address that developing a hierarchical semantic level Telecom world model is a key path towards network of collective intelligence.</p></p class="citation"></blockquote><h3 id=810--209279-value-preferences-estimation-and-disambiguation-in-hybrid-participatory-systems-enrico-liscio-et-al-2024>(8/10 | 209/279) Value Preferences Estimation and Disambiguation in Hybrid Participatory Systems (Enrico Liscio et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Enrico Liscio, Luciano C. Siebert, Catholijn M. Jonker, Pradeep K. Murukannaiah. (2024)<br><strong>Value Preferences Estimation and Disambiguation in Hybrid Participatory Systems</strong><br><button class=copy-to-clipboard title="Value Preferences Estimation and Disambiguation in Hybrid Participatory Systems" index=209>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-209 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CL, cs.AI<br>Keyword Score: 10<br>Keywords: Disambiguation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16751v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16751v1.pdf filename=2402.16751v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Understanding citizens&rsquo; values in participatory systems is crucial for citizen-centric policy-making. We envision a hybrid participatory system where participants make choices and provide motivations for those choices, and AI agents estimate their value preferences by interacting with them. We focus on situations where a conflict is detected between participants&rsquo; choices and motivations, and propose methods for estimating value preferences while addressing detected inconsistencies by interacting with the participants. We operationalize the philosophical stance that &ldquo;valuing is deliberatively consequential.&rdquo; That is, if a participant&rsquo;s choice is based on a deliberation of value preferences, the value preferences can be observed in the motivation the participant provides for the choice. Thus, we propose and compare value estimation methods that prioritize the values estimated from motivations over the values estimated from choices alone. Then, we introduce a <b>disambiguation</b> strategy that addresses the detected inconsistencies between choices and motivations by directly interacting with the participants. We evaluate the proposed methods on a dataset of a large-scale survey on energy transition. The results show that explicitly addressing inconsistencies between choices and motivations improves the estimation of an individual&rsquo;s value preferences. The <b>disambiguation</b> strategy does not show substantial improvements when compared to similar baselines&ndash;however, we discuss how the novelty of the approach can open new research avenues and propose improvements to address the current limitations.</p></p class="citation"></blockquote><h3 id=910--210279-memory-gaps-would-llms-pass-the-tulving-test-jean-marie-chauvet-2024>(9/10 | 210/279) Memory GAPS: Would LLMs pass the Tulving Test? (Jean-Marie Chauvet, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jean-Marie Chauvet. (2024)<br><strong>Memory GAPS: Would LLMs pass the Tulving Test?</strong><br><button class=copy-to-clipboard title="Memory GAPS: Would LLMs pass the Tulving Test?" index=210>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-210 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: I-2-4, cs-AI, cs.AI<br>Keyword Score: 10<br>Keywords: Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16505v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16505v2.pdf filename=2402.16505v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The Tulving Test was designed to investigate memory performance in recognition and recall tasks. Its results help assess the relevance of the &ldquo;Synergistic Ecphory Model&rdquo; of memory and similar RK paradigms in human performance. This paper starts investigating whether the more than forty-year-old framework sheds some light on <b>LLMs&rsquo;</b> acts of remembering.</p></p class="citation"></blockquote><h3 id=1010--211279-contingency-planning-using-bi-level-markov-decision-processes-for-space-missions-somrita-banerjee-et-al-2024>(10/10 | 211/279) Contingency Planning Using Bi-level Markov Decision Processes for Space Missions (Somrita Banerjee et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Somrita Banerjee, Edward Balaban, Mark Shirley, Kevin Bradner, Marco Pavone. (2024)<br><strong>Contingency Planning Using Bi-level Markov Decision Processes for Space Missions</strong><br><button class=copy-to-clipboard title="Contingency Planning Using Bi-level Markov Decision Processes for Space Missions" index=211>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-211 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-RO, cs.AI<br>Keyword Score: 10<br>Keywords: Markov Decision Process<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16342v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16342v1.pdf filename=2402.16342v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This work focuses on autonomous contingency planning for scientific missions by enabling rapid policy computation from any off-nominal point in the state space in the event of a delay or deviation from the nominal mission plan. Successful contingency planning involves managing risks and rewards, often probabilistically associated with actions, in stochastic scenarios. Markov Decision Processes <b>(MDPs)</b> are used to mathematically model decision-making in such scenarios. However, in the specific case of planetary rover traverse planning, the vast action space and long planning time horizon pose computational challenges. A bi-level MDP framework is proposed to improve computational tractability, while also aligning with existing mission planning practices and enhancing explainability and trustworthiness of AI-driven solutions. We discuss the conversion of a mission planning MDP into a bi-level MDP, and test the framework on RoverGridWorld, a modified GridWorld environment for rover mission planning. We demonstrate the computational tractability and near-optimal policies achievable with the bi-level MDP approach, highlighting the trade-offs between compute time and policy optimality as the problem&rsquo;s complexity grows. This work facilitates more efficient and flexible contingency planning in the context of scientific missions.</p></p class="citation"></blockquote><h2 id=csce-1>cs.CE (1)</h2><h3 id=11--212279-prollama-a-protein-large-language-model-for-multi-task-protein-language-processing-liuzhenghao-lv-et-al-2024>(1/1 | 212/279) ProLLaMA: A Protein Large Language Model for Multi-Task Protein Language Processing (Liuzhenghao Lv et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Liuzhenghao Lv, Zongying Lin, Hao Li, Yuyang Liu, Jiaxi Cui, Calvin Yu-Chian Chen, Li Yuan, Yonghong Tian. (2024)<br><strong>ProLLaMA: A Protein Large Language Model for Multi-Task Protein Language Processing</strong><br><button class=copy-to-clipboard title="ProLLaMA: A Protein Large Language Model for Multi-Task Protein Language Processing" index=212>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-212 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CE<br>Categories: cs-CE, cs.CE, q-bio-BM<br>Keyword Score: 40<br>Keywords: GPT, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16445v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16445v1.pdf filename=2402.16445v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs),</b> including <b>GPT-x</b> and LLaMA2, have achieved remarkable performance in multiple Natural Language Processing (NLP) tasks. Under the premise that protein sequences constitute the protein language, Protein <b>Large</b> <b>Language</b> <b>Models</b> (ProLLMs) trained on protein corpora excel at de novo protein sequence generation. However, as of now, unlike <b>LLMs</b> in NLP, no ProLLM is capable of multiple tasks in the Protein Language Processing (PLP) field. This <b>prompts</b> us to delineate the inherent limitations in current ProLLMs: (i) the lack of natural language capabilities, (ii) insufficient instruction understanding, and (iii) high training resource demands. To address these challenges, we introduce a training framework to transform any general <b>LLM</b> into a ProLLM capable of handling multiple PLP tasks. Specifically, our framework utilizes low-rank adaptation and employs a two-stage training approach, and it is distinguished by its universality, low overhead, and scalability. Through training under this framework, we propose the ProLLaMA model, the first known ProLLM to handle multiple PLP tasks simultaneously. Experiments show that ProLLaMA achieves state-of-the-art results in the unconditional protein sequence generation task. In the controllable protein sequence generation task, ProLLaMA can design novel proteins with desired functionalities. In the protein property prediction task, ProLLaMA achieves nearly 100% accuracy across many categories. The latter two tasks are beyond the reach of other ProLLMs. Code is available at \url{https://github.com/Lyu6PosHao/ProLLaMA}.</p></p class="citation"></blockquote><h2 id=csds-2>cs.DS (2)</h2><h3 id=12--213279-ingrass-incremental-graph-spectral-sparsification-via-low-resistance-diameter-decomposition-ali-aghdaei-et-al-2024>(1/2 | 213/279) inGRASS: Incremental Graph Spectral Sparsification via Low-Resistance-Diameter Decomposition (Ali Aghdaei et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ali Aghdaei, Zhuo Feng. (2024)<br><strong>inGRASS: Incremental Graph Spectral Sparsification via Low-Resistance-Diameter Decomposition</strong><br><button class=copy-to-clipboard title="inGRASS: Incremental Graph Spectral Sparsification via Low-Resistance-Diameter Decomposition" index=213>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-213 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DS<br>Categories: cs-DS, cs-LG, cs-SI, cs.DS<br>Keyword Score: 33<br>Keywords: Graph, Node Embedding, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16990v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16990v1.pdf filename=2402.16990v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This work presents inGRASS, a novel algorithm designed for incremental spectral sparsification of large undirected <b>graphs.</b> The proposed inGRASS algorithm is highly scalable and parallel-friendly, having a nearly-linear time complexity for the setup phase and the ability to update the spectral sparsifier in $O(\log N)$ time for each incremental change made to the original <b>graph</b> with $N$ <b>nodes.</b> <b>A</b> key component in the setup phase of inGRASS is a multilevel resistance embedding framework introduced for efficiently identifying spectrally-critical edges and effectively detecting redundant ones, which is achieved by decomposing the initial sparsifier into many <b>node</b> <b>clusters</b> with bounded effective-resistance diameters leveraging a low-resistance-diameter decomposition (LRD) scheme. The update phase of inGRASS exploits low-dimensional <b>node</b> <b>embedding</b> vectors for efficiently estimating the importance and uniqueness of each newly added edge. As demonstrated through extensive experiments, inGRASS achieves up to over $200 \times$ speedups while retaining comparable solution quality in incremental spectral sparsification of <b>graphs</b> obtained from various datasets, such as circuit <b>simulations,</b> finite element analysis, and social networks.</p></p class="citation"></blockquote><h3 id=22--214279-the-complexity-of-diameter-on-h-free-graphs-jelle-j-oostveen-et-al-2024>(2/2 | 214/279) The Complexity of Diameter on H-free graphs (Jelle J. Oostveen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jelle J. Oostveen, Daniël Paulusma, Erik Jan van Leeuwen. (2024)<br><strong>The Complexity of Diameter on H-free graphs</strong><br><button class=copy-to-clipboard title="The Complexity of Diameter on H-free graphs" index=214>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-214 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DS<br>Categories: cs-DM, cs-DS, cs.DS<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16678v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16678v1.pdf filename=2402.16678v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The intensively studied Diameter problem is to find the diameter of a given connected <b>graph.</b> We investigate, for the first time in a structured manner, the complexity of Diameter for H-free <b>graphs,</b> that is, <b>graphs</b> that do not contain a fixed <b>graph</b> H as an induced subgraph. We first show that if H is not a linear forest with small components, then Diameter cannot be solved in subquadratic time for H-free <b>graphs</b> under SETH. For some small linear forests, we do show linear-time algorithms for solving Diameter. For other linear forests H, we make progress towards linear-time algorithms by considering specific diameter values. If H is a linear forest, the maximum value of the diameter of any <b>graph</b> in a connected H-free <b>graph</b> class is some constant dmax dependent only on H. We give linear-time algorithms for deciding if a connected H-free <b>graph</b> has diameter dmax, for several linear forests H. In contrast, for one such linear forest H, Diameter cannot be solved in subquadratic time for H-free <b>graphs</b> under SETH. Moreover, we even show that, for several other linear forests H, one cannot decide in subquadratic time if a connected H-free <b>graph</b> has diameter dmax under SETH.</p></p class="citation"></blockquote><h2 id=csar-1>cs.AR (1)</h2><h3 id=11--215279-accelerating-graph-neural-networks-on-real-processing-in-memory-systems-christina-giannoula-et-al-2024>(1/1 | 215/279) Accelerating Graph Neural Networks on Real Processing-In-Memory Systems (Christina Giannoula et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Christina Giannoula, Peiming Yang, Ivan Fernandez Vega, Jiacheng Yang, Yu Xin Li, Juan Gomez Luna, Mohammad Sadrosadati, Onur Mutlu, Gennady Pekhimenko. (2024)<br><strong>Accelerating Graph Neural Networks on Real Processing-In-Memory Systems</strong><br><button class=copy-to-clipboard title="Accelerating Graph Neural Networks on Real Processing-In-Memory Systems" index=215>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-215 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AR<br>Categories: cs-AR, cs-DC, cs-LG, cs-PF, cs.AR<br>Keyword Score: 33<br>Keywords: Graph, Graph Neural Network, Graph Neural Network, Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16731v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16731v1.pdf filename=2402.16731v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Graph</b> <b>Neural</b> <b>Networks</b> <b>(GNNs)</b> are emerging ML models to analyze <b>graph-structure</b> <b>data.</b> <b>Graph</b> <b>Neural</b> <b>Network</b> <b>(GNN)</b> execution involves both compute-intensive and memory-intensive kernels, the latter dominates the total time, being significantly bottlenecked by data movement between memory and processors. Processing-In-Memory (PIM) systems can alleviate this data movement bottleneck by placing simple processors near or inside to memory arrays. In this work, we introduce PyGim, an efficient ML framework that accelerates <b>GNNs</b> on real PIM systems. We propose intelligent parallelization techniques for memory-intensive kernels of <b>GNNs</b> tailored for real PIM systems, and develop handy Python API for them. We provide hybrid <b>GNN</b> execution, in which the compute-intensive and memory-intensive kernels are executed in processor-centric and memory-centric computing systems, respectively, to match their algorithmic nature. We extensively evaluate PyGim on a real-world PIM system with 1992 PIM cores using emerging <b>GNN</b> models, and demonstrate that it outperforms its state-of-the-art CPU counterpart on Intel Xeon by on average 3.04x, and achieves higher resource utilization than CPU and GPU systems. Our work provides useful <b>recommendations</b> for software, system and hardware designers. PyGim will be open-sourced to enable the widespread use of PIM systems in <b>GNNs.</b></p></p class="citation"></blockquote><h2 id=physicsflu-dyn-1>physics.flu-dyn (1)</h2><h3 id=11--216279-model-based-deep-reinforcement-learning-for-accelerated-learning-from-flow-simulations-andre-weiner-et-al-2024>(1/1 | 216/279) Model-based deep reinforcement learning for accelerated learning from flow simulations (Andre Weiner et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Andre Weiner, Janis Geise. (2024)<br><strong>Model-based deep reinforcement learning for accelerated learning from flow simulations</strong><br><button class=copy-to-clipboard title="Model-based deep reinforcement learning for accelerated learning from flow simulations" index=216>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-216 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: physics.flu-dyn<br>Categories: cs-CE, cs-LG, physics-flu-dyn, physics.flu-dyn<br>Keyword Score: 33<br>Keywords: Benchmarking, Reinforcement Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16543v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16543v1.pdf filename=2402.16543v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In recent years, deep <b>reinforcement</b> <b>learning</b> has emerged as a technique to solve closed-loop flow control problems. Employing <b>simulation-based</b> environments in <b>reinforcement</b> <b>learning</b> enables a priori end-to-end optimization of the control system, provides a virtual testbed for safety-critical control applications, and allows to gain a deep understanding of the control mechanisms. While <b>reinforcement</b> <b>learning</b> has been applied successfully in a number of rather simple flow control <b>benchmarks,</b> a major bottleneck toward real-world applications is the high computational cost and turnaround time of flow <b>simulations.</b> In this contribution, we demonstrate the benefits of model-based <b>reinforcement</b> <b>learning</b> for flow control applications. Specifically, we optimize the policy by alternating between trajectories sampled from flow <b>simulations</b> and trajectories sampled from an ensemble of environment models. The model-based learning reduces the overall training time by up to $85%$ for the fluidic pinball test case. Even larger savings are expected for more demanding flow <b>simulations.</b></p></p class="citation"></blockquote><h2 id=cscy-3>cs.CY (3)</h2><h3 id=13--217279-unveiling-the-truth-and-facilitating-change-towards-agent-based-large-scale-social-movement-simulation-xinyi-mou-et-al-2024>(1/3 | 217/279) Unveiling the Truth and Facilitating Change: Towards Agent-based Large-scale Social Movement Simulation (Xinyi Mou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xinyi Mou, Zhongyu Wei, Xuanjing Huang. (2024)<br><strong>Unveiling the Truth and Facilitating Change: Towards Agent-based Large-scale Social Movement Simulation</strong><br><button class=copy-to-clipboard title="Unveiling the Truth and Facilitating Change: Towards Agent-based Large-scale Social Movement Simulation" index=217>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-217 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CY<br>Categories: cs-CL, cs-CY, cs.CY<br>Keyword Score: 33<br>Keywords: Benchmarking, Simulation, Simulator, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16333v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16333v1.pdf filename=2402.16333v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Social media has emerged as a cornerstone of social movements, wielding significant influence in driving societal change. Simulating the response of the public and forecasting the potential impact has become increasingly important. However, existing methods for simulating such phenomena encounter challenges concerning their efficacy and efficiency in capturing the behaviors of social movement participants. In this paper, we introduce a hybrid framework for social media user <b>simulation,</b> wherein users are categorized into two types. Core users are driven by <b>Large</b> <b>Language</b> <b>Models,</b> while numerous ordinary users are modeled by deductive agent-based models. We further construct a Twitter-like environment to replicate their response dynamics following trigger events. Subsequently, we develop a multi-faceted <b>benchmark</b> SoMoSiMu-Bench for evaluation and conduct comprehensive experiments across real-world datasets. Experimental results demonstrate the effectiveness and flexibility of our method.</p></p class="citation"></blockquote><h3 id=23--218279-algorithmic-arbitrariness-in-content-moderation-juan-felipe-gomez-et-al-2024>(2/3 | 218/279) Algorithmic Arbitrariness in Content Moderation (Juan Felipe Gomez et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Juan Felipe Gomez, Caio Vieira Machado, Lucas Monteiro Paes, Flavio P. Calmon. (2024)<br><strong>Algorithmic Arbitrariness in Content Moderation</strong><br><button class=copy-to-clipboard title="Algorithmic Arbitrariness in Content Moderation" index=218>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-218 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CY<br>Categories: cs-CY, cs-LG, cs-SI, cs.CY<br>Keyword Score: 20<br>Keywords: Fake News Detection, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16979v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16979v1.pdf filename=2402.16979v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Machine learning (ML) is widely used to moderate online content. Despite its scalability relative to human moderation, the use of ML introduces unique challenges to content moderation. One such challenge is predictive multiplicity: multiple competing models for content classification may perform equally well on average, yet assign conflicting predictions to the same content. This multiplicity can result from seemingly innocuous choices during model development, such as random seed selection for parameter initialization. We experimentally demonstrate how content moderation tools can arbitrarily classify samples as toxic, leading to arbitrary restrictions on speech. We discuss these findings in terms of human rights set out by the International Covenant on Civil and Political Rights (ICCPR), namely freedom of expression, non-discrimination, and procedural justice. We analyze (i) the extent of predictive multiplicity among state-of-the-art <b>LLMs</b> used for detecting toxic content; (ii) the disparate impact of this arbitrariness across social groups; and (iii) how model multiplicity compares to unambiguous human classifications. Our findings indicate that the up-scaled algorithmic moderation risks legitimizing an algorithmic leviathan, where an algorithm disproportionately manages human rights. To mitigate such risks, our study underscores the need to identify and increase the transparency of arbitrariness in content moderation applications. Since algorithmic content moderation is being fueled by pressing social concerns, such as disinformation and hate speech, our discussion on harms raises concerns relevant to policy debates. Our findings also contribute to content moderation and intermediary liability laws being discussed and passed in many countries, such as the Digital Services Act in the European Union, the Online Safety Act in the United Kingdom, and the <b>Fake</b> <b>News</b> Bill in Brazil.</p></p class="citation"></blockquote><h3 id=33--219279-integrating-dark-pattern-taxonomies-frank-lewis-et-al-2024>(3/3 | 219/279) Integrating Dark Pattern Taxonomies (Frank Lewis et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Frank Lewis, Julita Vassileva. (2024)<br><strong>Integrating Dark Pattern Taxonomies</strong><br><button class=copy-to-clipboard title="Integrating Dark Pattern Taxonomies" index=219>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-219 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CY<br>Categories: cs-CY, cs.CY<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16760v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16760v1.pdf filename=2402.16760v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The problem of ``Dark Patterns" in user interface/user experience (UI/UX) design has proven a difficult issue to tackle. Malicious and explotitative design has expanded to multiple domains in the past 10 years and which has in turn led to multiple taxonomies attempting to describe them. While these taxonomies holds their own merit, and constitute unique contributions to the literature, their usefulness as separate entities is limited. We believe that in order to make meaningful progress in regulating malicious interface design, we must first form a globally harmonized system (GHS) for the classification and labeling of Dark Patterns. By leaning on network analysis tools and methods, this paper synthesizes existing taxonomies and their elements through as a directed <b>graph.</b> In doing so, the interconnectedness of Dark patterns can be more clearly revealed via community (cluster) detection. Ultimately, we hope that this work can serve as the inspiration for the creation of a glyph-based GHS for the classification of Dark Patterns.</p></p class="citation"></blockquote><h2 id=mathst-1>math.ST (1)</h2><h3 id=11--220279-on-independent-samples-along-the-langevin-diffusion-and-the-unadjusted-langevin-algorithm-jiaming-liang-et-al-2024>(1/1 | 220/279) On Independent Samples Along the Langevin Diffusion and the Unadjusted Langevin Algorithm (Jiaming Liang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiaming Liang, Siddharth Mitra, Andre Wibisono. (2024)<br><strong>On Independent Samples Along the Langevin Diffusion and the Unadjusted Langevin Algorithm</strong><br><button class=copy-to-clipboard title="On Independent Samples Along the Langevin Diffusion and the Unadjusted Langevin Algorithm" index=220>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-220 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.ST<br>Categories: cs-IT, math-IT, math-ST, math.ST, stat-ML, stat-TH<br>Keyword Score: 30<br>Keywords: Continuous Time, Continuous Time, Discrete Time, Discrete Time, Mutual Information<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17067v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17067v1.pdf filename=2402.17067v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We study the rate at which the initial and current random variables become independent along a Markov chain, focusing on the Langevin diffusion in <b>continuous</b> <b>time</b> and the Unadjusted Langevin Algorithm (ULA) in <b>discrete</b> <b>time.</b> We measure the dependence between random variables via their <b>mutual</b> <b>information.</b> For the Langevin diffusion, we show the <b>mutual</b> <b>information</b> converges to $0$ exponentially fast when the target is strongly log-concave, and at a polynomial rate when the target is weakly log-concave. These rates are analogous to the mixing time of the Langevin diffusion under similar assumptions. For the ULA, we show the <b>mutual</b> <b>information</b> converges to $0$ exponentially fast when the target is strongly log-concave and smooth. We prove our results by developing the <b>mutual</b> <b>version</b> of the mixing time analyses of these Markov chains. We also provide alternative proofs based on strong data processing inequalities for the Langevin diffusion and the ULA, and by showing regularity results for these processes in <b>mutual</b> <b>information.</b></p></p class="citation"></blockquote><h2 id=eesssy-8>eess.SY (8)</h2><h3 id=18--221279-reinforcement-learning-based-oscillation-dampening-scaling-up-single-agent-rl-algorithms-to-a-100-av-highway-field-operational-test-kathy-jang-et-al-2024>(1/8 | 221/279) Reinforcement Learning Based Oscillation Dampening: Scaling up Single-Agent RL algorithms to a 100 AV highway field operational test (Kathy Jang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kathy Jang, Nathan Lichtlé, Eugene Vinitsky, Adit Shah, Matthew Bunting, Matthew Nice, Benedetto Piccoli, Benjamin Seibold, Daniel B. Work, Maria Laura Delle Monache, Jonathan Sprinkle, Jonathan W. Lee, Alexandre M. Bayen. (2024)<br><strong>Reinforcement Learning Based Oscillation Dampening: Scaling up Single-Agent RL algorithms to a 100 AV highway field operational test</strong><br><button class=copy-to-clipboard title="Reinforcement Learning Based Oscillation Dampening: Scaling up Single-Agent RL algorithms to a 100 AV highway field operational test" index=221>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-221 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-RO, cs-SY, eess-SY, eess.SY<br>Keyword Score: 30<br>Keywords: Reinforcement Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17050v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17050v1.pdf filename=2402.17050v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this article, we explore the technical details of the <b>reinforcement</b> <b>learning</b> (RL) algorithms that were deployed in the largest field test of automated vehicles designed to smooth traffic flow in history as of 2023, uncovering the challenges and breakthroughs that come with developing RL controllers for automated vehicles. We delve into the fundamental concepts behind RL algorithms and their application in the context of self-driving cars, discussing the developmental process from <b>simulation</b> to deployment in detail, from designing simulators to reward function shaping. We present the results in both <b>simulation</b> and deployment, discussing the flow-smoothing benefits of the RL controller. From understanding the basics of Markov decision processes to exploring advanced techniques such as deep RL, our article offers a comprehensive overview and deep dive of the theoretical foundations and practical implementations driving this rapidly evolving field. We also showcase real-world case studies and alternative research projects that highlight the impact of RL controllers in revolutionizing autonomous driving. From tackling complex urban environments to dealing with unpredictable traffic scenarios, these intelligent controllers are pushing the boundaries of what automated vehicles can achieve. Furthermore, we examine the safety considerations and hardware-focused technical details surrounding deployment of RL controllers into automated vehicles. As these algorithms learn and evolve through interactions with the environment, ensuring their behavior aligns with safety standards becomes crucial. We explore the methodologies and frameworks being developed to address these challenges, emphasizing the importance of building reliable control systems for automated vehicles.</p></p class="citation"></blockquote><h3 id=28--222279-physics-informed-lstm-based-delay-compensation-framework-for-teleoperated-ugvs-ahmad-abubakar-et-al-2024>(2/8 | 222/279) Physics-Informed LSTM-Based Delay Compensation Framework for Teleoperated UGVs (Ahmad Abubakar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ahmad Abubakar, Yahya Zweiri, AbdelGafoor Haddad, Mubarak Yakubu, Ruqayya Alhammadi, Lakmal Seneviratne. (2024)<br><strong>Physics-Informed LSTM-Based Delay Compensation Framework for Teleoperated UGVs</strong><br><button class=copy-to-clipboard title="Physics-Informed LSTM-Based Delay Compensation Framework for Teleoperated UGVs" index=222>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-222 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 30<br>Keywords: LSTM, LSTM, LSTM<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16587v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16587v1.pdf filename=2402.16587v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Bilateral teleoperation of low-speed Unmanned Ground Vehicles (UGVs) on soft terrains is crucial for applications like lunar exploration, offering effective control of terrain-induced longitudinal slippage. However, latency arising from transmission delays over a network presents a challenge in maintaining high-fidelity closed-loop integration, potentially hindering UGV controls and leading to poor command-tracking performance. To address this challenge, this paper proposes a novel predictor framework that employs a Physics-informed <b>Long</b> <b>Short-Term</b> <b>Memory</b> <b>(PiLSTM)</b> network for designing bilateral teleoperator controls that effectively compensate for large delays. Contrasting with conventional model-free predictor frameworks, which are limited by their linear nature in capturing nonlinear and temporal dynamic behaviors, our approach integrates the <b>LSTM</b> structure with physical constraints for enhanced performance and better generalization across varied scenarios. Specifically, four distinct predictors were employed in the framework: two compensate for forward delays, while the other two compensate for backward delays. Due to their effectiveness in learning from temporal data, the proposed PiLSTM framework demonstrates a 26.1\ improvement in delay compensation over the conventional model-free predictors for large delays in open-loop case studies. Subsequently, experiments were conducted to validate the efficacy of the framework in close-loop scenarios, particularly to compensate for the real-network delays experienced by teleoperated UGVs coupled with longitudinal slippage. The results confirm the proposed framework is effective in restoring the fidelity of the closed-loop integration. This improvement is showcased through improved performance and transparency, which leads to excellent command-tracking performance.</p></p class="citation"></blockquote><h3 id=38--223279-batch-estimation-of-a-steady-uniform-flow-field-from-ground-velocity-and-heading-measurements-artur-wolek-et-al-2024>(3/8 | 223/279) Batch Estimation of a Steady, Uniform, Flow-Field from Ground Velocity and Heading Measurements (Artur Wolek et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Artur Wolek, James McMahon. (2024)<br><strong>Batch Estimation of a Steady, Uniform, Flow-Field from Ground Velocity and Heading Measurements</strong><br><button class=copy-to-clipboard title="Batch Estimation of a Steady, Uniform, Flow-Field from Ground Velocity and Heading Measurements" index=223>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-223 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-RO, cs-SY, eess-SY, eess.SY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17078v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17078v1.pdf filename=2402.17078v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents three batch estimation methods that use noisy ground velocity and heading measurements from a vehicle executing a circular orbit (or similar large heading change maneuver) to estimate the speed and direction of a steady, uniform, flow-field. The methods are based on a simple kinematic model of the vehicle&rsquo;s motion and use curve-fitting or nonlinear least-square optimization. A Monte Carlo <b>simulation</b> with randomized flow conditions is used to evaluate the batch estimation methods while varying the measurement noise of the data and the interval of unique heading traversed during the maneuver. The methods are also compared using experimental data obtained with a Bluefin-21 unmanned underwater vehicle performing a series of circular orbit maneuvers over a five hour period in a tide-driven flow.</p></p class="citation"></blockquote><h3 id=48--224279-dynamic-model-of-back-to-back-converter-for-system-level-phasor-simulation-hisham-mahmood-et-al-2024>(4/8 | 224/279) Dynamic Model of Back-to-Back Converter for System-Level Phasor Simulation (Hisham Mahmood et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hisham Mahmood, Samrat Acharya, Francis Tuffner, Priya Mana, Alok Kumar Bharati. (2024)<br><strong>Dynamic Model of Back-to-Back Converter for System-Level Phasor Simulation</strong><br><button class=copy-to-clipboard title="Dynamic Model of Back-to-Back Converter for System-Level Phasor Simulation" index=224>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-224 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17056v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17056v1.pdf filename=2402.17056v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The power system is expected to evolve rapidly with the increasing deployment of power electronic interface and conditioning systems, microgrids, and hybrid AC/DC grids. Among power electronic systems, back-to-back (BTB) converters can be a powerful interface to integrate microgrids and networked microgrids. To study the integration of such devices into large power systems, a balance between power electronics model fidelity and system-level computational efficiency is critical. In system-level <b>simulations</b> of bulk power systems dominated by synchronous generators, detailed electromagnetic models of back-to-back converters may be unnecessary and also computationally inefficient. This paper focuses on developing a simple phasor model for back-to-back converters that can be easily integrated into powerflow solvers to facilitate large-scale power system <b>simulations.</b> The model is implemented using C$^{++}$ language and integrated into GridLAB-D, an open source software for distribution systems studies, as a potential new capability. The GridLAB-D phasor domain model is validated against the electromagnetic transient (EMT) <b>simulation</b> of the detailed switching model. <b>Simulation</b> results show that the phasor model successfully captures the dominant dynamics of the converter with significantly shorter <b>simulation</b> elapsed time.</p></p class="citation"></blockquote><h3 id=58--225279-hybrid-feedback-control-for-global-and-optimal-safe-navigation-ishak-cheniouni-et-al-2024>(5/8 | 225/279) Hybrid Feedback Control for Global and Optimal Safe Navigation (Ishak Cheniouni et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ishak Cheniouni, Soulaimane Berkane, Abdelhamid Tayebi. (2024)<br><strong>Hybrid Feedback Control for Global and Optimal Safe Navigation</strong><br><button class=copy-to-clipboard title="Hybrid Feedback Control for Global and Optimal Safe Navigation" index=225>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-225 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17038v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17038v1.pdf filename=2402.17038v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We propose a hybrid feedback control strategy that safely steers a point-mass robot to a target location optimally from all initial conditions in the n-dimensional Euclidean space with a single spherical obstacle. The robot moves straight to the target when it has a clear line-of-sight to the target location. Otherwise, it engages in an optimal obstacle avoidance maneuver via the shortest path inside the cone enclosing the obstacle and having the robot&rsquo;s position as a vertex. The switching strategy that avoids the undesired equilibria, leading to global asymptotic stability (GAS) of the target location, relies on using two appropriately designed virtual destinations, ensuring control continuity and shortest path generation. <b>Simulation</b> results illustrating the effectiveness of the proposed approach are presented.</p></p class="citation"></blockquote><h3 id=68--226279-hierarchical-speed-planner-for-automated-vehicles-a-framework-for-lagrangian-variable-speed-limit-in-mixed-autonomy-traffic-han-wang-et-al-2024>(6/8 | 226/279) Hierarchical Speed Planner for Automated Vehicles: A Framework for Lagrangian Variable Speed Limit in Mixed Autonomy Traffic (Han Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Han Wang, Zhe Fu, Jonathan Lee, Hossein Nick Zinat Matin, Arwa Alanqary, Daniel Urieli, Sharon Hornstein, Abdul Rahman Kreidieh, Raphael Chekroun, William Barbour, William A. Richardson, Dan Work, Benedetto Piccoli, Benjamin Seibold, Jonathan Sprinkle, Alexandre M. Bayen, Maria Laura Delle Monache. (2024)<br><strong>Hierarchical Speed Planner for Automated Vehicles: A Framework for Lagrangian Variable Speed Limit in Mixed Autonomy Traffic</strong><br><button class=copy-to-clipboard title="Hierarchical Speed Planner for Automated Vehicles: A Framework for Lagrangian Variable Speed Limit in Mixed Autonomy Traffic" index=226>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-226 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16993v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16993v1.pdf filename=2402.16993v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper introduces a novel control framework for Lagrangian variable speed limits in hybrid traffic flow environments utilizing automated vehicles (AVs). The framework was validated using a fleet of 100 connected automated vehicles as part of the largest coordinated open-road test designed to smooth traffic flow. The framework includes two main components: a high-level controller deployed on the server side, named Speed Planner, and low-level controllers called vehicle controllers deployed on the vehicle side. The Speed Planner designs and updates target speeds for the vehicle controllers based on real-time Traffic State Estimation (TSE) [1]. The Speed Planner comprises two modules: a TSE enhancement module and a target speed design module. The TSE enhancement module is designed to minimize the effects of inherent latency in the received traffic information and to improve the spatial and temporal resolution of the input traffic data. The target speed design module generates target speed profiles with the goal of improving traffic flow. The vehicle controllers are designed to track the target speed meanwhile responding to the surrounding situation. The numerical <b>simulation</b> indicates the performance of the proposed method: the bottleneck throughput has increased by 5.01%, and the speed standard deviation has been reduced by a significant 34.36%. We further showcase an operational study with a description of how the controller was implemented on a field-test with 100 AVs and its comprehensive effects on the traffic flow.</p></p class="citation"></blockquote><h3 id=78--227279-oscillations-aware-frequency-security-assessment-via-efficient-worst-case-frequency-nadir-computation-yan-jiang-et-al-2024>(7/8 | 227/279) Oscillations-Aware Frequency Security Assessment via Efficient Worst-Case Frequency Nadir Computation (Yan Jiang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yan Jiang, Hancheng Min, Baosen Zhang. (2024)<br><strong>Oscillations-Aware Frequency Security Assessment via Efficient Worst-Case Frequency Nadir Computation</strong><br><button class=copy-to-clipboard title="Oscillations-Aware Frequency Security Assessment via Efficient Worst-Case Frequency Nadir Computation" index=227>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-227 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16765v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16765v1.pdf filename=2402.16765v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Frequency security assessment following major disturbances has long been one of the central tasks in power system operations. The standard approach is to study the center of inertia frequency, an aggregate signal for an entire system, to avoid analyzing the frequency signal at individual buses. However, as the amount of low-inertia renewable resources in a grid increases, the center of inertia frequency is becoming too coarse to provide reliable frequency security assessment. In this paper, we propose an efficient algorithm to determine the worst-case frequency nadir across all buses for bounded power disturbances, as well as identify the power disturbances leading to that severest scenario. The proposed algorithm allows oscillations-aware frequency security assessment without conducting exhaustive <b>simulations</b> and intractable analysis.</p></p class="citation"></blockquote><h3 id=88--228279-path-planning-for-a-cooperative-navigation-aid-vehicle-to-assist-multiple-agents-intermittently-artur-wolek-2024>(8/8 | 228/279) Path Planning for a Cooperative Navigation Aid Vehicle to Assist Multiple Agents Intermittently (Artur Wolek, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Artur Wolek. (2024)<br><strong>Path Planning for a Cooperative Navigation Aid Vehicle to Assist Multiple Agents Intermittently</strong><br><button class=copy-to-clipboard title="Path Planning for a Cooperative Navigation Aid Vehicle to Assist Multiple Agents Intermittently" index=228>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-228 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-RO, cs-SY, eess-SY, eess.SY<br>Keyword Score: 10<br>Keywords: Discrete Time, Discrete Time<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17071v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17071v1.pdf filename=2402.17071v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper considers the problem of planning a path for a single underwater cooperative navigation aid (CNA) vehicle to intermittently aid a set of N agents to minimize average navigation uncertainty. Both the CNA and agents are modeled as constant-velocity vehicles. The agents traverse along known nominal trajectories and the CNA plans a path to sequentially intercept them. Navigation aiding is modeled by a scalar <b>discrete</b> <b>time</b> Kalman filter. During path planning, the CNA considers surfacing to reduce its own navigation uncertainty. A greedy planning algorithm is proposed that uses a heuristic based on an optimal time-to-aid, overall navigation uncertainty reduction, and transit time, to assign agents to the CNA. The approach is compared to an optimal (exhaustive enumeration) algorithm through a Monte Carlo experiment with randomized agent nominal trajectories and initial navigation uncertainty.</p></p class="citation"></blockquote><h2 id=statml-8>stat.ML (8)</h2><h3 id=18--229279-rate-optimal-rank-aggregation-with-private-pairwise-rankings-shirong-xu-et-al-2024>(1/8 | 229/279) Rate-Optimal Rank Aggregation with Private Pairwise Rankings (Shirong Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shirong Xu, Will Wei Sun, Guang Cheng. (2024)<br><strong>Rate-Optimal Rank Aggregation with Private Pairwise Rankings</strong><br><button class=copy-to-clipboard title="Rate-Optimal Rank Aggregation with Private Pairwise Rankings" index=229>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-229 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-CR, cs-LG, stat-ML, stat.ML<br>Keyword Score: 30<br>Keywords: Recommender System, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16792v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16792v1.pdf filename=2402.16792v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In various real-world scenarios like <b>recommender</b> <b>systems</b> and political surveys, pairwise rankings are commonly collected and utilized for rank aggregation to obtain an overall ranking of items. However, preference rankings can reveal individuals&rsquo; personal preferences, underscoring the need to protect them before releasing for downstream analysis. In this paper, we address the challenge of preserving privacy while ensuring the utility of rank aggregation based on pairwise rankings generated from the Bradley-Terry-Luce (BTL) model. Using the randomized response mechanism to perturb raw pairwise rankings is a common privacy protection strategy used in practice, but a critical challenge arises because the privatized rankings no longer adhere to the BTL model, resulting in significant bias in downstream rank aggregation tasks. Motivated from this, we propose a debiased randomized response mechanism to protect the raw pairwise rankings, ensuring consistent estimation of true preferences and rankings in downstream rank aggregation. Theoretically, we offer insights into the relationship between overall privacy guarantees and estimation errors from private ranking data, and establish minimax rates for estimation errors. This enables the determination of optimal privacy guarantees that balance consistency in rank aggregation with robust privacy protection. We also investigate convergence rates of expected ranking errors for partial and full ranking recovery, quantifying how privacy protection influences the specification of top-$K$ item sets and complete rankings. Our findings are validated through extensive <b>simulations</b> and a real application.</p></p class="citation"></blockquote><h3 id=28--230279-penalized-generative-variable-selection-tong-wang-et-al-2024>(2/8 | 230/279) Penalized Generative Variable Selection (Tong Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tong Wang, Jian Huang, Shuangge Ma. (2024)<br><strong>Penalized Generative Variable Selection</strong><br><button class=copy-to-clipboard title="Penalized Generative Variable Selection" index=230>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-230 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-LG, stat-ME, stat-ML, stat.ML<br>Keyword Score: 30<br>Keywords: Generative Adversarial Network, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16661v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16661v1.pdf filename=2402.16661v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep networks are increasingly applied to a wide variety of data, including data with high-dimensional predictors. In such analysis, variable selection can be needed along with estimation/model building. Many of the existing deep network studies that incorporate variable selection have been limited to methodological and numerical developments. In this study, we consider modeling/estimation using the conditional Wasserstein <b>Generative</b> <b>Adversarial</b> <b>networks.</b> Group Lasso penalization is applied for variable selection, which may improve model estimation/prediction, interpretability, stability, etc. Significantly advancing from the existing literature, the analysis of censored survival data is also considered. We establish the convergence rate for variable selection while considering the approximation error, and obtain a more efficient distribution estimation. <b>Simulations</b> and the analysis of real experimental data demonstrate satisfactory practical utility of the proposed analysis.</p></p class="citation"></blockquote><h3 id=38--231279-a-provably-accurate-randomized-sampling-algorithm-for-logistic-regression-agniva-chowdhury-et-al-2024>(3/8 | 231/279) A Provably Accurate Randomized Sampling Algorithm for Logistic Regression (Agniva Chowdhury et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Agniva Chowdhury, Pradeep Ramuhalli. (2024)<br><strong>A Provably Accurate Randomized Sampling Algorithm for Logistic Regression</strong><br><button class=copy-to-clipboard title="A Provably Accurate Randomized Sampling Algorithm for Logistic Regression" index=231>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-231 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-DS, cs-LG, stat-ML, stat.ML<br>Keyword Score: 30<br>Keywords: Logistic Regression, Supervised Learning, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16326v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16326v2.pdf filename=2402.16326v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In statistics and machine learning, <b>logistic</b> <b>regression</b> is a widely-used <b>supervised</b> <b>learning</b> technique primarily employed for binary classification tasks. When the number of observations greatly exceeds the number of predictor variables, we present a simple, randomized sampling-based algorithm for <b>logistic</b> <b>regression</b> problem that guarantees high-quality approximations to both the estimated probabilities and the overall discrepancy of the model. Our analysis builds upon two simple structural conditions that boil down to randomized matrix multiplication, a fundamental and well-understood primitive of randomized numerical linear algebra. We analyze the properties of estimated probabilities of <b>logistic</b> <b>regression</b> when leverage scores are used to sample observations, and prove that accurate approximations can be achieved with a sample whose size is much smaller than the total number of observations. To further validate our theoretical findings, we conduct comprehensive empirical evaluations. Overall, our work sheds light on the potential of using randomized sampling approaches to efficiently approximate the estimated probabilities in <b>logistic</b> <b>regression,</b> offering a practical and computationally efficient solution for large-scale datasets.</p></p class="citation"></blockquote><h3 id=48--232279-stable-training-of-normalizing-flows-for-high-dimensional-variational-inference-daniel-andrade-2024>(4/8 | 232/279) Stable Training of Normalizing Flows for High-dimensional Variational Inference (Daniel Andrade, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Daniel Andrade. (2024)<br><strong>Stable Training of Normalizing Flows for High-dimensional Variational Inference</strong><br><button class=copy-to-clipboard title="Stable Training of Normalizing Flows for High-dimensional Variational Inference" index=232>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-232 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-LG, stat-ML, stat.ML<br>Keyword Score: 20<br>Keywords: Logistic Regression, Stochastic Gradient Descent<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16408v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16408v1.pdf filename=2402.16408v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Variational inference with normalizing flows (NFs) is an increasingly popular alternative to MCMC methods. In particular, NFs based on coupling layers (Real NVPs) are frequently used due to their good empirical performance. In theory, increasing the depth of normalizing flows should lead to more accurate posterior approximations. However, in practice, training deep normalizing flows for approximating high-dimensional posterior distributions is often infeasible due to the high variance of the <b>stochastic</b> <b>gradients.</b> <b>In</b> this work, we show that previous methods for stabilizing the variance of <b>stochastic</b> <b>gradient</b> <b>descent</b> can be insufficient to achieve stable training of Real NVPs. As the source of the problem, we identify that, during training, samples often exhibit unusual high values. As a remedy, we propose a combination of two methods: (1) soft-thresholding of the scale in Real NVPs, and (2) a bijective soft log transformation of the samples. We evaluate these and other previously proposed modification on several challenging target distributions, including a high-dimensional horseshoe <b>logistic</b> <b>regression</b> model. Our experiments show that with our modifications, stable training of Real NVPs for posteriors with several thousand dimensions is possible, allowing for more accurate marginal likelihood estimation via importance sampling. Moreover, we evaluate several common training techniques and architecture choices and provide practical advise for training NFs for high-dimensional variational inference.</p></p class="citation"></blockquote><h3 id=58--233279-uncertainty-quantification-in-anomaly-detection-with-cross-conformal-p-values-oliver-hennhöfer-et-al-2024>(5/8 | 233/279) Uncertainty Quantification in Anomaly Detection with Cross-Conformal $p$-Values (Oliver Hennhöfer et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Oliver Hennhöfer, Christine Preisach. (2024)<br><strong>Uncertainty Quantification in Anomaly Detection with Cross-Conformal $p$-Values</strong><br><button class=copy-to-clipboard title="Uncertainty Quantification in Anomaly Detection with Cross-Conformal $p$-Values" index=233>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-233 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-LG, stat-ML, stat.ML<br>Keyword Score: 13<br>Keywords: Anomaly Detection, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16388v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16388v1.pdf filename=2402.16388v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Given the growing significance of reliable, trustworthy, and explainable machine learning, the requirement of uncertainty quantification for <b>anomaly</b> <b>detection</b> systems has become increasingly important. In this context, effectively controlling Type I error rates ($\alpha$) without compromising the statistical power ($1-\beta$) of these systems can build trust and reduce costs related to false discoveries, particularly when follow-up procedures are expensive. Leveraging the principles of conformal prediction emerges as a promising approach for providing respective statistical guarantees by calibrating a model&rsquo;s uncertainty. This work introduces a novel framework for <b>anomaly</b> <b>detection,</b> termed cross-conformal <b>anomaly</b> <b>detection,</b> building upon well-known cross-conformal methods designed for prediction tasks. With that, it addresses a natural research gap by extending previous works in the context of inductive conformal <b>anomaly</b> <b>detection,</b> relying on the split-conformal approach for model calibration. Drawing on insights from conformal prediction, we demonstrate that the derived methods for calculating cross-conformal $p$-values strike a practical compromise between statistical efficiency (full-conformal) and computational efficiency (split-conformal) for uncertainty-quantified <b>anomaly</b> <b>detection</b> on <b>benchmark</b> datasets.</p></p class="citation"></blockquote><h3 id=68--234279-a-phase-transition-in-diffusion-models-reveals-the-hierarchical-nature-of-data-antonio-sclocchi-et-al-2024>(6/8 | 234/279) A Phase Transition in Diffusion Models Reveals the Hierarchical Nature of Data (Antonio Sclocchi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Antonio Sclocchi, Alessandro Favero, Matthieu Wyart. (2024)<br><strong>A Phase Transition in Diffusion Models Reveals the Hierarchical Nature of Data</strong><br><button class=copy-to-clipboard title="A Phase Transition in Diffusion Models Reveals the Hierarchical Nature of Data" index=234>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-234 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cond-mat-dis-nn, cs-CV, cs-LG, stat-ML, stat.ML<br>Keyword Score: 10<br>Keywords: Diffusion Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16991v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16991v1.pdf filename=2402.16991v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Understanding the structure of real data is paramount in advancing modern deep-learning methodologies. Natural data such as images are believed to be composed of features organised in a hierarchical and combinatorial manner, which neural networks capture during learning. Recent advancements show that <b>diffusion</b> <b>models</b> can generate high-quality images, hinting at their ability to capture this underlying structure. We study this phenomenon in a hierarchical generative model of data. We find that the backward <b>diffusion</b> <b>process</b> acting after a time $t$ is governed by a phase transition at some threshold time, where the probability of reconstructing high-level features, like the class of an image, suddenly drops. Instead, the reconstruction of low-level features, such as specific details of an image, evolves smoothly across the whole <b>diffusion</b> <b>process.</b> This result implies that at times beyond the transition, the class has changed but the generated sample may still be composed of low-level elements of the initial image. We validate these theoretical insights through numerical experiments on class-unconditional ImageNet <b>diffusion</b> <b>models.</b> Our analysis characterises the relationship between time and scale in <b>diffusion</b> <b>models</b> and puts forward generative models as powerful tools to model combinatorial data properties.</p></p class="citation"></blockquote><h3 id=78--235279-stopping-bayesian-optimization-with-probabilistic-regret-bounds-james-t-wilson-2024>(7/8 | 235/279) Stopping Bayesian Optimization with Probabilistic Regret Bounds (James T. Wilson, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>James T. Wilson. (2024)<br><strong>Stopping Bayesian Optimization with Probabilistic Regret Bounds</strong><br><button class=copy-to-clipboard title="Stopping Bayesian Optimization with Probabilistic Regret Bounds" index=235>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-235 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-LG, stat-ML, stat.ML<br>Keyword Score: 10<br>Keywords: Gaussian Process<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16811v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16811v1.pdf filename=2402.16811v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Bayesian optimization is a popular framework for efficiently finding high-quality solutions to difficult problems based on limited prior information. As a rule, these algorithms operate by iteratively choosing what to try next until some predefined budget has been exhausted. We investigate replacing this de facto stopping rule with an $(\epsilon, \delta)$-criterion: stop when a solution has been found whose value is within $\epsilon > 0$ of the optimum with probability at least $1 - \delta$ under the model. Given access to the prior distribution of problems, we show how to verify this condition in practice using a limited number of draws from the posterior. For <b>Gaussian</b> <b>process</b> priors, we prove that Bayesian optimization with the proposed criterion stops in finite time and returns a point that satisfies the $(\epsilon, \delta)$-criterion under mild assumptions. These findings are accompanied by extensive empirical results which demonstrate the strengths and weaknesses of this approach.</p></p class="citation"></blockquote><h3 id=88--236279-on-the-connection-between-noise-contrastive-estimation-and-contrastive-divergence-amanda-olmin-et-al-2024>(8/8 | 236/279) On the connection between Noise-Contrastive Estimation and Contrastive Divergence (Amanda Olmin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Amanda Olmin, Jakob Lindqvist, Lennart Svensson, Fredrik Lindsten. (2024)<br><strong>On the connection between Noise-Contrastive Estimation and Contrastive Divergence</strong><br><button class=copy-to-clipboard title="On the connection between Noise-Contrastive Estimation and Contrastive Divergence" index=236>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-236 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-LG, stat-ML, stat.ML<br>Keyword Score: 10<br>Keywords: Probabilistic Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16688v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16688v1.pdf filename=2402.16688v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Noise-contrastive estimation (NCE) is a popular method for estimating unnormalised <b>probabilistic</b> <b>models,</b> such as energy-based models, which are effective for modelling complex data distributions. Unlike classical maximum likelihood (ML) estimation that relies on importance sampling (resulting in ML-IS) or MCMC (resulting in contrastive divergence, CD), NCE uses a proxy criterion to avoid the need for evaluating an often intractable normalisation constant. Despite apparent conceptual differences, we show that two NCE criteria, ranking NCE (RNCE) and conditional NCE (CNCE), can be viewed as ML estimation methods. Specifically, RNCE is equivalent to ML estimation combined with conditional importance sampling, and both RNCE and CNCE are special cases of CD. These findings bridge the gap between the two method classes and allow us to apply techniques from the ML-IS and CD literature to NCE, offering several advantageous extensions.</p></p class="citation"></blockquote><h2 id=quant-ph-1>quant-ph (1)</h2><h3 id=11--237279-quantum-linear-algebra-is-all-you-need-for-transformer-architectures-naixu-guo-et-al-2024>(1/1 | 237/279) Quantum linear algebra is all you need for Transformer architectures (Naixu Guo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Naixu Guo, Zhan Yu, Aman Agrawal, Patrick Rebentrost. (2024)<br><strong>Quantum linear algebra is all you need for Transformer architectures</strong><br><button class=copy-to-clipboard title="Quantum linear algebra is all you need for Transformer architectures" index=237>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-237 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: quant-ph<br>Categories: cs-AI, cs-CL, quant-ph, quant-ph<br>Keyword Score: 30<br>Keywords: Transformer, Large Language Model, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16714v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16714v1.pdf filename=2402.16714v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Generative machine learning methods such as <b>large-language</b> <b>models</b> <b>are</b> revolutionizing the creation of text and images. While these models are powerful they also harness a <b>large</b> <b>amount</b> <b>of</b> computational resources. The <b>transformer</b> is a key component in <b>large</b> <b>language</b> <b>models</b> that aims to generate a suitable completion of a given partial sequence. In this work, we investigate <b>transformer</b> architectures under the lens of fault-tolerant quantum computing. The input model is one where pre-trained weight matrices are given as block encodings to construct the query, key, and value matrices for the <b>transformer.</b> As a first step, we show how to prepare a block encoding of the <b>self-attention</b> matrix, with a row-wise application of the softmax function using the Hadamard product. In addition, we combine quantum subroutines to construct important building blocks in the <b>transformer,</b> the residual connection, layer normalization, and the feed-forward neural network. Our subroutines prepare an amplitude encoding of the <b>transformer</b> output, which can be measured to obtain a prediction. We discuss the potential and challenges for obtaining a quantum advantage.</p></p class="citation"></blockquote><h2 id=csit-9>cs.IT (9)</h2><h3 id=19--238279-slipt-in-joint-dimming-multi-led-owc-systems-with-rate-splitting-multiple-access-sepideh-javadi-et-al-2024>(1/9 | 238/279) SLIPT in Joint Dimming Multi-LED OWC Systems with Rate Splitting Multiple Access (Sepideh Javadi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sepideh Javadi, Sajad Faramarzi, Farshad Zeinali, Hosein Zarini, Mohammad Robat Mili, Panagiotis D. Diamantoulakis, Eduard Jorswieck, George K. Karagiannidis. (2024)<br><strong>SLIPT in Joint Dimming Multi-LED OWC Systems with Rate Splitting Multiple Access</strong><br><button class=copy-to-clipboard title="SLIPT in Joint Dimming Multi-LED OWC Systems with Rate Splitting Multiple Access" index=238>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-238 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, eess-SP, math-IT<br>Keyword Score: 30<br>Keywords: Reinforcement Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16629v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16629v2.pdf filename=2402.16629v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Optical wireless communication (OWC) systems with multiple light-emitting diodes (LEDs) have recently been explored to support energy-limited devices via simultaneous lightwave information and power transfer (SLIPT). The energy consumption, however, becomes considerable by increasing the number of incorporated LEDs. This paper proposes a joint dimming (JD) scheme that lowers the consumed power of a SLIPT-enabled OWC system by controlling the number of active LEDs. We further enhance the data rate of this system by utilizing rate splitting multiple access (RSMA). More specifically, we formulate a data rate maximization problem to optimize the beamforming design, LED selection and RSMA rate adaptation that guarantees the power budget of the OWC transmitter, as well as the quality-of-service (QoS) and an energy harvesting level for users. We propose a dynamic resource allocation solution based on proximal policy optimization (PPO) <b>reinforcement</b> <b>learning.</b> In <b>simulations,</b> the optimal dimming level is determined to initiate a trade-off between the data rate and power consumption. It is also verified that RSMA significantly improves the data rate.</p></p class="citation"></blockquote><h3 id=29--239279-performance-tradeoff-between-overhead-and-achievable-snr-in-ris-beam-training-friedemann-laue-et-al-2024>(2/9 | 239/279) Performance Tradeoff Between Overhead and Achievable SNR in RIS Beam Training (Friedemann Laue et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Friedemann Laue, Vahid Jamali, Robert Schober. (2024)<br><strong>Performance Tradeoff Between Overhead and Achievable SNR in RIS Beam Training</strong><br><button class=copy-to-clipboard title="Performance Tradeoff Between Overhead and Achievable SNR in RIS Beam Training" index=239>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-239 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, eess-SP, math-IT<br>Keyword Score: 30<br>Keywords: Simulation, Simulator, Scaling Law<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16372v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16372v1.pdf filename=2402.16372v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Efficient beam training is the key challenge in the codebook-based configuration of reconfigurable intelligent surfaces (RISs) because the beam training overhead can have a strong impact on the achievable system performance. In this paper, we study the performance tradeoff between overhead and achievable signal-to-noise ratio (SNR) in RIS beam training while taking into account the size of the targeted coverage area, the RIS response time, and the delay for feedback transmissions. Thereby, we consider three common beam training strategies: full search (FS), hierarchical search (HS), and tracking-based search (TS). Our analysis shows that the codebook-based illumination of a given coverage area can be realized with wide- or narrow-beam designs, which result in two different <b>scaling</b> <b>laws</b> for the achievable SNR. Similarly, there are two regimes for the overhead, where the number of pilot symbols required for reliable beam training is dependent on and independent of the SNR, respectively. Based on these insights, we investigate the impact of the beam training overhead on the effective rate and provide an upper bound on the user velocity for which the overhead is negligible. Moreover, when the overhead is not negligible, we show that TS beam training achieves higher effective rates than HS and FS beam training, while HS beam training may or may not outperform FS beam training, depending on the RIS response time, feedback delay, and codebook size. Finally, we present numerical <b>simulation</b> results that verify our theoretical analysis. In particular, our results confirm the existence of the proposed regimes, reveal that fast RISs can lead to negligible overhead for FS beam training, and show that large feedback delays can significantly reduce the performance for HS beam training.</p></p class="citation"></blockquote><h3 id=39--240279-quadratic-message-passing-for-generalized-quadratic-equations-model-huimin-zhu-2024>(3/9 | 240/279) Quadratic Message Passing for Generalized Quadratic Equations Model (Huimin Zhu, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Huimin Zhu. (2024)<br><strong>Quadratic Message Passing for Generalized Quadratic Equations Model</strong><br><button class=copy-to-clipboard title="Quadratic Message Passing for Generalized Quadratic Equations Model" index=240>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-240 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, math-IT<br>Keyword Score: 30<br>Keywords: Message-Passing, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16279v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16279v1.pdf filename=2402.16279v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>For approximate inference in the generalized quadratic equations model, many state-of-the-art algorithms lack any prior knowledge of the target signal structure, exhibits slow convergence, and can not handle any analytic prior knowledge of the target signal structure. So, this paper proposes a new algorithm, Quadratic Message passing (QMP). QMP has a complexity as low as $O(N^{3})$. The SE derived for QMP can capture precisely the per-iteration behavior of the simulated algorithm. <b>Simulation</b> results confirm QMP outperforms many state-of-the-art algorithms.</p></p class="citation"></blockquote><h3 id=49--241279-achievable-rate-optimization-for-stacked-intelligent-metasurface-assisted-holographic-mimo-communications-anastasios-papazafeiropoulos-et-al-2024>(4/9 | 241/279) Achievable Rate Optimization for Stacked Intelligent Metasurface-Assisted Holographic MIMO Communications (Anastasios Papazafeiropoulos et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Anastasios Papazafeiropoulos, Jiancheng An, Pandelis Kourtessis, Tharmalingam Ratnarajah, Symeon Chatzinotas. (2024)<br><strong>Achievable Rate Optimization for Stacked Intelligent Metasurface-Assisted Holographic MIMO Communications</strong><br><button class=copy-to-clipboard title="Achievable Rate Optimization for Stacked Intelligent Metasurface-Assisted Holographic MIMO Communications" index=241>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-241 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, math-IT<br>Keyword Score: 23<br>Keywords: Benchmarking, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16415v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16415v1.pdf filename=2402.16415v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Stacked intelligent metasurfaces (SIM) is a revolutionary technology, which can outperform its single-layer counterparts by performing advanced signal processing relying on wave propagation. In this work, we exploit SIM to enable transmit precoding and receiver combining in holographic multiple-input multiple-output (HMIMO) communications, and we study the achievable rate by formulating a joint optimization problem of the SIM phase shifts at both sides of the transceiver and the covariance matrix of the transmitted signal. Notably, we propose its solution by means of an iterative optimization algorithm that relies on the projected gradient method, and accounts for all optimization parameters simultaneously. We also obtain the step size guaranteeing the convergence of the proposed algorithm. <b>Simulation</b> results provide fundamental insights such the performance improvements compared to the single-RIS counterpart and conventional MIMO system. Remarkably, the proposed algorithm results in the same achievable rate as the alternating optimization (AO) <b>benchmark</b> but with a less number of iterations.</p></p class="citation"></blockquote><h3 id=59--242279-towards-bridging-the-gap-between-near-and-far-field-characterizations-of-the-wireless-channel-navneet-agrawal-et-al-2024>(5/9 | 242/279) Towards Bridging the Gap between Near and Far-Field Characterizations of the Wireless Channel (Navneet Agrawal et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Navneet Agrawal, Ehsan Tohidi, Renato L. G. Cavalcante, Sławomir Stańczak. (2024)<br><strong>Towards Bridging the Gap between Near and Far-Field Characterizations of the Wireless Channel</strong><br><button class=copy-to-clipboard title="Towards Bridging the Gap between Near and Far-Field Characterizations of the Wireless Channel" index=242>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-242 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, eess-SP, math-IT<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16755v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16755v1.pdf filename=2402.16755v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The &ldquo;near-field&rdquo; propagation modeling of wireless channels is necessary to support sixth-generation (6G) technologies, such as intelligent reflecting surface (IRS), that are enabled by large aperture antennas and higher frequency carriers. As the conventional far-field model proves inadequate in this context, there is a pressing need to explore and bridge the gap between near and far-field propagation models. Although far-field models are simple and provide computationally efficient solutions for many practical applications, near-field models provide the most accurate representation of wireless channels. This paper builds upon the foundations of electromagnetic wave propagation theory to derive near and far-field models as approximations of the Green&rsquo;s function (Maxwell&rsquo;s equations). We characterize the near and far-field models both theoretically and with the help of <b>simulations</b> in a line-of-sight (LOS)-only scenario. In particular, for two key applications in multiantenna systems, namely, beamforming and multiple-access, we showcase the advantages of using the near-field model over the far-field, and present a novel scheduling scheme for multiple-access in the near-field regime. Our findings offer insights into the challenge of incorporating near-field models in practical wireless systems, fostering enhanced performance in future communication technologies.</p></p class="citation"></blockquote><h3 id=69--243279-im-based-pilot-assisted-channel-estimation-for-ftn-signaling-hf-communications-simin-keykhosravi-et-al-2024>(6/9 | 243/279) IM-based Pilot-assisted Channel Estimation for FTN Signaling HF Communications (Simin Keykhosravi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Simin Keykhosravi, Ebrahim Bedeer. (2024)<br><strong>IM-based Pilot-assisted Channel Estimation for FTN Signaling HF Communications</strong><br><button class=copy-to-clipboard title="IM-based Pilot-assisted Channel Estimation for FTN Signaling HF Communications" index=243>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-243 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, eess-SP, math-IT<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16618v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16618v1.pdf filename=2402.16618v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper investigates doubly-selective (i.e., time- and frequency-selective) channel estimation in faster-than-Nyquist (FTN) signaling HF communications. In particular, we propose a novel IM-based channel estimation algorithm for FTN signaling HF communications including pilot sequence placement (PSP) and pilot sequence location identification (PSLI) algorithms. At the transmitter, we propose the PSP algorithm that utilizes the locations of pilot sequences to carry additional information bits, thereby improving the SE of HF communications. HF channels have two non-zero independent fading paths with specific fixed delay spread and frequency spread characteristics as outlined in the Union Radio communication Sector (ITU-R) F.1487 and F.520. Having said that, based on the aforementioned properties of the HF channels and the favorable auto-correlation characteristics of the optimal pilot sequence, we propose a novel PSLI algorithm that effectively identifies the pilot sequence location within a given frame at the receiver. This is achieved by showing that the square of the absolute value of the cross-correlation between the received symbols and the pilot sequence consists of a scaled version of the square of the absolute value of the auto-correlation of the pilot sequence weighted by the gain of the corresponding HF channel path. <b>Simulation</b> results show very low pilot sequence location identification errors for HF channels. Our <b>simulation</b> results show a 6 dB improvement in the MSE of the channel estimation as well as about 3.5 dB BER improvement of FTN signaling along with an enhancement in SE compared to the method in [1]. We also achieved an enhancement in SE compared to the work in [2] while maintaining comparable MSE of the channel estimation and BER performance.</p></p class="citation"></blockquote><h3 id=79--244279-performance-of-double-stacked-intelligent-metasurface-assisted-multiuser-massive-mimo-communications-in-the-wave-domain-anastasios-papazafeiropoulos-et-al-2024>(7/9 | 244/279) Performance of Double-Stacked Intelligent Metasurface-Assisted Multiuser Massive MIMO Communications in the Wave Domain (Anastasios Papazafeiropoulos et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Anastasios Papazafeiropoulos, Pandelis Kourtessis, Symeon Chatzinotas. (2024)<br><strong>Performance of Double-Stacked Intelligent Metasurface-Assisted Multiuser Massive MIMO Communications in the Wave Domain</strong><br><button class=copy-to-clipboard title="Performance of Double-Stacked Intelligent Metasurface-Assisted Multiuser Massive MIMO Communications in the Wave Domain" index=244>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-244 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, eess-SP, math-IT<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16405v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16405v1.pdf filename=2402.16405v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Although reconfigurable intelligent surface (RIS) is a promising technology for shaping the propagation environment, it consists of a single-layer structure within inherent limitations regarding the number of beam steering patterns. Based on the recently revolutionary technology, denoted as stacked intelligent metasurface (SIM), we propose its implementation not only on the base station (BS) side in a massive multiple-input multiple-output (mMIMO) setup but also in the intermediate space between the base station and the users to adjust the environment further as needed. For the sake of convenience, we call the former BS SIM (BSIM), and the latter channel SIM (CSIM). Hence, we achieve wave-based combining at the BS and wave-based configuration at the intermediate space. Specifically, we propose a channel estimation method with reduced overhead, being crucial for SIMassisted communications. Next, we derive the uplink sum spectral efficiency (SE) in closed form in terms of statistical channel state information (CSI). Notably, we optimize the phase shifts of both BSIM and CSIM simultaneously by using the projected gradient ascent method (PGAM). Compared to previous works on SIMs, we study the uplink transmission, a mMIMO setup, channel estimation in a single phase, a second SIM at the intermediate space, and simultaneous optimization of the two SIMs. <b>Simulation</b> results show the impact of various parameters on the sum SE, and demonstrate the superiority of our optimization approach compared to the alternating optimization (AO) method.</p></p class="citation"></blockquote><h3 id=89--245279-random-staircase-generator-matrix-codes-qianfan-wang-et-al-2024>(8/9 | 245/279) Random Staircase Generator Matrix Codes (Qianfan Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qianfan Wang, Yiwen Wang, Yixin Wang, Jifan Liang, Xiao Ma. (2024)<br><strong>Random Staircase Generator Matrix Codes</strong><br><button class=copy-to-clipboard title="Random Staircase Generator Matrix Codes" index=245>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-245 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, math-IT<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16245v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16245v1.pdf filename=2402.16245v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we propose a class of codes, referred to as random staircase generator matrix codes (SGMCs), which have staircase-like generator matrices. In the infinite-length region, we prove that the random SGMC is capacity-achieving over binary-input output-symmetric (BIOS) channels. In the finite-length region, we present the representative ordered statistics decoding with local constraints (LC-ROSD) algorithm for the SGMCs. The most distinguished feature of the SGMCs with LC-ROSD is that the staircase-like matrices enable parallel implementation of the Gaussian elimination (GE), avoiding the serial GE of conventional OSD and supporting a potential low decoding latency, as implied from <b>simulations.</b> To analyze the performance of random SGMCs in the finite-length region, we derive the ensemble weight spectrum and invoke the conventional union bound. We also derive a partially random coding union (RCU) bound, which is tighter than the conventional one and is used as a criterion to design the SGMCs. Staircase-like generator matrices allow us to derive a series of (tighter and tighter) lower bounds based on the second-order Bonferroni inequality with the incremental number of codewords. The numerical results show that the decoding performance can match well with the proposed partially RCU bound for different code rates and different profiles. The numerical results also show that the tailored SGMCs with the LC-ROSD algorithm can approach the finite-length performance bound, outperforming the 5G low-density parity-check (LDPC) codes, 5G polar codes, and Reed-Muller (RM) codes.</p></p class="citation"></blockquote><h3 id=99--246279-a-joint-communication-and-computation-design-for-probabilistic-semantic-communications-zhouxiang-zhao-et-al-2024>(9/9 | 246/279) A Joint Communication and Computation Design for Probabilistic Semantic Communications (Zhouxiang Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhouxiang Zhao, Zhaohui Yang, Mingzhe Chen, Zhaoyang Zhang, H. Vincent Poor. (2024)<br><strong>A Joint Communication and Computation Design for Probabilistic Semantic Communications</strong><br><button class=copy-to-clipboard title="A Joint Communication and Computation Design for Probabilistic Semantic Communications" index=246>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-246 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, eess-SP, math-IT<br>Keyword Score: 18<br>Keywords: Graph, Knowledge Graph, Information Retrieval<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16328v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16328v2.pdf filename=2402.16328v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, the problem of joint transmission and computation resource allocation for a multi-user probabilistic semantic communication (PSC) network is investigated. In the considered model, users employ semantic <b>information</b> <b>extraction</b> techniques to compress their large-sized data before transmitting them to a multi-antenna base station (BS). Our model represents large-sized data through substantial <b>knowledge</b> <b>graphs,</b> utilizing shared probability <b>graphs</b> between the users and the BS for efficient semantic compression. The resource allocation problem is formulated as an optimization problem with the objective of maximizing the sum of equivalent rate of all users, considering total power budget and semantic resource limit constraints. The computation load considered in the PSC network is formulated as a non-smooth piecewise function with respect to the semantic compression ratio. To tackle this non-convex non-smooth optimization challenge, a three-stage algorithm is proposed where the solutions for the receive beamforming matrix of the BS, transmit power of each user, and semantic compression ratio of each user are obtained stage by stage. Numerical results validate the effectiveness of our proposed scheme.</p></p class="citation"></blockquote><h2 id=csma-2>cs.MA (2)</h2><h3 id=12--247279-distributed-finite-time-differentiator-for-multi-agent-systems-under-directed-graph-weile-chen-et-al-2024>(1/2 | 247/279) Distributed Finite-time Differentiator for Multi-agent Systems Under Directed Graph (Weile Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Weile Chen, Haibo Du, Shihua Li. (2024)<br><strong>Distributed Finite-time Differentiator for Multi-agent Systems Under Directed Graph</strong><br><button class=copy-to-clipboard title="Distributed Finite-time Differentiator for Multi-agent Systems Under Directed Graph" index=247>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-247 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.MA<br>Categories: cs-MA, cs.MA<br>Keyword Score: 23<br>Keywords: Graph, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16260v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16260v1.pdf filename=2402.16260v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper proposes a new distributed finite-time differentiator (DFD) for multi-agent systems (MAS) under directed <b>graph,</b> which extends the differentiator algorithm from the centralized case to the distributed case by only using relative/absolute position information. By skillfully constructing a Lyapunov function, the finite-time stability of the closed-loop system under DFD is proved. Inspired by the duality principle of control theory, a distributed continuous finite-time output consensus algorithm extended from DFD for a class of leader-follower MAS is provided, which not only completely suppresses disturbance, but also avoids chattering. Finally, several <b>simulation</b> examples are given to verify the effectiveness of the DFD.</p></p class="citation"></blockquote><h3 id=22--248279-navigating-complexity-orchestrated-problem-solving-with-multi-agent-llms-sumedh-rasal-et-al-2024>(2/2 | 248/279) Navigating Complexity: Orchestrated Problem Solving with Multi-Agent LLMs (Sumedh Rasal et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sumedh Rasal, E. J. Hauer. (2024)<br><strong>Navigating Complexity: Orchestrated Problem Solving with Multi-Agent LLMs</strong><br><button class=copy-to-clipboard title="Navigating Complexity: Orchestrated Problem Solving with Multi-Agent LLMs" index=248>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-248 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.MA<br>Categories: cs-MA, cs.MA<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16713v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16713v1.pdf filename=2402.16713v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have demonstrated remarkable capabilities in solving various tasks, yet they often struggle with comprehensively addressing complex and vague problems. Existing approaches, including multi-agent <b>LLM</b> systems, offer solutions to certain challenges but still require manual setup and lack scalability. To address this gap, we propose a novel approach leveraging decomposition to enable <b>LLMs</b> to tackle vague problems effectively. Our approach involves an orchestrating <b>LLM</b> that interacts with users to understand the problem and then decomposes it into tangible sub-problems. Instead of expecting the <b>LLM</b> to solve the entire problem in one go, we train it to ask follow-up questions to gain a deeper understanding of the user&rsquo;s requirements. Once the problem is adequately understood, the orchestrating <b>LLM</b> divides it into smaller, manageable sub-problems. Each sub-problem is then assigned to specialized <b>LLM</b> agents or non-LLM functions for resolution. These agents work in parallel to solve their respective sub-problems, with the orchestrating <b>LLM</b> overseeing the process and compiling the solutions into a comprehensive answer for the user. By adopting this decomposition approach, we alleviate the constraints imposed by token limitations on <b>LLM</b> outputs and empower them to provide nuanced solutions to complex and ambiguous problems. Through our approach, we aim to enable <b>LLMs</b> to think and operate more like humans, breaking down complex problems into manageable parts and collaboratively solving them. This not only enhances the problem-solving capabilities of <b>LLMs</b> but also offers a scalable and efficient method for addressing a wide range of real-world challenges.</p></p class="citation"></blockquote><h2 id=csgr-2>cs.GR (2)</h2><h3 id=12--249279-cell-constrained-particles-for-incompressible-fluids-zohar-levi-2024>(1/2 | 249/279) Cell-Constrained Particles for Incompressible Fluids (Zohar Levi, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zohar Levi. (2024)<br><strong>Cell-Constrained Particles for Incompressible Fluids</strong><br><button class=copy-to-clipboard title="Cell-Constrained Particles for Incompressible Fluids" index=249>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-249 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.GR<br>Categories: cs-GR, cs.GR, physics-flu-dyn<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17088v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17088v1.pdf filename=2402.17088v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Incompressibility is a fundamental condition in most fluid models. Accumulation of <b>simulation</b> errors violates it and causes volume loss. Past work suggested correction methods to battle it. These methods, however, are imperfect and in some cases inadequate. We present a method for fluid <b>simulation</b> that strictly enforces incompressibility based on a grid-related definition of discrete incompressibility. We formulate a linear programming (LP) problem that bounds the number of particles that end up in each grid cell. A variant of the band method is offered for acceleration, which requires special constraints to ensure volume preservation. Further acceleration is achieved by simplifying the problem and adding a special band correction step that is formulated as a minimum-cost flow problem (MCFP). We also address coupling with solids in our framework and demonstrate advantages over prior work.</p></p class="citation"></blockquote><h3 id=22--250279-non-euclidean-sliced-optimal-transport-sampling-baptiste-genest-et-al-2024>(2/2 | 250/279) Non-Euclidean Sliced Optimal Transport Sampling (Baptiste Genest et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Baptiste Genest, Nicolas Courty, David Coeurjolly. (2024)<br><strong>Non-Euclidean Sliced Optimal Transport Sampling</strong><br><button class=copy-to-clipboard title="Non-Euclidean Sliced Optimal Transport Sampling" index=250>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-250 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.GR<br>Categories: cs-GR, cs.GR<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16981v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16981v1.pdf filename=2402.16981v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In machine learning and computer graphics, a fundamental task is the approximation of a probability density function through a well-dispersed collection of samples. Providing a formal metric for measuring the distance between probability measures on general spaces, Optimal Transport (OT) emerges as a pivotal theoretical framework within this context. However, the associated computational burden is prohibitive in most real-world scenarios. Leveraging the simple structure of OT in 1D, Sliced Optimal Transport (SOT) has appeared as an efficient alternative to generate samples in Euclidean spaces. This paper pushes the boundaries of SOT utilization in computational <b>geometry</b> problems by extending its application to sample densities residing on more diverse mathematical domains, including the spherical space Sd , the hyperbolic plane Hd , and the real projective plane Pd . Moreover, it ensures the quality of these samples by achieving a blue noise characteristic, regardless of the dimensionality involved. The robustness of our approach is highlighted through its application to various <b>geometry</b> processing tasks, such as the intrinsic blue noise sampling of meshes, as well as the sampling of directions and rotations. These applications collectively underscore the efficacy of our methodology.</p></p class="citation"></blockquote><h2 id=q-bioqm-1>q-bio.QM (1)</h2><h3 id=11--251279-an-optimal-transport-model-for-dynamical-shapes-collective-motion-and-cellular-aggregates-antoine-diez-et-al-2024>(1/1 | 251/279) An optimal transport model for dynamical shapes, collective motion and cellular aggregates (Antoine Diez et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Antoine Diez, Jean Feydy. (2024)<br><strong>An optimal transport model for dynamical shapes, collective motion and cellular aggregates</strong><br><button class=copy-to-clipboard title="An optimal transport model for dynamical shapes, collective motion and cellular aggregates" index=251>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-251 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: q-bio.QM<br>Categories: 35Q92, 92-08, 49Q22, 91-10, 92-04, 74-10, 70-10, cs-NA, math-DS, math-NA, q-bio-QM, q-bio.QM<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17086v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17086v1.pdf filename=2402.17086v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Many biological systems such as cell aggregates, tissues or bacterial colonies behave as unconventional systems of particles that are strongly constrained by volume exclusion and shape interactions. Understanding how these constraints lead to macroscopic self-organized structures is a fundamental question in e.g. developmental biology. To this end, various types of computational models have been developed: phase fields, cellular automata, vertex models, level-set, finite element <b>simulations,</b> etc. We introduce a new framework based on optimal transport theory to model particle systems with arbitrary dynamical shapes and deformability. Our method builds upon the pioneering work of Brenier on incompressible fluids and its recent applications to materials science. It lets us specify the shapes of individual cells and supports a wide range of interaction mechanisms, while automatically taking care of the volume exclusion constraint at an affordable numerical cost. We showcase the versatility of this approach by reproducing several classical systems in computational biology. Our Python code is freely available at: <a href=https://www.github.com/antoinediez/ICeShOT>www.github.com/antoinediez/ICeShOT</a></p></p class="citation"></blockquote><h2 id=mathna-7>math.NA (7)</h2><h3 id=17--252279-a-stochastic-perturbation-approach-to-nonlinear-bifurcating-problems-isabella-carla-gonnella-et-al-2024>(1/7 | 252/279) A stochastic perturbation approach to nonlinear bifurcating problems (Isabella Carla Gonnella et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Isabella Carla Gonnella, Moaad Khamlich, Federico Pichi, Gianluigi Rozza. (2024)<br><strong>A stochastic perturbation approach to nonlinear bifurcating problems</strong><br><button class=copy-to-clipboard title="A stochastic perturbation approach to nonlinear bifurcating problems" index=252>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-252 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: cs-NA, math-AP, math-NA, math-PR, math.NA<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16803v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16803v1.pdf filename=2402.16803v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Incorporating probabilistic terms in mathematical models is crucial for capturing and quantifying uncertainties in real-world systems. Indeed, randomness can have a significant impact on the behavior of the problem&rsquo;s solution, and a deeper analysis is needed to obtain more realistic and informative results. On the other hand, the investigation of stochastic models may require great computational resources due to the importance of generating numerous realizations of the system to have meaningful statistics. This makes the development of complexity reduction techniques, such as surrogate models, essential for enabling efficient and scalable <b>simulations.</b> In this work, we exploit polynomial chaos (PC) expansion to study the accuracy of surrogate representations for a bifurcating phenomena in fluid dynamics, namely the Coanda effect, where the stochastic setting gives a different perspective on the non-uniqueness of the solution. Then, its inclusion in the finite element setting is described, arriving to the formulation of the enhanced Spectral Stochastic Finite Element Method (SSFEM). Moreover, we investigate the connections between the deterministic bifurcation diagram and the PC polynomials, underlying their capability in reconstructing the whole solution manifold.</p></p class="citation"></blockquote><h3 id=27--253279-finite-element-schemes-with-tangential-motion-for-fourth-order-geometric-curve-evolutions-in-arbitrary-codimension-klaus-deckelnick-et-al-2024>(2/7 | 253/279) Finite element schemes with tangential motion for fourth order geometric curve evolutions in arbitrary codimension (Klaus Deckelnick et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Klaus Deckelnick, Robert Nürnberg. (2024)<br><strong>Finite element schemes with tangential motion for fourth order geometric curve evolutions in arbitrary codimension</strong><br><button class=copy-to-clipboard title="Finite element schemes with tangential motion for fourth order geometric curve evolutions in arbitrary codimension" index=253>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-253 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: cs-NA, math-NA, math.NA<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16799v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16799v1.pdf filename=2402.16799v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce novel finite element schemes for curve diffusion and elastic flow in arbitrary codimension. The schemes are based on a variational form of a system that includes a specifically chosen tangential motion. We derive optimal $L^2$- and $H^1$-error bounds for continuous-in-time semidiscrete finite element approximations that use piecewise linear elements. In addition, we consider fully discrete schemes and, in the case of curve diffusion, prove unconditional stability for it. Finally, we present several numerical <b>simulations,</b> including some convergence experiments that confirm the derived error bounds. The presented <b>simulations</b> suggest that the tangential motion leads to equidistribution in practice.</p></p class="citation"></blockquote><h3 id=37--254279-structure-preserving-operator-learning-modeling-the-collision-operator-of-kinetic-equations-jae-yong-lee-et-al-2024>(3/7 | 254/279) Structure-Preserving Operator Learning: Modeling the Collision Operator of Kinetic Equations (Jae Yong Lee et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jae Yong Lee, Steffen Schotthöfer, Tianbai Xiao, Sebastian Krumscheid, Martin Frank. (2024)<br><strong>Structure-Preserving Operator Learning: Modeling the Collision Operator of Kinetic Equations</strong><br><button class=copy-to-clipboard title="Structure-Preserving Operator Learning: Modeling the Collision Operator of Kinetic Equations" index=254>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-254 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: cs-NA, math-NA, math.NA<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16613v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16613v1.pdf filename=2402.16613v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This work explores the application of deep operator learning principles to a problem in statistical physics. Specifically, we consider the linear kinetic equation, consisting of a differential advection operator and an integral collision operator, which is a powerful yet expensive mathematical model for interacting particle systems with ample applications, e.g., in radiation transport. We investigate the capabilities of the Deep Operator network (DeepONet) approach to modelling the high dimensional collision operator of the linear kinetic equation. This integral operator has crucial analytical structures that a surrogate model, e.g., a DeepONet, needs to preserve to enable meaningful physical <b>simulation.</b> We propose several DeepONet modifications to encapsulate essential structural properties of this integral operator in a DeepONet model. To be precise, we adapt the architecture of the trunk-net so the DeepONet has the same collision invariants as the theoretical kinetic collision operator, thus preserving conserved quantities, e.g., mass, of the modeled many-particle system. Further, we propose an entropy-inspired data-sampling method tailored to train the modified DeepONet surrogates without requiring an excessive expensive <b>simulation-based</b> data generation.</p></p class="citation"></blockquote><h3 id=47--255279-isogeometric-analysis-of-the-laplace-eigenvalue-problem-on-circular-sectors-regularity-properties-graded-meshes--variational-crimes-thomas-apel-et-al-2024>(4/7 | 255/279) Isogeometric analysis of the Laplace eigenvalue problem on circular sectors: Regularity properties, graded meshes & variational crimes (Thomas Apel et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Thomas Apel, Philipp Zilk. (2024)<br><strong>Isogeometric analysis of the Laplace eigenvalue problem on circular sectors: Regularity properties, graded meshes & variational crimes</strong><br><button class=copy-to-clipboard title="Isogeometric analysis of the Laplace eigenvalue problem on circular sectors: Regularity properties, graded meshes & variational crimes" index=255>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-255 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: 65N25, 65D07, 65N50, 35A21, 35C05, cs-NA, math-AP, math-NA, math.NA<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16589v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16589v1.pdf filename=2402.16589v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The Laplace eigenvalue problem on circular sectors has eigenfunctions with corner singularities. Standard methods may produce suboptimal approximation results. To address this issue, a novel numerical algorithm that enhances standard isogeometric analysis is proposed in this paper by using a single-patch graded mesh refinement scheme. Numerical tests demonstrate optimal convergence rates for both the eigenvalues and eigenfunctions. Furthermore, the results show that smooth splines possess a superior approximation constant compared to their $C^0$-continuous counterparts for the lower part of the Laplace spectrum. This is an extension of previous findings about excellent spectral approximation properties of smooth splines on rectangular domains to circular sectors. In addition, graded meshes prove to be particularly advantageous for an accurate approximation of a limited number of eigenvalues. The novel algorithm applied here has a drawback in the singularity of the isogeometric parameterization. It results in some basis functions not belonging to the solution space of the corresponding weak problem, which is considered a variational crime. However, the approach proves to be robust. Finally, a hierarchical mesh structure is presented to avoid anisotropic elements, omit redundant degrees of freedom and keep the number of basis functions contributing to the variational crime constant, independent of the mesh size. Numerical results validate the effectiveness of hierarchical mesh grading for the <b>simulation</b> of eigenfunctions with and without corner singularities.</p></p class="citation"></blockquote><h3 id=57--256279-to-be-or-not-to-be-that-is-the-question-exploring-the-pseudorandom-generation-of-texts-to-write-hamlet-from-the-perspective-of-the-infinite-monkey-theorem-ergon-cugler-de-moraes-silva-2024>(5/7 | 256/279) To be, or not to be, that is the Question: Exploring the pseudorandom generation of texts to write Hamlet from the perspective of the Infinite Monkey Theorem (Ergon Cugler de Moraes Silva, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ergon Cugler de Moraes Silva. (2024)<br><strong>To be, or not to be, that is the Question: Exploring the pseudorandom generation of texts to write Hamlet from the perspective of the Infinite Monkey Theorem</strong><br><button class=copy-to-clipboard title="To be, or not to be, that is the Question: Exploring the pseudorandom generation of texts to write Hamlet from the perspective of the Infinite Monkey Theorem" index=256>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-256 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: cs-NA, math-NA, math.NA, stat-CO<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16253v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16253v1.pdf filename=2402.16253v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This article explores the theoretical and computational aspects of the Infinite Monkey Theorem, investigating the number of attempts and the time required for a set of pseudorandom characters to assemble and recite Hamlets iconic phrase, To be, or not to be, that is the Question. Drawing inspiration from Emile Borels original concept (1913), the study delves into the practical implications of pseudorandomness using Python. Employing Python <b>simulations</b> to generate excerpts from Hamlet, the research navigates historical perspectives and bridges early theoretical foundations with contemporary computational approaches. A set of tests reveals the attempts and time required to generate incremental parts of the target phrase. Utilizing these results, growth factors are calculated, projecting estimated attempts and time for each text part. The findings indicate an astronomical challenge to generate the entire phrase, requiring approximately 2.68x10e69 attempts and 2.95x10e66 seconds - equivalent to 8.18x10e62 hours or 9.32x10e55 years. This temporal scale, exceeding the age of the universe by 6.75x10e45 times, underscores the immense complexity and improbability of random literary creation. The article concludes with reflections on the mathematical intricacies and statistical feasibility within the context of the Infinite Monkey Theorem, emphasizing the theoretical musings surrounding infinite time and the profound limitations inherent in such endeavors. And that only infinity could write Hamlet randomly.</p></p class="citation"></blockquote><h3 id=67--257279-point-collocation-with-mollified-piecewise-polynomial-approximants-for-high-order-partial-differential-equations-dewangga-alfarisy-et-al-2024>(6/7 | 257/279) Point collocation with mollified piecewise polynomial approximants for high-order partial differential equations (Dewangga Alfarisy et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dewangga Alfarisy, Lavi Zuhal, Michael Ortiz, Fehmi Cirak, Eky Febrianto. (2024)<br><strong>Point collocation with mollified piecewise polynomial approximants for high-order partial differential equations</strong><br><button class=copy-to-clipboard title="Point collocation with mollified piecewise polynomial approximants for high-order partial differential equations" index=257>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-257 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: cs-NA, math-NA, math.NA<br>Keyword Score: 10<br>Keywords: Convolution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16548v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16548v1.pdf filename=2402.16548v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The solution approximation for partial differential equations (PDEs) can be substantially improved using smooth basis functions. The recently introduced mollified basis functions are constructed through mollification, or <b>convolution,</b> of cell-wise defined piecewise polynomials with a smooth mollifier of certain characteristics. The properties of the mollified basis functions are governed by the order of the piecewise functions and the smoothness of the mollifier. In this work, we exploit the high-order and high-smoothness properties of the mollified basis functions for solving PDEs through the point collocation method. The basis functions are evaluated at a set of collocation points in the domain. In addition, boundary conditions are imposed at a set of boundary collocation points distributed over the domain boundaries. To ensure the stability of the resulting linear system of equations, the number of collocation points is set larger than the total number of basis functions. The resulting linear system is overdetermined and is solved using the least square technique. The presented numerical examples confirm the convergence of the proposed approximation scheme for Poisson, linear elasticity, and biharmonic problems. We study in particular the influence of the mollifier and the spatial distribution of the collocation points.</p></p class="citation"></blockquote><h3 id=77--258279-discovering-artificial-viscosity-models-for-discontinuous-galerkin-approximation-of-conservation-laws-using-physics-informed-machine-learning-matteo-caldana-et-al-2024>(7/7 | 258/279) Discovering Artificial Viscosity Models for Discontinuous Galerkin Approximation of Conservation Laws using Physics-Informed Machine Learning (Matteo Caldana et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Matteo Caldana, Paola F. Antonietti, Luca Dede&rsquo;. (2024)<br><strong>Discovering Artificial Viscosity Models for Discontinuous Galerkin Approximation of Conservation Laws using Physics-Informed Machine Learning</strong><br><button class=copy-to-clipboard title="Discovering Artificial Viscosity Models for Discontinuous Galerkin Approximation of Conservation Laws using Physics-Informed Machine Learning" index=258>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-258 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: 35L65, 65M60, 68T01, cs-LG, cs-NA, math-NA, math.NA<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16517v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16517v1.pdf filename=2402.16517v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Finite element-based high-order solvers of conservation laws offer large accuracy but face challenges near discontinuities due to the Gibbs phenomenon. Artificial viscosity is a popular and effective solution to this problem based on physical insight. In this work, we present a physics-informed machine learning algorithm to automate the discovery of artificial viscosity models in a non-supervised paradigm. The algorithm is inspired by <b>reinforcement</b> <b>learning</b> and trains a neural network acting cell-by-cell (the viscosity model) by minimizing a loss defined as the difference with respect to a reference solution thanks to automatic differentiation. This enables a dataset-free training procedure. We prove that the algorithm is effective by integrating it into a state-of-the-art Runge-Kutta discontinuous Galerkin solver. We showcase several numerical tests on scalar and vectorial problems, such as Burgers&rsquo; and Euler&rsquo;s equations in one and two dimensions. Results demonstrate that the proposed approach trains a model that is able to outperform classical viscosity models. Moreover, we show that the learnt artificial viscosity model is able to generalize across different problems and parameters.</p></p class="citation"></blockquote><h2 id=cshc-5>cs.HC (5)</h2><h3 id=15--259279-if-in-a-crowdsourced-data-annotation-pipeline-a-gpt-4-zeyu-he-et-al-2024>(1/5 | 259/279) If in a Crowdsourced Data Annotation Pipeline, a GPT-4 (Zeyu He et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zeyu He, Chieh-Yang Huang, Chien-Kuang Cornelia Ding, Shaurya Rohatgi, Ting-Hao &lsquo;Kenneth&rsquo; Huang. (2024)<br><strong>If in a Crowdsourced Data Annotation Pipeline, a GPT-4</strong><br><button class=copy-to-clipboard title="If in a Crowdsourced Data Annotation Pipeline, a GPT-4" index=259>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-259 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-AI, cs-CL, cs-HC, cs-LG, cs.HC<br>Keyword Score: 20<br>Keywords: GPT, GPT-4<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16795v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16795v1.pdf filename=2402.16795v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent studies indicated <b>GPT-4</b> outperforms online crowd workers in data labeling accuracy, notably workers from Amazon Mechanical Turk (MTurk). However, these studies were criticized for deviating from standard crowdsourcing practices and emphasizing individual workers&rsquo; performances over the whole data-annotation process. This paper compared <b>GPT-4</b> and an ethical and well-executed MTurk pipeline, with 415 workers labeling 3,177 sentence segments from 200 scholarly articles using the CODA-19 scheme. Two worker interfaces yielded 127,080 labels, which were then used to infer the final labels through eight label-aggregation algorithms. Our evaluation showed that despite best practices, MTurk pipeline&rsquo;s highest accuracy was 81.5%, whereas <b>GPT-4</b> achieved 83.6%. Interestingly, when combining <b>GPT-4&rsquo;s</b> labels with crowd labels collected via an advanced worker interface for aggregation, 2 out of the 8 algorithms achieved an even higher accuracy (87.5%, 87.0%). Further analysis suggested that, when the crowd&rsquo;s and <b>GPT-4&rsquo;s</b> labeling strengths are complementary, aggregating them could increase labeling accuracy.</p></p class="citation"></blockquote><h3 id=25--260279-the-interaction-fidelity-model-a-taxonomy-to-distinguish-the-aspects-of-fidelity-in-virtual-reality-michael-bonfert-et-al-2024>(2/5 | 260/279) The Interaction Fidelity Model: A Taxonomy to Distinguish the Aspects of Fidelity in Virtual Reality (Michael Bonfert et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Michael Bonfert, Thomas Muender, Ryan P. McMahan, Frank Steinicke, Doug Bowman, Rainer Malaka, Tanja Döring. (2024)<br><strong>The Interaction Fidelity Model: A Taxonomy to Distinguish the Aspects of Fidelity in Virtual Reality</strong><br><button class=copy-to-clipboard title="The Interaction Fidelity Model: A Taxonomy to Distinguish the Aspects of Fidelity in Virtual Reality" index=260>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-260 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: H-5-2; H-5-1; I-3-7; H-1-2, cs-GR, cs-HC, cs-MM, cs.HC<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16665v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16665v1.pdf filename=2402.16665v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Fidelity describes how closely a replication resembles the original. It can be helpful to analyze how faithful interactions in virtual reality (VR) are to a reference interaction. In prior research, fidelity has been restricted to the <b>simulation</b> of reality - also called realism. Our definition includes other reference interactions, such as superpowers or fiction. Interaction fidelity is a multilayered concept. Unfortunately, different aspects of fidelity have either not been distinguished in scientific discourse or referred to with inconsistent terminology. Therefore, we present the Interaction Fidelity Model (IntFi Model). Based on the human-computer interaction loop, it systematically covers all stages of VR interactions. The conceptual model establishes a clear structure and precise definitions of eight distinct components. It was reviewed through interviews with fourteen VR experts. We provide guidelines, diverse examples, and educational material to universally apply the IntFi Model to any VR experience. We identify common patterns and propose foundational research opportunities.</p></p class="citation"></blockquote><h3 id=35--261279-deconstructing-the-veneer-of-simplicity-co-designing-introductory-generative-ai-workshops-with-local-entrepreneurs-yasmine-kotturi-et-al-2024>(3/5 | 261/279) Deconstructing the Veneer of Simplicity: Co-Designing Introductory Generative AI Workshops with Local Entrepreneurs (Yasmine Kotturi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yasmine Kotturi, Angel Anderson, Glenn Ford, Michael Skirpan, Jeffrey P. Bigham. (2024)<br><strong>Deconstructing the Veneer of Simplicity: Co-Designing Introductory Generative AI Workshops with Local Entrepreneurs</strong><br><button class=copy-to-clipboard title="Deconstructing the Veneer of Simplicity: Co-Designing Introductory Generative AI Workshops with Local Entrepreneurs" index=261>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-261 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-AI, cs-HC, cs.HC<br>Keyword Score: 10<br>Keywords: Generative AI<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17082v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17082v1.pdf filename=2402.17082v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Generative</b> <b>AI</b> platforms and features are permeating many aspects of work. Entrepreneurs from lean economies in particular are well positioned to outsource tasks to <b>generative</b> <b>AI</b> given limited resources. In this paper, we work to address a growing disparity in use of these technologies by building on a four-year partnership with a local entrepreneurial hub dedicated to equity in tech and entrepreneurship. Together, we co-designed an interactive workshops series aimed to onboard local entrepreneurs to <b>generative</b> <b>AI</b> platforms. Alongside four community-driven and iterative workshops with entrepreneurs across five months, we conducted interviews with 15 local entrepreneurs and community providers. We detail the importance of communal and supportive exposure to <b>generative</b> <b>AI</b> tools for local entrepreneurs, scaffolding actionable use (and supporting non-use), demystifying <b>generative</b> <b>AI</b> technologies by emphasizing entrepreneurial power, while simultaneously deconstructing the veneer of simplicity to address the many operational skills needed for successful application.</p></p class="citation"></blockquote><h3 id=45--262279-a-visualization-tool-to-explore-alphabet-orderings-for-the-burrows-wheeler-transform-lily-major-et-al-2024>(4/5 | 262/279) A visualization tool to explore alphabet orderings for the Burrows-Wheeler Transform (Lily Major et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lily Major, Dave Davies, Amanda Clare, Jacqueline W. Daykin, Benjamin Mora, Christine Zarges. (2024)<br><strong>A visualization tool to explore alphabet orderings for the Burrows-Wheeler Transform</strong><br><button class=copy-to-clipboard title="A visualization tool to explore alphabet orderings for the Burrows-Wheeler Transform" index=262>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-262 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: H-5-2, cs-HC, cs.HC<br>Keyword Score: 10<br>Keywords: Text Transformation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17005v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17005v1.pdf filename=2402.17005v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The Burrows-Wheeler Transform (BWT) is an efficient invertible <b>text</b> <b>transformation</b> algorithm with the properties of tending to group identical characters together in a run, and enabling search of the <b>text.</b> <b>This</b> transformation has extensive uses particularly in lossless compression algorithms, indexing, and within bioinformatics for sequence alignment tasks. There has been recent interest in minimizing the number of identical character runs ($r$) for a transform and in finding useful alphabet orderings for the sorting step of the matrix associated with the BWT construction. This motivates the inspection of many transforms while developing algorithms. However, the full Burrows-Wheeler matrix is $O(n^2)$ space and therefore very difficult to display and inspect for large input sizes. In this paper we present a graphical user interface (GUI) for working with BWTs, which includes features for searching for matrix row prefixes, skipping over sections in the right-most column (the transform), and displaying BWTs while exploring alphabet orderings with the goal of minimizing the number of runs.</p></p class="citation"></blockquote><h3 id=55--263279-speech-as-interactive-design-material-sidm-how-to-design-and-evaluate-task-tailored-synthetic-voices-mateusz-dubiel-et-al-2024>(5/5 | 263/279) Speech as Interactive Design Material (SIDM): How to design and evaluate task-tailored synthetic voices? (Mateusz Dubiel et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mateusz Dubiel, Matthew Aylett, Anuschka Schmitt, Zilin Ma, Gary Hsieh, Thiemo Wambsganss. (2024)<br><strong>Speech as Interactive Design Material (SIDM): How to design and evaluate task-tailored synthetic voices?</strong><br><button class=copy-to-clipboard title="Speech as Interactive Design Material (SIDM): How to design and evaluate task-tailored synthetic voices?" index=263>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-263 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-HC, cs.HC<br>Keyword Score: 10<br>Keywords: Text-to-speech<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16592v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16592v1.pdf filename=2402.16592v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The aim of this workshop is two-fold. First, it aims to establish a research community focused on design and evaluation of synthetic speech <b>(TTS)</b> interfaces that are tailored not only to goal oriented tasks (e.g., food ordering, online shopping) but also personal growth and resilience promoting applications (e.g., coaching, mindful reflection, and tutoring). Second, through discussion and collaborative efforts, to establish a set of practices and standards that will help to improve ecological validity of <b>TTS</b> evaluation. In particular, the workshop will explore the topics such as: interaction design of voice-based conversational interfaces; the interplay between prosodic aspects (e.g., pitch variance, loudness, jitter) of <b>TTS</b> and its impact on voice perception. This workshop will serve as a platform on which to build a community that is better equipped to tackle the dynamic field of interactive <b>TTS</b> interfaces, which remains understudied, yet increasingly pertinent to everyday lives of users.</p></p class="citation"></blockquote><h2 id=astro-phsr-1>astro-ph.SR (1)</h2><h3 id=11--264279-performance-of-high-order-godunov-type-methods-in-simulations-of-astrophysical-low-mach-number-flows-g-leidi-et-al-2024>(1/1 | 264/279) Performance of high-order Godunov-type methods in simulations of astrophysical low Mach number flows (G. Leidi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>G. Leidi, R. Andrassy, W. Barsukow, J. Higl, P. V. F. Edelmann, F. K. Röpke. (2024)<br><strong>Performance of high-order Godunov-type methods in simulations of astrophysical low Mach number flows</strong><br><button class=copy-to-clipboard title="Performance of high-order Godunov-type methods in simulations of astrophysical low Mach number flows" index=264>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-264 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: astro-ph.SR<br>Categories: astro-ph-IM, astro-ph-SR, astro-ph.SR, cs-NA, math-NA, physics-flu-dyn<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16706v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16706v1.pdf filename=2402.16706v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>High-order Godunov methods for gas dynamics have become a standard tool for simulating different classes of astrophysical flows. Their accuracy is mostly determined by the spatial interpolant used to reconstruct the pair of Riemann states at cell interfaces and by the Riemann solver that computes the interface fluxes. In most Godunov-type methods, these two steps can be treated independently, so that many different schemes can in principle be built from the same numerical framework. In this work, we use our fully compressible Seven-League Hydro (SLH) code to test the accuracy of six reconstruction methods and three approximate Riemann solvers on two- and three-dimensional (2D and 3D) problems involving subsonic flows only. We consider Mach numbers in the range from $10^{-3}$ to $10^{-1}$ in a well-posed, 2D, Kelvin&ndash;Helmholtz instability problem and a 3D turbulent convection zone that excites internal gravity waves in an overlying stable layer. We find that (i) there is a spread of almost four orders of magnitude in computational cost per fixed accuracy between the methods tested in this study, with the most performant method being a combination of a &ldquo;low-dissipation&rdquo; Riemann solver and a sextic reconstruction scheme, (ii) the low-dissipation solver always outperforms conventional Riemann solvers on a fixed grid when the reconstruction scheme is kept the same, (iii) in <b>simulations</b> of turbulent flows, increasing the order of spatial reconstruction reduces the characteristic dissipation length scale achieved on a given grid even if the overall scheme is only second order accurate, (iv) reconstruction methods based on slope-limiting techniques tend to generate artificial, high-frequency acoustic waves during the evolution of the flow, (v) unlimited reconstruction methods introduce oscillations in the thermal stratification near the convective boundary, where the entropy gradient is steep.</p></p class="citation"></blockquote><h2 id=physicschem-ph-1>physics.chem-ph (1)</h2><h3 id=11--265279-trustmol-trustworthy-inverse-molecular-design-via-alignment-with-molecular-dynamics-kevin-tirta-wijaya-et-al-2024>(1/1 | 265/279) TrustMol: Trustworthy Inverse Molecular Design via Alignment with Molecular Dynamics (Kevin Tirta Wijaya et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kevin Tirta Wijaya, Navid Ansari, Hans-Peter Seidel, Vahid Babaei. (2024)<br><strong>TrustMol: Trustworthy Inverse Molecular Design via Alignment with Molecular Dynamics</strong><br><button class=copy-to-clipboard title="TrustMol: Trustworthy Inverse Molecular Design via Alignment with Molecular Dynamics" index=265>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-265 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: physics.chem-ph<br>Categories: cs-LG, physics-chem-ph, physics.chem-ph, q-bio-QM<br>Keyword Score: 20<br>Keywords: Autoencoder, Variational Autoencoder<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16930v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16930v1.pdf filename=2402.16930v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Data-driven generation of molecules with desired properties, also known as inverse molecular design (IMD), has attracted significant attention in recent years. Despite the significant progress in the accuracy and diversity of solutions, existing IMD methods lag behind in terms of trustworthiness. The root issue is that the design process of these methods is increasingly more implicit and indirect, and this process is also isolated from the native forward process (NFP), the ground-truth function that models the molecular dynamics. Following this insight, we propose TrustMol, an IMD method built to be trustworthy. For this purpose, TrustMol relies on a set of technical novelties including a new <b>variational</b> <b>autoencoder</b> network. Moreover, we propose a latent-property pairs acquisition method to effectively navigate the complexities of molecular latent optimization, a process that seems intuitive yet challenging due to the high-frequency and discontinuous nature of molecule space. TrustMol also integrates uncertainty-awareness into molecular latent optimization. These lead to improvements in both explainability and reliability of the IMD process. We validate the trustworthiness of TrustMol through a wide range of experiments.</p></p class="citation"></blockquote><h2 id=eesssp-1>eess.SP (1)</h2><h3 id=11--266279-egnn-c-interpretable-evolving-granular-neural-network-and-application-in-classification-of-weakly-supervised-eeg-data-streams-daniel-leite-et-al-2024>(1/1 | 266/279) EGNN-C+: Interpretable Evolving Granular Neural Network and Application in Classification of Weakly-Supervised EEG Data Streams (Daniel Leite et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Daniel Leite, Alisson Silva, Gabriella Casalino, Arnab Sharma, Danielle Fortunato, Axel-Cyrille Ngomo. (2024)<br><strong>EGNN-C+: Interpretable Evolving Granular Neural Network and Application in Classification of Weakly-Supervised EEG Data Streams</strong><br><button class=copy-to-clipboard title="EGNN-C+: Interpretable Evolving Granular Neural Network and Application in Classification of Weakly-Supervised EEG Data Streams" index=266>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-266 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SP<br>Categories: cs-AI, cs-LG, cs-NE, eess-SP, eess.SP<br>Keyword Score: 20<br>Keywords: Weakly-supervised Learning, Emotion Recognition<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17792v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17792v1.pdf filename=2402.17792v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce a modified incremental learning algorithm for evolving Granular Neural Network Classifiers (eGNN-C+). We use double-boundary hyper-boxes to represent granules, and customize the adaptation procedures to enhance the robustness of outer boxes for data coverage and noise suppression, while ensuring that inner boxes remain flexible to capture drifts. The classifier evolves from scratch, incorporates new classes on the fly, and performs local incremental feature weighting. As an application, we focus on the classification of <b>emotion-related</b> <b>patterns</b> within electroencephalogram (EEG) signals. <b>Emotion</b> <b>recognition</b> is crucial for enhancing the realism and interactivity of computer systems. We extract features from the Fourier spectrum of EEG signals obtained from 28 individuals engaged in playing computer games &ndash; a public dataset. Each game elicits a different predominant <b>emotion:</b> <b>boredom,</b> calmness, horror, or joy. We analyze individual electrodes, time window lengths, and frequency bands to assess the accuracy and interpretability of resulting user-independent neural models. The findings indicate that both brain hemispheres assist classification, especially electrodes on the temporal (T8) and parietal (P7) areas, alongside contributions from frontal and occipital electrodes. While patterns may manifest in any band, the Alpha (8-13Hz), Delta (1-4Hz), and Theta (4-8Hz) bands, in this order, exhibited higher correspondence with the <b>emotion</b> <b>classes.</b> The eGNN-C+ demonstrates effectiveness in learning EEG data. It achieves an accuracy of 81.7% and a 0.0029 II interpretability using 10-second time windows, even in face of a highly-stochastic time-varying 4-class classification problem.</p></p class="citation"></blockquote><h2 id=csne-3>cs.NE (3)</h2><h3 id=13--267279-single-neuromorphic-memristor-closely-emulates-multiple-synaptic-mechanisms-for-energy-efficient-neural-networks-christoph-weilenmann-et-al-2024>(1/3 | 267/279) Single Neuromorphic Memristor closely Emulates Multiple Synaptic Mechanisms for Energy Efficient Neural Networks (Christoph Weilenmann et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Christoph Weilenmann, Alexandros Ziogas, Till Zellweger, Kevin Portner, Marko Mladenović, Manasa Kaniselvan, Timoleon Moraitis, Mathieu Luisier, Alexandros Emboras. (2024)<br><strong>Single Neuromorphic Memristor closely Emulates Multiple Synaptic Mechanisms for Energy Efficient Neural Networks</strong><br><button class=copy-to-clipboard title="Single Neuromorphic Memristor closely Emulates Multiple Synaptic Mechanisms for Energy Efficient Neural Networks" index=267>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-267 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NE<br>Categories: cond-mat-mes-hall, cs-NE, cs.NE<br>Keyword Score: 20<br>Keywords: Meta Learning, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16628v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16628v1.pdf filename=2402.16628v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Biological neural networks do not only include long-term memory and weight multiplication capabilities, as commonly assumed in artificial neural networks, but also more complex functions such as short-term memory, short-term plasticity, and <b>meta-plasticity</b> <b>-</b> all collocated within each synapse. Here, we demonstrate memristive nano-devices based on SrTiO3 that inherently emulate all these synaptic functions. These memristors operate in a non-filamentary, low conductance regime, which enables stable and energy efficient operation. They can act as multi-functional hardware synapses in a class of bio-inspired deep neural networks (DNN) that make use of both long- and short-term synaptic dynamics and are capable of <b>meta-learning</b> <b>or</b> &ldquo;learning-to-learn&rdquo;. The resulting bio-inspired DNN is then trained to play the video game Atari Pong, a complex <b>reinforcement</b> <b>learning</b> task in a dynamic environment. Our analysis shows that the energy consumption of the DNN with multi-functional memristive synapses decreases by about two orders of magnitude as compared to a pure GPU implementation. Based on this finding, we infer that memristive devices with a better emulation of the synaptic functionalities do not only broaden the applicability of neuromorphic computing, but could also improve the performance and energy costs of certain artificial intelligence applications.</p></p class="citation"></blockquote><h3 id=23--268279-exploratory-landscape-analysis-for-mixed-variable-problems-raphael-patrick-prager-et-al-2024>(2/3 | 268/279) Exploratory Landscape Analysis for Mixed-Variable Problems (Raphael Patrick Prager et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Raphael Patrick Prager, Heike Trautmann. (2024)<br><strong>Exploratory Landscape Analysis for Mixed-Variable Problems</strong><br><button class=copy-to-clipboard title="Exploratory Landscape Analysis for Mixed-Variable Problems" index=268>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-268 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NE<br>Categories: cs-NE, cs.NE<br>Keyword Score: 6<br>Keywords: Benchmarking, Clustering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16467v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16467v1.pdf filename=2402.16467v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Exploratory landscape analysis and fitness landscape analysis in general have been pivotal in facilitating problem understanding, algorithm design and endeavors such as automated algorithm selection and configuration. These techniques have largely been limited to search spaces of a single domain. In this work, we provide the means to compute exploratory landscape features for mixed-variable problems where the decision space is a mixture of continuous, binary, integer, and categorical variables. This is achieved by utilizing existing encoding techniques originating from machine learning. We provide a comprehensive juxtaposition of the results based on these different techniques. To further highlight their merit for practical applications, we design and conduct an automated algorithm selection study based on a hyperparameter optimization <b>benchmark</b> suite. We derive a meaningful compartmentalization of these <b>benchmark</b> problems by <b>clustering</b> based on the used landscape features. The identified clusters mimic the behavior the used algorithms exhibit. Meaning, the different clusters have different best performing algorithms. Finally, our trained algorithm selector is able to close the gap between the single best and the virtual best solver by 57.5% over all <b>benchmark</b> problems.</p></p class="citation"></blockquote><h3 id=33--269279-performance-comparison-of-surrogate-assisted-evolutionary-algorithms-on-computational-fluid-dynamics-problems-jakub-kudela-et-al-2024>(3/3 | 269/279) Performance Comparison of Surrogate-Assisted Evolutionary Algorithms on Computational Fluid Dynamics Problems (Jakub Kudela et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jakub Kudela, Ladislav Dobrovsky. (2024)<br><strong>Performance Comparison of Surrogate-Assisted Evolutionary Algorithms on Computational Fluid Dynamics Problems</strong><br><button class=copy-to-clipboard title="Performance Comparison of Surrogate-Assisted Evolutionary Algorithms on Computational Fluid Dynamics Problems" index=269>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-269 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NE<br>Categories: cs-AI, cs-NE, cs.NE, math-OC<br>Keyword Score: 6<br>Keywords: Benchmarking, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16455v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16455v1.pdf filename=2402.16455v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Surrogate-assisted evolutionary algorithms (SAEAs) are recently among the most widely studied methods for their capability to solve expensive real-world optimization problems. However, the development of new methods and <b>benchmarking</b> with other techniques still relies almost exclusively on artificially created problems. In this paper, we use two real-world computational fluid dynamics problems to compare the performance of eleven state-of-the-art single-objective SAEAs. We analyze the performance by investigating the quality and robustness of the obtained solutions and the convergence properties of the selected methods. Our findings suggest that the more recently published methods, as well as the techniques that utilize differential evolution as one of their optimization mechanisms, perform significantly better than the other considered methods.</p></p class="citation"></blockquote><h2 id=cssi-1>cs.SI (1)</h2><h3 id=11--270279-two-stage-information-spreading-evolution-on-the-control-role-of-announcements-jinhu-ren-et-al-2024>(1/1 | 270/279) Two-stage Information Spreading Evolution on The Control Role of Announcements (Jinhu Ren et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jinhu Ren, Fuzhong Nian, Xiaochen Yang. (2024)<br><strong>Two-stage Information Spreading Evolution on The Control Role of Announcements</strong><br><button class=copy-to-clipboard title="Two-stage Information Spreading Evolution on The Control Role of Announcements" index=270>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-270 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SI<br>Categories: cs-SI, cs.SI<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16416v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16416v1.pdf filename=2402.16416v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Modern social media networks have become an important platform for information competition among countries, regions, companies and other parties. This paper utilizes the research method of spread dynamics to investigate the influence of the control role of announcements in social networks on the spreading process. This paper distinguishes two spreading phases using the authentication intervention as a boundary: the unconfirmed spreading phase and the confirmed spreading phase. Based on the actual rules of spreading in online social networks, two kinds of verification results are defined: true information and false information. The Two-stage information spreading dynamics model is developed to analyze the changes in spreading effects due to different validation results. The impact of the intervention time on the overall spread process is analyzed by combining important control factors such as response cost and time-sensitivity. The validity of the model is verified by comparing the model <b>simulation</b> results with real cases and the adaptive capacity experiments. This work is analyzed and visualized from multiple perspectives, providing more quantitative results. The research content will provide a scientific basis for the intervention behavior of information management control by relevant departments or authorities.</p></p class="citation"></blockquote><h2 id=cond-matsupr-con-1>cond-mat.supr-con (1)</h2><h3 id=11--271279-scalable-superconductor-neuron-with-ternary-synaptic-connections-for-ultra-fast-snn-hardware-mustafa-altay-karamuftuoglu-et-al-2024>(1/1 | 271/279) Scalable Superconductor Neuron with Ternary Synaptic Connections for Ultra-Fast SNN Hardware (Mustafa Altay Karamuftuoglu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mustafa Altay Karamuftuoglu, Beyza Zeynep Ucpinar, Arash Fayyazi, Sasan Razmkhah, Mehdi Kamal, Massoud Pedram. (2024)<br><strong>Scalable Superconductor Neuron with Ternary Synaptic Connections for Ultra-Fast SNN Hardware</strong><br><button class=copy-to-clipboard title="Scalable Superconductor Neuron with Ternary Synaptic Connections for Ultra-Fast SNN Hardware" index=271>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-271 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cond-mat.supr-con<br>Categories: cond-mat-supr-con, cond-mat.supr-con, cs-ET, cs-NE<br>Keyword Score: 20<br>Keywords: MNIST, Pruning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16384v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16384v2.pdf filename=2402.16384v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A novel high-fan-in differential superconductor neuron structure designed for ultra-high-performance Spiking Neural Network (SNN) accelerators is presented. Utilizing a high-fan-in neuron structure allows us to design SNN accelerators with more synaptic connections, enhancing the overall network capabilities. The proposed neuron design is based on superconductor electronics fabric, incorporating multiple superconducting loops, each with two Josephson Junctions. This arrangement enables each input data branch to have positive and negative inductive coupling, supporting excitatory and inhibitory synaptic data. Compatibility with synaptic devices and thresholding operation is achieved using a single flux quantum (SFQ) pulse-based logic style. The neuron design, along with ternary synaptic connections, forms the foundation for a superconductor-based SNN inference. To demonstrate the capabilities of our design, we train the SNN using snnTorch, augmenting the PyTorch framework. After <b>pruning,</b> the demonstrated SNN inference achieves an impressive 96.1% accuracy on <b>MNIST</b> images. Notably, the network exhibits a remarkable throughput of 8.92 GHz while consuming only 1.5 nJ per inference, including the energy consumption associated with cooling to 4K. These results underscore the potential of superconductor electronics in developing high-performance and ultra-energy-efficient neural network accelerator architectures.</p></p class="citation"></blockquote><h2 id=csni-1>cs.NI (1)</h2><h3 id=11--272279-autonomous-integration-of-tsn-unaware-applications-with-qos-requirements-in-tsn-networks-moritz-fluechter-et-al-2024>(1/1 | 272/279) Autonomous Integration of TSN-unaware Applications with QoS Requirements in TSN Networks (Moritz Fluechter et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Moritz Fluechter, Steffen Lindner, Lukas Osswald, Jérôme Arnaud, Michael Menth. (2024)<br><strong>Autonomous Integration of TSN-unaware Applications with QoS Requirements in TSN Networks</strong><br><button class=copy-to-clipboard title="Autonomous Integration of TSN-unaware Applications with QoS Requirements in TSN Networks" index=272>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-272 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NI<br>Categories: cs-NI, cs.NI<br>Keyword Score: 10<br>Keywords: Recurrent Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16454v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16454v1.pdf filename=2402.16454v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Modern industrial networks transport both best-effort and real-time traffic. Time-Sensitive Networking (TSN) was introduced by the IEEE TSN Task Group as an enhancement to Ethernet to provide high quality of service (QoS) for real-time traffic. In a TSN network, applications signal their QoS requirements to the network before transmitting data. The network then allocates resources to meet these requirements. However, TSN-unaware applications can neither perform this registration process nor profit from TSN&rsquo;s QoS benefits. The contributions of this paper are twofold. First, we introduce a novel network architecture in which an additional device autonomously signals the QoS requirements of TSN-unaware applications to the network. Second, we propose a processing method to detect real-time streams in a network and extract the necessary information for the TSN stream signaling. It leverages a Deep <b>Recurrent</b> <b>Neural</b> <b>Network</b> (DRNN) to detect periodic traffic, extracts an accurate traffic description, and uses traffic classification to determine the source application. As a result, our proposal allows TSN-unaware applications to benefit from TSNs QoS guarantees. Our evaluations underline the effectiveness of the proposed architecture and processing method.</p></p class="citation"></blockquote><h2 id=mathoc-1>math.OC (1)</h2><h3 id=11--273279-event-triggered-parameterized-control-of-nonlinear-systems-anusree-rajan-et-al-2024>(1/1 | 273/279) Event-Triggered Parameterized Control of Nonlinear Systems (Anusree Rajan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Anusree Rajan, Pavankumar Tallapragada. (2024)<br><strong>Event-Triggered Parameterized Control of Nonlinear Systems</strong><br><button class=copy-to-clipboard title="Event-Triggered Parameterized Control of Nonlinear Systems" index=273>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-273 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.OC<br>Categories: cs-SY, eess-SY, math-OC, math.OC<br>Keyword Score: 10<br>Keywords: Continuous Time, Continuous Time<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16337v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16337v1.pdf filename=2402.16337v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper deals with event-triggered parameterized control (ETPC) of nonlinear systems with external disturbances. In this control method, between two successive events, each control input to the plant is a linear combination of a set of linearly independent scalar functions. At each event, the controller updates the coefficients of the parameterized control input so as to minimize the error in approximating a <b>continuous</b> <b>time</b> control signal and communicates the same to the actuator. We design an event-triggering rule that guarantees global uniform ultimate boundedness of trajectories of the closed loop system. We also ensure the absence of Zeno behavior by showing the existence of a uniform positive lower bound on the inter-event times. We illustrate our results through numerical examples, which indicate that the proposed control method leads to a significant improvement in average inter-event time and minimum inter-event time compared to the event-triggered zero-order-hold control.</p></p class="citation"></blockquote><h2 id=q-bionc-1>q-bio.NC (1)</h2><h3 id=11--274279-neural-population-geometry-and-optimal-coding-of-tasks-with-shared-latent-structure-albert-j-wakhloo-et-al-2024>(1/1 | 274/279) Neural Population Geometry and Optimal Coding of Tasks with Shared Latent Structure (Albert J. Wakhloo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Albert J. Wakhloo, Will Slatton, SueYeon Chung. (2024)<br><strong>Neural Population Geometry and Optimal Coding of Tasks with Shared Latent Structure</strong><br><button class=copy-to-clipboard title="Neural Population Geometry and Optimal Coding of Tasks with Shared Latent Structure" index=274>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-274 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: q-bio.NC<br>Categories: cond-mat-dis-nn, cond-mat-stat-mech, cs-LG, cs-NE, q-bio-NC, q-bio.NC<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16770v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16770v1.pdf filename=2402.16770v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Humans and animals can recognize latent structures in their environment and apply this information to efficiently navigate the world. Several recent works argue that the brain supports these abilities by forming neural representations that encode such latent structures in flexible, generalizable ways. However, it remains unclear what aspects of neural population activity are contributing to these computational capabilities. Here, we develop an analytical theory linking the mesoscopic statistics of a neural population&rsquo;s activity to generalization performance on a multi-task learning problem. To do this, we rely on a generative model in which different tasks depend on a common, unobserved latent structure and predictions are formed from a linear readout of a neural population&rsquo;s activity. We show that three geometric measures of the population activity determine generalization performance in these settings. Using this theory, we find that experimentally observed factorized (or disentangled) representations naturally emerge as an optimal solution to the multi-task learning problem. We go on to show that when data is scarce, optimal codes compress less informative latent variables, and when data is abundant, optimal codes expand this information in the state space. We validate predictions from our theory using biological and artificial neural network data. Our results therefore tie neural population <b>geometry</b> to the multi-task learning problem and make normative predictions of the structure of population activity in these settings.</p></p class="citation"></blockquote><h2 id=csfl-2>cs.FL (2)</h2><h3 id=12--275279-tree-verifiable-graph-grammars-mark-chimes-et-al-2024>(1/2 | 275/279) Tree-Verifiable Graph Grammars (Mark Chimes et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mark Chimes, Radu Iosif, Florian Zuleger. (2024)<br><strong>Tree-Verifiable Graph Grammars</strong><br><button class=copy-to-clipboard title="Tree-Verifiable Graph Grammars" index=275>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-275 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.FL<br>Categories: cs-FL, cs-LO, cs.FL<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17015v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17015v2.pdf filename=2402.17015v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Hyperedge-Replacement grammars (HR) have been introduced by Courcelle in order to extend the notion of context-free sets from words and trees to <b>graphs</b> of bounded tree-width. While for words and trees the syntactic restrictions that guarantee that the associated languages of words resp. trees are regular - and hence, MSO-definable - are known, the situation is far more complicated for <b>graphs.</b> Here, Courcelle proposed the notion of regular <b>graph</b> grammars, a syntactic restriction of HR grammars that guarantees the definability of the associated languages of <b>graphs</b> in Counting Monadic Second Order Logic (CMSO). However, these grammars are not complete in the sense that not every CMSO-definable set of <b>graphs</b> of bounded tree-width can be generated by a regular <b>graph</b> grammar. In this paper, we introduce a new syntactic restriction of HR grammars, called tree-verifiable <b>graph</b> grammars, and a new notion of bounded tree-width, called embeddable bounded tree-width, where the later restricts the trees of a tree-decomposition to be a subgraph of the analyzed <b>graph.</b> The main property of tree-verifiable <b>graph</b> grammars is that their associated languages are CMSO-definable and that the have bounded embeddable tree-width. We show further that they strictly generalize the regular <b>graph</b> grammars of Courcelle. Finally, we establish a completeness result, showing that every language of <b>graphs</b> that is CMSO-definable and of bounded embeddable tree-width can be generated by a tree-verifiable <b>graph</b> grammar.</p></p class="citation"></blockquote><h3 id=22--276279-on-the-complexity-of-initial-and-final-state-opacity-for-discrete-event-systems-tomáš-masopust-et-al-2024>(2/2 | 276/279) On the Complexity of Initial-and-Final-State Opacity for Discrete Event Systems (Tomáš Masopust et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tomáš Masopust, Petr Osička. (2024)<br><strong>On the Complexity of Initial-and-Final-State Opacity for Discrete Event Systems</strong><br><button class=copy-to-clipboard title="On the Complexity of Initial-and-Final-State Opacity for Discrete Event Systems" index=276>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-276 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.FL<br>Categories: cs-FL, cs-SY, cs.FL, eess-SY<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17000v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17000v1.pdf filename=2402.17000v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Opacity is a general framework modeling security properties of systems interacting with a passive attacker by asserting that a part of the systems behaviour remains secret. In initial-and-final-state opacity (IFO, for short) the secret is whether the system evolved from a given initial state to a given final state or not. Two algorithms for IFO verification are discussed in the literature. One algorithm arises from a trellis-based state estimator, which builds a semigroup of binary relations generated by the events of the automaton, and the other is based on the reduction to language inclusion. The worst-case time complexity of both algorithms is bounded by a super-exponential function. We show that the super-exponential time complexity is tight for both algorithms; however, we leave open whether there is an algorithm with a lower time complexity. Finally, we use extensive <b>benchmarks</b> based on real data to experimentally compare the existing algorithms.</p></p class="citation"></blockquote><h2 id=physicsoptics-1>physics.optics (1)</h2><h3 id=11--277279-photonic-neural-network-fabricated-on-thin-film-lithium-niobate-for-high-fidelity-and-power-efficient-matrix-computation-yong-zheng-et-al-2024>(1/1 | 277/279) Photonic Neural Network Fabricated on Thin Film Lithium Niobate for High-Fidelity and Power-Efficient Matrix Computation (Yong Zheng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yong Zheng, Rongbo Wu, Yuan Ren, Rui Bao, Jian Liu, Yu Ma, Min Wang, Ya Cheng. (2024)<br><strong>Photonic Neural Network Fabricated on Thin Film Lithium Niobate for High-Fidelity and Power-Efficient Matrix Computation</strong><br><button class=copy-to-clipboard title="Photonic Neural Network Fabricated on Thin Film Lithium Niobate for High-Fidelity and Power-Efficient Matrix Computation" index=277>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-277 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: physics.optics<br>Categories: cs-ET, physics-app-ph, physics-optics, physics.optics<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16513v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16513v1.pdf filename=2402.16513v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Photonic neural networks (PNNs) have emerged as a promising platform to address the energy consumption issue that comes with the advancement of artificial intelligence technology, and thin film lithium niobate (TFLN) offers an attractive solution as a material platform mainly for its combined characteristics of low optical loss and large electro-optic (EO) coefficients. Here, we present the first implementation of an EO tunable PNN based on the TFLN platform. Our device features ultra-high fidelity, high computation speed, and exceptional power efficiency. We <b>benchmark</b> the performance of our device with several deep learning missions including in-situ training of Circle and Moons nonlinear datasets classification, Iris flower species recognition, and handwriting digits recognition. Our work paves the way for sustainable up-scaling of high-speed, energy-efficient PNNs.</p></p class="citation"></blockquote><h2 id=csdm-1>cs.DM (1)</h2><h3 id=11--278279-a-note-on-solving-basic-equations-over-the-semiring-of-functional-digraphs-alberto-dennunzio-et-al-2024>(1/1 | 278/279) A note on solving basic equations over the semiring of functional digraphs (Alberto Dennunzio et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Alberto Dennunzio, Enrico Formenti, Luciano Margara, Sara Riva. (2024)<br><strong>A note on solving basic equations over the semiring of functional digraphs</strong><br><button class=copy-to-clipboard title="A note on solving basic equations over the semiring of functional digraphs" index=278>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-278 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DM<br>Categories: 68R01, 68R010, cs-DM, cs.DM, math-CO<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16923v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16923v1.pdf filename=2402.16923v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Endowing the set of functional <b>graphs</b> (FGs) with the sum (disjoint union of <b>graphs)</b> and product (standard direct product on <b>graphs)</b> operations induces on FGs a structure of a commutative semiring $\ring$. The operations on $\ring$ can be naturally extended to the set of univariate polynomials $\ring[X]$ over $\ring$. This paper provides a polynomial time algorithm for deciding if equations of the type $AX=B$ have solutions when $A$ is just a single cycle and $B$ a set of cycles of identical size. We also prove a similar complexity result for some variants of the previous equation.</p></p class="citation"></blockquote><h2 id=cslo-1>cs.LO (1)</h2><h3 id=11--279279-equational-bit-vector-solving-via-strong-gröbner-bases-jiaxin-song-et-al-2024>(1/1 | 279/279) Equational Bit-Vector Solving via Strong Gröbner Bases (Jiaxin Song et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiaxin Song, Hongfei Fu, Charles Zhang. (2024)<br><strong>Equational Bit-Vector Solving via Strong Gröbner Bases</strong><br><button class=copy-to-clipboard title="Equational Bit-Vector Solving via Strong Gröbner Bases" index=279>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-279 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LO<br>Categories: cs-LO, cs-PL, cs.LO<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16314v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16314v1.pdf filename=2402.16314v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Bit-vectors, which are integers in a finite number of bits, are ubiquitous in software and hardware systems. In this work, we consider the satisfiability modulo theories (SMT) of bit-vectors. Unlike normal integers, the arithmetics of bit-vectors are modular upon integer overflow. Therefore, the SMT solving of bit-vectors needs to resolve the underlying modular arithmetics. In the literature, two prominent approaches for SMT solving are bit-blasting (that transforms the SMT problem into boolean satisfiability) and integer solving (that transforms the SMT problem into integer properties). Both approaches ignore the algebraic properties of the modular arithmetics and hence could not utilize these properties to improve the efficiency of SMT solving. In this work, we consider the equational theory of bit-vectors and capture the algebraic properties behind them via strong Gr"obner bases. First, we apply strong Gr"obner bases to the quantifier-free equational theory of bit-vectors and propose a novel algorithmic improvement in the key computation of multiplicative inverse modulo a power of two. Second, we resolve the important case of invariant generation in quantified equational bit-vector properties via strong Gr"obner bases and linear congruence solving. Experimental results over an extensive range of <b>benchmarks</b> show that our approach outperforms existing methods in both time efficiency and memory consumption.</p></p class="citation"></blockquote></div><div class="row pl-3 pr-3"><div class="col-md-6 share-buttons"></div></div><hr><div class="row next-prev-navigator"><div class="col-md-6 previous-article"><a href=/akitenkrad-blog/posts/arxiv/202402/20240227000000/ title="arXiv @ 2024.02.27" class="btn btn-outline-info"><div><i class="fas fa-chevron-circle-left"></i> Prev</div><div class=next-prev-text>arXiv @ 2024.02.27</div></a></div><div class="col-md-6 next-article"><a href=/akitenkrad-blog/posts/arxiv/202402/20240229000000/ title="arXiv @ 2024.02.29" class="btn btn-outline-info"><div>Next <i class="fas fa-chevron-circle-right"></i></div><div class=next-prev-text>arXiv @ 2024.02.29</div></a></div></div><hr></div></div></div><a id=scroll-to-top class=btn><i class="fas fa-chevron-circle-up"></i></a></section><section class=toc-section id=toc-section><div class=toc-holder><h5 class="text-center pl-3">Table of Contents</h5><hr><div class=toc><nav id=TableOfContents><ul><li><a href=#primary-categories>Primary Categories</a></li><li><a href=#keywords>Keywords</a></li><li><a href=#csir-11>cs.IR (11)</a><ul><li><a href=#111--1279-integrating-large-language-models-with-graphical-session-based-recommendation-naicheng-guo-et-al-2024>(1/11 | 1/279) Integrating Large Language Models with Graphical Session-Based Recommendation (Naicheng Guo et al., 2024)</a></li><li><a href=#211--2279-a-fine-tuning-enhanced-rag-system-with-quantized-influence-measure-as-ai-judge-keshav-rangan-et-al-2024>(2/11 | 2/279) A Fine-tuning Enhanced RAG System with Quantized Influence Measure as AI Judge (Keshav Rangan et al., 2024)</a></li><li><a href=#311--3279-high-frequency-aware-hierarchical-contrastive-selective-coding-for-representation-learning-on-text-attributed-graphs-peiyan-zhang-et-al-2024>(3/11 | 3/279) High-Frequency-aware Hierarchical Contrastive Selective Coding for Representation Learning on Text-attributed Graphs (Peiyan Zhang et al., 2024)</a></li><li><a href=#411--4279-llm-assisted-multi-teacher-continual-learning-for-visual-question-answering-in-robotic-surgery-kexin-chen-et-al-2024>(4/11 | 4/279) LLM-Assisted Multi-Teacher Continual Learning for Visual Question Answering in Robotic Surgery (Kexin Chen et al., 2024)</a></li><li><a href=#511--5279-deep-rating-elicitation-for-new-users-in-collaborative-filtering-wonbin-kweon-et-al-2024>(5/11 | 5/279) Deep Rating Elicitation for New Users in Collaborative Filtering (Wonbin Kweon et al., 2024)</a></li><li><a href=#611--6279-confidence-calibration-for-recommender-systems-and-its-applications-wonbin-kweon-2024>(6/11 | 6/279) Confidence Calibration for Recommender Systems and Its Applications (Wonbin Kweon, 2024)</a></li><li><a href=#711--7279-against-filter-bubbles-diversified-music-recommendation-via-weighted-hypergraph-embedding-learning-chaoguang-luo-et-al-2024>(7/11 | 7/279) Against Filter Bubbles: Diversified Music Recommendation via Weighted Hypergraph Embedding Learning (Chaoguang Luo et al., 2024)</a></li><li><a href=#811--8279-boxrec-recommending-a-box-of-preferred-outfits-in-online-shopping-debopriyo-banerjee-et-al-2024>(8/11 | 8/279) BOXREC: Recommending a Box of Preferred Outfits in Online Shopping (Debopriyo Banerjee et al., 2024)</a></li><li><a href=#911--9279-retrouver-linventeur-auteur--la-levée-dhomonymies-dautorat-entre-les-brevets-et-les-publications-scientifiques-david-reymond-et-al-2024>(9/11 | 9/279) Retrouver l&rsquo;inventeur-auteur : la lev{é}e d&rsquo;homonymies d&rsquo;autorat entre les brevets et les publications scientifiques (David Reymond et al., 2024)</a></li><li><a href=#1011--10279-top-personalized-k-recommendation-wonbin-kweon-et-al-2024>(10/11 | 10/279) Top-Personalized-K Recommendation (Wonbin Kweon et al., 2024)</a></li><li><a href=#1111--11279-corpusbrain-a-continual-generative-pre-training-framework-for-knowledge-intensive-language-tasks-jiafeng-guo-et-al-2024>(11/11 | 11/279) CorpusBrain++: A Continual Generative Pre-Training Framework for Knowledge-Intensive Language Tasks (Jiafeng Guo et al., 2024)</a></li></ul></li><li><a href=#cscr-8>cs.CR (8)</a><ul><li><a href=#18--12279-deep-learning-algorithms-used-in-intrusion-detection-systems----a-review-richard-kimanzi-et-al-2024>(1/8 | 12/279) Deep Learning Algorithms Used in Intrusion Detection Systems &ndash; A Review (Richard Kimanzi et al., 2024)</a></li><li><a href=#28--13279-pandoras-white-box-increased-training-data-leakage-in-open-llms-jeffrey-g-wang-et-al-2024>(2/8 | 13/279) Pandora&rsquo;s White-Box: Increased Training Data Leakage in Open LLMs (Jeffrey G. Wang et al., 2024)</a></li><li><a href=#38--14279-wipi-a-new-web-threat-for-llm-driven-web-agents-fangzhou-wu-et-al-2024>(3/8 | 14/279) WIPI: A New Web Threat for LLM-Driven Web Agents (Fangzhou Wu et al., 2024)</a></li><li><a href=#48--15279-synthesizing-tight-privacy-and-accuracy-bounds-via-weighted-model-counting-lisa-oakley-et-al-2024>(4/8 | 15/279) Synthesizing Tight Privacy and Accuracy Bounds via Weighted Model Counting (Lisa Oakley et al., 2024)</a></li><li><a href=#58--16279-improving-behavior-based-authentication-against-adversarial-attack-using-xai-dong-qin-et-al-2024>(5/8 | 16/279) Improving behavior based authentication against adversarial attack using XAI (Dong Qin et al., 2024)</a></li><li><a href=#68--17279-a-survey-of-large-language-models-in-cybersecurity-gabriel-de-jesus-coelho-da-silva-et-al-2024>(6/8 | 17/279) A Survey of Large Language Models in Cybersecurity (Gabriel de Jesus Coelho da Silva et al., 2024)</a></li><li><a href=#78--18279-decentralized-federated-unlearning-on-blockchain-xiao-liu-et-al-2024>(7/8 | 18/279) Decentralized Federated Unlearning on Blockchain (Xiao Liu et al., 2024)</a></li><li><a href=#88--19279-on-the-infeasibility-of-ml-backdoor-detection-as-an-hypothesis-testing-problem-georg-pichler-et-al-2024>(8/8 | 19/279) On the (In)feasibility of ML Backdoor Detection as an Hypothesis Testing Problem (Georg Pichler et al., 2024)</a></li></ul></li><li><a href=#cscl-62>cs.CL (62)</a><ul><li><a href=#162--20279-codes-towards-building-open-source-language-models-for-text-to-sql-haoyang-li-et-al-2024>(1/62 | 20/279) CodeS: Towards Building Open-source Language Models for Text-to-SQL (Haoyang Li et al., 2024)</a></li><li><a href=#262--21279-rocoins-enhancing-robustness-of-large-language-models-through-code-style-instructions-yuansen-zhang-et-al-2024>(2/62 | 21/279) RoCoIns: Enhancing Robustness of Large Language Models through Code-Style Instructions (Yuansen Zhang et al., 2024)</a></li><li><a href=#362--22279-two-stage-generative-question-answering-on-temporal-knowledge-graph-using-large-language-models-yifu-gao-et-al-2024>(3/62 | 22/279) Two-stage Generative Question Answering on Temporal Knowledge Graph Using Large Language Models (Yifu Gao et al., 2024)</a></li><li><a href=#462--23279-benchmarking-llms-on-the-semantic-overlap-summarization-task-john-salvador-et-al-2024>(4/62 | 23/279) Benchmarking LLMs on the Semantic Overlap Summarization Task (John Salvador et al., 2024)</a></li><li><a href=#562--24279-look-before-you-leap-towards-decision-aware-and-generalizable-tool-usage-for-large-language-models-anchun-gui-et-al-2024>(5/62 | 24/279) Look Before You Leap: Towards Decision-Aware and Generalizable Tool-Usage for Large Language Models (Anchun Gui et al., 2024)</a></li><li><a href=#662--25279-esg-sentiment-analysis-comparing-human-and-language-model-performance-including-gpt-karim-derrick-2024>(6/62 | 25/279) ESG Sentiment Analysis: comparing human and language model performance including GPT (Karim Derrick, 2024)</a></li><li><a href=#762--26279-long-context-language-modeling-with-parallel-context-encoding-howard-yen-et-al-2024>(7/62 | 26/279) Long-Context Language Modeling with Parallel Context Encoding (Howard Yen et al., 2024)</a></li><li><a href=#862--27279-medit-multilingual-text-editing-via-instruction-tuning-vipul-raheja-et-al-2024>(8/62 | 27/279) mEdIT: Multilingual Text Editing via Instruction Tuning (Vipul Raheja et al., 2024)</a></li><li><a href=#962--28279-retrievalqa-assessing-adaptive-retrieval-augmented-generation-for-short-form-open-domain-question-answering-zihan-zhang-et-al-2024>(9/62 | 28/279) RetrievalQA: Assessing Adaptive Retrieval-Augmented Generation for Short-form Open-Domain Question Answering (Zihan Zhang et al., 2024)</a></li><li><a href=#1062--29279-mozip-a-multilingual-benchmark-to-evaluate-large-language-models-in-intellectual-property-shiwen-ni-et-al-2024>(10/62 | 29/279) MoZIP: A Multilingual Benchmark to Evaluate Large Language Models in Intellectual Property (Shiwen Ni et al., 2024)</a></li><li><a href=#1162--30279-perltqa-a-personal-long-term-memory-dataset-for-memory-classification-retrieval-and-synthesis-in-question-answering-yiming-du-et-al-2024>(11/62 | 30/279) PerLTQA: A Personal Long-Term Memory Dataset for Memory Classification, Retrieval, and Synthesis in Question Answering (Yiming Du et al., 2024)</a></li><li><a href=#1262--31279-rethinking-negative-instances-for-generative-named-entity-recognition-yuyang-ding-et-al-2024>(12/62 | 31/279) Rethinking Negative Instances for Generative Named Entity Recognition (Yuyang Ding et al., 2024)</a></li><li><a href=#1362--32279-llm-based-privacy-data-augmentation-guided-by-knowledge-distillation-with-a-distribution-tutor-for-medical-text-classification-yiping-song-et-al-2024>(13/62 | 32/279) LLM-based Privacy Data Augmentation Guided by Knowledge Distillation with a Distribution Tutor for Medical Text Classification (Yiping Song et al., 2024)</a></li><li><a href=#1462--33279-pre-training-cross-lingual-open-domain-question-answering-with-large-scale-synthetic-supervision-fan-jiang-et-al-2024>(14/62 | 33/279) Pre-training Cross-lingual Open Domain Question Answering with Large-scale Synthetic Supervision (Fan Jiang et al., 2024)</a></li><li><a href=#1562--34279-from-rags-to-riches-using-large-language-models-to-write-documents-for-clinical-trials-nigel-markey-et-al-2024>(15/62 | 34/279) From RAGs to riches: Using large language models to write documents for clinical trials (Nigel Markey et al., 2024)</a></li><li><a href=#1662--35279-improving-llm-based-machine-translation-with-systematic-self-correction-zhaopeng-feng-et-al-2024>(16/62 | 35/279) Improving LLM-based Machine Translation with Systematic Self-Correction (Zhaopeng Feng et al., 2024)</a></li><li><a href=#1762--36279-llm-inference-unveiled-survey-and-roofline-model-insights-zhihang-yuan-et-al-2024>(17/62 | 36/279) LLM Inference Unveiled: Survey and Roofline Model Insights (Zhihang Yuan et al., 2024)</a></li><li><a href=#1862--37279-layer-wise-regularized-dropout-for-neural-language-models-shiwen-ni-et-al-2024>(18/62 | 37/279) Layer-wise Regularized Dropout for Neural Language Models (Shiwen Ni et al., 2024)</a></li><li><a href=#1962--38279-mathgenie-generating-synthetic-data-with-question-back-translation-for-enhancing-mathematical-reasoning-of-llms-zimu-lu-et-al-2024>(19/62 | 38/279) MathGenie: Generating Synthetic Data with Question Back-translation for Enhancing Mathematical Reasoning of LLMs (Zimu Lu et al., 2024)</a></li><li><a href=#2062--39279-mysterious-projections-multimodal-llms-gain-domain-specific-visual-capabilities-without-richer-cross-modal-projections-gaurav-verma-et-al-2024>(20/62 | 39/279) Mysterious Projections: Multimodal LLMs Gain Domain-Specific Visual Capabilities Without Richer Cross-Modal Projections (Gaurav Verma et al., 2024)</a></li><li><a href=#2162--40279-a-comprehensive-evaluation-of-quantization-strategies-for-large-language-models-renren-jin-et-al-2024>(21/62 | 40/279) A Comprehensive Evaluation of Quantization Strategies for Large Language Models (Renren Jin et al., 2024)</a></li><li><a href=#2262--41279-structlm-towards-building-generalist-models-for-structured-knowledge-grounding-alex-zhuang-et-al-2024>(22/62 | 41/279) StructLM: Towards Building Generalist Models for Structured Knowledge Grounding (Alex Zhuang et al., 2024)</a></li><li><a href=#2362--42279-aligning-large-language-models-to-a-domain-specific-graph-database-yuanyuan-liang-et-al-2024>(23/62 | 42/279) Aligning Large Language Models to a Domain-specific Graph Database (Yuanyuan Liang et al., 2024)</a></li><li><a href=#2462--43279-z-agi-labs-at-climateactivism-2024-stance-and-hate-event-detection-on-social-media-nikhil-narayan-et-al-2024>(24/62 | 43/279) Z-AGI Labs at ClimateActivism 2024: Stance and Hate Event Detection on Social Media (Nikhil Narayan et al., 2024)</a></li><li><a href=#2562--44279-rainbow-teaming-open-ended-generation-of-diverse-adversarial-prompts-mikayel-samvelyan-et-al-2024>(25/62 | 44/279) Rainbow Teaming: Open-Ended Generation of Diverse Adversarial Prompts (Mikayel Samvelyan et al., 2024)</a></li><li><a href=#2662--45279-investigating-the-effectiveness-of-hypertuning-via-gisting-jason-phang-2024>(26/62 | 45/279) Investigating the Effectiveness of HyperTuning via Gisting (Jason Phang, 2024)</a></li><li><a href=#2762--46279-oncogpt-a-medical-conversational-model-tailored-with-oncology-domain-expertise-on-a-large-language-model-meta-ai-llama-fujian-jia-et-al-2024>(27/62 | 46/279) OncoGPT: A Medical Conversational Model Tailored with Oncology Domain Expertise on a Large Language Model Meta-AI (LLaMA) (Fujian Jia et al., 2024)</a></li><li><a href=#2862--47279-codechameleon-personalized-encryption-framework-for-jailbreaking-large-language-models-huijie-lv-et-al-2024>(28/62 | 47/279) CodeChameleon: Personalized Encryption Framework for Jailbreaking Large Language Models (Huijie Lv et al., 2024)</a></li><li><a href=#2962--48279-selectit-selective-instruction-tuning-for-large-language-models-via-uncertainty-aware-self-reflection-liangxin-liu-et-al-2024>(29/62 | 48/279) SelectIT: Selective Instruction Tuning for Large Language Models via Uncertainty-Aware Self-Reflection (Liangxin Liu et al., 2024)</a></li><li><a href=#3062--49279-unveiling-vulnerability-of-self-attention-khai-jiet-liong-et-al-2024>(30/62 | 49/279) Unveiling Vulnerability of Self-Attention (Khai Jiet Liong et al., 2024)</a></li><li><a href=#3162--50279-language-specific-neurons-the-key-to-multilingual-capabilities-in-large-language-models-tianyi-tang-et-al-2024>(31/62 | 50/279) Language-Specific Neurons: The Key to Multilingual Capabilities in Large Language Models (Tianyi Tang et al., 2024)</a></li><li><a href=#3262--51279-data-freeweight-compress-and-denoise-for-large-language-models-runyu-peng-et-al-2024>(32/62 | 51/279) Data-freeWeight Compress and Denoise for Large Language Models (Runyu Peng et al., 2024)</a></li><li><a href=#3362--52279-generating-effective-ensembles-for-sentiment-analysis-itay-etelis-et-al-2024>(33/62 | 52/279) Generating Effective Ensembles for Sentiment Analysis (Itay Etelis et al., 2024)</a></li><li><a href=#3462--53279-humaneval-xl-a-multilingual-code-generation-benchmark-for-cross-lingual-natural-language-generalization-qiwei-peng-et-al-2024>(34/62 | 53/279) HumanEval-XL: A Multilingual Code Generation Benchmark for Cross-lingual Natural Language Generalization (Qiwei Peng et al., 2024)</a></li><li><a href=#3562--54279-mobillama-towards-accurate-and-lightweight-fully-transparent-gpt-omkar-thawakar-et-al-2024>(35/62 | 54/279) MobiLlama: Towards Accurate and Lightweight Fully Transparent GPT (Omkar Thawakar et al., 2024)</a></li><li><a href=#3662--55279-do-large-language-models-latently-perform-multi-hop-reasoning-sohee-yang-et-al-2024>(36/62 | 55/279) Do Large Language Models Latently Perform Multi-Hop Reasoning? (Sohee Yang et al., 2024)</a></li><li><a href=#3762--56279-set-the-clock-temporal-alignment-of-pretrained-language-models-bowen-zhao-et-al-2024>(37/62 | 56/279) Set the Clock: Temporal Alignment of Pretrained Language Models (Bowen Zhao et al., 2024)</a></li><li><a href=#3862--57279-political-compass-or-spinning-arrow-towards-more-meaningful-evaluations-for-values-and-opinions-in-large-language-models-paul-röttger-et-al-2024>(38/62 | 57/279) Political Compass or Spinning Arrow? Towards More Meaningful Evaluations for Values and Opinions in Large Language Models (Paul Röttger et al., 2024)</a></li><li><a href=#3962--58279-predicting-sustainable-development-goals-using-course-descriptions----from-llms-to-conventional-foundation-models-lev-kharlashkin-et-al-2024>(39/62 | 58/279) Predicting Sustainable Development Goals Using Course Descriptions &ndash; from LLMs to Conventional Foundation Models (Lev Kharlashkin et al., 2024)</a></li><li><a href=#4062--59279-immunization-against-harmful-fine-tuning-attacks-domenic-rosati-et-al-2024>(40/62 | 59/279) Immunization against harmful fine-tuning attacks (Domenic Rosati et al., 2024)</a></li><li><a href=#4162--60279-unraveling-babel-exploring-multilingual-activation-patterns-within-large-language-models-weize-liu-et-al-2024>(41/62 | 60/279) Unraveling Babel: Exploring Multilingual Activation Patterns within Large Language Models (Weize Liu et al., 2024)</a></li><li><a href=#4262--61279-chain-of-discussion-a-multi-model-framework-for-complex-evidence-based-question-answering-mingxu-tao-et-al-2024>(42/62 | 61/279) Chain-of-Discussion: A Multi-Model Framework for Complex Evidence-Based Question Answering (Mingxu Tao et al., 2024)</a></li><li><a href=#4362--62279-towards-explainability-and-fairness-in-swiss-judgement-prediction-benchmarking-on-a-multilingual-dataset-santosh-t-y-s-s-et-al-2024>(43/62 | 62/279) Towards Explainability and Fairness in Swiss Judgement Prediction: Benchmarking on a Multilingual Dataset (Santosh T. Y. S. S et al., 2024)</a></li><li><a href=#4462--63279-llmarena-assessing-capabilities-of-large-language-models-in-dynamic-multi-agent-environments-junzhe-chen-et-al-2024>(44/62 | 63/279) LLMArena: Assessing Capabilities of Large Language Models in Dynamic Multi-Agent Environments (Junzhe Chen et al., 2024)</a></li><li><a href=#4562--64279-can-large-language-models-recall-reference-location-like-humans-ye-wang-et-al-2024>(45/62 | 64/279) Can Large Language Models Recall Reference Location Like Humans? (Ye Wang et al., 2024)</a></li><li><a href=#4662--65279-eight-methods-to-evaluate-robust-unlearning-in-llms-aengus-lynch-et-al-2024>(46/62 | 65/279) Eight Methods to Evaluate Robust Unlearning in LLMs (Aengus Lynch et al., 2024)</a></li><li><a href=#4762--66279-adaptation-of-biomedical-and-clinical-pretrained-models-to-french-long-documents-a-comparative-study-adrien-bazoge-et-al-2024>(47/62 | 66/279) Adaptation of Biomedical and Clinical Pretrained Models to French Long Documents: A Comparative Study (Adrien Bazoge et al., 2024)</a></li><li><a href=#4862--67279-repoagent-an-llm-powered-open-source-framework-for-repository-level-code-documentation-generation-qinyu-luo-et-al-2024>(48/62 | 67/279) RepoAgent: An LLM-Powered Open-Source Framework for Repository-level Code Documentation Generation (Qinyu Luo et al., 2024)</a></li><li><a href=#4962--68279-defending-llms-against-jailbreaking-attacks-via-backtranslation-yihan-wang-et-al-2024>(49/62 | 68/279) Defending LLMs against Jailbreaking Attacks via Backtranslation (Yihan Wang et al., 2024)</a></li><li><a href=#5062--69279-id-xcb-data-independent-debiasing-for-fair-and-accurate-transformer-based-cyberbullying-detection-peiling-yi-et-al-2024>(50/62 | 69/279) ID-XCB: Data-independent Debiasing for Fair and Accurate Transformer-based Cyberbullying Detection (Peiling Yi et al., 2024)</a></li><li><a href=#5162--70279-multi-task-contrastive-learning-for-8192-token-bilingual-text-embeddings-isabelle-mohr-et-al-2024>(51/62 | 70/279) Multi-Task Contrastive Learning for 8192-Token Bilingual Text Embeddings (Isabelle Mohr et al., 2024)</a></li><li><a href=#5262--71279-where-do-we-go-from-here-multi-scale-allocentric-relational-inference-from-natural-spatial-descriptions-tzuf-paz-argaman-et-al-2024>(52/62 | 71/279) Where Do We Go from Here? Multi-scale Allocentric Relational Inference from Natural Spatial Descriptions (Tzuf Paz-Argaman et al., 2024)</a></li><li><a href=#5362--72279-leveraging-large-language-models-for-learning-complex-legal-concepts-through-storytelling-hang-jiang-et-al-2024>(53/62 | 72/279) Leveraging Large Language Models for Learning Complex Legal Concepts through Storytelling (Hang Jiang et al., 2024)</a></li><li><a href=#5462--73279-a-survey-on-data-selection-for-language-models-alon-albalak-et-al-2024>(54/62 | 73/279) A Survey on Data Selection for Language Models (Alon Albalak et al., 2024)</a></li><li><a href=#5562--74279-multi-bit-distortion-free-watermarking-for-large-language-models-massieh-kordi-boroujeny-et-al-2024>(55/62 | 74/279) Multi-Bit Distortion-Free Watermarking for Large Language Models (Massieh Kordi Boroujeny et al., 2024)</a></li><li><a href=#5662--75279-shieldlm-empowering-llms-as-aligned-customizable-and-explainable-safety-detectors-zhexin-zhang-et-al-2024>(56/62 | 75/279) ShieldLM: Empowering LLMs as Aligned, Customizable and Explainable Safety Detectors (Zhexin Zhang et al., 2024)</a></li><li><a href=#5762--76279-cross-domain-chinese-sentence-pattern-parsing-jingsi-yu-et-al-2024>(57/62 | 76/279) Cross-domain Chinese Sentence Pattern Parsing (Jingsi Yu et al., 2024)</a></li><li><a href=#5862--77279-topic-to-essay-generation-with-knowledge-based-content-selection-jieyong-wang-et-al-2024>(58/62 | 77/279) Topic-to-essay generation with knowledge-based content selection (Jieyong Wang et al., 2024)</a></li><li><a href=#5962--78279-long-dialog-summarization-an-analysis-ankan-mullick-et-al-2024>(59/62 | 78/279) Long Dialog Summarization: An Analysis (Ankan Mullick et al., 2024)</a></li><li><a href=#6062--79279-paqa-toward-proactive-open-retrieval-question-answering-pierre-erbacher-et-al-2024>(60/62 | 79/279) PAQA: Toward ProActive Open-Retrieval Question Answering (Pierre Erbacher et al., 2024)</a></li><li><a href=#6162--80279-uniretriever-multi-task-candidates-selection-for-various-context-adaptive-conversational-retrieval-hongru-wang-et-al-2024>(61/62 | 80/279) UniRetriever: Multi-task Candidates Selection for Various Context-Adaptive Conversational Retrieval (Hongru Wang et al., 2024)</a></li><li><a href=#6262--81279-diffucomet-contextual-commonsense-knowledge-diffusion-silin-gao-et-al-2024>(62/62 | 81/279) DiffuCOMET: Contextual Commonsense Knowledge Diffusion (Silin Gao et al., 2024)</a></li></ul></li><li><a href=#eessas-3>eess.AS (3)</a><ul><li><a href=#13--82279-skill-similarity-aware-knowledge-distillation-for-speech-self-supervised-learning-luca-zampierin-et-al-2024>(1/3 | 82/279) SKILL: Similarity-aware Knowledge distILLation for Speech Self-Supervised Learning (Luca Zampierin et al., 2024)</a></li><li><a href=#23--83279-an-automated-end-to-end-open-source-software-for-high-quality-text-to-speech-dataset-generation-ahmet-gunduz-et-al-2024>(2/3 | 83/279) An Automated End-to-End Open-Source Software for High-Quality Text-to-Speech Dataset Generation (Ahmet Gunduz et al., 2024)</a></li><li><a href=#33--84279-audio-visual-speech-enhancement-in-noisy-environments-via-emotion-based-contextual-cues-tassadaq-hussain-et-al-2024>(3/3 | 84/279) Audio-Visual Speech Enhancement in Noisy Environments via Emotion-Based Contextual Cues (Tassadaq Hussain et al., 2024)</a></li></ul></li><li><a href=#cslg-51>cs.LG (51)</a><ul><li><a href=#151--85279-gistembed-guided-in-sample-selection-of-training-negatives-for-text-embedding-fine-tuning-aivin-v-solatorio-2024>(1/51 | 85/279) GISTEmbed: Guided In-sample Selection of Training Negatives for Text Embedding Fine-tuning (Aivin V. Solatorio, 2024)</a></li><li><a href=#251--86279-think-big-generate-quick-llm-to-slm-for-fast-autoregressive-decoding-benjamin-bergner-et-al-2024>(2/51 | 86/279) Think Big, Generate Quick: LLM-to-SLM for Fast Autoregressive Decoding (Benjamin Bergner et al., 2024)</a></li><li><a href=#351--87279-asymmetry-in-low-rank-adapters-of-foundation-models-jiacheng-zhu-et-al-2024>(3/51 | 87/279) Asymmetry in Low-Rank Adapters of Foundation Models (Jiacheng Zhu et al., 2024)</a></li><li><a href=#451--88279-graph-learning-under-distribution-shifts-a-comprehensive-survey-on-domain-adaptation-out-of-distribution-and-continual-learning-man-wu-et-al-2024>(4/51 | 88/279) Graph Learning under Distribution Shifts: A Comprehensive Survey on Domain Adaptation, Out-of-distribution, and Continual Learning (Man Wu et al., 2024)</a></li><li><a href=#551--89279-generative-pretrained-hierarchical-transformer-for-time-series-forecasting-zhiding-liu-et-al-2024>(5/51 | 89/279) Generative Pretrained Hierarchical Transformer for Time Series Forecasting (Zhiding Liu et al., 2024)</a></li><li><a href=#651--90279-personalized-federated-instruction-tuning-via-neural-architecture-search-pengyu-zhang-et-al-2024>(6/51 | 90/279) Personalized Federated Instruction Tuning via Neural Architecture Search (Pengyu Zhang et al., 2024)</a></li><li><a href=#751--91279-referee-can-play-an-alternative-approach-to-conditional-generation-via-model-inversion-xuantong-liu-et-al-2024>(7/51 | 91/279) Referee Can Play: An Alternative Approach to Conditional Generation via Model Inversion (Xuantong Liu et al., 2024)</a></li><li><a href=#851--92279-interrogate-learning-to-share-specialize-and-prune-representations-for-multi-task-learning-babak-ehteshami-bejnordi-et-al-2024>(8/51 | 92/279) InterroGate: Learning to Share, Specialize, and Prune Representations for Multi-task Learning (Babak Ehteshami Bejnordi et al., 2024)</a></li><li><a href=#951--93279-training-neural-networks-from-scratch-with-parallel-low-rank-adapters-minyoung-huh-et-al-2024>(9/51 | 93/279) Training Neural Networks from Scratch with Parallel Low-Rank Adapters (Minyoung Huh et al., 2024)</a></li><li><a href=#1051--94279-training-implicit-generative-models-via-an-invariant-statistical-loss-josé-manuel-de-frutos-et-al-2024>(10/51 | 94/279) Training Implicit Generative Models via an Invariant Statistical Loss (José Manuel de Frutos et al., 2024)</a></li><li><a href=#1151--95279-feedback-efficient-online-fine-tuning-of-diffusion-models-masatoshi-uehara-et-al-2024>(11/51 | 95/279) Feedback Efficient Online Fine-Tuning of Diffusion Models (Masatoshi Uehara et al., 2024)</a></li><li><a href=#1251--96279-an-integrated-data-processing-framework-for-pretraining-foundation-models-yiding-sun-et-al-2024>(12/51 | 96/279) An Integrated Data Processing Framework for Pretraining Foundation Models (Yiding Sun et al., 2024)</a></li><li><a href=#1351--97279-language-guided-skill-learning-with-temporal-variational-inference-haotian-fu-et-al-2024>(13/51 | 97/279) Language-guided Skill Learning with Temporal Variational Inference (Haotian Fu et al., 2024)</a></li><li><a href=#1451--98279-minimize-control-inputs-for-strong-structural-controllability-using-reinforcement-learning-with-graph-neural-network-mengbang-zou-et-al-2024>(14/51 | 98/279) Minimize Control Inputs for Strong Structural Controllability Using Reinforcement Learning with Graph Neural Network (Mengbang Zou et al., 2024)</a></li><li><a href=#1551--99279-graph-learning-with-distributional-edge-layouts-xinjian-zhao-et-al-2024>(15/51 | 99/279) Graph Learning with Distributional Edge Layouts (Xinjian Zhao et al., 2024)</a></li><li><a href=#1651--100279-boosting-graph-pooling-with-persistent-homology-chaolong-ying-et-al-2024>(16/51 | 100/279) Boosting Graph Pooling with Persistent Homology (Chaolong Ying et al., 2024)</a></li><li><a href=#1751--101279-watch-your-head-assembling-projection-heads-to-save-the-reliability-of-federated-models-jinqian-chen-et-al-2024>(17/51 | 101/279) Watch Your Head: Assembling Projection Heads to Save the Reliability of Federated Models (Jinqian Chen et al., 2024)</a></li><li><a href=#1851--102279-one-shot-graph-representation-learning-using-hyperdimensional-computing-abhishek-dalvi-et-al-2024>(18/51 | 102/279) One-Shot Graph Representation Learning Using Hyperdimensional Computing (Abhishek Dalvi et al., 2024)</a></li><li><a href=#1951--103279-neural-operators-with-localized-integral-and-differential-kernels-miguel-liu-schiaffini-et-al-2024>(19/51 | 103/279) Neural Operators with Localized Integral and Differential Kernels (Miguel Liu-Schiaffini et al., 2024)</a></li><li><a href=#2051--104279-why-transformers-need-adam-a-hessian-perspective-yushun-zhang-et-al-2024>(20/51 | 104/279) Why Transformers Need Adam: A Hessian Perspective (Yushun Zhang et al., 2024)</a></li><li><a href=#2151--105279-q-fox-learning-breaking-tradition-in-reinforcement-learning-mahmood-alqaseer-et-al-2024>(21/51 | 105/279) Q-FOX Learning: Breaking Tradition in Reinforcement Learning (Mahmood Alqaseer et al., 2024)</a></li><li><a href=#2251--106279-c-gail-stabilizing-generative-adversarial-imitation-learning-with-control-theory-tianjiao-luo-et-al-2024>(22/51 | 106/279) C-GAIL: Stabilizing Generative Adversarial Imitation Learning with Control Theory (Tianjiao Luo et al., 2024)</a></li><li><a href=#2351--107279-m2mkd-module-to-module-knowledge-distillation-for-modular-transformers-ka-man-lo-et-al-2024>(23/51 | 107/279) m2mKD: Module-to-Module Knowledge Distillation for Modular Transformers (Ka Man Lo et al., 2024)</a></li><li><a href=#2451--108279-poisson-gamma-dynamical-systems-with-non-stationary-transition-dynamics-jiahao-wang-et-al-2024>(24/51 | 108/279) Poisson-Gamma Dynamical Systems with Non-Stationary Transition Dynamics (Jiahao Wang et al., 2024)</a></li><li><a href=#2551--109279-foundation-model-transparency-reports-rishi-bommasani-et-al-2024>(25/51 | 109/279) Foundation Model Transparency Reports (Rishi Bommasani et al., 2024)</a></li><li><a href=#2651--110279-self-supervised-correlation-based-permutations-for-multi-view-clustering-ran-eisenberg-et-al-2024>(26/51 | 110/279) Self Supervised Correlation-based Permutations for Multi-View Clustering (Ran Eisenberg et al., 2024)</a></li><li><a href=#2751--111279-garnn-an-interpretable-graph-attentive-recurrent-neural-network-for-predicting-blood-glucose-levels-via-multivariate-time-series-chengzhe-piao-et-al-2024>(27/51 | 111/279) GARNN: An Interpretable Graph Attentive Recurrent Neural Network for Predicting Blood Glucose Levels via Multivariate Time Series (Chengzhe Piao et al., 2024)</a></li><li><a href=#2851--112279-a-curious-case-of-remarkable-resilience-to-gradient-attacks-via-fully-convolutional-and-differentiable-front-end-with-a-skip-connection-leonid-boytsov-et-al-2024>(28/51 | 112/279) A Curious Case of Remarkable Resilience to Gradient Attacks via Fully Convolutional and Differentiable Front End with a Skip Connection (Leonid Boytsov et al., 2024)</a></li><li><a href=#2951--113279-craftax-a-lightning-fast-benchmark-for-open-ended-reinforcement-learning-michael-matthews-et-al-2024>(29/51 | 113/279) Craftax: A Lightning-Fast Benchmark for Open-Ended Reinforcement Learning (Michael Matthews et al., 2024)</a></li><li><a href=#3051--114279-on-the-generalization-capability-of-temporal-graph-learning-algorithms-theoretical-insights-and-a-simpler-method-weilin-cong-et-al-2024>(30/51 | 114/279) On the Generalization Capability of Temporal Graph Learning Algorithms: Theoretical Insights and a Simpler Method (Weilin Cong et al., 2024)</a></li><li><a href=#3151--115279-graph-diffusion-policy-optimization-yijing-liu-et-al-2024>(31/51 | 115/279) Graph Diffusion Policy Optimization (Yijing Liu et al., 2024)</a></li><li><a href=#3251--116279-parallelized-spatiotemporal-binding-gautam-singh-et-al-2024>(32/51 | 116/279) Parallelized Spatiotemporal Binding (Gautam Singh et al., 2024)</a></li><li><a href=#3351--117279-monitoring-fidelity-of-online-reinforcement-learning-algorithms-in-clinical-trials-anna-l-trella-et-al-2024>(33/51 | 117/279) Monitoring Fidelity of Online Reinforcement Learning Algorithms in Clinical Trials (Anna L. Trella et al., 2024)</a></li><li><a href=#3451--118279-discovering-symmetry-group-structures-via-implicit-orthogonality-bias-dongsung-huh-2024>(34/51 | 118/279) Discovering Symmetry Group Structures via Implicit Orthogonality Bias (Dongsung Huh, 2024)</a></li><li><a href=#3551--119279-enhancing-continuous-domain-adaptation-with-multi-path-transfer-curriculum-hanbing-liu-et-al-2024>(35/51 | 119/279) Enhancing Continuous Domain Adaptation with Multi-Path Transfer Curriculum (Hanbing Liu et al., 2024)</a></li><li><a href=#3651--120279-program-based-strategy-induction-for-reinforcement-learning-carlos-g-correa-et-al-2024>(36/51 | 120/279) Program-Based Strategy Induction for Reinforcement Learning (Carlos G. Correa et al., 2024)</a></li><li><a href=#3751--121279-achieving-tildeo1ε-sample-complexity-for-constrained-markov-decision-process-jiashuo-jiang-et-al-2024>(37/51 | 121/279) Achieving $\tilde{O}(1/ε)$ Sample Complexity for Constrained Markov Decision Process (Jiashuo Jiang et al., 2024)</a></li><li><a href=#3851--122279-federated-contextual-cascading-bandits-with-asynchronous-communication-and-heterogeneous-users-hantao-yang-et-al-2024>(38/51 | 122/279) Federated Contextual Cascading Bandits with Asynchronous Communication and Heterogeneous Users (Hantao Yang et al., 2024)</a></li><li><a href=#3951--123279-replay-modeling-time-varying-temporal-regularities-of-human-mobility-for-location-prediction-over-sparse-trajectories-bangchao-deng-et-al-2024>(39/51 | 123/279) REPLAY: Modeling Time-Varying Temporal Regularities of Human Mobility for Location Prediction over Sparse Trajectories (Bangchao Deng et al., 2024)</a></li><li><a href=#4051--124279-learning-to-schedule-online-tasks-with-bandit-feedback-yongxin-xu-et-al-2024>(40/51 | 124/279) Learning to Schedule Online Tasks with Bandit Feedback (Yongxin Xu et al., 2024)</a></li><li><a href=#4151--125279-totem-tokenized-time-series-embeddings-for-general-time-series-analysis-sabera-talukder-et-al-2024>(41/51 | 125/279) TOTEM: TOkenized Time Series EMbeddings for General Time Series Analysis (Sabera Talukder et al., 2024)</a></li><li><a href=#4251--126279-fedreview-a-review-mechanism-for-rejecting-poisoned-updates-in-federated-learning-tianhang-zheng-et-al-2024>(42/51 | 126/279) FedReview: A Review Mechanism for Rejecting Poisoned Updates in Federated Learning (Tianhang Zheng et al., 2024)</a></li><li><a href=#4351--127279-interpreting-grokked-transformers-in-complex-modular-arithmetic-hiroki-furuta-et-al-2024>(43/51 | 127/279) Interpreting Grokked Transformers in Complex Modular Arithmetic (Hiroki Furuta et al., 2024)</a></li><li><a href=#4451--128279-conformalized-selective-regression-anna-sokol-et-al-2024>(44/51 | 128/279) Conformalized Selective Regression (Anna Sokol et al., 2024)</a></li><li><a href=#4551--129279-learning-translations-emergent-communication-pretraining-for-cooperative-language-acquisition-dylan-cope-et-al-2024>(45/51 | 129/279) Learning Translations: Emergent Communication Pretraining for Cooperative Language Acquisition (Dylan Cope et al., 2024)</a></li><li><a href=#4651--130279-carte-pretraining-and-transfer-for-tabular-learning-myung-jun-kim-et-al-2024>(46/51 | 130/279) CARTE: pretraining and transfer for tabular learning (Myung Jun Kim et al., 2024)</a></li><li><a href=#4751--131279-partial-rankings-of-optimizers-julian-rodemann-et-al-2024>(47/51 | 131/279) Partial Rankings of Optimizers (Julian Rodemann et al., 2024)</a></li><li><a href=#4851--132279-label-learning-method-based-on-tensor-projection-jing-li-et-al-2024>(48/51 | 132/279) Label Learning Method Based on Tensor Projection (Jing Li et al., 2024)</a></li><li><a href=#4951--133279-active-level-set-estimation-for-continuous-search-space-with-theoretical-guarantee-giang-ngo-et-al-2024>(49/51 | 133/279) Active Level Set Estimation for Continuous Search Space with Theoretical Guarantee (Giang Ngo et al., 2024)</a></li><li><a href=#5051--134279-a-multi-fidelity-methodology-for-reduced-order-models-with-high-dimensional-inputs-bilal-mufti-et-al-2024>(50/51 | 134/279) A Multi-Fidelity Methodology for Reduced Order Models with High-Dimensional Inputs (Bilal Mufti et al., 2024)</a></li><li><a href=#5151--135279-dagnosis-localized-identification-of-data-inconsistencies-using-structures-nicolas-huynh-et-al-2024>(51/51 | 135/279) DAGnosis: Localized Identification of Data Inconsistencies using Structures (Nicolas Huynh et al., 2024)</a></li></ul></li><li><a href=#cscv-40>cs.CV (40)</a><ul><li><a href=#140--136279-few-shot-learning-for-annotation-efficient-nucleus-instance-segmentation-yu-ming-et-al-2024>(1/40 | 136/279) Few-Shot Learning for Annotation-Efficient Nucleus Instance Segmentation (Yu Ming et al., 2024)</a></li><li><a href=#240--137279-consept-continual-semantic-segmentation-via-adapter-based-vision-transformer-bowen-dong-et-al-2024>(2/40 | 137/279) ConSept: Continual Semantic Segmentation via Adapter-based Vision Transformer (Bowen Dong et al., 2024)</a></li><li><a href=#340--138279-generative-ai-in-vision-a-survey-on-models-metrics-and-applications-gaurav-raut-et-al-2024>(3/40 | 138/279) Generative AI in Vision: A Survey on Models, Metrics and Applications (Gaurav Raut et al., 2024)</a></li><li><a href=#440--139279-groundhog-grounding-large-language-models-to-holistic-segmentation-yichi-zhang-et-al-2024>(4/40 | 139/279) GROUNDHOG: Grounding Large Language Models to Holistic Segmentation (Yichi Zhang et al., 2024)</a></li><li><a href=#540--140279-placing-objects-in-context-via-inpainting-for-out-of-distribution-segmentation-pau-de-jorge-et-al-2024>(5/40 | 140/279) Placing Objects in Context via Inpainting for Out-of-distribution Segmentation (Pau de Jorge et al., 2024)</a></li><li><a href=#640--141279-finer-investigating-and-enhancing-fine-grained-visual-concept-recognition-in-large-vision-language-models-jeonghwan-kim-et-al-2024>(6/40 | 141/279) Finer: Investigating and Enhancing Fine-Grained Visual Concept Recognition in Large Vision Language Models (Jeonghwan Kim et al., 2024)</a></li><li><a href=#740--142279-blo-sam-bi-level-optimization-based-overfitting-preventing-finetuning-of-sam-li-zhang-et-al-2024>(7/40 | 142/279) BLO-SAM: Bi-level Optimization Based Overfitting-Preventing Finetuning of SAM (Li Zhang et al., 2024)</a></li><li><a href=#840--143279-offline-writer-identification-using-convolutional-neural-network-activation-features-vincent-christlein-et-al-2024>(8/40 | 143/279) Offline Writer Identification Using Convolutional Neural Network Activation Features (Vincent Christlein et al., 2024)</a></li><li><a href=#940--144279-saliency-aware-automatic-buddhas-statue-recognition-yong-qi-et-al-2024>(9/40 | 144/279) Saliency-Aware Automatic Buddhas Statue Recognition (Yong Qi et al., 2024)</a></li><li><a href=#1040--145279-weighted-monte-carlo-augmented-spherical-fourier-bessel-convolutional-layers-for-3d-abdominal-organ-segmentation-wenzhao-zhao-et-al-2024>(10/40 | 145/279) Weighted Monte Carlo augmented spherical Fourier-Bessel convolutional layers for 3D abdominal organ segmentation (Wenzhao Zhao et al., 2024)</a></li><li><a href=#1140--146279-intelligent-known-and-novel-aircraft-recognition----a-shift-from-classification-to-similarity-learning-for-combat-identification-ahmad-saeed-et-al-2024>(11/40 | 146/279) Intelligent Known and Novel Aircraft Recognition &ndash; A Shift from Classification to Similarity Learning for Combat Identification (Ahmad Saeed et al., 2024)</a></li><li><a href=#1240--147279-deyo-detr-with-yolo-for-end-to-end-object-detection-haodong-ouyang-2024>(12/40 | 147/279) DEYO: DETR with YOLO for End-to-End Object Detection (Haodong Ouyang, 2024)</a></li><li><a href=#1340--148279-mapm-multi-scale-attention-pyramid-module-for-enhanced-scale-variation-in-rld-detection-yunusa-haruna-et-al-2024>(13/40 | 148/279) mAPm: multi-scale Attention Pyramid module for Enhanced scale-variation in RLD detection (Yunusa Haruna et al., 2024)</a></li><li><a href=#1440--149279-automated-floodwater-depth-estimation-using-large-multimodal-model-for-rapid-flood-mapping-temitope-akinboyewa-et-al-2024>(14/40 | 149/279) Automated Floodwater Depth Estimation Using Large Multimodal Model for Rapid Flood Mapping (Temitope Akinboyewa et al., 2024)</a></li><li><a href=#1540--150279-cursor-scalable-mixed-order-hypergraph-matching-with-cur-decomposition-qixuan-zheng-et-al-2024>(15/40 | 150/279) CURSOR: Scalable Mixed-Order Hypergraph Matching with CUR Decomposition (Qixuan Zheng et al., 2024)</a></li><li><a href=#1640--151279-taming-the-tail-in-class-conditional-gans-knowledge-sharing-via-unconditional-training-at-lower-resolutions-saeed-khorram-et-al-2024>(16/40 | 151/279) Taming the Tail in Class-Conditional GANs: Knowledge Sharing via Unconditional Training at Lower Resolutions (Saeed Khorram et al., 2024)</a></li><li><a href=#1740--152279-towards-open-ended-visual-quality-comparison-haoning-wu-et-al-2024>(17/40 | 152/279) Towards Open-ended Visual Quality Comparison (Haoning Wu et al., 2024)</a></li><li><a href=#1840--153279-multi-lora-composition-for-image-generation-ming-zhong-et-al-2024>(18/40 | 153/279) Multi-LoRA Composition for Image Generation (Ming Zhong et al., 2024)</a></li><li><a href=#1940--154279-disentangled-3d-scene-generation-with-layout-learning-dave-epstein-et-al-2024>(19/40 | 154/279) Disentangled 3D Scene Generation with Layout Learning (Dave Epstein et al., 2024)</a></li><li><a href=#2040--155279-unifying-latent-and-lexicon-representations-for-effective-video-text-retrieval-haowei-liu-et-al-2024>(20/40 | 155/279) Unifying Latent and Lexicon Representations for Effective Video-Text Retrieval (Haowei Liu et al., 2024)</a></li><li><a href=#2140--156279-cross-modal-contextualized-diffusion-models-for-text-guided-visual-generation-and-editing-ling-yang-et-al-2024>(21/40 | 156/279) Cross-Modal Contextualized Diffusion Models for Text-Guided Visual Generation and Editing (Ling Yang et al., 2024)</a></li><li><a href=#2240--157279-improving-the-jpeg-resistance-of-adversarial-attacks-on-face-recognition-by-interpolation-smoothing-kefu-guo-et-al-2024>(22/40 | 157/279) Improving the JPEG-resistance of Adversarial Attacks on Face Recognition by Interpolation Smoothing (Kefu Guo et al., 2024)</a></li><li><a href=#2340--158279-edge-detectors-can-make-deep-convolutional-neural-networks-more-robust-jin-ding-et-al-2024>(23/40 | 158/279) Edge Detectors Can Make Deep Convolutional Neural Networks More Robust (Jin Ding et al., 2024)</a></li><li><a href=#2440--159279-comae-comprehensive-attribute-exploration-for-zero-shot-hashing-yihang-zhou-et-al-2024>(24/40 | 159/279) COMAE: COMprehensive Attribute Exploration for Zero-shot Hashing (Yihang Zhou et al., 2024)</a></li><li><a href=#2540--160279-spc-nerf-spatial-predictive-compression-for-voxel-based-radiance-field-zetian-song-et-al-2024>(25/40 | 160/279) SPC-NeRF: Spatial Predictive Compression for Voxel Based Radiance Field (Zetian Song et al., 2024)</a></li><li><a href=#2640--161279-mv-swin-t-mammogram-classification-with-multi-view-swin-transformer-sushmita-sarker-et-al-2024>(26/40 | 161/279) MV-Swin-T: Mammogram Classification with Multi-view Swin Transformer (Sushmita Sarker et al., 2024)</a></li><li><a href=#2740--162279-gradient-guided-modality-decoupling-for-missing-modality-robustness-hao-wang-et-al-2024>(27/40 | 162/279) Gradient-Guided Modality Decoupling for Missing-Modality Robustness (Hao Wang et al., 2024)</a></li><li><a href=#2840--163279-gem3d-generative-medial-abstractions-for-3d-shape-synthesis-dmitry-petrov-et-al-2024>(28/40 | 163/279) GEM3D: GEnerative Medial Abstractions for 3D Shape Synthesis (Dmitry Petrov et al., 2024)</a></li><li><a href=#2940--164279-multi-human-mesh-recovery-with-transformers-zeyu-wang-et-al-2024>(29/40 | 164/279) Multi-Human Mesh Recovery with Transformers (Zeyu Wang et al., 2024)</a></li><li><a href=#3040--165279-stochastic-conditional-diffusion-models-for-semantic-image-synthesis-juyeon-ko-et-al-2024>(30/40 | 165/279) Stochastic Conditional Diffusion Models for Semantic Image Synthesis (Juyeon Ko et al., 2024)</a></li><li><a href=#3140--166279-infrared-and-visible-image-fusion-with-language-driven-loss-in-clip-embedding-space-yuhao-wang-et-al-2024>(31/40 | 166/279) Infrared and visible Image Fusion with Language-driven Loss in CLIP Embedding Space (Yuhao Wang et al., 2024)</a></li><li><a href=#3240--167279-neural-mesh-fusion-unsupervised-3d-planar-surface-understanding-farhad-g-zanjani-et-al-2024>(32/40 | 167/279) Neural Mesh Fusion: Unsupervised 3D Planar Surface Understanding (Farhad G. Zanjani et al., 2024)</a></li><li><a href=#3340--168279-pretrained-visual-uncertainties-michael-kirchhof-et-al-2024>(33/40 | 168/279) Pretrained Visual Uncertainties (Michael Kirchhof et al., 2024)</a></li><li><a href=#3440--169279-outline-guided-object-inpainting-with-diffusion-models-markus-pobitzer-et-al-2024>(34/40 | 169/279) Outline-Guided Object Inpainting with Diffusion Models (Markus Pobitzer et al., 2024)</a></li><li><a href=#3540--170279-cmc-few-shot-novel-view-synthesis-via-cross-view-multiplane-consistency-hanxin-zhu-et-al-2024>(35/40 | 170/279) CMC: Few-shot Novel View Synthesis via Cross-view Multiplane Consistency (Hanxin Zhu et al., 2024)</a></li><li><a href=#3640--171279-what-text-design-characterizes-book-genres-daichi-haraguchi-et-al-2024>(36/40 | 171/279) What Text Design Characterizes Book Genres? (Daichi Haraguchi et al., 2024)</a></li><li><a href=#3740--172279-real-time-vehicle-detection-and-urban-traffic-behavior-analysis-based-on-uav-traffic-videos-on-mobile-devices-yuan-zhu-et-al-2024>(37/40 | 172/279) Real-Time Vehicle Detection and Urban Traffic Behavior Analysis Based on UAV Traffic Videos on Mobile Devices (Yuan Zhu et al., 2024)</a></li><li><a href=#3840--173279-misc-ultra-low-bitrate-image-semantic-compression-driven-by-large-multimodal-model-chunyi-li-et-al-2024>(38/40 | 173/279) MISC: Ultra-low Bitrate Image Semantic Compression Driven by Large Multimodal Model (Chunyi Li et al., 2024)</a></li><li><a href=#3940--174279-dcvsmnet-double-cost-volume-stereo-matching-network-mahmoud-tahmasebi-et-al-2024>(39/40 | 174/279) DCVSMNet: Double Cost Volume Stereo Matching Network (Mahmoud Tahmasebi et al., 2024)</a></li><li><a href=#4040--175279-hoisdf-constraining-3d-hand-object-pose-estimation-with-global-signed-distance-fields-haozhe-qi-et-al-2024>(40/40 | 175/279) HOISDF: Constraining 3D Hand-Object Pose Estimation with Global Signed Distance Fields (Haozhe Qi et al., 2024)</a></li></ul></li><li><a href=#cssd-2>cs.SD (2)</a><ul><li><a href=#12--176279-self-supervised-speech-quality-estimation-and-enhancement-using-only-clean-speech-szu-wei-fu-et-al-2024>(1/2 | 176/279) Self-Supervised Speech Quality Estimation and Enhancement Using Only Clean Speech (Szu-Wei Fu et al., 2024)</a></li><li><a href=#22--177279-towards-environmental-preference-based-speech-enhancement-for-individualised-multi-modal-hearing-aids-jasper-kirton-wingate-et-al-2024>(2/2 | 177/279) Towards Environmental Preference Based Speech Enhancement For Individualised Multi-Modal Hearing Aids (Jasper Kirton-Wingate et al., 2024)</a></li></ul></li><li><a href=#csse-7>cs.SE (7)</a><ul><li><a href=#17--178279-clap-learning-transferable-binary-code-representations-with-natural-language-supervision-hao-wang-et-al-2024>(1/7 | 178/279) CLAP: Learning Transferable Binary Code Representations with Natural Language Supervision (Hao Wang et al., 2024)</a></li><li><a href=#27--179279-beyond-self-learned-attention-mitigating-attention-bias-in-transformer-based-models-using-attention-guidance-jiri-gesi-et-al-2024>(2/7 | 179/279) Beyond Self-learned Attention: Mitigating Attention Bias in Transformer-based Models Using Attention Guidance (Jiri Gesi et al., 2024)</a></li><li><a href=#37--180279-dealing-with-data-for-re-mitigating-challenges-while-using-nlp-and-generative-ai-smita-ghaisas-et-al-2024>(3/7 | 180/279) Dealing with Data for RE: Mitigating Challenges while using NLP and Generative AI (Smita Ghaisas et al., 2024)</a></li><li><a href=#47--181279-unveiling-chatgpts-usage-in-open-source-projects-a-mining-based-study-rosalia-tufano-et-al-2024>(4/7 | 181/279) Unveiling ChatGPT&rsquo;s Usage in Open Source Projects: A Mining-based Study (Rosalia Tufano et al., 2024)</a></li><li><a href=#57--182279-quality-assurance-for-artificial-intelligence-a-study-of-industrial-concerns-challenges-and-best-practices-chenyu-wang-et-al-2024>(5/7 | 182/279) Quality Assurance for Artificial Intelligence: A Study of Industrial Concerns, Challenges and Best Practices (Chenyu Wang et al., 2024)</a></li><li><a href=#67--183279-promptset-a-programmers-prompting-dataset-kaiser-pister-et-al-2024>(6/7 | 183/279) PromptSet: A Programmer&rsquo;s Prompting Dataset (Kaiser Pister et al., 2024)</a></li><li><a href=#77--184279-langgpt-rethinking-structured-reusable-prompt-design-framework-for-llms-from-the-programming-language-ming-wang-et-al-2024>(7/7 | 184/279) LangGPT: Rethinking Structured Reusable Prompt Design Framework for LLMs from the Programming Language (Ming Wang et al., 2024)</a></li></ul></li><li><a href=#eessiv-3>eess.IV (3)</a><ul><li><a href=#13--185279-investigating-the-robustness-of-vision-transformers-against-label-noise-in-medical-image-classification-bidur-khanal-et-al-2024>(1/3 | 185/279) Investigating the Robustness of Vision Transformers against Label Noise in Medical Image Classification (Bidur Khanal et al., 2024)</a></li><li><a href=#23--186279-un-sam-universal-prompt-free-segmentation-for-generalized-nuclei-images-zhen-chen-et-al-2024>(2/3 | 186/279) UN-SAM: Universal Prompt-Free Segmentation for Generalized Nuclei Images (Zhen Chen et al., 2024)</a></li><li><a href=#33--187279-spineps----automatic-whole-spine-segmentation-of-t2-weighted-mr-images-using-a-two-phase-approach-to-multi-class-semantic-and-instance-segmentation-hendrik-möller-et-al-2024>(3/3 | 187/279) SPINEPS &ndash; Automatic Whole Spine Segmentation of T2-weighted MR images using a Two-Phase Approach to Multi-class Semantic and Instance Segmentation (Hendrik Möller et al., 2024)</a></li></ul></li><li><a href=#hep-ex-1>hep-ex (1)</a><ul><li><a href=#11--188279-a-comparison-of-deep-learning-models-for-proton-background-rejection-with-the-ams-electromagnetic-calorimeter-raheem-karim-hashmani-et-al-2024>(1/1 | 188/279) A Comparison of Deep Learning Models for Proton Background Rejection with the AMS Electromagnetic Calorimeter (Raheem Karim Hashmani et al., 2024)</a></li></ul></li><li><a href=#csro-13>cs.RO (13)</a><ul><li><a href=#113--189279-phygrasp-generalizing-robotic-grasping-with-physics-informed-large-multimodal-models-dingkun-guo-et-al-2024>(1/13 | 189/279) PhyGrasp: Generalizing Robotic Grasping with Physics-informed Large Multimodal Models (Dingkun Guo et al., 2024)</a></li><li><a href=#213--190279-trajectory-prediction-for-autonomous-driving-using-a-transformer-network-zhenning-li-et-al-2024>(2/13 | 190/279) Trajectory Prediction for Autonomous Driving Using a Transformer Network (Zhenning Li et al., 2024)</a></li><li><a href=#313--191279-expressive-whole-body-control-for-humanoid-robots-xuxin-cheng-et-al-2024>(3/13 | 191/279) Expressive Whole-Body Control for Humanoid Robots (Xuxin Cheng et al., 2024)</a></li><li><a href=#413--192279-learning-based-nmpc-adaptation-for-autonomous-driving-using-parallelized-digital-twin-jean-pierre-allamaa-et-al-2024>(4/13 | 192/279) Learning Based NMPC Adaptation for Autonomous Driving using Parallelized Digital Twin (Jean Pierre Allamaa et al., 2024)</a></li><li><a href=#513--193279-the-door-and-drawer-reset-mechanisms-automated-mechanisms-for-testing-and-data-collection-kyle-dufrene-et-al-2024>(5/13 | 193/279) The Door and Drawer Reset Mechanisms: Automated Mechanisms for Testing and Data Collection (Kyle DuFrene et al., 2024)</a></li><li><a href=#613--194279-star-searcher-a-complete-and-efficient-aerial-system-for-autonomous-target-search-in-complex-unknown-environments-yiming-luo-et-al-2024>(6/13 | 194/279) Star-Searcher: A Complete and Efficient Aerial System for Autonomous Target Search in Complex Unknown Environments (Yiming Luo et al., 2024)</a></li><li><a href=#713--195279-swarmprm-probabilistic-roadmap-motion-planning-for-swarm-robotic-systems-yunze-hu-et-al-2024>(7/13 | 195/279) SwarmPRM: Probabilistic Roadmap Motion Planning for Swarm Robotic Systems (Yunze Hu et al., 2024)</a></li><li><a href=#813--196279-rover-risk-aware-swarm-robotics-motion-planner-using-conditional-value-at-risk-xuru-yang-et-al-2024>(8/13 | 196/279) ROVER: Risk-Aware Swarm Robotics MOtion Planner Using Conditional ValuE at Risk (Xuru Yang et al., 2024)</a></li><li><a href=#913--197279-efficient-continuous-time-ego-motion-estimation-for-asynchronous-event-based-data-associations-zhixiang-wang-et-al-2024>(9/13 | 197/279) Efficient Continuous-Time Ego-Motion Estimation for Asynchronous Event-based Data Associations (Zhixiang Wang et al., 2024)</a></li><li><a href=#1013--198279-scaling-robust-optimization-for-multi-agent-robotic-systems-a-distributed-perspective-arshiya-taj-abdul-et-al-2024>(10/13 | 198/279) Scaling Robust Optimization for Multi-Agent Robotic Systems: A Distributed Perspective (Arshiya Taj Abdul et al., 2024)</a></li><li><a href=#1113--199279-dreamup3d-object-centric-generative-models-for-single-view-3d-scene-understanding-and-real-to-sim-transfer-yizhe-wu-et-al-2024>(11/13 | 199/279) DreamUp3D: Object-Centric Generative Models for Single-View 3D Scene Understanding and Real-to-Sim Transfer (Yizhe Wu et al., 2024)</a></li><li><a href=#1213--200279-think2drive-efficient-reinforcement-learning-by-thinking-in-latent-world-model-for-quasi-realistic-autonomous-driving-in-carla-v2-qifeng-li-et-al-2024>(12/13 | 200/279) Think2Drive: Efficient Reinforcement Learning by Thinking in Latent World Model for Quasi-Realistic Autonomous Driving (in CARLA-v2) (Qifeng Li et al., 2024)</a></li><li><a href=#1313--201279-online-efficient-safety-critical-control-for-mobile-robots-in-unknown-dynamic-multi-obstacle-environments-yu-zhang-et-al-2024>(13/13 | 201/279) Online Efficient Safety-Critical Control for Mobile Robots in Unknown Dynamic Multi-Obstacle Environments (Yu Zhang et al., 2024)</a></li></ul></li><li><a href=#csai-10>cs.AI (10)</a><ul><li><a href=#110--202279-a-surprising-failure-multimodal-llms-and-the-nlvr-challenge-anne-wu-et-al-2024>(1/10 | 202/279) A Surprising Failure? Multimodal LLMs and the NLVR Challenge (Anne Wu et al., 2024)</a></li><li><a href=#210--203279-language-agents-as-optimizable-graphs-mingchen-zhuge-et-al-2024>(2/10 | 203/279) Language Agents as Optimizable Graphs (Mingchen Zhuge et al., 2024)</a></li><li><a href=#310--204279-label-informed-contrastive-pretraining-for-node-importance-estimation-on-knowledge-graphs-tianyu-zhang-et-al-2024>(3/10 | 204/279) Label Informed Contrastive Pretraining for Node Importance Estimation on Knowledge Graphs (Tianyu Zhang et al., 2024)</a></li><li><a href=#410--205279-on-languaging-a-simulation-engine-han-liu-et-al-2024>(4/10 | 205/279) On Languaging a Simulation Engine (Han Liu et al., 2024)</a></li><li><a href=#510--206279-from-large-language-models-and-optimization-to-decision-optimization-copilot-a-research-manifesto-segev-wasserkrug-et-al-2024>(5/10 | 206/279) From Large Language Models and Optimization to Decision Optimization CoPilot: A Research Manifesto (Segev Wasserkrug et al., 2024)</a></li><li><a href=#610--207279-gigapevt-multimodal-medical-assistant-pavel-blinov-et-al-2024>(6/10 | 207/279) GigaPevt: Multimodal Medical Assistant (Pavel Blinov et al., 2024)</a></li><li><a href=#710--208279-genainet-enabling-wireless-collective-intelligence-via-knowledge-transfer-and-reasoning-hang-zou-et-al-2024>(7/10 | 208/279) GenAINet: Enabling Wireless Collective Intelligence via Knowledge Transfer and Reasoning (Hang Zou et al., 2024)</a></li><li><a href=#810--209279-value-preferences-estimation-and-disambiguation-in-hybrid-participatory-systems-enrico-liscio-et-al-2024>(8/10 | 209/279) Value Preferences Estimation and Disambiguation in Hybrid Participatory Systems (Enrico Liscio et al., 2024)</a></li><li><a href=#910--210279-memory-gaps-would-llms-pass-the-tulving-test-jean-marie-chauvet-2024>(9/10 | 210/279) Memory GAPS: Would LLMs pass the Tulving Test? (Jean-Marie Chauvet, 2024)</a></li><li><a href=#1010--211279-contingency-planning-using-bi-level-markov-decision-processes-for-space-missions-somrita-banerjee-et-al-2024>(10/10 | 211/279) Contingency Planning Using Bi-level Markov Decision Processes for Space Missions (Somrita Banerjee et al., 2024)</a></li></ul></li><li><a href=#csce-1>cs.CE (1)</a><ul><li><a href=#11--212279-prollama-a-protein-large-language-model-for-multi-task-protein-language-processing-liuzhenghao-lv-et-al-2024>(1/1 | 212/279) ProLLaMA: A Protein Large Language Model for Multi-Task Protein Language Processing (Liuzhenghao Lv et al., 2024)</a></li></ul></li><li><a href=#csds-2>cs.DS (2)</a><ul><li><a href=#12--213279-ingrass-incremental-graph-spectral-sparsification-via-low-resistance-diameter-decomposition-ali-aghdaei-et-al-2024>(1/2 | 213/279) inGRASS: Incremental Graph Spectral Sparsification via Low-Resistance-Diameter Decomposition (Ali Aghdaei et al., 2024)</a></li><li><a href=#22--214279-the-complexity-of-diameter-on-h-free-graphs-jelle-j-oostveen-et-al-2024>(2/2 | 214/279) The Complexity of Diameter on H-free graphs (Jelle J. Oostveen et al., 2024)</a></li></ul></li><li><a href=#csar-1>cs.AR (1)</a><ul><li><a href=#11--215279-accelerating-graph-neural-networks-on-real-processing-in-memory-systems-christina-giannoula-et-al-2024>(1/1 | 215/279) Accelerating Graph Neural Networks on Real Processing-In-Memory Systems (Christina Giannoula et al., 2024)</a></li></ul></li><li><a href=#physicsflu-dyn-1>physics.flu-dyn (1)</a><ul><li><a href=#11--216279-model-based-deep-reinforcement-learning-for-accelerated-learning-from-flow-simulations-andre-weiner-et-al-2024>(1/1 | 216/279) Model-based deep reinforcement learning for accelerated learning from flow simulations (Andre Weiner et al., 2024)</a></li></ul></li><li><a href=#cscy-3>cs.CY (3)</a><ul><li><a href=#13--217279-unveiling-the-truth-and-facilitating-change-towards-agent-based-large-scale-social-movement-simulation-xinyi-mou-et-al-2024>(1/3 | 217/279) Unveiling the Truth and Facilitating Change: Towards Agent-based Large-scale Social Movement Simulation (Xinyi Mou et al., 2024)</a></li><li><a href=#23--218279-algorithmic-arbitrariness-in-content-moderation-juan-felipe-gomez-et-al-2024>(2/3 | 218/279) Algorithmic Arbitrariness in Content Moderation (Juan Felipe Gomez et al., 2024)</a></li><li><a href=#33--219279-integrating-dark-pattern-taxonomies-frank-lewis-et-al-2024>(3/3 | 219/279) Integrating Dark Pattern Taxonomies (Frank Lewis et al., 2024)</a></li></ul></li><li><a href=#mathst-1>math.ST (1)</a><ul><li><a href=#11--220279-on-independent-samples-along-the-langevin-diffusion-and-the-unadjusted-langevin-algorithm-jiaming-liang-et-al-2024>(1/1 | 220/279) On Independent Samples Along the Langevin Diffusion and the Unadjusted Langevin Algorithm (Jiaming Liang et al., 2024)</a></li></ul></li><li><a href=#eesssy-8>eess.SY (8)</a><ul><li><a href=#18--221279-reinforcement-learning-based-oscillation-dampening-scaling-up-single-agent-rl-algorithms-to-a-100-av-highway-field-operational-test-kathy-jang-et-al-2024>(1/8 | 221/279) Reinforcement Learning Based Oscillation Dampening: Scaling up Single-Agent RL algorithms to a 100 AV highway field operational test (Kathy Jang et al., 2024)</a></li><li><a href=#28--222279-physics-informed-lstm-based-delay-compensation-framework-for-teleoperated-ugvs-ahmad-abubakar-et-al-2024>(2/8 | 222/279) Physics-Informed LSTM-Based Delay Compensation Framework for Teleoperated UGVs (Ahmad Abubakar et al., 2024)</a></li><li><a href=#38--223279-batch-estimation-of-a-steady-uniform-flow-field-from-ground-velocity-and-heading-measurements-artur-wolek-et-al-2024>(3/8 | 223/279) Batch Estimation of a Steady, Uniform, Flow-Field from Ground Velocity and Heading Measurements (Artur Wolek et al., 2024)</a></li><li><a href=#48--224279-dynamic-model-of-back-to-back-converter-for-system-level-phasor-simulation-hisham-mahmood-et-al-2024>(4/8 | 224/279) Dynamic Model of Back-to-Back Converter for System-Level Phasor Simulation (Hisham Mahmood et al., 2024)</a></li><li><a href=#58--225279-hybrid-feedback-control-for-global-and-optimal-safe-navigation-ishak-cheniouni-et-al-2024>(5/8 | 225/279) Hybrid Feedback Control for Global and Optimal Safe Navigation (Ishak Cheniouni et al., 2024)</a></li><li><a href=#68--226279-hierarchical-speed-planner-for-automated-vehicles-a-framework-for-lagrangian-variable-speed-limit-in-mixed-autonomy-traffic-han-wang-et-al-2024>(6/8 | 226/279) Hierarchical Speed Planner for Automated Vehicles: A Framework for Lagrangian Variable Speed Limit in Mixed Autonomy Traffic (Han Wang et al., 2024)</a></li><li><a href=#78--227279-oscillations-aware-frequency-security-assessment-via-efficient-worst-case-frequency-nadir-computation-yan-jiang-et-al-2024>(7/8 | 227/279) Oscillations-Aware Frequency Security Assessment via Efficient Worst-Case Frequency Nadir Computation (Yan Jiang et al., 2024)</a></li><li><a href=#88--228279-path-planning-for-a-cooperative-navigation-aid-vehicle-to-assist-multiple-agents-intermittently-artur-wolek-2024>(8/8 | 228/279) Path Planning for a Cooperative Navigation Aid Vehicle to Assist Multiple Agents Intermittently (Artur Wolek, 2024)</a></li></ul></li><li><a href=#statml-8>stat.ML (8)</a><ul><li><a href=#18--229279-rate-optimal-rank-aggregation-with-private-pairwise-rankings-shirong-xu-et-al-2024>(1/8 | 229/279) Rate-Optimal Rank Aggregation with Private Pairwise Rankings (Shirong Xu et al., 2024)</a></li><li><a href=#28--230279-penalized-generative-variable-selection-tong-wang-et-al-2024>(2/8 | 230/279) Penalized Generative Variable Selection (Tong Wang et al., 2024)</a></li><li><a href=#38--231279-a-provably-accurate-randomized-sampling-algorithm-for-logistic-regression-agniva-chowdhury-et-al-2024>(3/8 | 231/279) A Provably Accurate Randomized Sampling Algorithm for Logistic Regression (Agniva Chowdhury et al., 2024)</a></li><li><a href=#48--232279-stable-training-of-normalizing-flows-for-high-dimensional-variational-inference-daniel-andrade-2024>(4/8 | 232/279) Stable Training of Normalizing Flows for High-dimensional Variational Inference (Daniel Andrade, 2024)</a></li><li><a href=#58--233279-uncertainty-quantification-in-anomaly-detection-with-cross-conformal-p-values-oliver-hennhöfer-et-al-2024>(5/8 | 233/279) Uncertainty Quantification in Anomaly Detection with Cross-Conformal $p$-Values (Oliver Hennhöfer et al., 2024)</a></li><li><a href=#68--234279-a-phase-transition-in-diffusion-models-reveals-the-hierarchical-nature-of-data-antonio-sclocchi-et-al-2024>(6/8 | 234/279) A Phase Transition in Diffusion Models Reveals the Hierarchical Nature of Data (Antonio Sclocchi et al., 2024)</a></li><li><a href=#78--235279-stopping-bayesian-optimization-with-probabilistic-regret-bounds-james-t-wilson-2024>(7/8 | 235/279) Stopping Bayesian Optimization with Probabilistic Regret Bounds (James T. Wilson, 2024)</a></li><li><a href=#88--236279-on-the-connection-between-noise-contrastive-estimation-and-contrastive-divergence-amanda-olmin-et-al-2024>(8/8 | 236/279) On the connection between Noise-Contrastive Estimation and Contrastive Divergence (Amanda Olmin et al., 2024)</a></li></ul></li><li><a href=#quant-ph-1>quant-ph (1)</a><ul><li><a href=#11--237279-quantum-linear-algebra-is-all-you-need-for-transformer-architectures-naixu-guo-et-al-2024>(1/1 | 237/279) Quantum linear algebra is all you need for Transformer architectures (Naixu Guo et al., 2024)</a></li></ul></li><li><a href=#csit-9>cs.IT (9)</a><ul><li><a href=#19--238279-slipt-in-joint-dimming-multi-led-owc-systems-with-rate-splitting-multiple-access-sepideh-javadi-et-al-2024>(1/9 | 238/279) SLIPT in Joint Dimming Multi-LED OWC Systems with Rate Splitting Multiple Access (Sepideh Javadi et al., 2024)</a></li><li><a href=#29--239279-performance-tradeoff-between-overhead-and-achievable-snr-in-ris-beam-training-friedemann-laue-et-al-2024>(2/9 | 239/279) Performance Tradeoff Between Overhead and Achievable SNR in RIS Beam Training (Friedemann Laue et al., 2024)</a></li><li><a href=#39--240279-quadratic-message-passing-for-generalized-quadratic-equations-model-huimin-zhu-2024>(3/9 | 240/279) Quadratic Message Passing for Generalized Quadratic Equations Model (Huimin Zhu, 2024)</a></li><li><a href=#49--241279-achievable-rate-optimization-for-stacked-intelligent-metasurface-assisted-holographic-mimo-communications-anastasios-papazafeiropoulos-et-al-2024>(4/9 | 241/279) Achievable Rate Optimization for Stacked Intelligent Metasurface-Assisted Holographic MIMO Communications (Anastasios Papazafeiropoulos et al., 2024)</a></li><li><a href=#59--242279-towards-bridging-the-gap-between-near-and-far-field-characterizations-of-the-wireless-channel-navneet-agrawal-et-al-2024>(5/9 | 242/279) Towards Bridging the Gap between Near and Far-Field Characterizations of the Wireless Channel (Navneet Agrawal et al., 2024)</a></li><li><a href=#69--243279-im-based-pilot-assisted-channel-estimation-for-ftn-signaling-hf-communications-simin-keykhosravi-et-al-2024>(6/9 | 243/279) IM-based Pilot-assisted Channel Estimation for FTN Signaling HF Communications (Simin Keykhosravi et al., 2024)</a></li><li><a href=#79--244279-performance-of-double-stacked-intelligent-metasurface-assisted-multiuser-massive-mimo-communications-in-the-wave-domain-anastasios-papazafeiropoulos-et-al-2024>(7/9 | 244/279) Performance of Double-Stacked Intelligent Metasurface-Assisted Multiuser Massive MIMO Communications in the Wave Domain (Anastasios Papazafeiropoulos et al., 2024)</a></li><li><a href=#89--245279-random-staircase-generator-matrix-codes-qianfan-wang-et-al-2024>(8/9 | 245/279) Random Staircase Generator Matrix Codes (Qianfan Wang et al., 2024)</a></li><li><a href=#99--246279-a-joint-communication-and-computation-design-for-probabilistic-semantic-communications-zhouxiang-zhao-et-al-2024>(9/9 | 246/279) A Joint Communication and Computation Design for Probabilistic Semantic Communications (Zhouxiang Zhao et al., 2024)</a></li></ul></li><li><a href=#csma-2>cs.MA (2)</a><ul><li><a href=#12--247279-distributed-finite-time-differentiator-for-multi-agent-systems-under-directed-graph-weile-chen-et-al-2024>(1/2 | 247/279) Distributed Finite-time Differentiator for Multi-agent Systems Under Directed Graph (Weile Chen et al., 2024)</a></li><li><a href=#22--248279-navigating-complexity-orchestrated-problem-solving-with-multi-agent-llms-sumedh-rasal-et-al-2024>(2/2 | 248/279) Navigating Complexity: Orchestrated Problem Solving with Multi-Agent LLMs (Sumedh Rasal et al., 2024)</a></li></ul></li><li><a href=#csgr-2>cs.GR (2)</a><ul><li><a href=#12--249279-cell-constrained-particles-for-incompressible-fluids-zohar-levi-2024>(1/2 | 249/279) Cell-Constrained Particles for Incompressible Fluids (Zohar Levi, 2024)</a></li><li><a href=#22--250279-non-euclidean-sliced-optimal-transport-sampling-baptiste-genest-et-al-2024>(2/2 | 250/279) Non-Euclidean Sliced Optimal Transport Sampling (Baptiste Genest et al., 2024)</a></li></ul></li><li><a href=#q-bioqm-1>q-bio.QM (1)</a><ul><li><a href=#11--251279-an-optimal-transport-model-for-dynamical-shapes-collective-motion-and-cellular-aggregates-antoine-diez-et-al-2024>(1/1 | 251/279) An optimal transport model for dynamical shapes, collective motion and cellular aggregates (Antoine Diez et al., 2024)</a></li></ul></li><li><a href=#mathna-7>math.NA (7)</a><ul><li><a href=#17--252279-a-stochastic-perturbation-approach-to-nonlinear-bifurcating-problems-isabella-carla-gonnella-et-al-2024>(1/7 | 252/279) A stochastic perturbation approach to nonlinear bifurcating problems (Isabella Carla Gonnella et al., 2024)</a></li><li><a href=#27--253279-finite-element-schemes-with-tangential-motion-for-fourth-order-geometric-curve-evolutions-in-arbitrary-codimension-klaus-deckelnick-et-al-2024>(2/7 | 253/279) Finite element schemes with tangential motion for fourth order geometric curve evolutions in arbitrary codimension (Klaus Deckelnick et al., 2024)</a></li><li><a href=#37--254279-structure-preserving-operator-learning-modeling-the-collision-operator-of-kinetic-equations-jae-yong-lee-et-al-2024>(3/7 | 254/279) Structure-Preserving Operator Learning: Modeling the Collision Operator of Kinetic Equations (Jae Yong Lee et al., 2024)</a></li><li><a href=#47--255279-isogeometric-analysis-of-the-laplace-eigenvalue-problem-on-circular-sectors-regularity-properties-graded-meshes--variational-crimes-thomas-apel-et-al-2024>(4/7 | 255/279) Isogeometric analysis of the Laplace eigenvalue problem on circular sectors: Regularity properties, graded meshes & variational crimes (Thomas Apel et al., 2024)</a></li><li><a href=#57--256279-to-be-or-not-to-be-that-is-the-question-exploring-the-pseudorandom-generation-of-texts-to-write-hamlet-from-the-perspective-of-the-infinite-monkey-theorem-ergon-cugler-de-moraes-silva-2024>(5/7 | 256/279) To be, or not to be, that is the Question: Exploring the pseudorandom generation of texts to write Hamlet from the perspective of the Infinite Monkey Theorem (Ergon Cugler de Moraes Silva, 2024)</a></li><li><a href=#67--257279-point-collocation-with-mollified-piecewise-polynomial-approximants-for-high-order-partial-differential-equations-dewangga-alfarisy-et-al-2024>(6/7 | 257/279) Point collocation with mollified piecewise polynomial approximants for high-order partial differential equations (Dewangga Alfarisy et al., 2024)</a></li><li><a href=#77--258279-discovering-artificial-viscosity-models-for-discontinuous-galerkin-approximation-of-conservation-laws-using-physics-informed-machine-learning-matteo-caldana-et-al-2024>(7/7 | 258/279) Discovering Artificial Viscosity Models for Discontinuous Galerkin Approximation of Conservation Laws using Physics-Informed Machine Learning (Matteo Caldana et al., 2024)</a></li></ul></li><li><a href=#cshc-5>cs.HC (5)</a><ul><li><a href=#15--259279-if-in-a-crowdsourced-data-annotation-pipeline-a-gpt-4-zeyu-he-et-al-2024>(1/5 | 259/279) If in a Crowdsourced Data Annotation Pipeline, a GPT-4 (Zeyu He et al., 2024)</a></li><li><a href=#25--260279-the-interaction-fidelity-model-a-taxonomy-to-distinguish-the-aspects-of-fidelity-in-virtual-reality-michael-bonfert-et-al-2024>(2/5 | 260/279) The Interaction Fidelity Model: A Taxonomy to Distinguish the Aspects of Fidelity in Virtual Reality (Michael Bonfert et al., 2024)</a></li><li><a href=#35--261279-deconstructing-the-veneer-of-simplicity-co-designing-introductory-generative-ai-workshops-with-local-entrepreneurs-yasmine-kotturi-et-al-2024>(3/5 | 261/279) Deconstructing the Veneer of Simplicity: Co-Designing Introductory Generative AI Workshops with Local Entrepreneurs (Yasmine Kotturi et al., 2024)</a></li><li><a href=#45--262279-a-visualization-tool-to-explore-alphabet-orderings-for-the-burrows-wheeler-transform-lily-major-et-al-2024>(4/5 | 262/279) A visualization tool to explore alphabet orderings for the Burrows-Wheeler Transform (Lily Major et al., 2024)</a></li><li><a href=#55--263279-speech-as-interactive-design-material-sidm-how-to-design-and-evaluate-task-tailored-synthetic-voices-mateusz-dubiel-et-al-2024>(5/5 | 263/279) Speech as Interactive Design Material (SIDM): How to design and evaluate task-tailored synthetic voices? (Mateusz Dubiel et al., 2024)</a></li></ul></li><li><a href=#astro-phsr-1>astro-ph.SR (1)</a><ul><li><a href=#11--264279-performance-of-high-order-godunov-type-methods-in-simulations-of-astrophysical-low-mach-number-flows-g-leidi-et-al-2024>(1/1 | 264/279) Performance of high-order Godunov-type methods in simulations of astrophysical low Mach number flows (G. Leidi et al., 2024)</a></li></ul></li><li><a href=#physicschem-ph-1>physics.chem-ph (1)</a><ul><li><a href=#11--265279-trustmol-trustworthy-inverse-molecular-design-via-alignment-with-molecular-dynamics-kevin-tirta-wijaya-et-al-2024>(1/1 | 265/279) TrustMol: Trustworthy Inverse Molecular Design via Alignment with Molecular Dynamics (Kevin Tirta Wijaya et al., 2024)</a></li></ul></li><li><a href=#eesssp-1>eess.SP (1)</a><ul><li><a href=#11--266279-egnn-c-interpretable-evolving-granular-neural-network-and-application-in-classification-of-weakly-supervised-eeg-data-streams-daniel-leite-et-al-2024>(1/1 | 266/279) EGNN-C+: Interpretable Evolving Granular Neural Network and Application in Classification of Weakly-Supervised EEG Data Streams (Daniel Leite et al., 2024)</a></li></ul></li><li><a href=#csne-3>cs.NE (3)</a><ul><li><a href=#13--267279-single-neuromorphic-memristor-closely-emulates-multiple-synaptic-mechanisms-for-energy-efficient-neural-networks-christoph-weilenmann-et-al-2024>(1/3 | 267/279) Single Neuromorphic Memristor closely Emulates Multiple Synaptic Mechanisms for Energy Efficient Neural Networks (Christoph Weilenmann et al., 2024)</a></li><li><a href=#23--268279-exploratory-landscape-analysis-for-mixed-variable-problems-raphael-patrick-prager-et-al-2024>(2/3 | 268/279) Exploratory Landscape Analysis for Mixed-Variable Problems (Raphael Patrick Prager et al., 2024)</a></li><li><a href=#33--269279-performance-comparison-of-surrogate-assisted-evolutionary-algorithms-on-computational-fluid-dynamics-problems-jakub-kudela-et-al-2024>(3/3 | 269/279) Performance Comparison of Surrogate-Assisted Evolutionary Algorithms on Computational Fluid Dynamics Problems (Jakub Kudela et al., 2024)</a></li></ul></li><li><a href=#cssi-1>cs.SI (1)</a><ul><li><a href=#11--270279-two-stage-information-spreading-evolution-on-the-control-role-of-announcements-jinhu-ren-et-al-2024>(1/1 | 270/279) Two-stage Information Spreading Evolution on The Control Role of Announcements (Jinhu Ren et al., 2024)</a></li></ul></li><li><a href=#cond-matsupr-con-1>cond-mat.supr-con (1)</a><ul><li><a href=#11--271279-scalable-superconductor-neuron-with-ternary-synaptic-connections-for-ultra-fast-snn-hardware-mustafa-altay-karamuftuoglu-et-al-2024>(1/1 | 271/279) Scalable Superconductor Neuron with Ternary Synaptic Connections for Ultra-Fast SNN Hardware (Mustafa Altay Karamuftuoglu et al., 2024)</a></li></ul></li><li><a href=#csni-1>cs.NI (1)</a><ul><li><a href=#11--272279-autonomous-integration-of-tsn-unaware-applications-with-qos-requirements-in-tsn-networks-moritz-fluechter-et-al-2024>(1/1 | 272/279) Autonomous Integration of TSN-unaware Applications with QoS Requirements in TSN Networks (Moritz Fluechter et al., 2024)</a></li></ul></li><li><a href=#mathoc-1>math.OC (1)</a><ul><li><a href=#11--273279-event-triggered-parameterized-control-of-nonlinear-systems-anusree-rajan-et-al-2024>(1/1 | 273/279) Event-Triggered Parameterized Control of Nonlinear Systems (Anusree Rajan et al., 2024)</a></li></ul></li><li><a href=#q-bionc-1>q-bio.NC (1)</a><ul><li><a href=#11--274279-neural-population-geometry-and-optimal-coding-of-tasks-with-shared-latent-structure-albert-j-wakhloo-et-al-2024>(1/1 | 274/279) Neural Population Geometry and Optimal Coding of Tasks with Shared Latent Structure (Albert J. Wakhloo et al., 2024)</a></li></ul></li><li><a href=#csfl-2>cs.FL (2)</a><ul><li><a href=#12--275279-tree-verifiable-graph-grammars-mark-chimes-et-al-2024>(1/2 | 275/279) Tree-Verifiable Graph Grammars (Mark Chimes et al., 2024)</a></li><li><a href=#22--276279-on-the-complexity-of-initial-and-final-state-opacity-for-discrete-event-systems-tomáš-masopust-et-al-2024>(2/2 | 276/279) On the Complexity of Initial-and-Final-State Opacity for Discrete Event Systems (Tomáš Masopust et al., 2024)</a></li></ul></li><li><a href=#physicsoptics-1>physics.optics (1)</a><ul><li><a href=#11--277279-photonic-neural-network-fabricated-on-thin-film-lithium-niobate-for-high-fidelity-and-power-efficient-matrix-computation-yong-zheng-et-al-2024>(1/1 | 277/279) Photonic Neural Network Fabricated on Thin Film Lithium Niobate for High-Fidelity and Power-Efficient Matrix Computation (Yong Zheng et al., 2024)</a></li></ul></li><li><a href=#csdm-1>cs.DM (1)</a><ul><li><a href=#11--278279-a-note-on-solving-basic-equations-over-the-semiring-of-functional-digraphs-alberto-dennunzio-et-al-2024>(1/1 | 278/279) A note on solving basic equations over the semiring of functional digraphs (Alberto Dennunzio et al., 2024)</a></li></ul></li><li><a href=#cslo-1>cs.LO (1)</a><ul><li><a href=#11--279279-equational-bit-vector-solving-via-strong-gröbner-bases-jiaxin-song-et-al-2024>(1/1 | 279/279) Equational Bit-Vector Solving via Strong Gröbner Bases (Jiaxin Song et al., 2024)</a></li></ul></li></ul></nav></div></div></section></div><footer id=footer class="container-fluid text-center align-content-center footer pb-2"><div class="container pt-5"><div class="row text-left"><div class="col-md-4 col-sm-12"><h5>Navigation</h5><ul><li class=nav-item><a class=smooth-scroll href=https://akitenkrad.github.io/akitenkrad-blog/#about>About</a></li><li class=nav-item><a class=smooth-scroll href=https://akitenkrad.github.io/akitenkrad-blog/#recent-posts>Recent Posts</a></li></ul></div><div class="col-md-4 col-sm-12"><h5>Contact me:</h5><ul><li><a href=mailto:contact.to.akitenkrad@gmail.com target=_blank rel=noopener><span><i class="fas fa-envelope"></i></span> <span>contact.to.akitenkrad@gmail.com</span></a></li></ul></div></div></div><hr><div class=container><div class="row text-left"><div class=col-md-4><a id=theme href=https://github.com/hossainemruz/toha target=_blank rel=noopener><img src=/akitenkrad-blog/images/theme-logo_hu8376fd15465fef26ffe66b6bcf0ca686_13669_32x0_resize_box_3.png alt="Toha Theme Logo">
Toha</a></div><div class="col-md-4 text-center">© 2020 Akitenkrad.</div><div class="col-md-4 text-right"><a id=hugo href=https://gohugo.io/ target=_blank rel=noopener>Powered by
<img src=/akitenkrad-blog/images/hugo-logo.svg alt="Hugo Logo" height=18></a></div></div></div></footer><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/jquery-3.4.1.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/popper.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/bootstrap.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/navbar.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/plyr.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/main.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js></script><script src=https://akitenkrad.github.io/akitenkrad-blog/js/single.js></script><script>hljs.initHighlightingOnLoad()</script><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/katex/katex.min.css><script type=text/javascript defer src=https://akitenkrad.github.io/akitenkrad-blog/katex/katex.min.js></script><script type=text/javascript defer src=https://akitenkrad.github.io/akitenkrad-blog/katex/auto-render.min.js></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\[",right:"\\]",display:!0},{left:"\\(",right:"\\)",display:!1}],throwOnError:!0})})</script></body></html>