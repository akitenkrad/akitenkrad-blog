<!doctype html><html><head><title>arXiv @ 2024.02.15</title>
<meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/bootstrap.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/layouts/main.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/navigators/navbar.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/plyr.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/flag-icon.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/custom/style.css><link href=https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css rel=stylesheet integrity=sha384-9ndCyUaIbzAi2FUVXJi0CjmCapSmO7SnpJef0486qhLnuZ2cdeRhO02iuK6FUUVM crossorigin=anonymous><script async src=https://cdn.jsdelivr.net/npm/es-module-shims@1/dist/es-module-shims.min.js crossorigin=anonymous></script><script type=importmap>
{
  "imports": {
    "@popperjs/core": "https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.8/dist/esm/popper.min.js",
    "bootstrap": "https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.esm.min.js"
  }
}
</script><script src=https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js integrity=sha384-geWF76RCwLtnZ8qwWowPQNguL3RmwHVBC9FhGdlKrxdiJJigb/j/68SIy3Te4Bkz crossorigin=anonymous></script><script type=module src=https://akitenkrad.github.io/akitenkrad-blog/js/custom/custom.js></script><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Muli:wght@300;400;500;600"><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/fontawesome/css/all.min.css><link rel=icon type=image/png href=https://akitenkrad.github.io/akitenkrad-blog/images/favicons/favicon-96x96_huf1ee13f0caf27d1547f91fb46207d708_13005_42x0_resize_box_3.png><meta property="og:title" content="arXiv @ 2024.02.15"><meta property="og:description" content="Primary Categories cs.AI (27) cs.CE (1) cs.CG (1) cs.CL (34) cs.CR (5) cs.CV (37) cs.CY (2) cs.DB (2) cs.DC (1) cs.DL (2) cs.DM (1) cs.DS (5) cs.GT (5) cs.HC (8) cs.IR (6) cs.IT (7) cs.LG (52) cs.MA (1) cs.NE (1) cs.NI (1) cs.RO (9) cs.SE (9) econ.GN (1) eess.AS (3) eess.IV (5) eess.SP (1) eess.SY (3) math.CO (1) math.OC (1) physics.chem-ph (1) q-bio.BM (1) q-bio.GN (1) q-fin.TR (1) quant-ph (4) stat."><meta property="og:type" content="article"><meta property="og:url" content="https://akitenkrad.github.io/akitenkrad-blog/posts/arxiv/202402/20240215000000/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-02-15T00:00:00+00:00"><meta property="article:modified_time" content="2024-02-15T00:00:00+00:00"><meta name=description content="arXiv @ 2024.02.15"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/atom-one-dark.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/layouts/single.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/navigators/sidebar.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/style.css><script type=text/javascript src=//ajax.googleapis.com/ajax/libs/jquery/3.1.1/jquery.min.js></script><script language=JavaScript>$(document).ready(function(){$("a[href^='http']:not([href*='"+location.hostname+"'])").attr("target","_blank")})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-1MYYZQG0WE"></script><script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-1MYYZQG0WE",{anonymize_ip:!1})}</script></head><body data-spy=scroll data-target=#TableOfContents data-offset=80><div class="container-fluid bg-dimmed wrapper"><nav class="navbar navbar-expand-xl top-navbar final-navbar shadow" style=position:fixed><div class=container><button class="navbar-toggler navbar-light navbar-for-sidebar" id=sidebar-toggler type=button onclick=toggleSidebar()>
<span class=navbar-toggler-icon></span>
</button>
<button class="navbar-toggler navbar-light" id=toc-toggler type=button onclick=toggleTOC()>
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse lang-selector" id=top-nav-items><ul class="navbar-nav mr-auto"><li class=nav-item><a class=navbar-brand href=/akitenkrad-blog><img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png alt=Logo>
Akitenkrad's Blog</a></li></ul><ul class="navbar-nav ml-auto"><li class=nav-item><a class=nav-link href=/akitenkrad-blog#home>Home</a></li><li class=nav-item><a class=nav-link href=/akitenkrad-blog#about>About</a></li><li class=nav-item><a class=nav-link href=/akitenkrad-blog#recent-posts>Recent Posts</a></li><li class=nav-item><a class=nav-link id=blog-link href=https://akitenkrad.github.io/akitenkrad-blog/posts>Posts</a></li><li class=nav-item><a class=nav-link id=tag-link href=https://akitenkrad.github.io/akitenkrad-blog/tags>Tags</a></li></ul></div></div><img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png class=d-none id=main-logo alt=Logo>
<img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png class=d-none id=inverted-logo alt="Inverted Logo"></nav><section class=sidebar-section id=sidebar-section><div class=sidebar-holder><div class=sidebar id=sidebar><form class=mx-auto method=get action=/akitenkrad-blog/search><input type=text name=keyword placeholder=Search data-search id=search-box></form><div class=sidebar-tree><ul class=tree id=tree><li id=list-heading><a href=/posts data-filter=all>Posts</a></li><div class=subtree><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/>Papers</a><ul><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202205/>2022.05</a><ul><li><a href=/akitenkrad-blog/posts/papers/202205/20220518224923/ title="A Context-Aware Citation Recommendation Model with BERT and Graph Convolutional Networks">A Context-Aware Citation Recommendation Model with BERT and Graph Convolutional Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220508162318/ title="A Deep Cascade Model for Multi-Document Reading Comprehension">A Deep Cascade Model for Multi-Document Reading Comprehension</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220506021208/ title="A Primer in BERTology: What We Know About How BERT Works">A Primer in BERTology: What We Know About How BERT Works</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220529131339/ title="Attention Is All You Need">Attention Is All You Need</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220503010000/ title="DeBERTa: Decoding-Enhanced BERT with Disentangled Attention">DeBERTa: Decoding-Enhanced BERT with Disentangled Attention</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220505222900/ title="Dense Passage Retrieval for Open-Domain Question Answering">Dense Passage Retrieval for Open-Domain Question Answering</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220511010217/ title="Multi-Style Generative Reading Comprehension">Multi-Style Generative Reading Comprehension</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220530102936/ title="Neural Machine Translation of Rare Words with Subword Units">Neural Machine Translation of Rare Words with Subword Units</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220523223206/ title="RoBERTa: A Robustly Optimized BERT Pretraining Approach">RoBERTa: A Robustly Optimized BERT Pretraining Approach</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220520124748/ title="Semi-Supervised Classification with Graph Convolutional Networks">Semi-Supervised Classification with Graph Convolutional Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220509110738/ title="Survey on graph embeddings and their applications to machine learning problems on graphs">Survey on graph embeddings and their applications to machine learning problems on graphs</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220514151839/ title="UnitedQA: A Hybrid Approach for Open Domain Question Answering">UnitedQA: A Hybrid Approach for Open Domain Question Answering</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202206/>2022.06</a><ul><li><a href=/akitenkrad-blog/posts/papers/202206/20220612105422/ title="Attributed Network Embedding for Learning in a Dynamic Environment">Attributed Network Embedding for Learning in a Dynamic Environment</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220608085622/ title="CodeBERT: A Pre-Trained Model for Programming and Natural Languages">CodeBERT: A Pre-Trained Model for Programming and Natural Languages</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220618223844/ title="High-order Proximity Preserved Embedding for Dynamic Networks">High-order Proximity Preserved Embedding for Dynamic Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220602171700/ title="S-Net: From Answer Extraction to Answer Generation for Machine Reading Comprehension">S-Net: From Answer Extraction to Answer Generation for Machine Reading Comprehension</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202207/>2022.07</a><ul><li><a href=/akitenkrad-blog/posts/papers/202207/20220727145036/ title="Dynamic Heterogeneous Graph Embedding Using Hierarchical Attentions">Dynamic Heterogeneous Graph Embedding Using Hierarchical Attentions</a></li><li><a href=/akitenkrad-blog/posts/papers/202207/20220726163444/ title="Dynamic Network Embedding Survey">Dynamic Network Embedding Survey</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202208/>2022.08</a><ul><li><a href=/akitenkrad-blog/posts/papers/202208/20220802103319/ title="Modeling Dynamic Heterogeneous Network for Link Prediction using Hierarchical Attention with Temporal RNN">Modeling Dynamic Heterogeneous Network for Link Prediction using Hierarchical Attention with Temporal RNN</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202209/>2022.09</a><ul><li><a href=/akitenkrad-blog/posts/papers/202209/20220909180827/ title="Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting">Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202305/>2023.05</a><ul><li><a href=/akitenkrad-blog/posts/papers/202305/20230513094804/ title="Improving Language Understanding by Generative Pre-Training">Improving Language Understanding by Generative Pre-Training</a></li><li><a href=/akitenkrad-blog/posts/papers/202305/20230514200241/ title="Language Models are Unsupervised Multitask Learners">Language Models are Unsupervised Multitask Learners</a></li></ul></li><li><a href=/akitenkrad-blog/posts/papers/202306/ title=2023.06>2023.06</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/akitenkrad-blog/posts/arxiv/>arXiv</a><ul class=active><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202307/>2023.07</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230701000000/ title="arXiv @ 2023.07.01">arXiv @ 2023.07.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230702000000/ title="arXiv @ 2023.07.02">arXiv @ 2023.07.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230703000000/ title="arXiv @ 2023.07.03">arXiv @ 2023.07.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230704000000/ title="arXiv @ 2023.07.04">arXiv @ 2023.07.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230705000000/ title="arXiv @ 2023.07.05">arXiv @ 2023.07.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230706000000/ title="arXiv @ 2023.07.06">arXiv @ 2023.07.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230707000000/ title="arXiv @ 2023.07.07">arXiv @ 2023.07.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230708000000/ title="arXiv @ 2023.07.08">arXiv @ 2023.07.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230709000000/ title="arXiv @ 2023.07.09">arXiv @ 2023.07.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230710000000/ title="arXiv @ 2023.07.10">arXiv @ 2023.07.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230711000000/ title="arXiv @ 2023.07.11">arXiv @ 2023.07.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230712000000/ title="arXiv @ 2023.07.12">arXiv @ 2023.07.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230713000000/ title="arXiv @ 2023.07.13">arXiv @ 2023.07.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230714000000/ title="arXiv @ 2023.07.14">arXiv @ 2023.07.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230715000000/ title="arXiv @ 2023.07.15">arXiv @ 2023.07.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230716000000/ title="arXiv @ 2023.07.16">arXiv @ 2023.07.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230717000000/ title="arXiv @ 2023.07.17">arXiv @ 2023.07.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230718000000/ title="arXiv @ 2023.07.18">arXiv @ 2023.07.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230719000000/ title="arXiv @ 2023.07.19">arXiv @ 2023.07.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230720000000/ title="arXiv @ 2023.07.20">arXiv @ 2023.07.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230721000000/ title="arXiv @ 2023.07.21">arXiv @ 2023.07.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230722000000/ title="arXiv @ 2023.07.22">arXiv @ 2023.07.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230723000000/ title="arXiv @ 2023.07.23">arXiv @ 2023.07.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230724000000/ title="arXiv @ 2023.07.24">arXiv @ 2023.07.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230725000000/ title="arXiv @ 2023.07.25">arXiv @ 2023.07.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230726000000/ title="arXiv @ 2023.07.26">arXiv @ 2023.07.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230727000000/ title="arXiv @ 2023.07.27">arXiv @ 2023.07.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230728000000/ title="arXiv @ 2023.07.28">arXiv @ 2023.07.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230729000000/ title="arXiv @ 2023.07.29">arXiv @ 2023.07.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230730000000/ title="arXiv @ 2023.07.30">arXiv @ 2023.07.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230731000000/ title="arXiv @ 2023.07.31">arXiv @ 2023.07.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202308/>2023.08</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230801000000/ title="arXiv @ 2023.08.01">arXiv @ 2023.08.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230802000000/ title="arXiv @ 2023.08.02">arXiv @ 2023.08.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230803000000/ title="arXiv @ 2023.08.03">arXiv @ 2023.08.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230804000000/ title="arXiv @ 2023.08.04">arXiv @ 2023.08.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230805000000/ title="arXiv @ 2023.08.05">arXiv @ 2023.08.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230806000000/ title="arXiv @ 2023.08.06">arXiv @ 2023.08.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230807000000/ title="arXiv @ 2023.08.07">arXiv @ 2023.08.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230808000000/ title="arXiv @ 2023.08.08">arXiv @ 2023.08.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230809000000/ title="arXiv @ 2023.08.09">arXiv @ 2023.08.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230810000000/ title="arXiv @ 2023.08.10">arXiv @ 2023.08.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230811000000/ title="arXiv @ 2023.08.11">arXiv @ 2023.08.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230812000000/ title="arXiv @ 2023.08.12">arXiv @ 2023.08.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230813000000/ title="arXiv @ 2023.08.13">arXiv @ 2023.08.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230814000000/ title="arXiv @ 2023.08.14">arXiv @ 2023.08.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230815000000/ title="arXiv @ 2023.08.15">arXiv @ 2023.08.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230816000000/ title="arXiv @ 2023.08.16">arXiv @ 2023.08.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230817000000/ title="arXiv @ 2023.08.17">arXiv @ 2023.08.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230818000000/ title="arXiv @ 2023.08.18">arXiv @ 2023.08.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230819000000/ title="arXiv @ 2023.08.19">arXiv @ 2023.08.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230820000000/ title="arXiv @ 2023.08.20">arXiv @ 2023.08.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230821000000/ title="arXiv @ 2023.08.21">arXiv @ 2023.08.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230822000000/ title="arXiv @ 2023.08.22">arXiv @ 2023.08.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230823000000/ title="arXiv @ 2023.08.23">arXiv @ 2023.08.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230824000000/ title="arXiv @ 2023.08.24">arXiv @ 2023.08.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230825000000/ title="arXiv @ 2023.08.25">arXiv @ 2023.08.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230826000000/ title="arXiv @ 2023.08.26">arXiv @ 2023.08.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230827000000/ title="arXiv @ 2023.08.27">arXiv @ 2023.08.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230828000000/ title="arXiv @ 2023.08.28">arXiv @ 2023.08.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230829000000/ title="arXiv @ 2023.08.29">arXiv @ 2023.08.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230830000000/ title="arXiv @ 2023.08.30">arXiv @ 2023.08.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230831000000/ title="arXiv @ 2023.08.31">arXiv @ 2023.08.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202309/>2023.09</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230901000000/ title="arXiv @ 2023.09.01">arXiv @ 2023.09.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230902000000/ title="arXiv @ 2023.09.02">arXiv @ 2023.09.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230903000000/ title="arXiv @ 2023.09.03">arXiv @ 2023.09.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230904000000/ title="arXiv @ 2023.09.04">arXiv @ 2023.09.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230905000000/ title="arXiv @ 2023.09.05">arXiv @ 2023.09.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230906000000/ title="arXiv @ 2023.09.06">arXiv @ 2023.09.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230907000000/ title="arXiv @ 2023.09.07">arXiv @ 2023.09.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230908000000/ title="arXiv @ 2023.09.08">arXiv @ 2023.09.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230909000000/ title="arXiv @ 2023.09.09">arXiv @ 2023.09.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230910000000/ title="arXiv @ 2023.09.10">arXiv @ 2023.09.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230911000000/ title="arXiv @ 2023.09.11">arXiv @ 2023.09.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230912000000/ title="arXiv @ 2023.09.12">arXiv @ 2023.09.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230913000000/ title="arXiv @ 2023.09.13">arXiv @ 2023.09.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230914000000/ title="arXiv @ 2023.09.14">arXiv @ 2023.09.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230915000000/ title="arXiv @ 2023.09.15">arXiv @ 2023.09.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230916000000/ title="arXiv @ 2023.09.16">arXiv @ 2023.09.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230917000000/ title="arXiv @ 2023.09.17">arXiv @ 2023.09.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230918000000/ title="arXiv @ 2023.09.18">arXiv @ 2023.09.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230919000000/ title="arXiv @ 2023.09.19">arXiv @ 2023.09.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230920000000/ title="arXiv @ 2023.09.20">arXiv @ 2023.09.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230921000000/ title="arXiv @ 2023.09.21">arXiv @ 2023.09.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230922000000/ title="arXiv @ 2023.09.22">arXiv @ 2023.09.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230923000000/ title="arXiv @ 2023.09.23">arXiv @ 2023.09.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230924000000/ title="arXiv @ 2023.09.24">arXiv @ 2023.09.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230925000000/ title="arXiv @ 2023.09.25">arXiv @ 2023.09.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230926000000/ title="arXiv @ 2023.09.26">arXiv @ 2023.09.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230927000000/ title="arXiv @ 2023.09.27">arXiv @ 2023.09.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230928000000/ title="arXiv @ 2023.09.28">arXiv @ 2023.09.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230929000000/ title="arXiv @ 2023.09.29">arXiv @ 2023.09.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230930000000/ title="arXiv @ 2023.09.30">arXiv @ 2023.09.30</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202310/>2023.1</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231001000000/ title="arXiv @ 2023.10.01">arXiv @ 2023.10.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231002000000/ title="arXiv @ 2023.10.02">arXiv @ 2023.10.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231003000000/ title="arXiv @ 2023.10.03">arXiv @ 2023.10.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231004000000/ title="arXiv @ 2023.10.04">arXiv @ 2023.10.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231005000000/ title="arXiv @ 2023.10.05">arXiv @ 2023.10.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231006000000/ title="arXiv @ 2023.10.06">arXiv @ 2023.10.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231007000000/ title="arXiv @ 2023.10.07">arXiv @ 2023.10.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231008000000/ title="arXiv @ 2023.10.08">arXiv @ 2023.10.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231009000000/ title="arXiv @ 2023.10.09">arXiv @ 2023.10.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231010000000/ title="arXiv @ 2023.10.10">arXiv @ 2023.10.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231011000000/ title="arXiv @ 2023.10.11">arXiv @ 2023.10.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231012000000/ title="arXiv @ 2023.10.12">arXiv @ 2023.10.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231013000000/ title="arXiv @ 2023.10.13">arXiv @ 2023.10.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231014000000/ title="arXiv @ 2023.10.14">arXiv @ 2023.10.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231015000000/ title="arXiv @ 2023.10.15">arXiv @ 2023.10.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231016000000/ title="arXiv @ 2023.10.16">arXiv @ 2023.10.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231017000000/ title="arXiv @ 2023.10.17">arXiv @ 2023.10.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231018000000/ title="arXiv @ 2023.10.18">arXiv @ 2023.10.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231019000000/ title="arXiv @ 2023.10.19">arXiv @ 2023.10.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231020000000/ title="arXiv @ 2023.10.20">arXiv @ 2023.10.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231021000000/ title="arXiv @ 2023.10.21">arXiv @ 2023.10.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231022000000/ title="arXiv @ 2023.10.22">arXiv @ 2023.10.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231023000000/ title="arXiv @ 2023.10.23">arXiv @ 2023.10.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231024000000/ title="arXiv @ 2023.10.24">arXiv @ 2023.10.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231025000000/ title="arXiv @ 2023.10.25">arXiv @ 2023.10.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231026000000/ title="arXiv @ 2023.10.26">arXiv @ 2023.10.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231027000000/ title="arXiv @ 2023.10.27">arXiv @ 2023.10.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231028000000/ title="arXiv @ 2023.10.28">arXiv @ 2023.10.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231029000000/ title="arXiv @ 2023.10.29">arXiv @ 2023.10.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231030000000/ title="arXiv @ 2023.10.30">arXiv @ 2023.10.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231031000000/ title="arXiv @ 2023.10.31">arXiv @ 2023.10.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202311/>2023.11</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231101000000/ title="arXiv @ 2023.11.01">arXiv @ 2023.11.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231102000000/ title="arXiv @ 2023.11.02">arXiv @ 2023.11.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231103000000/ title="arXiv @ 2023.11.03">arXiv @ 2023.11.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231104000000/ title="arXiv @ 2023.11.04">arXiv @ 2023.11.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231105000000/ title="arXiv @ 2023.11.05">arXiv @ 2023.11.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231106000000/ title="arXiv @ 2023.11.06">arXiv @ 2023.11.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231107000000/ title="arXiv @ 2023.11.07">arXiv @ 2023.11.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231108000000/ title="arXiv @ 2023.11.08">arXiv @ 2023.11.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231109000000/ title="arXiv @ 2023.11.09">arXiv @ 2023.11.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231110000000/ title="arXiv @ 2023.11.10">arXiv @ 2023.11.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231111000000/ title="arXiv @ 2023.11.11">arXiv @ 2023.11.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231112000000/ title="arXiv @ 2023.11.12">arXiv @ 2023.11.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231113000000/ title="arXiv @ 2023.11.13">arXiv @ 2023.11.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231114000000/ title="arXiv @ 2023.11.14">arXiv @ 2023.11.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231115000000/ title="arXiv @ 2023.11.15">arXiv @ 2023.11.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231116000000/ title="arXiv @ 2023.11.16">arXiv @ 2023.11.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231117000000/ title="arXiv @ 2023.11.17">arXiv @ 2023.11.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231118000000/ title="arXiv @ 2023.11.18">arXiv @ 2023.11.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231119000000/ title="arXiv @ 2023.11.19">arXiv @ 2023.11.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231120000000/ title="arXiv @ 2023.11.20">arXiv @ 2023.11.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231121000000/ title="arXiv @ 2023.11.21">arXiv @ 2023.11.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231122000000/ title="arXiv @ 2023.11.22">arXiv @ 2023.11.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231123000000/ title="arXiv @ 2023.11.23">arXiv @ 2023.11.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231124000000/ title="arXiv @ 2023.11.24">arXiv @ 2023.11.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231125000000/ title="arXiv @ 2023.11.25">arXiv @ 2023.11.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231126000000/ title="arXiv @ 2023.11.26">arXiv @ 2023.11.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231127000000/ title="arXiv @ 2023.11.27">arXiv @ 2023.11.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231128000000/ title="arXiv @ 2023.11.28">arXiv @ 2023.11.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231129000000/ title="arXiv @ 2023.11.29">arXiv @ 2023.11.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231130000000/ title="arXiv @ 2023.11.30">arXiv @ 2023.11.30</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202312/>2023.12</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231201000000/ title="arXiv @ 2023.12.01">arXiv @ 2023.12.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231202000000/ title="arXiv @ 2023.12.02">arXiv @ 2023.12.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231203000000/ title="arXiv @ 2023.12.03">arXiv @ 2023.12.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231204000000/ title="arXiv @ 2023.12.04">arXiv @ 2023.12.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231205000000/ title="arXiv @ 2023.12.05">arXiv @ 2023.12.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231206000000/ title="arXiv @ 2023.12.06">arXiv @ 2023.12.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231207000000/ title="arXiv @ 2023.12.07">arXiv @ 2023.12.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231208000000/ title="arXiv @ 2023.12.08">arXiv @ 2023.12.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231209000000/ title="arXiv @ 2023.12.09">arXiv @ 2023.12.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231210000000/ title="arXiv @ 2023.12.10">arXiv @ 2023.12.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231211000000/ title="arXiv @ 2023.12.11">arXiv @ 2023.12.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231212000000/ title="arXiv @ 2023.12.12">arXiv @ 2023.12.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231213000000/ title="arXiv @ 2023.12.13">arXiv @ 2023.12.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231214000000/ title="arXiv @ 2023.12.14">arXiv @ 2023.12.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231215000000/ title="arXiv @ 2023.12.15">arXiv @ 2023.12.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231216000000/ title="arXiv @ 2023.12.16">arXiv @ 2023.12.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231217000000/ title="arXiv @ 2023.12.17">arXiv @ 2023.12.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231218000000/ title="arXiv @ 2023.12.18">arXiv @ 2023.12.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231219000000/ title="arXiv @ 2023.12.19">arXiv @ 2023.12.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231220000000/ title="arXiv @ 2023.12.20">arXiv @ 2023.12.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231221000000/ title="arXiv @ 2023.12.21">arXiv @ 2023.12.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231222000000/ title="arXiv @ 2023.12.22">arXiv @ 2023.12.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231223000000/ title="arXiv @ 2023.12.23">arXiv @ 2023.12.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231224000000/ title="arXiv @ 2023.12.24">arXiv @ 2023.12.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231225000000/ title="arXiv @ 2023.12.25">arXiv @ 2023.12.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231226000000/ title="arXiv @ 2023.12.26">arXiv @ 2023.12.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231227000000/ title="arXiv @ 2023.12.27">arXiv @ 2023.12.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231228000000/ title="arXiv @ 2023.12.28">arXiv @ 2023.12.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231229000000/ title="arXiv @ 2023.12.29">arXiv @ 2023.12.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231230000000/ title="arXiv @ 2023.12.30">arXiv @ 2023.12.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231231000000/ title="arXiv @ 2023.12.31">arXiv @ 2023.12.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202401/>2024.01</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240101000000/ title="arXiv @ 2024.01.01">arXiv @ 2024.01.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240102000000/ title="arXiv @ 2024.01.02">arXiv @ 2024.01.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240103000000/ title="arXiv @ 2024.01.03">arXiv @ 2024.01.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240104000000/ title="arXiv @ 2024.01.04">arXiv @ 2024.01.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240105000000/ title="arXiv @ 2024.01.05">arXiv @ 2024.01.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240106000000/ title="arXiv @ 2024.01.06">arXiv @ 2024.01.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240107000000/ title="arXiv @ 2024.01.07">arXiv @ 2024.01.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240108000000/ title="arXiv @ 2024.01.08">arXiv @ 2024.01.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240109000000/ title="arXiv @ 2024.01.09">arXiv @ 2024.01.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240110000000/ title="arXiv @ 2024.01.10">arXiv @ 2024.01.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240111000000/ title="arXiv @ 2024.01.11">arXiv @ 2024.01.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240112000000/ title="arXiv @ 2024.01.12">arXiv @ 2024.01.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240113000000/ title="arXiv @ 2024.01.13">arXiv @ 2024.01.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240114000000/ title="arXiv @ 2024.01.14">arXiv @ 2024.01.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240115000000/ title="arXiv @ 2024.01.15">arXiv @ 2024.01.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240116000000/ title="arXiv @ 2024.01.16">arXiv @ 2024.01.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240117000000/ title="arXiv @ 2024.01.17">arXiv @ 2024.01.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240118000000/ title="arXiv @ 2024.01.18">arXiv @ 2024.01.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240119000000/ title="arXiv @ 2024.01.19">arXiv @ 2024.01.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240120000000/ title="arXiv @ 2024.01.20">arXiv @ 2024.01.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240121000000/ title="arXiv @ 2024.01.21">arXiv @ 2024.01.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240122000000/ title="arXiv @ 2024.01.22">arXiv @ 2024.01.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240123000000/ title="arXiv @ 2024.01.23">arXiv @ 2024.01.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240124000000/ title="arXiv @ 2024.01.24">arXiv @ 2024.01.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240125000000/ title="arXiv @ 2024.01.25">arXiv @ 2024.01.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240126000000/ title="arXiv @ 2024.01.26">arXiv @ 2024.01.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240127000000/ title="arXiv @ 2024.01.27">arXiv @ 2024.01.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240128000000/ title="arXiv @ 2024.01.28">arXiv @ 2024.01.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240129000000/ title="arXiv @ 2024.01.29">arXiv @ 2024.01.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240130000000/ title="arXiv @ 2024.01.30">arXiv @ 2024.01.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240131000000/ title="arXiv @ 2024.01.31">arXiv @ 2024.01.31</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/akitenkrad-blog/posts/arxiv/202402/>2024.02</a><ul class=active><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240201000000/ title="arXiv @ 2024.02.01">arXiv @ 2024.02.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240202000000/ title="arXiv @ 2024.02.02">arXiv @ 2024.02.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240203000000/ title="arXiv @ 2024.02.03">arXiv @ 2024.02.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240204000000/ title="arXiv @ 2024.02.04">arXiv @ 2024.02.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240205000000/ title="arXiv @ 2024.02.05">arXiv @ 2024.02.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240206000000/ title="arXiv @ 2024.02.06">arXiv @ 2024.02.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240207000000/ title="arXiv @ 2024.02.07">arXiv @ 2024.02.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240208000000/ title="arXiv @ 2024.02.08">arXiv @ 2024.02.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240209000000/ title="arXiv @ 2024.02.09">arXiv @ 2024.02.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240210000000/ title="arXiv @ 2024.02.10">arXiv @ 2024.02.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240211000000/ title="arXiv @ 2024.02.11">arXiv @ 2024.02.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240212000000/ title="arXiv @ 2024.02.12">arXiv @ 2024.02.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240213000000/ title="arXiv @ 2024.02.13">arXiv @ 2024.02.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240214000000/ title="arXiv @ 2024.02.14">arXiv @ 2024.02.14</a></li><li><a class=active href=/akitenkrad-blog/posts/arxiv/202402/20240215000000/ title="arXiv @ 2024.02.15">arXiv @ 2024.02.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240216000000/ title="arXiv @ 2024.02.16">arXiv @ 2024.02.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240217000000/ title="arXiv @ 2024.02.17">arXiv @ 2024.02.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240218000000/ title="arXiv @ 2024.02.18">arXiv @ 2024.02.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240219000000/ title="arXiv @ 2024.02.19">arXiv @ 2024.02.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240220000000/ title="arXiv @ 2024.02.20">arXiv @ 2024.02.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240221000000/ title="arXiv @ 2024.02.21">arXiv @ 2024.02.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240222000000/ title="arXiv @ 2024.02.22">arXiv @ 2024.02.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240223000000/ title="arXiv @ 2024.02.23">arXiv @ 2024.02.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240224000000/ title="arXiv @ 2024.02.24">arXiv @ 2024.02.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240225000000/ title="arXiv @ 2024.02.25">arXiv @ 2024.02.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240226000000/ title="arXiv @ 2024.02.26">arXiv @ 2024.02.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240227000000/ title="arXiv @ 2024.02.27">arXiv @ 2024.02.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240228000000/ title="arXiv @ 2024.02.28">arXiv @ 2024.02.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240229000000/ title="arXiv @ 2024.02.29">arXiv @ 2024.02.29</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202403/>2024.03</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240301000000/ title="arXiv @ 2024.03.01">arXiv @ 2024.03.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240302000000/ title="arXiv @ 2024.03.02">arXiv @ 2024.03.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240303000000/ title="arXiv @ 2024.03.03">arXiv @ 2024.03.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240304000000/ title="arXiv @ 2024.03.04">arXiv @ 2024.03.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240305000000/ title="arXiv @ 2024.03.05">arXiv @ 2024.03.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240306000000/ title="arXiv @ 2024.03.06">arXiv @ 2024.03.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240307000000/ title="arXiv @ 2024.03.07">arXiv @ 2024.03.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240308000000/ title="arXiv @ 2024.03.08">arXiv @ 2024.03.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240309000000/ title="arXiv @ 2024.03.09">arXiv @ 2024.03.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240310000000/ title="arXiv @ 2024.03.10">arXiv @ 2024.03.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240311000000/ title="arXiv @ 2024.03.11">arXiv @ 2024.03.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240312000000/ title="arXiv @ 2024.03.12">arXiv @ 2024.03.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240313000000/ title="arXiv @ 2024.03.13">arXiv @ 2024.03.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240314000000/ title="arXiv @ 2024.03.14">arXiv @ 2024.03.14</a></li></ul></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/algorithms/>Algorithms</a><ul><li><a href=/akitenkrad-blog/posts/algorithms/bandit-algorithm-basic/ title="Bandit Algorithm Basic">Bandit Algorithm Basic</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/conference/>Conference</a><ul><li><a href=/akitenkrad-blog/posts/conference/acl/ title="Annual Meeting of the Association for Computational Linguistics">Annual Meeting of the Association for Computational Linguistics</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/>Figures</a><ul><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/202208/>2022.08</a><ul><li><a href=/akitenkrad-blog/posts/figures/202208/20220822092748/ title=1ヶ月当たりの実労働時間の推移>1ヶ月当たりの実労働時間の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220813115200/ title=コンビニエンスストアの店舗数の推移>コンビニエンスストアの店舗数の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220821113322/ title=地域別1世帯当たり1ヶ月間の支出額の推移>地域別1世帯当たり1ヶ月間の支出額の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220828111836/ title="夫婦別子供の有無別生活時間 (2016)">夫婦別子供の有無別生活時間 (2016)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220827103615/ title=家族類型の変遷>家族類型の変遷</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220815131016/ title="年齢別大学院入学者数 (2021)">年齢別大学院入学者数 (2021)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220818122628/ title=情報サービス業における企業特殊的人的資本（名目）の推移>情報サービス業における企業特殊的人的資本（名目）の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220824095127/ title=有業者の社会生活における活動の変化>有業者の社会生活における活動の変化</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220814175022/ title=東京23区の乗用車保有台数>東京23区の乗用車保有台数</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220817121117/ title="業種別企業特殊的人的資本 (2018)">業種別企業特殊的人的資本 (2018)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220825094111/ title=無業者の社会生活における活動の変化>無業者の社会生活における活動の変化</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220823104100/ title=睡眠時間の推移>睡眠時間の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220826102652/ title=総人口の推移>総人口の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220820225528/ title=都道府県別国公立別学校数・学生数>都道府県別国公立別学校数・学生数</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220816120845/ title="都道府県別消費者物価指数/教育（全国平均=100）">都道府県別消費者物価指数/教育（全国平均=100）</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220812173814/ title=金沢市の夏の月平均気温の遷移>金沢市の夏の月平均気温の遷移</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/202209/>2022.09</a><ul><li><a href=/akitenkrad-blog/posts/figures/202209/20220907122906/ title="ACL-2022 WordCloud">ACL-2022 WordCloud</a></li></ul></li></ul></li><li><a href=/akitenkrad-blog/posts/latex/ title="Latex Mathematics Syntax Guide">Latex Mathematics Syntax Guide</a></li><li><a href=/akitenkrad-blog/posts/markdown/ title="Markdown Sample">Markdown Sample</a></li></div></ul></div></div></div></section><section class=content-section id=content-section><div class=content><div class="container p-0 read-area"><div class="hero-area col-sm-12" id=hero-area style=background-image:url(/akitenkrad-blog/posts/arxiv/202402/20240215000000/hero.png)></div><div class=page-content><div class="author-profile ml-auto align-self-lg-center"><img class=rounded-circle src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_120x120_fit_box_3.png alt="Author Image"><h5 class=author-name></h5><p>Thursday, Feb 15, 2024</p></div><div class=title><h1>arXiv @ 2024.02.15</h1></div><div class=taxonomy-terms><ul><li class=rounded><a href=/akitenkrad-blog/tags/arxiv class="btn, btn-sm">arXiv</a></li><li class=rounded><a href=/akitenkrad-blog/tags/published2024 class="btn, btn-sm">2024</a></li></ul></div><div class=post-content id=post-content><figure style=border:none;width:100%;display:flex;justify-content:center><iframe src=pie.html width=900 height=620 style=border:none></iframe></figure><h2 id=primary-categories>Primary Categories</h2><ul><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240215000000/#csai-27>cs.AI (27)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240215000000/#csce-1>cs.CE (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240215000000/#cscg-1>cs.CG (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240215000000/#cscl-34>cs.CL (34)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240215000000/#cscr-5>cs.CR (5)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240215000000/#cscv-37>cs.CV (37)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240215000000/#cscy-2>cs.CY (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240215000000/#csdb-2>cs.DB (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240215000000/#csdc-1>cs.DC (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240215000000/#csdl-2>cs.DL (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240215000000/#csdm-1>cs.DM (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240215000000/#csds-5>cs.DS (5)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240215000000/#csgt-5>cs.GT (5)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240215000000/#cshc-8>cs.HC (8)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240215000000/#csir-6>cs.IR (6)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240215000000/#csit-7>cs.IT (7)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240215000000/#cslg-52>cs.LG (52)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240215000000/#csma-1>cs.MA (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240215000000/#csne-1>cs.NE (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240215000000/#csni-1>cs.NI (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240215000000/#csro-9>cs.RO (9)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240215000000/#csse-9>cs.SE (9)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240215000000/#econgn-1>econ.GN (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240215000000/#eessas-3>eess.AS (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240215000000/#eessiv-5>eess.IV (5)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240215000000/#eesssp-1>eess.SP (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240215000000/#eesssy-3>eess.SY (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240215000000/#mathco-1>math.CO (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240215000000/#mathoc-1>math.OC (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240215000000/#physicschem-ph-1>physics.chem-ph (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240215000000/#q-biobm-1>q-bio.BM (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240215000000/#q-biogn-1>q-bio.GN (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240215000000/#q-fintr-1>q-fin.TR (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240215000000/#quant-ph-4>quant-ph (4)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240215000000/#statme-1>stat.ME (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240215000000/#statml-6>stat.ML (6)</a></li></ul><h2 id=keywords>Keywords</h2><table border=1 class=dataframe><thead><tr style=text-align:right><th>keyword</th><th>cs.AI</th><th>cs.CL</th><th>cs.CV</th><th>cs.LG</th></tr></thead><tbody><tr><td>Adversarial Attack</td><td></td><td>1</td><td>1</td><td>1</td></tr><tr><td>Adversarial Learning</td><td></td><td></td><td>1</td><td>1</td></tr><tr><td>Anomaly Detection</td><td></td><td></td><td></td><td>1</td></tr><tr><td>Autoencoder</td><td></td><td></td><td>1</td><td>1</td></tr><tr><td>Automatic Speech Recognition</td><td></td><td>4</td><td></td><td></td></tr><tr><td>BERT</td><td>1</td><td></td><td></td><td>1</td></tr><tr><td>BLOOM</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Bandit Algorithm</td><td></td><td></td><td>1</td><td>5</td></tr><tr><td>Benchmarking</td><td>4</td><td>13</td><td>12</td><td>8</td></tr><tr><td>Black Box</td><td>1</td><td>6</td><td></td><td>5</td></tr><tr><td>ChatGPT</td><td>1</td><td>2</td><td></td><td></td></tr><tr><td>Chatbot</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Clustering</td><td>2</td><td></td><td></td><td>1</td></tr><tr><td>Code Generation</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Cohere</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Continual Learning</td><td></td><td></td><td>1</td><td>1</td></tr><tr><td>Contrastive Learning</td><td></td><td></td><td>3</td><td>1</td></tr><tr><td>ControlNet</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Convolution</td><td></td><td></td><td>2</td><td></td></tr><tr><td>Convolutional Neural Network</td><td></td><td></td><td>7</td><td></td></tr><tr><td>Counter-factual</td><td>1</td><td></td><td></td><td>2</td></tr><tr><td>Data Augmentation</td><td>1</td><td>1</td><td></td><td></td></tr><tr><td>Disambiguation</td><td></td><td></td><td></td><td>1</td></tr><tr><td>Distributional Reinforcement Learning</td><td></td><td></td><td></td><td>1</td></tr><tr><td>Document Classification</td><td></td><td></td><td></td><td>1</td></tr><tr><td>Domain Adaptation</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Fairness</td><td>3</td><td></td><td></td><td>1</td></tr><tr><td>Fake News Detection</td><td></td><td></td><td></td><td>2</td></tr><tr><td>Federated Learning</td><td></td><td></td><td></td><td>2</td></tr><tr><td>Fine-tuning</td><td>1</td><td>10</td><td>5</td><td>4</td></tr><tr><td>Foundation Model</td><td></td><td>1</td><td></td><td>1</td></tr><tr><td>GPT</td><td>2</td><td>6</td><td>1</td><td>3</td></tr><tr><td>GPT-2</td><td></td><td>1</td><td></td><td></td></tr><tr><td>GPT-3</td><td></td><td>3</td><td></td><td>2</td></tr><tr><td>GPT-3.5</td><td></td><td>3</td><td></td><td>2</td></tr><tr><td>GPT-4</td><td>3</td><td>5</td><td></td><td>1</td></tr><tr><td>Gemini</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Generative AI</td><td></td><td></td><td>1</td><td>1</td></tr><tr><td>Generative Adversarial Network</td><td></td><td></td><td>2</td><td>2</td></tr><tr><td>Geometry</td><td>1</td><td></td><td>3</td><td></td></tr><tr><td>Graph</td><td>2</td><td>1</td><td>1</td><td>18</td></tr><tr><td>Graph Attention Networks</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Graph Embedding</td><td></td><td></td><td></td><td>2</td></tr><tr><td>Graph Neural Network</td><td></td><td></td><td></td><td>25</td></tr><tr><td>Grounding</td><td></td><td></td><td>1</td><td>1</td></tr><tr><td>High-Resource</td><td></td><td></td><td></td><td>1</td></tr><tr><td>Image2text</td><td></td><td></td><td>1</td><td></td></tr><tr><td>In-context Learning</td><td>1</td><td>1</td><td></td><td>2</td></tr><tr><td>Information Retrieval</td><td></td><td>1</td><td>1</td><td></td></tr><tr><td>Instruction Tuning</td><td></td><td>2</td><td>1</td><td></td></tr><tr><td>Knowledge Distillation</td><td></td><td>1</td><td>1</td><td>2</td></tr><tr><td>Knowledge Graph</td><td>1</td><td></td><td></td><td>1</td></tr><tr><td>Knowledge Transfer</td><td>1</td><td></td><td></td><td></td></tr><tr><td>LLaMA</td><td>1</td><td>2</td><td></td><td>1</td></tr><tr><td>Large Language Model</td><td>12</td><td>38</td><td>3</td><td>10</td></tr><tr><td>Low-Resource</td><td></td><td></td><td>1</td><td></td></tr><tr><td>MNIST</td><td></td><td></td><td></td><td>1</td></tr><tr><td>Machine Unlearning</td><td></td><td></td><td></td><td>1</td></tr><tr><td>Markov Decision Process</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Message-Passing</td><td></td><td></td><td></td><td>4</td></tr><tr><td>Meta Learning</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Mistral</td><td></td><td>1</td><td></td><td>1</td></tr><tr><td>Model Pruning</td><td></td><td></td><td></td><td>1</td></tr><tr><td>Multi-modal</td><td>4</td><td>8</td><td>5</td><td>2</td></tr><tr><td>Named Entity Recognition</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Natural Language Generation</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Natural Language Inference</td><td></td><td>3</td><td></td><td></td></tr><tr><td>Natural Language Understanding</td><td></td><td>2</td><td></td><td></td></tr><tr><td>Node Classification</td><td></td><td></td><td></td><td>1</td></tr><tr><td>Node Embedding</td><td></td><td></td><td></td><td>1</td></tr><tr><td>Object Detection</td><td></td><td></td><td>3</td><td></td></tr><tr><td>Open Information Extraction</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Out-of-distribution</td><td></td><td>2</td><td></td><td>3</td></tr><tr><td>Out-of-domain</td><td></td><td>2</td><td></td><td></td></tr><tr><td>PaLM</td><td>1</td><td>1</td><td></td><td></td></tr><tr><td>Parameter Sharing</td><td></td><td></td><td></td><td>1</td></tr><tr><td>Pre-trained Language Model</td><td></td><td>3</td><td></td><td>1</td></tr><tr><td>Probabilistic Model</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Prompt</td><td>2</td><td>7</td><td>6</td><td>2</td></tr><tr><td>Pruning</td><td></td><td></td><td></td><td>1</td></tr><tr><td>Quantization</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Question Answering</td><td></td><td>6</td><td>4</td><td>1</td></tr><tr><td>Reasoning</td><td>5</td><td>3</td><td></td><td>2</td></tr><tr><td>Recommendation</td><td>2</td><td></td><td>1</td><td>2</td></tr><tr><td>Recurrent Neural Network</td><td></td><td></td><td></td><td>3</td></tr><tr><td>Reinforcement Learning</td><td>3</td><td></td><td></td><td>10</td></tr><tr><td>Representation Learning</td><td></td><td>1</td><td>1</td><td>2</td></tr><tr><td>Sample Size</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Scaling Law</td><td></td><td></td><td></td><td>1</td></tr><tr><td>Selective Prediction</td><td></td><td></td><td></td><td>1</td></tr><tr><td>Self-Attention</td><td>1</td><td></td><td></td><td>1</td></tr><tr><td>Self-supervised Learning</td><td></td><td>2</td><td>2</td><td></td></tr><tr><td>Semantic Parsing</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Simulation</td><td>1</td><td></td><td></td><td>1</td></tr><tr><td>Simulator</td><td>1</td><td></td><td></td><td>1</td></tr><tr><td>Stochastic Gradient Descent</td><td></td><td>1</td><td>2</td><td></td></tr><tr><td>Summarization</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Supervised Learning</td><td></td><td>1</td><td>4</td><td>5</td></tr><tr><td>T5</td><td></td><td></td><td></td><td>1</td></tr><tr><td>Text Embedding</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Text Generation</td><td></td><td></td><td>1</td><td>1</td></tr><tr><td>Text2image</td><td></td><td></td><td>4</td><td></td></tr><tr><td>Tokenization</td><td></td><td>1</td><td></td><td>1</td></tr><tr><td>Transfer Learning</td><td>1</td><td>1</td><td></td><td>1</td></tr><tr><td>Transformer</td><td>2</td><td>2</td><td>8</td><td>4</td></tr><tr><td>Unsupervised Learning</td><td>1</td><td>5</td><td>1</td><td></td></tr><tr><td>Video-and-Language</td><td></td><td></td><td></td><td>1</td></tr><tr><td>Vision Transformer</td><td></td><td></td><td>4</td><td></td></tr><tr><td>Vision-and-Language</td><td>2</td><td>2</td><td>3</td><td>2</td></tr><tr><td>Visual Question Answering</td><td></td><td>2</td><td>3</td><td></td></tr><tr><td>Weakly Supervised Learning</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Weakly-supervised Learning</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Word Embedding</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Word2vec</td><td></td><td>1</td><td></td><td></td></tr><tr><td>XLNet</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Zero-shot</td><td>1</td><td>2</td><td>2</td><td>2</td></tr><tr><td>falcon</td><td></td><td>1</td><td></td><td></td></tr></tbody></table><script>$(function(){$("table").addClass("keyword-table table-bordered border-success"),$("table thead").addClass("sticky-top"),$("table tbody td").css("text-align","")})</script><h2 id=cscl-34>cs.CL (34)</h2><h3 id=134--1247-learning-how-to-ask-cycle-consistency-refines-prompts-in-multimodal-foundation-models-maurice-diesendruck-et-al-2024>(1/34 | 1/247) Learning How To Ask: Cycle-Consistency Refines Prompts in Multimodal Foundation Models (Maurice Diesendruck et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Maurice Diesendruck, Jianzhe Lin, Shima Imani, Gayathri Mahalingam, Mingyang Xu, Jie Zhao. (2024)<br><strong>Learning How To Ask: Cycle-Consistency Refines Prompts in Multimodal Foundation Models</strong><br><button class=copy-to-clipboard title="Learning How To Ask: Cycle-Consistency Refines Prompts in Multimodal Foundation Models" index=1>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-1 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-CV, cs.CL<br>Keyword Score: 139<br>Keywords: Benchmarking, Fine-tuning, Foundation Model, Multi-modal, Multi-modal, Self-supervised Learning, Self-supervised Learning, Zero-shot, GPT-4, Code Generation, Question Answering, Visual Question Answering, In-context Learning, Large Language Model, Prompt, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08756v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08756v1.pdf filename=2402.08756v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>When <b>LLMs</b> perform <b>zero-shot</b> inference, they typically use a <b>prompt</b> with a task specification, and generate a completion. However, there is no work to explore the possibility of the reverse - going from completion to task specification. In this paper, we employ both directions to perform cycle-supervised learning entirely <b>in-context.</b> Our goal is to create a forward map f : X -> Y (e.g. image -> generated caption), coupled with a backward map g : Y -> X (e.g. caption -> generated image) to construct a cycle-consistency &ldquo;loss&rdquo; (formulated as an update to the <b>prompt)</b> to enforce g(f(X)) ~= X. The technique, called CyclePrompt, uses cycle-consistency as a free supervisory signal to iteratively craft the <b>prompt.</b> Importantly, CyclePrompt reinforces model performance without expensive <b>fine-tuning,</b> without training data, and without the complexity of external environments (e.g. compilers, APIs). We demonstrate CyclePrompt in two domains: <b>code</b> <b>generation</b> and image captioning. Our results on the HumanEval coding <b>benchmark</b> put us in first place on the leaderboard among models that do not rely on extra training data or usage of external environments, and third overall. Compared to the <b>GPT4</b> baseline, we improve accuracy from 80.5% to 87.2%. In the <b>vision-language</b> space, we generate detailed image captions which outperform baseline <b>zero-shot</b> GPT4V captions, when tested against natural (VQAv2) and diagrammatic (FigureQA) <b>visual</b> <b>question-answering</b> <b>benchmarks.</b> To the best of our knowledge, this is the first use of <b>self-supervised</b> <b>learning</b> for <b>prompting.</b></p></p class="citation"></blockquote><h3 id=234--2247-eliciting-personality-traits-in-large-language-models-airlie-hilliard-et-al-2024>(2/34 | 2/247) Eliciting Personality Traits in Large Language Models (Airlie Hilliard et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Airlie Hilliard, Cristian Munoz, Zekun Wu, Adriano Soares Koshiyama. (2024)<br><strong>Eliciting Personality Traits in Large Language Models</strong><br><button class=copy-to-clipboard title="Eliciting Personality Traits in Large Language Models" index=2>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-2 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 105<br>Keywords: Black Box, Fine-tuning, BLOOM, GPT, LLaMA, Mistral, XLNet, falcon, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08341v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08341v2.pdf filename=2402.08341v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> are increasingly being utilized by both candidates and employers in the recruitment context. However, with this comes numerous ethical concerns, particularly related to the lack of transparency in these <b>&ldquo;black-box&rdquo;</b> <b>models.</b> Although previous studies have sought to increase the transparency of these models by investigating the personality traits of <b>LLMs,</b> many of the previous studies have provided them with personality assessments to complete. On the other hand, this study seeks to obtain a better understanding of such models by examining their output variations based on different input <b>prompts.</b> Specifically, we use a novel elicitation approach using <b>prompts</b> derived from common interview questions, as well as <b>prompts</b> designed to elicit particular Big Five personality traits to examine whether the models were susceptible to trait-activation like humans are, to measure their personality based on the language used in their outputs. To do so, we repeatedly <b>prompted</b> multiple LMs with different parameter sizes, including <b>Llama-2,</b> <b>Falcon,</b> <b>Mistral,</b> <b>Bloom,</b> <b>GPT,</b> OPT, and <b>XLNet</b> (base and fine tuned versions) and examined their personality using classifiers trained on the myPersonality dataset. Our results reveal that, generally, all <b>LLMs</b> demonstrate high openness and low extraversion. However, whereas LMs with fewer parameters exhibit similar behaviour in personality traits, newer and LMs with more parameters exhibit a broader range of personality traits, with increased agreeableness, emotional stability, and openness. Furthermore, a greater number of parameters is positively associated with openness and conscientiousness. Moreover, <b>fine-tuned</b> models exhibit minor modulations in their personality traits, contingent on the dataset. Implications and directions for future research are discussed.</p></p class="citation"></blockquote><h3 id=334--3247-punctuation-restoration-improves-structure-understanding-without-supervision-junghyun-min-et-al-2024>(3/34 | 3/247) Punctuation Restoration Improves Structure Understanding without Supervision (Junghyun Min et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Junghyun Min, Minho Lee, Woochul Lee, Yeonsoo Lee. (2024)<br><strong>Punctuation Restoration Improves Structure Understanding without Supervision</strong><br><button class=copy-to-clipboard title="Punctuation Restoration Improves Structure Understanding without Supervision" index=3>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-3 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 80<br>Keywords: Out-of-distribution, Unsupervised Learning, Unsupervised Learning, Information Retrieval, Named Entity Recognition, Natural Language Understanding, Open Information Extraction, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08382v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08382v1.pdf filename=2402.08382v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Unsupervised</b> <b>learning</b> objectives like language modeling and de-noising constitute a significant part in producing pre-trained models that perform various downstream applications from <b>natural</b> <b>language</b> <b>understanding</b> to conversational tasks. However, despite impressive conversational capabilities of recent <b>large</b> <b>language</b> <b>model,</b> their abilities to capture syntactic or semantic structure within text lag behind. We hypothesize that the mismatch between linguistic performance and competence in machines is attributable to insufficient transfer of linguistic structure knowledge to computational systems with currently popular pre-training objectives. We show that punctuation restoration transfers to improvements in in- and <b>out-of-distribution</b> performance on structure-related tasks like <b>named</b> <b>entity</b> <b>recognition,</b> <b>open</b> <b>information</b> <b>extraction,</b> chunking, and part-of-speech tagging. Punctuation restoration is an effective learning objective that can improve structure understanding and yield a more robust structure-aware representations of natural language.</p></p class="citation"></blockquote><h3 id=434--4247-prompt-optimization-in-multi-step-tasks-promst-integrating-human-feedback-and-preference-alignment-yongchao-chen-et-al-2024>(4/34 | 4/247) PRompt Optimization in Multi-Step Tasks (PROMST): Integrating Human Feedback and Preference Alignment (Yongchao Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yongchao Chen, Jacob Arkin, Yilun Hao, Yang Zhang, Nicholas Roy, Chuchu Fan. (2024)<br><strong>PRompt Optimization in Multi-Step Tasks (PROMST): Integrating Human Feedback and Preference Alignment</strong><br><button class=copy-to-clipboard title="PRompt Optimization in Multi-Step Tasks (PROMST): Integrating Human Feedback and Preference Alignment" index=4>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-4 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-HC, cs-RO, cs.CL<br>Keyword Score: 73<br>Keywords: Benchmarking, GPT, GPT-3, GPT-3.5, GPT-4, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08702v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08702v1.pdf filename=2402.08702v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Prompt</b> optimization aims to find the best <b>prompt</b> to a <b>large</b> <b>language</b> <b>model</b> <b>(LLM)</b> for a given task. <b>LLMs</b> have been successfully used to help find and improve <b>prompt</b> candidates for single-step tasks. However, realistic tasks for agents are multi-step and introduce new challenges: (1) <b>Prompt</b> content is likely to be more extensive and complex, making it more difficult for <b>LLMs</b> to analyze errors, (2) the impact of an individual step is difficult to evaluate, and (3) different people may have varied preferences about task execution. While humans struggle to optimize <b>prompts,</b> they are good at providing feedback about <b>LLM</b> outputs; we therefore introduce a new <b>LLM-driven</b> discrete <b>prompt</b> optimization framework that incorporates human-designed feedback rules about potential errors to automatically offer direct suggestions for improvement. Our framework is stylized as a genetic algorithm in which an <b>LLM</b> generates new candidate <b>prompts</b> from a parent <b>prompt</b> and its associated feedback; we use a learned heuristic function that predicts <b>prompt</b> performance to efficiently sample from these candidates. This approach significantly outperforms both human-engineered <b>prompts</b> and several other <b>prompt</b> optimization methods across eight representative multi-step tasks (an average 27.7% and 28.2% improvement to current best methods on <b>GPT-3.5</b> and <b>GPT-4,</b> respectively). We further show that the score function for tasks can be modified to better align with individual preferences. We believe our work can serve as a <b>benchmark</b> for automatic <b>prompt</b> optimization for <b>LLM-driven</b> multi-step tasks. Datasets and Codes are available at <a href=https://github.com/yongchao98/PROMST>https://github.com/yongchao98/PROMST</a>. Project Page is available at <a href=https://yongchao98.github.io/MIT-REALM-PROMST>https://yongchao98.github.io/MIT-REALM-PROMST</a>.</p></p class="citation"></blockquote><h3 id=534--5247-towards-faithful-and-robust-llm-specialists-for-evidence-based-question-answering-tobias-schimanski-et-al-2024>(5/34 | 5/247) Towards Faithful and Robust LLM Specialists for Evidence-Based Question-Answering (Tobias Schimanski et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tobias Schimanski, Jingwei Ni, Mathias Kraus, Elliott Ash, Markus Leippold. (2024)<br><strong>Towards Faithful and Robust LLM Specialists for Evidence-Based Question-Answering</strong><br><button class=copy-to-clipboard title="Towards Faithful and Robust LLM Specialists for Evidence-Based Question-Answering" index=5>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-5 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keyword Score: 73<br>Keywords: Benchmarking, Fine-tuning, Fine-tuning, Out-of-distribution, Question Answering, Question Answering, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08277v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08277v2.pdf filename=2402.08277v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Advances towards more faithful and traceable answers of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> are crucial for various research and practical endeavors. One avenue in reaching this goal is basing the answers on reliable sources. However, this Evidence-Based <b>QA</b> has proven to work insufficiently with <b>LLMs</b> in terms of citing the correct sources (source quality) and truthfully representing the information within sources (answer attributability). In this work, we systematically investigate how to robustly <b>fine-tune</b> <b>LLMs</b> for better source quality and answer attributability. Specifically, we introduce a data generation pipeline with automated data quality filters, which can synthesize diversified high-quality training and testing data at scale. We further introduce four test sets to <b>benchmark</b> the robustness of <b>fine-tuned</b> specialist models. Extensive evaluation shows that <b>fine-tuning</b> on synthetic data improves performance on both in- and <b>out-of-distribution.</b> Furthermore, we show that data quality, which can be drastically improved by proposed quality filters, matters more than quantity in improving Evidence-Based <b>QA.</b></p></p class="citation"></blockquote><h3 id=634--6247-auditing-counterfire-evaluating-advanced-counterargument-generation-with-evidence-and-style-preetika-verma-et-al-2024>(6/34 | 6/247) Auditing Counterfire: Evaluating Advanced Counterargument Generation with Evidence and Style (Preetika Verma et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Preetika Verma, Kokil Jaidka, Svetlana Churina. (2024)<br><strong>Auditing Counterfire: Evaluating Advanced Counterargument Generation with Evidence and Style</strong><br><button class=copy-to-clipboard title="Auditing Counterfire: Evaluating Advanced Counterargument Generation with Evidence and Style" index=6>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-6 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 70<br>Keywords: Fine-tuning, GPT, GPT-3, GPT-3.5, PaLM, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08498v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08498v1.pdf filename=2402.08498v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present a novel dataset for the controlled composition of counterarguments designed for further applications in argument refining, mining, and evaluation. Our dataset constitutes enriched counter-arguments to posts in the Reddit ChangeMyView dataset that are integrated with evidence retrieved from high-quality sources and generated based on user preferences, adjusting the critical attributes of evidence and argument style. The resultant Counterfire corpus comprises arguments generated from <b>GPT-3.5</b> turbo, Koala, and <b>PaLM</b> 2 models and two of their <b>finetuned</b> variants (N = 32,000). Model evaluation indicates strong paraphrasing abilities with evidence, albeit limited word overlap, while demonstrating high style integration (0.9682 for &lsquo;reciprocity&rsquo;), showing the ability of <b>LLM</b> to assimilate diverse styles. Of all models, <b>GPT-3.5</b> turbo showed the highest scores in argument quality evaluation, showing consistent accuracy (score >0.8). In further analyses, reciprocity-style counterarguments display higher counts in most categories, possibly indicating a more creatively persuasive use of evidence. In contrast, human-written counterarguments exhibited greater argumentative richness and diversity across categories. Despite human-written arguments being favored as the most persuasive in human evaluation, the &lsquo;No Style&rsquo; generated text surprisingly exhibited the highest score, <b>prompting</b> further exploration and investigation on the trade-offs in generation for facts and style.</p></p class="citation"></blockquote><h3 id=734--7247-plausible-extractive-rationalization-through-semi-supervised-entailment-signal-yeo-wei-jie-et-al-2024>(7/34 | 7/247) Plausible Extractive Rationalization through Semi-Supervised Entailment Signal (Yeo Wei Jie et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yeo Wei Jie, Ranjan Satapathy, Erik Cambria. (2024)<br><strong>Plausible Extractive Rationalization through Semi-Supervised Entailment Signal</strong><br><button class=copy-to-clipboard title="Plausible Extractive Rationalization through Semi-Supervised Entailment Signal" index=7>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-7 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 65<br>Keywords: Black Box, Fine-tuning, Supervised Learning, Unsupervised Learning, Natural Language Inference, Natural Language Inference, Question Answering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08479v3 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08479v3.pdf filename=2402.08479v3.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The increasing use of complex and opaque <b>black</b> <b>box</b> models requires the adoption of interpretable measures, one such option is extractive rationalizing models, which serve as a more interpretable alternative. These models, also known as Explain-Then-Predict models, employ an explainer model to extract rationales and subsequently condition the predictor with the extracted information. Their primary objective is to provide precise and faithful explanations, represented by the extracted rationales. In this paper, we take a semi-supervised approach to optimize for the plausibility of extracted rationales. We adopt a pre-trained <b>natural</b> <b>language</b> <b>inference</b> <b>(NLI)</b> model and further <b>fine-tune</b> it on a small set of <b>supervised</b> rationales ($10%$). The <b>NLI</b> predictor is leveraged as a source of supervisory signals to the explainer via entailment alignment. We show that, by enforcing the alignment agreement between the explanation and answer in a <b>question-answering</b> <b>task,</b> the performance can be improved without access to ground truth labels. We evaluate our approach on the ERASER dataset and show that our approach achieves comparable results with <b>supervised</b> extractive models and outperforms <b>unsupervised</b> approaches by $> 100%$.</p></p class="citation"></blockquote><h3 id=834--8247-bbox-adapter-lightweight-adapting-for-black-box-large-language-models-haotian-sun-et-al-2024>(8/34 | 8/247) BBox-Adapter: Lightweight Adapting for Black-Box Large Language Models (Haotian Sun et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haotian Sun, Yuchen Zhuang, Wei Wei, Chao Zhang, Bo Dai. (2024)<br><strong>BBox-Adapter: Lightweight Adapting for Black-Box Large Language Models</strong><br><button class=copy-to-clipboard title="BBox-Adapter: Lightweight Adapting for Black-Box Large Language Models" index=8>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-8 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 65<br>Keywords: Black Box, Fine-tuning, GPT, GPT-4, Gemini, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08219v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08219v1.pdf filename=2402.08219v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Adapting state-of-the-art <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> like <b>GPT-4</b> and <b>Gemini</b> for specific tasks is challenging. Due to the opacity in their parameters, embeddings, and even output probabilities, existing <b>fine-tuning</b> adaptation methods are inapplicable. Consequently, adapting these <b>black-box</b> <b>LLMs</b> is only possible through their API services, raising concerns about transparency, privacy, and cost. To address these challenges, we introduce BBox-Adapter, a novel lightweight adapter for <b>black-box</b> <b>LLMs.</b> BBox-Adapter distinguishes target and source domain data by treating target data as positive and source data as negative. It employs a ranking-based Noise Contrastive Estimation (NCE) loss to promote the likelihood of target domain data while penalizing that of the source domain. Furthermore, it features an online adaptation mechanism, which incorporates real-time positive data sampling from ground-truth, human, or AI feedback, coupled with negative data from previous adaptations. Extensive experiments demonstrate BBox-Adapter&rsquo;s effectiveness and cost efficiency. It improves model performance by up to 6.77% across diverse tasks and domains, while reducing training and inference costs by 31.30x and 1.84x, respectively.</p></p class="citation"></blockquote><h3 id=934--9247-an-embarrassingly-simple-approach-for-llm-with-strong-asr-capacity-ziyang-ma-et-al-2024>(9/34 | 9/247) An Embarrassingly Simple Approach for LLM with Strong ASR Capacity (Ziyang Ma et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ziyang Ma, Guanrou Yang, Yifan Yang, Zhifu Gao, Jiaming Wang, Zhihao Du, Fan Yu, Qian Chen, Siqi Zheng, Shiliang Zhang, Xie Chen. (2024)<br><strong>An Embarrassingly Simple Approach for LLM with Strong ASR Capacity</strong><br><button class=copy-to-clipboard title="An Embarrassingly Simple Approach for LLM with Strong ASR Capacity" index=9>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-9 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-MM, cs-SD, cs.CL, eess-AS<br>Keyword Score: 63<br>Keywords: Benchmarking, Fine-tuning, Automatic Speech Recognition, Automatic Speech Recognition, Automatic Speech Recognition, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08846v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08846v1.pdf filename=2402.08846v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we focus on solving one of the most important tasks in the field of <b>speech</b> <b>processing,</b> i.e., <b>automatic</b> <b>speech</b> <b>recognition</b> <b>(ASR),</b> with <b>speech</b> <b>foundation</b> encoders and <b>large</b> <b>language</b> <b>models</b> <b>(LLM).</b> Recent works have complex designs such as compressing the output temporally for the <b>speech</b> <b>encoder,</b> tackling modal alignment for the projector, and utilizing parameter-efficient <b>fine-tuning</b> for the <b>LLM.</b> We found that delicate designs are not necessary, while an embarrassingly simple composition of off-the-shelf <b>speech</b> <b>encoder,</b> <b>LLM,</b> and the only trainable linear projector is competent for the <b>ASR</b> task. To be more specific, we <b>benchmark</b> and explore various combinations of <b>LLMs</b> and <b>speech</b> <b>encoders,</b> leading to the optimal <b>LLM-based</b> <b>ASR</b> system, which we call SLAM-ASR. The proposed SLAM-ASR provides a clean setup and little task-specific design, where only the linear projector is trained. To the best of our knowledge, SLAM-ASR achieves the best performance on the Librispeech <b>benchmark</b> among <b>LLM-based</b> <b>ASR</b> models and even outperforms the latest <b>LLM-based</b> audio-universal model trained on massive pair data. Finally, we explore the capability emergence of <b>LLM-based</b> <b>ASR</b> in the process of modal alignment. We hope that our study can facilitate the research on extending <b>LLM</b> with cross-modality capacity and shed light on the <b>LLM-based</b> <b>ASR</b> community.</p></p class="citation"></blockquote><h3 id=1034--10247-ecellm-generalizing-large-language-models-for-e-commerce-from-large-scale-high-quality-instruction-data-bo-peng-et-al-2024>(10/34 | 10/247) eCeLLM: Generalizing Large Language Models for E-commerce from Large-scale, High-quality Instruction Data (Bo Peng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bo Peng, Xinyi Ling, Ziru Chen, Huan Sun, Xia Ning. (2024)<br><strong>eCeLLM: Generalizing Large Language Models for E-commerce from Large-scale, High-quality Instruction Data</strong><br><button class=copy-to-clipboard title="eCeLLM: Generalizing Large Language Models for E-commerce from Large-scale, High-quality Instruction Data" index=10>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-10 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-IR, cs.CL<br>Keyword Score: 63<br>Keywords: Benchmarking, Out-of-domain, GPT, GPT-4, Instruction Tuning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08831v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08831v1.pdf filename=2402.08831v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With tremendous efforts on developing effective e-commerce models, conventional e-commerce models show limited success in generalist e-commerce modeling, and suffer from unsatisfactory performance on new users and new products - a typical <b>out-of-domain</b> generalization challenge. Meanwhile, <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> demonstrate outstanding performance in generalist modeling and <b>out-of-domain</b> generalizability in many fields. Toward fully unleashing their power for e-commerce, in this paper, we construct ECInstruct, the first open-sourced, <b>large-scale,</b> <b>and</b> <b>high-quality</b> <b>benchmark</b> <b>instruction</b> <b>dataset</b> for e-commerce. Leveraging ECInstruct, we develop eCeLLM, a series of e-commerce <b>LLMs,</b> by <b>instruction-tuning</b> <b>general-purpose</b> <b>LLMs.</b> Our comprehensive experiments and evaluation demonstrate that eCeLLM models substantially outperform baseline models, including the most advanced <b>GPT-4,</b> and the state-of-the-art task-specific models in in-domain evaluation. Moreover, eCeLLM exhibits excellent generalizability to <b>out-of-domain</b> settings, including unseen products and unseen <b>instructions,</b> <b>highlighting</b> its superiority as a generalist e-commerce model. Both the ECInstruct dataset and the eCeLLM models show great potential in empowering versatile and effective <b>LLMs</b> for e-commerce. ECInstruct and eCeLLM models are publicly accessible through <a href=https://ninglab.github.io/eCeLLM>https://ninglab.github.io/eCeLLM</a>.</p></p class="citation"></blockquote><h3 id=1134--11247-instructgraph-boosting-large-language-models-via-graph-centric-instruction-tuning-and-preference-alignment-jianing-wang-et-al-2024>(11/34 | 11/247) InstructGraph: Boosting Large Language Models via Graph-centric Instruction Tuning and Preference Alignment (Jianing Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jianing Wang, Junda Wu, Yupeng Hou, Yao Liu, Ming Gao, Julian McAuley. (2024)<br><strong>InstructGraph: Boosting Large Language Models via Graph-centric Instruction Tuning and Preference Alignment</strong><br><button class=copy-to-clipboard title="InstructGraph: Boosting Large Language Models via Graph-centric Instruction Tuning and Preference Alignment" index=11>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-11 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 63<br>Keywords: Graph, GPT, GPT-4, Reasoning, Instruction Tuning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08785v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08785v1.pdf filename=2402.08785v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Do current <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> better solve <b>graph</b> <b>reasoning</b> and generation tasks with parameter updates? In this paper, we propose InstructGraph, a framework that empowers <b>LLMs</b> with the abilities of <b>graph</b> <b>reasoning</b> and generation by <b>instruction</b> <b>tuning</b> and preference alignment. Specifically, we first propose a structured format verbalizer to unify all <b>graph</b> data into a universal code-like format, which can simply represent the <b>graph</b> without any external <b>graph-specific</b> encoders. Furthermore, a <b>graph</b> <b>instruction</b> <b>tuning</b> stage is introduced to guide <b>LLMs</b> in solving <b>graph</b> <b>reasoning</b> and generation tasks. Finally, we identify potential hallucination problems in <b>graph</b> tasks and sample negative instances for preference alignment, the target of which is to enhance the output&rsquo;s reliability of the model. Extensive experiments across multiple <b>graph-centric</b> tasks exhibit that InstructGraph can achieve the best performance and outperform <b>GPT-4</b> and LLaMA2 by more than 13% and 38%, respectively.</p></p class="citation"></blockquote><h3 id=1234--12247-jamdec-unsupervised-authorship-obfuscation-using-constrained-decoding-over-small-language-models-jillian-fisher-et-al-2024>(12/34 | 12/247) JAMDEC: Unsupervised Authorship Obfuscation using Constrained Decoding over Small Language Models (Jillian Fisher et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jillian Fisher, Ximing Lu, Jaehun Jung, Liwei Jiang, Zaid Harchaoui, Yejin Choi. (2024)<br><strong>JAMDEC: Unsupervised Authorship Obfuscation using Constrained Decoding over Small Language Models</strong><br><button class=copy-to-clipboard title="JAMDEC: Unsupervised Authorship Obfuscation using Constrained Decoding over Small Language Models" index=12>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-12 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 60<br>Keywords: Unsupervised Learning, GPT-2, GPT-3, GPT-3.5, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08761v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08761v1.pdf filename=2402.08761v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The permanence of online content combined with the enhanced authorship identification techniques calls for stronger computational methods to protect the identity and privacy of online authorship when needed, e.g., blind reviews for scientific papers, anonymous online reviews, or anonymous interactions in the mental health forums. In this paper, we propose an <b>unsupervised</b> inference-time approach to authorship obfuscation to address the unique challenges of authorship obfuscation: lack of supervision data for diverse authorship and domains, and the need for a sufficient level of revision beyond simple paraphrasing to obfuscate the authorship, all the while preserving the original content and fluency. We introduce JAMDEC, a user-controlled, inference-time algorithm for authorship obfuscation that can be in principle applied to any text and authorship. Our approach builds on small language models such as <b>GPT2-XL</b> in order to help avoid disclosing the original content to proprietary <b>LLM&rsquo;s</b> APIs, while also reducing the performance gap between small and <b>large</b> <b>language</b> <b>models</b> via algorithmic enhancement. The key idea behind our approach is to boost the creative power of smaller language models through constrained decoding, while also allowing for user-specified controls and flexibility. Experimental results demonstrate that our approach based on <b>GPT2-XL</b> outperforms previous state-of-the-art methods based on comparably small models, while performing competitively against <b>GPT3.5</b> 175B, a propriety model that is two orders of magnitudes larger.</p></p class="citation"></blockquote><h3 id=1334--13247-pixel-sentence-representation-learning-chenghao-xiao-et-al-2024>(13/34 | 13/247) Pixel Sentence Representation Learning (Chenghao Xiao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chenghao Xiao, Zhuoxu Huang, Danlu Chen, G Thomas Hudson, Yizhi Li, Haoran Duan, Chenghua Lin, Jie Fu, Jungong Han, Noura Al Moubayed. (2024)<br><strong>Pixel Sentence Representation Learning</strong><br><button class=copy-to-clipboard title="Pixel Sentence Representation Learning" index=13>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-13 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-CV, cs.CL<br>Keyword Score: 55<br>Keywords: Representation Learning, Unsupervised Learning, Zero-shot, Natural Language Inference, Tokenization, Pre-trained Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08183v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08183v1.pdf filename=2402.08183v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Pretrained</b> <b>language</b> <b>models</b> are long known to be subpar in capturing sentence and document-level semantics. Though heavily investigated, transferring perturbation-based methods from <b>unsupervised</b> visual <b>representation</b> <b>learning</b> to NLP remains an unsolved problem. This is largely due to the discreteness of subword units brought by <b>tokenization</b> of language models, limiting small perturbations of inputs to form semantics-preserved positive pairs. In this work, we conceptualize the learning of sentence-level textual semantics as a visual <b>representation</b> <b>learning</b> process. Drawing from cognitive and linguistic sciences, we introduce an <b>unsupervised</b> visual sentence <b>representation</b> <b>learning</b> framework, employing visually-grounded text perturbation methods like typos and word order shuffling, resonating with human cognitive patterns, and enabling perturbation to texts to be perceived as continuous. Our approach is further bolstered by large-scale <b>unsupervised</b> topical alignment training and <b>natural</b> <b>language</b> <b>inference</b> supervision, achieving comparable performance in semantic textual similarity (STS) to existing state-of-the-art NLP methods. Additionally, we unveil our method&rsquo;s inherent <b>zero-shot</b> cross-lingual transferability and a unique leapfrogging pattern across languages during iterative training. To our knowledge, this is the first <b>representation</b> <b>learning</b> method devoid of traditional language models for understanding sentence and document semantics, marking a stride closer to human-like textual comprehension. Our code is available at <a href=https://github.com/gowitheflow-1998/Pixel-Linguist>https://github.com/gowitheflow-1998/Pixel-Linguist</a></p></p class="citation"></blockquote><h3 id=1434--14247-improving-generalization-in-semantic-parsing-by-increasing-natural-language-variation-irina-saparina-et-al-2024>(14/34 | 14/247) Improving Generalization in Semantic Parsing by Increasing Natural Language Variation (Irina Saparina et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Irina Saparina, Mirella Lapata. (2024)<br><strong>Improving Generalization in Semantic Parsing by Increasing Natural Language Variation</strong><br><button class=copy-to-clipboard title="Improving Generalization in Semantic Parsing by Increasing Natural Language Variation" index=14>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-14 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 53<br>Keywords: Benchmarking, Data Augmentation, Out-of-domain, Semantic Parsing, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08666v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08666v1.pdf filename=2402.08666v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Text-to-SQL <b>semantic</b> <b>parsing</b> has made significant progress in recent years, with various models demonstrating impressive performance on the challenging Spider <b>benchmark.</b> However, it has also been shown that these models often struggle to generalize even when faced with small perturbations of previously (accurately) parsed expressions. This is mainly due to the linguistic form of questions in Spider which are overly specific, unnatural, and display limited variation. In this work, we use <b>data</b> <b>augmentation</b> to enhance the robustness of text-to-SQL parsers against natural language variations. Existing approaches generate question reformulations either via models trained on Spider or only introduce local changes. In contrast, we leverage the capabilities of <b>large</b> <b>language</b> <b>models</b> to generate more realistic and diverse questions. Using only a few <b>prompts,</b> we achieve a two-fold increase in the number of questions in Spider. Training on this augmented dataset yields substantial improvements on a range of evaluation sets, including robustness <b>benchmarks</b> and <b>out-of-domain</b> data.</p></p class="citation"></blockquote><h3 id=1534--15247-bayesian-multi-task-transfer-learning-for-soft-prompt-tuning-haeju-lee-et-al-2024>(15/34 | 15/247) Bayesian Multi-Task Transfer Learning for Soft Prompt Tuning (Haeju Lee et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haeju Lee, Minchan Jeong, Se-Young Yun, Kee-Eung Kim. (2024)<br><strong>Bayesian Multi-Task Transfer Learning for Soft Prompt Tuning</strong><br><button class=copy-to-clipboard title="Bayesian Multi-Task Transfer Learning for Soft Prompt Tuning" index=15>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-15 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 43<br>Keywords: Benchmarking, Fine-tuning, Transfer Learning, Pre-trained Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08594v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08594v1.pdf filename=2402.08594v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Prompt</b> tuning, in which <b>prompts</b> are optimized to adapt large-scale <b>pre-trained</b> <b>language</b> <b>models</b> to downstream tasks instead of <b>fine-tuning</b> the full model parameters, has been shown to be particularly effective when the <b>prompts</b> are trained in a multi-task <b>transfer</b> <b>learning</b> setting. These methods generally involve individually training <b>prompts</b> for each source task and then aggregating them to provide the initialization of the <b>prompt</b> for the target task. However, this approach critically ignores the fact that some of the source tasks could be negatively or positively interfering with each other. We argue that when we extract knowledge from source tasks via training source <b>prompts,</b> we need to consider this correlation among source tasks for better <b>transfer</b> <b>to</b> target tasks. To this end, we propose a Bayesian approach where we work with the posterior distribution of <b>prompts</b> across source tasks. We obtain representative source <b>prompts</b> corresponding to the samples from the posterior utilizing Stein Variational Gradient Descent, which are then aggregated to constitute the initial target <b>prompt.</b> We show extensive experimental results on the standard <b>benchmark</b> NLP tasks, where our Bayesian multi-task <b>transfer</b> <b>learning</b> approach outperforms the state-of-the-art methods in many settings. Furthermore, our approach requires no auxiliary models other than the <b>prompt</b> itself, achieving a high degree of parameter efficiency.</p></p class="citation"></blockquote><h3 id=1634--16247-glore-when-where-and-how-to-improve-llm-reasoning-via-global-and-local-refinements-alex-havrilla-et-al-2024>(16/34 | 16/247) GLoRe: When, Where, and How to Improve LLM Reasoning via Global and Local Refinements (Alex Havrilla et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Alex Havrilla, Sharath Raparthy, Christoforus Nalmpantis, Jane Dwivedi-Yu, Maksym Zhuravinskyi, Eric Hambro, Roberta Railneau. (2024)<br><strong>GLoRe: When, Where, and How to Improve LLM Reasoning via Global and Local Refinements</strong><br><button class=copy-to-clipboard title="GLoRe: When, Where, and How to Improve LLM Reasoning via Global and Local Refinements" index=16>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-16 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keyword Score: 40<br>Keywords: Fine-tuning, LLaMA, Reasoning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10963v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10963v1.pdf filename=2402.10963v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>State-of-the-art language models can exhibit impressive <b>reasoning</b> refinement capabilities on math, science or coding tasks. However, recent work demonstrates that even the best models struggle to identify \textit{when and where to refine} without access to external feedback. Outcome-based Reward Models (\textbf{ORMs}), trained to predict correctness of the final answer indicating when to refine, offer one convenient solution for deciding when to refine. Process Based Reward Models (\textbf{PRMs}), trained to predict correctness of intermediate steps, can then be used to indicate where to refine. But they are expensive to train, requiring extensive human annotations. In this paper, we propose Stepwise ORMs (\textbf{SORMs}) which are trained, only on synthetic data, to approximate the expected future reward of the optimal policy or $V^{\star}$. More specifically, SORMs are trained to predict the correctness of the final answer when sampling the current policy many times (rather than only once as in the case of ORMs). Our experiments show that SORMs can more accurately detect incorrect <b>reasoning</b> steps compared to ORMs, thus improving downstream accuracy when doing refinements. We then train \textit{global} refinement models, which take only the question and a draft solution as input and predict a corrected solution, and \textit{local} refinement models which also take as input a critique indicating the location of the first <b>reasoning</b> error. We generate training data for both models synthetically by reusing data used to train the SORM. We find combining global and local refinements, using the ORM as a reranker, significantly outperforms either one individually, as well as a best of three sample baseline. With this strategy we can improve the accuracy of a <b>LLaMA-2</b> 13B model (already <b>fine-tuned</b> with RL) on GSM8K from 53% to 65% when greedily sampled.</p></p class="citation"></blockquote><h3 id=1734--17247-preflmr-scaling-up-fine-grained-late-interaction-multi-modal-retrievers-weizhe-lin-et-al-2024>(17/34 | 17/247) PreFLMR: Scaling Up Fine-Grained Late-Interaction Multi-modal Retrievers (Weizhe Lin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Weizhe Lin, Jingbiao Mei, Jinghong Chen, Bill Byrne. (2024)<br><strong>PreFLMR: Scaling Up Fine-Grained Late-Interaction Multi-modal Retrievers</strong><br><button class=copy-to-clipboard title="PreFLMR: Scaling Up Fine-Grained Late-Interaction Multi-modal Retrievers" index=17>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-17 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 39<br>Keywords: Benchmarking, Multi-modal, Multi-modal, Question Answering, Visual Question Answering, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08327v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08327v1.pdf filename=2402.08327v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Large <b>Multimodal</b> Models (LMMs) excel in natural language and <b>visual</b> <b>understanding</b> <b>but</b> are challenged by exacting tasks such as Knowledge-based <b>Visual</b> <b>Question</b> <b>Answering</b> (KB-VQA) which involve the retrieval of relevant information from document collections to use in shaping answers to <b>questions.</b> <b>We</b> present an extensive training and evaluation framework, M2KR, for KB-VQA. M2KR contains a collection of vision and language tasks which we have incorporated into a single suite of <b>benchmark</b> tasks for training and evaluating general-purpose <b>multi-modal</b> retrievers. We use M2KR to develop PreFLMR, a pre-trained version of the recently developed Fine-grained Late-interaction <b>Multi-modal</b> Retriever (FLMR) approach to KB-VQA, and we report new state-of-the-art results across a range of tasks. We also present investigations into the scaling behaviors of PreFLMR intended to be useful in future developments in general-purpose <b>multi-modal</b> retrievers.</p></p class="citation"></blockquote><h3 id=1834--18247-measuring-and-controlling-persona-drift-in-language-model-dialogs-kenneth-li-et-al-2024>(18/34 | 18/247) Measuring and Controlling Persona Drift in Language Model Dialogs (Kenneth Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kenneth Li, Tianle Liu, Naomi Bashkansky, David Bau, Fernanda Viégas, Hanspeter Pfister, Martin Wattenberg. (2024)<br><strong>Measuring and Controlling Persona Drift in Language Model Dialogs</strong><br><button class=copy-to-clipboard title="Measuring and Controlling Persona Drift in Language Model Dialogs" index=18>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-18 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 33<br>Keywords: Benchmarking, Transformer, Chatbot, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10962v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10962v1.pdf filename=2402.10962v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Prompting</b> is a standard tool for customizing language-model <b>chatbots,</b> enabling them to take on a specific &ldquo;persona&rdquo;. An implicit assumption in the use of <b>prompts</b> is that they will be stable, so the <b>chatbot</b> will continue to generate text according to the stipulated persona for the duration of a conversation. We propose a quantitative <b>benchmark</b> to test this assumption, evaluating persona stability via self-chats between two personalized <b>chatbots.</b> Testing popular models like LLaMA2-chat-70B, we reveal a significant persona drift within eight rounds of conversations. An empirical and theoretical analysis of this phenomenon suggests the <b>transformer</b> attention mechanism plays a role, due to attention decay over long exchanges. To combat attention decay and persona drift, we propose a lightweight method called split-softmax, which compares favorably against two strong baselines.</p></p class="citation"></blockquote><h3 id=1934--19247-higher-layers-need-more-lora-experts-chongyang-gao-et-al-2024>(19/34 | 19/247) Higher Layers Need More LoRA Experts (Chongyang Gao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chongyang Gao, Kezhen Chen, Jinmeng Rao, Baochen Sun, Ruibo Liu, Daiyi Peng, Yawen Zhang, Xiaoyuan Guo, Jie Yang, VS Subrahmanian. (2024)<br><strong>Higher Layers Need More LoRA Experts</strong><br><button class=copy-to-clipboard title="Higher Layers Need More LoRA Experts" index=19>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-19 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 33<br>Keywords: Benchmarking, Transformer, Question Answering, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08562v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08562v1.pdf filename=2402.08562v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Parameter-efficient tuning (PEFT) techniques like low-rank adaptation (LoRA) offer training efficiency on <b>Large</b> <b>Language</b> <b>Models,</b> but their impact on model performance remains limited. Recent efforts integrate LoRA and Mixture-of-Experts (MoE) to improve the performance of PEFT methods. Despite promising results, research on improving the efficiency of LoRA with MoE is still in its early stages. Recent studies have shown that experts in the MoE architecture have different strengths and also exhibit some redundancy. Does this statement also apply to parameter-efficient MoE? In this paper, we introduce a novel parameter-efficient MoE method, \textit{\textbf{M}oE-L\textbf{o}RA with \textbf{L}ayer-wise Expert \textbf{A}llocation (MoLA)} for <b>Transformer-based</b> models, where each model layer has the flexibility to employ a varying number of LoRA experts. We investigate several architectures with varying layer-wise expert configurations. Experiments on six well-known NLP and commonsense <b>QA</b> <b>benchmarks</b> demonstrate that MoLA achieves equal or superior performance compared to all baselines. We find that allocating more LoRA experts to higher layers further enhances the effectiveness of models with a certain number of experts in total. With much fewer parameters, this allocation strategy outperforms the setting with the same number of experts in every layer. This work can be widely used as a plug-and-play parameter-efficient tuning approach for various applications. The code is available at <a href=https://github.com/GCYZSL/MoLA>https://github.com/GCYZSL/MoLA</a>.</p></p class="citation"></blockquote><h3 id=2034--20247-privacy-preserving-language-model-inference-with-instance-obfuscation-yixiang-yao-et-al-2024>(20/34 | 20/247) Privacy-Preserving Language Model Inference with Instance Obfuscation (Yixiang Yao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yixiang Yao, Fei Wang, Srivatsan Ravi, Muhao Chen. (2024)<br><strong>Privacy-Preserving Language Model Inference with Instance Obfuscation</strong><br><button class=copy-to-clipboard title="Privacy-Preserving Language Model Inference with Instance Obfuscation" index=20>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-20 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 31<br>Keywords: Benchmarking, Benchmarking, Black Box, Natural Language Understanding, Pre-trained Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08227v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08227v1.pdf filename=2402.08227v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Language Models as a Service (LMaaS) offers convenient access for developers and researchers to perform inference using <b>pre-trained</b> <b>language</b> <b>models.</b> Nonetheless, the input data and the inference results containing private information are exposed as plaintext during the service call, leading to privacy issues. Recent studies have started tackling the privacy issue by transforming input data into privacy-preserving representation from the user-end with the techniques such as noise addition and content perturbation, while the exploration of inference result protection, namely decision privacy, is still a blank page. In order to maintain the <b>black-box</b> <b>manner</b> of LMaaS, conducting data privacy protection, especially for the decision, is a challenging task because the process has to be seamless to the models and accompanied by limited communication and computation overhead. We thus propose Instance-Obfuscated Inference (IOI) method, which focuses on addressing the decision privacy issue of <b>natural</b> <b>language</b> <b>understanding</b> tasks in their complete life-cycle. Besides, we conduct comprehensive experiments to evaluate the performance as well as the privacy-protection strength of the proposed method on various <b>benchmarking</b> tasks.</p></p class="citation"></blockquote><h3 id=2134--21247-lying-blindly-bypassing-chatgpts-safeguards-to-generate-hard-to-detect-disinformation-claims-at-scale-freddy-heppell-et-al-2024>(21/34 | 21/247) Lying Blindly: Bypassing ChatGPT&rsquo;s Safeguards to Generate Hard-to-Detect Disinformation Claims at Scale (Freddy Heppell et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Freddy Heppell, Mehmet E. Bakir, Kalina Bontcheva. (2024)<br><strong>Lying Blindly: Bypassing ChatGPT&rsquo;s Safeguards to Generate Hard-to-Detect Disinformation Claims at Scale</strong><br><button class=copy-to-clipboard title="Lying Blindly: Bypassing ChatGPT's Safeguards to Generate Hard-to-Detect Disinformation Claims at Scale" index=21>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-21 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: ChatGPT, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08467v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08467v1.pdf filename=2402.08467v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>As <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> become more proficient, their misuse in <b>large-scale</b> <b>viral</b> <b>disinformation</b> campaigns is a growing concern. This study explores the capability of <b>ChatGPT</b> to generate unconditioned claims about the war in Ukraine, an event beyond its knowledge cutoff, and evaluates whether such claims can be differentiated by human readers and automated tools from human-written ones. We compare war-related claims from ClaimReview, authored by IFCN-registered fact-checkers, and similar short-form content generated by <b>ChatGPT.</b> We demonstrate that <b>ChatGPT</b> can produce realistic, target-specific disinformation cheaply, fast, and at scale, and that these claims cannot be reliably distinguished by humans or existing automated tools.</p></p class="citation"></blockquote><h3 id=2234--22247-a-survey-of-table-reasoning-with-large-language-models-xuanliang-zhang-et-al-2024>(22/34 | 22/247) A Survey of Table Reasoning with Large Language Models (Xuanliang Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xuanliang Zhang, Dingzirui Wang, Longxu Dou, Qingfu Zhu, Wanxiang Che. (2024)<br><strong>A Survey of Table Reasoning with Large Language Models</strong><br><button class=copy-to-clipboard title="A Survey of Table Reasoning with Large Language Models" index=22>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-22 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08259v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08259v1.pdf filename=2402.08259v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Table <b>reasoning,</b> which aims to generate the corresponding answer to the question following the user requirement according to the provided table, and optionally a text description of the table, effectively improving the efficiency of obtaining information. Recently, using <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> has become the mainstream method for table <b>reasoning,</b> because it not only significantly reduces the annotation cost but also exceeds the performance of previous methods. However, existing research still lacks a summary of <b>LLM-based</b> table <b>reasoning</b> works. Due to the existing lack of research, questions about which techniques can improve table <b>reasoning</b> performance in the era of <b>LLMs,</b> why <b>LLMs</b> excel at table <b>reasoning,</b> and how to enhance table <b>reasoning</b> abilities in the future, remain largely unexplored. This gap significantly limits progress in research. To answer the above questions and advance table <b>reasoning</b> research with <b>LLMs,</b> we present this survey to analyze existing research, inspiring future work. In this paper, we analyze the mainstream techniques used to improve table <b>reasoning</b> performance in the <b>LLM</b> era, and the advantages of <b>LLMs</b> compared to pre-LLMs for solving table <b>reasoning.</b> We provide research directions from both the improvement of existing methods and the expansion of practical applications to inspire future research.</p></p class="citation"></blockquote><h3 id=2334--23247-knowledge-editing-on-black-box-large-language-models-xiaoshuai-song-et-al-2024>(23/34 | 23/247) Knowledge Editing on Black-box Large Language Models (Xiaoshuai Song et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiaoshuai Song, Zhengyang Wang, Keqing He, Guanting Dong, Yutao Mou, Jinxu Zhao, Weiran Xu. (2024)<br><strong>Knowledge Editing on Black-box Large Language Models</strong><br><button class=copy-to-clipboard title="Knowledge Editing on Black-box Large Language Models" index=23>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-23 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 28<br>Keywords: Benchmarking, Black Box, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08631v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08631v2.pdf filename=2402.08631v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Knowledge editing (KE) aims to efficiently and precisely modify the behavior of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> to update specific knowledge without negatively influencing other knowledge. Current research primarily focuses on white-box <b>LLMs</b> editing, overlooking an important scenario: <b>black-box</b> <b>LLMs</b> editing, where <b>LLMs</b> are accessed through interfaces and only textual output is available. In this paper, we first officially introduce KE on <b>black-box</b> <b>LLMs</b> and then propose a comprehensive evaluation framework to overcome the limitations of existing evaluations that are not applicable to <b>black-box</b> <b>LLMs</b> editing and lack comprehensiveness. To tackle privacy leaks of editing data and style over-editing in current methods, we introduce a novel postEdit framework, resolving privacy concerns through downstream post-processing and maintaining textual style consistency via fine-grained editing to original responses. Experiments and analysis on two <b>benchmarks</b> demonstrate that postEdit outperforms all baselines and achieves strong generalization, especially with huge improvements on style retention (average $+20.82%\uparrow$).</p></p class="citation"></blockquote><h3 id=2434--24247-test-time-backdoor-attacks-on-multimodal-large-language-models-dong-lu-et-al-2024>(24/34 | 24/247) Test-Time Backdoor Attacks on Multimodal Large Language Models (Dong Lu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dong Lu, Tianyu Pang, Chao Du, Qian Liu, Xianjun Yang, Min Lin. (2024)<br><strong>Test-Time Backdoor Attacks on Multimodal Large Language Models</strong><br><button class=copy-to-clipboard title="Test-Time Backdoor Attacks on Multimodal Large Language Models" index=24>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-24 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-CR, cs-CV, cs-LG, cs-MM, cs.CL<br>Keyword Score: 26<br>Keywords: Multi-modal, Multi-modal, Large Language Model, Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08577v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08577v1.pdf filename=2402.08577v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Backdoor attacks are commonly executed by contaminating training data, such that a trigger can activate predetermined harmful effects during the test phase. In this work, we present AnyDoor, a test-time backdoor attack against <b>multimodal</b> <b>large</b> <b>language</b> <b>models</b> (MLLMs), which involves injecting the backdoor into the textual modality using <b>adversarial</b> <b>test</b> images (sharing the same universal perturbation), without requiring access to or modification of the training data. AnyDoor employs similar techniques used in universal <b>adversarial</b> <b>attacks,</b> but distinguishes itself by its ability to decouple the timing of setup and activation of harmful effects. In our experiments, we validate the effectiveness of AnyDoor against popular MLLMs such as LLaVA-1.5, MiniGPT-4, InstructBLIP, and BLIP-2, as well as provide comprehensive ablation studies. Notably, because the backdoor is injected by a universal perturbation, AnyDoor can dynamically change its backdoor trigger prompts/harmful effects, exposing a new challenge for defending against backdoor attacks. Our project page is available at <a href=https://sail-sg.github.io/AnyDoor/>https://sail-sg.github.io/AnyDoor/</a>.</p></p class="citation"></blockquote><h3 id=2534--25247-agent-smith-a-single-image-can-jailbreak-one-million-multimodal-llm-agents-exponentially-fast-xiangming-gu-et-al-2024>(25/34 | 25/247) Agent Smith: A Single Image Can Jailbreak One Million Multimodal LLM Agents Exponentially Fast (Xiangming Gu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiangming Gu, Xiaosen Zheng, Tianyu Pang, Chao Du, Qian Liu, Ye Wang, Jing Jiang, Min Lin. (2024)<br><strong>Agent Smith: A Single Image Can Jailbreak One Million Multimodal LLM Agents Exponentially Fast</strong><br><button class=copy-to-clipboard title="Agent Smith: A Single Image Can Jailbreak One Million Multimodal LLM Agents Exponentially Fast" index=25>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-25 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-CR, cs-CV, cs-LG, cs-MA, cs.CL<br>Keyword Score: 26<br>Keywords: Multi-modal, Multi-modal, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08567v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08567v1.pdf filename=2402.08567v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A <b>multimodal</b> <b>large</b> <b>language</b> <b>model</b> (MLLM) agent can receive instructions, capture images, retrieve histories from memory, and decide which tools to use. Nonetheless, red-teaming efforts have revealed that adversarial images/prompts can jailbreak an MLLM and cause unaligned behaviors. In this work, we report an even more severe safety issue in multi-agent environments, referred to as infectious jailbreak. It entails the adversary simply jailbreaking a single agent, and without any further intervention from the adversary, (almost) all agents will become infected exponentially fast and exhibit harmful behaviors. To validate the feasibility of infectious jailbreak, we simulate multi-agent environments containing up to one million LLaVA-1.5 agents, and employ randomized pair-wise chat as a proof-of-concept instantiation for multi-agent interaction. Our results show that feeding an (infectious) adversarial image into the memory of any randomly chosen agent is sufficient to achieve infectious jailbreak. Finally, we derive a simple principle for determining whether a defense mechanism can provably restrain the spread of infectious jailbreak, but how to design a practical defense that meets this principle remains an open question to investigate. Our project page is available at <a href=https://sail-sg.github.io/Agent-Smith/>https://sail-sg.github.io/Agent-Smith/</a>.</p></p class="citation"></blockquote><h3 id=2634--26247-syllable-based-dnn-hmm-cantonese-speech-to-text-system-timothy-wong-et-al-2024>(26/34 | 26/247) Syllable based DNN-HMM Cantonese Speech to Text System (Timothy Wong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Timothy Wong, Claire Li, Sam Lam, Billy Chiu, Qin Lu, Minglei Li, Dan Xiong, Roy Shing Yu, Vincent T. Y. Ng. (2024)<br><strong>Syllable based DNN-HMM Cantonese Speech to Text System</strong><br><button class=copy-to-clipboard title="Syllable based DNN-HMM Cantonese Speech to Text System" index=26>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-26 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: 94-06, I-2-7, cs-CL, cs-SD, cs.CL, eess-AS<br>Keyword Score: 20<br>Keywords: Stochastic Gradient Descent, Automatic Speech Recognition<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08788v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08788v1.pdf filename=2402.08788v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper reports our work on building up a Cantonese <b>Speech-to-Text</b> <b>(STT)</b> system with a syllable based acoustic model. This is a part of an effort in building a STT system to aid dyslexic students who have cognitive deficiency in writing skills but have no problem expressing their ideas through <b>speech.</b> <b>For</b> Cantonese <b>speech</b> <b>recognition,</b> the basic unit of acoustic models can either be the conventional Initial-Final (IF) syllables, or the Onset-Nucleus-Coda (ONC) syllables where finals are further split into nucleus and coda to reflect the intra-syllable variations in Cantonese. By using the Kaldi toolkit, our system is trained using the <b>stochastic</b> <b>gradient</b> <b>descent</b> optimization model with the aid of GPUs for the hybrid Deep Neural Network and Hidden Markov Model (DNN-HMM) with and without I-vector based speaker adaptive training technique. The input features of the same Gaussian Mixture Model with speaker adaptive training (GMM-SAT) to DNN are used in all cases. Experiments show that the ONC-based syllable acoustic modeling with I-vector based DNN-HMM achieves the best performance with the word error rate (WER) of 9.66% and the real time factor (RTF) of 1.38812.</p></p class="citation"></blockquote><h3 id=2734--27247-semrel2024-a-collection-of-semantic-textual-relatedness-datasets-for-14-languages-nedjma-ousidhoum-et-al-2024>(27/34 | 27/247) SemRel2024: A Collection of Semantic Textual Relatedness Datasets for 14 Languages (Nedjma Ousidhoum et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nedjma Ousidhoum, Shamsuddeen Hassan Muhammad, Mohamed Abdalla, Idris Abdulmumin, Ibrahim Said Ahmad, Sanchit Ahuja, Alham Fikri Aji, Vladimir Araujo, Abinew Ali Ayele, Pavan Baswani, Meriem Beloucif, Chris Biemann, Sofia Bourhim, Christine De Kock, Genet Shanko Dekebo, Oumaima Hourrane, Gopichand Kanumolu, Lokesh Madasu, Samuel Rutunda, Manish Shrivastava, Thamar Solorio, Nirmal Surange, Hailegnaw Getaneh Tilaye, Krishnapriya Vishnubhotla, Genta Winata, Seid Muhie Yimam, Saif M. Mohammad. (2024)<br><strong>SemRel2024: A Collection of Semantic Textual Relatedness Datasets for 14 Languages</strong><br><button class=copy-to-clipboard title="SemRel2024: A Collection of Semantic Textual Relatedness Datasets for 14 Languages" index=27>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-27 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08638v3 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08638v3.pdf filename=2402.08638v3.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Exploring and quantifying semantic relatedness is central to representing language. It holds significant implications across various NLP tasks, including offering insights into the capabilities and performance of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs).</b> While earlier NLP research primarily focused on semantic similarity, often within the English language context, we instead investigate the broader phenomenon of semantic relatedness. In this paper, we present SemRel, a new semantic relatedness dataset collection annotated by native speakers across 14 languages:Afrikaans, Algerian Arabic, Amharic, English, Hausa, Hindi, Indonesian, Kinyarwanda, Marathi, Moroccan Arabic, Modern Standard Arabic, Punjabi, Spanish, and Telugu. These languages originate from five distinct language families and are predominantly spoken in Africa and Asia &ndash; regions characterised by a relatively limited availability of NLP resources. Each instance in the SemRel datasets is a sentence pair associated with a score that represents the degree of semantic textual relatedness between the two sentences. The scores are obtained using a comparative annotation framework. We describe the data collection and annotation processes, related challenges when building the datasets, and their impact and utility in NLP. We further report experiments for each language and across the different languages.</p></p class="citation"></blockquote><h3 id=2834--28247-improving-factual-error-correction-for-abstractive-summarization-via-data-distillation-and-conditional-generation-cloze-yiyang-li-et-al-2024>(28/34 | 28/247) Improving Factual Error Correction for Abstractive Summarization via Data Distillation and Conditional-generation Cloze (Yiyang Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yiyang Li, Lei Li, Dingxin Hu, Xueyi Hao, Marina Litvak, Natalia Vanetik, Yanquan Zhou. (2024)<br><strong>Improving Factual Error Correction for Abstractive Summarization via Data Distillation and Conditional-generation Cloze</strong><br><button class=copy-to-clipboard title="Improving Factual Error Correction for Abstractive Summarization via Data Distillation and Conditional-generation Cloze" index=28>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-28 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Knowledge Distillation, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08581v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08581v1.pdf filename=2402.08581v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Improving factual consistency in abstractive <b>summarization</b> has been a focus of current research. One promising approach is the post-editing method. However, previous works have yet to make sufficient use of factual factors in summaries and suffers from the negative effect of the training datasets. In this paper, we first propose a novel factual error correction model FactCloze based on a conditional-generation cloze task. FactCloze can construct the causality among factual factors while being able to determine whether the blank can be answered or not. Then, we propose a data <b>distillation</b> method to generate a more faithful <b>summarization</b> dataset SummDSC via multiple-dimensional evaluation. We experimentally validate the effectiveness of our approach, which leads to an improvement in multiple factual consistency metrics compared to baselines.</p></p class="citation"></blockquote><h3 id=2934--29247-llms-and-the-human-condition-peter-wallis-2024>(29/34 | 29/247) LLMs and the Human Condition (Peter Wallis, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Peter Wallis. (2024)<br><strong>LLMs and the Human Condition</strong><br><button class=copy-to-clipboard title="LLMs and the Human Condition" index=29>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-29 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: ChatGPT, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08403v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08403v1.pdf filename=2402.08403v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents three established theories of human decision-making and describes how they can be integrated to provide a model of purposive human action. Taking seriously the idea of language as action the model is then applied to the conversational user interfaces. Theory based AI research has had a hard time recently and the aim here is to revitalise interest in understanding what <b>LLMs</b> are actually doing other than running poorly understood machine learning routines over all the data the relevant Big Tech company can hoover up. When a raspberry pi computer for under 50USD is up to 400 times faster than the first commercial Cray super computer~\cite{crayVpi}, Big Tech can get really close to having an infinite number of monkeys typing at random and producing text, some of which will make sense. By understanding where <b>ChatGPT&rsquo;s</b> apparent intelligence comes from, perhaps we can perform the magic with fewer resources and at the same time gain some understanding about our relationship with our world.</p></p class="citation"></blockquote><h3 id=3034--30247-large-language-models-as-minecraft-agents-chris-madge-et-al-2024>(30/34 | 30/247) Large Language Models as Minecraft Agents (Chris Madge et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chris Madge, Massimo Poesio. (2024)<br><strong>Large Language Models as Minecraft Agents</strong><br><button class=copy-to-clipboard title="Large Language Models as Minecraft Agents" index=30>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-30 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08392v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08392v1.pdf filename=2402.08392v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this work we examine the use of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> in the challenging setting of acting as a Minecraft agent. We apply and evaluate <b>LLMs</b> in the builder and architect settings, introduce clarification questions and examining the challenges and opportunities for improvement. In addition, we present a platform for online interaction with the agents and an evaluation against previous works.</p></p class="citation"></blockquote><h3 id=3134--31247-explicit-references-to-social-values-in-fairy-tales-a-comparison-between-three-european-cultures-alba-morollon-diaz-faes-et-al-2024>(31/34 | 31/247) Explicit References to Social Values in Fairy Tales: A Comparison between Three European Cultures (Alba Morollon Diaz-Faes et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Alba Morollon Diaz-Faes, Carla Sofia Ribeiro Murteira, Martin Ruskov. (2024)<br><strong>Explicit References to Social Values in Fairy Tales: A Comparison between Three European Cultures</strong><br><button class=copy-to-clipboard title="Explicit References to Social Values in Fairy Tales: A Comparison between Three European Cultures" index=31>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-31 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: J-5; K-4-m, cs-CL, cs-CY, cs.CL<br>Keyword Score: 20<br>Keywords: Word2vec, Word Embedding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08318v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08318v1.pdf filename=2402.08318v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The study of social values in fairy tales opens the possibility to learn about the communication of values across space and time. We propose to study the communication of values in fairy tales from Portugal, Italy and Germany using a technique called <b>word</b> <b>embedding</b> with a compass to quantify vocabulary differences and commonalities. We study how these three national traditions of fairy tales differ in their explicit references to values. To do this, we specify a list of value-charged tokens, consider their <b>word</b> <b>stems</b> and analyse the distance between these in a bespoke pre-trained <b>Word2Vec</b> model. We triangulate and critically discuss the validity of the resulting hypotheses emerging from this quantitative model. Our claim is that this is a reusable and reproducible method for the study of the values explicitly referenced in historical corpora. Finally, our preliminary findings hint at a shared cultural understanding and the expression of values such as Benevolence, Conformity, and Universalism across European societies, suggesting the existence of a pan-European cultural memory.</p></p class="citation"></blockquote><h3 id=3234--32247-chatcell-facilitating-single-cell-analysis-with-natural-language-yin-fang-et-al-2024>(32/34 | 32/247) ChatCell: Facilitating Single-Cell Analysis with Natural Language (Yin Fang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yin Fang, Kangwei Liu, Ningyu Zhang, Xinle Deng, Penghui Yang, Zhuo Chen, Xiangru Tang, Mark Gerstein, Xiaohui Fan, Huajun Chen. (2024)<br><strong>ChatCell: Facilitating Single-Cell Analysis with Natural Language</strong><br><button class=copy-to-clipboard title="ChatCell: Facilitating Single-Cell Analysis with Natural Language" index=32>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-32 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CE, cs-CL, cs-HC, cs-LG, cs.CL<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08303v3 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08303v3.pdf filename=2402.08303v3.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>As <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> rapidly evolve, their influence in science is becoming increasingly prominent. The emerging capabilities of <b>LLMs</b> in task generalization and free-form dialogue can significantly advance fields like chemistry and biology. However, the field of single-cell biology, which forms the foundational building blocks of living organisms, still faces several challenges. High knowledge barriers and limited scalability in current methods restrict the full exploitation of <b>LLMs</b> in mastering single-cell data, impeding direct accessibility and rapid iteration. To this end, we introduce ChatCell, which signifies a paradigm shift by facilitating single-cell analysis with natural language. Leveraging vocabulary adaptation and unified sequence generation, ChatCell has acquired profound expertise in single-cell biology and the capability to accommodate a diverse range of analysis tasks. Extensive experiments further demonstrate ChatCell&rsquo;s robust performance and potential to deepen single-cell insights, paving the way for more accessible and intuitive exploration in this pivotal field. Our project homepage is available at <a href=https://zjunlp.github.io/project/ChatCell>https://zjunlp.github.io/project/ChatCell</a>.</p></p class="citation"></blockquote><h3 id=3334--33247-a-systematic-review-of-data-to-text-nlg-chinonso-cynthia-osuji-et-al-2024>(33/34 | 33/247) A Systematic Review of Data-to-Text NLG (Chinonso Cynthia Osuji et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chinonso Cynthia Osuji, Thiago Castro Ferreira, Brian Davis. (2024)<br><strong>A Systematic Review of Data-to-Text NLG</strong><br><button class=copy-to-clipboard title="A Systematic Review of Data-to-Text NLG" index=33>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-33 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 10<br>Keywords: Natural Language Generation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08496v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08496v1.pdf filename=2402.08496v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This systematic review aims to provide a comprehensive analysis of the state of data-to-text generation research, focusing on identifying research gaps, offering future directions, and addressing challenges found during the review. We thoroughly examined the literature, including approaches, datasets, evaluation metrics, applications, multilingualism, and hallucination mitigation measures. Our review provides a roadmap for future research in this rapidly evolving field.</p></p class="citation"></blockquote><h3 id=3434--34247-cma-rcausal-mediation-analysis-for-explaining-rumour-detection-lin-tian-et-al-2024>(34/34 | 34/247) CMA-R:Causal Mediation Analysis for Explaining Rumour Detection (Lin Tian et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lin Tian, Xiuzhen Zhang, Jey Han Lau. (2024)<br><strong>CMA-R:Causal Mediation Analysis for Explaining Rumour Detection</strong><br><button class=copy-to-clipboard title="CMA-R:Causal Mediation Analysis for Explaining Rumour Detection" index=34>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-34 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 5<br>Keywords: Black Box<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08155v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08155v1.pdf filename=2402.08155v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We apply causal mediation analysis to explain the decision-making process of neural models for rumour detection on Twitter. Interventions at the input and network level reveal the causal impacts of tweets and words in the model output. We find that our approach CMA-R &ndash; Causal Mediation Analysis for Rumour detection &ndash; identifies salient tweets that explain model predictions and show strong agreement with human judgements for critical tweets determining the truthfulness of stories. CMA-R can further highlight causally impactful words in the salient tweets, providing another layer of interpretability and transparency into these blackbox rumour detection systems. Code is available at: <a href=https://github.com/ltian678/cma-r>https://github.com/ltian678/cma-r</a>.</p></p class="citation"></blockquote><h2 id=cslg-52>cs.LG (52)</h2><h3 id=152--35247-cold-attack-jailbreaking-llms-with-stealthiness-and-controllability-xingang-guo-et-al-2024>(1/52 | 35/247) COLD-Attack: Jailbreaking LLMs with Stealthiness and Controllability (Xingang Guo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xingang Guo, Fangxu Yu, Huan Zhang, Lianhui Qin, Bin Hu. (2024)<br><strong>COLD-Attack: Jailbreaking LLMs with Stealthiness and Controllability</strong><br><button class=copy-to-clipboard title="COLD-Attack: Jailbreaking LLMs with Stealthiness and Controllability" index=35>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-35 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CL, cs-LG, cs.LG<br>Keyword Score: 90<br>Keywords: GPT, GPT-3, GPT-3.5, LLaMA, Mistral, Text Generation, In-context Learning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08679v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08679v1.pdf filename=2402.08679v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Jailbreaks on <b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have recently received increasing attention. For a comprehensive assessment of <b>LLM</b> safety, it is essential to consider jailbreaks with diverse attributes, such as contextual coherence and sentiment/stylistic variations, and hence it is beneficial to study controllable jailbreaking, i.e. how to enforce control on <b>LLM</b> attacks. In this paper, we formally formulate the controllable attack generation problem, and build a novel connection between this problem and controllable <b>text</b> <b>generation,</b> a well-explored topic of natural language processing. Based on this connection, we adapt the Energy-based Constrained Decoding with Langevin Dynamics (COLD), a state-of-the-art, highly efficient algorithm in controllable <b>text</b> <b>generation,</b> and introduce the COLD-Attack framework which unifies and automates the search of adversarial <b>LLM</b> attacks under a variety of control requirements such as fluency, stealthiness, sentiment, and left-right-coherence. The controllability enabled by COLD-Attack leads to diverse new jailbreak scenarios which not only cover the standard setting of generating fluent suffix attacks, but also allow us to address new controllable attack settings such as revising a user query adversarially with minimal paraphrasing, and inserting stealthy attacks in context with left-right-coherence. Our extensive experiments on various <b>LLMs</b> <b>(Llama-2,</b> <b>Mistral,</b> Vicuna, Guanaco, <b>GPT-3.5)</b> show COLD-Attack&rsquo;s broad applicability, strong controllability, high success rate, and attack transferability. Our code is available at <a href=https://github.com/Yu-Fangxu/COLD-Attack>https://github.com/Yu-Fangxu/COLD-Attack</a>.</p></p class="citation"></blockquote><h3 id=252--36247-llaga-large-language-and-graph-assistant-runjin-chen-et-al-2024>(2/52 | 36/247) LLaGA: Large Language and Graph Assistant (Runjin Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Runjin Chen, Tong Zhao, Ajay Jaiswal, Neil Shah, Zhangyang Wang. (2024)<br><strong>LLaGA: Large Language and Graph Assistant</strong><br><button class=copy-to-clipboard title="LLaGA: Large Language and Graph Assistant" index=36>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-36 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 86<br>Keywords: Graph, Graph Neural Network, Graph Neural Network, Benchmarking, Supervised Learning, Zero-shot, GPT, GPT-4, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08170v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08170v2.pdf filename=2402.08170v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Graph</b> <b>Neural</b> <b>Networks</b> <b>(GNNs)</b> have empowered the advance in <b>graph-structured</b> <b>data</b> <b>analysis.</b> Recently, the rise of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> like <b>GPT-4</b> has heralded a new era in deep learning. However, their application to <b>graph</b> <b>data</b> <b>poses</b> distinct challenges due to the inherent difficulty of translating <b>graph</b> <b>structures</b> <b>to</b> language. To this end, we introduce the <b>Large</b> <b>Language</b> <b>and</b> <b>Graph</b> <b>Assistant</b> <b>(LLaGA),</b> an innovative model that effectively integrates <b>LLM</b> capabilities to handle the complexities of <b>graph-structured</b> <b>data.</b> <b>LLaGA</b> retains the general-purpose nature of <b>LLMs</b> while adapting <b>graph</b> <b>data</b> <b>into</b> a format compatible with <b>LLM</b> input. LLaGA achieves this by reorganizing <b>graph</b> <b>nodes</b> <b>to</b> structure-aware sequences and then mapping these into the token embedding space through a versatile projector. LLaGA excels in versatility, generalizability and interpretability, allowing it to perform consistently well across different datasets and tasks, extend its ability to unseen datasets or tasks, and provide explanations for <b>graphs.</b> <b>Our</b> <b>extensive</b> experiments across popular <b>graph</b> <b>benchmarks</b> <b>show</b> that LLaGA delivers outstanding performance across four datasets and three tasks using one single model, surpassing state-of-the-art <b>graph</b> <b>models</b> <b>in</b> both <b>supervised</b> and <b>zero-shot</b> scenarios. Our code is available at \url{https://github.com/VITA-Group/LLaGA}.</p></p class="citation"></blockquote><h3 id=352--37247-mitigating-object-hallucination-in-large-vision-language-models-via-classifier-free-guidance-linxi-zhao-et-al-2024>(3/52 | 37/247) Mitigating Object Hallucination in Large Vision-Language Models via Classifier-Free Guidance (Linxi Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Linxi Zhao, Yihe Deng, Weitong Zhang, Quanquan Gu. (2024)<br><strong>Mitigating Object Hallucination in Large Vision-Language Models via Classifier-Free Guidance</strong><br><button class=copy-to-clipboard title="Mitigating Object Hallucination in Large Vision-Language Models via Classifier-Free Guidance" index=37>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-37 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CL, cs-CV, cs-LG, cs.LG<br>Keyword Score: 70<br>Keywords: Fine-tuning, GPT, GPT-3, GPT-3.5, Grounding, Large Language Model, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08680v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08680v1.pdf filename=2402.08680v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The advancement of Large <b>Vision-Language</b> Models (LVLMs) has increasingly highlighted the critical issue of their tendency to hallucinate non-existing objects in the images. To address this issue, previous works focused on using specially curated datasets or powerful <b>LLMs</b> (e.g., <b>GPT-3.5)</b> to rectify the outputs of LVLMs. However, these approaches require either expensive training/fine-tuning or API access to advanced <b>LLMs</b> to correct the model&rsquo;s output post-generation. In this paper, we tackle this challenge by introducing a framework called Mitigating hallucinAtion via classifieR-Free guIdaNcE (MARINE), which is both training-free and API-free, and can effectively and efficiently reduce object hallucinations during the generation process. Specifically, MARINE enriches the visual context of LVLMs by integrating existing open-source vision models, and employs classifier-free guidance to incorporate the additional object <b>grounding</b> features to improve the precision of LVLMs&rsquo; generations. Through comprehensive evaluations across $6$ popular LVLMs with diverse evaluation metrics, we demonstrate the effectiveness of MARINE, which even outperforms existing <b>fine-tuning-based</b> methods. Remarkably, it not only reduces hallucinations but also improves the detailedness of LVLMs&rsquo; generations, as assessed by <b>GPT-4V.</b></p></p class="citation"></blockquote><h3 id=452--38247-disambiguated-node-classification-with-graph-neural-networks-tianxiang-zhao-et-al-2024>(4/52 | 38/247) Disambiguated Node Classification with Graph Neural Networks (Tianxiang Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tianxiang Zhao, Xiang Zhang, Suhang Wang. (2024)<br><strong>Disambiguated Node Classification with Graph Neural Networks</strong><br><button class=copy-to-clipboard title="Disambiguated Node Classification with Graph Neural Networks" index=38>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-38 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs-SI, cs.LG<br>Keyword Score: 68<br>Keywords: Node Classification, Graph, Graph Neural Network, Graph Neural Network, Node Embedding, Contrastive Learning, Representation Learning, Disambiguation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08824v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08824v1.pdf filename=2402.08824v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Graph</b> <b>Neural</b> <b>Networks</b> <b>(GNNs)</b> have demonstrated significant success in learning from <b>graph-structured</b> <b>data</b> <b>across</b> various domains. Despite their great successful, one critical challenge is often overlooked by existing works, i.e., the learning of message propagation that can generalize effectively to underrepresented <b>graph</b> <b>regions.</b> <b>These</b> minority regions often exhibit irregular homophily/heterophily patterns and diverse neighborhood class distributions, resulting in ambiguity. In this work, we investigate the ambiguity problem within <b>GNNs,</b> its impact on <b>representation</b> <b>learning,</b> and the development of richer supervision signals to fight against this problem. We conduct a fine-grained evaluation of <b>GNN,</b> analyzing the existence of ambiguity in different <b>graph</b> <b>regions</b> <b>and</b> its relation with <b>node</b> <b>positions.</b> To disambiguate <b>node</b> <b>embeddings,</b> we propose a novel method, {\method}, which exploits additional optimization guidance to enhance <b>representation</b> <b>learning,</b> particularly for <b>nodes</b> <b>in</b> ambiguous regions. {\method} identifies ambiguous <b>nodes</b> <b>based</b> on temporal inconsistency of predictions and introduces a <b>disambiguation</b> regularization by employing <b>contrastive</b> <b>learning</b> in a topology-aware manner. {\method} promotes discriminativity of <b>node</b> <b>representations</b> <b>and</b> can alleviating semantic mixing caused by message propagation, effectively addressing the ambiguity problem. Empirical results validate the efficiency of {\method} and highlight its potential to improve <b>GNN</b> performance in underrepresented <b>graph</b> regions.</p></p class="citation"></blockquote><h3 id=552--39247-improving-black-box-robustness-with-in-context-rewriting-kyle-obrien-et-al-2024>(5/52 | 39/247) Improving Black-box Robustness with In-Context Rewriting (Kyle O&rsquo;Brien et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kyle O&rsquo;Brien, Nathan Ng, Isha Puri, Jorge Mendez, Hamid Palangi, Yoon Kim, Marzyeh Ghassemi, Thomas Hartvigsen. (2024)<br><strong>Improving Black-box Robustness with In-Context Rewriting</strong><br><button class=copy-to-clipboard title="Improving Black-box Robustness with In-Context Rewriting" index=39>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-39 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 65<br>Keywords: Black Box, High-Resource, Out-of-distribution, BERT, T5, In-context Learning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08225v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08225v2.pdf filename=2402.08225v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Machine learning models often excel on in-distribution (ID) data but struggle with unseen <b>out-of-distribution</b> (OOD) inputs. Most techniques for improving OOD robustness are not applicable to settings where the model is effectively a <b>black</b> <b>box,</b> such as when the weights are frozen, retraining is costly, or the model is leveraged via an API. Test-time augmentation (TTA) is a simple post-hoc technique for improving robustness that sidesteps <b>black-box</b> <b>constraints</b> by aggregating predictions across multiple augmentations of the test input. TTA has seen limited use in NLP due to the challenge of generating effective natural language augmentations. In this work, we propose <b>LLM-TTA,</b> which uses <b>LLM-generated</b> augmentations as TTA&rsquo;s augmentation function. <b>LLM-TTA</b> outperforms conventional augmentation functions across sentiment, toxicity, and news classification tasks for <b>BERT</b> and <b>T5</b> models, with <b>BERT&rsquo;s</b> OOD robustness improving by an average of 4.30 percentage points without regressing average ID performance. We explore selectively augmenting inputs based on prediction entropy to reduce the rate of expensive <b>LLM</b> augmentations, allowing us to maintain performance gains while reducing the average number of generated augmentations by 57.76%. <b>LLM-TTA</b> is agnostic to the task model architecture, does not require OOD labels, and is effective across low and <b>high-resource</b> settings. We share our data, models, and code for reproducibility.</p></p class="citation"></blockquote><h3 id=652--40247-graph-mamba-towards-learning-on-graphs-with-state-space-models-ali-behrouz-et-al-2024>(6/52 | 40/247) Graph Mamba: Towards Learning on Graphs with State Space Models (Ali Behrouz et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ali Behrouz, Farnoosh Hashemi. (2024)<br><strong>Graph Mamba: Towards Learning on Graphs with State Space Models</strong><br><button class=copy-to-clipboard title="Graph Mamba: Towards Learning on Graphs with State Space Models" index=40>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-40 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 61<br>Keywords: Message-Passing, Graph, Graph Neural Network, Graph Neural Network, Benchmarking, Representation Learning, Transformer, Tokenization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08678v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08678v2.pdf filename=2402.08678v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Graph</b> <b>Neural</b> <b>Networks</b> <b>(GNNs)</b> have shown promising potential in <b>graph</b> <b>representation</b> <b>learning.</b> The majority of <b>GNNs</b> define a local <b>message-passing</b> mechanism, propagating information over the <b>graph</b> <b>by</b> <b>stacking</b> multiple layers. These methods, however, are known to suffer from two major limitations: over-squashing and poor capturing of long-range dependencies. Recently, <b>Graph</b> <b>Transformers</b> <b>(GTs)</b> emerged as a powerful alternative to <b>Message-Passing</b> Neural Networks (MPNNs). GTs, however, have quadratic computational cost, lack inductive biases on <b>graph</b> <b>structures,</b> <b>and</b> rely on complex Positional/Structural Encodings (SE/PE). In this paper, we show that while <b>Transformers,</b> complex <b>message-passing,</b> and SE/PE are sufficient for good performance in practice, neither is necessary. Motivated by the recent success of State Space Models (SSMs), such as Mamba, we present <b>Graph</b> <b>Mamba</b> <b>Networks</b> (GMNs), a general framework for a new class of <b>GNNs</b> based on selective SSMs. We discuss and categorize the new challenges when adapting SSMs to <b>graph-structured</b> <b>data,</b> <b>and</b> present four required and one optional steps to design GMNs, where we choose (1) Neighborhood <b>Tokenization,</b> (2) Token Ordering, (3) Architecture of Bidirectional Selective SSM Encoder, (4) Local Encoding, and dispensable (5) PE and SE. We further provide theoretical justification for the power of GMNs. Experiments demonstrate that despite much less computational cost, GMNs attain an outstanding performance in long-range, small-scale, large-scale, and heterophilic <b>benchmark</b> datasets.</p></p class="citation"></blockquote><h3 id=752--41247-rethinking-machine-unlearning-for-large-language-models-sijia-liu-et-al-2024>(7/52 | 41/247) Rethinking Machine Unlearning for Large Language Models (Sijia Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sijia Liu, Yuanshun Yao, Jinghan Jia, Stephen Casper, Nathalie Baracaldo, Peter Hase, Xiaojun Xu, Yuguang Yao, Hang Li, Kush R. Varshney, Mohit Bansal, Sanmi Koyejo, Yang Liu. (2024)<br><strong>Rethinking Machine Unlearning for Large Language Models</strong><br><button class=copy-to-clipboard title="Rethinking Machine Unlearning for Large Language Models" index=41>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-41 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CL, cs-LG, cs.LG<br>Keyword Score: 60<br>Keywords: Adversarial Learning, Generative AI, Machine Unlearning, Reinforcement Learning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08787v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08787v2.pdf filename=2402.08787v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We explore <b>machine</b> <b>unlearning</b> (MU) in the domain of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs),</b> referred to as <b>LLM</b> unlearning. This initiative aims to eliminate undesirable data influence (e.g., sensitive or illegal information) and the associated model capabilities, while maintaining the integrity of essential knowledge generation and not affecting causally unrelated information. We envision <b>LLM</b> unlearning becoming a pivotal element in the life-cycle management of <b>LLMs,</b> potentially standing as an essential foundation for developing <b>generative</b> <b>AI</b> that is not only safe, secure, and trustworthy, but also resource-efficient without the need of full retraining. We navigate the unlearning landscape in <b>LLMs</b> from conceptual formulation, methodologies, metrics, and applications. In particular, we highlight the often-overlooked aspects of existing <b>LLM</b> unlearning research, e.g., unlearning scope, data-model interaction, and multifaceted efficacy assessment. We also draw connections between <b>LLM</b> unlearning and related areas such as model editing, influence functions, model explanation, <b>adversarial</b> <b>training,</b> and <b>reinforcement</b> <b>learning.</b> Furthermore, we outline an effective assessment framework for <b>LLM</b> unlearning and explore its applications in copyright and privacy safeguards and sociotechnical harm reduction.</p></p class="citation"></blockquote><h3 id=852--42247-prompted-contextual-vectors-for-spear-phishing-detection-daniel-nahmias-et-al-2024>(8/52 | 42/247) Prompted Contextual Vectors for Spear-Phishing Detection (Daniel Nahmias et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Daniel Nahmias, Gal Engelberg, Dan Klein, Asaf Shabtai. (2024)<br><strong>Prompted Contextual Vectors for Spear-Phishing Detection</strong><br><button class=copy-to-clipboard title="Prompted Contextual Vectors for Spear-Phishing Detection" index=42>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-42 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: I-2-7, cs-CL, cs-CR, cs-LG, cs.LG<br>Keyword Score: 60<br>Keywords: Supervised Learning, Document Classification, Reasoning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08309v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08309v2.pdf filename=2402.08309v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Spear-phishing attacks present a significant security challenge, with <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> escalating the threat by generating convincing emails and facilitating target reconnaissance. To address this, we propose a detection approach based on a novel <b>document</b> <b>vectorization</b> method that utilizes an ensemble of <b>LLMs</b> to create representation vectors. By <b>prompting</b> <b>LLMs</b> to reason and respond to human-crafted questions, we quantify the presence of common persuasion principles in the email&rsquo;s content, producing <b>prompted</b> contextual <b>document</b> <b>vectors</b> for a downstream <b>supervised</b> machine learning model. We evaluate our method using a unique dataset generated by a proprietary system that automates target reconnaissance and spear-phishing email creation. Our method achieves a 91% F1 score in identifying <b>LLM-generated</b> spear-phishing emails, with the training set comprising only traditional phishing and benign emails. Key contributions include an innovative <b>document</b> <b>vectorization</b> method utilizing <b>LLM</b> <b>reasoning,</b> a publicly available dataset of high-quality spear-phishing emails, and the demonstrated effectiveness of our method in detecting such emails. This methodology can be utilized for various <b>document</b> <b>classification</b> tasks, particularly in adversarial problem domains.</p></p class="citation"></blockquote><h3 id=952--43247-prdp-proximal-reward-difference-prediction-for-large-scale-reward-finetuning-of-diffusion-models-fei-deng-et-al-2024>(9/52 | 43/247) PRDP: Proximal Reward Difference Prediction for Large-Scale Reward Finetuning of Diffusion Models (Fei Deng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fei Deng, Qifei Wang, Wei Wei, Matthias Grundmann, Tingbo Hou. (2024)<br><strong>PRDP: Proximal Reward Difference Prediction for Large-Scale Reward Finetuning of Diffusion Models</strong><br><button class=copy-to-clipboard title="PRDP: Proximal Reward Difference Prediction for Large-Scale Reward Finetuning of Diffusion Models" index=43>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-43 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 55<br>Keywords: Black Box, Fine-tuning, Foundation Model, Reinforcement Learning, Supervised Learning, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08714v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08714v1.pdf filename=2402.08714v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Reward <b>finetuning</b> has emerged as a promising approach to aligning <b>foundation</b> <b>models</b> with downstream objectives. Remarkable success has been achieved in the language domain by using <b>reinforcement</b> <b>learning</b> (RL) to maximize rewards that reflect human preference. However, in the vision domain, existing RL-based reward <b>finetuning</b> methods are limited by their instability in large-scale training, rendering them incapable of generalizing to complex, unseen <b>prompts.</b> In this paper, we propose Proximal Reward Difference Prediction (PRDP), enabling stable <b>black-box</b> <b>reward</b> <b>finetuning</b> for diffusion models for the first time on large-scale <b>prompt</b> datasets with over 100K <b>prompts.</b> Our key innovation is the Reward Difference Prediction (RDP) objective that has the same optimal solution as the RL objective while enjoying better training stability. Specifically, the RDP objective is a <b>supervised</b> regression objective that tasks the diffusion model with predicting the reward difference of generated image pairs from their denoising trajectories. We theoretically prove that the diffusion model that obtains perfect reward difference prediction is exactly the maximizer of the RL objective. We further develop an online algorithm with proximal updates to stably optimize the RDP objective. In experiments, we demonstrate that PRDP can match the reward maximization ability of well-established RL-based methods in small-scale training. Furthermore, through large-scale training on text <b>prompts</b> from the Human Preference Dataset v2 and the Pick-a-Pic v1 dataset, PRDP achieves superior generation quality on a diverse set of complex, unseen <b>prompts</b> whereas RL-based methods completely fail.</p></p class="citation"></blockquote><h3 id=1052--44247-fedlps-heterogeneous-federated-learning-for-multiple-tasks-with-local-parameter-sharing-yongzhe-jia-et-al-2024>(10/52 | 44/247) FedLPS: Heterogeneous Federated Learning for Multiple Tasks with Local Parameter Sharing (Yongzhe Jia et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yongzhe Jia, Xuyun Zhang, Amin Beheshti, Wanchun Dou. (2024)<br><strong>FedLPS: Heterogeneous Federated Learning for Multiple Tasks with Local Parameter Sharing</strong><br><button class=copy-to-clipboard title="FedLPS: Heterogeneous Federated Learning for Multiple Tasks with Local Parameter Sharing" index=44>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-44 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-DC, cs-LG, cs.LG<br>Keyword Score: 50<br>Keywords: Federated Learning, Model Pruning, Parameter Sharing, Pruning, Transfer Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08578v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08578v1.pdf filename=2402.08578v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Federated</b> <b>Learning</b> (FL) has emerged as a promising solution in Edge Computing (EC) environments to process the proliferation of data generated by edge devices. By collaboratively optimizing the global machine learning <b>models</b> <b>on</b> distributed edge devices, FL circumvents the need for transmitting raw data and enhances user privacy. Despite practical successes, FL still confronts significant challenges including constrained edge device resources, multiple tasks deployment, and data heterogeneity. However, existing studies focus on mitigating the FL training costs of each single task whereas neglecting the resource consumption across multiple tasks in heterogeneous FL scenarios. In this paper, we propose Heterogeneous <b>Federated</b> <b>Learning</b> with Local <b>Parameter</b> <b>Sharing</b> (FedLPS) to fill this gap. FedLPS leverages principles from <b>transfer</b> <b>learning</b> to facilitate the deployment of multiple tasks on a single device by dividing the local <b>model</b> <b>into</b> a shareable encoder and task-specific encoders. To further reduce resource consumption, a channel-wise <b>model</b> <b>pruning</b> algorithm that shrinks the footprint of local <b>models</b> <b>while</b> accounting for both data and system heterogeneity is employed in FedLPS. Additionally, a novel heterogeneous <b>model</b> <b>aggregation</b> algorithm is proposed to aggregate the heterogeneous predictors in FedLPS. We implemented the proposed FedLPS on a real FL platform and compared it with state-of-the-art (SOTA) FL frameworks. The experimental results on five popular datasets and two modern DNN <b>models</b> <b>illustrate</b> that the proposed FedLPS significantly outperforms the SOTA FL frameworks by up to 4.88% and reduces the computational resource consumption by 21.3%. Our code is available at:https://github.com/jyzgh/FedLPS.</p></p class="citation"></blockquote><h3 id=1152--45247-apalu-a-trainable-adaptive-activation-function-for-deep-learning-networks-barathi-subramanian-et-al-2024>(11/52 | 45/247) APALU: A Trainable, Adaptive Activation Function for Deep Learning Networks (Barathi Subramanian et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Barathi Subramanian, Rathinaraja Jeyaraj, Rakhmonov Akhrorjon Akhmadjon Ugli, Jeonghong Kim. (2024)<br><strong>APALU: A Trainable, Adaptive Activation Function for Deep Learning Networks</strong><br><button class=copy-to-clipboard title="APALU: A Trainable, Adaptive Activation Function for Deep Learning Networks" index=45>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-45 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs-NE, cs.LG<br>Keyword Score: 50<br>Keywords: MNIST, Anomaly Detection, Knowledge Distillation, Knowledge Distillation, Recurrent Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08244v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08244v1.pdf filename=2402.08244v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Activation function is a pivotal component of deep learning, facilitating the extraction of intricate data patterns. While classical activation functions like ReLU and its variants are extensively utilized, their static nature and simplicity, despite being advantageous, often limit their effectiveness in specialized tasks. The trainable activation functions also struggle sometimes to adapt to the unique characteristics of the data. Addressing these limitations, we introduce a novel trainable activation function, adaptive piecewise approximated activation linear unit (APALU), to enhance the learning performance of deep learning across a broad range of tasks. It presents a unique set of features that enable it to maintain stability and efficiency in the learning process while adapting to complex data representations. Experiments reveal significant improvements over widely used activation functions for different tasks. In image classification, APALU increases MobileNet and GoogleNet accuracy by 0.37% and 0.04%, respectively, on the CIFAR10 dataset. In <b>anomaly</b> <b>detection,</b> it improves the average area under the curve of One-CLASS Deep SVDD by 0.8% on the <b>MNIST</b> dataset, 1.81% and 1.11% improvements with DifferNet, and <b>knowledge</b> <b>distillation,</b> respectively, on the MVTech dataset. Notably, APALU achieves 100% accuracy on a sign language recognition task with a limited dataset. For regression tasks, APALU enhances the performance of deep neural networks and <b>recurrent</b> <b>neural</b> <b>networks</b> on different datasets. These improvements highlight the robustness and adaptability of APALU across diverse deep-learning applications.</p></p class="citation"></blockquote><h3 id=1252--46247-world-model-on-million-length-video-and-language-with-ringattention-hao-liu-et-al-2024>(12/52 | 46/247) World Model on Million-Length Video And Language With RingAttention (Hao Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hao Liu, Wilson Yan, Matei Zaharia, Pieter Abbeel. (2024)<br><strong>World Model on Million-Length Video And Language With RingAttention</strong><br><button class=copy-to-clipboard title="World Model on Million-Length Video And Language With RingAttention" index=46>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-46 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 49<br>Keywords: Benchmarking, Multi-modal, Multi-modal, Transformer, Question Answering, Video-and-Language, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08268v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08268v1.pdf filename=2402.08268v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Current language models fall short in understanding aspects of the world not easily described in words, and struggle with complex, long-form tasks. Video sequences offer valuable temporal information absent in language and static images, making them attractive for joint modeling with language. Such models could develop a understanding of both human textual knowledge and the physical world, enabling broader AI capabilities for assisting humans. However, learning from millions of tokens of video and language sequences poses challenges due to memory constraints, computational complexity, and limited datasets. To address these challenges, we curate a large dataset of diverse videos and books, utilize the RingAttention technique to scalably train on long sequences, and gradually increase context size from 4K to 1M tokens. This paper makes the following contributions: (a) Largest context size neural network: We train one of the largest context size <b>transformers</b> on long video and language sequences, setting new <b>benchmarks</b> in difficult retrieval tasks and long video understanding. (b) Solutions for overcoming <b>vision-language</b> training challenges, including using masked sequence packing for mixing different sequence lengths, loss weighting to balance language and vision, and model-generated <b>QA</b> dataset for long sequence chat. (c) A highly-optimized implementation with RingAttention, masked sequence packing, and other key features for training on millions-length <b>multimodal</b> sequences. (d) Fully open-sourced a family of 7B parameter models capable of processing long text documents (LWM-Text, LWM-Text-Chat) and videos (LWM, LWM-Chat) of over 1M tokens. This work paves the way for training on massive datasets of long video and language to develop understanding of both human knowledge and the <b>multimodal</b> world, and broader capabilities.</p></p class="citation"></blockquote><h3 id=1352--47247-sagman-stability-analysis-of-graph-neural-networks-on-the-manifolds-wuxinlin-cheng-et-al-2024>(13/52 | 47/247) SAGMAN: Stability Analysis of Graph Neural Networks on the Manifolds (Wuxinlin Cheng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wuxinlin Cheng, Chenhui Deng, Ali Aghdaei, Zhiru Zhang, Zhuo Feng. (2024)<br><strong>SAGMAN: Stability Analysis of Graph Neural Networks on the Manifolds</strong><br><button class=copy-to-clipboard title="SAGMAN: Stability Analysis of Graph Neural Networks on the Manifolds" index=47>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-47 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 43<br>Keywords: Graph, Graph Embedding, Graph Neural Network, Graph Neural Network, Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08653v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08653v2.pdf filename=2402.08653v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Modern <b>graph</b> <b>neural</b> <b>networks</b> <b>(GNNs)</b> can be sensitive to changes in the input <b>graph</b> <b>structure</b> <b>and</b> node features, potentially resulting in unpredictable behavior and degraded performance. In this work, we introduce a spectral framework known as SAGMAN for examining the stability of <b>GNNs.</b> This framework assesses the distance distortions that arise from the nonlinear mappings of <b>GNNs</b> between the input and output manifolds: when two nearby nodes on the input manifold are mapped (through a <b>GNN</b> model) to two distant ones on the output manifold, it implies a large distance distortion and thus a poor <b>GNN</b> stability. We propose a distance-preserving <b>graph</b> <b>dimension</b> <b>reduction</b> (GDR) approach that utilizes spectral <b>graph</b> <b>embedding</b> <b>and</b> probabilistic graphical models (PGMs) to create low-dimensional input/output <b>graph-based</b> <b>manifolds</b> <b>for</b> meaningful stability analysis. Our empirical evaluations show that SAGMAN effectively assesses the stability of each node when subjected to various edge or feature perturbations, offering a scalable approach for evaluating the stability of <b>GNNs,</b> extending to applications within <b>recommendation</b> systems. Furthermore, we illustrate its utility in downstream tasks, notably in enhancing <b>GNN</b> stability and facilitating adversarial targeted attacks.</p></p class="citation"></blockquote><h3 id=1452--48247-subgraphormer-unifying-subgraph-gnns-and-graph-transformers-via-graph-products-guy-bar-shalom-et-al-2024>(14/52 | 48/247) Subgraphormer: Unifying Subgraph GNNs and Graph Transformers via Graph Products (Guy Bar-Shalom et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Guy Bar-Shalom, Beatrice Bevilacqua, Haggai Maron. (2024)<br><strong>Subgraphormer: Unifying Subgraph GNNs and Graph Transformers via Graph Products</strong><br><button class=copy-to-clipboard title="Subgraphormer: Unifying Subgraph GNNs and Graph Transformers via Graph Products" index=48>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-48 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 43<br>Keywords: Message-Passing, Graph, Graph Neural Network, Graph Neural Network, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08450v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08450v1.pdf filename=2402.08450v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the realm of <b>Graph</b> <b>Neural</b> <b>Networks</b> <b>(GNNs),</b> two exciting research directions have recently emerged: Subgraph <b>GNNs</b> and <b>Graph</b> <b>Transformers.</b> <b>In</b> this paper, we propose an architecture that integrates both approaches, dubbed Subgraphormer, which combines the enhanced expressive power, <b>message-passing</b> mechanisms, and aggregation schemes from Subgraph <b>GNNs</b> with attention and positional encodings, arguably the most important components in <b>Graph</b> <b>Transformers.</b> <b>Our</b> method is based on an intriguing new connection we reveal between Subgraph <b>GNNs</b> and product <b>graphs,</b> <b>suggesting</b> <b>that</b> Subgraph <b>GNNs</b> can be formulated as Message Passing Neural Networks (MPNNs) operating on a product of the <b>graph</b> <b>with</b> <b>itself.</b> We use this formulation to design our architecture: first, we devise an attention mechanism based on the connectivity of the product <b>graph.</b> <b>Following</b> <b>this,</b> we propose a novel and efficient positional encoding scheme for Subgraph <b>GNNs,</b> which we derive as a positional encoding for the product <b>graph.</b> <b>Our</b> <b>experimental</b> results demonstrate significant performance improvements over both Subgraph <b>GNNs</b> and <b>Graph</b> <b>Transformers</b> <b>on</b> a wide range of datasets.</p></p class="citation"></blockquote><h3 id=1552--49247-loss-gat-label-propagation-and-one-class-semi-supervised-graph-attention-network-for-fake-news-detection-batool-lakzaei-et-al-2024>(15/52 | 49/247) LOSS-GAT: Label Propagation and One-Class Semi-Supervised Graph Attention Network for Fake News Detection (Batool Lakzaei et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Batool Lakzaei, Mostafa Haghir Chehreghani, Alireza Bagheri. (2024)<br><strong>LOSS-GAT: Label Propagation and One-Class Semi-Supervised Graph Attention Network for Fake News Detection</strong><br><button class=copy-to-clipboard title="LOSS-GAT: Label Propagation and One-Class Semi-Supervised Graph Attention Network for Fake News Detection" index=49>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-49 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs-SI, cs.LG<br>Keyword Score: 43<br>Keywords: Graph, Graph Neural Network, Graph Neural Network, Fake News Detection, Fake News Detection<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08401v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08401v1.pdf filename=2402.08401v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the era of widespread social networks, the rapid dissemination of <b>fake</b> <b>news</b> <b>has</b> emerged as a significant threat, inflicting detrimental consequences across various dimensions of people&rsquo;s lives. Machine learning and deep learning approaches have been extensively employed for identifying <b>fake</b> <b>news.</b> <b>However,</b> a significant challenge in identifying <b>fake</b> <b>news</b> <b>is</b> the limited availability of labeled news datasets. Therefore, the One-Class Learning (OCL) approach, utilizing only a small set of labeled data from the interest class, can be a suitable approach to address this challenge. On the other hand, representing data as a <b>graph</b> <b>enables</b> <b>access</b> to diverse content and structural information, and label propagation methods on <b>graphs</b> <b>can</b> <b>be</b> effective in predicting node labels. In this paper, we adopt a <b>graph-based</b> <b>model</b> <b>for</b> data representation and introduce a semi-supervised and one-class approach for <b>fake</b> <b>news</b> <b>detection,</b> called LOSS-GAT. Initially, we employ a two-step label propagation algorithm, utilizing <b>Graph</b> <b>Neural</b> <b>Networks</b> <b>(GNNs)</b> as an initial classifier to categorize news into two groups: interest <b>(fake)</b> <b>and</b> <b>non-interest</b> (real). Subsequently, we enhance the <b>graph</b> <b>structure</b> <b>using</b> structural augmentation techniques. Ultimately, we predict the final labels for all unlabeled data using a <b>GNN</b> that induces randomness within the local neighborhood of nodes through the aggregation function. We evaluate our proposed method on five common datasets and compare the results against a set of baseline models, including both OCL and binary labeled models. The results demonstrate that LOSS-GAT achieves a notable improvement, surpassing 10%, with the advantage of utilizing only a limited set of labeled <b>fake</b> <b>news.</b> <b>Noteworthy,</b> LOSS-GAT even outperforms binary labeled models.</p></p class="citation"></blockquote><h3 id=1652--50247-multi-level-gnn-preconditioner-for-solving-large-scale-problems-matthieu-nastorg-et-al-2024>(16/52 | 50/247) Multi-Level GNN Preconditioner for Solving Large Scale Problems (Matthieu Nastorg et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Matthieu Nastorg, Jean-Marc Gratien, Thibault Faney, Michele Alessandro Bucci, Guillaume Charpiat, Marc Schoenauer. (2024)<br><strong>Multi-Level GNN Preconditioner for Solving Large Scale Problems</strong><br><button class=copy-to-clipboard title="Multi-Level GNN Preconditioner for Solving Large Scale Problems" index=50>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-50 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs-NA, cs.LG, math-NA<br>Keyword Score: 43<br>Keywords: Graph, Graph Neural Network, Graph Neural Network, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08296v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08296v1.pdf filename=2402.08296v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Large-scale numerical <b>simulations</b> often come at the expense of daunting computations. High-Performance Computing has enhanced the process, but adapting legacy codes to leverage parallel GPU computations remains challenging. Meanwhile, Machine Learning models can harness GPU computations effectively but often struggle with generalization and accuracy. <b>Graph</b> <b>Neural</b> <b>Networks</b> <b>(GNNs),</b> in particular, are great for learning from unstructured data like meshes but are often limited to small-scale problems. Moreover, the capabilities of the trained model usually restrict the accuracy of the data-driven solution. To benefit from both worlds, this paper introduces a novel preconditioner integrating a <b>GNN</b> model within a multi-level Domain Decomposition framework. The proposed <b>GNN-based</b> preconditioner is used to enhance the efficiency of a Krylov method, resulting in a hybrid solver that can converge with any desired level of accuracy. The efficiency of the Krylov method greatly benefits from the <b>GNN</b> preconditioner, which is adaptable to meshes of any size and shape, is executed on GPUs, and features a multi-level approach to enforce the scalability of the entire process. Several experiments are conducted to validate the numerical behavior of the hybrid solver, and an in-depth analysis of its performance is proposed to assess its competitiveness against a C++ legacy solver.</p></p class="citation"></blockquote><h3 id=1752--51247-investigating-out-of-distribution-generalization-of-gnns-an-architecture-perspective-kai-guo-et-al-2024>(17/52 | 51/247) Investigating Out-of-Distribution Generalization of GNNs: An Architecture Perspective (Kai Guo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kai Guo, Hongzhi Wen, Wei Jin, Yaming Guo, Jiliang Tang, Yi Chang. (2024)<br><strong>Investigating Out-of-Distribution Generalization of GNNs: An Architecture Perspective</strong><br><button class=copy-to-clipboard title="Investigating Out-of-Distribution Generalization of GNNs: An Architecture Perspective" index=51>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-51 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 43<br>Keywords: Graph, Graph Neural Network, Graph Neural Network, Out-of-distribution, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08228v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08228v2.pdf filename=2402.08228v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Graph</b> <b>neural</b> <b>networks</b> <b>(GNNs)</b> have exhibited remarkable performance under the assumption that test data comes from the same distribution of training data. However, in real-world scenarios, this assumption may not always be valid. Consequently, there is a growing focus on exploring the <b>Out-of-Distribution</b> (OOD) problem in the context of <b>graphs.</b> <b>Most</b> <b>existing</b> efforts have primarily concentrated on improving <b>graph</b> <b>OOD</b> <b>generalization</b> from two \textbf{model-agnostic} perspectives: data-driven methods and strategy-based learning. However, there has been limited attention dedicated to investigating the impact of well-known \textbf{GNN model architectures} on <b>graph</b> <b>OOD</b> <b>generalization,</b> which is orthogonal to existing research. In this work, we provide the first comprehensive investigation of OOD generalization on <b>graphs</b> <b>from</b> <b>an</b> architecture perspective, by examining the common building blocks of modern <b>GNNs.</b> Through extensive experiments, we reveal that both the <b>graph</b> <b>self-attention</b> <b>mechanism</b> and the decoupled architecture contribute positively to <b>graph</b> <b>OOD</b> <b>generalization.</b> In contrast, we observe that the linear classification layer tends to compromise <b>graph</b> <b>OOD</b> <b>generalization</b> capability. Furthermore, we provide in-depth theoretical insights and discussions to underpin these discoveries. These insights have empowered us to develop a novel <b>GNN</b> backbone model, DGAT, designed to harness the robust properties of both <b>graph</b> <b>self-attention</b> <b>mechanism</b> and the decoupled architecture. Extensive experimental results demonstrate the effectiveness of our model under <b>graph</b> <b>OOD,</b> <b>exhibiting</b> substantial and consistent enhancements across various training strategies.</p></p class="citation"></blockquote><h3 id=1852--52247-intelligent-agricultural-management-considering-n_2o-emission-and-climate-variability-with-uncertainties-zhaoan-wang-et-al-2024>(18/52 | 52/247) Intelligent Agricultural Management Considering N$_2$O Emission and Climate Variability with Uncertainties (Zhaoan Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhaoan Wang, Shaoping Xiao, Jun Wang, Ashwin Parab, Shivam Patel. (2024)<br><strong>Intelligent Agricultural Management Considering N$_2$O Emission and Climate Variability with Uncertainties</strong><br><button class=copy-to-clipboard title="Intelligent Agricultural Management Considering N$_2$O Emission and Climate Variability with Uncertainties" index=52>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-52 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CY, cs-LG, cs.LG<br>Keyword Score: 40<br>Keywords: Fine-tuning, Reinforcement Learning, Recurrent Neural Network, Recurrent Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08832v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08832v1.pdf filename=2402.08832v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This study examines how artificial intelligence (AI), especially <b>Reinforcement</b> <b>Learning</b> (RL), can be used in farming to boost crop yields, <b>fine-tune</b> nitrogen use and watering, and reduce nitrate runoff and greenhouse gases, focusing on Nitrous Oxide (N$_2$O) emissions from soil. Facing climate change and limited agricultural knowledge, we use Partially Observable Markov Decision Processes (POMDPs) with a crop simulator to model AI agents&rsquo; interactions with farming environments. We apply deep Q-learning with <b>Recurrent</b> <b>Neural</b> <b>Network</b> <b>(RNN)-based</b> Q networks for training agents on optimal actions. Also, we develop Machine Learning (ML) models to predict N$_2$O emissions, integrating these predictions into the simulator. Our research tackles uncertainties in N$_2$O emission estimates with a probabilistic ML approach and climate variability through a stochastic weather model, offering a range of emission outcomes to improve forecast reliability and decision-making. By incorporating climate change effects, we enhance agents&rsquo; climate adaptability, aiming for resilient agricultural practices. Results show these agents can align crop productivity with environmental concerns by penalizing N$_2$O emissions, adapting effectively to climate shifts like warmer temperatures and less rain. This strategy improves farm management under climate change, highlighting AI&rsquo;s role in sustainable agriculture.</p></p class="citation"></blockquote><h3 id=1952--53247-mixtures-of-experts-unlock-parameter-scaling-for-deep-rl-johan-obando-ceron-et-al-2024>(19/52 | 53/247) Mixtures of Experts Unlock Parameter Scaling for Deep RL (Johan Obando-Ceron et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Johan Obando-Ceron, Ghada Sokar, Timon Willi, Clare Lyle, Jesse Farebrother, Jakob Foerster, Gintare Karolina Dziugaite, Doina Precup, Pablo Samuel Castro. (2024)<br><strong>Mixtures of Experts Unlock Parameter Scaling for Deep RL</strong><br><button class=copy-to-clipboard title="Mixtures of Experts Unlock Parameter Scaling for Deep RL" index=53>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-53 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 40<br>Keywords: Reinforcement Learning, Supervised Learning, Supervised Learning, Scaling Law<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08609v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08609v1.pdf filename=2402.08609v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The recent rapid progress in (self) <b>supervised</b> <b>learning</b> models is in large part predicted by empirical <b>scaling</b> <b>laws:</b> a model&rsquo;s performance scales proportionally to its size. Analogous <b>scaling</b> <b>laws</b> remain elusive for <b>reinforcement</b> <b>learning</b> domains, however, where increasing the parameter count of a model often hurts its final performance. In this paper, we demonstrate that incorporating Mixture-of-Expert (MoE) modules, and in particular Soft MoEs (Puigcerver et al., 2023), into value-based networks results in more parameter-scalable models, evidenced by substantial performance increases across a variety of training regimes and model sizes. This work thus provides strong empirical evidence towards developing <b>scaling</b> <b>laws</b> for <b>reinforcement</b> <b>learning.</b></p></p class="citation"></blockquote><h3 id=2052--54247-a-distributional-analogue-to-the-successor-representation-harley-wiltzer-et-al-2024>(20/52 | 54/247) A Distributional Analogue to the Successor Representation (Harley Wiltzer et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Harley Wiltzer, Jesse Farebrother, Arthur Gretton, Yunhao Tang, André Barreto, Will Dabney, Marc G. Bellemare, Mark Rowland. (2024)<br><strong>A Distributional Analogue to the Successor Representation</strong><br><button class=copy-to-clipboard title="A Distributional Analogue to the Successor Representation" index=54>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-54 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG, stat-ML<br>Keyword Score: 30<br>Keywords: Distributional Reinforcement Learning, Reinforcement Learning, Zero-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08530v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08530v1.pdf filename=2402.08530v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper contributes a new approach for <b>distributional</b> <b>reinforcement</b> <b>learning</b> which elucidates a clean separation of transition structure and reward in the learning process. Analogous to how the successor representation (SR) describes the expected consequences of behaving according to a given policy, our <b>distributional</b> <b>successor</b> <b>measure</b> (SM) describes the <b>distributional</b> <b>consequences</b> <b>of</b> this behaviour. We formulate the <b>distributional</b> <b>SM</b> <b>as</b> a distribution over distributions and provide theory connecting it with <b>distributional</b> <b>and</b> <b>model-based</b> <b>reinforcement</b> <b>learning.</b> Moreover, we propose an algorithm that learns the <b>distributional</b> <b>SM</b> <b>from</b> data by minimizing a two-level maximum mean discrepancy. Key to our method are a number of algorithmic techniques that are independently valuable for learning generative models of state. As an illustration of the usefulness of the <b>distributional</b> <b>SM,</b> <b>we</b> show that it enables <b>zero-shot</b> risk-sensitive policy evaluation in a way that was not previously possible.</p></p class="citation"></blockquote><h3 id=2152--55247-mixture-of-link-predictors-li-ma-et-al-2024>(21/52 | 55/247) Mixture of Link Predictors (Li Ma et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Li Ma, Haoyu Han, Juanhui Li, Harry Shomer, Hui Liu, Xiaofeng Gao, Jiliang Tang. (2024)<br><strong>Mixture of Link Predictors</strong><br><button class=copy-to-clipboard title="Mixture of Link Predictors" index=55>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-55 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 23<br>Keywords: Graph, Graph Neural Network, Graph Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08583v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08583v1.pdf filename=2402.08583v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Link prediction, which aims to forecast unseen connections in <b>graphs,</b> <b>is</b> <b>a</b> fundamental task in <b>graph</b> <b>machine</b> <b>learning.</b> Heuristic methods, leveraging a range of different pairwise measures such as common neighbors and shortest paths, often rival the performance of vanilla <b>Graph</b> <b>Neural</b> <b>Networks</b> <b>(GNNs).</b> Therefore, recent advancements in <b>GNNs</b> for link prediction (GNN4LP) have primarily focused on integrating one or a few types of pairwise information. In this work, we reveal that different node pairs within the same dataset necessitate varied pairwise information for accurate prediction and models that only apply the same pairwise information uniformly could achieve suboptimal performance. As a result, we propose a simple mixture of experts model Link-MoE for link prediction. Link-MoE utilizes various <b>GNNs</b> as experts and strategically selects the appropriate expert for each node pair based on various types of pairwise information. Experimental results across diverse real-world datasets demonstrate substantial performance improvement from Link-MoE. Notably, Link-MoE achieves a relative improvement of 18.82% on the MRR metric for the Pubmed dataset and 10.8% on the Hits@100 metric for the ogbl-ppa dataset, compared to the best baselines.</p></p class="citation"></blockquote><h3 id=2252--56247-concept-1k-a-novel-benchmark-for-instance-incremental-learning-junhao-zheng-et-al-2024>(22/52 | 56/247) Concept-1K: A Novel Benchmark for Instance Incremental Learning (Junhao Zheng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Junhao Zheng, Shengjie Qiu, Qianli Ma. (2024)<br><strong>Concept-1K: A Novel Benchmark for Instance Incremental Learning</strong><br><button class=copy-to-clipboard title="Concept-1K: A Novel Benchmark for Instance Incremental Learning" index=56>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-56 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CL, cs-LG, cs.LG<br>Keyword Score: 23<br>Keywords: Benchmarking, Fine-tuning, Pre-trained Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08526v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08526v1.pdf filename=2402.08526v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Incremental learning (IL) is essential to realize the human-level intelligence in the neural network. However, existing IL scenarios and datasets are unqualified for assessing forgetting in <b>PLMs,</b> giving an illusion that <b>PLMs</b> do not suffer from catastrophic forgetting. To this end, we propose a challenging IL scenario called instance-incremental learning (IIL) and a novel dataset called Concept-1K, which supports an order of magnitude larger IL steps. Based on the experiments on Concept-1K, we reveal that billion-parameter <b>PLMs</b> still suffer from catastrophic forgetting, and the forgetting is affected by both model scale, pretraining, and buffer size. Furthermore, existing IL methods and a popular <b>finetuning</b> technique, LoRA, fail to achieve satisfactory performance. Our study provides a novel scenario for future studies to explore the catastrophic forgetting of <b>PLMs</b> and encourage more powerful techniques to be designed for alleviating the forgetting in <b>PLMs.</b> The data, code and scripts are publicly available at <a href=https://github.com/zzz47zzz/pretrained-lm-for-incremental-learning>https://github.com/zzz47zzz/pretrained-lm-for-incremental-learning</a>.</p></p class="citation"></blockquote><h3 id=2352--57247-revealing-decurve-flows-for-generalized-graph-propagation-chen-lin-et-al-2024>(23/52 | 57/247) Revealing Decurve Flows for Generalized Graph Propagation (Chen Lin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chen Lin, Liheng Ma, Yiyang Chen, Wanli Ouyang, Michael M. Bronstein, Philip H. S. Torr. (2024)<br><strong>Revealing Decurve Flows for Generalized Graph Propagation</strong><br><button class=copy-to-clipboard title="Revealing Decurve Flows for Generalized Graph Propagation" index=57>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-57 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, math-DG<br>Keyword Score: 23<br>Keywords: Message-Passing, Graph, Graph Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08480v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08480v1.pdf filename=2402.08480v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This study addresses the limitations of the traditional analysis of <b>message-passing,</b> central to <b>graph</b> <b>learning,</b> <b>by</b> defining {\em \textbf{generalized propagation}} with directed and weighted <b>graphs.</b> <b>The</b> <b>significance</b> manifest in two ways. \textbf{Firstly}, we propose {\em Generalized Propagation Neural Networks} (\textbf{GPNNs}), a framework that unifies most propagation-based <b>graph</b> <b>neural</b> <b>networks.</b> By generating directed-weighted propagation <b>graphs</b> <b>with</b> <b>adjacency</b> function and connectivity function, GPNNs offer enhanced insights into attention mechanisms across various <b>graph</b> <b>models.</b> <b>We</b> delve into the trade-offs within the design space with empirical experiments and emphasize the crucial role of the adjacency function for model expressivity via theoretical analysis. \textbf{Secondly}, we propose the {\em Continuous Unified Ricci Curvature} (\textbf{CURC}), an extension of celebrated {\em Ollivier-Ricci Curvature} for directed and weighted <b>graphs.</b> <b>Theoretically,</b> <b>we</b> demonstrate that CURC possesses continuity, scale invariance, and a lower bound connection with the Dirichlet isoperimetric constant validating bottleneck analysis for GPNNs. We include a preliminary exploration of learned propagation patterns in datasets, a first in the field. We observe an intriguing ``{\em \textbf{decurve flow}}&rsquo;&rsquo; - a curvature reduction during training for models with learnable propagation, revealing the evolution of propagation over time and a deeper connection to over-smoothing and bottleneck trade-off.</p></p class="citation"></blockquote><h3 id=2452--58247-parallel-friendly-spatio-temporal-graph-learning-for-photovoltaic-degradation-analysis-at-scale-yangxin-fan-et-al-2024>(24/52 | 58/247) Parallel-friendly Spatio-Temporal Graph Learning for Photovoltaic Degradation Analysis at Scale (Yangxin Fan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yangxin Fan, Raymond Wieser, Laura Bruckman, Roger French, Yinghui Wu. (2024)<br><strong>Parallel-friendly Spatio-Temporal Graph Learning for Photovoltaic Degradation Analysis at Scale</strong><br><button class=copy-to-clipboard title="Parallel-friendly Spatio-Temporal Graph Learning for Photovoltaic Degradation Analysis at Scale" index=58>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-58 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-DC, cs-LG, cs.LG<br>Keyword Score: 23<br>Keywords: Graph, Graph Neural Network, Autoencoder<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08470v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08470v1.pdf filename=2402.08470v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We propose a novel Spatio-Temporal <b>Graph</b> <b>Neural</b> <b>Network</b> empowered trend analysis approach (ST-GTrend) to perform fleet-level performance degradation analysis for Photovoltaic (PV) power networks. PV power stations have become an integral component to the global sustainable energy production landscape. Accurately estimating the performance of PV systems is critical to their feasibility as a power generation technology and as a financial asset. One of the most challenging problems in assessing the Levelized Cost of Energy (LCOE) of a PV system is to understand and estimate the long-term Performance Loss Rate (PLR) for large fleets of PV inverters. ST-GTrend integrates spatio-temporal coherence and <b>graph</b> <b>attention</b> <b>to</b> separate PLR as a long-term &ldquo;aging&rdquo; trend from multiple fluctuation terms in the PV input data. To cope with diverse degradation patterns in timeseries, ST-GTrend adopts a paralleled <b>graph</b> <b>autoencoder</b> <b>array</b> to extract aging and fluctuation terms simultaneously. ST-GTrend imposes flatness and smoothness regularization to ensure the disentanglement between aging and fluctuation. To scale the analysis to large PV systems, we also introduce Para-GTrend, a parallel algorithm to accelerate the training and inference of ST-GTrend. We have evaluated ST-GTrend on three large-scale PV datasets, spanning a time period of 10 years. Our results show that ST-GTrend reduces Mean Absolute Percent Error (MAPE) and Euclidean Distances by 34.74% and 33.66% compared to the SOTA methods. Our results demonstrate that Para-GTrend can speed up ST-GTrend by up to 7.92 times. We further verify the generality and effectiveness of ST-GTrend for trend analysis using financial and economic datasets.</p></p class="citation"></blockquote><h3 id=2552--59247-learning-time-dependent-pde-via-graph-neural-networks-and-deep-operator-network-for-robust-accuracy-on-irregular-grids-sung-woong-cho-et-al-2024>(25/52 | 59/247) Learning time-dependent PDE via graph neural networks and deep operator network for robust accuracy on irregular grids (Sung Woong Cho et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sung Woong Cho, Jae Yong Lee, Hyung Ju Hwang. (2024)<br><strong>Learning time-dependent PDE via graph neural networks and deep operator network for robust accuracy on irregular grids</strong><br><button class=copy-to-clipboard title="Learning time-dependent PDE via graph neural networks and deep operator network for robust accuracy on irregular grids" index=59>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-59 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: 65D17, 68U07, cs-LG, cs-NA, cs.LG, math-NA<br>Keyword Score: 23<br>Keywords: Graph, Graph Neural Network, Graph Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08187v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08187v1.pdf filename=2402.08187v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Scientific computing using deep learning has seen significant advancements in recent years. There has been growing interest in models that learn the operator from the parameters of a partial differential equation (PDE) to the corresponding solutions. Deep Operator Network (DeepONet) and Fourier Neural operator, among other models, have been designed with structures suitable for handling functions as inputs and outputs, enabling real-time predictions as surrogate models for solution operators. There has also been significant progress in the research on surrogate models based on <b>graph</b> <b>neural</b> <b>networks</b> <b>(GNNs),</b> specifically targeting the dynamics in time-dependent PDEs. In this paper, we propose GraphDeepONet, an autoregressive model based on <b>GNNs,</b> to effectively adapt DeepONet, which is well-known for successful operator learning. GraphDeepONet exhibits robust accuracy in predicting solutions compared to existing <b>GNN-based</b> PDE solver models. It maintains consistent performance even on irregular grids, leveraging the advantages inherited from DeepONet and enabling predictions on arbitrary grids. Additionally, unlike traditional DeepONet and its variants, GraphDeepONet enables time extrapolation for time-dependent PDE solutions. We also provide theoretical analysis of the universal approximation capability of GraphDeepONet in approximating continuous operators across arbitrary time intervals.</p></p class="citation"></blockquote><h3 id=2652--60247-improving-molecule-generation-and-drug-discovery-with-a-knowledge-enhanced-generative-model-aditya-malusare-et-al-2024>(26/52 | 60/247) Improving Molecule Generation and Drug Discovery with a Knowledge-enhanced Generative Model (Aditya Malusare et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Aditya Malusare, Vaneet Aggarwal. (2024)<br><strong>Improving Molecule Generation and Drug Discovery with a Knowledge-enhanced Generative Model</strong><br><button class=copy-to-clipboard title="Improving Molecule Generation and Drug Discovery with a Knowledge-enhanced Generative Model" index=60>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-60 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, q-bio-QM<br>Keyword Score: 21<br>Keywords: Graph, Graph Embedding, Benchmarking, Knowledge Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08790v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08790v1.pdf filename=2402.08790v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advancements in generative models have established state-of-the-art <b>benchmarks</b> in generating molecules and novel drug candidates. Despite these successes, a significant gap persists between generative models and the utilization of extensive biomedical <b>knowledge,</b> <b>often</b> systematized within <b>knowledge</b> <b>graphs,</b> <b>whose</b> potential to inform and enhance generative processes has not been realized. In this paper, we present a novel approach that bridges this divide by developing a framework for <b>knowledge-enhanced</b> <b>generative</b> models called K-DReAM. We develop a scalable methodology to extend the functionality of <b>knowledge</b> <b>graphs</b> <b>while</b> preserving semantic integrity and incorporate this contextual information into a generative framework to guide a diffusion-based model. The integration of <b>knowledge</b> <b>graph</b> <b>embeddings</b> with our generative model furnishes a robust mechanism for producing novel drug candidates possessing specific characteristics while ensuring validity and synthesizability. K-DReAM outperforms state-of-the-art generative models on both unconditional and targeted generation tasks.</p></p class="citation"></blockquote><h3 id=2752--61247-flash-federated-learning-across-simultaneous-heterogeneities-xiangyu-chang-et-al-2024>(27/52 | 61/247) FLASH: Federated Learning Across Simultaneous Heterogeneities (Xiangyu Chang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiangyu Chang, Sk Miraj Ahmed, Srikanth V. Krishnamurthy, Basak Guler, Ananthram Swami, Samet Oymak, Amit K. Roy-Chowdhury. (2024)<br><strong>FLASH: Federated Learning Across Simultaneous Heterogeneities</strong><br><button class=copy-to-clipboard title="FLASH: Federated Learning Across Simultaneous Heterogeneities" index=61>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-61 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-DC, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Bandit Algorithm, Federated Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08769v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08769v1.pdf filename=2402.08769v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The key premise of <b>federated</b> <b>learning</b> (FL) is to train ML models across a diverse set of data-owners (clients), without exchanging local data. An overarching challenge to this date is client heterogeneity, which may arise not only from variations in data distribution, but also in data quality, as well as compute/communication latency. An integrated view of these diverse and concurrent sources of heterogeneity is critical; for instance, low-latency clients may have poor data quality, and vice versa. In this work, we propose FLASH(Federated Learning Across Simultaneous Heterogeneities), a lightweight and flexible client selection algorithm that outperforms state-of-the-art FL frameworks under extensive sources of heterogeneity, by trading-off the statistical information associated with the client&rsquo;s data quality, data distribution, and latency. FLASH is the first method, to our knowledge, for handling all these heterogeneities in a unified manner. To do so, FLASH models the learning dynamics through contextual multi-armed <b>bandits</b> (CMAB) and dynamically selects the most promising clients. Through extensive experiments, we demonstrate that FLASH achieves substantial and consistent improvements over state-of-the-art baselines &ndash; as much as 10% in absolute accuracy &ndash; thanks to its unified approach. Importantly, FLASH also outperforms <b>federated</b> <b>aggregation</b> methods that are designed to handle highly heterogeneous settings and even enjoys a performance boost when integrated with them.</p></p class="citation"></blockquote><h3 id=2852--62247-generative-vs-non-generative-models-in-engineering-shape-optimization-muhammad-usama-et-al-2024>(28/52 | 62/247) Generative VS non-Generative Models in Engineering Shape Optimization (Muhammad Usama et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Muhammad Usama, Zahid Masood, Shahroz Khan, Konstantinos Kostas, Panagiotis Kaklis. (2024)<br><strong>Generative VS non-Generative Models in Engineering Shape Optimization</strong><br><button class=copy-to-clipboard title="Generative VS non-Generative Models in Engineering Shape Optimization" index=62>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-62 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Generative Adversarial Network, Generative Adversarial Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08540v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08540v1.pdf filename=2402.08540v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this work, we perform a systematic comparison of the effectiveness and efficiency of <b>generative</b> <b>and</b> <b>non-generative</b> models in constructing design spaces for novel and efficient design exploration and shape optimization. We apply these models in the case of airfoil/hydrofoil design and conduct the comparison on the resulting design spaces. A conventional <b>Generative</b> <b>Adversarial</b> <b>Network</b> <b>(GAN)</b> and a state-of-the-art <b>generative</b> <b>model,</b> <b>the</b> Performance-Augmented Diverse <b>Generative</b> <b>Adversarial</b> <b>Network</b> (PaDGAN), are juxtaposed with a linear non-generative model based on the coupling of the Karhunen-Lo`eve Expansion and a physics-informed Shape Signature Vector (SSV-KLE). The comparison demonstrates that, with an appropriate shape encoding and a physics-augmented design space, non-generative models have the potential to cost-effectively generate high-performing valid designs with enhanced coverage of the design space. In this work, both approaches are applied to two large foil profile datasets comprising real-world and artificial designs generated through either a profile-generating parametric model or deep-learning approach. These datasets are further enriched with integral properties of their members&rsquo; shapes as well as physics-informed parameters. Our results illustrate that the design spaces constructed by the non-generative model outperform the <b>generative</b> <b>model</b> <b>in</b> terms of design validity, generating robust latent spaces with none or significantly fewer invalid designs when compared to <b>generative</b> <b>models.</b> <b>We</b> aspire that these findings will aid the engineering design community in making informed decisions when constructing designs spaces for shape optimization, as we have show that under certain conditions computationally inexpensive approaches can closely match or even outperform state-of-the art generative models.</p></p class="citation"></blockquote><h3 id=2952--63247-uncertainty-quantification-via-stable-distribution-propagation-felix-petersen-et-al-2024>(29/52 | 63/247) Uncertainty Quantification via Stable Distribution Propagation (Felix Petersen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Felix Petersen, Aashwin Mishra, Hilde Kuehne, Christian Borgelt, Oliver Deussen, Mikhail Yurochkin. (2024)<br><strong>Uncertainty Quantification via Stable Distribution Propagation</strong><br><button class=copy-to-clipboard title="Uncertainty Quantification via Stable Distribution Propagation" index=63>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-63 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Out-of-distribution, Selective Prediction<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08324v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08324v1.pdf filename=2402.08324v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We propose a new approach for propagating stable probability distributions through neural networks. Our method is based on local linearization, which we show to be an optimal approximation in terms of total variation distance for the ReLU non-linearity. This allows propagating Gaussian and Cauchy input uncertainties through neural networks to quantify their output uncertainties. To demonstrate the utility of propagating distributions, we apply the proposed method to predicting calibrated confidence intervals and <b>selective</b> <b>prediction</b> on <b>out-of-distribution</b> data. The results demonstrate a broad applicability of propagating distributions and show the advantages of our method over other approaches such as moment matching.</p></p class="citation"></blockquote><h3 id=3052--64247-thresholding-data-shapley-for-data-cleansing-using-multi-armed-bandits-hiroyuki-namba-et-al-2024>(30/52 | 64/247) Thresholding Data Shapley for Data Cleansing Using Multi-Armed Bandits (Hiroyuki Namba et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hiroyuki Namba, Shota Horiguchi, Masaki Hamamoto, Masashi Egi. (2024)<br><strong>Thresholding Data Shapley for Data Cleansing Using Multi-Armed Bandits</strong><br><button class=copy-to-clipboard title="Thresholding Data Shapley for Data Cleansing Using Multi-Armed Bandits" index=64>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-64 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Bandit Algorithm, Bandit Algorithm<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08209v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08209v1.pdf filename=2402.08209v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Data cleansing aims to improve model performance by removing a set of harmful instances from the training dataset. Data Shapley is a common theoretically guaranteed method to evaluate the contribution of each instance to model performance; however, it requires training on all subsets of the training data, which is computationally expensive. In this paper, we propose an iterativemethod to fast identify a subset of instances with low data Shapley values by using the thresholding <b>bandit</b> <b>algorithm.</b> We provide a theoretical guarantee that the proposed method can accurately select harmful instances if a sufficiently large number of iterations is conducted. Empirical evaluation using various models and datasets demonstrated that the proposed method efficiently improved the computational speed while maintaining the model performance.</p></p class="citation"></blockquote><h3 id=3152--65247-homomorphism-counts-for-graph-neural-networks-all-about-that-basis-emily-jin-et-al-2024>(31/52 | 65/247) Homomorphism Counts for Graph Neural Networks: All About That Basis (Emily Jin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Emily Jin, Michael Bronstein, Ismail Ilkan Ceylan, Matthias Lanzinger. (2024)<br><strong>Homomorphism Counts for Graph Neural Networks: All About That Basis</strong><br><button class=copy-to-clipboard title="Homomorphism Counts for Graph Neural Networks: All About That Basis" index=65>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-65 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 16<br>Keywords: Graph, Graph Neural Network, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08595v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08595v2.pdf filename=2402.08595v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Graph</b> <b>neural</b> <b>networks</b> are architectures for learning invariant functions over <b>graphs.</b> <b>A</b> <b>large</b> body of work has investigated the properties of <b>graph</b> <b>neural</b> <b>networks</b> and identified several limitations, particularly pertaining to their expressive power. Their inability to count certain patterns (e.g., cycles) in a <b>graph</b> <b>lies</b> <b>at</b> the heart of such limitations, since many functions to be learned rely on the ability of counting such patterns. Two prominent paradigms aim to address this limitation by enriching the <b>graph</b> <b>features</b> <b>with</b> subgraph or homomorphism pattern counts. In this work, we show that both of these approaches are sub-optimal in a certain sense and argue for a more fine-grained approach, which incorporates the homomorphism counts of all structures in the &ldquo;basis&rdquo; of the target pattern. This yields strictly more expressive architectures without incurring any additional overhead in terms of computational complexity compared to existing approaches. We prove a series of theoretical results on node-level and <b>graph-level</b> <b>motif</b> <b>parameters</b> and empirically validate them on standard <b>benchmark</b> datasets.</p></p class="citation"></blockquote><h3 id=3252--66247-transition-constrained-bayesian-optimization-via-markov-decision-processes-jose-pablo-folch-et-al-2024>(32/52 | 66/247) Transition Constrained Bayesian Optimization via Markov Decision Processes (Jose Pablo Folch et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jose Pablo Folch, Calvin Tsay, Robert M Lee, Behrang Shafei, Weronika Ormaniec, Andreas Krause, Mark van der Wilk, Ruth Misener, Mojmír Mutný. (2024)<br><strong>Transition Constrained Bayesian Optimization via Markov Decision Processes</strong><br><button class=copy-to-clipboard title="Transition Constrained Bayesian Optimization via Markov Decision Processes" index=66>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-66 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 15<br>Keywords: Black Box, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08406v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08406v1.pdf filename=2402.08406v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Bayesian optimization is a methodology to optimize <b>black-box</b> <b>functions.</b> Traditionally, it focuses on the setting where you can arbitrarily query the search space. However, many real-life problems do not offer this flexibility; in particular, the search space of the next query may depend on previous ones. Example challenges arise in the physical sciences in the form of local movement constraints, required monotonicity in certain variables, and transitions influencing the accuracy of measurements. Altogether, such transition constraints necessitate a form of planning. This work extends Bayesian optimization via the framework of Markov Decision Processes, iteratively solving a tractable linearization of our objective using <b>reinforcement</b> <b>learning</b> to obtain a policy that plans ahead over long horizons. The resulting policy is potentially history-dependent and non-Markovian. We showcase applications in chemical reactor optimization, informative path planning, machine calibration, and other synthetic examples.</p></p class="citation"></blockquote><h3 id=3352--67247-the-effect-of-data-poisoning-on-counterfactual-explanations-andré-artelt-et-al-2024>(33/52 | 67/247) The Effect of Data Poisoning on Counterfactual Explanations (André Artelt et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>André Artelt, Shubham Sharma, Freddy Lecué, Barbara Hammer. (2024)<br><strong>The Effect of Data Poisoning on Counterfactual Explanations</strong><br><button class=copy-to-clipboard title="The Effect of Data Poisoning on Counterfactual Explanations" index=67>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-67 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 15<br>Keywords: Black Box, Counter-factual<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08290v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08290v1.pdf filename=2402.08290v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Counterfactual</b> explanations provide a popular method for analyzing the predictions of <b>black-box</b> <b>systems,</b> and they can offer the opportunity for computational recourse by suggesting actionable changes on how to change the input to obtain a different (i.e. more favorable) system output. However, recent work highlighted their vulnerability to different types of manipulations. This work studies the vulnerability of <b>counterfactual</b> explanations to data poisoning. We formalize data poisoning in the context of <b>counterfactual</b> explanations for increasing the cost of recourse on three different levels: locally for a single instance, or a sub-group of instances, or globally for all instances. We demonstrate that state-of-the-art <b>counterfactual</b> generation methods & toolboxes are vulnerable to such data poisoning.</p></p class="citation"></blockquote><h3 id=3452--68247-feature-attribution-with-necessity-and-sufficiency-via-dual-stage-perturbation-test-for-causal-explanation-xuexin-chen-et-al-2024>(34/52 | 68/247) Feature Attribution with Necessity and Sufficiency via Dual-stage Perturbation Test for Causal Explanation (Xuexin Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xuexin Chen, Ruichu Cai, Zhengting Huang, Yuxuan Zhu, Julien Horwood, Zhifeng Hao, Zijian Li, Jose Miguel Hernandez-Lobato. (2024)<br><strong>Feature Attribution with Necessity and Sufficiency via Dual-stage Perturbation Test for Causal Explanation</strong><br><button class=copy-to-clipboard title="Feature Attribution with Necessity and Sufficiency via Dual-stage Perturbation Test for Causal Explanation" index=68>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-68 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-ME<br>Keyword Score: 13<br>Keywords: Benchmarking, Counter-factual<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08845v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08845v1.pdf filename=2402.08845v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We investigate the problem of explainability in machine learning.To address this problem, Feature Attribution Methods (FAMs) measure the contribution of each feature through a perturbation test, where the difference in prediction is compared under different perturbations.However, such perturbation tests may not accurately distinguish the contributions of different features, when their change in prediction is the same after perturbation.In order to enhance the ability of FAMs to distinguish different features&rsquo; contributions in this challenging setting, we propose to utilize the probability (PNS) that perturbing a feature is a necessary and sufficient cause for the prediction to change as a measure of feature importance.Our approach, Feature Attribution with Necessity and Sufficiency (FANS), computes the PNS via a perturbation test involving two stages (factual and interventional).In practice, to generate <b>counterfactual</b> samples, we use a resampling-based approach on the observed samples to approximate the required conditional distribution.Finally, we combine FANS and gradient-based optimization to extract the subset with the largest PNS.We demonstrate that FANS outperforms existing feature attribution methods on six <b>benchmarks.</b></p></p class="citation"></blockquote><h3 id=3552--69247-graph-feature-preprocessor-real-time-extraction-of-subgraph-based-features-from-transaction-graphs-jovan-blanuša-et-al-2024>(35/52 | 69/247) Graph Feature Preprocessor: Real-time Extraction of Subgraph-based Features from Transaction Graphs (Jovan Blanuša et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jovan Blanuša, Maximo Cravero Baraja, Andreea Anghel, Luc von Niederhäusern, Erik Altman, Haris Pozidis, Kubilay Atasu. (2024)<br><strong>Graph Feature Preprocessor: Real-time Extraction of Subgraph-based Features from Transaction Graphs</strong><br><button class=copy-to-clipboard title="Graph Feature Preprocessor: Real-time Extraction of Subgraph-based Features from Transaction Graphs" index=69>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-69 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 13<br>Keywords: Graph, Graph Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08593v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08593v1.pdf filename=2402.08593v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we present <b>&ldquo;Graph</b> <b>Feature</b> <b>Preprocessor&rdquo;,</b> a software library for detecting typical money laundering and fraud patterns in financial transaction <b>graphs</b> <b>in</b> <b>real</b> time. These patterns are used to produce a rich set of transaction features for downstream machine learning training and inference tasks such as money laundering detection. We show that our enriched transaction features dramatically improve the prediction accuracy of gradient-boosting-based machine learning models. Our library exploits multicore parallelism, maintains a dynamic in-memory <b>graph,</b> <b>and</b> <b>efficiently</b> mines subgraph patterns in the incoming transaction stream, which enables it to be operated in a streaming manner. We evaluate our library using highly-imbalanced synthetic anti-money laundering (AML) and real-life Ethereum phishing datasets. In these datasets, the proportion of illicit transactions is very small, which makes the learning process challenging. Our solution, which combines our <b>Graph</b> <b>Feature</b> <b>Preprocessor</b> and gradient-boosting-based machine learning models, is able to detect these illicit transactions with higher minority-class F1 scores than standard <b>graph</b> <b>neural</b> <b>networks.</b> In addition, the end-to-end throughput rate of our solution executed on a multicore CPU outperforms the <b>graph</b> <b>neural</b> <b>network</b> baselines executed on a powerful V100 GPU. Overall, the combination of high accuracy, a high throughput rate, and low latency of our solution demonstrates the practical value of our library in real-world applications. <b>Graph</b> <b>Feature</b> <b>Preprocessor</b> has been integrated into IBM mainframe software products, namely &ldquo;IBM Cloud Pak for Data on Z&rdquo; and &ldquo;AI Toolkit for IBM Z and LinuxONE&rdquo;.</p></p class="citation"></blockquote><h3 id=3652--70247-neures-learning-proofs-of-propositional-satisfiability-mohamed-ghanem-et-al-2024>(36/52 | 70/247) NeuRes: Learning Proofs of Propositional Satisfiability (Mohamed Ghanem et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mohamed Ghanem, Frederik Schmitt, Julian Siber, Bernd Finkbeiner. (2024)<br><strong>NeuRes: Learning Proofs of Propositional Satisfiability</strong><br><button class=copy-to-clipboard title="NeuRes: Learning Proofs of Propositional Satisfiability" index=70>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-70 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs-LO, cs.LG<br>Keyword Score: 13<br>Keywords: Graph, Graph Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08365v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08365v1.pdf filename=2402.08365v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce NeuRes, a neuro-symbolic proof-based SAT solver. Unlike other neural SAT solving methods, NeuRes is capable of proving unsatisfiability as opposed to merely predicting it. By design, NeuRes operates in a certificate-driven fashion by employing propositional resolution to prove unsatisfiability and to accelerate the process of finding satisfying truth assignments in case of unsat and sat formulas, respectively. To realize this, we propose a novel architecture that adapts elements from <b>Graph</b> <b>Neural</b> <b>Networks</b> and Pointer Networks to autoregressively select pairs of nodes from a dynamic <b>graph</b> <b>structure,</b> <b>which</b> is essential to the generation of resolution proofs. Our model is trained and evaluated on a dataset of teacher proofs and truth assignments that we compiled with the same random formula distribution used by NeuroSAT. In our experiments, we show that NeuRes solves more test formulas than NeuroSAT by a rather wide margin on different distributions while being much more data-efficient. Furthermore, we show that NeuRes is capable of largely shortening teacher proofs by notable proportions. We use this feature to devise a bootstrapped training procedure that manages to reduce a dataset of proofs generated by an advanced solver by ~23% after training on it with no extra guidance.</p></p class="citation"></blockquote><h3 id=3752--71247-distal-interference-exploring-the-limits-of-model-based-continual-learning-heinrich-van-deventer-et-al-2024>(37/52 | 71/247) Distal Interference: Exploring the Limits of Model-Based Continual Learning (Heinrich van Deventer et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Heinrich van Deventer, Anna Sergeevna Bosman. (2024)<br><strong>Distal Interference: Exploring the Limits of Model-Based Continual Learning</strong><br><button class=copy-to-clipboard title="Distal Interference: Exploring the Limits of Model-Based Continual Learning" index=71>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-71 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: 68T07, I-5-1, cs-AI, cs-LG, cs-NE, cs.LG<br>Keyword Score: 13<br>Keywords: Benchmarking, Continual Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08255v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08255v1.pdf filename=2402.08255v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Continual</b> <b>learning</b> is the sequential learning of different tasks by a machine learning model. <b>Continual</b> <b>learning</b> is known to be hindered by catastrophic interference or forgetting, i.e. rapid unlearning of earlier learned tasks when new tasks are learned. Despite their practical success, artificial neural networks (ANNs) are prone to catastrophic interference. This study analyses how gradient descent and overlapping representations between distant input points lead to distal interference and catastrophic interference. Distal interference refers to the phenomenon where training a model on a subset of the domain leads to non-local changes on other subsets of the domain. This study shows that uniformly trainable models without distal interference must be exponentially large. A novel antisymmetric bounded exponential layer B-spline ANN architecture named ABEL-Spline is proposed that can approximate any continuous function, is uniformly trainable, has polynomial computational complexity, and provides some guarantees for distal interference. Experiments are presented to demonstrate the theoretical properties of ABEL-Splines. ABEL-Splines are also evaluated on <b>benchmark</b> regression problems. It is concluded that the weaker distal interference guarantees in ABEL-Splines are insufficient for model-only <b>continual</b> <b>learning.</b> It is conjectured that <b>continual</b> <b>learning</b> with polynomial complexity models requires augmentation of the training data or algorithm.</p></p class="citation"></blockquote><h3 id=3852--72247-approximation-of-relation-functions-and-attention-mechanisms-awni-altabaa-et-al-2024>(38/52 | 72/247) Approximation of relation functions and attention mechanisms (Awni Altabaa et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Awni Altabaa, John Lafferty. (2024)<br><strong>Approximation of relation functions and attention mechanisms</strong><br><button class=copy-to-clipboard title="Approximation of relation functions and attention mechanisms" index=72>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-72 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-ML<br>Keyword Score: 10<br>Keywords: Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08856v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08856v1.pdf filename=2402.08856v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Inner products of neural network feature maps arises in a wide variety of machine learning frameworks as a method of modeling relations between inputs. This work studies the approximation properties of inner products of neural networks. It is shown that the inner product of a multi-layer perceptron with itself is a universal approximator for symmetric positive-definite relation functions. In the case of asymmetric relation functions, it is shown that the inner product of two different multi-layer perceptrons is a universal approximator. In both cases, a bound is obtained on the number of neurons required to achieve a given accuracy of approximation. In the symmetric case, the function class can be identified with kernels of reproducing kernel Hilbert spaces, whereas in the asymmetric case the function class can be identified with kernels of reproducing kernel Banach spaces. Finally, these approximation results are applied to analyzing the attention mechanism underlying <b>Transformers,</b> showing that any retrieval mechanism defined by an abstract preorder can be approximated by attention through its inner product relations. This result uses the Debreu representation theorem in economics to represent preference relations in terms of utility functions.</p></p class="citation"></blockquote><h3 id=3952--73247-hybrid-inverse-reinforcement-learning-juntao-ren-et-al-2024>(39/52 | 73/247) Hybrid Inverse Reinforcement Learning (Juntao Ren et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Juntao Ren, Gokul Swamy, Zhiwei Steven Wu, J. Andrew Bagnell, Sanjiban Choudhury. (2024)<br><strong>Hybrid Inverse Reinforcement Learning</strong><br><button class=copy-to-clipboard title="Hybrid Inverse Reinforcement Learning" index=73>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-73 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08848v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08848v1.pdf filename=2402.08848v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The inverse <b>reinforcement</b> <b>learning</b> approach to imitation learning is a double-edged sword. On the one hand, it can enable learning from a smaller number of expert demonstrations with more robustness to error compounding than behavioral cloning approaches. On the other hand, it requires that the learner repeatedly solve a computationally expensive <b>reinforcement</b> <b>learning</b> (RL) problem. Often, much of this computation is wasted searching over policies very dissimilar to the expert&rsquo;s. In this work, we propose using hybrid RL &ndash; training on a mixture of online and expert data &ndash; to curtail unnecessary exploration. Intuitively, the expert data focuses the learner on good states during training, which reduces the amount of exploration required to compute a strong policy. Notably, such an approach doesn&rsquo;t need the ability to reset the learner to arbitrary states in the environment, a requirement of prior work in efficient inverse RL. More formally, we derive a reduction from inverse RL to expert-competitive RL (rather than globally optimal RL) that allows us to dramatically reduce interaction during the inner policy search loop while maintaining the benefits of the IRL approach. This allows us to derive both model-free and model-based hybrid inverse RL algorithms with strong policy performance guarantees. Empirically, we find that our approaches are significantly more sample efficient than standard inverse RL and several other baselines on a suite of continuous control tasks.</p></p class="citation"></blockquote><h3 id=4052--74247-projection-free-online-convex-optimization-with-time-varying-constraints-dan-garber-et-al-2024>(40/52 | 74/247) Projection-Free Online Convex Optimization with Time-Varying Constraints (Dan Garber et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dan Garber, Ben Kretzu. (2024)<br><strong>Projection-Free Online Convex Optimization with Time-Varying Constraints</strong><br><button class=copy-to-clipboard title="Projection-Free Online Convex Optimization with Time-Varying Constraints" index=74>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-74 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, math-OC, stat-ML<br>Keyword Score: 10<br>Keywords: Bandit Algorithm<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08799v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08799v1.pdf filename=2402.08799v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We consider the setting of online convex optimization with adversarial time-varying constraints in which actions must be feasible w.r.t. a fixed constraint set, and are also required on average to approximately satisfy additional time-varying constraints. Motivated by scenarios in which the fixed feasible set (hard constraint) is difficult to project on, we consider projection-free algorithms that access this set only through a linear optimization oracle (LOO). We present an algorithm that, on a sequence of length $T$ and using overall $T$ calls to the LOO, guarantees $\tilde{O}(T^{3/4})$ regret w.r.t. the losses and $O(T^{7/8})$ constraints violation (ignoring all quantities except for $T$) . In particular, these bounds hold w.r.t. any interval of the sequence. We also present a more efficient algorithm that requires only first-order oracle access to the soft constraints and achieves similar bounds w.r.t. the entire sequence. We extend the latter to the setting of <b>bandit</b> feedback and obtain similar bounds (as a function of $T$) in expectation.</p></p class="citation"></blockquote><h3 id=4152--75247-bayesian-strategic-classification-lee-cohen-et-al-2024>(41/52 | 75/247) Bayesian Strategic Classification (Lee Cohen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lee Cohen, Saeed Sharifi-Malvajerdi, Kevin Stangl, Ali Vakilian, Juba Ziani. (2024)<br><strong>Bayesian Strategic Classification</strong><br><button class=copy-to-clipboard title="Bayesian Strategic Classification" index=75>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-75 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-GT, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08758v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08758v1.pdf filename=2402.08758v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In strategic classification, agents modify their features, at a cost, to ideally obtain a positive classification from the learner&rsquo;s classifier. The typical response of the learner is to carefully modify their classifier to be robust to such strategic behavior. When <b>reasoning</b> about agent manipulations, most papers that study strategic classification rely on the following strong assumption: agents fully know the exact parameters of the deployed classifier by the learner. This often is an unrealistic assumption when using complex or proprietary machine learning techniques in real-world prediction tasks. We initiate the study of partial information release by the learner in strategic classification. We move away from the traditional assumption that agents have full knowledge of the classifier. Instead, we consider agents that have a common distributional prior on which classifier the learner is using. The learner in our model can reveal truthful, yet not necessarily complete, information about the deployed classifier to the agents. The learner&rsquo;s goal is to release just enough information about the classifier to maximize accuracy. We show how such partial information release can, counter-intuitively, benefit the learner&rsquo;s accuracy, despite increasing agents&rsquo; abilities to manipulate. We show that while it is intractable to compute the best response of an agent in the general case, there exist oracle-efficient algorithms that can solve the best response of the agents when the learner&rsquo;s hypothesis class is the class of linear classifiers, or when the agents&rsquo; cost function satisfies a natural notion of submodularity as we define. We then turn our attention to the learner&rsquo;s optimization problem and provide both positive and negative results on the algorithmic problem of how much information the learner should release about the classifier to maximize their expected accuracy.</p></p class="citation"></blockquote><h3 id=4252--76247-a-convergence-analysis-of-approximate-message-passing-with-non-separable-functions-and-applications-to-multi-class-classification-burak-çakmak-et-al-2024>(42/52 | 76/247) A Convergence Analysis of Approximate Message Passing with Non-Separable Functions and Applications to Multi-Class Classification (Burak Çakmak et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Burak Çakmak, Yue M. Lu, Manfred Opper. (2024)<br><strong>A Convergence Analysis of Approximate Message Passing with Non-Separable Functions and Applications to Multi-Class Classification</strong><br><button class=copy-to-clipboard title="A Convergence Analysis of Approximate Message Passing with Non-Separable Functions and Applications to Multi-Class Classification" index=76>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-76 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-IT, cs-LG, cs.LG, math-IT<br>Keyword Score: 10<br>Keywords: Message-Passing<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08676v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08676v1.pdf filename=2402.08676v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Motivated by the recent application of approximate message passing (AMP) to the analysis of convex optimizations in multi-class classifications [Loureiro, et. al., 2021], we present a convergence analysis of AMP dynamics with non-separable multivariate nonlinearities. As an application, we present a complete (and independent) analysis of the motivated convex optimization problem.</p></p class="citation"></blockquote><h3 id=4352--77247-generating-universal-adversarial-perturbations-for-quantum-classifiers-gautham-anil-et-al-2024>(43/52 | 77/247) Generating Universal Adversarial Perturbations for Quantum Classifiers (Gautham Anil et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Gautham Anil, Vishnu Vinod, Apurva Narayan. (2024)<br><strong>Generating Universal Adversarial Perturbations for Quantum Classifiers</strong><br><button class=copy-to-clipboard title="Generating Universal Adversarial Perturbations for Quantum Classifiers" index=77>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-77 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08648v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08648v1.pdf filename=2402.08648v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Quantum Machine Learning (QML) has emerged as a promising field of research, aiming to leverage the capabilities of quantum computing to enhance existing machine learning methodologies. Recent studies have revealed that, like their classical counterparts, QML models based on Parametrized Quantum Circuits (PQCs) are also vulnerable to <b>adversarial</b> <b>attacks.</b> Moreover, the existence of Universal <b>Adversarial</b> <b>Perturbations</b> (UAPs) in the quantum domain has been demonstrated theoretically in the context of quantum classifiers. In this work, we introduce QuGAP: a novel framework for generating UAPs for quantum classifiers. We conceptualize the notion of additive UAPs for PQC-based classifiers and theoretically demonstrate their existence. We then utilize generative models (QuGAP-A) to craft additive UAPs and experimentally show that quantum classifiers are susceptible to such attacks. Moreover, we formulate a new method for generating unitary UAPs (QuGAP-U) using quantum generative models and a novel loss function based on fidelity constraints. We evaluate the performance of the proposed framework and show that our method achieves state-of-the-art misclassification rates, while maintaining high fidelity between legitimate and <b>adversarial</b> <b>samples.</b></p></p class="citation"></blockquote><h3 id=4452--78247-a-generalized-approach-to-online-convex-optimization-mohammad-pedramfar-et-al-2024>(44/52 | 78/247) A Generalized Approach to Online Convex Optimization (Mohammad Pedramfar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mohammad Pedramfar, Vaneet Aggarwal. (2024)<br><strong>A Generalized Approach to Online Convex Optimization</strong><br><button class=copy-to-clipboard title="A Generalized Approach to Online Convex Optimization" index=78>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-78 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, math-OC, stat-ML<br>Keyword Score: 10<br>Keywords: Bandit Algorithm<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08621v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08621v1.pdf filename=2402.08621v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we analyze the problem of online convex optimization in different settings. We show that any algorithm for online linear optimization with fully adaptive adversaries is an algorithm for online convex optimization. We also show that any such algorithm that requires full-information feedback may be transformed to an algorithm with semi-bandit feedback with comparable regret bound. We further show that algorithms that are designed for fully adaptive adversaries using deterministic semi-bandit feedback can obtain similar bounds using only stochastic semi-bandit feedback when facing oblivious adversaries. We use this to describe general meta-algorithms to convert first order algorithms to zeroth order algorithms with comparable regret bounds. Our framework allows us to analyze online optimization in various settings, such full-information feedback, <b>bandit</b> feedback, stochastic regret, adversarial regret and various forms of non-stationary regret. Using our analysis, we provide the first efficient projection-free online convex optimization algorithm using linear optimization oracles.</p></p class="citation"></blockquote><h3 id=4552--79247-fairness-auditing-with-multi-agent-collaboration-martijn-de-vos-et-al-2024>(45/52 | 79/247) Fairness Auditing with Multi-Agent Collaboration (Martijn de Vos et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Martijn de Vos, Akash Dhasade, Jade Garcia Bourrée, Anne-Marie Kermarrec, Erwan Le Merrer, Benoit Rottembourg, Gilles Tredan. (2024)<br><strong>Fairness Auditing with Multi-Agent Collaboration</strong><br><button class=copy-to-clipboard title="Fairness Auditing with Multi-Agent Collaboration" index=79>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-79 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Fairness<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08522v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08522v1.pdf filename=2402.08522v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Existing work in <b>fairness</b> audits assumes that agents operate independently. In this paper, we consider the case of multiple agents auditing the same platform for different tasks. Agents have two levers: their collaboration strategy, with or without coordination beforehand, and their sampling method. We theoretically study their interplay when agents operate independently or collaborate. We prove that, surprisingly, coordination can sometimes be detrimental to audit accuracy, whereas uncoordinated collaboration generally yields good results. Experimentation on real-world datasets confirms this observation, as the audit accuracy of uncoordinated collaboration matches that of collaborative optimal sampling.</p></p class="citation"></blockquote><h3 id=4652--80247-provable-traffic-rule-compliance-in-safe-reinforcement-learning-on-the-open-sea-hanna-krasowski-et-al-2024>(46/52 | 80/247) Provable Traffic Rule Compliance in Safe Reinforcement Learning on the Open Sea (Hanna Krasowski et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hanna Krasowski, Matthias Althoff. (2024)<br><strong>Provable Traffic Rule Compliance in Safe Reinforcement Learning on the Open Sea</strong><br><button class=copy-to-clipboard title="Provable Traffic Rule Compliance in Safe Reinforcement Learning on the Open Sea" index=80>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-80 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs-SY, cs.LG, eess-SY<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08502v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08502v1.pdf filename=2402.08502v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Autonomous vehicles have to obey traffic rules. These rules are often formalized using temporal logic, resulting in constraints that are hard to solve using optimization-based motion planners. <b>Reinforcement</b> <b>Learning</b> (RL) is a promising method to find motion plans adhering to temporal logic specifications. However, vanilla RL algorithms are based on random exploration, which is inherently unsafe. To address this issue, we propose a provably safe RL approach that always complies with traffic rules. As a specific application area, we consider vessels on the open sea, which must adhere to the Convention on the International Regulations for Preventing Collisions at Sea (COLREGS). We introduce an efficient verification approach that determines the compliance of actions with respect to the COLREGS formalized using temporal logic. Our action verification is integrated into the RL process so that the agent only selects verified actions. In contrast to agents that only integrate the traffic rule information in the reward function, our provably safe agent always complies with the formalized rules in critical maritime traffic situations and, thus, never causes a collision.</p></p class="citation"></blockquote><h3 id=4752--81247-deep-reinforcement-learning-for-controlled-traversing-of-the-attractor-landscape-of-boolean-models-in-the-context-of-cellular-reprogramming-andrzej-mizera-et-al-2024>(47/52 | 81/247) Deep Reinforcement Learning for Controlled Traversing of the Attractor Landscape of Boolean Models in the Context of Cellular Reprogramming (Andrzej Mizera et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Andrzej Mizera, Jakub Zarzycki. (2024)<br><strong>Deep Reinforcement Learning for Controlled Traversing of the Attractor Landscape of Boolean Models in the Context of Cellular Reprogramming</strong><br><button class=copy-to-clipboard title="Deep Reinforcement Learning for Controlled Traversing of the Attractor Landscape of Boolean Models in the Context of Cellular Reprogramming" index=81>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-81 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG, q-bio-MN, q-bio-QM<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08491v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08491v1.pdf filename=2402.08491v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Cellular reprogramming can be used for both the prevention and cure of different diseases. However, the efficiency of discovering reprogramming strategies with classical wet-lab experiments is hindered by lengthy time commitments and high costs. In this study, we develop a~novel computational framework based on deep <b>reinforcement</b> <b>learning</b> that facilitates the identification of reprogramming strategies. For this aim, we formulate a~control problem in the context of cellular reprogramming for the frameworks of BNs and PBNs under the asynchronous update mode. Furthermore, we introduce the notion of a~pseudo-attractor and a~procedure for identification of pseudo-attractor state during training. Finally, we devise a~computational framework for solving the control problem, which we test on a~number of different models.</p></p class="citation"></blockquote><h3 id=4852--82247-conservative-and-risk-aware-offline-multi-agent-reinforcement-learning-for-digital-twins-eslam-eldeeb-et-al-2024>(48/52 | 82/247) Conservative and Risk-Aware Offline Multi-Agent Reinforcement Learning for Digital Twins (Eslam Eldeeb et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Eslam Eldeeb, Houssem Sifaou, Osvaldo Simeone, Mohammad Shehab, Hirley Alves. (2024)<br><strong>Conservative and Risk-Aware Offline Multi-Agent Reinforcement Learning for Digital Twins</strong><br><button class=copy-to-clipboard title="Conservative and Risk-Aware Offline Multi-Agent Reinforcement Learning for Digital Twins" index=82>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-82 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs-MA, cs.LG<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08421v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08421v1.pdf filename=2402.08421v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Digital twin (DT) platforms are increasingly regarded as a promising technology for controlling, optimizing, and monitoring complex engineering systems such as next-generation wireless networks. An important challenge in adopting DT solutions is their reliance on data collected offline, lacking direct access to the physical environment. This limitation is particularly severe in multi-agent systems, for which conventional multi-agent <b>reinforcement</b> <b>(MARL)</b> requires online interactions with the environment. A direct application of online MARL schemes to an offline setting would generally fail due to the epistemic uncertainty entailed by the limited availability of data. In this work, we propose an offline MARL scheme for DT-based wireless networks that integrates distributional RL and conservative Q-learning to address the environment&rsquo;s inherent aleatoric uncertainty and the epistemic uncertainty arising from limited data. To further exploit the offline data, we adapt the proposed scheme to the centralized training decentralized execution framework, allowing joint training of the agents&rsquo; policies. The proposed MARL scheme, referred to as multi-agent conservative quantile regression (MA-CQR) addresses general risk-sensitive design criteria and is applied to the trajectory planning problem in drone networks, showcasing its advantages.</p></p class="citation"></blockquote><h3 id=4952--83247-helping-university-students-to-choose-elective-courses-by-using-a-hybrid-multi-criteria-recommendation-system-with-genetic-optimization-a-esteban-et-al-2024>(49/52 | 83/247) Helping university students to choose elective courses by using a hybrid multi-criteria recommendation system with genetic optimization (A. Esteban et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>A. Esteban, A. Zafra, C. Romero. (2024)<br><strong>Helping university students to choose elective courses by using a hybrid multi-criteria recommendation system with genetic optimization</strong><br><button class=copy-to-clipboard title="Helping university students to choose elective courses by using a hybrid multi-criteria recommendation system with genetic optimization" index=83>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-83 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08371v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08371v1.pdf filename=2402.08371v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The wide availability of specific courses together with the flexibility of academic plans in university studies reveal the importance of <b>Recommendation</b> Systems (RSs) in this area. These systems appear as tools that help students to choose courses that suit to their personal interests and their academic performance. This paper presents a hybrid RS that combines Collaborative Filtering (CF) and Content-based Filtering (CBF) using multiple criteria related both to student and course information to recommend the most suitable courses to the students. A Genetic Algorithm (GA) has been developed to automatically discover the optimal RS configuration which include both the most relevant criteria and the configuration of the rest of parameters. The experimental study has used real information of Computer Science Degree of University of Cordoba (Spain) including information gathered from students during three academic years, counting on 2500 entries of 95 students and 63 courses. Experimental results show a study of the most relevant criteria for the course <b>recommendation,</b> the importance of using a hybrid model that combines both student information and course information to increase the reliability of the <b>recommendations</b> as well as an excellent performance compared to previous models.</p></p class="citation"></blockquote><h3 id=5052--84247-randomized-algorithms-for-symmetric-nonnegative-matrix-factorization-koby-hayashi-et-al-2024>(50/52 | 84/247) Randomized Algorithms for Symmetric Nonnegative Matrix Factorization (Koby Hayashi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Koby Hayashi, Sinan G. Aksoy, Grey Ballard, Haesun Park. (2024)<br><strong>Randomized Algorithms for Symmetric Nonnegative Matrix Factorization</strong><br><button class=copy-to-clipboard title="Randomized Algorithms for Symmetric Nonnegative Matrix Factorization" index=84>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-84 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: 65F55, 65F20, cs-LG, cs-NA, cs.LG, math-NA, math-OC<br>Keyword Score: 6<br>Keywords: Graph, Clustering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08134v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08134v1.pdf filename=2402.08134v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Symmetric Nonnegative Matrix Factorization (SymNMF) is a technique in data analysis and machine learning that approximates a symmetric matrix with a product of a nonnegative, low-rank matrix and its transpose. To design faster and more scalable algorithms for SymNMF we develop two randomized algorithms for its computation. The first algorithm uses randomized matrix sketching to compute an initial low-rank input matrix and proceeds to use this input to rapidly compute a SymNMF. The second algorithm uses randomized leverage score sampling to approximately solve constrained least squares problems. Many successful methods for SymNMF rely on (approximately) solving sequences of constrained least squares problems. We prove theoretically that leverage score sampling can approximately solve nonnegative least squares problems to a chosen accuracy with high probability. Finally we demonstrate that both methods work well in practice by applying them to <b>graph</b> <b>clustering</b> tasks on large real world data sets. These experiments show that our methods approximately maintain solution quality and achieve significant speed ups for both large dense and large sparse problems.</p></p class="citation"></blockquote><h3 id=5152--85247-gaussian-ensemble-belief-propagation-for-efficient-inference-in-high-dimensional-systems-dan-mackinlay-et-al-2024>(51/52 | 85/247) Gaussian Ensemble Belief Propagation for Efficient Inference in High-Dimensional Systems (Dan MacKinlay et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dan MacKinlay, Russell Tsuchida, Dan Pagendam, Petra Kuhnert. (2024)<br><strong>Gaussian Ensemble Belief Propagation for Efficient Inference in High-Dimensional Systems</strong><br><button class=copy-to-clipboard title="Gaussian Ensemble Belief Propagation for Efficient Inference in High-Dimensional Systems" index=85>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-85 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: 62-07 (Primary) 62F15, 62M40, 68T05, 68W25, I-2-6; H-2-4; I-2-8; J-2, cs-LG, cs.LG, stat-ML<br>Keyword Score: 5<br>Keywords: Black Box<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08193v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08193v1.pdf filename=2402.08193v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Efficient inference in high-dimensional models remains a central challenge in machine learning. This paper introduces the Gaussian Ensemble Belief Propagation (GEnBP) algorithm, a fusion of the Ensemble Kalman filter and Gaussian belief propagation (GaBP) methods. GEnBP updates ensembles by passing low-rank local messages in a graphical model structure. This combination inherits favourable qualities from each method. Ensemble techniques allow GEnBP to handle high-dimensional states, parameters and intricate, noisy, <b>black-box</b> <b>generation</b> processes. The use of local messages in a graphical model structure ensures that the approach is suited to distributed computing and can efficiently handle complex dependence structures. GEnBP is particularly advantageous when the ensemble size is considerably smaller than the inference dimension. This scenario often arises in fields such as spatiotemporal modelling, image processing and physical model inversion. GEnBP can be applied to general problem structures, including jointly learning system parameters, observation parameters, and latent state variables.</p></p class="citation"></blockquote><h3 id=5252--86247-causal-discovery-under-off-target-interventions-davin-choo-et-al-2024>(52/52 | 86/247) Causal Discovery under Off-Target Interventions (Davin Choo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Davin Choo, Kirankumar Shiragur, Caroline Uhler. (2024)<br><strong>Causal Discovery under Off-Target Interventions</strong><br><button class=copy-to-clipboard title="Causal Discovery under Off-Target Interventions" index=86>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-86 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-DS, cs-LG, cs.LG, stat-ME, stat-ML<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08229v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08229v1.pdf filename=2402.08229v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Causal <b>graph</b> discovery is a significant problem with applications across various disciplines. However, with observational data alone, the underlying causal <b>graph</b> can only be recovered up to its Markov equivalence class, and further assumptions or interventions are necessary to narrow down the true <b>graph.</b> This work addresses the causal discovery problem under the setting of stochastic interventions with the natural goal of minimizing the number of interventions performed. We propose the following stochastic intervention model which subsumes existing adaptive noiseless interventions in the literature while capturing scenarios such as fat-hand interventions and CRISPR gene knockouts: any intervention attempt results in an actual intervention on a random subset of vertices, drawn from a distribution dependent on attempted action. Under this model, we study the two fundamental problems in causal discovery of verification and search and provide approximation algorithms with polylogarithmic competitive ratios and provide some preliminary experimental results.</p></p class="citation"></blockquote><h2 id=cscr-5>cs.CR (5)</h2><h3 id=15--87247-pandora-jailbreak-gpts-by-retrieval-augmented-generation-poisoning-gelei-deng-et-al-2024>(1/5 | 87/247) Pandora: Jailbreak GPTs by Retrieval Augmented Generation Poisoning (Gelei Deng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Gelei Deng, Yi Liu, Kailong Wang, Yuekang Li, Tianwei Zhang, Yang Liu. (2024)<br><strong>Pandora: Jailbreak GPTs by Retrieval Augmented Generation Poisoning</strong><br><button class=copy-to-clipboard title="Pandora: Jailbreak GPTs by Retrieval Augmented Generation Poisoning" index=87>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-87 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs.CR<br>Keyword Score: 90<br>Keywords: Retrieval-Augmented Generation, Retrieval-Augmented Generation, Retrieval-Augmented Generation, GPT, GPT-3, GPT-3.5, GPT-4, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08416v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08416v1.pdf filename=2402.08416v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Large Language Models~(LLMs) have gained immense popularity and are being increasingly applied in various domains. Consequently, ensuring the security of these models is of paramount importance. Jailbreak attacks, which manipulate <b>LLMs</b> to generate malicious content, are recognized as a significant vulnerability. While existing research has predominantly focused on direct jailbreak attacks on <b>LLMs,</b> there has been limited exploration of indirect methods. The integration of various plugins into <b>LLMs,</b> notably <b>Retrieval</b> <b>Augmented</b> <b>Generation~(RAG),</b> which enables <b>LLMs</b> to incorporate external knowledge bases into their response generation such as <b>GPTs,</b> introduces new avenues for indirect jailbreak attacks. To fill this gap, we investigate indirect jailbreak attacks on <b>LLMs,</b> particularly <b>GPTs,</b> introducing a novel attack vector named <b>Retrieval</b> <b>Augmented</b> <b>Generation</b> Poisoning. This method, Pandora, exploits the synergy between <b>LLMs</b> and <b>RAG</b> through <b>prompt</b> manipulation to generate unexpected responses. Pandora uses maliciously crafted content to influence the <b>RAG</b> process, effectively initiating jailbreak attacks. Our preliminary tests show that Pandora successfully conducts jailbreak attacks in four different scenarios, achieving higher success rates than direct attacks, with 64.3% for <b>GPT-3.5</b> and 34.8% for <b>GPT-4.</b></p></p class="citation"></blockquote><h3 id=25--88247-data-reconstruction-attacks-and-defenses-a-systematic-evaluation-sheng-liu-et-al-2024>(2/5 | 88/247) Data Reconstruction Attacks and Defenses: A Systematic Evaluation (Sheng Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sheng Liu, Zihan Wang, Qi Lei. (2024)<br><strong>Data Reconstruction Attacks and Defenses: A Systematic Evaluation</strong><br><button class=copy-to-clipboard title="Data Reconstruction Attacks and Defenses: A Systematic Evaluation" index=88>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-88 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs-LG, cs.CR<br>Keyword Score: 30<br>Keywords: Federated Learning, Pruning, Grounding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09478v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09478v1.pdf filename=2402.09478v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Reconstruction attacks and defenses are essential in understanding the data leakage problem in machine learning. However, prior work has centered around empirical observations of gradient inversion attacks, lacks theoretical <b>groundings,</b> and was unable to disentangle the usefulness of defending methods versus the computational limitation of attacking methods. In this work, we propose a strong reconstruction attack in the setting of <b>federated</b> <b>learning.</b> The attack reconstructs intermediate features and nicely integrates with and outperforms most of the previous methods. On this stronger attack, we thoroughly investigate both theoretically and empirically the effect of the most common defense methods. Our findings suggest that among various defense mechanisms, such as gradient clipping, dropout, additive noise, local aggregation, etc., gradient <b>pruning</b> emerges as the most effective strategy to defend against state-of-the-art attacks.</p></p class="citation"></blockquote><h3 id=35--89247-cryptomite-a-versatile-and-user-friendly-library-of-randomness-extractors-cameron-foreman-et-al-2024>(3/5 | 89/247) Cryptomite: A versatile and user-friendly library of randomness extractors (Cameron Foreman et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Cameron Foreman, Richie Yeung, Alec Edgington, Florian J. Curchod. (2024)<br><strong>Cryptomite: A versatile and user-friendly library of randomness extractors</strong><br><button class=copy-to-clipboard title="Cryptomite: A versatile and user-friendly library of randomness extractors" index=89>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-89 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs.CR, quant-ph<br>Keyword Score: 16<br>Keywords: Benchmarking, Benchmarking, Convolution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09481v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09481v1.pdf filename=2402.09481v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present Cryptomite, a Python library of randomness extractor implementations. The library offers a range of two-source, seeded and deterministic randomness extractors, together with parameter calculation modules, making it easy to use and suitable for a variety of applications. We also present theoretical results, including new extractor constructions and improvements to existing extractor parameters. The extractor implementations are efficient in practice and tolerate input sizes of up to $2^{40} > 10^{12}$ bits. They are also numerically precise (implementing <b>convolutions</b> using the Number Theoretic Transform to avoid floating point arithmetic), making them well suited to cryptography. The algorithms and parameter calculation are described in detail, including illustrative code examples and performance <b>benchmarking.</b></p></p class="citation"></blockquote><h3 id=45--90247-zero-trust-score-based-network-level-access-control-in-enterprise-networks-leonard-bradatsch-et-al-2024>(4/5 | 90/247) Zero Trust Score-based Network-level Access Control in Enterprise Networks (Leonard Bradatsch et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Leonard Bradatsch, Oleksandr Miroshkin, Natasa Trkulja, Frank Kargl. (2024)<br><strong>Zero Trust Score-based Network-level Access Control in Enterprise Networks</strong><br><button class=copy-to-clipboard title="Zero Trust Score-based Network-level Access Control in Enterprise Networks" index=90>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-90 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs.CR<br>Keyword Score: 10<br>Keywords: Zero Trust<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08299v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08299v1.pdf filename=2402.08299v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Zero</b> <b>Trust</b> security has recently gained attention in enterprise network security. One of its key ideas is making network-level access decisions based on trust scores. However, score-based access control in the enterprise domain still lacks essential elements in our understanding, and in this paper, we contribute with respect to three crucial aspects. First, we provide a comprehensive list of 29 trust attributes that can be used to calculate a trust score. By introducing a novel mathematical approach, we demonstrate how to quantify these attributes. Second, we describe a dynamic risk-based method to calculate the trust threshold the trust score must meet for permitted access. Third, we introduce a novel trust algorithm based on Subjective Logic that incorporates the first two contributions and offers fine-grained decision possibilities. We discuss how this algorithm shows a higher expressiveness compared to a lightweight additive trust algorithm. Performance-wise, a prototype of the Subjective Logic-based approach showed similar calculation times for making an access decision as the additive approach. In addition, the dynamic threshold calculation showed only 7% increased decision-making times compared to a static threshold.</p></p class="citation"></blockquote><h3 id=55--91247-neurobench-an-open-source-benchmark-framework-for-the-standardization-of-methodology-in-brainwave-based-authentication-research-avinash-kumar-chaurasia-et-al-2024>(5/5 | 91/247) NeuroBench: An Open-Source Benchmark Framework for the Standardization of Methodology in Brainwave-based Authentication Research (Avinash Kumar Chaurasia et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Avinash Kumar Chaurasia, Matin Fallahi, Thorsten Strufe, Philipp Terhörst, Patricia Arias Cabarcos. (2024)<br><strong>NeuroBench: An Open-Source Benchmark Framework for the Standardization of Methodology in Brainwave-based Authentication Research</strong><br><button class=copy-to-clipboard title="NeuroBench: An Open-Source Benchmark Framework for the Standardization of Methodology in Brainwave-based Authentication Research" index=91>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-91 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs.CR<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08656v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08656v2.pdf filename=2402.08656v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Biometric systems based on brain activity have been proposed as an alternative to passwords or to complement current authentication techniques. By leveraging the unique brainwave patterns of individuals, these systems offer the possibility of creating authentication solutions that are resistant to theft, hands-free, accessible, and potentially even revocable. However, despite the growing stream of research in this area, faster advance is hindered by reproducibility problems. Issues such as the lack of standard reporting schemes for performance results and system configuration, or the absence of common evaluation <b>benchmarks,</b> make comparability and proper assessment of different biometric solutions challenging. Further, barriers are erected to future work when, as so often, source code is not published open access. To bridge this gap, we introduce NeuroBench, a flexible open source tool to <b>benchmark</b> brainwave-based authentication models. It incorporates nine diverse datasets, implements a comprehensive set of pre-processing parameters and machine learning algorithms, enables testing under two common adversary models (known vs unknown attacker), and allows researchers to generate full performance reports and visualizations. We use NeuroBench to investigate the shallow classifiers and deep learning-based approaches proposed in the literature, and to test robustness across multiple sessions. We observe a 37.6% reduction in Equal Error Rate (EER) for unknown attacker scenarios (typically not tested in the literature), and we highlight the importance of session variability to brainwave authentication. All in all, our results demonstrate the viability and relevance of NeuroBench in streamlining fair comparisons of algorithms, thereby furthering the advancement of brainwave-based authentication through robust methodological practices.</p></p class="citation"></blockquote><h2 id=csro-9>cs.RO (9)</h2><h3 id=19--92247-grounding-llms-for-robot-task-planning-using-closed-loop-state-feedback-vineet-bhat-et-al-2024>(1/9 | 92/247) Grounding LLMs For Robot Task Planning Using Closed-loop State Feedback (Vineet Bhat et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Vineet Bhat, Ali Umut Kaypak, Prashanth Krishnamurthy, Ramesh Karri, Farshad Khorrami. (2024)<br><strong>Grounding LLMs For Robot Task Planning Using Closed-loop State Feedback</strong><br><button class=copy-to-clipboard title="Grounding LLMs For Robot Task Planning Using Closed-loop State Feedback" index=92>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-92 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 83<br>Keywords: Benchmarking, GPT, GPT-3, GPT-3.5, GPT-4, PaLM, Grounding, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08546v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08546v1.pdf filename=2402.08546v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Robotic planning algorithms direct agents to perform actions within diverse environments to accomplish a task. <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> like <b>PaLM</b> 2, <b>GPT-3.5,</b> and <b>GPT-4</b> have revolutionized this domain, using their embedded real-world knowledge to tackle complex tasks involving multiple agents and objects. This paper introduces an innovative planning algorithm that integrates <b>LLMs</b> into the robotics context, enhancing task-focused execution and success rates. Key to our algorithm is a closed-loop feedback which provides real-time environmental states and error messages, crucial for refining plans when discrepancies arise. The algorithm draws inspiration from the human neural system, emulating its brain-body architecture by dividing planning across two <b>LLMs</b> in a structured, hierarchical fashion. Our method not only surpasses baselines within the VirtualHome Environment, registering a notable 35% average increase in task-oriented success rates, but achieves an impressive execution score of 85%, approaching the human-level <b>benchmark</b> of 94%. Moreover, effectiveness of the algorithm in real robot scenarios is shown using a realistic physics simulator and the Franka Research 3 Arm.</p></p class="citation"></blockquote><h3 id=29--93247-online-foundation-model-selection-in-robotics-po-han-li-et-al-2024>(2/9 | 93/247) Online Foundation Model Selection in Robotics (Po-han Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Po-han Li, Oyku Selin Toprak, Aditya Narayanan, Ufuk Topcu, Sandeep Chinchali. (2024)<br><strong>Online Foundation Model Selection in Robotics</strong><br><button class=copy-to-clipboard title="Online Foundation Model Selection in Robotics" index=93>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-93 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-LG, cs-RO, cs.RO<br>Keyword Score: 40<br>Keywords: Foundation Model, Knowledge Distillation, Supervised Learning, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08570v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08570v1.pdf filename=2402.08570v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Foundation</b> <b>models</b> have recently expanded into robotics after excelling in computer vision and natural language processing. The models are accessible in two ways: open-source or paid, closed-source options. Users with access to both face a problem when deciding between effective yet costly closed-source models and free but less powerful open-source alternatives. We call it the model selection problem. Existing <b>supervised-learning</b> <b>methods</b> are impractical due to the high cost of collecting extensive training data from closed-source models. Hence, we focus on the online learning setting where algorithms learn while collecting data, eliminating the need for large pre-collected datasets. We thus formulate a user-centric online model selection problem and propose a novel solution that combines an open-source encoder to output context and an online learning algorithm that processes this context. The encoder <b>distills</b> vast data distributions into low-dimensional features, i.e., the context, without additional training. The online learning algorithm aims to maximize a composite reward that includes model performance, execution time, and costs based on the context extracted from the data. It results in an improved trade-off between selecting open-source and closed-source models compared to non-contextual methods, as validated by our theoretical analysis. Experiments across language-based robotic tasks such as Waymo Open Dataset, ALFRED, and Open X-Embodiment demonstrate real-world applications of the solution. The results show that the solution significantly improves the task success rate by up to 14%.</p></p class="citation"></blockquote><h3 id=39--94247-the-colosseum-a-benchmark-for-evaluating-generalization-for-robotic-manipulation-wilbert-pumacay-et-al-2024>(3/9 | 94/247) THE COLOSSEUM: A Benchmark for Evaluating Generalization for Robotic Manipulation (Wilbert Pumacay et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wilbert Pumacay, Ishika Singh, Jiafei Duan, Ranjay Krishna, Jesse Thomason, Dieter Fox. (2024)<br><strong>THE COLOSSEUM: A Benchmark for Evaluating Generalization for Robotic Manipulation</strong><br><button class=copy-to-clipboard title="THE COLOSSEUM: A Benchmark for Evaluating Generalization for Robotic Manipulation" index=94>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-94 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-LG, cs-RO, cs.RO<br>Keyword Score: 23<br>Keywords: Benchmarking, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08191v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08191v1.pdf filename=2402.08191v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>To realize effective large-scale, real-world robotic applications, we must evaluate how well our robot policies adapt to changes in environmental conditions. Unfortunately, a majority of studies evaluate robot performance in environments closely resembling or even identical to the training setup. We present THE COLOSSEUM, a novel <b>simulation</b> <b>benchmark,</b> with 20 diverse manipulation tasks, that enables systematical evaluation of models across 12 axes of environmental perturbations. These perturbations include changes in color, texture, and size of objects, table-tops, and backgrounds; we also vary lighting, distractors, and camera pose. Using THE COLOSSEUM, we compare 4 state-of-the-art manipulation models to reveal that their success rate degrades between 30-50% across these perturbation factors. When multiple perturbations are applied in unison, the success rate degrades $\geq$75%. We identify that changing the number of distractor objects, target object color, or lighting conditions are the perturbations that reduce model performance the most. To verify the ecological validity of our results, we show that our results in <b>simulation</b> are correlated ($\bar{R}^2 = 0.614$) to similar perturbations in real-world experiments. We open source code for others to use THE COLOSSEUM, and also release code to 3D print the objects used to replicate the real-world perturbations. Ultimately, we hope that THE COLOSSEUM will serve as a <b>benchmark</b> to identify modeling decisions that systematically improve generalization for manipulation. See <a href=https://robot-colosseum.github.io/>https://robot-colosseum.github.io/</a> for more details.</p></p class="citation"></blockquote><h3 id=49--95247-gaussian-sum-filter-for-range-based-3d-relative-pose-estimation-in-the-presence-of-ambiguities-syed-s-ahmed-et-al-2024>(4/9 | 95/247) Gaussian-Sum Filter for Range-based 3D Relative Pose Estimation in the Presence of Ambiguities (Syed S. Ahmed et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Syed S. Ahmed, Mohammed A. Shalaby, Charles C. Cossette, Jerome Le Ny, James R. Forbes. (2024)<br><strong>Gaussian-Sum Filter for Range-based 3D Relative Pose Estimation in the Presence of Ambiguities</strong><br><button class=copy-to-clipboard title="Gaussian-Sum Filter for Range-based 3D Relative Pose Estimation in the Presence of Ambiguities" index=95>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-95 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08566v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08566v1.pdf filename=2402.08566v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Multi-robot systems must have the ability to accurately estimate relative states between robots in order to perform collaborative tasks, possibly with no external aiding. Three-dimensional relative pose estimation using range measurements oftentimes suffers from a finite number of non-unique solutions, or ambiguities. This paper: 1) identifies and accurately estimates all possible ambiguities in 2D; 2) treats them as components of a Gaussian mixture model; and 3) presents a computationally-efficient estimator, in the form of a Gaussian-sum filter (GSF), to realize range-based relative pose estimation in an infrastructure-free, 3D, setup. This estimator is evaluated in <b>simulation</b> and experiment and is shown to avoid divergence to local minima induced by the ambiguous poses. Furthermore, the proposed GSF outperforms an extended Kalman filter, demonstrates similar performance to the computationally-demanding particle filter, and is shown to be consistent.</p></p class="citation"></blockquote><h3 id=59--96247-mavrl-learn-to-fly-in-cluttered-environments-with-varying-speed-hang-yu-et-al-2024>(5/9 | 96/247) MAVRL: Learn to Fly in Cluttered Environments with Varying Speed (Hang Yu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hang Yu, Christophe De Wagter, Guido C. H. E de Croon. (2024)<br><strong>MAVRL: Learn to Fly in Cluttered Environments with Varying Speed</strong><br><button class=copy-to-clipboard title="MAVRL: Learn to Fly in Cluttered Environments with Varying Speed" index=96>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-96 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Fine-tuning, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08381v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08381v1.pdf filename=2402.08381v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Many existing obstacle avoidance algorithms overlook the crucial balance between safety and agility, especially in environments of varying complexity. In our study, we introduce an obstacle avoidance pipeline based on <b>reinforcement</b> <b>learning.</b> This pipeline enables drones to adapt their flying speed according to the environmental complexity. Moreover, to improve the obstacle avoidance performance in cluttered environments, we propose a novel latent space. The latent space in this representation is explicitly trained to retain memory of previous depth map observations. Our findings confirm that varying speed leads to a superior balance of success rate and agility in cluttered environments. Additionally, our memory-augmented latent representation outperforms the latent representation commonly used in <b>reinforcement</b> <b>learning.</b> Finally, after minimal <b>fine-tuning,</b> we successfully deployed our network on a real drone for enhanced obstacle avoidance.</p></p class="citation"></blockquote><h3 id=69--97247-self-reconfigurable-v-shape-formation-of-multiple-uavs-in-narrow-space-environments-duy-nam-bui-et-al-2024>(6/9 | 97/247) Self-Reconfigurable V-shape Formation of Multiple UAVs in Narrow Space Environments (Duy Nam Bui et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Duy Nam Bui, Manh Duong Phung, Hung Pham Duy. (2024)<br><strong>Self-Reconfigurable V-shape Formation of Multiple UAVs in Narrow Space Environments</strong><br><button class=copy-to-clipboard title="Self-Reconfigurable V-shape Formation of Multiple UAVs in Narrow Space Environments" index=97>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-97 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08245v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08245v1.pdf filename=2402.08245v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents the design and implementation of a self-reconfigurable V-shape formation controller for multiple unmanned aerial vehicles (UAVs) navigating through narrow spaces in a dense obstacle environment. The selection of the V-shape formation is motivated by its maneuverability and visibility advantages. The main objective is to develop an effective formation control strategy that allows UAVs to autonomously adjust their positions to form the desired formation while navigating through obstacles. To achieve this, we propose a distributed behavior-based control algorithm that combines the behaviors designed for individual UAVs so that they together navigate the UAVs to their desired positions. The reconfiguration process is automatic, utilizing individual UAV sensing within the formation, allowing for dynamic adaptations such as opening/closing wings or merging into a straight line. <b>Simulation</b> results show that the self-reconfigurable V-shape formation offers adaptability and effectiveness for UAV formations in complex operational scenarios.</p></p class="citation"></blockquote><h3 id=79--98247-metatra-meta-learning-for-generalized-trajectory-prediction-in-unseen-domain-xiaohe-li-et-al-2024>(7/9 | 98/247) MetaTra: Meta-Learning for Generalized Trajectory Prediction in Unseen Domain (Xiaohe Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiaohe Li, Feilong Huang, Zide Fan, Fangli Mou, Yingyan Hou, Chen Qian, Lijie Wen. (2024)<br><strong>MetaTra: Meta-Learning for Generalized Trajectory Prediction in Unseen Domain</strong><br><button class=copy-to-clipboard title="MetaTra: Meta-Learning for Generalized Trajectory Prediction in Unseen Domain" index=98>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-98 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-CV, cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Meta Learning, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08221v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08221v1.pdf filename=2402.08221v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Trajectory prediction has garnered widespread attention in different fields, such as autonomous driving and robotic navigation. However, due to the significant variations in trajectory patterns across different scenarios, models trained in known environments often falter in unseen ones. To learn a generalized model that can directly handle unseen domains without requiring any model updating, we propose a novel <b>meta-learning-based</b> <b>trajectory</b> prediction method called MetaTra. This approach incorporates a Dual Trajectory <b>Transformer</b> (Dual-TT), which enables a thorough exploration of the individual intention and the interactions within group motion patterns in diverse scenarios. Building on this, we propose a <b>meta-learning</b> <b>framework</b> to simulate the generalization process between source and target domains. Furthermore, to enhance the stability of our prediction outcomes, we propose a Serial and Parallel Training (SPT) strategy along with a feature augmentation method named MetaMix. Experimental results on several real-world datasets confirm that MetaTra not only surpasses other state-of-the-art methods but also exhibits plug-and-play capabilities, particularly in the realm of domain generalization.</p></p class="citation"></blockquote><h3 id=89--99247-bbsea-an-exploration-of-brain-body-synchronization-for-embodied-agents-sizhe-yang-et-al-2024>(8/9 | 99/247) BBSEA: An Exploration of Brain-Body Synchronization for Embodied Agents (Sizhe Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sizhe Yang, Qian Luo, Anumpam Pani, Yanchao Yang. (2024)<br><strong>BBSEA: An Exploration of Brain-Body Synchronization for Embodied Agents</strong><br><button class=copy-to-clipboard title="BBSEA: An Exploration of Brain-Body Synchronization for Embodied Agents" index=99>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-99 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 10<br>Keywords: Foundation Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08212v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08212v1.pdf filename=2402.08212v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Embodied agents capable of complex physical skills can improve productivity, elevate life quality, and reshape human-machine collaboration. We aim at autonomous training of embodied agents for various tasks involving mainly large <b>foundation</b> <b>models.</b> It is believed that these models could act as a brain for embodied agents; however, existing methods heavily rely on humans for task proposal and scene customization, limiting the learning autonomy, training efficiency, and generalization of the learned policies. In contrast, we introduce a brain-body synchronization ({\it BBSEA}) scheme to promote embodied learning in unknown environments without human involvement. The proposed combines the wisdom of <b>foundation</b> <b>models</b> (<code>brain'') with the physical capabilities of embodied agents (</code>body&rsquo;&rsquo;). Specifically, it leverages the <code>brain'' to propose learnable physical tasks and success metrics, enabling the </code>body&rsquo;&rsquo; to automatically acquire various skills by continuously interacting with the scene. We carry out an exploration of the proposed autonomous learning scheme in a table-top setting, and we demonstrate that the proposed synchronization can generate diverse tasks and develop multi-task policies with promising adaptability to new tasks and configurations. We will release our data, code, and trained models to facilitate future studies in building autonomously learning agents with large <b>foundation</b> <b>models</b> in more complex scenarios. More visualizations are available at \href{https://bbsea-embodied-ai.github.io}{https://bbsea-embodied-ai.github.io}</p></p class="citation"></blockquote><h3 id=99--100247-approximate-sequential-optimization-for-informative-path-planning-joshua-ott-et-al-2024>(9/9 | 100/247) Approximate Sequential Optimization for Informative Path Planning (Joshua Ott et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Joshua Ott, Mykel J. Kochenderfer, Stephen Boyd. (2024)<br><strong>Approximate Sequential Optimization for Informative Path Planning</strong><br><button class=copy-to-clipboard title="Approximate Sequential Optimization for Informative Path Planning" index=100>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-100 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 9<br>Keywords: Graph, Multi-modal, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08841v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08841v1.pdf filename=2402.08841v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We consider the problem of finding an informative path through a <b>graph,</b> given initial and terminal nodes and a given maximum path length. We assume that a linear noise corrupted measurement is taken at each node of an underlying unknown vector that we wish to estimate. The informativeness is measured by the reduction in uncertainty in our estimate, evaluated using several metrics. We present a convex relaxation for this informative path planning problem, which we can readily solve to obtain a bound on the possible performance. We develop an approximate sequential method where the path is constructed segment by segment through dynamic programming. This involves solving an orienteering problem, with the node reward acting as a surrogate for informativeness, taking the first step, and then repeating the process. The method scales to very large problem instances and achieves performance not too far from the bound produced by the convex relaxation. We also demonstrate our method&rsquo;s ability to handle adaptive objectives, <b>multimodal</b> sensing, and multi-agent variations of the informative path planning problem.</p></p class="citation"></blockquote><h2 id=cscv-37>cs.CV (37)</h2><h3 id=137--101247-pin-positional-insert-unlocks-object-localisation-abilities-in-vlms-michael-dorkenwald-et-al-2024>(1/37 | 101/247) PIN: Positional Insert Unlocks Object Localisation Abilities in VLMs (Michael Dorkenwald et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Michael Dorkenwald, Nimrod Barazani, Cees G. M. Snoek, Yuki M. Asano. (2024)<br><strong>PIN: Positional Insert Unlocks Object Localisation Abilities in VLMs</strong><br><button class=copy-to-clipboard title="PIN: Positional Insert Unlocks Object Localisation Abilities in VLMs" index=101>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-101 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 76<br>Keywords: Multi-modal, Multi-modal, Supervised Learning, Zero-shot, GPT, Grounding, Large Language Model, Prompt, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08657v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08657v1.pdf filename=2402.08657v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Vision-Language</b> Models (VLMs), such as Flamingo and <b>GPT-4V,</b> have shown immense potential by integrating <b>large</b> <b>language</b> <b>models</b> with vision systems. Nevertheless, these models face challenges in the fundamental computer vision task of object localisation, due to their training on <b>multimodal</b> data containing mostly captions without explicit spatial <b>grounding.</b> While it is possible to construct custom, <b>supervised</b> training pipelines with bounding box annotations that integrate with VLMs, these result in specialized and hard-to-scale models. In this paper, we aim to explore the limits of caption-based VLMs and instead propose to tackle the challenge in a simpler manner by i) keeping the weights of a caption-based VLM frozen and ii) not using any <b>supervised</b> detection data. To this end, we introduce an input-agnostic Positional Insert (PIN), a learnable spatial <b>prompt,</b> containing a minimal set of parameters that are slid inside the frozen VLM, unlocking object localisation capabilities. Our PIN module is trained with a simple next-token prediction task on synthetic data without requiring the introduction of new output heads. Our experiments demonstrate strong <b>zero-shot</b> localisation performances on a variety of images, including Pascal VOC, COCO, LVIS, and diverse images like paintings or cartoons.</p></p class="citation"></blockquote><h3 id=237--102247-visual-question-answering-instruction-unlocking-multimodal-large-language-model-to-domain-specific-visual-multitasks-jusung-lee-et-al-2024>(2/37 | 102/247) Visual Question Answering Instruction: Unlocking Multimodal Large Language Model To Domain-Specific Visual Multitasks (Jusung Lee et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jusung Lee, Sungguk Cha, Younghyun Lee, Cheoljong Yang. (2024)<br><strong>Visual Question Answering Instruction: Unlocking Multimodal Large Language Model To Domain-Specific Visual Multitasks</strong><br><button class=copy-to-clipboard title="Visual Question Answering Instruction: Unlocking Multimodal Large Language Model To Domain-Specific Visual Multitasks" index=102>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-102 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 66<br>Keywords: Multi-modal, Multi-modal, Question Answering, Visual Question Answering, Visual Question Answering, Large Language Model, Large Language Model, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08360v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08360v1.pdf filename=2402.08360v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Having revolutionized natural language processing (NLP) applications, <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> are expanding into the realm of <b>multimodal</b> inputs. Owing to their ability to interpret images, <b>multimodal</b> <b>LLMs</b> (MLLMs) have been primarily used for <b>vision-language</b> tasks. Currently, MLLMs have not yet been extended for domain-specific <b>visual</b> <b>tasks,</b> <b>which</b> require a more explicit understanding of <b>visual</b> <b>information.</b> <b>We</b> developed a method to transform domain-specific <b>visual</b> <b>and</b> <b>vision-language</b> datasets into a unified <b>question</b> <b>answering</b> format called <b>Visual</b> <b>Question</b> <b>Answering</b> Instruction <b>(VQA-IN),</b> thereby extending MLLM to domain-specific tasks. The <b>VQA-IN</b> was applied to train multiple MLLM architectures using smaller versions of <b>LLMs</b> (sLLMs). The experimental results indicated that the proposed method achieved a high score metric on domainspecific <b>visual</b> <b>tasks</b> <b>while</b> also maintaining its performance on <b>vision-language</b> tasks in a multitask manner.</p></p class="citation"></blockquote><h3 id=337--103247-befunet-a-hybrid-cnn-transformer-architecture-for-precise-medical-image-segmentation-omid-nejati-manzari-et-al-2024>(3/37 | 103/247) BEFUnet: A Hybrid CNN-Transformer Architecture for Precise Medical Image Segmentation (Omid Nejati Manzari et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Omid Nejati Manzari, Javad Mirzapour Kaleybar, Hooman Saadat, Shahin Maleki. (2024)<br><strong>BEFUnet: A Hybrid CNN-Transformer Architecture for Precise Medical Image Segmentation</strong><br><button class=copy-to-clipboard title="BEFUnet: A Hybrid CNN-Transformer Architecture for Precise Medical Image Segmentation" index=103>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-103 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 60<br>Keywords: Convolution, Convolutional Neural Network, Convolutional Neural Network, Convolutional Neural Network, Transformer, Information Retrieval<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08793v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08793v1.pdf filename=2402.08793v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The accurate segmentation of medical images is critical for various healthcare applications. <b>Convolutional</b> <b>neural</b> <b>networks</b> <b>(CNNs),</b> especially Fully <b>Convolutional</b> <b>Networks</b> <b>(FCNs)</b> like U-Net, have shown remarkable success in medical image segmentation tasks. However, they have limitations in capturing global context and long-range relations, especially for objects with significant variations in shape, scale, and texture. While <b>transformers</b> have achieved state-of-the-art results in natural language processing and image recognition, they face challenges in medical image segmentation due to image locality and translational invariance issues. To address these challenges, this paper proposes an innovative U-shaped network called BEFUnet, which enhances the fusion of body and edge <b>information</b> <b>for</b> precise medical image segmentation. The BEFUnet comprises three main modules, including a novel Local Cross-Attention Feature (LCAF) fusion module, a novel Double-Level Fusion (DLF) module, and dual-branch encoder. The dual-branch encoder consists of an edge encoder and a body encoder. The edge encoder employs PDC blocks for effective edge <b>information</b> <b>extraction,</b> while the body encoder uses the Swin <b>Transformer</b> to capture semantic <b>information</b> <b>with</b> global attention. The LCAF module efficiently fuses edge and body features by selectively performing local cross-attention on features that are spatially close between the two modalities. This local approach significantly reduces computational complexity compared to global cross-attention while ensuring accurate feature matching. BEFUnet demonstrates superior performance over existing methods across various evaluation metrics on medical image segmentation datasets.</p></p class="citation"></blockquote><h3 id=437--104247-visually-dehallucinative-instruction-generation-sungguk-cha-et-al-2024>(4/37 | 104/247) Visually Dehallucinative Instruction Generation (Sungguk Cha et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sungguk Cha, Jusung Lee, Younghyun Lee, Cheoljong Yang. (2024)<br><strong>Visually Dehallucinative Instruction Generation</strong><br><button class=copy-to-clipboard title="Visually Dehallucinative Instruction Generation" index=104>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-104 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 60<br>Keywords: Image2text, Question Answering, Question Answering, Text Generation, Visual Question Answering, Instruction Tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08348v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08348v1.pdf filename=2402.08348v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In recent years, synthetic <b>visual</b> <b>instructions</b> <b>by</b> generative language model have demonstrated plausible <b>text</b> <b>generation</b> performance on the <b>visual</b> <b>question-answering</b> <b>tasks.</b> However, challenges persist in the hallucination of generative language models, i.e., the generated <b>image-text</b> data contains unintended contents. This paper presents a novel and scalable method for generating visually dehallucinative <b>instructions,</b> <b>dubbed</b> CAP2QA, that constrains the scope to only image contents. Our key contributions lie in introducing image-aligned instructive <b>QA</b> dataset CAP2QA-COCO and its scalable recipe. In our experiments, we compare synthetic <b>visual</b> <b>instruction</b> <b>datasets</b> that share the same source data by <b>visual</b> <b>instruction</b> <b>tuning</b> and conduct general <b>visual</b> <b>recognition</b> <b>tasks.</b> It shows that our proposed method significantly reduces <b>visual</b> <b>hallucination</b> <b>while</b> consistently improving <b>visual</b> <b>recognition</b> <b>ability</b> and expressiveness.</p></p class="citation"></blockquote><h3 id=537--105247-leveraging-self-supervised-instance-contrastive-learning-for-radar-object-detection-colin-decourt-et-al-2024>(5/37 | 105/247) Leveraging Self-Supervised Instance Contrastive Learning for Radar Object Detection (Colin Decourt et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Colin Decourt, Rufin VanRullen, Didier Salle, Thomas Oberlin. (2024)<br><strong>Leveraging Self-Supervised Instance Contrastive Learning for Radar Object Detection</strong><br><button class=copy-to-clipboard title="Leveraging Self-Supervised Instance Contrastive Learning for Radar Object Detection" index=105>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-105 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 50<br>Keywords: Object Detection, Contrastive Learning, Self-supervised Learning, Self-supervised Learning, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08427v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08427v1.pdf filename=2402.08427v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In recent years, driven by the need for safer and more autonomous transport systems, the automotive industry has shifted toward integrating a growing number of Advanced Driver Assistance Systems (ADAS). Among the array of sensors employed for <b>object</b> <b>recognition</b> tasks, radar sensors have emerged as a formidable contender due to their abilities in adverse weather conditions or low-light scenarios and their robustness in maintaining consistent performance across diverse environments. However, the small size of radar datasets and the complexity of the labelling of those data limit the performance of radar <b>object</b> <b>detectors.</b> Driven by the promising results of <b>self-supervised</b> <b>learning</b> in computer vision, this paper presents RiCL, an instance <b>contrastive</b> <b>learning</b> framework to pre-train radar <b>object</b> <b>detectors.</b> We propose to exploit the detection from the radar and the temporal information to pre-train the radar <b>object</b> <b>detection</b> model in a <b>self-supervised</b> <b>way</b> using <b>contrastive</b> <b>learning.</b> We aim to pre-train an <b>object</b> <b>detector&rsquo;s</b> backbone, head and neck to learn with fewer data. Experiments on the CARRADA and the RADDet datasets show the effectiveness of our approach in learning generic representations of <b>objects</b> <b>in</b> range-Doppler maps. Notably, our pre-training strategy allows us to use only 20% of the labelled data to reach a similar <a href=mailto:mAP@0.5>mAP@0.5</a> than a <b>supervised</b> approach using the whole training set.</p></p class="citation"></blockquote><h3 id=637--106247-peeking-behind-the-curtains-of-residual-learning-tunhou-zhang-et-al-2024>(6/37 | 106/247) Peeking Behind the Curtains of Residual Learning (Tunhou Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tunhou Zhang, Feng Yan, Hai Li, Yiran Chen. (2024)<br><strong>Peeking Behind the Curtains of Residual Learning</strong><br><button class=copy-to-clipboard title="Peeking Behind the Curtains of Residual Learning" index=106>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-106 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 43<br>Keywords: Vision Transformer, Benchmarking, Convolutional Neural Network, Transformer, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08645v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08645v1.pdf filename=2402.08645v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The utilization of residual learning has become widespread in deep and scalable neural nets. However, the fundamental principles that contribute to the success of residual learning remain elusive, thus hindering effective training of plain nets with depth scalability. In this paper, we peek behind the curtains of residual learning by uncovering the &ldquo;dissipating inputs&rdquo; phenomenon that leads to convergence failure in plain neural nets: the input is gradually compromised through plain layers due to non-linearities, resulting in challenges of learning feature representations. We theoretically demonstrate how plain neural nets degenerate the input to random noise and emphasize the significance of a residual connection that maintains a better lower bound of surviving neurons as a solution. With our theoretical discoveries, we propose &ldquo;The Plain Neural Net Hypothesis&rdquo; (PNNH) that identifies the internal path across non-linear layers as the most critical part in residual learning, and establishes a paradigm to support the training of deep plain neural nets devoid of residual connections. We thoroughly evaluate PNNH-enabled <b>CNN</b> architectures and <b>Transformers</b> on popular <b>vision</b> <b>benchmarks,</b> showing on-par accuracy, up to 0.3% higher training throughput, and 2x better parameter efficiency compared to ResNets and <b>vision</b> <b>Transformers.</b></p></p class="citation"></blockquote><h3 id=737--107247-towards-the-detection-of-ai-synthesized-human-face-images-yuhang-lu-et-al-2024>(7/37 | 107/247) Towards the Detection of AI-Synthesized Human Face Images (Yuhang Lu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuhang Lu, Touradj Ebrahimi. (2024)<br><strong>Towards the Detection of AI-Synthesized Human Face Images</strong><br><button class=copy-to-clipboard title="Towards the Detection of AI-Synthesized Human Face Images" index=107>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-107 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV, eess-IV<br>Keyword Score: 33<br>Keywords: Benchmarking, Generative AI, Generative Adversarial Network, Generative Adversarial Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08750v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08750v1.pdf filename=2402.08750v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Over the past years, image generation and manipulation have achieved remarkable progress due to the rapid development of <b>generative</b> <b>AI</b> <b>based</b> on deep learning. Recent studies have devoted significant efforts to address the problem of face image manipulation caused by deepfake techniques. However, the problem of detecting purely synthesized face images has been explored to a lesser extent. In particular, the recent popular Diffusion Models (DMs) have shown remarkable success in image synthesis. Existing detectors struggle to generalize between synthesized images created by different <b>generative</b> <b>models.</b> <b>In</b> this work, a comprehensive <b>benchmark</b> including human face images produced by <b>Generative</b> <b>Adversarial</b> <b>Networks</b> <b>(GANs)</b> and a variety of DMs has been established to evaluate both the generalization ability and robustness of state-of-the-art detectors. Then, the forgery traces introduced by different <b>generative</b> <b>models</b> <b>have</b> been analyzed in the frequency domain to draw various insights. The paper further demonstrates that a detector trained with frequency representation can generalize well to other unseen <b>generative</b> <b>models.</b></p></p class="citation"></blockquote><h3 id=837--108247-intriguing-differences-between-zero-shot-and-systematic-evaluations-of-vision-language-transformer-models-shaeke-salman-et-al-2024>(8/37 | 108/247) Intriguing Differences Between Zero-Shot and Systematic Evaluations of Vision-Language Transformer Models (Shaeke Salman et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shaeke Salman, Md Montasir Bin Shams, Xiuwen Liu, Lingjiong Zhu. (2024)<br><strong>Intriguing Differences Between Zero-Shot and Systematic Evaluations of Vision-Language Transformer Models</strong><br><button class=copy-to-clipboard title="Intriguing Differences Between Zero-Shot and Systematic Evaluations of Vision-Language Transformer Models" index=108>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-108 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-LG, cs.CV<br>Keyword Score: 33<br>Keywords: Benchmarking, Zero-shot, Transformer, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08473v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08473v1.pdf filename=2402.08473v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Transformer-based</b> models have dominated natural language processing and other areas in the last few years due to their superior <b>(zero-shot)</b> performance on <b>benchmark</b> datasets. However, these models are poorly understood due to their complexity and size. While probing-based methods are widely used to understand specific properties, the structures of the representation space are not systematically characterized; consequently, it is unclear how such models generalize and overgeneralize to new inputs beyond datasets. In this paper, based on a new gradient descent optimization method, we are able to explore the embedding space of a commonly used <b>vision-language</b> model. Using the Imagenette dataset, we show that while the model achieves over 99% <b>zero-shot</b> classification performance, it fails systematic evaluations completely. Using a linear approximation, we provide a framework to explain the striking differences. We have also obtained similar results using a different model to support that our results are applicable to other <b>transformer</b> models with continuous inputs. We also propose a robust way to detect the modified images.</p></p class="citation"></blockquote><h3 id=937--109247-automated-detection-of-motion-artifacts-in-brain-mr-images-using-deep-learning-and-explainable-artificial-intelligence-marina-manso-jimeno-et-al-2024>(9/37 | 109/247) Automated detection of motion artifacts in brain MR images using deep learning and explainable artificial intelligence (Marina Manso Jimeno et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Marina Manso Jimeno, Keerthi Sravan Ravi, Maggie Fung, John Thomas Vaughan, Jr., Sairam Geethanath. (2024)<br><strong>Automated detection of motion artifacts in brain MR images using deep learning and explainable artificial intelligence</strong><br><button class=copy-to-clipboard title="Automated detection of motion artifacts in brain MR images using deep learning and explainable artificial intelligence" index=109>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-109 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 30<br>Keywords: Convolutional Neural Network, Low-Resource, Question Answering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08749v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08749v1.pdf filename=2402.08749v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Quality assessment, including inspecting the images for artifacts, is a critical step during MRI data acquisition to ensure data quality and downstream analysis or interpretation success. This study demonstrates a deep learning model to detect rigid motion in T1-weighted brain images. We leveraged a 2D <b>CNN</b> for three-class classification and tested it on publicly available retrospective and prospective datasets. Grad-CAM heatmaps enabled the identification of failure modes and provided an interpretation of the model&rsquo;s results. The model achieved average precision and recall metrics of 85% and 80% on six motion-simulated retrospective datasets. Additionally, the model&rsquo;s classifications on the prospective dataset showed a strong inverse correlation (-0.84) compared to average edge strength, an image quality metric indicative of motion. This model is part of the ArtifactID tool, aimed at inline automatic detection of Gibbs ringing, wrap-around, and motion artifacts. This tool automates part of the time-consuming <b>QA</b> process and augments expertise on-site, particularly relevant in <b>low-resource</b> settings where local MR knowledge is scarce.</p></p class="citation"></blockquote><h3 id=1037--110247-im-3d-iterative-multiview-diffusion-and-reconstruction-for-high-quality-3d-generation-luke-melas-kyriazi-et-al-2024>(10/37 | 110/247) IM-3D: Iterative Multiview Diffusion and Reconstruction for High-Quality 3D Generation (Luke Melas-Kyriazi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Luke Melas-Kyriazi, Iro Laina, Christian Rupprecht, Natalia Neverova, Andrea Vedaldi, Oran Gafni, Filippos Kokkinos. (2024)<br><strong>IM-3D: Iterative Multiview Diffusion and Reconstruction for High-Quality 3D Generation</strong><br><button class=copy-to-clipboard title="IM-3D: Iterative Multiview Diffusion and Reconstruction for High-Quality 3D Generation" index=110>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-110 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-LG, cs.CV<br>Keyword Score: 30<br>Keywords: Fine-tuning, Knowledge Distillation, Text2image<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08682v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08682v1.pdf filename=2402.08682v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Most text-to-3D generators build upon off-the-shelf <b>text-to-image</b> models trained on billions of images. They use variants of Score <b>Distillation</b> Sampling (SDS), which is slow, somewhat unstable, and prone to artifacts. A mitigation is to <b>fine-tune</b> the 2D generator to be multi-view aware, which can help <b>distillation</b> or can be combined with reconstruction networks to output 3D objects directly. In this paper, we further explore the design space of text-to-3D models. We significantly improve multi-view generation by considering video instead of image generators. Combined with a 3D reconstruction algorithm which, by using Gaussian splatting, can optimize a robust image-based loss, we directly produce high-quality 3D outputs from the generated views. Our new method, IM-3D, reduces the number of evaluations of the 2D generator network 10-100x, resulting in a much more efficient pipeline, better quality, fewer geometric inconsistencies, and higher yield of usable 3D assets.</p></p class="citation"></blockquote><h3 id=1137--111247-learning-continuous-3d-words-for-text-to-image-generation-ta-ying-cheng-et-al-2024>(11/37 | 111/247) Learning Continuous 3D Words for Text-to-Image Generation (Ta-Ying Cheng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ta-Ying Cheng, Matheus Gadelha, Thibault Groueix, Matthew Fisher, Radomir Mech, Andrew Markham, Niki Trigoni. (2024)<br><strong>Learning Continuous 3D Words for Text-to-Image Generation</strong><br><button class=copy-to-clipboard title="Learning Continuous 3D Words for Text-to-Image Generation" index=111>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-111 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: ControlNet, Text2image, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08654v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08654v1.pdf filename=2402.08654v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Current controls over diffusion models (e.g., through text or <b>ControlNet)</b> for image generation fall short in recognizing abstract, continuous attributes like illumination direction or non-rigid shape change. In this paper, we present an approach for allowing users of <b>text-to-image</b> models to have fine-grained control of several attributes in an image. We do this by engineering special sets of input tokens that can be transformed in a continuous manner &ndash; we call them Continuous 3D Words. These attributes can, for example, be represented as sliders and applied jointly with text <b>prompts</b> for fine-grained control over image generation. Given only a single mesh and a rendering engine, we show that our approach can be adopted to provide continuous user control over several 3D-aware attributes, including time-of-day illumination, bird wing orientation, dollyzoom effect, and object poses. Our method is capable of conditioning image creation with multiple Continuous 3D Words and text descriptions simultaneously while adding no overhead to the generative process. Project Page: <a href=https://ttchengab.github.io/continuous_3d_words>https://ttchengab.github.io/continuous_3d_words</a></p></p class="citation"></blockquote><h3 id=1237--112247-p-mamba-marrying-perona-malik-diffusion-with-mamba-for-efficient-pediatric-echocardiographic-left-ventricular-segmentation-zi-ye-et-al-2024>(12/37 | 112/247) P-Mamba: Marrying Perona Malik Diffusion with Mamba for Efficient Pediatric Echocardiographic Left Ventricular Segmentation (Zi Ye et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zi Ye, Tianxiang Chen. (2024)<br><strong>P-Mamba: Marrying Perona Malik Diffusion with Mamba for Efficient Pediatric Echocardiographic Left Ventricular Segmentation</strong><br><button class=copy-to-clipboard title="P-Mamba: Marrying Perona Malik Diffusion with Mamba for Efficient Pediatric Echocardiographic Left Ventricular Segmentation" index=112>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-112 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Vision Transformer, Transformer, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08506v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08506v1.pdf filename=2402.08506v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In pediatric cardiology, the accurate and immediate assessment of cardiac function through echocardiography is important since it can determine whether urgent intervention is required in many emergencies. However, echocardiography is characterized by ambiguity and heavy background noise interference, bringing more difficulty to accurate segmentation. Present methods lack efficiency and are also prone to mistakenly segmenting some background noise areas as the left ventricular area due to noise disturbance. To relieve the two issues, we introduce P-Mamba for efficient pediatric echocardiographic left ventricular segmentation. Specifically, we turn to the recently proposed <b>vision</b> <b>mamba</b> layers in our <b>vision</b> <b>mamba</b> encoder branch to improve the computing and memory efficiency of our model while modeling global dependencies. In the other DWT-based PMD encoder branch, we devise DWT-based Perona-Malik Diffusion (PMD) Blocks that utilize PMD for noise suppression, while simultaneously preserving the local shape cues of the left ventricle. Leveraging the strengths of both the two encoder branches, P-Mamba achieves superior accuracy and efficiency to established models, such as <b>vision</b> <b>transformers</b> with quadratic and linear computational complexity. This innovative approach promises significant advancements in pediatric cardiac imaging and beyond.</p></p class="citation"></blockquote><h3 id=1337--113247-latent-space-configuration-for-improved-generalization-in-supervised-autoencoder-neural-networks-nikita-gabdullin-2024>(13/37 | 113/247) Latent space configuration for improved generalization in supervised autoencoder neural networks (Nikita Gabdullin, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nikita Gabdullin. (2024)<br><strong>Latent space configuration for improved generalization in supervised autoencoder neural networks</strong><br><button class=copy-to-clipboard title="Latent space configuration for improved generalization in supervised autoencoder neural networks" index=113>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-113 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: 68T45, 62H30, 62H35, I-4, cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Autoencoder, Fine-tuning, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08441v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08441v1.pdf filename=2402.08441v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Autoencoders</b> (AE) are simple yet powerful class of neural networks that compress data by projecting input into low-dimensional latent space (LS). Whereas LS is formed according to the loss function minimization during training, its properties and topology are not controlled directly. In this paper we focus on AE LS properties and propose two methods for obtaining LS with desired topology, called LS configuration. The proposed methods include loss configuration using a geometric loss term that acts directly in LS, and encoder configuration. We show that the former allows to reliably obtain LS with desired configuration by defining the positions and shapes of LS clusters for <b>supervised</b> AE (SAE). Knowing LS configuration allows to define similarity measure in LS to predict labels or estimate similarity for multiple inputs without using decoders or classifiers. We also show that this leads to more stable and interpretable training. We show that SAE trained for clothes texture classification using the proposed method generalizes well to unseen data from LIP, Market1501, and WildTrack datasets without <b>fine-tuning,</b> and even allows to evaluate similarity for unseen classes. We further illustrate the advantages of pre-configured LS similarity estimation with cross-dataset searches and text-based search using a text query without language models.</p></p class="citation"></blockquote><h3 id=1437--114247-conditional-information-gain-trellis-ufuk-can-bicici-et-al-2024>(14/37 | 114/247) Conditional Information Gain Trellis (Ufuk Can Bicici et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ufuk Can Bicici, Tuna Han Salih Meral, Lale Akarun. (2024)<br><strong>Conditional Information Gain Trellis</strong><br><button class=copy-to-clipboard title="Conditional Information Gain Trellis" index=114>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-114 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Convolution, Convolutional Neural Network, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08345v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08345v1.pdf filename=2402.08345v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Conditional computing processes an input using only part of the neural network&rsquo;s computational units. Learning to execute parts of a deep <b>convolutional</b> <b>network</b> <b>by</b> routing individual samples has several advantages: Reducing the computational burden is an obvious advantage. Furthermore, if similar classes are routed to the same path, that part of the network learns to discriminate between finer differences and better classification accuracies can be attained with fewer parameters. Recently, several papers have exploited this idea to take a particular child of a node in a tree-shaped network or to skip parts of a network. In this work, we follow a Trellis-based approach for generating specific execution paths in a deep <b>convolutional</b> <b>neural</b> <b>network.</b> We have designed routing mechanisms that use differentiable information gain-based cost functions to determine which subset of features in a <b>convolutional</b> <b>layer</b> <b>will</b> be executed. We call our method Conditional Information Gain Trellis (CIGT). We show that our conditional execution mechanism achieves comparable or better model performance compared to unconditional baselines, using only a fraction of the computational resources.</p></p class="citation"></blockquote><h3 id=1537--115247-scribble-based-fast-weak-supervision-and-interactive-corrections-for-segmenting-whole-slide-images-antoine-habis-et-al-2024>(15/37 | 115/247) Scribble-based fast weak-supervision and interactive corrections for segmenting whole slide images (Antoine Habis et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Antoine Habis, Roy Rosman Nathanson, Vannary Meas-Yedid, Elsa D. Angelini, Jean-Christophe Olivo-Marin. (2024)<br><strong>Scribble-based fast weak-supervision and interactive corrections for segmenting whole slide images</strong><br><button class=copy-to-clipboard title="Scribble-based fast weak-supervision and interactive corrections for segmenting whole slide images" index=115>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-115 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Supervised Learning, Weakly-supervised Learning, Weakly Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08333v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08333v1.pdf filename=2402.08333v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper proposes a dynamic interactive and weakly <b>supervised</b> segmentation method with minimal user interactions to address two major challenges in the segmentation of whole slide histopathology images. First, the lack of hand-annotated datasets to train algorithms. Second, the lack of interactive paradigms to enable a dialogue between the pathologist and the machine, which can be a major obstacle for use in clinical routine. We therefore propose a fast and user oriented method to bridge this gap by giving the pathologist control over the final result while limiting the number of interactions needed to achieve a good result (over 90% on all our metrics with only 4 correction scribbles).</p></p class="citation"></blockquote><h3 id=1637--116247-a-dense-reward-view-on-aligning-text-to-image-diffusion-with-preference-shentao-yang-et-al-2024>(16/37 | 116/247) A Dense Reward View on Aligning Text-to-Image Diffusion with Preference (Shentao Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shentao Yang, Tianqi Chen, Mingyuan Zhou. (2024)<br><strong>A Dense Reward View on Aligning Text-to-Image Diffusion with Preference</strong><br><button class=copy-to-clipboard title="A Dense Reward View on Aligning Text-to-Image Diffusion with Preference" index=116>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-116 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Bandit Algorithm, Text2image, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08265v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08265v1.pdf filename=2402.08265v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Aligning <b>text-to-image</b> diffusion model (T2I) with preference has been gaining increasing research attention. While prior works exist on directly optimizing T2I by preference data, these methods are developed under the <b>bandit</b> assumption of a latent reward on the entire diffusion reverse chain, while ignoring the sequential nature of the generation process. From literature, this may harm the efficacy and efficiency of alignment. In this paper, we take on a finer dense reward perspective and derive a tractable alignment objective that emphasizes the initial steps of the T2I reverse chain. In particular, we introduce temporal discounting into the DPO-style explicit-reward-free loss, to break the temporal symmetry therein and suit the T2I generation hierarchy. In experiments on single and multiple <b>prompt</b> generation, our method is competitive with strong relevant baselines, both quantitatively and qualitatively. Further studies are conducted to illustrate the insight of our approach.</p></p class="citation"></blockquote><h3 id=1737--117247-fine-tuning-text-to-image-diffusion-models-for-class-wise-spurious-feature-generation-aprilpyone-maungmaung-et-al-2024>(17/37 | 117/247) Fine-Tuning Text-To-Image Diffusion Models for Class-Wise Spurious Feature Generation (AprilPyone MaungMaung et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>AprilPyone MaungMaung, Huy H. Nguyen, Hitoshi Kiya, Isao Echizen. (2024)<br><strong>Fine-Tuning Text-To-Image Diffusion Models for Class-Wise Spurious Feature Generation</strong><br><button class=copy-to-clipboard title="Fine-Tuning Text-To-Image Diffusion Models for Class-Wise Spurious Feature Generation" index=117>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-117 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Fine-tuning, Fine-tuning, Text2image<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08200v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08200v1.pdf filename=2402.08200v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We propose a method for generating spurious features by leveraging large-scale <b>text-to-image</b> diffusion models. Although the previous work detects spurious features in a large-scale dataset like ImageNet and introduces Spurious ImageNet, we found that not all spurious images are spurious across different classifiers. Although spurious images help measure the reliance of a classifier, filtering many images from the Internet to find more spurious features is time-consuming. To this end, we utilize an existing approach of personalizing large-scale <b>text-to-image</b> diffusion models with available discovered spurious images and propose a new spurious feature similarity loss based on neural features of an adversarially robust model. Precisely, we <b>fine-tune</b> Stable Diffusion with several reference images from Spurious ImageNet with a modified objective incorporating the proposed spurious-feature similarity loss. Experiment results show that our method can generate spurious images that are consistently spurious across different classifiers. Moreover, the generated spurious images are visually similar to reference images from Spurious ImageNet.</p></p class="citation"></blockquote><h3 id=1837--118247-ads-approximate-densest-subgraph-for-novel-image-discovery-shanfeng-hu-2024>(18/37 | 118/247) ADS: Approximate Densest Subgraph for Novel Image Discovery (Shanfeng Hu, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shanfeng Hu. (2024)<br><strong>ADS: Approximate Densest Subgraph for Novel Image Discovery</strong><br><button class=copy-to-clipboard title="ADS: Approximate Densest Subgraph for Novel Image Discovery" index=118>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-118 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV, eess-IV<br>Keyword Score: 23<br>Keywords: Graph, Stochastic Gradient Descent, Stochastic Gradient Descent<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08743v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08743v1.pdf filename=2402.08743v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The volume of image repositories continues to grow. Despite the availability of content-based addressing, we still lack a lightweight tool that allows us to discover images of distinct characteristics from a large collection. In this paper, we propose a fast and training-free algorithm for novel image discovery. The key of our algorithm is formulating a collection of images as a perceptual distance-weighted <b>graph,</b> within which our task is to locate the K-densest subgraph that corresponds to a subset of the most unique images. While solving this problem is not just NP-hard but also requires a full computation of the potentially huge distance matrix, we propose to relax it into a K-sparse eigenvector problem that we can efficiently solve using <b>stochastic</b> <b>gradient</b> <b>descent</b> <b>(SGD)</b> without explicitly computing the distance matrix. We compare our algorithm against state-of-the-arts on both synthetic and real datasets, showing that it is considerably faster to run with a smaller memory footprint while able to mine novel images more accurately.</p></p class="citation"></blockquote><h3 id=1937--119247-the-paradox-of-motion-evidence-for-spurious-correlations-in-skeleton-based-gait-recognition-models-andy-cătrună-et-al-2024>(19/37 | 119/247) The Paradox of Motion: Evidence for Spurious Correlations in Skeleton-based Gait Recognition Models (Andy Cătrună et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Andy Cătrună, Adrian Cosma, Emilian Rădoi. (2024)<br><strong>The Paradox of Motion: Evidence for Spurious Correlations in Skeleton-based Gait Recognition Models</strong><br><button class=copy-to-clipboard title="The Paradox of Motion: Evidence for Spurious Correlations in Skeleton-based Gait Recognition Models" index=119>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-119 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 23<br>Keywords: Benchmarking, Transformer, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08320v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08320v1.pdf filename=2402.08320v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Gait, an unobtrusive biometric, is valued for its capability to identify individuals at a distance, across external outfits and environmental conditions. This study challenges the prevailing assumption that vision-based gait recognition, in particular skeleton-based gait recognition, relies primarily on motion patterns, revealing a significant role of the implicit anthropometric information encoded in the walking sequence. We show through a comparative analysis that removing height information leads to notable performance degradation across three models and two <b>benchmarks</b> (CASIA-B and GREW). Furthermore, we propose a spatial <b>transformer</b> model processing individual poses, disregarding any temporal information, which achieves unreasonably good accuracy, emphasizing the bias towards appearance information and indicating spurious correlations in existing <b>benchmarks.</b> These findings underscore the need for a nuanced understanding of the interplay between motion and appearance in vision-based gait recognition, <b>prompting</b> a reevaluation of the methodological assumptions in this field. Our experiments indicate that &ldquo;in-the-wild&rdquo; datasets are less prone to spurious correlations, <b>prompting</b> the need for more diverse and large scale datasets for advancing the field.</p></p class="citation"></blockquote><h3 id=2037--120247-enhancing-robustness-of-indoor-robotic-navigation-with-free-space-segmentation-models-against-adversarial-attacks-qiyuan-an-et-al-2024>(20/37 | 120/247) Enhancing Robustness of Indoor Robotic Navigation with Free-Space Segmentation Models Against Adversarial Attacks (Qiyuan An et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qiyuan An, Christos Sevastopoulos, Fillia Makedon. (2024)<br><strong>Enhancing Robustness of Indoor Robotic Navigation with Free-Space Segmentation Models Against Adversarial Attacks</strong><br><button class=copy-to-clipboard title="Enhancing Robustness of Indoor Robotic Navigation with Free-Space Segmentation Models Against Adversarial Attacks" index=120>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-120 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: 93C85, cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Adversarial Learning, Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08763v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08763v1.pdf filename=2402.08763v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Endeavors in indoor robotic navigation rely on the accuracy of segmentation models to identify free space in RGB images. However, deep learning models are vulnerable to <b>adversarial</b> <b>attacks,</b> posing a significant challenge to their real-world deployment. In this study, we identify vulnerabilities within the hidden layers of neural networks and introduce a practical approach to reinforce traditional <b>adversarial</b> <b>training.</b> Our method incorporates a novel distance loss function, minimizing the gap between hidden layers in clean and <b>adversarial</b> <b>images.</b> Experiments demonstrate satisfactory performance in improving the model&rsquo;s robustness against <b>adversarial</b> <b>perturbations.</b></p></p class="citation"></blockquote><h3 id=2137--121247-latent-inversion-with-timestep-aware-sampling-for-training-free-non-rigid-editing-yunji-jung-et-al-2024>(21/37 | 121/247) Latent Inversion with Timestep-aware Sampling for Training-free Non-rigid Editing (Yunji Jung et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yunji Jung, Seokju Lee, Tair Djanibekov, Hyunjung Shim, Jong Chul Ye. (2024)<br><strong>Latent Inversion with Timestep-aware Sampling for Training-free Non-rigid Editing</strong><br><button class=copy-to-clipboard title="Latent Inversion with Timestep-aware Sampling for Training-free Non-rigid Editing" index=121>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-121 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Fine-tuning, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08601v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08601v2.pdf filename=2402.08601v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Text-guided non-rigid editing involves complex edits for input images, such as changing motion or compositions within their surroundings. Since it requires manipulating the input structure, existing methods often struggle with preserving object identity and background, particularly when combined with Stable Diffusion. In this work, we propose a training-free approach for non-rigid editing with Stable Diffusion, aimed at improving the identity preservation quality without compromising editability. Our approach comprises three stages: text optimization, latent inversion, and timestep-aware text injection sampling. Inspired by the recent success of Imagic, we employ their text optimization for smooth editing. Then, we introduce latent inversion to preserve the input image&rsquo;s identity without additional model <b>fine-tuning.</b> To fully utilize the input reconstruction ability of latent inversion, we suggest timestep-aware text inject sampling. This effectively retains the structure of the input image by injecting the source text <b>prompt</b> in early sampling steps and then transitioning to the target <b>prompt</b> in subsequent sampling steps. This strategic approach seamlessly harmonizes with text optimization, facilitating complex non-rigid edits to the input without losing the original identity. We demonstrate the effectiveness of our method in terms of identity preservation, editability, and aesthetic quality through extensive experiments.</p></p class="citation"></blockquote><h3 id=2237--122247-object-detection-in-thermal-images-using-deep-learning-for-unmanned-aerial-vehicles-minh-dang-tu-et-al-2024>(22/37 | 122/247) Object Detection in Thermal Images Using Deep Learning for Unmanned Aerial Vehicles (Minh Dang Tu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Minh Dang Tu, Kieu Trang Le, Manh Duong Phung. (2024)<br><strong>Object Detection in Thermal Images Using Deep Learning for Unmanned Aerial Vehicles</strong><br><button class=copy-to-clipboard title="Object Detection in Thermal Images Using Deep Learning for Unmanned Aerial Vehicles" index=122>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-122 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 20<br>Keywords: Object Detection, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08251v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08251v1.pdf filename=2402.08251v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This work presents a neural network model capable of recognizing small and tiny <b>objects</b> <b>in</b> thermal images collected by unmanned aerial vehicles. Our model consists of three parts, the backbone, the neck, and the prediction head. The backbone is developed based on the structure of YOLOv5 combined with the use of a <b>transformer</b> encoder at the end. The neck includes a BI-FPN block combined with the use of a sliding window and a <b>transformer</b> to increase the information fed into the prediction head. The prediction head carries out the detection by evaluating feature maps with the Sigmoid function. The use of <b>transformers</b> with attention and sliding windows increases recognition accuracy while keeping the model at a reasonable number of parameters and computation requirements for embedded systems. Experiments conducted on public dataset VEDAI and our collected datasets show that our model has a higher accuracy than state-of-the-art methods such as ResNet, Faster RCNN, ComNet, ViT, YOLOv5, SMPNet, and DPNetV3. Experiments on the embedded computer Jetson AGX show that our model achieves a real-time computation speed with a stability rate of over 90%.</p></p class="citation"></blockquote><h3 id=2337--123247-randumb-a-simple-approach-that-questions-the-efficacy-of-continual-representation-learning-ameya-prabhu-et-al-2024>(23/37 | 123/247) RanDumb: A Simple Approach that Questions the Efficacy of Continual Representation Learning (Ameya Prabhu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ameya Prabhu, Shiven Sinha, Ponnurangam Kumaraguru, Philip H. S. Torr, Ozan Sener, Puneet K. Dokania. (2024)<br><strong>RanDumb: A Simple Approach that Questions the Efficacy of Continual Representation Learning</strong><br><button class=copy-to-clipboard title="RanDumb: A Simple Approach that Questions the Efficacy of Continual Representation Learning" index=123>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-123 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 18<br>Keywords: Benchmarking, Continual Learning, Representation Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08823v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08823v1.pdf filename=2402.08823v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We propose RanDumb to examine the efficacy of <b>continual</b> <b>representation</b> <b>learning.</b> RanDumb embeds raw pixels using a fixed random transform which approximates an RBF-Kernel, initialized before seeing any data, and learns a simple linear classifier on top. We present a surprising and consistent finding: RanDumb significantly outperforms the continually learned <b>representations</b> <b>using</b> deep networks across numerous <b>continual</b> <b>learning</b> <b>benchmarks,</b> demonstrating the poor performance of <b>representation</b> <b>learning</b> in these scenarios. RanDumb stores no exemplars and performs a single pass over the data, processing one sample at a time. It complements GDumb, operating in a low-exemplar regime where GDumb has especially poor performance. We reach the same consistent conclusions when RanDumb is extended to scenarios with pretrained models replacing the random transform with pretrained feature extractor. Our investigation is both surprising and alarming as it questions our understanding of how to effectively design and train models that require efficient <b>continual</b> <b>representation</b> <b>learning,</b> and necessitates a principled reinvestigation of the widely explored problem formulation itself. Our code is available at <a href=https://github.com/drimpossible/RanDumb>https://github.com/drimpossible/RanDumb</a>.</p></p class="citation"></blockquote><h3 id=2437--124247-one-to-many-reconstruction-of-3d-geometry-of-cultural-artifacts-using-a-synthetically-trained-generative-model-thomas-pöllabauer-et-al-2024>(24/37 | 124/247) One-to-many Reconstruction of 3D Geometry of cultural Artifacts using a synthetically trained Generative Model (Thomas Pöllabauer et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Thomas Pöllabauer, Julius Kühn, Jiayi Li, Arjan Kuijper. (2024)<br><strong>One-to-many Reconstruction of 3D Geometry of cultural Artifacts using a synthetically trained Generative Model</strong><br><button class=copy-to-clipboard title="One-to-many Reconstruction of 3D Geometry of cultural Artifacts using a synthetically trained Generative Model" index=124>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-124 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 18<br>Keywords: Geometry, Multi-modal, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08310v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08310v1.pdf filename=2402.08310v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Estimating the 3D shape of an object using a single image is a difficult problem. Modern approaches achieve good results for general objects, based on real photographs, but worse results on less expressive representations such as historic sketches. Our automated approach generates a variety of detailed 3D representation from a single sketch, depicting a medieval statue, and can be guided by <b>multi-modal</b> inputs, such as text <b>prompts.</b> It relies solely on synthetic data for training, making it adoptable even in cases of only small numbers of training examples. Our solution allows domain experts such as a curators to interactively reconstruct potential appearances of lost artifacts.</p></p class="citation"></blockquote><h3 id=2537--125247-bdslw60-a-word-level-bangla-sign-language-dataset-husne-ara-rubaiyeat-et-al-2024>(25/37 | 125/247) BdSLW60: A Word-Level Bangla Sign Language Dataset (Husne Ara Rubaiyeat et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Husne Ara Rubaiyeat, Hasan Mahmud, Ahsan Habib, Md. Kamrul Hasan. (2024)<br><strong>BdSLW60: A Word-Level Bangla Sign Language Dataset</strong><br><button class=copy-to-clipboard title="BdSLW60: A Word-Level Bangla Sign Language Dataset" index=125>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-125 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 16<br>Keywords: Benchmarking, Benchmarking, Quantization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08635v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08635v1.pdf filename=2402.08635v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Sign language discourse is an essential mode of daily communication for the deaf and hard-of-hearing people. However, research on Bangla Sign Language (BdSL) faces notable limitations, primarily due to the lack of datasets. Recognizing wordlevel signs in BdSL (WL-BdSL) presents a multitude of challenges, including the need for well-annotated datasets, capturing the dynamic nature of sign gestures from facial or hand landmarks, developing suitable machine learning or deep learning-based models with substantial video samples, and so on. In this paper, we address these challenges by creating a comprehensive BdSL word-level dataset named BdSLW60 in an unconstrained and natural setting, allowing positional and temporal variations and allowing sign users to change hand dominance freely. The dataset encompasses 60 Bangla sign words, with a significant scale of 9307 video trials provided by 18 signers under the supervision of a sign language professional. The dataset was rigorously annotated and cross-checked by 60 annotators. We also introduced a unique approach of a relative <b>quantization-based</b> key frame encoding technique for landmark based sign gesture recognition. We report the <b>benchmarking</b> of our BdSLW60 dataset using the Support Vector Machine (SVM) with testing accuracy up to 67.6% and an attention-based bi-LSTM with testing accuracy up to 75.1%. The dataset is available at <a href=https://www.kaggle.com/datasets/hasaniut/bdslw60>https://www.kaggle.com/datasets/hasaniut/bdslw60</a> and the code base is accessible from <a href=https://github.com/hasanssl/BdSLW60_Code>https://github.com/hasanssl/BdSLW60_Code</a>.</p></p class="citation"></blockquote><h3 id=2637--126247-camera-calibration-through-geometric-constraints-from-rotation-and-projection-matrices-muhammad-waleed-et-al-2024>(26/37 | 126/247) Camera Calibration through Geometric Constraints from Rotation and Projection Matrices (Muhammad Waleed et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Muhammad Waleed, Abdul Rauf, Murtaza Taj. (2024)<br><strong>Camera Calibration through Geometric Constraints from Rotation and Projection Matrices</strong><br><button class=copy-to-clipboard title="Camera Calibration through Geometric Constraints from Rotation and Projection Matrices" index=126>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-126 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 13<br>Keywords: Benchmarking, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08437v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08437v1.pdf filename=2402.08437v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The process of camera calibration involves estimating the intrinsic and extrinsic parameters, which are essential for accurately performing tasks such as 3D reconstruction, object tracking and augmented reality. In this work, we propose a novel constraints-based loss for measuring the intrinsic (focal length: $(f_x, f_y)$ and principal point: $(p_x, p_y)$) and extrinsic (baseline: ($b$), disparity: ($d$), translation: $(t_x, t_y, t_z)$, and rotation specifically pitch: $(\theta_p)$) camera parameters. Our novel constraints are based on geometric properties inherent in the camera model, including the anatomy of the projection matrix (vanishing points, image of world origin, axis planes) and the orthonormality of the rotation matrix. Thus we proposed a novel <b>Unsupervised</b> Geometric Constraint Loss (UGCL) via a multitask learning framework. Our methodology is a hybrid approach that employs the learning power of a neural network to estimate the desired parameters along with the underlying mathematical properties inherent in the camera projection matrix. This distinctive approach not only enhances the interpretability of the model but also facilitates a more informed learning process. Additionally, we introduce a new CVGL Camera Calibration dataset, featuring over 900 configurations of camera parameters, incorporating 63,600 image pairs that closely mirror real-world conditions. By training and testing on both synthetic and real-world datasets, our proposed approach demonstrates improvements across all parameters when compared to the state-of-the-art (SOTA) <b>benchmarks.</b> The code and the updated dataset can be found here: <a href=https://github.com/CVLABLUMS/CVGL-Camera-Calibration>https://github.com/CVLABLUMS/CVGL-Camera-Calibration</a></p></p class="citation"></blockquote><h3 id=2737--127247-seprep-net-multi-source-free-domain-adaptation-via-model-separation-and-reparameterization-ying-jin-et-al-2024>(27/37 | 127/247) SepRep-Net: Multi-source Free Domain Adaptation via Model Separation And Reparameterization (Ying Jin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ying Jin, Jiaqi Wang, Dahua Lin. (2024)<br><strong>SepRep-Net: Multi-source Free Domain Adaptation via Model Separation And Reparameterization</strong><br><button class=copy-to-clipboard title="SepRep-Net: Multi-source Free Domain Adaptation via Model Separation And Reparameterization" index=127>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-127 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 13<br>Keywords: Benchmarking, Domain Adaptation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08249v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08249v1.pdf filename=2402.08249v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We consider multi-source free <b>domain</b> <b>adaptation,</b> the problem of adapting multiple existing models to a new <b>domain</b> <b>without</b> accessing the source data. Among existing approaches, methods based on model ensemble are effective in both the source and target <b>domains,</b> <b>but</b> incur significantly increased computational costs. Towards this dilemma, in this work, we propose a novel framework called SepRep-Net, which tackles multi-source free <b>domain</b> <b>adaptation</b> via model Separation and Reparameterization.Concretely, SepRep-Net reassembled multiple existing models to a unified network, while maintaining separate pathways (Separation). During training, separate pathways are optimized in parallel with the information exchange regularly performed via an additional feature merging unit. With our specific design, these pathways can be further reparameterized into a single one to facilitate inference (Reparameterization). SepRep-Net is characterized by 1) effectiveness: competitive performance on the target <b>domain,</b> <b>2)</b> efficiency: low computational costs, and 3) generalizability: maintaining more source knowledge than existing solutions. As a general approach, SepRep-Net can be seamlessly plugged into various methods. Extensive experiments validate the performance of SepRep-Net on mainstream <b>benchmarks.</b></p></p class="citation"></blockquote><h3 id=2837--128247-optimized-information-flow-for-transformer-tracking-janani-kugarajeevan-et-al-2024>(28/37 | 128/247) Optimized Information Flow for Transformer Tracking (Janani Kugarajeevan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Janani Kugarajeevan, Thanikasalam Kokul, Amirthalingam Ramanan, Subha Fernando. (2024)<br><strong>Optimized Information Flow for Transformer Tracking</strong><br><button class=copy-to-clipboard title="Optimized Information Flow for Transformer Tracking" index=128>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-128 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 13<br>Keywords: Benchmarking, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08195v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08195v1.pdf filename=2402.08195v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>One-stream <b>Transformer</b> trackers have shown outstanding performance in challenging <b>benchmark</b> datasets over the last three years, as they enable interaction between the target template and search region tokens to extract target-oriented features with mutual guidance. Previous approaches allow free bidirectional information flow between template and search tokens without investigating their influence on the tracker&rsquo;s discriminative capability. In this study, we conducted a detailed study on the information flow of the tokens and based on the findings, we propose a novel Optimized Information Flow Tracking (OIFTrack) framework to enhance the discriminative capability of the tracker. The proposed OIFTrack blocks the interaction from all search tokens to target template tokens in early encoder layers, as the large number of non-target tokens in the search region diminishes the importance of target-specific features. In the deeper encoder layers of the proposed tracker, search tokens are partitioned into target search tokens and non-target search tokens, allowing bidirectional flow from target search tokens to template tokens to capture the appearance changes of the target. In addition, since the proposed tracker incorporates dynamic background cues, distractor objects are successfully avoided by capturing the surrounding information of the target. The OIFTrack demonstrated outstanding performance in challenging <b>benchmarks,</b> particularly excelling in the one-shot tracking <b>benchmark</b> GOT-10k, achieving an average overlap of 74.6%. The code, models, and results of this work are available at \url{https://github.com/JananiKugaa/OIFTrack}</p></p class="citation"></blockquote><h3 id=2937--129247-fess-loss-feature-enhanced-spatial-segmentation-loss-for-optimizing-medical-image-analysis-charulkumar-chodvadiya-et-al-2024>(29/37 | 129/247) FESS Loss: Feature-Enhanced Spatial Segmentation Loss for Optimizing Medical Image Analysis (Charulkumar Chodvadiya et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Charulkumar Chodvadiya, Navyansh Mahla, Kinshuk Gaurav Singh, Kshitij Sharad Jadhav. (2024)<br><strong>FESS Loss: Feature-Enhanced Spatial Segmentation Loss for Optimizing Medical Image Analysis</strong><br><button class=copy-to-clipboard title="FESS Loss: Feature-Enhanced Spatial Segmentation Loss for Optimizing Medical Image Analysis" index=129>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-129 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Contrastive Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08582v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08582v1.pdf filename=2402.08582v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Medical image segmentation is a critical process in the field of medical imaging, playing a pivotal role in diagnosis, treatment, and research. It involves partitioning of an image into multiple regions, representing distinct anatomical or pathological structures. Conventional methods often grapple with the challenge of balancing spatial precision and comprehensive feature representation due to their reliance on traditional loss functions. To overcome this, we propose Feature-Enhanced Spatial Segmentation Loss (FESS Loss), that integrates the benefits of <b>contrastive</b> <b>learning</b> (which extracts intricate features, particularly in the nuanced domain of medical imaging) with the spatial accuracy inherent in the Dice loss. The objective is to augment both spatial precision and feature-based representation in the segmentation of medical images. FESS Loss signifies a notable advancement, offering a more accurate and refined segmentation process, ultimately contributing to heightened precision in the analysis of medical images. Further, FESS loss demonstrates superior performance in limited annotated data availability scenarios often present in the medical domain.</p></p class="citation"></blockquote><h3 id=3037--130247-an-order-complexity-aesthetic-assessment-model-for-aesthetic-aware-music-recommendation-xin-jin-et-al-2024>(30/37 | 130/247) An Order-Complexity Aesthetic Assessment Model for Aesthetic-aware Music Recommendation (Xin Jin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xin Jin, Wu Zhou, Jingyu Wang, Duo Xu, Yongsen Zheng. (2024)<br><strong>An Order-Complexity Aesthetic Assessment Model for Aesthetic-aware Music Recommendation</strong><br><button class=copy-to-clipboard title="An Order-Complexity Aesthetic Assessment Model for Aesthetic-aware Music Recommendation" index=130>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-130 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08300v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08300v1.pdf filename=2402.08300v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Computational aesthetic evaluation has made remarkable contribution to visual art works, but its application to music is still rare. Currently, subjective evaluation is still the most effective form of evaluating artistic works. However, subjective evaluation of artistic works will consume a lot of human and material resources. The popular AI generated content (AIGC) tasks nowadays have flooded all industries, and music is no exception. While compared to music produced by humans, AI generated music still sounds mechanical, monotonous, and lacks aesthetic appeal. Due to the lack of music datasets with rating annotations, we have to choose traditional aesthetic equations to objectively measure the beauty of music. In order to improve the quality of AI music generation and further guide computer music production, synthesis, <b>recommendation</b> and other tasks, we use Birkhoff&rsquo;s aesthetic measure to design a aesthetic model, objectively measuring the aesthetic beauty of music, and form a <b>recommendation</b> list according to the aesthetic feeling of music. Experiments show that our objective aesthetic model and <b>recommendation</b> method are effective.</p></p class="citation"></blockquote><h3 id=3137--131247-improving-image-coding-for-machines-through-optimizing-encoder-via-auxiliary-loss-kei-iino-et-al-2024>(31/37 | 131/247) Improving Image Coding for Machines through Optimizing Encoder via Auxiliary Loss (Kei Iino et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kei Iino, Shunsuke Akamatsu, Hiroshi Watanabe, Shohei Enomoto, Akira Sakamoto, Takeharu Eda. (2024)<br><strong>Improving Image Coding for Machines through Optimizing Encoder via Auxiliary Loss</strong><br><button class=copy-to-clipboard title="Improving Image Coding for Machines through Optimizing Encoder via Auxiliary Loss" index=131>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-131 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Object Detection<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08267v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08267v1.pdf filename=2402.08267v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Image coding for machines (ICM) aims to compress images for machine analysis using recognition models rather than human vision. Hence, in ICM, it is important for the encoder to recognize and compress the information necessary for the machine recognition task. There are two main approaches in learned ICM; optimization of the compression model based on task loss, and Region of Interest (ROI) based bit allocation. These approaches provide the encoder with the recognition capability. However, optimization with task loss becomes difficult when the recognition model is deep, and ROI-based methods often involve extra overhead during evaluation. In this study, we propose a novel training method for learned ICM models that applies auxiliary loss to the encoder to improve its recognition capability and rate-distortion performance. Our method achieves Bjontegaard Delta rate improvements of 27.7% and 20.3% in <b>object</b> <b>detection</b> and semantic segmentation tasks, compared to the conventional training method.</p></p class="citation"></blockquote><h3 id=3237--132247-translating-images-to-road-networka-non-autoregressive-sequence-to-sequence-approach-jiachen-lu-et-al-2024>(32/37 | 132/247) Translating Images to Road Network:A Non-Autoregressive Sequence-to-Sequence Approach (Jiachen Lu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiachen Lu, Renyuan Peng, Xinyue Cai, Hang Xu, Hongyang Li, Feng Wen, Wei Zhang, Li Zhang. (2024)<br><strong>Translating Images to Road Network:A Non-Autoregressive Sequence-to-Sequence Approach</strong><br><button class=copy-to-clipboard title="Translating Images to Road Network:A Non-Autoregressive Sequence-to-Sequence Approach" index=132>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-132 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08207v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08207v1.pdf filename=2402.08207v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The extraction of road network is essential for the generation of high-definition maps since it enables the precise localization of road landmarks and their interconnections. However, generating road network poses a significant challenge due to the conflicting underlying combination of Euclidean (e.g., road landmarks location) and non-Euclidean (e.g., road topological connectivity) structures. Existing methods struggle to merge the two types of data domains effectively, but few of them address it properly. Instead, our work establishes a unified representation of both types of data domain by projecting both Euclidean and non-Euclidean data into an integer series called RoadNet Sequence. Further than modeling an auto-regressive sequence-to-sequence <b>Transformer</b> model to understand RoadNet Sequence, we decouple the dependency of RoadNet Sequence into a mixture of auto-regressive and non-autoregressive dependency. Building on this, our proposed non-autoregressive sequence-to-sequence approach leverages non-autoregressive dependencies while fixing the gap towards auto-regressive dependencies, resulting in success on both efficiency and accuracy. Extensive experiments on nuScenes dataset demonstrate the superiority of RoadNet Sequence representation and the non-autoregressive approach compared to existing state-of-the-art alternatives. The code is open-source on <a href=https://github.com/fudan-zvg/RoadNetworkTRansformer>https://github.com/fudan-zvg/RoadNetworkTRansformer</a>.</p></p class="citation"></blockquote><h3 id=3337--133247-amend-a-mixture-of-experts-framework-for-long-tailed-trajectory-prediction-ray-coden-mercurius-et-al-2024>(33/37 | 133/247) AMEND: A Mixture of Experts Framework for Long-tailed Trajectory Prediction (Ray Coden Mercurius et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ray Coden Mercurius, Ehsan Ahmadi, Soheil Mohamad Alizadeh Shabestary, Amir Rasouli. (2024)<br><strong>AMEND: A Mixture of Experts Framework for Long-tailed Trajectory Prediction</strong><br><button class=copy-to-clipboard title="AMEND: A Mixture of Experts Framework for Long-tailed Trajectory Prediction" index=133>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-133 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs-RO, cs.CV<br>Keyword Score: 10<br>Keywords: Contrastive Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08698v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08698v1.pdf filename=2402.08698v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Accurate prediction of pedestrians&rsquo; future motions is critical for intelligent driving systems. Developing models for this task requires rich datasets containing diverse sets of samples. However, the existing naturalistic trajectory prediction datasets are generally imbalanced in favor of simpler samples and lack challenging scenarios. Such a long-tail effect causes prediction models to underperform on the tail portion of the data distribution containing safety-critical scenarios. Previous methods tackle the long-tail problem using methods such as <b>contrastive</b> <b>learning</b> and class-conditioned hypernetworks. These approaches, however, are not modular and cannot be applied to many machine learning architectures. In this work, we propose a modular model-agnostic framework for trajectory prediction that leverages a specialized mixture of experts. In our approach, each expert is trained with a specialized skill with respect to a particular part of the data. To produce predictions, we utilise a router network that selects the best expert by generating relative confidence scores. We conduct experimentation on common pedestrian trajectory prediction datasets and show that besides achieving state-of-the-art performance, our method significantly performs better on long-tail scenarios. We further conduct ablation studies to highlight the contribution of different proposed components.</p></p class="citation"></blockquote><h3 id=3437--134247-preconditioners-for-the-stochastic-training-of-implicit-neural-representations-shin-fang-chng-et-al-2024>(34/37 | 134/247) Preconditioners for the Stochastic Training of Implicit Neural Representations (Shin-Fang Chng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shin-Fang Chng, Hemanth Saratchandran, Simon Lucey. (2024)<br><strong>Preconditioners for the Stochastic Training of Implicit Neural Representations</strong><br><button class=copy-to-clipboard title="Preconditioners for the Stochastic Training of Implicit Neural Representations" index=134>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-134 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08784v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08784v1.pdf filename=2402.08784v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Implicit neural representations have emerged as a powerful technique for encoding complex continuous multidimensional signals as neural networks, enabling a wide range of applications in computer vision, robotics, and <b>geometry.</b> While Adam is commonly used for training due to its stochastic proficiency, it entails lengthy training durations. To address this, we explore alternative optimization techniques for accelerated training without sacrificing accuracy. Traditional second-order optimizers like L-BFGS are suboptimal in stochastic settings, making them unsuitable for large-scale data sets. Instead, we propose stochastic training using curvature-aware diagonal preconditioners, showcasing their effectiveness across various signal modalities such as images, shape reconstruction, and Neural Radiance Fields (NeRF).</p></p class="citation"></blockquote><h3 id=3537--135247-nerf-analogies-example-based-visual-attribute-transfer-for-nerfs-michael-fischer-et-al-2024>(35/37 | 135/247) NeRF Analogies: Example-Based Visual Attribute Transfer for NeRFs (Michael Fischer et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Michael Fischer, Zhengqin Li, Thu Nguyen-Phuoc, Aljaz Bozic, Zhao Dong, Carl Marshall, Tobias Ritschel. (2024)<br><strong>NeRF Analogies: Example-Based Visual Attribute Transfer for NeRFs</strong><br><button class=copy-to-clipboard title="NeRF Analogies: Example-Based Visual Attribute Transfer for NeRFs" index=135>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-135 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-GR, cs.CV<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08622v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08622v1.pdf filename=2402.08622v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A Neural Radiance Field (NeRF) encodes the specific relation of 3D <b>geometry</b> and appearance of a scene. We here ask the question whether we can transfer the appearance from a source NeRF onto a target 3D <b>geometry</b> in a semantically meaningful way, such that the resulting new NeRF retains the target <b>geometry</b> but has an appearance that is an analogy to the source NeRF. To this end, we generalize classic image analogies from 2D images to NeRFs. We leverage correspondence transfer along semantic affinity that is driven by semantic features from large, pre-trained 2D image models to achieve multi-view consistent appearance transfer. Our method allows exploring the mix-and-match product space of 3D <b>geometry</b> and appearance. We show that our method outperforms traditional stylization-based methods and that a large majority of users prefer our method over several typical baselines.</p></p class="citation"></blockquote><h3 id=3637--136247-learning-to-produce-semi-dense-correspondences-for-visual-localization-khang-truong-giang-et-al-2024>(36/37 | 136/247) Learning to Produce Semi-dense Correspondences for Visual Localization (Khang Truong Giang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Khang Truong Giang, Soohwan Song, Sungho Jo. (2024)<br><strong>Learning to Produce Semi-dense Correspondences for Visual Localization</strong><br><button class=copy-to-clipboard title="Learning to Produce Semi-dense Correspondences for Visual Localization" index=136>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-136 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08359v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08359v1.pdf filename=2402.08359v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This study addresses the challenge of performing visual localization in demanding conditions such as night-time scenarios, adverse weather, and seasonal changes. While many prior studies have focused on improving image-matching performance to facilitate reliable dense keypoint matching between images, existing methods often heavily rely on predefined feature points on a reconstructed 3D model. Consequently, they tend to overlook unobserved keypoints during the matching process. Therefore, dense keypoint matches are not fully exploited, leading to a notable reduction in accuracy, particularly in noisy scenes. To tackle this issue, we propose a novel localization method that extracts reliable semi-dense 2D-3D matching points based on dense keypoint matches. This approach involves regressing semi-dense 2D keypoints into 3D scene coordinates using a point inference network. The network utilizes both geometric and visual cues to effectively infer 3D coordinates for unobserved keypoints from the observed ones. The abundance of matching information significantly enhances the accuracy of camera pose estimation, even in scenarios involving noisy or sparse 3D models. Comprehensive evaluations demonstrate that the proposed method outperforms other methods in challenging scenes and achieves competitive results in large-scale visual localization <b>benchmarks.</b> The code will be available.</p></p class="citation"></blockquote><h3 id=3737--137247-crossgaze-a-strong-method-for-3d-gaze-estimation-in-the-wild-andy-cătrună-et-al-2024>(37/37 | 137/247) CrossGaze: A Strong Method for 3D Gaze Estimation in the Wild (Andy Cătrună et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Andy Cătrună, Adrian Cosma, Emilian Rădoi. (2024)<br><strong>CrossGaze: A Strong Method for 3D Gaze Estimation in the Wild</strong><br><button class=copy-to-clipboard title="CrossGaze: A Strong Method for 3D Gaze Estimation in the Wild" index=137>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-137 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08316v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08316v1.pdf filename=2402.08316v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Gaze estimation, the task of predicting where an individual is looking, is a critical task with direct applications in areas such as human-computer interaction and virtual reality. Estimating the direction of looking in unconstrained environments is difficult, due to the many factors that can obscure the face and eye regions. In this work we propose CrossGaze, a strong baseline for gaze estimation, that leverages recent developments in computer vision architectures and attention-based modules. Unlike previous approaches, our method does not require a specialised architecture, utilizing already established models that we integrate in our architecture and adapt for the task of 3D gaze estimation. This approach allows for seamless updates to the architecture as any module can be replaced with more powerful feature extractors. On the Gaze360 <b>benchmark,</b> our model surpasses several state-of-the-art methods, achieving a mean angular error of 9.94 degrees. Our proposed model serves as a strong foundation for future research and development in gaze estimation, paving the way for practical and accurate gaze prediction in real-world scenarios.</p></p class="citation"></blockquote><h2 id=csai-27>cs.AI (27)</h2><h3 id=127--138247-combining-insights-from-multiple-large-language-models-improves-diagnostic-accuracy-gioele-barabucci-et-al-2024>(1/27 | 138/247) Combining Insights From Multiple Large Language Models Improves Diagnostic Accuracy (Gioele Barabucci et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Gioele Barabucci, Victor Shia, Eugene Chu, Benjamin Harack, Nathan Fu. (2024)<br><strong>Combining Insights From Multiple Large Language Models Improves Diagnostic Accuracy</strong><br><button class=copy-to-clipboard title="Combining Insights From Multiple Large Language Models Improves Diagnostic Accuracy" index=138>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-138 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: I-2-1; J-3, cs-AI, cs.AI<br>Keyword Score: 70<br>Keywords: Cohere, GPT, GPT-4, LLaMA, PaLM, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08806v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08806v1.pdf filename=2402.08806v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Background: <b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> such as OpenAI&rsquo;s <b>GPT-4</b> or Google&rsquo;s <b>PaLM</b> 2 are proposed as viable diagnostic support tools or even spoken of as replacements for &ldquo;curbside consults&rdquo;. However, even <b>LLMs</b> specifically trained on medical topics may lack sufficient diagnostic accuracy for real-life applications. Methods: Using collective intelligence methods and a dataset of 200 clinical vignettes of real-life cases, we assessed and compared the accuracy of differential diagnoses obtained by asking individual commercial <b>LLMs</b> (OpenAI <b>GPT-4,</b> Google <b>PaLM</b> 2, <b>Cohere</b> Command, Meta <b>Llama</b> 2) against the accuracy of differential diagnoses synthesized by aggregating responses from combinations of the same <b>LLMs.</b> Results: We find that aggregating responses from multiple, various <b>LLMs</b> leads to more accurate differential diagnoses (average accuracy for 3 <b>LLMs:</b> $75.3%\pm 1.6pp$) compared to the differential diagnoses produced by single <b>LLMs</b> (average accuracy for single <b>LLMs:</b> $59.0%\pm 6.1pp$). Discussion: The use of collective intelligence methods to synthesize differential diagnoses combining the responses of different <b>LLMs</b> achieves two of the necessary steps towards advancing acceptance of <b>LLMs</b> as a diagnostic support tool: (1) demonstrate high diagnostic accuracy and (2) eliminate dependence on a single commercial vendor.</p></p class="citation"></blockquote><h3 id=227--139247-rec-gpt4v-multimodal-recommendation-with-large-vision-language-models-yuqing-liu-et-al-2024>(2/27 | 139/247) Rec-GPT4V: Multimodal Recommendation with Large Vision-Language Models (Yuqing Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuqing Liu, Yu Wang, Lichao Sun, Philip S. Yu. (2024)<br><strong>Rec-GPT4V: Multimodal Recommendation with Large Vision-Language Models</strong><br><button class=copy-to-clipboard title="Rec-GPT4V: Multimodal Recommendation with Large Vision-Language Models" index=139>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-139 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 66<br>Keywords: Multi-modal, Multi-modal, Recommendation, GPT-4, Reasoning, In-context Learning, Prompt, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08670v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08670v1.pdf filename=2402.08670v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The development of large <b>vision-language</b> models (LVLMs) offers the potential to address challenges faced by traditional <b>multimodal</b> <b>recommendations</b> thanks to their proficient understanding of static images and textual dynamics. However, the application of LVLMs in this field is still limited due to the following complexities: First, LVLMs lack user preference knowledge as they are trained from vast general datasets. Second, LVLMs suffer setbacks in addressing multiple image dynamics in scenarios involving discrete, noisy, and redundant image sequences. To overcome these issues, we propose the novel <b>reasoning</b> scheme named Rec-GPT4V: Visual-Summary Thought (VST) of leveraging large <b>vision-language</b> models for <b>multimodal</b> <b>recommendation.</b> We utilize user history as <b>in-context</b> user preferences to address the first challenge. Next, we <b>prompt</b> LVLMs to generate item image summaries and utilize image comprehension in natural language space combined with item titles to query the user preferences over candidate items. We conduct comprehensive experiments across four datasets with three LVLMs: <b>GPT4-V,</b> LLaVa-7b, and LLaVa-13b. The numerical results indicate the efficacy of VST.</p></p class="citation"></blockquote><h3 id=327--140247-llm-driven-imitation-of-subrational-behavior--illusion-or-reality-andrea-coletta-et-al-2024>(3/27 | 140/247) LLM-driven Imitation of Subrational Behavior : Illusion or Reality? (Andrea Coletta et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Andrea Coletta, Kshama Dwarakanath, Penghang Liu, Svitlana Vyetrenko, Tucker Balch. (2024)<br><strong>LLM-driven Imitation of Subrational Behavior : Illusion or Reality?</strong><br><button class=copy-to-clipboard title="LLM-driven Imitation of Subrational Behavior : Illusion or Reality?" index=140>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-140 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI, econ-GN, q-fin-EC<br>Keyword Score: 60<br>Keywords: Reinforcement Learning, Simulation, Simulator, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08755v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08755v1.pdf filename=2402.08755v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Modeling subrational agents, such as humans or economic households, is inherently challenging due to the difficulty in calibrating <b>reinforcement</b> <b>learning</b> models or collecting data that involves human subjects. Existing work highlights the ability of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> to address complex <b>reasoning</b> tasks and mimic human communication, while <b>simulation</b> using <b>LLMs</b> as agents shows emergent social behaviors, potentially improving our comprehension of human conduct. In this paper, we propose to investigate the use of <b>LLMs</b> to generate synthetic human demonstrations, which are then used to learn subrational agent policies though Imitation Learning. We make an assumption that <b>LLMs</b> can be used as implicit computational models of humans, and propose a framework to use synthetic demonstrations derived from <b>LLMs</b> to model subrational behaviors that are characteristic of humans (e.g., myopic behavior or preference for risk aversion). We experimentally evaluate the ability of our framework to model sub-rationality through four simple scenarios, including the well-researched ultimatum game and marshmallow experiment. To gain confidence in our framework, we are able to replicate well-established findings from prior human studies associated with the above scenarios. We conclude by discussing the potential benefits, challenges and limitations of our framework.</p></p class="citation"></blockquote><h3 id=427--141247-large-language-models-for-the-automated-analysis-of-optimization-algorithms-camilo-chacón-sartori-et-al-2024>(4/27 | 141/247) Large Language Models for the Automated Analysis of Optimization Algorithms (Camilo Chacón Sartori et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Camilo Chacón Sartori, Christian Blum, Gabriela Ochoa. (2024)<br><strong>Large Language Models for the Automated Analysis of Optimization Algorithms</strong><br><button class=copy-to-clipboard title="Large Language Models for the Automated Analysis of Optimization Algorithms" index=141>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-141 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 40<br>Keywords: GPT, GPT-4, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08472v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08472v1.pdf filename=2402.08472v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The ability of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> to generate high-quality text and code has fuelled their rise in popularity. In this paper, we aim to demonstrate the potential of <b>LLMs</b> within the realm of optimization algorithms by integrating them into STNWeb. This is a web-based tool for the generation of Search Trajectory Networks (STNs), which are visualizations of optimization algorithm behavior. Although visualizations produced by STNWeb can be very informative for algorithm designers, they often require a certain level of prior knowledge to be interpreted. In an attempt to bridge this knowledge gap, we have incorporated <b>LLMs,</b> specifically <b>GPT-4,</b> into STNWeb to produce extensive written reports, complemented by automatically generated plots, thereby enhancing the user experience and reducing the barriers to the adoption of this tool by the research community. Moreover, our approach can be expanded to other tools from the optimization community, showcasing the versatility and potential of <b>LLMs</b> in this field.</p></p class="citation"></blockquote><h3 id=527--142247-lota-bench-benchmarking-language-oriented-task-planners-for-embodied-agents-jae-woo-choi-et-al-2024>(5/27 | 142/247) LoTa-Bench: Benchmarking Language-oriented Task Planners for Embodied Agents (Jae-Woo Choi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jae-Woo Choi, Youngwoo Yoon, Hyobin Ong, Jaehong Kim, Minsu Jang. (2024)<br><strong>LoTa-Bench: Benchmarking Language-oriented Task Planners for Embodied Agents</strong><br><button class=copy-to-clipboard title="LoTa-Bench: Benchmarking Language-oriented Task Planners for Embodied Agents" index=142>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-142 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 36<br>Keywords: Benchmarking, Benchmarking, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08178v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08178v1.pdf filename=2402.08178v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have recently received considerable attention as alternative solutions for task planning. However, comparing the performance of language-oriented task planners becomes difficult, and there exists a dearth of detailed exploration regarding the effects of various factors such as pre-trained model selection and <b>prompt</b> construction. To address this, we propose a <b>benchmark</b> system for automatically quantifying performance of task planning for home-service embodied agents. Task planners are tested on two pairs of datasets and simulators: 1) ALFRED and AI2-THOR, 2) an extension of Watch-And-Help and VirtualHome. Using the proposed <b>benchmark</b> system, we perform extensive experiments with <b>LLMs</b> and <b>prompts,</b> and explore several enhancements of the baseline planner. We expect that the proposed <b>benchmark</b> tool would accelerate the development of language-oriented task planners.</p></p class="citation"></blockquote><h3 id=627--143247-one-shot-imitation-in-a-non-stationary-environment-via-multi-modal-skill-sangwoo-shin-et-al-2024>(6/27 | 143/247) One-shot Imitation in a Non-Stationary Environment via Multi-Modal Skill (Sangwoo Shin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sangwoo Shin, Daehee Lee, Minjong Yoo, Woo Kyung Kim, Honguk Woo. (2024)<br><strong>One-shot Imitation in a Non-Stationary Environment via Multi-Modal Skill</strong><br><button class=copy-to-clipboard title="One-shot Imitation in a Non-Stationary Environment via Multi-Modal Skill" index=143>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-143 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 33<br>Keywords: Meta Learning, Multi-modal, Zero-shot, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08369v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08369v1.pdf filename=2402.08369v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>One-shot imitation is to learn a new task from a single demonstration, yet it is a challenging problem to adopt it for complex tasks with the high domain diversity inherent in a non-stationary environment. To tackle the problem, we explore the compositionality of complex tasks, and present a novel skill-based imitation learning framework enabling one-shot imitation and <b>zero-shot</b> adaptation; from a single demonstration for a complex unseen task, a semantic skill sequence is inferred and then each skill in the sequence is converted into an action sequence optimized for environmental hidden dynamics that can vary over time. Specifically, we leverage a <b>vision-language</b> model to learn a semantic skill set from offline video datasets, where each skill is represented on the <b>vision-language</b> embedding space, and adapt <b>meta-learning</b> <b>with</b> dynamics inference to enable <b>zero-shot</b> skill adaptation. We evaluate our framework with various one-shot imitation scenarios for extended multi-stage <b>Meta-world</b> <b>tasks,</b> showing its superiority in learning complex tasks, generalizing to dynamics changes, and extending to different demonstration conditions and modalities, compared to other baselines.</p></p class="citation"></blockquote><h3 id=727--144247-tandem-transformers-for-inference-efficient-llms-aishwarya-p-s-et-al-2024>(7/27 | 144/247) Tandem Transformers for Inference Efficient LLMs (Aishwarya P S et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Aishwarya P S, Pranav Ajit Nair, Yashas Samaga, Toby Boyd, Sanjiv Kumar, Prateek Jain, Praneeth Netrapalli. (2024)<br><strong>Tandem Transformers for Inference Efficient LLMs</strong><br><button class=copy-to-clipboard title="Tandem Transformers for Inference Efficient LLMs" index=144>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-144 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CL, cs.AI<br>Keyword Score: 30<br>Keywords: Transformer, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08644v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08644v1.pdf filename=2402.08644v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The autoregressive nature of conventional <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> inherently limits inference speed, as tokens are generated sequentially. While speculative and parallel decoding techniques attempt to mitigate this, they face limitations: either relying on less accurate smaller models for generation or failing to fully leverage the base <b>LLM&rsquo;s</b> representations. We introduce a novel architecture, Tandem <b>transformers,</b> to address these issues. This architecture uniquely combines (1) a small autoregressive model and (2) a <b>large</b> <b>model</b> <b>operating</b> in block mode (processing multiple tokens simultaneously). The small model&rsquo;s predictive accuracy is substantially enhanced by granting it attention to the <b>large</b> <b>model&rsquo;s</b> <b>richer</b> representations. On the PaLM2 pretraining dataset, a tandem of PaLM2-Bison and PaLM2-Gecko demonstrates a 3.3% improvement in next-token prediction accuracy over a standalone PaLM2-Gecko, offering a 1.16x speedup compared to a PaLM2-Otter model with comparable downstream performance. We further incorporate the tandem model within the speculative decoding (SPEED) framework where the <b>large</b> <b>model</b> <b>validates</b> tokens from the small model. This ensures that the Tandem of PaLM2-Bison and PaLM2-Gecko achieves substantial speedup (around 1.14x faster than using vanilla PaLM2-Gecko in SPEED) while maintaining identical downstream task accuracy.</p></p class="citation"></blockquote><h3 id=827--145247-the-application-of-chatgpt-in-responding-to-questions-related-to-the-boston-bowel-preparation-scale-xiaoqiang-liu-et-al-2024>(8/27 | 145/247) The Application of ChatGPT in Responding to Questions Related to the Boston Bowel Preparation Scale (Xiaoqiang Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiaoqiang Liu, Yubin Wang, Zicheng Huang, Boming Xu, Yilin Zeng, Xinqi Chen, Zilong Wang, Enning Yang, Xiaoxuan Lei, Yisen Huang, Xiaobo Liu. (2024)<br><strong>The Application of ChatGPT in Responding to Questions Related to the Boston Bowel Preparation Scale</strong><br><button class=copy-to-clipboard title="The Application of ChatGPT in Responding to Questions Related to the Boston Bowel Preparation Scale" index=145>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-145 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 30<br>Keywords: Fine-tuning, ChatGPT, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08492v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08492v1.pdf filename=2402.08492v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Background: Colonoscopy, a crucial diagnostic tool in gastroenterology, depends heavily on superior bowel preparation. <b>ChatGPT,</b> a <b>large</b> <b>language</b> <b>model</b> with emergent intelligence which also exhibits potential in medical applications. This study aims to assess the accuracy and consistency of <b>ChatGPT</b> in using the Boston Bowel Preparation Scale (BBPS) for colonoscopy assessment. Methods: We retrospectively collected 233 colonoscopy images from 2020 to 2023. These images were evaluated using the BBPS by 3 senior endoscopists and 3 novice endoscopists. Additionally, <b>ChatGPT</b> also assessed these images, having been divided into three groups and undergone specific <b>Fine-tuning.</b> Consistency was evaluated through two rounds of testing. Results: In the initial round, <b>ChatGPT&rsquo;s</b> accuracy varied between 48.93% and 62.66%, trailing the endoscopists&rsquo; accuracy of 76.68% to 77.83%. Kappa values for <b>ChatGPT</b> was between 0.52 and 0.53, compared to 0.75 to 0.87 for the endoscopists. Conclusion: While <b>ChatGPT</b> shows promise in bowel preparation scoring, it currently does not match the accuracy and consistency of experienced endoscopists. Future research should focus on in-depth <b>Fine-tuning.</b></p></p class="citation"></blockquote><h3 id=927--146247-transformer-mechanisms-mimic-frontostriatal-gating-operations-when-trained-on-human-working-memory-tasks-aaron-traylor-et-al-2024>(9/27 | 146/247) Transformer Mechanisms Mimic Frontostriatal Gating Operations When Trained on Human Working Memory Tasks (Aaron Traylor et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Aaron Traylor, Jack Merullo, Michael J. Frank, Ellie Pavlick. (2024)<br><strong>Transformer Mechanisms Mimic Frontostriatal Gating Operations When Trained on Human Working Memory Tasks</strong><br><button class=copy-to-clipboard title="Transformer Mechanisms Mimic Frontostriatal Gating Operations When Trained on Human Working Memory Tasks" index=146>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-146 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: I-2-6, cs-AI, cs.AI<br>Keyword Score: 30<br>Keywords: Graph Attention Networks, Transformer, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08211v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08211v1.pdf filename=2402.08211v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Models based on the <b>Transformer</b> neural network architecture have seen success on a wide variety of tasks that appear to require complex &ldquo;cognitive branching&rdquo; &ndash; or the ability to maintain pursuit of one goal while accomplishing others. In cognitive neuroscience, success on such tasks is thought to rely on sophisticated frontostriatal mechanisms for selective \textit{gating}, which enable role-addressable updating &ndash; and later readout &ndash; of information to and from distinct &ldquo;addresses&rdquo; of memory, in the form of clusters of neurons. However, <b>Transformer</b> models have no such mechanisms intentionally built-in. It is thus an open question how <b>Transformers</b> solve such tasks, and whether the mechanisms that emerge to help them to do so bear any resemblance to the <b>gating</b> mechanisms in the human brain. In this work, we analyze the mechanisms that emerge within a vanilla attention-only <b>Transformer</b> trained on a simple sequence modeling task inspired by a task explicitly designed to study working memory <b>gating</b> in computational cognitive neuroscience. We find that, as a result of training, the <b>self-attention</b> mechanism within the <b>Transformer</b> specializes in a way that mirrors the input and output <b>gating</b> mechanisms which were explicitly incorporated into earlier, more biologically-inspired architectures. These results suggest opportunities for future research on computational similarities between modern AI architectures and models of the human brain.</p></p class="citation"></blockquote><h3 id=1027--147247-enabling-multi-agent-transfer-reinforcement-learning-via-scenario-independent-representation-ayesha-siddika-nipu-et-al-2024>(10/27 | 147/247) Enabling Multi-Agent Transfer Reinforcement Learning via Scenario Independent Representation (Ayesha Siddika Nipu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ayesha Siddika Nipu, Siming Liu, Anthony Harris. (2024)<br><strong>Enabling Multi-Agent Transfer Reinforcement Learning via Scenario Independent Representation</strong><br><button class=copy-to-clipboard title="Enabling Multi-Agent Transfer Reinforcement Learning via Scenario Independent Representation" index=147>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-147 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-LG, cs.AI<br>Keyword Score: 30<br>Keywords: Knowledge Transfer, Reinforcement Learning, Transfer Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08184v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08184v1.pdf filename=2402.08184v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Multi-Agent <b>Reinforcement</b> <b>Learning</b> (MARL) algorithms are widely adopted in tackling complex tasks that require collaboration and competition among agents in dynamic Multi-Agent Systems (MAS). However, learning such tasks from scratch is arduous and may not always be feasible, particularly for MASs with a large number of interactive agents due to the extensive sample complexity. Therefore, reusing <b>knowledge</b> <b>gained</b> from past experiences or other agents could efficiently accelerate the learning process and upscale MARL algorithms. In this study, we introduce a novel framework that enables <b>transfer</b> <b>learning</b> for MARL through unifying various state spaces into fixed-size inputs that allow one unified deep-learning policy viable in different scenarios within a MAS. We evaluated our approach in a range of scenarios within the StarCraft Multi-Agent Challenge (SMAC) environment, and the findings show significant enhancements in multi-agent learning performance using maneuvering skills learned from other scenarios compared to agents learning from scratch. Furthermore, we adopted Curriculum <b>Transfer</b> <b>Learning</b> (CTL), enabling our deep learning policy to progressively acquire <b>knowledge</b> <b>and</b> skills across pre-designed homogeneous learning scenarios organized by difficulty levels. This process promotes inter- and intra-agent <b>knowledge</b> <b>transfer,</b> <b>leading</b> to high multi-agent learning performance in more complicated heterogeneous scenarios.</p></p class="citation"></blockquote><h3 id=1127--148247-counterfactual-influence-in-markov-decision-processes-milad-kazemi-et-al-2024>(11/27 | 148/247) Counterfactual Influence in Markov Decision Processes (Milad Kazemi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Milad Kazemi, Jessica Lally, Ekaterina Tishchenko, Hana Chockler, Nicola Paoletti. (2024)<br><strong>Counterfactual Influence in Markov Decision Processes</strong><br><button class=copy-to-clipboard title="Counterfactual Influence in Markov Decision Processes" index=148>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-148 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 20<br>Keywords: Counter-factual, Markov Decision Process<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08514v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08514v1.pdf filename=2402.08514v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Our work addresses a fundamental problem in the context of <b>counterfactual</b> inference for Markov Decision Processes <b>(MDPs).</b> Given an MDP path $\tau$, this kind of inference allows us to derive <b>counterfactual</b> paths $\tau&rsquo;$ describing what-if versions of $\tau$ obtained under different action sequences than those observed in $\tau$. However, as the <b>counterfactual</b> states and actions deviate from the observed ones over time, the observation $\tau$ may no longer influence the <b>counterfactual</b> world, meaning that the analysis is no longer tailored to the individual observation, resulting in interventional outcomes rather than <b>counterfactual</b> ones. Even though this issue specifically affects the popular Gumbel-max structural causal model used for MDP <b>counterfactuals,</b> it has remained overlooked until now. In this work, we introduce a formal characterisation of influence based on comparing <b>counterfactual</b> and interventional distributions. We devise an algorithm to construct <b>counterfactual</b> models that automatically satisfy influence constraints. Leveraging such models, we derive <b>counterfactual</b> policies that are not just optimal for a given reward structure but also remain tailored to the observed path. Even though there is an unavoidable trade-off between policy optimality and strength of influence constraints, our experiments demonstrate that it is possible to derive (near-)optimal policies while remaining under the influence of the observation.</p></p class="citation"></blockquote><h3 id=1227--149247-pix2code-learning-to-compose-neural-visual-concepts-as-programs-antonia-wüst-et-al-2024>(12/27 | 149/247) Pix2Code: Learning to Compose Neural Visual Concepts as Programs (Antonia Wüst et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Antonia Wüst, Wolfgang Stammer, Quentin Delfosse, Devendra Singh Dhami, Kristian Kersting. (2024)<br><strong>Pix2Code: Learning to Compose Neural Visual Concepts as Programs</strong><br><button class=copy-to-clipboard title="Pix2Code: Learning to Compose Neural Visual Concepts as Programs" index=149>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-149 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CV, cs-LG, cs.AI<br>Keyword Score: 20<br>Keywords: Unsupervised Learning, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08280v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08280v1.pdf filename=2402.08280v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The challenge in learning abstract concepts from images in an <b>unsupervised</b> fashion lies in the required integration of visual perception and generalizable relational <b>reasoning.</b> Moreover, the <b>unsupervised</b> nature of this task makes it necessary for human users to be able to understand a model&rsquo;s learnt concepts and potentially revise false behaviours. To tackle both the generalizability and interpretability constraints of visual concept learning, we propose Pix2Code, a framework that extends program synthesis to visual relational <b>reasoning</b> by utilizing the abilities of both explicit, compositional symbolic and implicit neural representations. This is achieved by retrieving object representations from images and synthesizing relational concepts as lambda-calculus programs. We evaluate the diverse properties of Pix2Code on the challenging <b>reasoning</b> domains, Kandinsky Patterns and CURI, thereby testing its ability to identify compositional visual concepts that generalize to novel data and concept configurations. Particularly, in stark contrast to neural approaches, we show that Pix2Code&rsquo;s representations remain human interpretable and can be easily revised for improved performance.</p></p class="citation"></blockquote><h3 id=1327--150247-bert4fca-a-method-for-bipartite-link-prediction-using-formal-concept-analysis-and-bert-siqi-peng-et-al-2024>(13/27 | 150/247) BERT4FCA: A Method for Bipartite Link Prediction using Formal Concept Analysis and BERT (Siqi Peng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Siqi Peng, Hongyuan Yang, Akihiro Yamamoto. (2024)<br><strong>BERT4FCA: A Method for Bipartite Link Prediction using Formal Concept Analysis and BERT</strong><br><button class=copy-to-clipboard title="BERT4FCA: A Method for Bipartite Link Prediction using Formal Concept Analysis and BERT" index=150>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-150 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 20<br>Keywords: Recommendation, BERT<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08236v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08236v1.pdf filename=2402.08236v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We propose BERT4FCA, a novel method for link prediction in bipartite networks, using formal concept analysis (FCA) and <b>BERT.</b> Link prediction in bipartite networks is an important task that can solve various practical problems like friend <b>recommendation</b> in social networks and co-authorship prediction in author-paper networks. Recent research has found that in bipartite networks, maximal bi-cliques provide important information for link prediction, and they can be extracted by FCA. Some FCA-based bipartite link prediction methods have achieved good performance. However, we figured out that their performance could be further improved because these methods did not fully capture the rich information of the extracted maximal bi-cliques. To address this limitation, we propose an approach using <b>BERT,</b> which can learn more information from the maximal bi-cliques extracted by FCA and use them to make link prediction. We conduct experiments on three real-world bipartite networks and demonstrate that our method outperforms previous FCA-based methods, and some classic methods such as matrix-factorization and node2vec.</p></p class="citation"></blockquote><h3 id=1427--151247-vehicle-behavior-prediction-by-episodic-memory-implanted-ndt-peining-shen-et-al-2024>(14/27 | 151/247) Vehicle Behavior Prediction by Episodic-Memory Implanted NDT (Peining Shen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Peining Shen, Jianwu Fang, Hongkai Yu, Jianru Xue. (2024)<br><strong>Vehicle Behavior Prediction by Episodic-Memory Implanted NDT</strong><br><button class=copy-to-clipboard title="Vehicle Behavior Prediction by Episodic-Memory Implanted NDT" index=151>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-151 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 18<br>Keywords: Black Box, Clustering, Text Embedding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08423v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08423v1.pdf filename=2402.08423v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In autonomous driving, predicting the behavior (turning left, stopping, etc.) of target vehicles is crucial for the self-driving vehicle to make safe decisions and avoid accidents. Existing deep learning-based methods have shown excellent and accurate performance, but the <b>black-box</b> <b>nature</b> makes it untrustworthy to apply them in practical use. In this work, we explore the interpretability of behavior prediction of target vehicles by an Episodic Memory implanted Neural Decision Tree (abbrev. eMem-NDT). The structure of eMem-NDT is constructed by hierarchically <b>clustering</b> the <b>text</b> <b>embedding</b> of vehicle behavior descriptions. eMem-NDT is a neural-backed part of a pre-trained deep learning model by changing the soft-max layer of the deep model to eMem-NDT, for grouping and aligning the memory prototypes of the historical vehicle behavior features in training data on a neural decision tree. Each leaf node of eMem-NDT is modeled by a neural network for aligning the behavior memory prototypes. By eMem-NDT, we infer each instance in behavior prediction of vehicles by bottom-up Memory Prototype Matching (MPM) (searching the appropriate leaf node and the links to the root node) and top-down Leaf Link Aggregation (LLA) (obtaining the probability of future behaviors of vehicles for certain instances). We validate eMem-NDT on BLVD and LOKI datasets, and the results show that our model can obtain a superior performance to other methods with clear explainability. The code is available at <a href=https://github.com/JWFangit/eMem-NDT>https://github.com/JWFangit/eMem-NDT</a>.</p></p class="citation"></blockquote><h3 id=1527--152247-a-logical-approach-to-criminal-case-investigation-takanori-ugai-et-al-2024>(15/27 | 152/247) A Logical Approach to Criminal Case Investigation (Takanori Ugai et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Takanori Ugai, Yusuke Koyanagi, Fumihito Nishino. (2024)<br><strong>A Logical Approach to Criminal Case Investigation</strong><br><button class=copy-to-clipboard title="A Logical Approach to Criminal Case Investigation" index=152>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-152 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 18<br>Keywords: Graph, Knowledge Graph, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08284v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08284v1.pdf filename=2402.08284v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>XAI (eXplanable AI) techniques that have the property of explaining the reasons for their conclusions, i.e. explainability or interpretability, are attracting attention. XAI is expected to be used in the development of forensic science and the justice system. In today&rsquo;s forensic and criminal investigation environment, experts face many challenges due to large amounts of data, small pieces of evidence in a chaotic and complex environment, traditional laboratory structures and sometimes inadequate <b>knowledge.</b> <b>All</b> these can lead to failed investigations and miscarriages of justice. In this paper, we describe the application of one logical approach to crime scene investigation. The subject of the application is <code>The Adventure of the Speckled Band'' from the Sherlock Holmes short stories. The applied data is the &lt;b>knowledge&lt;/b> &lt;b>graph&lt;/b> created for the &lt;b>Knowledge&lt;/b> &lt;b>Graph&lt;/b> &lt;b>Reasoning&lt;/b> Challenge. We tried to find the murderer by inferring each person with the motive, opportunity, and method. We created an ontology of motives and methods of murder from dictionaries and dictionaries, added it to the &lt;b>knowledge&lt;/b> &lt;b>graph&lt;/b> of </code>The Adventure of the Speckled Band&rsquo;&rsquo;, and applied scripts to determine motives, opportunities, and methods.</p></p class="citation"></blockquote><h3 id=1627--153247-advancing-data-driven-weather-forecasting-time-sliding-data-augmentation-of-era5-minjong-cheon-et-al-2024>(16/27 | 153/247) Advancing Data-driven Weather Forecasting: Time-Sliding Data Augmentation of ERA5 (Minjong Cheon et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Minjong Cheon, Daehyun Kang, Yo-Hwan Choi, Seon-Yu Kang. (2024)<br><strong>Advancing Data-driven Weather Forecasting: Time-Sliding Data Augmentation of ERA5</strong><br><button class=copy-to-clipboard title="Advancing Data-driven Weather Forecasting: Time-Sliding Data Augmentation of ERA5" index=153>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-153 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CV, cs.AI, physics-ao-ph<br>Keyword Score: 13<br>Keywords: Data Augmentation, Sample Size<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08185v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08185v1.pdf filename=2402.08185v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Modern deep learning techniques, which mimic traditional numerical weather prediction (NWP) models and are derived from global atmospheric reanalysis <b>data,</b> <b>have</b> caused a significant revolution within a few years. In this new paradigm, our research introduces a novel strategy that deviates from the common dependence on high-resolution <b>data,</b> <b>which</b> is often constrained by computational resources, and instead utilizes low-resolution <b>data</b> <b>(2.5</b> degrees) for global weather prediction and climate <b>data</b> <b>analysis.</b> Our main focus is evaluating <b>data-driven</b> <b>weather</b> prediction (DDWP) frameworks, specifically addressing <b>sample</b> <b>size</b> adequacy, structural improvements to the model, and the ability of climate <b>data</b> <b>to</b> represent current climatic trends. By using the Adaptive Fourier Neural Operator (AFNO) model via FourCastNet and a proposed time-sliding method to inflate the dataset of the ECMWF Reanalysis v5 (ERA5), this paper improves on conventional approaches by adding more variables and a novel approach to <b>data</b> <b>augmentation</b> and processing. Our findings reveal that despite the lower resolution, the proposed approach demonstrates considerable accuracy in predicting atmospheric conditions, effectively rivaling higher-resolution models. Furthermore, the study confirms the model&rsquo;s proficiency in reflecting current climate trends and its potential in predicting future climatic events, underscoring its utility in climate change strategies. This research marks a pivotal step in the realm of meteorological forecasting, showcasing the feasibility of lower-resolution <b>data</b> <b>in</b> producing reliable predictions and opening avenues for more accessible and inclusive climate modeling. The insights gleaned from this study not only contribute to the advancement of climate science but also lay the groundwork for future innovations in the field.</p></p class="citation"></blockquote><h3 id=1727--154247-epistemic-exploration-for-generalizable-planning-and-learning-in-non-stationary-settings-rushang-karia-et-al-2024>(17/27 | 154/247) Epistemic Exploration for Generalizable Planning and Learning in Non-Stationary Settings (Rushang Karia et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rushang Karia, Pulkit Verma, Alberto Speranzon, Siddharth Srivastava. (2024)<br><strong>Epistemic Exploration for Generalizable Planning and Learning in Non-Stationary Settings</strong><br><button class=copy-to-clipboard title="Epistemic Exploration for Generalizable Planning and Learning in Non-Stationary Settings" index=154>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-154 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 13<br>Keywords: Benchmarking, Probabilistic Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08145v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08145v1.pdf filename=2402.08145v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper introduces a new approach for continual planning and model learning in non-stationary stochastic environments expressed using relational representations. Such capabilities are essential for the deployment of sequential decision-making systems in the uncertain, constantly evolving real world. Working in such practical settings with unknown (and non-stationary) transition systems and changing tasks, the proposed framework models gaps in the agent&rsquo;s current state of knowledge and uses them to conduct focused, investigative explorations. Data collected using these explorations is used for learning generalizable <b>probabilistic</b> <b>models</b> for solving the current task despite continual changes in the environment dynamics. Empirical evaluations on several <b>benchmark</b> domains show that this approach significantly outperforms planning and RL baselines in terms of sample complexity in non-stationary settings. Theoretical results show that the system reverts to exhibit desirable convergence properties when stationarity holds.</p></p class="citation"></blockquote><h3 id=1827--155247-optimal-task-assignment-and-path-planning-using-conflict-based-search-with-precedence-and-temporal-constraints-yu-quan-chong-et-al-2024>(18/27 | 155/247) Optimal Task Assignment and Path Planning using Conflict-Based Search with Precedence and Temporal Constraints (Yu Quan Chong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yu Quan Chong, Jiaoyang Li, Katia Sycara. (2024)<br><strong>Optimal Task Assignment and Path Planning using Conflict-Based Search with Precedence and Temporal Constraints</strong><br><button class=copy-to-clipboard title="Optimal Task Assignment and Path Planning using Conflict-Based Search with Precedence and Temporal Constraints" index=155>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-155 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: I-2-11, cs-AI, cs-MA, cs.AI<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08772v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08772v1.pdf filename=2402.08772v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The Multi-Agent Path Finding (MAPF) problem entails finding collision-free paths for a set of agents, guiding them from their start to goal locations. However, MAPF does not account for several practical task-related constraints. For example, agents may need to perform actions at goal locations with specific execution times, adhering to predetermined orders and timeframes. Moreover, goal assignments may not be predefined for agents, and the optimization objective may lack an explicit definition. To incorporate task assignment, path planning, and a user-defined objective into a coherent framework, this paper examines the Task Assignment and Path Finding with Precedence and Temporal Constraints (TAPF-PTC) problem. We augment Conflict-Based Search (CBS) to simultaneously generate task assignments and collision-free paths that adhere to precedence and temporal constraints, maximizing an objective quantified by the return from a user-defined reward function in <b>reinforcement</b> <b>learning</b> (RL). Experimentally, we demonstrate that our algorithm, CBS-TA-PTC, can solve highly challenging bomb-defusing tasks with precedence and temporal constraints efficiently relative to MARL and adapted Target Assignment and Path Finding (TAPF) methods.</p></p class="citation"></blockquote><h3 id=1927--156247-inference-of-abstraction-for-a-unified-account-of-symbolic-reasoning-from-data-hiroyuki-kido-2024>(19/27 | 156/247) Inference of Abstraction for a Unified Account of Symbolic Reasoning from Data (Hiroyuki Kido, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hiroyuki Kido. (2024)<br><strong>Inference of Abstraction for a Unified Account of Symbolic Reasoning from Data</strong><br><button class=copy-to-clipboard title="Inference of Abstraction for a Unified Account of Symbolic Reasoning from Data" index=156>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-156 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 10<br>Keywords: Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08646v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08646v1.pdf filename=2402.08646v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Inspired by empirical work in neuroscience for Bayesian approaches to brain function, we give a unified probabilistic account of various types of symbolic <b>reasoning</b> from data. We characterise them in terms of formal logic using the classical consequence relation, an empirical consequence relation, maximal consistent sets, maximal possible sets and maximum likelihood estimation. The theory gives new insights into <b>reasoning</b> towards human-like machine intelligence.</p></p class="citation"></blockquote><h3 id=2027--157247-artificial-intelligence-for-literature-reviews-opportunities-and-challenges-francisco-bolanos-et-al-2024>(20/27 | 157/247) Artificial Intelligence for Literature Reviews: Opportunities and Challenges (Francisco Bolanos et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Francisco Bolanos, Angelo Salatino, Francesco Osborne, Enrico Motta. (2024)<br><strong>Artificial Intelligence for Literature Reviews: Opportunities and Challenges</strong><br><button class=copy-to-clipboard title="Artificial Intelligence for Literature Reviews: Opportunities and Challenges" index=157>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-157 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-HC, cs-IR, cs.AI<br>Keyword Score: 10<br>Keywords: Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08565v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08565v1.pdf filename=2402.08565v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This manuscript presents a comprehensive review of the use of Artificial Intelligence (AI) in Systematic Literature Reviews (SLRs). A SLR is a rigorous and organised methodology that assesses and integrates previous research on a given topic. Numerous tools have been developed to assist and partially automate the SLR process. The increasing role of AI in this field shows great potential in providing more effective support for researchers, moving towards the semi-automatic creation of literature reviews. Our study focuses on how AI techniques are applied in the semi-automation of SLRs, specifically in the screening and extraction phases. We examine 21 leading SLR tools using a framework that combines 23 traditional features with 11 AI features. We also analyse 11 recent tools that leverage <b>large</b> <b>language</b> <b>models</b> for searching the literature and assisting academic writing. Finally, the paper discusses current trends in the field, outlines key research challenges, and suggests directions for future research.</p></p class="citation"></blockquote><h3 id=2127--158247-taking-training-seriously-human-guidance-and-management-based-regulation-of-artificial-intelligence-cary-coglianese-et-al-2024>(21/27 | 158/247) Taking Training Seriously: Human Guidance and Management-Based Regulation of Artificial Intelligence (Cary Coglianese et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Cary Coglianese, Colton R. Crum. (2024)<br><strong>Taking Training Seriously: Human Guidance and Management-Based Regulation of Artificial Intelligence</strong><br><button class=copy-to-clipboard title="Taking Training Seriously: Human Guidance and Management-Based Regulation of Artificial Intelligence" index=158>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-158 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CV, cs.AI<br>Keyword Score: 10<br>Keywords: Fairness<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08466v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08466v1.pdf filename=2402.08466v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Fervent calls for more robust governance of the harms associated with artificial intelligence (AI) are leading to the adoption around the world of what regulatory scholars have called a management-based approach to regulation. Recent initiatives in the United States and Europe, as well as the adoption of major self-regulatory standards by the International Organization for Standardization, share in common a core management-based paradigm. These management-based initiatives seek to motivate an increase in human oversight of how AI tools are trained and developed. Refinements and systematization of human-guided training techniques will thus be needed to fit within this emerging era of management-based regulatory paradigm. If taken seriously, human-guided training can alleviate some of the technical and ethical pressures on AI, boosting AI performance with human intuition as well as better addressing the needs for <b>fairness</b> and effective explainability. In this paper, we discuss the connection between the emerging management-based regulatory frameworks governing AI and the need for human oversight during training. We broadly cover some of the technical components involved in human-guided training and then argue that the kinds of high-stakes use cases for AI that appear of most concern to regulators should lean more on human-guided training than on data-only training. We hope to foster a discussion between legal scholars and computer scientists involving how to govern a domain of technology that is vast, heterogenous, and dynamic in its applications and risks.</p></p class="citation"></blockquote><h3 id=2227--159247-a-survey-of-recent-methods-for-addressing-ai-fairness-and-bias-in-biomedicine-yifan-yang-et-al-2024>(22/27 | 159/247) A survey of recent methods for addressing AI fairness and bias in biomedicine (Yifan Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yifan Yang, Mingquan Lin, Han Zhao, Yifan Peng, Furong Huang, Zhiyong Lu. (2024)<br><strong>A survey of recent methods for addressing AI fairness and bias in biomedicine</strong><br><button class=copy-to-clipboard title="A survey of recent methods for addressing AI fairness and bias in biomedicine" index=159>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-159 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 10<br>Keywords: Fairness<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08250v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08250v1.pdf filename=2402.08250v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Artificial intelligence (AI) systems have the potential to revolutionize clinical practices, including improving diagnostic accuracy and surgical decision-making, while also reducing costs and manpower. However, it is important to recognize that these systems may perpetuate social inequities or demonstrate biases, such as those based on race or gender. Such biases can occur before, during, or after the development of AI models, making it critical to understand and address potential biases to enable the accurate and reliable application of AI models in clinical settings. To mitigate bias concerns during model development, we surveyed recent publications on different debiasing methods in the fields of biomedical natural language processing (NLP) or computer vision (CV). Then we discussed the methods that have been applied in the biomedical domain to address bias. We performed our literature search on PubMed, ACM digital library, and IEEE Xplore of relevant articles published between January 2018 and December 2023 using multiple combinations of keywords. We then filtered the result of 10,041 articles automatically with loose constraints, and manually inspected the abstracts of the remaining 890 articles to identify the 55 articles included in this review. Additional articles in the references are also included in this review. We discuss each method and compare its strengths and weaknesses. Finally, we review other potential methods from the general domain that could be applied to biomedicine to address bias and improve <b>fairness.The</b> bias of AIs in biomedicine can originate from multiple sources. Existing debiasing methods that focus on algorithms can be categorized into distributional or algorithmic.</p></p class="citation"></blockquote><h3 id=2327--160247-towards-equitable-agile-research-and-development-of-ai-and-robotics-andrew-hundt-et-al-2024>(23/27 | 160/247) Towards Equitable Agile Research and Development of AI and Robotics (Andrew Hundt et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Andrew Hundt, Julia Schuller, Severin Kacianka. (2024)<br><strong>Towards Equitable Agile Research and Development of AI and Robotics</strong><br><button class=copy-to-clipboard title="Towards Equitable Agile Research and Development of AI and Robotics" index=160>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-160 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CY, cs-LG, cs-RO, cs-SE, cs.AI<br>Keyword Score: 10<br>Keywords: Fairness<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08242v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08242v1.pdf filename=2402.08242v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Machine Learning (ML) and &lsquo;Artificial Intelligence&rsquo; (&lsquo;AI&rsquo;) methods tend to replicate and amplify existing biases and prejudices, as do Robots with AI. For example, robots with facial recognition have failed to identify Black Women as human, while others have categorized people, such as Black Men, as criminals based on appearance alone. A &lsquo;culture of modularity&rsquo; means harms are perceived as &lsquo;out of scope&rsquo;, or someone else&rsquo;s responsibility, throughout employment positions in the &lsquo;AI supply chain&rsquo;. Incidents are routine enough (incidentdatabase.ai lists over 2000 examples) to indicate that few organizations are capable of completely respecting peoples&rsquo; rights; meeting claimed equity, diversity, and inclusion (EDI or DEI) goals; or recognizing and then addressing such failures in their organizations and artifacts. We propose a framework for adapting widely practiced Research and Development (R&amp;D) project management methodologies to build organizational equity capabilities and better integrate known evidence-based best practices. We describe how project teams can organize and operationalize the most promising practices, skill sets, organizational cultures, and methods to detect and address rights-based <b>fairness,</b> equity, accountability, and ethical problems as early as possible when they are often less harmful and easier to mitigate; then monitor for unforeseen incidents to adaptively and constructively address them. Our primary example adapts an Agile development process based on Scrum, one of the most widely adopted approaches to organizing R&amp;D teams. We also discuss limitations of our proposed framework and future research directions.</p></p class="citation"></blockquote><h3 id=2427--161247-hierarchical-position-embedding-of-graphs-with-landmarks-and-clustering-for-link-prediction-minsang-kim-et-al-2024>(24/27 | 161/247) Hierarchical Position Embedding of Graphs with Landmarks and Clustering for Link Prediction (Minsang Kim et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Minsang Kim, Seungjun Baek. (2024)<br><strong>Hierarchical Position Embedding of Graphs with Landmarks and Clustering for Link Prediction</strong><br><button class=copy-to-clipboard title="Hierarchical Position Embedding of Graphs with Landmarks and Clustering for Link Prediction" index=161>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-161 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-LG, cs.AI<br>Keyword Score: 6<br>Keywords: Graph, Clustering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08174v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08174v1.pdf filename=2402.08174v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Learning positional information of nodes in a <b>graph</b> is important for link prediction tasks. We propose a representation of positional information using representative nodes called landmarks. A small number of nodes with high degree centrality are selected as landmarks, which serve as reference points for the nodes&rsquo; positions. We justify this selection strategy for well-known random <b>graph</b> models and derive closed-form bounds on the average path lengths involving landmarks. In a model for power-law <b>graphs,</b> we prove that landmarks provide asymptotically exact information on inter-node distances. We apply theoretical insights to practical networks and propose Hierarchical Position embedding with Landmarks and <b>Clustering</b> (HPLC). HPLC combines landmark selection and <b>graph</b> <b>clustering,</b> where the <b>graph</b> is partitioned into densely connected clusters in which nodes with the highest degree are selected as landmarks. HPLC leverages the positional information of nodes based on landmarks at various levels of hierarchy such as nodes&rsquo; distances to landmarks, inter-landmark distances and hierarchical grouping of clusters. Experiments show that HPLC achieves state-of-the-art performances of link prediction on various datasets in terms of HIT@K, MRR, and AUC. The code is available at \url{https://github.com/kmswin1/HPLC}.</p></p class="citation"></blockquote><h3 id=2527--162247-geometry-induced-implicit-regularization-in-deep-relu-neural-networks-joachim-bona-pellissier-et-al-2024>(25/27 | 162/247) Geometry-induced Implicit Regularization in Deep ReLU Neural Networks (Joachim Bona-Pellissier et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Joachim Bona-Pellissier, Fran çois Malgouyres, Fran çois Bachoc. (2024)<br><strong>Geometry-induced Implicit Regularization in Deep ReLU Neural Networks</strong><br><button class=copy-to-clipboard title="Geometry-induced Implicit Regularization in Deep ReLU Neural Networks" index=162>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-162 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-LG, cs-NE, cs.AI, math-OC, math-ST, stat-TH<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08269v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08269v1.pdf filename=2402.08269v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>It is well known that neural networks with many more parameters than training examples do not overfit. Implicit regularization phenomena, which are still not well understood, occur during optimization and &lsquo;good&rsquo; networks are favored. Thus the number of parameters is not an adequate measure of complexity if we do not consider all possible networks but only the &lsquo;good&rsquo; ones. To better understand which networks are favored during optimization, we study the <b>geometry</b> of the output set as parameters vary. When the inputs are fixed, we prove that the dimension of this set changes and that the local dimension, called batch functional dimension, is almost surely determined by the activation patterns in the hidden layers. We prove that the batch functional dimension is invariant to the symmetries of the network parameterization: neuron permutations and positive rescalings. Empirically, we establish that the batch functional dimension decreases during optimization. As a consequence, optimization leads to parameters with low batch functional dimensions. We call this phenomenon <b>geometry-induced</b> implicit regularization.The batch functional dimension depends on both the network parameters and inputs. To understand the impact of the inputs, we study, for fixed parameters, the largest attainable batch functional dimension when the inputs vary. We prove that this quantity, called computable full functional dimension, is also invariant to the symmetries of the network&rsquo;s parameterization, and is determined by the achievable activation patterns. We also provide a sampling theorem, showing a fast convergence of the estimation of the computable full functional dimension for a random input of increasing size. Empirically we find that the computable full functional dimension remains close to the number of parameters, which is related to the notion of local identifiability. This differs from the observed values for the batch functional dimension computed on training inputs and test inputs. The latter are influenced by <b>geometry-induced</b> implicit regularization.</p></p class="citation"></blockquote><h3 id=2627--163247-time-to-stop-and-think-what-kind-of-research-do-we-want-to-do-josu-ceberio-et-al-2024>(26/27 | 163/247) Time to Stop and Think: What kind of research do we want to do? (Josu Ceberio et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Josu Ceberio, Borja Calvo. (2024)<br><strong>Time to Stop and Think: What kind of research do we want to do?</strong><br><button class=copy-to-clipboard title="Time to Stop and Think: What kind of research do we want to do?" index=163>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-163 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08298v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08298v1.pdf filename=2402.08298v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Experimentation is an intrinsic part of research in artificial intelligence since it allows for collecting quantitative observations, validating hypotheses, and providing evidence for their reformulation. For that reason, experimentation must be coherent with the purposes of the research, properly addressing the relevant questions in each case. Unfortunately, the literature is full of works whose experimentation is neither rigorous nor convincing, oftentimes designed to support prior beliefs rather than answering the relevant research questions. In this paper, we focus on the field of metaheuristic optimization, since it is our main field of work, and it is where we have observed the misconduct that has motivated this letter. Even if we limit the focus of this manuscript to the experimental part of the research, our main goal is to sew the seed of sincere critical assessment of our work, sparking a reflection process both at the individual and the community level. Such a reflection process is too complex and extensive to be tackled as a whole. Therefore, to bring our feet to the ground, we will include in this document our reflections about the role of experimentation in our work, discussing topics such as the use of <b>benchmark</b> instances vs instance generators, or the statistical assessment of empirical results. That is, all the statements included in this document are personal views and opinions, which can be shared by others or not. Certainly, having different points of view is the basis to establish a good discussion process.</p></p class="citation"></blockquote><h3 id=2727--164247-inherent-diverse-redundant-safety-mechanisms-for-ai-based-software-elements-in-automotive-applications-mandar-pitale-et-al-2024>(27/27 | 164/247) Inherent Diverse Redundant Safety Mechanisms for AI-based Software Elements in Automotive Applications (Mandar Pitale et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mandar Pitale, Alireza Abbaspour, Devesh Upadhyay. (2024)<br><strong>Inherent Diverse Redundant Safety Mechanisms for AI-based Software Elements in Automotive Applications</strong><br><button class=copy-to-clipboard title="Inherent Diverse Redundant Safety Mechanisms for AI-based Software Elements in Automotive Applications" index=164>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-164 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 3<br>Keywords: Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08208v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08208v1.pdf filename=2402.08208v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper explores the role and challenges of Artificial Intelligence (AI) algorithms, specifically AI-based software elements, in autonomous driving systems. These AI systems are fundamental in executing real-time critical functions in complex and high-dimensional environments. They handle vital tasks like <b>multi-modal</b> perception, cognition, and decision-making tasks such as motion planning, lane keeping, and emergency braking. A primary concern relates to the ability (and necessity) of AI models to generalize beyond their initial training data. This generalization issue becomes evident in real-time scenarios, where models frequently encounter inputs not represented in their training or validation data. In such cases, AI systems must still function effectively despite facing distributional or domain shifts. This paper investigates the risk associated with overconfident AI models in safety-critical applications like autonomous driving. To mitigate these risks, methods for training AI models that help maintain performance without overconfidence are proposed. This involves implementing certainty reporting architectures and ensuring diverse training data. While various distribution-based methods exist to provide safety mechanisms for AI models, there is a noted lack of systematic assessment of these methods, especially in the context of safety-critical automotive applications. Many methods in the literature do not adapt well to the quick response times required in safety-critical edge applications. This paper reviews these methods, discusses their suitability for safety-critical applications, and highlights their strengths and limitations. The paper also proposes potential improvements to enhance the safety and reliability of AI algorithms in autonomous vehicles in the context of rapid and accurate decision-making processes.</p></p class="citation"></blockquote><h2 id=csne-1>cs.NE (1)</h2><h3 id=11--165247-human-curriculum-effects-emerge-with-in-context-learning-in-neural-networks-jacob-russin-et-al-2024>(1/1 | 165/247) Human Curriculum Effects Emerge with In-Context Learning in Neural Networks (Jacob Russin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jacob Russin, Ellie Pavlick, Michael J. Frank. (2024)<br><strong>Human Curriculum Effects Emerge with In-Context Learning in Neural Networks</strong><br><button class=copy-to-clipboard title="Human Curriculum Effects Emerge with In-Context Learning in Neural Networks" index=165>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-165 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NE<br>Categories: cs-LG, cs-NE, cs.NE, q-bio-NC<br>Keyword Score: 70<br>Keywords: Meta Learning, Transformer, In-context Learning, In-context Learning, In-context Learning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08674v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08674v1.pdf filename=2402.08674v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Human learning is sensitive to rule-like structure and the curriculum of examples used for training. In tasks governed by succinct rules, learning is more robust when related examples are blocked across trials, but in the absence of such rules, interleaving is more effective. To date, no neural model has simultaneously captured these seemingly contradictory effects. Here we show that this same tradeoff spontaneously emerges with <b>&ldquo;in-context</b> <b>learning&rdquo;</b> <b>(ICL)</b> both in neural networks trained with metalearning and in <b>large</b> <b>language</b> <b>models</b> <b>(LLMs).</b> <b>ICL</b> is the ability to learn new tasks &ldquo;in context&rdquo; - without weight changes - via an inner-loop algorithm implemented in activation dynamics. Experiments with pretrained <b>LLMs</b> and metalearning <b>transformers</b> show that <b>ICL</b> exhibits the blocking advantage demonstrated in humans on a task involving rule-like structure, and conversely, that concurrent in-weight learning reproduces the interleaving advantage observed in humans on tasks lacking such structure.</p></p class="citation"></blockquote><h2 id=csir-6>cs.IR (6)</h2><h3 id=16--166247-modeling-balanced-explicit-and-implicit-relations-with-contrastive-learning-for-knowledge-concept-recommendation-in-moocs-hengnian-gu-et-al-2024>(1/6 | 166/247) Modeling Balanced Explicit and Implicit Relations with Contrastive Learning for Knowledge Concept Recommendation in MOOCs (Hengnian Gu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hengnian Gu, Zhiyi Duan, Pan Xie, Dongdai Zhou. (2024)<br><strong>Modeling Balanced Explicit and Implicit Relations with Contrastive Learning for Knowledge Concept Recommendation in MOOCs</strong><br><button class=copy-to-clipboard title="Modeling Balanced Explicit and Implicit Relations with Contrastive Learning for Knowledge Concept Recommendation in MOOCs" index=166>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-166 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-AI, cs-IR, cs.IR<br>Keyword Score: 63<br>Keywords: Graph Convolutional Network, Graph, Graph Neural Network, Contrastive Learning, Convolution, Convolutional Neural Network, Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08256v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08256v1.pdf filename=2402.08256v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The knowledge concept <b>recommendation</b> in Massive Open Online Courses (MOOCs) is a significant issue that has garnered widespread attention. Existing methods primarily rely on the explicit relations between users and knowledge concepts on the MOOC platforms for <b>recommendation.</b> However, there are numerous implicit relations (e.g., shared interests or same knowledge levels between users) generated within the users&rsquo; learning activities on the MOOC platforms. Existing methods fail to consider these implicit relations, and these relations themselves are difficult to learn and represent, causing poor performance in knowledge concept <b>recommendation</b> and an inability to meet users&rsquo; personalized needs. To address this issue, we propose a novel framework based on <b>contrastive</b> <b>learning,</b> which can represent and balance the explicit and implicit relations for knowledge concept <b>recommendation</b> in MOOCs (CL-KCRec). Specifically, we first construct a MOOCs heterogeneous information network (HIN) by modeling the data from the MOOC platforms. Then, we utilize a relation-updated <b>graph</b> <b>convolutional</b> <b>network</b> and stacked multi-channel <b>graph</b> <b>neural</b> <b>network</b> to represent the explicit and implicit relations in the HIN, respectively. Considering that the quantity of explicit relations is relatively fewer compared to implicit relations in MOOCs, we propose a <b>contrastive</b> <b>learning</b> with prototypical <b>graph</b> <b>to</b> <b>enhance</b> the representations of both relations to capture their fruitful inherent relational knowledge, which can guide the propagation of students&rsquo; preferences within the HIN. Based on these enhanced representations, to ensure the balanced contribution of both towards the final <b>recommendation,</b> we propose a dual-head attention mechanism for balanced fusion. Experimental results demonstrate that CL-KCRec outperforms several state-of-the-art baselines on real-world datasets in terms of HR, NDCG and MRR.</p></p class="citation"></blockquote><h3 id=26--167247-multi-label-zero-shot-product-attribute-value-extraction-jiaying-gong-et-al-2024>(2/6 | 167/247) Multi-Label Zero-Shot Product Attribute-Value Extraction (Jiaying Gong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiaying Gong, Hoda Eldardiry. (2024)<br><strong>Multi-Label Zero-Shot Product Attribute-Value Extraction</strong><br><button class=copy-to-clipboard title="Multi-Label Zero-Shot Product Attribute-Value Extraction" index=167>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-167 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 53<br>Keywords: Graph, Recommendation, Supervised Learning, Supervised Learning, Zero-shot, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08802v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08802v1.pdf filename=2402.08802v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>E-commerce platforms should provide detailed product descriptions (attribute values) for effective product search and <b>recommendation.</b> However, attribute value information is typically not available for new products. To predict unseen attribute values, <b>large</b> <b>quantities</b> <b>of</b> labeled training data are needed to train a traditional <b>supervised</b> <b>learning</b> model. Typically, it is difficult, time-consuming, and costly to manually label <b>large</b> <b>quantities</b> <b>of</b> new product profiles. In this paper, we propose a novel method to efficiently and effectively extract unseen attribute values from new products in the absence of labeled data <b>(zero-shot</b> setting). We propose HyperPAVE, a multi-label <b>zero-shot</b> attribute value extraction model that leverages inductive inference in heterogeneous hypergraphs. In particular, our proposed technique constructs heterogeneous hypergraphs to capture complex higher-order relations (i.e. user behavior information) to learn more accurate feature representations for <b>graph</b> nodes. Furthermore, our proposed HyperPAVE model uses an inductive link prediction mechanism to infer future connections between unseen nodes. This enables HyperPAVE to identify new attribute values without the need for labeled training data. We conduct extensive experiments with ablation studies on different categories of the MAVE dataset. The results demonstrate that our proposed HyperPAVE model significantly outperforms existing classification-based, generation-based <b>large</b> <b>language</b> <b>models</b> for attribute value extraction in the <b>zero-shot</b> setting.</p></p class="citation"></blockquote><h3 id=36--168247-causal-learning-for-trustworthy-recommender-systems-a-survey-jin-li-et-al-2024>(3/6 | 168/247) Causal Learning for Trustworthy Recommender Systems: A Survey (Jin Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jin Li, Shoujin Wang, Qi Zhang, Longbing Cao, Fang Chen, Xiuzhen Zhang, Dietmar Jannach, Charu C. Aggarwal. (2024)<br><strong>Causal Learning for Trustworthy Recommender Systems: A Survey</strong><br><button class=copy-to-clipboard title="Causal Learning for Trustworthy Recommender Systems: A Survey" index=168>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-168 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 30<br>Keywords: Fairness, Recommendation, Recommender System<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08241v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08241v1.pdf filename=2402.08241v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Recommender</b> <b>Systems</b> (RS) have significantly advanced online content discovery and personalized decision-making. However, emerging vulnerabilities in RS have catalyzed a paradigm shift towards Trustworthy RS (TRS). Despite numerous progress on TRS, most of them focus on data correlations while overlooking the fundamental causal nature in <b>recommendation.</b> This drawback hinders TRS from identifying the cause in addressing trustworthiness issues, leading to limited <b>fairness,</b> robustness, and explainability. To bridge this gap, causal learning emerges as a class of promising methods to augment TRS. These methods, grounded in reliable causality, excel in mitigating various biases and noises while offering insightful explanations for TRS. However, there lacks a timely survey in this vibrant area. This paper creates an overview of TRS from the perspective of causal learning. We begin by presenting the advantages and common procedures of Causality-oriented TRS (CTRS). Then, we identify potential trustworthiness challenges at each stage and link them to viable causal solutions, followed by a classification of CTRS methods. Finally, we discuss several future directions for advancing this field.</p></p class="citation"></blockquote><h3 id=46--169247-captions-are-worth-a-thousand-words-enhancing-product-retrieval-with-pretrained-image-to-text-models-jason-tang-et-al-2024>(4/6 | 169/247) Captions Are Worth a Thousand Words: Enhancing Product Retrieval with Pretrained Image-to-Text Models (Jason Tang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jason Tang, Garrin McGoldrick, Marie Al-Ghossein, Ching-Wei Chen. (2024)<br><strong>Captions Are Worth a Thousand Words: Enhancing Product Retrieval with Pretrained Image-to-Text Models</strong><br><button class=copy-to-clipboard title="Captions Are Worth a Thousand Words: Enhancing Product Retrieval with Pretrained Image-to-Text Models" index=169>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-169 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 26<br>Keywords: Multi-modal, Multi-modal, Recommendation, Image2text<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08532v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08532v1.pdf filename=2402.08532v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper explores the usage of <b>multimodal</b> <b>image-to-text</b> models to enhance text-based item retrieval. We propose utilizing pre-trained image captioning and tagging models, such as instructBLIP and CLIP, to generate text-based product descriptions which are combined with existing text descriptions. Our work is particularly impactful for smaller eCommerce businesses who are unable to maintain the high-quality text descriptions necessary to effectively perform item retrieval for search and <b>recommendation</b> use cases. We evaluate the searchability of ground-truth text, image-generated text, and combinations of both texts on several subsets of Amazon&rsquo;s publicly available ESCI dataset. The results demonstrate the dual capability of our proposed models to enhance the retrieval of existing text and generate highly-searchable standalone descriptions.</p></p class="citation"></blockquote><h3 id=56--170247-frequency-aware-graph-signal-processing-for-collaborative-filtering-jiafeng-xia-et-al-2024>(5/6 | 170/247) Frequency-aware Graph Signal Processing for Collaborative Filtering (Jiafeng Xia et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiafeng Xia, Dongsheng Li, Hansu Gu, Tun Lu, Peng Zhang, Li Shang, Ning Gu. (2024)<br><strong>Frequency-aware Graph Signal Processing for Collaborative Filtering</strong><br><button class=copy-to-clipboard title="Frequency-aware Graph Signal Processing for Collaborative Filtering" index=170>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-170 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs-LG, cs.IR<br>Keyword Score: 23<br>Keywords: Graph Convolutional Network, Graph, Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08426v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08426v1.pdf filename=2402.08426v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Graph</b> Signal Processing (GSP) based <b>recommendation</b> algorithms have recently attracted lots of attention due to its high efficiency. However, these methods failed to consider the importance of various interactions that reflect unique user/item characteristics and failed to utilize user and item high-order neighborhood information to model user preference, thus leading to sub-optimal performance. To address the above issues, we propose a frequency-aware <b>graph</b> signal processing method (FaGSP) for collaborative filtering. Firstly, we design a Cascaded Filter Module, consisting of an ideal high-pass filter and an ideal low-pass filter that work in a successive manner, to capture both unique and common user/item characteristics to more accurately model user preference. Then, we devise a Parallel Filter Module, consisting of two low-pass filters that can easily capture the hierarchy of neighborhood, to fully utilize high-order neighborhood information of users/items for more accurate user preference modeling. Finally, we combine these two modules via a linear model to further improve <b>recommendation</b> accuracy. Extensive experiments on six public datasets demonstrate the superiority of our method from the perspectives of prediction accuracy and training efficiency compared with state-of-the-art <b>GCN-based</b> <b>recommendation</b> methods and GSP-based <b>recommendation</b> methods.</p></p class="citation"></blockquote><h3 id=66--171247-implementation-of-recommendation-algorithm-based-on-recommendation-sessions-in-e-commerce-it-system-michał-malinowski-2024>(6/6 | 171/247) Implementation of Recommendation Algorithm based on Recommendation Sessions in E-commerce IT System (Michał Malinowski, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Michał Malinowski. (2024)<br><strong>Implementation of Recommendation Algorithm based on Recommendation Sessions in E-commerce IT System</strong><br><button class=copy-to-clipboard title="Implementation of Recommendation Algorithm based on Recommendation Sessions in E-commerce IT System" index=171>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-171 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 13<br>Keywords: Graph, Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08275v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08275v1.pdf filename=2402.08275v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents a study on the implementation of the author&rsquo;s Algorithm of <b>Recommendation</b> Sessions (ARS) in an operational e-commerce information system and analyses the basic parameters of the resulting <b>recommendation</b> system. It begins with a synthetic overview of <b>recommendation</b> systems, followed by a presentation of the proprietary ARS algorithm, which is based on <b>recommendation</b> sessions. A mathematical model of the <b>recommendation</b> session, constructed using <b>graph</b> and network theory, serves as the input for the ARS algorithm. This paper also explores <b>graph</b> structure representation methods and the implementation of a G <b>graph</b> (representing a set of <b>recommendation</b> sessions) in a relational database using the SQL standard. The ARS algorithm was implemented in a working e-commerce information system, leading to the development of a fully functional <b>recommendation</b> system adaptable to various e-commerce IT systems. The effectiveness of the algorithm is demonstrated by research on the <b>recommendation</b> system&rsquo;s parameters presented in the final section of the paper.</p></p class="citation"></blockquote><h2 id=cshc-8>cs.HC (8)</h2><h3 id=18--172247-ghostwriter-augmenting-collaborative-human-ai-writing-experiences-through-personalization-and-agency-catherine-yeh-et-al-2024>(1/8 | 172/247) GhostWriter: Augmenting Collaborative Human-AI Writing Experiences Through Personalization and Agency (Catherine Yeh et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Catherine Yeh, Gonzalo Ramos, Rachel Ng, Andy Huntington, Richard Banks. (2024)<br><strong>GhostWriter: Augmenting Collaborative Human-AI Writing Experiences Through Personalization and Agency</strong><br><button class=copy-to-clipboard title="GhostWriter: Augmenting Collaborative Human-AI Writing Experiences Through Personalization and Agency" index=172>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-172 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-AI, cs-HC, cs.HC<br>Keyword Score: 50<br>Keywords: Recommendation, Text Generation, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08855v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08855v1.pdf filename=2402.08855v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> are becoming more prevalent and have found a ubiquitous use in providing different forms of writing assistance. However, <b>LLM-powered</b> writing systems can frustrate users due to their limited personalization and control, which can be exacerbated when users lack experience with <b>prompt</b> engineering. We see design as one way to address these challenges and introduce GhostWriter, an AI-enhanced writing design probe where users can exercise enhanced agency and personalization. GhostWriter leverages <b>LLMs</b> to learn the user&rsquo;s intended writing style implicitly as they write, while allowing explicit teaching moments through manual style edits and annotations. We study 18 participants who use GhostWriter on two different writing tasks, observing that it helps users craft personalized <b>text</b> <b>generations</b> and empowers them by providing multiple ways to control the system&rsquo;s writing style. From this study, we present insights regarding people&rsquo;s relationship with AI-assisted writing and offer design <b>recommendations</b> for future work.</p></p class="citation"></blockquote><h3 id=28--173247-the-last-jitai-the-unreasonable-effectiveness-of-large-language-models-in-issuing-just-in-time-adaptive-interventions-fostering-physical-activity-in-a-prospective-cardiac-rehabilitation-setting-david-haag-et-al-2024>(2/8 | 173/247) The Last JITAI? The Unreasonable Effectiveness of Large Language Models in Issuing Just-in-Time Adaptive Interventions: Fostering Physical Activity in a Prospective Cardiac Rehabilitation Setting (David Haag et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>David Haag, Devender Kumar, Sebastian Gruber, Mahdi Sareban, Gunnar Treff, Josef Niebauer, Christopher Bull, Jan David Smeddinck. (2024)<br><strong>The Last JITAI? The Unreasonable Effectiveness of Large Language Models in Issuing Just-in-Time Adaptive Interventions: Fostering Physical Activity in a Prospective Cardiac Rehabilitation Setting</strong><br><button class=copy-to-clipboard title="The Last JITAI? The Unreasonable Effectiveness of Large Language Models in Issuing Just-in-Time Adaptive Interventions: Fostering Physical Activity in a Prospective Cardiac Rehabilitation Setting" index=173>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-173 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: J-3, cs-AI, cs-HC, cs.HC<br>Keyword Score: 40<br>Keywords: GPT, GPT-4, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08658v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08658v1.pdf filename=2402.08658v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We explored the viability of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> for triggering and personalizing content for Just-in-Time Adaptive Interventions (JITAIs) in digital health. JITAIs are being explored as a key mechanism for sustainable behavior change, adapting interventions to an individual&rsquo;s current context and needs. However, traditional rule-based and machine learning models for JITAI implementation face scalability and reliability limitations, such as lack of personalization, difficulty in managing multi-parametric systems, and issues with data sparsity. To investigate JITAI implementation via <b>LLMs,</b> we tested the contemporary overall performance-leading model <b>&lsquo;GPT-4&rsquo;</b> with examples grounded in the use case of fostering heart-healthy physical activity in outpatient cardiac rehabilitation. Three personas and five sets of context information per persona were used as a basis of triggering and personalizing JITAIs. Subsequently, we generated a total of 450 proposed JITAI decisions and message content, divided equally into JITAIs generated by 10 iterations with <b>GPT-4,</b> a baseline provided by 10 laypersons (LayPs), and a gold standard set by 10 healthcare professionals (HCPs). Ratings from 27 LayPs indicated that JITAIs generated by <b>GPT-4</b> were superior to those by HCPs and LayPs over all assessed scales: i.e., appropriateness, engagement, effectiveness, and professionality. This study indicates that <b>LLMs</b> have significant potential for implementing JITAIs as a building block of personalized or &ldquo;precision&rdquo; health, offering scalability, effective personalization based on opportunistically sampled information, and good acceptability.</p></p class="citation"></blockquote><h3 id=38--174247-vision-based-hand-gesture-customization-from-a-single-demonstration-soroush-shahi-et-al-2024>(3/8 | 174/247) Vision-Based Hand Gesture Customization from a Single Demonstration (Soroush Shahi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Soroush Shahi, Cori Tymoszek Park, Richard Kang, Asaf Liberman, Oron Levy, Jun Gong, Abdelkareem Bedri, Gierad Laput. (2024)<br><strong>Vision-Based Hand Gesture Customization from a Single Demonstration</strong><br><button class=copy-to-clipboard title="Vision-Based Hand Gesture Customization from a Single Demonstration" index=174>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-174 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: H-5-2; I-4, cs-HC, cs.HC<br>Keyword Score: 40<br>Keywords: Few-shot, Few-shot Learning, Meta Learning, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08420v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08420v1.pdf filename=2402.08420v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Hand gesture recognition is becoming a more prevalent mode of human-computer interaction, especially as cameras proliferate across everyday devices. Despite continued progress in this field, gesture customization is often underexplored. Customization is crucial since it enables users to define and demonstrate gestures that are more natural, memorable, and accessible. However, customization requires efficient usage of user-provided data. We introduce a method that enables users to easily design bespoke gestures with a monocular camera from one demonstration. We employ <b>transformers</b> and <b>meta-learning</b> <b>techniques</b> to address <b>few-shot</b> <b>learning</b> challenges. Unlike prior work, our method supports any combination of one-handed, two-handed, static, and dynamic gestures, including different viewpoints. We evaluated our customization method through a user study with 20 gestures collected from 21 participants, achieving up to 97% average recognition accuracy from one demonstration. Our work provides a viable path for vision-based gesture customization, laying the foundation for future advancements in this domain.</p></p class="citation"></blockquote><h3 id=48--175247-simulating-human-strategic-behavior-comparing-single-and-multi-agent-llms-karthik-sreedhar-et-al-2024>(4/8 | 175/247) Simulating Human Strategic Behavior: Comparing Single and Multi-agent LLMs (Karthik Sreedhar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Karthik Sreedhar, Lydia Chilton. (2024)<br><strong>Simulating Human Strategic Behavior: Comparing Single and Multi-agent LLMs</strong><br><button class=copy-to-clipboard title="Simulating Human Strategic Behavior: Comparing Single and Multi-agent LLMs" index=175>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-175 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-HC, cs.HC<br>Keyword Score: 40<br>Keywords: Simulation, Simulator, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08189v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08189v1.pdf filename=2402.08189v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>When creating plans, policies, or applications for people, it is challenging for designers to think through the strategic ways that different people will behave. Recently, <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have been shown to create realistic <b>simulations</b> of human-like behavior based on personas. We build on this to investigate whether <b>LLMs</b> can simulate human strategic behavior. Human strategies are complex because they take into account social norms in addition to aiming to maximize personal gain. The ultimatum game is a classic economics experiment used to understand human strategic behavior in a social setting. It shows that people will often choose to &ldquo;punish&rdquo; other players to enforce social norms rather than to maximize personal profits. We test whether <b>LLMs</b> can replicate this complex behavior in <b>simulations.</b> We compare two architectures: single- and multi-agent <b>LLMs.</b> We compare their abilities to (1) simulate human-like actions in the ultimatum game, (2) simulate two player personalities, greedy and fair, and (3) create robust strategies that are logically complete and consistent with personality. Our evaluation shows the multi-agent architecture is much more accurate than single <b>LLMs</b> (88% vs. 50%) in simulating human strategy creation and actions for personality pairs. Thus there is potential to use <b>LLMs</b> to simulate human strategic behavior to help designers, planners, and policymakers perform preliminary exploration of how people behave in systems.</p></p class="citation"></blockquote><h3 id=58--176247-intelligent-canvas-enabling-design-like-exploratory-visual-data-analysis-with-generative-ai-through-rapid-prototyping-iteration-and-curation-zijian-ding-et-al-2024>(5/8 | 176/247) Intelligent Canvas: Enabling Design-Like Exploratory Visual Data Analysis with Generative AI through Rapid Prototyping, Iteration and Curation (Zijian Ding et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zijian Ding, Joel Chan. (2024)<br><strong>Intelligent Canvas: Enabling Design-Like Exploratory Visual Data Analysis with Generative AI through Rapid Prototyping, Iteration and Curation</strong><br><button class=copy-to-clipboard title="Intelligent Canvas: Enabling Design-Like Exploratory Visual Data Analysis with Generative AI through Rapid Prototyping, Iteration and Curation" index=176>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-176 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-AI, cs-HC, cs.HC<br>Keyword Score: 10<br>Keywords: Generative AI<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08812v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08812v2.pdf filename=2402.08812v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Complex data analysis inherently seeks unexpected insights through exploratory \re{visual analysis} methods, transcending logical, step-by-step processing. However, \re{existing interfaces such as notebooks and dashboards have limitations in exploration and comparison for visual data analysis}. Addressing these limitations, we introduce a &ldquo;design-like&rdquo; intelligent canvas environment integrating <b>generative</b> <b>AI</b> into data analysis, offering rapid prototyping, iteration, and comparative visualization management. Our dual contributions include the integration of <b>generative</b> <b>AI</b> components into a canvas interface, and empirical findings from a user study (N=10) evaluating the effectiveness of the canvas interface.</p></p class="citation"></blockquote><h3 id=68--177247-exploring-diversity-perceptions-in-a-community-through-a-qa-chatbot-peter-kun-et-al-2024>(6/8 | 177/247) Exploring diversity perceptions in a community through a Q&amp;A chatbot (Peter Kun et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Peter Kun, Amalia De Götzen, Miriam Bidoglia, Niels Jørgen Gommesen, George Gaskell. (2024)<br><strong>Exploring diversity perceptions in a community through a Q&amp;A chatbot</strong><br><button class=copy-to-clipboard title="Exploring diversity perceptions in a community through a Q&A chatbot" index=177>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-177 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-HC, cs.HC<br>Keyword Score: 10<br>Keywords: Chatbot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08558v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08558v1.pdf filename=2402.08558v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>While diversity has become a debated issue in design, very little research exists on positive use-cases for diversity beyond scholarly criticism. The current work addresses this gap through the case of a diversity-aware <b>chatbot,</b> exploring what benefits a diversity-aware <b>chatbot</b> could bring to people and how do people interpret diversity when being presented with it. In this paper, we motivate a Q&amp;A <b>chatbot</b> as a technology probe and deploy it in two student communities within a study. During the study, we collected contextual data on people&rsquo;s expectations and perceptions when presented with diversity during the study. Our key findings show that people seek out others with shared niche interests, or their search is driven by exploration and inspiration when presented with diversity. Although interacting with <b>chatbots</b> is limited, participants found the engagement novel and interesting to motivate future research.</p></p class="citation"></blockquote><h3 id=78--178247-moonwalk-advancing-gait-based-user-recognition-on-wearable-devices-with-metric-learning-asaf-liberman-et-al-2024>(7/8 | 178/247) Moonwalk: Advancing Gait-Based User Recognition on Wearable Devices with Metric Learning (Asaf Liberman et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Asaf Liberman, Oron Levy, Soroush Shahi, Cori Tymoszek Park, Mike Ralph, Richard Kang, Abdelkareem Bedri, Gierad Laput. (2024)<br><strong>Moonwalk: Advancing Gait-Based User Recognition on Wearable Devices with Metric Learning</strong><br><button class=copy-to-clipboard title="Moonwalk: Advancing Gait-Based User Recognition on Wearable Devices with Metric Learning" index=178>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-178 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: H-5-2, cs-HC, cs.HC<br>Keyword Score: 10<br>Keywords: Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08451v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08451v1.pdf filename=2402.08451v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Personal devices have adopted diverse authentication methods, including biometric recognition and passcodes. In contrast, headphones have limited input mechanisms, depending solely on the authentication of connected devices. We present Moonwalk, a novel method for passive user recognition utilizing the built-in headphone accelerometer. Our approach centers on gait recognition; enabling users to establish their identity simply by walking for a brief interval, despite the sensor&rsquo;s placement away from the feet. We employ <b>self-supervised</b> metric learning to train a model that yields a highly discriminative representation of a user&rsquo;s 3D acceleration, with no retraining required. We tested our method in a study involving 50 participants, achieving an average F1 score of 92.9% and equal error rate of 2.3%. We extend our evaluation by assessing performance under various conditions (e.g. shoe types and surfaces). We discuss the opportunities and challenges these variations introduce and propose new directions for advancing passive authentication for wearable devices.</p></p class="citation"></blockquote><h3 id=88--179247-mood-as-a-contextual-cue-for-improved-emotion-inference-soujanya-narayana-et-al-2024>(8/8 | 179/247) Mood as a Contextual Cue for Improved Emotion Inference (Soujanya Narayana et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Soujanya Narayana, Ibrahim Radwan, Ramanathan Subramanian, Roland Goecke. (2024)<br><strong>Mood as a Contextual Cue for Improved Emotion Inference</strong><br><button class=copy-to-clipboard title="Mood as a Contextual Cue for Improved Emotion Inference" index=179>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-179 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-HC, cs.HC<br>Keyword Score: 6<br>Keywords: Multi-modal, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08413v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08413v1.pdf filename=2402.08413v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Psychological studies observe that emotions are rarely expressed in isolation and are typically influenced by the surrounding context. While recent studies effectively harness uni- and <b>multimodal</b> cues for emotion inference, hardly any study has considered the effect of long-term affect, or \emph{mood}, on short-term \emph{emotion} inference. This study (a) proposes time-continuous \emph{valence} prediction from videos, fusing <b>multimodal</b> cues including \emph{mood} and \emph{emotion-change} ($\Delta$) labels, (b) serially integrates spatial and channel attention for improved inference, and (c) demonstrates algorithmic generalisability with experiments on the \emph{EMMA} and \emph{AffWild2} datasets. Empirical results affirm that utilising mood labels is highly beneficial for dynamic valence prediction. Comparing \emph{unimodal} (training only with mood labels) vs \emph{multimodal} (training with mood and $\Delta$ labels) results, inference performance improves for the latter, conveying that both long and short-term contextual cues are critical for time-continuous emotion inference.</p></p class="citation"></blockquote><h2 id=csse-9>cs.SE (9)</h2><h3 id=19--180247-chatgpt-vs-llama-impact-reliability-and-challenges-in-stack-overflow-discussions-leuson-da-silva-et-al-2024>(1/9 | 180/247) ChatGPT vs LLaMA: Impact, Reliability, and Challenges in Stack Overflow Discussions (Leuson Da Silva et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Leuson Da Silva, Jordan Samhi, Foutse Khomh. (2024)<br><strong>ChatGPT vs LLaMA: Impact, Reliability, and Challenges in Stack Overflow Discussions</strong><br><button class=copy-to-clipboard title="ChatGPT vs LLaMA: Impact, Reliability, and Challenges in Stack Overflow Discussions" index=180>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-180 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-AI, cs-SE, cs.SE<br>Keyword Score: 50<br>Keywords: Generative AI, ChatGPT, LLaMA, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08801v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08801v1.pdf filename=2402.08801v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Since its release in November 2022, <b>ChatGPT</b> has shaken up Stack Overflow, the premier platform for developers&rsquo; queries on programming and software development. Demonstrating an ability to generate instant, human-like responses to technical questions, <b>ChatGPT</b> has ignited debates within the developer community about the evolving role of human-driven platforms in the age of <b>generative</b> <b>AI.</b> Two months after <b>ChatGPT&rsquo;s</b> release, Meta released its answer with its own <b>Large</b> <b>Language</b> <b>Model</b> <b>(LLM)</b> called <b>LLaMA:</b> the race was on. We conducted an empirical study analyzing questions from Stack Overflow and using these <b>LLMs</b> to address them. This way, we aim to (ii) measure user engagement evolution with Stack Overflow over time; (ii) quantify the reliability of <b>LLMs&rsquo;</b> answers and their potential to replace Stack Overflow in the long term; (iii) identify and understand why <b>LLMs</b> fails; and (iv) compare <b>LLMs</b> together. Our empirical results are unequivocal: <b>ChatGPT</b> and <b>LLaMA</b> challenge human expertise, yet do not outperform it for some domains, while a significant decline in user posting activity has been observed. Furthermore, we also discuss the impact of our findings regarding the usage and development of new <b>LLMs.</b></p></p class="citation"></blockquote><h3 id=29--181247-unsupervised-evaluation-of-code-llms-with-round-trip-correctness-miltiadis-allamanis-et-al-2024>(2/9 | 181/247) Unsupervised Evaluation of Code LLMs with Round-Trip Correctness (Miltiadis Allamanis et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Miltiadis Allamanis, Sheena Panthaplackel, Pengcheng Yin. (2024)<br><strong>Unsupervised Evaluation of Code LLMs with Round-Trip Correctness</strong><br><button class=copy-to-clipboard title="Unsupervised Evaluation of Code LLMs with Round-Trip Correctness" index=181>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-181 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-LG, cs-SE, cs.SE<br>Keyword Score: 33<br>Keywords: Benchmarking, Unsupervised Learning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08699v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08699v1.pdf filename=2402.08699v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>To evaluate code <b>large</b> <b>language</b> <b>models</b> <b>(LLMs),</b> research has relied on a few small manually curated <b>benchmarks,</b> such as HumanEval and MBPP, which represent a narrow part of the real-world software domains. In this work, we introduce round-trip correctness (RTC) as an alternative evaluation method. RTC allows Code <b>LLM</b> evaluation on a broader spectrum of real-world software domains without the need for costly human curation. RTC rests on the idea that we can ask a model to make a prediction (e.g., describe some code using natural language), feed that prediction back (e.g., synthesize code from the predicted description), and check if this round-trip leads to code that is semantically equivalent to the original input. We show how to employ RTC to evaluate code synthesis and editing. We find that RTC strongly correlates with model performance on existing narrow-domain code synthesis <b>benchmarks</b> while allowing us to expand to a much broader set of domains and tasks which was not previously possible without costly human annotations.</p></p class="citation"></blockquote><h3 id=39--182247-on-the-fly-syntax-highlighting-generalisation-and-speed-ups-marco-edoardo-palma-et-al-2024>(3/9 | 182/247) On-the-Fly Syntax Highlighting: Generalisation and Speed-ups (Marco Edoardo Palma et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Marco Edoardo Palma, Alex Wolf, Pasquale Salza, Harald C. Gall. (2024)<br><strong>On-the-Fly Syntax Highlighting: Generalisation and Speed-ups</strong><br><button class=copy-to-clipboard title="On-the-Fly Syntax Highlighting: Generalisation and Speed-ups" index=182>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-182 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-SE, cs.SE<br>Keyword Score: 30<br>Keywords: Convolution, Convolutional Neural Network, Recurrent Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08754v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08754v1.pdf filename=2402.08754v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>On-the-fly syntax highlighting is the task of rapidly associating visual secondary notation values with each character of a language derivation. Research in this domain is driven by the prevalence of online software development tools, which frequently display source code on screen and heavily rely on syntax highlighting mechanisms. In this context, three contrasting demands confront resolvers in this space: speed, accuracy, and development costs. Speed constraints are essential to ensure tool usability, manifesting as responsiveness for end users accessing online source code and minimising system overhead. Simultaneously, achieving precise highlighting is critical for enhancing code comprehensibility. Nevertheless, obtaining accurate results necessitates the capacity to perform grammatical analysis on the code under consideration, even in cases of varying grammatical correctness. Furthermore, addressing the development costs of such resolvers is imperative, given the multitude of programming language versions. The current state-of-the-art approach in this field leverages the original lexer and parser of programming languages to create syntax highlighting oracles, subsequently used for training base <b>Recurrent</b> <b>Neural</b> <b>Network</b> models. As the question of the generalisation of such a solution persists, this paper addresses this aspect by extending the original work to three additional mainstream programming languages and conducting a comprehensive review of the outcomes. Moreover, the original limitations in evaluation performance and training costs are mitigated through the introduction of a novel <b>Convolutional</b> based Neural Network model. This study examines the performance gains of running models on GPUs, finding that the new <b>CNN</b> implementation is much faster than previous methods while maintaining high accuracy.</p></p class="citation"></blockquote><h3 id=49--183247-verified-multi-step-synthesis-using-large-language-models-and-monte-carlo-tree-search-david-brandfonbrener-et-al-2024>(4/9 | 183/247) Verified Multi-Step Synthesis using Large Language Models and Monte Carlo Tree Search (David Brandfonbrener et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>David Brandfonbrener, Sibi Raja, Tarun Prasad, Chloe Loughridge, Jianang Yang, Simon Henniger, William E. Byrd, Robert Zinkov, Nada Amin. (2024)<br><strong>Verified Multi-Step Synthesis using Large Language Models and Monte Carlo Tree Search</strong><br><button class=copy-to-clipboard title="Verified Multi-Step Synthesis using Large Language Models and Monte Carlo Tree Search" index=183>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-183 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-AI, cs-LG, cs-LO, cs-SE, cs.SE<br>Keyword Score: 23<br>Keywords: Benchmarking, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08147v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08147v1.pdf filename=2402.08147v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present an approach using Monte Carlo Tree Search (MCTS) to guide <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> to generate verified programs in Dafny, Lean and Coq. Our method, which we call VMCTS, leverages the verifier inside the search algorithm by checking partial programs at each step. In combination with the <b>LLM</b> prior, the verifier feedback raises the synthesis capabilities of open source models. On a set of five verified programming problems, we find that in four problems where the base model cannot solve the question even when re-sampling solutions for one hour, VMCTS can solve the problems within 6 minutes. The base model with VMCTS is even competitive with ChatGPT4 augmented with plugins and multiple re-tries on these problems. Our code and <b>benchmarks</b> are available at <a href=https://github.com/namin/llm-verified-with-monte-carlo-tree-search>https://github.com/namin/llm-verified-with-monte-carlo-tree-search</a> .</p></p class="citation"></blockquote><h3 id=59--184247-generating-java-methods-an-empirical-assessment-of-four-ai-based-code-assistants-vincenzo-corso-et-al-2024>(5/9 | 184/247) Generating Java Methods: An Empirical Assessment of Four AI-Based Code Assistants (Vincenzo Corso et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Vincenzo Corso, Leonardo Mariani, Daniela Micucci, Oliviero Riganelli. (2024)<br><strong>Generating Java Methods: An Empirical Assessment of Four AI-Based Code Assistants</strong><br><button class=copy-to-clipboard title="Generating Java Methods: An Empirical Assessment of Four AI-Based Code Assistants" index=184>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-184 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-SE, cs.SE<br>Keyword Score: 20<br>Keywords: Bard, ChatGPT<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08431v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08431v2.pdf filename=2402.08431v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>AI-based code assistants are promising tools that can facilitate and speed up code development. They exploit machine learning algorithms and natural language processing to interact with developers, suggesting code snippets (e.g., method implementations) that can be incorporated into projects. Recent studies empirically investigated the effectiveness of code assistants using simple exemplary problems (e.g., the re-implementation of well-known algorithms), which fail to capture the spectrum and nature of the tasks actually faced by developers. In this paper, we expand the knowledge in the area by comparatively assessing four popular AI-based code assistants, namely GitHub Copilot, Tabnine, <b>ChatGPT,</b> and Google <b>Bard,</b> with a dataset of 100 methods that we constructed from real-life open-source Java projects, considering a variety of cases for complexity and dependency from contextual elements. Results show that Copilot is often more accurate than other techniques, yet none of the assistants is completely subsumed by the rest of the approaches. Interestingly, the effectiveness of these solutions dramatically decreases when dealing with dependencies outside the boundaries of single classes.</p></p class="citation"></blockquote><h3 id=69--185247-analyzing-prompt-influence-on-automated-method-generation-an-empirical-study-with-copilot-ionut-daniel-fagadau-et-al-2024>(6/9 | 185/247) Analyzing Prompt Influence on Automated Method Generation: An Empirical Study with Copilot (Ionut Daniel Fagadau et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ionut Daniel Fagadau, Leonardo Mariani, Daniela Micucci, Oliviero Riganelli. (2024)<br><strong>Analyzing Prompt Influence on Automated Method Generation: An Empirical Study with Copilot</strong><br><button class=copy-to-clipboard title="Analyzing Prompt Influence on Automated Method Generation: An Empirical Study with Copilot" index=185>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-185 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-AI, cs-SE, cs.SE<br>Keyword Score: 20<br>Keywords: Generative AI, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08430v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08430v1.pdf filename=2402.08430v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Generative</b> <b>AI</b> is changing the way developers interact with software systems, providing services that can produce and deliver new content, crafted to satisfy the actual needs of developers. For instance, developers can ask for new code directly from within their IDEs by writing natural language <b>prompts,</b> and integrated services based on <b>generative</b> <b>AI,</b> such as Copilot, immediately respond to <b>prompts</b> by providing ready-to-use code snippets. Formulating the <b>prompt</b> appropriately, and incorporating the useful information while avoiding any information overload, can be an important factor in obtaining the right piece of code. The task of designing good <b>prompts</b> is known as <b>prompt</b> engineering. In this paper, we systematically investigate the influence of eight <b>prompt</b> features on the style and the content of <b>prompts,</b> on the level of correctness, complexity, size, and similarity to the developers&rsquo; code of the generated code. We specifically consider the task of using Copilot with 124,800 <b>prompts</b> obtained by systematically combining the eight considered <b>prompt</b> features to generate the implementation of 200 Java methods. Results show how some <b>prompt</b> features, such as the presence of examples and the summary of the purpose of the method, can significantly influence the quality of the result.</p></p class="citation"></blockquote><h3 id=79--186247-migration-to-microservices-a-comparative-study-of-decomposition-strategies-and-analysis-metrics-meryam-chaieb-et-al-2024>(7/9 | 186/247) Migration to Microservices: A Comparative Study of Decomposition Strategies and Analysis Metrics (Meryam chaieb et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Meryam chaieb, Mohamed Aymen Saied. (2024)<br><strong>Migration to Microservices: A Comparative Study of Decomposition Strategies and Analysis Metrics</strong><br><button class=copy-to-clipboard title="Migration to Microservices: A Comparative Study of Decomposition Strategies and Analysis Metrics" index=186>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-186 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-SE, cs.SE<br>Keyword Score: 13<br>Keywords: Clustering, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08481v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08481v1.pdf filename=2402.08481v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The microservices architectural style is widely favored for its scalability, reusability, and easy maintainability, <b>prompting</b> increased adoption by developers. However, transitioning from a monolithic to a microservices-based architecture is intricate and costly. In response, we present a novel method utilizing <b>clustering</b> to identify potential microservices in a given monolithic application. Our approach employs a density-based <b>clustering</b> algorithm considering static analysis, structural, and semantic relationships between classes, ensuring a functionally and contextually coherent partitioning. To assess the reliability of our microservice suggestion approach, we conducted an in-depth analysis of hyperparameter sensitivity and compared it with two established <b>clustering</b> algorithms. A comprehensive comparative analysis involved seven applications, evaluating against six baselines, utilizing a dataset of four open-source Java projects. Metrics assessed the quality of generated microservices. Furthermore, we meticulously compared our suggested microservices with manually identified ones in three microservices-based applications. This comparison provided a nuanced understanding of our approach&rsquo;s efficacy and reliability. Our methodology demonstrated promising outcomes, showcasing remarkable effectiveness and commendable stability.</p></p class="citation"></blockquote><h3 id=89--187247-insights-towards-better-case-study-reporting-in-software-engineering-sergio-rico-2024>(8/9 | 187/247) Insights Towards Better Case Study Reporting in Software Engineering (Sergio Rico, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sergio Rico. (2024)<br><strong>Insights Towards Better Case Study Reporting in Software Engineering</strong><br><button class=copy-to-clipboard title="Insights Towards Better Case Study Reporting in Software Engineering" index=187>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-187 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-SE, cs.SE<br>Keyword Score: 10<br>Keywords: Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08411v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08411v1.pdf filename=2402.08411v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Case studies are a popular and noteworthy type of research study in software engineering, offering significant potential to impact industry practices by investigating phenomena in their natural contexts. This potential to reach a broad audience beyond the academic community is often undermined by deficiencies in reporting, particularly in the context description, study classification, generalizability, and the handling of validity threats. This paper presents a reflective analysis aiming to share insights that can enhance the quality and impact of case study reporting. We emphasize the need to follow established guidelines, accurate classification, and detailed context descriptions in case studies. Additionally, particular focus is placed on articulating generalizable findings and thoroughly discussing generalizability threats. We aim to encourage researchers to adopt more rigorous and communicative strategies, ensuring that case studies are methodologically sound, resonate with, and apply to software engineering practitioners and the broader academic community. The reflections and <b>recommendations</b> offered in this paper aim to ensure that insights from case studies are transparent, understandable, and tailored to meet the needs of both academic researchers and industry practitioners. In doing so, we seek to enhance the real-world applicability of academic research, bridging the gap between theoretical research and practical implementation in industry.</p></p class="citation"></blockquote><h3 id=99--188247-what-the-fix-a-study-of-asats-rule-documentation-corentin-latappy-et-al-2024>(9/9 | 188/247) What the Fix? A Study of ASATs Rule Documentation (Corentin Latappy et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Corentin Latappy, Thomas Degueule, Jean-Rémy Falleri, Romain Robbes, Xavier Blanc, Cédric Teyton. (2024)<br><strong>What the Fix? A Study of ASATs Rule Documentation</strong><br><button class=copy-to-clipboard title="What the Fix? A Study of ASATs Rule Documentation" index=188>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-188 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-SE, cs.SE<br>Keyword Score: 10<br>Keywords: Knowledge Distillation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08270v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08270v1.pdf filename=2402.08270v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Automatic Static Analysis Tools (ASATs) are widely used by software developers to diffuse and enforce coding practices. Yet, we know little about the documentation of ASATs, despite it being critical to learn about the coding practices in the first place. We shed light on this through several contributions. First, we analyze the documentation of more than 100 rules of 16 ASATs for multiple programming languages, and <b>distill</b> a taxonomy of the purposes of the documentation-What triggers a rule; Why it is important; and how to Fix an issue-and its types of contents. Then, we conduct a survey to assess the effectiveness of the documentation in terms of its goals and types of content. We highlight opportunities for improvement in ASAT documentation. In particular, we find that the Why purpose is missing in half of the rules we survey; moreover, when the Why is present, it is more likely to have quality issues than the What and the Fix.</p></p class="citation"></blockquote><h2 id=cscy-2>cs.CY (2)</h2><h3 id=12--189247-mapping-the-ethics-of-generative-ai-a-comprehensive-scoping-review-thilo-hagendorff-2024>(1/2 | 189/247) Mapping the Ethics of Generative AI: A Comprehensive Scoping Review (Thilo Hagendorff, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Thilo Hagendorff. (2024)<br><strong>Mapping the Ethics of Generative AI: A Comprehensive Scoping Review</strong><br><button class=copy-to-clipboard title="Mapping the Ethics of Generative AI: A Comprehensive Scoping Review" index=189>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-189 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CY<br>Categories: cs-AI, cs-CY, cs.CY<br>Keyword Score: 40<br>Keywords: Fairness, Generative AI, Text2image, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08323v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08323v1.pdf filename=2402.08323v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The advent of <b>generative</b> <b>artificial</b> intelligence and the widespread adoption of it in society engendered intensive debates about its ethical implications and risks. These risks often differ from those associated with traditional discriminative machine learning. To synthesize the recent discourse and map its normative concepts, we conducted a scoping review on the ethics of <b>generative</b> <b>artificial</b> intelligence, including especially <b>large</b> <b>language</b> <b>models</b> and <b>text-to-image</b> models. Our analysis provides a taxonomy of 378 normative issues in 19 topic areas and ranks them according to their prevalence in the literature. The study offers a comprehensive overview for scholars, practitioners, or policymakers, condensing the ethical debates surrounding <b>fairness,</b> safety, harmful content, hallucinations, privacy, interaction risks, security, alignment, societal impacts, and others. We discuss the results, evaluate imbalances in the literature, and explore unsubstantiated risk scenarios.</p></p class="citation"></blockquote><h3 id=22--190247-unveiling-hidden-energy-anomalies-harnessing-deep-learning-to-optimize-energy-management-in-sports-facilities-fodil-fadli-et-al-2024>(2/2 | 190/247) Unveiling Hidden Energy Anomalies: Harnessing Deep Learning to Optimize Energy Management in Sports Facilities (Fodil Fadli et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fodil Fadli, Yassine Himeur, Mariam Elnour, Abbes Amira. (2024)<br><strong>Unveiling Hidden Energy Anomalies: Harnessing Deep Learning to Optimize Energy Management in Sports Facilities</strong><br><button class=copy-to-clipboard title="Unveiling Hidden Energy Anomalies: Harnessing Deep Learning to Optimize Energy Management in Sports Facilities" index=190>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-190 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CY<br>Categories: cs-CY, cs-LG, cs.CY<br>Keyword Score: 10<br>Keywords: Anomaly Detection<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08742v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08742v1.pdf filename=2402.08742v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Anomaly</b> <b>detection</b> in sport facilities has gained significant attention due to its potential to promote energy saving and optimizing operational efficiency. In this research article, we investigate the role of machine learning, particularly deep learning, in <b>anomaly</b> <b>detection</b> for sport facilities. We explore the challenges and perspectives of utilizing deep learning methods for this task, aiming to address the drawbacks and limitations of conventional approaches. Our proposed approach involves feature extraction from the data collected in sport facilities. We present a problem formulation using Deep Feedforward Neural Networks (DFNN) and introduce threshold estimation techniques to identify anomalies effectively. Furthermore, we propose methods to reduce false alarms, ensuring the reliability and accuracy of <b>anomaly</b> <b>detection.</b> To evaluate the effectiveness of our approach, we conduct experiments on aquatic center dataset at Qatar University. The results demonstrate the superiority of our deep learning-based method over conventional techniques, highlighting its potential in real-world applications. Typically, 94.33% accuracy and 92.92% F1-score have been achieved using the proposed scheme.</p></p class="citation"></blockquote><h2 id=eessiv-5>eess.IV (5)</h2><h3 id=15--191247-poisson-flow-consistency-models-for-low-dose-ct-image-denoising-dennis-hein-et-al-2024>(1/5 | 191/247) Poisson flow consistency models for low-dose CT image denoising (Dennis Hein et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dennis Hein, Adam Wang, Ge Wang. (2024)<br><strong>Poisson flow consistency models for low-dose CT image denoising</strong><br><button class=copy-to-clipboard title="Poisson flow consistency models for low-dose CT image denoising" index=191>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-191 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 40<br>Keywords: Adversarial Learning, Knowledge Distillation, Knowledge Distillation, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08159v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08159v1.pdf filename=2402.08159v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Diffusion and Poisson flow models have demonstrated remarkable success for a wide range of generative tasks. Nevertheless, their iterative nature results in computationally expensive sampling and the number of function evaluations (NFE) required can be orders of magnitude larger than for single-step methods. Consistency models are a recent class of deep generative models which enable single-step sampling of high quality data without the need for <b>adversarial</b> <b>training.</b> In this paper, we introduce a novel image denoising technique which combines the flexibility afforded in Poisson flow generative models (PFGM)++ with the, high quality, single step sampling of consistency models. The proposed method first learns a trajectory between a noise distribution and the posterior distribution of interest by training PFGM++ in a <b>supervised</b> fashion. These pre-trained PFGM++ are subsequently <b>&ldquo;distilled&rdquo;</b> into Poisson flow consistency models (PFCM) via an updated version of consistency <b>distillation.</b> We call this approach posterior sampling Poisson flow consistency models (PS-PFCM). Our results indicate that the added flexibility of tuning the hyperparameter D, the dimensionality of the augmentation variables in PFGM++, allows us to outperform consistency models, a current state-of-the-art diffusion-style model with NFE=1 on clinical low-dose CT images. Notably, PFCM is in itself a novel family of deep generative models and we provide initial results on the CIFAR-10 dataset.</p></p class="citation"></blockquote><h3 id=25--192247-convolutional-neural-networks-towards-facial-skin-lesions-detection-reza-sarshar-et-al-2024>(2/5 | 192/247) Convolutional Neural Networks Towards Facial Skin Lesions Detection (Reza Sarshar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Reza Sarshar, Mohammad Heydari, Elham Akhondzadeh Noughabi. (2024)<br><strong>Convolutional Neural Networks Towards Facial Skin Lesions Detection</strong><br><button class=copy-to-clipboard title="Convolutional Neural Networks Towards Facial Skin Lesions Detection" index=192>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-192 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, cs-LG, eess-IV, eess.IV<br>Keyword Score: 30<br>Keywords: Convolution, Convolutional Neural Network, Transfer Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08592v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08592v1.pdf filename=2402.08592v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Facial analysis has emerged as a prominent area of research with diverse applications, including cosmetic surgery programs, the beauty industry, photography, and entertainment. Manipulating patient images often necessitates professional image processing software. This study contributes by providing a model that facilitates the detection of blemishes and skin lesions on facial images through a <b>convolutional</b> <b>neural</b> <b>network</b> and machine learning approach. The proposed method offers advantages such as simple architecture, speed and suitability for image processing while avoiding the complexities associated with traditional methods. The model comprises four main steps: area selection, scanning the chosen region, lesion diagnosis, and marking the identified lesion. Raw data for this research were collected from a reputable clinic in Tehran specializing in skincare and beauty services. The dataset includes administrative information, clinical data, and facial and profile images. A total of 2300 patient images were extracted from this raw data. A software tool was developed to crop and label lesions, with input from two treatment experts. In the lesion preparation phase, the selected area was standardized to 50 * 50 pixels. Subsequently, a <b>convolutional</b> <b>neural</b> <b>network</b> model was employed for lesion labeling. The classification model demonstrated high accuracy, with a measure of 0.98 for healthy skin and 0.97 for lesioned skin specificity. Internal validation involved performance indicators and cross-validation, while external validation compared the model&rsquo;s performance indicators with those of the <b>transfer</b> <b>learning</b> method using the Vgg16 deep network model. Compared to existing studies, the results of this research showcase the efficacy and desirability of the proposed model and methodology.</p></p class="citation"></blockquote><h3 id=35--193247-adversarially-robust-feature-learning-for-breast-cancer-diagnosis-degan-hao-et-al-2024>(3/5 | 193/247) Adversarially Robust Feature Learning for Breast Cancer Diagnosis (Degan Hao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Degan Hao, Dooman Arefan, Margarita Zuley, Wendie Berg, Shandong Wu. (2024)<br><strong>Adversarially Robust Feature Learning for Breast Cancer Diagnosis</strong><br><button class=copy-to-clipboard title="Adversarially Robust Feature Learning for Breast Cancer Diagnosis" index=193>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-193 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-LG, eess-IV, eess.IV<br>Keyword Score: 20<br>Keywords: Adversarial Learning, Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08768v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08768v1.pdf filename=2402.08768v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Adversarial</b> <b>data</b> can lead to malfunction of deep learning applications. It is essential to develop deep learning models that are robust to <b>adversarial</b> <b>data</b> while accurate on standard, clean data. In this study, we proposed a novel adversarially robust feature learning (ARFL) method for a real-world application of breast cancer diagnosis. ARFL facilitates <b>adversarial</b> <b>training</b> using both standard data and <b>adversarial</b> <b>data,</b> where a feature correlation measure is incorporated as an objective function to encourage learning of robust features and restrain spurious features. To show the effects of ARFL in breast cancer diagnosis, we built and evaluated diagnosis models using two independent clinically collected breast imaging datasets, comprising a total of 9,548 mammogram images. We performed extensive experiments showing that our method outperformed several state-of-the-art methods and that our method can enhance safer breast cancer diagnosis against <b>adversarial</b> <b>attacks</b> in clinical settings.</p></p class="citation"></blockquote><h3 id=45--194247-rethinking-u-net-skip-connections-for-biomedical-image-segmentation-frauke-wilm-et-al-2024>(4/5 | 194/247) Rethinking U-net Skip Connections for Biomedical Image Segmentation (Frauke Wilm et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Frauke Wilm, Jonas Ammeling, Mathias Öttl, Rutger H. J. Fick, Marc Aubreville, Katharina Breininger. (2024)<br><strong>Rethinking U-net Skip Connections for Biomedical Image Segmentation</strong><br><button class=copy-to-clipboard title="Rethinking U-net Skip Connections for Biomedical Image Segmentation" index=194>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-194 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 10<br>Keywords: Out-of-distribution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08276v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08276v1.pdf filename=2402.08276v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The U-net architecture has significantly impacted deep learning-based segmentation of medical images. Through the integration of long-range skip connections, it facilitated the preservation of high-resolution features. <b>Out-of-distribution</b> data can, however, substantially impede the performance of neural networks. Previous works showed that the trained network layers differ in their susceptibility to this domain shift, e.g., shallow layers are more affected than deeper layers. In this work, we investigate the implications of this observation of layer sensitivity to domain shifts of U-net-style segmentation networks. By copying features of shallow layers to corresponding decoder blocks, these bear the risk of re-introducing domain-specific information. We used a synthetic dataset to model different levels of data distribution shifts and evaluated the impact on downstream segmentation performance. We quantified the inherent domain susceptibility of each network layer, using the Hellinger distance. These experiments confirmed the higher domain susceptibility of earlier network layers. When gradually removing skip connections, a decrease in domain susceptibility of deeper layers could be observed. For downstream segmentation performance, the original U-net outperformed the variant without any skip connections. The best performance, however, was achieved when removing the uppermost skip connection - not only in the presence of domain shifts but also for in-domain test data. We validated our results on three clinical datasets - two histopathology datasets and one magnetic resonance dataset - with performance increases of up to 10% in-domain and 13% cross-domain when removing the uppermost skip connection.</p></p class="citation"></blockquote><h3 id=55--195247-deep-and-shallow-data-science-for-multi-scale-optical-neuroscience-gal-mishne-et-al-2024>(5/5 | 195/247) Deep and shallow data science for multi-scale optical neuroscience (Gal Mishne et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Gal Mishne, Adam Charles. (2024)<br><strong>Deep and shallow data science for multi-scale optical neuroscience</strong><br><button class=copy-to-clipboard title="Deep and shallow data science for multi-scale optical neuroscience" index=195>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-195 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-LG, eess-IV, eess.IV, q-bio-QM<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08811v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08811v1.pdf filename=2402.08811v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Optical imaging of the brain has expanded dramatically in the past two decades. New optics, indicators, and experimental paradigms are now enabling in-vivo imaging from the synaptic to the cortex-wide scales. To match the resulting flood of data across scales, computational methods are continuously being developed to meet the need of extracting biologically relevant information. In this pursuit, challenges arise in some domains (e.g., SNR and resolution limits in micron-scale data) that require specialized algorithms. These algorithms can, for example, make use of state-of-the-art machine learning to maximally learn the details of a given scale to optimize the processing pipeline. In contrast, other methods, however, such as <b>graph</b> signal processing, seek to abstract away from some of the details that are scale-specific to provide solutions to specific sub-problems common across scales of neuroimaging. Here we discuss limitations and tradeoffs in algorithmic design with the goal of identifying how data quality and variability can hamper algorithm use and dissemination.</p></p class="citation"></blockquote><h2 id=quant-ph-4>quant-ph (4)</h2><h3 id=14--196247-trained-quantum-neural-networks-are-gaussian-processes-filippo-girardi-et-al-2024>(1/4 | 196/247) Trained quantum neural networks are Gaussian processes (Filippo Girardi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Filippo Girardi, Giacomo De Palma. (2024)<br><strong>Trained quantum neural networks are Gaussian processes</strong><br><button class=copy-to-clipboard title="Trained quantum neural networks are Gaussian processes" index=196>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-196 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: quant-ph<br>Categories: cs-LG, math-MP, math-PR, math-ph, quant-ph, quant-ph<br>Keyword Score: 35<br>Keywords: Gaussian Process, Square Loss, Supervised Learning, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08726v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08726v1.pdf filename=2402.08726v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We study quantum neural networks made by parametric one-qubit gates and fixed two-qubit gates in the limit of infinite width, where the generated function is the expectation value of the sum of single-qubit observables over all the qubits. First, we prove that the probability distribution of the function generated by the untrained network with randomly initialized parameters converges in distribution to a <b>Gaussian</b> <b>process</b> whenever each measured qubit is correlated only with few other measured qubits. Then, we analytically characterize the training of the network via gradient descent with <b>square</b> <b>loss</b> on <b>supervised</b> <b>learning</b> problems. We prove that, as long as the network is not affected by barren plateaus, the trained network can perfectly fit the training set and that the probability distribution of the function generated after training still converges in distribution to a <b>Gaussian</b> <b>process.</b> Finally, we consider the statistical noise of the measurement at the output of the network and prove that a polynomial number of measurements is sufficient for all the previous results to hold and that the network can always be trained in polynomial time.</p></p class="citation"></blockquote><h3 id=24--197247-edge-coloring-lattice-graphs-joris-kattemölle-2024>(2/4 | 197/247) Edge coloring lattice graphs (Joris Kattemölle, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Joris Kattemölle. (2024)<br><strong>Edge coloring lattice graphs</strong><br><button class=copy-to-clipboard title="Edge coloring lattice graphs" index=197>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-197 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: quant-ph<br>Categories: G-2-2, cs-DM, math-CO, quant-ph, quant-ph<br>Keyword Score: 23<br>Keywords: Graph, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08752v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08752v1.pdf filename=2402.08752v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We develop the theory of the edge coloring of infinite lattice <b>graphs,</b> proving a necessary and sufficient condition for a proper edge coloring of a patch of a lattice <b>graph</b> to induce a proper edge coloring of the entire lattice <b>graph</b> by translation. This condition forms the cornerstone of a method that finds nearly minimal or minimal edge colorings of infinite lattice <b>graphs.</b> In case a nearly minimal edge coloring is requested, the running time is $O(\mu^2 D^4)$, where $\mu$ is the number of edges in one cell (or `basis <b>graph&rsquo;)</b> of the lattice <b>graph</b> and $D$ is the maximum distance between two cells so that there is an edge from within one cell to the other. In case a minimal edge coloring is requested, we lack an upper bound on the running time, which we find need not pose a limitation in practice; we use the method to minimal edge color the meshes of all $k$-uniform tilings of the plane for $k\leq 6$, while utilizing modest computational resources. We find that all these lattice <b>graphs</b> are Vizing class~I. Relating edge colorings to quantum circuits, our work finds direct application by offering minimal-depth quantum circuits in the areas of quantum <b>simulation,</b> quantum optimization, and quantum state verification.</p></p class="citation"></blockquote><h3 id=34--198247-arbitrary-polynomial-separations-in-trainable-quantum-machine-learning-eric-r-anschuetz-et-al-2024>(3/4 | 198/247) Arbitrary Polynomial Separations in Trainable Quantum Machine Learning (Eric R. Anschuetz et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Eric R. Anschuetz, Xun Gao. (2024)<br><strong>Arbitrary Polynomial Separations in Trainable Quantum Machine Learning</strong><br><button class=copy-to-clipboard title="Arbitrary Polynomial Separations in Trainable Quantum Machine Learning" index=198>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-198 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: quant-ph<br>Categories: cs-LG, quant-ph, quant-ph<br>Keyword Score: 20<br>Keywords: Recurrent Neural Network, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08606v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08606v1.pdf filename=2402.08606v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent theoretical results in quantum machine learning have demonstrated a general trade-off between the expressive power of quantum neural networks (QNNs) and their trainability; as a corollary of these results, practical exponential separations in expressive power over classical machine learning models are believed to be infeasible as such QNNs take a time to train that is exponential in the model size. We here circumvent these negative results by constructing a hierarchy of efficiently trainable QNNs that exhibit unconditionally provable, polynomial memory separations of arbitrary constant degree over classical neural networks in performing a classical sequence modeling task. Furthermore, each unit cell of the introduced class of QNNs is computationally efficient, implementable in constant time on a quantum device. The classical networks we prove a separation over include well-known examples such as <b>recurrent</b> <b>neural</b> <b>networks</b> and <b>Transformers.</b> We show that quantum contextuality is the source of the expressivity separation, suggesting that other classical sequence learning problems with long-time correlations may be a regime where practical advantages in quantum machine learning may exist.</p></p class="citation"></blockquote><h3 id=44--199247-on-black-box-separations-of-quantum-digital-signatures-from-pseudorandom-states-andrea-coladangelo-et-al-2024>(4/4 | 199/247) On black-box separations of quantum digital signatures from pseudorandom states (Andrea Coladangelo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Andrea Coladangelo, Saachi Mutreja. (2024)<br><strong>On black-box separations of quantum digital signatures from pseudorandom states</strong><br><button class=copy-to-clipboard title="On black-box separations of quantum digital signatures from pseudorandom states" index=199>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-199 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: quant-ph<br>Categories: cs-CR, quant-ph, quant-ph<br>Keyword Score: 5<br>Keywords: Black Box<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08194v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08194v1.pdf filename=2402.08194v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>It is well-known that digital signatures can be constructed from one-way functions in a <b>black-box</b> <b>way.</b> While one-way functions are essentially the minimal assumption in classical cryptography, this is not the case in the quantum setting. A variety of qualitatively weaker and inherently quantum assumptions (e.g. EFI pairs, one-way state generators, and pseudorandom states) are known to be sufficient for non-trivial quantum cryptography. While it is known that commitments, zero-knowledge proofs, and even multiparty computation can be constructed from these assumptions, it has remained an open question whether the same is true for quantum digital signatures schemes (QDS). In this work, we show that there $\textit{does not}$ exist a <b>black-box</b> <b>construction</b> of a QDS scheme with classical signatures from pseudorandom states with linear, or greater, output length. Our result complements that of Morimae and Yamakawa (2022), who described a $\textit{one-time}$ secure QDS scheme with classical signatures, but left open the question of constructing a standard $\textit{multi-time}$ secure one.</p></p class="citation"></blockquote><h2 id=eessas-3>eess.AS (3)</h2><h3 id=13--200247-leveraging-cough-sounds-to-optimize-chest-x-ray-usage-in-low-resource-settings-alexander-philip-et-al-2024>(1/3 | 200/247) Leveraging cough sounds to optimize chest x-ray usage in low-resource settings (Alexander Philip et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Alexander Philip, Sanya Chawla, Lola Jover, George P. Kafentzis, Joe Brew, Vishakh Saraf, Shibu Vijayan, Peter Small, Carlos Chaccour. (2024)<br><strong>Leveraging cough sounds to optimize chest x-ray usage in low-resource settings</strong><br><button class=copy-to-clipboard title="Leveraging cough sounds to optimize chest x-ray usage in low-resource settings" index=200>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-200 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.AS<br>Categories: cs-AI, cs-LG, eess-AS, eess.AS, q-bio-QM<br>Keyword Score: 33<br>Keywords: Logistic Regression, Low-Resource, Sample Size, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08789v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08789v1.pdf filename=2402.08789v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Chest X-ray is a commonly used tool during triage, diagnosis and management of respiratory diseases. In resource-constricted settings, optimizing this resource can lead to valuable cost savings for the health care system and the patients as well as to and improvement in consult time. We used prospectively-collected data from 137 patients referred for chest X-ray at the Christian Medical Center and Hospital (CMCH) in Purnia, Bihar, India. Each patient provided at least five coughs while awaiting radiography. Collected cough sounds were analyzed using acoustic AI methods. Cross-validation was done on temporal and spectral features on the cough sounds of each patient. Features were <b>summarized</b> using standard statistical approaches. Three models were developed, tested and compared in their capacity to predict an abnormal result in the chest X-ray. All three methods yielded models that could discriminate to some extent between normal and abnormal with the <b>logistic</b> <b>regression</b> performing best with an area under the receiver operating characteristic curves ranging from 0.7 to 0.78. Despite limitations and its relatively small <b>sample</b> <b>size,</b> this study shows that AI-enabled algorithms can use cough sounds to predict which individuals presenting for chest radiographic examination will have a normal or abnormal results. These results call for expanding this research given the potential optimization of limited health care resources in low- and middle-income countries.</p></p class="citation"></blockquote><h3 id=23--201247-channel-combination-algorithms-for-robust-distant-voice-activity-and-overlapped-speech-detection-théo-mariotte-et-al-2024>(2/3 | 201/247) Channel-Combination Algorithms for Robust Distant Voice Activity and Overlapped Speech Detection (Théo Mariotte et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Théo Mariotte, Anthony Larcher, Silvio Montrésor, Jean-Hugh Thomas. (2024)<br><strong>Channel-Combination Algorithms for Robust Distant Voice Activity and Overlapped Speech Detection</strong><br><button class=copy-to-clipboard title="Channel-Combination Algorithms for Robust Distant Voice Activity and Overlapped Speech Detection" index=201>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-201 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.AS<br>Categories: cs-SD, eess-AS, eess.AS<br>Keyword Score: 30<br>Keywords: Supervised Learning, Supervised Learning, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08312v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08312v1.pdf filename=2402.08312v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Voice Activity Detection (VAD) and Overlapped Speech Detection (OSD) are key pre-processing tasks for speaker diarization. In the meeting context, it is often easier to capture speech with a distant device. This consideration however leads to severe performance degradation. We study a unified <b>supervised</b> <b>learning</b> framework to solve distant multi-microphone joint VAD and OSD (VAD+OSD). This paper investigates various multi-channel VAD+OSD front-ends that weight and combine incoming channels. We propose three algorithms based on the <b>Self-Attention</b> Channel Combinator (SACC), previously proposed in the literature. Experiments conducted on the AMI meeting corpus exhibit that channel combination approaches bring significant VAD+OSD improvements in the distant speech scenario. Specifically, we explore the use of learned complex combination weights and demonstrate the benefits of such an approach in terms of explainability. Channel combination-based VAD+OSD systems are evaluated on the final back-end task, i.e. speaker diarization, and show significant improvements. Finally, since multi-channel systems are trained given a fixed array configuration, they may fail in generalizing to other array set-ups, e.g. mismatched number of microphones. A channel-number invariant loss is proposed to learn a unique feature representation regardless of the number of available microphones. The evaluation conducted on mismatched array configurations highlights the robustness of this training strategy.</p></p class="citation"></blockquote><h3 id=33--202247-unrestricted-global-phase-bias-aware-single-channel-speech-enhancement-with-conformer-based-metric-gan-shiqi-zhang-et-al-2024>(3/3 | 202/247) Unrestricted Global Phase Bias-Aware Single-channel Speech Enhancement with Conformer-based Metric GAN (Shiqi Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shiqi Zhang, Zheng Qiu, Daiki Takeuchi, Noboru Harada, Shoji Makino. (2024)<br><strong>Unrestricted Global Phase Bias-Aware Single-channel Speech Enhancement with Conformer-based Metric GAN</strong><br><button class=copy-to-clipboard title="Unrestricted Global Phase Bias-Aware Single-channel Speech Enhancement with Conformer-based Metric GAN" index=202>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-202 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.AS<br>Categories: cs-SD, eess-AS, eess.AS<br>Keyword Score: 20<br>Keywords: Generative Adversarial Network, Generative Adversarial Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08252v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08252v1.pdf filename=2402.08252v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the rapid development of neural networks in recent years, the ability of various networks to enhance the magnitude spectrum of noisy speech in the single-channel speech enhancement domain has become exceptionally outstanding. However, enhancing the phase spectrum using neural networks is often ineffective, which remains a challenging problem. In this paper, we found that the human ear cannot sensitively perceive the difference between a precise phase spectrum and a biased phase (BP) spectrum. Therefore, we propose an optimization method of phase reconstruction, allowing freedom on the global-phase bias instead of reconstructing the precise phase spectrum. We applied it to a Conformer-based Metric <b>Generative</b> <b>Adversarial</b> <b>Networks</b> (CMGAN) baseline model, which relaxes the existing constraints of precise phase and gives the neural network a broader learning space. Results show that this method achieves a new state-of-the-art performance without incurring additional computational overhead.</p></p class="citation"></blockquote><h2 id=q-biogn-1>q-bio.GN (1)</h2><h3 id=11--203247-dnabert-s-learning-species-aware-dna-embedding-with-genome-foundation-models-zhihan-zhou-et-al-2024>(1/1 | 203/247) DNABERT-S: Learning Species-Aware DNA Embedding with Genome Foundation Models (Zhihan Zhou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhihan Zhou, Weimin Wu, Harrison Ho, Jiayi Wang, Lizhen Shi, Ramana V Davuluri, Zhong Wang, Han Liu. (2024)<br><strong>DNABERT-S: Learning Species-Aware DNA Embedding with Genome Foundation Models</strong><br><button class=copy-to-clipboard title="DNABERT-S: Learning Species-Aware DNA Embedding with Genome Foundation Models" index=203>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-203 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: q-bio.GN<br>Categories: cs-AI, cs-CE, cs-CL, q-bio-GN, q-bio.GN<br>Keyword Score: 33<br>Keywords: Clustering, Contrastive Learning, Fine-tuning, Foundation Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08777v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08777v2.pdf filename=2402.08777v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Effective DNA embedding remains crucial in genomic analysis, particularly in scenarios lacking labeled data for model <b>fine-tuning,</b> despite the significant advancements in genome <b>foundation</b> <b>models.</b> A prime example is metagenomics binning, a critical process in microbiome research that aims to group DNA sequences by their species from a complex mixture of DNA sequences derived from potentially thousands of distinct, often uncharacterized species. To fill the lack of effective DNA embedding models, we introduce DNABERT-S, a genome <b>foundation</b> <b>model</b> that specializes in creating species-aware DNA embeddings. To encourage effective embeddings to error-prone long-read DNA sequences, we introduce Manifold Instance Mixup (MI-Mix), a <b>contrastive</b> <b>objective</b> that mixes the hidden representations of DNA sequences at randomly selected layers and trains the model to recognize and differentiate these mixed proportions at the output layer. We further enhance it with the proposed Curriculum <b>Contrastive</b> <b>Learning</b> (C$^2$LR) strategy. Empirical results on 18 diverse datasets showed DNABERT-S&rsquo;s remarkable performance. It outperforms the top baseline&rsquo;s performance in 10-shot species classification with just a 2-shot training while doubling the Adjusted Rand Index (ARI) in species <b>clustering</b> and substantially increasing the number of correctly identified species in metagenomics binning. The code, data, and pre-trained model are publicly available at <a href=https://github.com/Zhihan1996/DNABERT_S>https://github.com/Zhihan1996/DNABERT_S</a>.</p></p class="citation"></blockquote><h2 id=statml-6>stat.ML (6)</h2><h3 id=16--204247-space-time-bridge-diffusion-hamidreza-behjoo-et-al-2024>(1/6 | 204/247) Space-Time Bridge-Diffusion (Hamidreza Behjoo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hamidreza Behjoo, Michael Chertkov. (2024)<br><strong>Space-Time Bridge-Diffusion</strong><br><button class=copy-to-clipboard title="Space-Time Bridge-Diffusion" index=204>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-204 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-LG, stat-ML, stat.ML<br>Keyword Score: 30<br>Keywords: Fine-tuning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08847v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08847v1.pdf filename=2402.08847v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this study, we introduce a novel method for generating new synthetic samples that are independent and identically distributed (i.i.d.) from high-dimensional real-valued probability distributions, as defined implicitly by a set of Ground Truth (GT) samples. Central to our method is the integration of space-time mixing strategies that extend across temporal and spatial dimensions. Our methodology is underpinned by three interrelated stochastic processes designed to enable optimal transport from an easily tractable initial probability distribution to the target distribution represented by the GT samples: (a) linear processes incorporating space-time mixing that yield Gaussian conditional probability densities, (b) their bridge-diffusion analogs that are conditioned to the initial and final state vectors, and (c) nonlinear stochastic processes refined through score-matching techniques. The crux of our training regime involves <b>fine-tuning</b> the nonlinear model, and potentially the linear models - to align closely with the GT data. We validate the efficacy of our space-time diffusion approach with numerical experiments, laying the groundwork for more extensive future theory and experiments to fully authenticate the method, particularly providing a more efficient (possibly <b>simulation-free)</b> inference.</p></p class="citation"></blockquote><h3 id=26--205247-on-limitations-of-the-transformer-architecture-binghui-peng-et-al-2024>(2/6 | 205/247) On Limitations of the Transformer Architecture (Binghui Peng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Binghui Peng, Srini Narayanan, Christos Papadimitriou. (2024)<br><strong>On Limitations of the Transformer Architecture</strong><br><button class=copy-to-clipboard title="On Limitations of the Transformer Architecture" index=205>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-205 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-AI, cs-LG, stat-ML, stat.ML<br>Keyword Score: 30<br>Keywords: Transformer, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08164v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08164v1.pdf filename=2402.08164v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>What are the root causes of hallucinations in <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)?</b> We use Communication Complexity to prove that the <b>Transformer</b> layer is incapable of composing functions (e.g., identify a grandparent of a person in a genealogy) if the domains of the functions are <b>large</b> <b>enough;</b> <b>we</b> show through examples that this inability is already empirically present when the domains are quite small. We also point out that several mathematical tasks that are at the core of the so-called compositional tasks thought to be hard for <b>LLMs</b> are unlikely to be solvable by <b>Transformers,</b> for <b>large</b> <b>enough</b> <b>instances</b> and assuming that certain well accepted conjectures in the field of Computational Complexity are true.</p></p class="citation"></blockquote><h3 id=36--206247-implicit-bias-in-noisy-sgd-with-applications-to-differentially-private-training-tom-sander-et-al-2024>(3/6 | 206/247) Implicit Bias in Noisy-SGD: With Applications to Differentially Private Training (Tom Sander et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tom Sander, Maxime Sylvestre, Alain Durmus. (2024)<br><strong>Implicit Bias in Noisy-SGD: With Applications to Differentially Private Training</strong><br><button class=copy-to-clipboard title="Implicit Bias in Noisy-SGD: With Applications to Differentially Private Training" index=206>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-206 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-LG, stat-ML, stat.ML<br>Keyword Score: 20<br>Keywords: Stochastic Gradient Descent, Stochastic Gradient Descent<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08344v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08344v1.pdf filename=2402.08344v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Training Deep Neural Networks (DNNs) with small batches using <b>Stochastic</b> <b>Gradient</b> <b>Descent</b> <b>(SGD)</b> yields superior test performance compared to larger batches. The specific noise structure inherent to <b>SGD</b> is known to be responsible for this implicit bias. DP-SGD, used to ensure differential privacy (DP) in DNNs&rsquo; training, adds Gaussian noise to the clipped gradients. Surprisingly, large-batch training still results in a significant decrease in performance, which poses an important challenge because strong DP guarantees necessitate the use of massive batches. We first show that the phenomenon extends to Noisy-SGD (DP-SGD without clipping), suggesting that the stochasticity (and not the clipping) is the cause of this implicit bias, even with additional isotropic Gaussian noise. We theoretically analyse the solutions obtained with continuous versions of Noisy-SGD for the Linear Least Square and Diagonal Linear Network settings, and reveal that the implicit bias is indeed amplified by the additional noise. Thus, the performance issues of large-batch DP-SGD training are rooted in the same underlying principles as <b>SGD,</b> offering hope for potential improvements in large batch training strategies.</p></p class="citation"></blockquote><h3 id=46--207247-off-policy-evaluation-in-markov-decision-processes-under-weak-distributional-overlap-mohammad-mehrabi-et-al-2024>(4/6 | 207/247) Off-Policy Evaluation in Markov Decision Processes under Weak Distributional Overlap (Mohammad Mehrabi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mohammad Mehrabi, Stefan Wager. (2024)<br><strong>Off-Policy Evaluation in Markov Decision Processes under Weak Distributional Overlap</strong><br><button class=copy-to-clipboard title="Off-Policy Evaluation in Markov Decision Processes under Weak Distributional Overlap" index=207>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-207 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-LG, stat-ML, stat.ML<br>Keyword Score: 20<br>Keywords: Markov Decision Process, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08201v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08201v1.pdf filename=2402.08201v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Doubly robust methods hold considerable promise for off-policy evaluation in Markov decision processes <b>(MDPs)</b> under sequential ignorability: They have been shown to converge as $1/\sqrt{T}$ with the horizon $T$, to be statistically efficient in large samples, and to allow for modular implementation where preliminary estimation tasks can be executed using standard <b>reinforcement</b> <b>learning</b> techniques. Existing results, however, make heavy use of a strong distributional overlap assumption whereby the stationary distributions of the target policy and the data-collection policy are within a bounded factor of each other &ndash; and this assumption is typically only credible when the state space of the MDP is bounded. In this paper, we re-visit the task of off-policy evaluation in <b>MDPs</b> under a weaker notion of distributional overlap, and introduce a class of truncated doubly robust (TDR) estimators which we find to perform well in this setting. When the distribution ratio of the target and data-collection policies is square-integrable (but not necessarily bounded), our approach recovers the large-sample behavior previously established under strong distributional overlap. When this ratio is not square-integrable, TDR is still consistent but with a slower-than-$1/\sqrt{T}$; furthermore, this rate of convergence is minimax over a class of <b>MDPs</b> defined only using mixing conditions. We validate our approach numerically and find that, in our experiments, appropriate truncation plays a major role in enabling accurate off-policy evaluation when strong distributional overlap does not hold.</p></p class="citation"></blockquote><h3 id=56--208247-corridor-geometry-in-gradient-based-optimization-benoit-dherin-et-al-2024>(5/6 | 208/247) Corridor Geometry in Gradient-Based Optimization (Benoit Dherin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Benoit Dherin, Mihaela Rosca. (2024)<br><strong>Corridor Geometry in Gradient-Based Optimization</strong><br><button class=copy-to-clipboard title="Corridor Geometry in Gradient-Based Optimization" index=208>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-208 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-LG, math-OC, stat-ML, stat.ML<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08818v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08818v1.pdf filename=2402.08818v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We characterize regions of a loss surface as corridors when the continuous curves of steepest descent &ndash; the solutions of the gradient flow &ndash; become straight lines. We show that corridors provide insights into gradient-based optimization, since corridors are exactly the regions where gradient descent and the gradient flow follow the same trajectory, while the loss decreases linearly. As a result, inside corridors there are no implicit regularization effects or training instabilities that have been shown to occur due to the drift between gradient descent and the gradient flow. Using the loss linear decrease on corridors, we devise a learning rate adaptation scheme for gradient descent; we call this scheme Corridor Learning Rate (CLR). The CLR formulation coincides with a special case of Polyak step-size, discovered in the context of convex optimization. The Polyak step-size has been shown recently to have also good convergence properties for neural networks; we further confirm this here with results on CIFAR-10 and ImageNet.</p></p class="citation"></blockquote><h3 id=66--209247-adjustment-identification-distance-a-gadjid-for-causal-structure-learning-leonard-henckel-et-al-2024>(6/6 | 209/247) Adjustment Identification Distance: A gadjid for Causal Structure Learning (Leonard Henckel et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Leonard Henckel, Theo Würtzen, Sebastian Weichwald. (2024)<br><strong>Adjustment Identification Distance: A gadjid for Causal Structure Learning</strong><br><button class=copy-to-clipboard title="Adjustment Identification Distance: A gadjid for Causal Structure Learning" index=209>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-209 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-LG, stat-ME, stat-ML, stat.ML<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08616v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08616v1.pdf filename=2402.08616v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Evaluating <b>graphs</b> learned by causal discovery algorithms is difficult: The number of edges that differ between two <b>graphs</b> does not reflect how the <b>graphs</b> differ with respect to the identifying formulas they suggest for causal effects. We introduce a framework for developing causal distances between <b>graphs</b> which includes the structural intervention distance for directed acyclic <b>graphs</b> as a special case. We use this framework to develop improved adjustment-based distances as well as extensions to completed partially directed acyclic <b>graphs</b> and causal orders. We develop polynomial-time reachability algorithms to compute the distances efficiently. In our package gadjid (open source at <a href=https://github.com/CausalDisco/gadjid)>https://github.com/CausalDisco/gadjid)</a>, we provide implementations of our distances; they are orders of magnitude faster than the structural intervention distance and thereby provide a success metric for causal discovery that scales to <b>graph</b> sizes that were previously prohibitive.</p></p class="citation"></blockquote><h2 id=csce-1>cs.CE (1)</h2><h3 id=11--210247-a-projection-based-time-segmented-reduced-order-model-for-fluid-structure-interactions-qijia-zhai-et-al-2024>(1/1 | 210/247) A Projection-Based Time-Segmented Reduced Order Model for Fluid-Structure Interactions (Qijia Zhai et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qijia Zhai, Shiquan Zhang, Pengtao Sun, Xiaoping Xie. (2024)<br><strong>A Projection-Based Time-Segmented Reduced Order Model for Fluid-Structure Interactions</strong><br><button class=copy-to-clipboard title="A Projection-Based Time-Segmented Reduced Order Model for Fluid-Structure Interactions" index=210>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-210 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CE<br>Categories: cs-CE, cs-NA, cs.CE, math-NA<br>Keyword Score: 23<br>Keywords: Benchmarking, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08172v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08172v2.pdf filename=2402.08172v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, a type of novel projection-based, time-segmented reduced order model (ROM) is proposed for dynamic fluid-structure interaction (FSI) problems based upon the arbitrary Lagrangian&ndash;Eulerian (ALE)-finite element method (FEM) in a monolithic frame, where spatially, each variable is separated from others in terms of their attribution (fluid/structure), category (velocity/pressure) and component (horizontal/vertical) while temporally, the proper orthogonal decomposition (POD) bases are constructed in some deliberately partitioned time segments tailored through extensive numerical trials. By the combination of spatial and temporal decompositions, the developed ROM approach enables prolonged <b>simulations</b> under prescribed accuracy thresholds. Numerical experiments are carried out to compare numerical performances of the proposed ROM with corresponding full-order model (FOM) by solving a two-dimensional FSI <b>benchmark</b> problem that involves a vibrating elastic beam in the fluid, where the performance of offline ROM on perturbed physical parameters in the online phase is investigated as well. Extensive numerical results demonstrate that the proposed ROM has a comparable accuracy to while much higher efficiency than the FOM. The developed ROM approach is dimension-independent and can be seamlessly extended to solve high dimensional FSI problems.</p></p class="citation"></blockquote><h2 id=csgt-5>cs.GT (5)</h2><h3 id=15--211247-equilibria-of-data-marketplaces-with-privacy-aware-sellers-under-endogenous-privacy-costs-diptangshu-sen-et-al-2024>(1/5 | 211/247) Equilibria of Data Marketplaces with Privacy-Aware Sellers under Endogenous Privacy Costs (Diptangshu Sen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Diptangshu Sen, Jingyan Wang, Juba Ziani. (2024)<br><strong>Equilibria of Data Marketplaces with Privacy-Aware Sellers under Endogenous Privacy Costs</strong><br><button class=copy-to-clipboard title="Equilibria of Data Marketplaces with Privacy-Aware Sellers under Endogenous Privacy Costs" index=211>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-211 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.GT<br>Categories: cs-GT, cs.GT<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08826v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08826v1.pdf filename=2402.08826v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We study a two-sided online data ecosystem comprised of an online platform, users on the platform, and downstream learners or data buyers. The learners can buy user data on the platform (to run a statistic or machine learning task). Potential users decide whether to join by looking at the trade-off between i) their benefit from joining the platform and interacting with other users and ii) the privacy costs they incur from sharing their data. First, we introduce a novel modeling element for two-sided data platforms: the privacy costs of the users are endogenous and depend on how much of their data is purchased by the downstream learners. Then, we characterize marketplace equilibria in certain simple settings. In particular, we provide a full characterization in two variants of our model that correspond to different utility functions for the users: i) when each user gets a constant benefit for participating in the platform and ii) when each user&rsquo;s benefit is linearly increasing in the number of other users that participate. In both variants, equilibria in our setting are significantly different from equilibria when privacy costs are exogenous and fixed, highlighting the importance of taking endogeneity in the privacy costs into account. Finally, we provide <b>simulations</b> and semi-synthetic experiments to extend our results to more general assumptions. We experiment with different distributions of users&rsquo; privacy costs and different functional forms of the users&rsquo; utilities for joining the platform.</p></p class="citation"></blockquote><h3 id=25--212247-nfgtransformer-equivariant-representation-learning-for-normal-form-games-siqi-liu-et-al-2024>(2/5 | 212/247) NfgTransformer: Equivariant Representation Learning for Normal-form Games (Siqi Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Siqi Liu, Luke Marris, Georgios Piliouras, Ian Gemp, Nicolas Heess. (2024)<br><strong>NfgTransformer: Equivariant Representation Learning for Normal-form Games</strong><br><button class=copy-to-clipboard title="NfgTransformer: Equivariant Representation Learning for Normal-form Games" index=212>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-212 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.GT<br>Categories: cs-GT, cs.GT<br>Keyword Score: 15<br>Keywords: Representation Learning, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08393v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08393v1.pdf filename=2402.08393v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Normal-form games (NFGs) are the fundamental model of strategic interaction. We study their <b>representation</b> <b>using</b> neural networks. We describe the inherent equivariance of NFGs &ndash; any permutation of strategies describes an equivalent game &ndash; as well as the challenges this poses for <b>representation</b> <b>learning.</b> We then propose the NfgTransformer architecture that leverages this equivariance, leading to state-of-the-art performance in a range of game-theoretic tasks including equilibrium-solving, deviation gain estimation and ranking, with a common approach to NFG <b>representation.</b> <b>We</b> show that the resulting model is interpretable and versatile, paving the way towards deep learning systems capable of game-theoretic <b>reasoning</b> when interacting with humans and with each other.</p></p class="citation"></blockquote><h3 id=35--213247-grace-period-is-all-you-need-individual-fairness-without-revenue-loss-in-revenue-management-patrick-jaillet-et-al-2024>(3/5 | 213/247) Grace Period is All You Need: Individual Fairness without Revenue Loss in Revenue Management (Patrick Jaillet et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Patrick Jaillet, Chara Podimata, Zijie Zhou. (2024)<br><strong>Grace Period is All You Need: Individual Fairness without Revenue Loss in Revenue Management</strong><br><button class=copy-to-clipboard title="Grace Period is All You Need: Individual Fairness without Revenue Loss in Revenue Management" index=213>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-213 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.GT<br>Categories: cs-GT, cs.GT, math-OC<br>Keyword Score: 10<br>Keywords: Fairness<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08533v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08533v1.pdf filename=2402.08533v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Imagine you and a friend purchase identical items at a store, yet only your friend received a discount. Would your friend&rsquo;s discount make you feel unfairly treated by the store? And would you be less willing to purchase from that store again in the future? Based on a large-scale online survey that we ran on Prolific, it turns out that the answers to the above questions are positive. Motivated by these findings, in this work we propose a notion of individual <b>fairness</b> in online revenue management and an algorithmic module (called ``Grace Period&rsquo;&rsquo;) that can be embedded in traditional revenue management algorithms and guarantee individual <b>fairness.</b> Specifically, we show how to embed the Grace Period in five common revenue management algorithms including Deterministic Linear Programming with Probabilistic Assignment, Resolving Deterministic Linear Programming with Probabilistic Assignment, Static Bid Price Control, Booking Limit, and Nesting, thus covering both stochastic and adversarial customer arrival settings. Embedding the Grace Period does not incur additional regret for any of these algorithms. This finding indicates that there is no tradeoff between a seller maximizing their revenue and guaranteeing that each customer feels fairly treated.</p></p class="citation"></blockquote><h3 id=45--214247-willy-wonka-mechanisms-thomas-archbold-et-al-2024>(4/5 | 214/247) Willy Wonka Mechanisms (Thomas Archbold et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Thomas Archbold, Bart de Keijzer, Carmine Ventre. (2024)<br><strong>Willy Wonka Mechanisms</strong><br><button class=copy-to-clipboard title="Willy Wonka Mechanisms" index=214>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-214 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.GT<br>Categories: cs-GT, cs.GT<br>Keyword Score: 10<br>Keywords: Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08314v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08314v1.pdf filename=2402.08314v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Bounded rationality in mechanism design aims to ensure incentive-compatibility for agents who are cognitively limited. These agents lack the contingent <b>reasoning</b> skills that traditional mechanism design assumes, and depending on how these cognitive limitations are modelled this alters the class of incentive-compatible mechanisms. In this work we design mechanisms without any &ldquo;obvious&rdquo; manipulations for several auction settings that aim to either maximise revenue or minimise the compensation paid to the agents. A mechanism without obvious manipulations is said to be &ldquo;not obviously manipulable&rdquo; (NOM), and assumes agents act truthfully as long as the maximum and minimum utilities from doing so are no worse than the maximum and minimum utilities from lying, with the extremes taken over all possible actions of the other agents. We exploit the definition of NOM by introducing the concept of &ldquo;golden tickets&rdquo; and &ldquo;wooden spoons&rdquo;, which designate bid profiles ensuring the mechanism&rsquo;s incentive-compatibility for each agent. We then characterise these &ldquo;Willy Wonka&rdquo; mechanisms, and by carefully choosing the golden tickets and wooden spoons we use this to design revenue-maximising auctions and frugal procurement auctions.</p></p class="citation"></blockquote><h3 id=55--215247-strategizing-against-no-regret-learners-in-first-price-auctions-aviad-rubinstein-et-al-2024>(5/5 | 215/247) Strategizing against No-Regret Learners in First-Price Auctions (Aviad Rubinstein et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Aviad Rubinstein, Junyao Zhao. (2024)<br><strong>Strategizing against No-Regret Learners in First-Price Auctions</strong><br><button class=copy-to-clipboard title="Strategizing against No-Regret Learners in First-Price Auctions" index=215>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-215 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.GT<br>Categories: cs-DS, cs-GT, cs-LG, cs.GT<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08637v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08637v1.pdf filename=2402.08637v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We study repeated first-price auctions and general repeated Bayesian games between two players, where one player, the learner, employs a no-regret learning algorithm, and the other player, the optimizer, knowing the learner&rsquo;s algorithm, strategizes to maximize its own utility. For a commonly used class of no-regret learning algorithms called mean-based algorithms, we show that (i) in standard (i.e., full-information) first-price auctions, the optimizer cannot get more than the Stackelberg utility &ndash; a standard <b>benchmark</b> in the literature, but (ii) in Bayesian first-price auctions, there are instances where the optimizer can achieve much higher than the Stackelberg utility. On the other hand, Mansour et al. (2022) showed that a more sophisticated class of algorithms called no-polytope-swap-regret algorithms are sufficient to cap the optimizer&rsquo;s utility at the Stackelberg utility in any repeated Bayesian game (including Bayesian first-price auctions), and they pose the open question whether no-polytope-swap-regret algorithms are necessary to cap the optimizer&rsquo;s utility. For general Bayesian games, under a reasonable and necessary condition, we prove that no-polytope-swap-regret algorithms are indeed necessary to cap the optimizer&rsquo;s utility and thus answer their open question. For Bayesian first-price auctions, we give a simple improvement of the standard algorithm for minimizing the polytope swap regret by exploiting the structure of Bayesian first-price auctions.</p></p class="citation"></blockquote><h2 id=eesssy-3>eess.SY (3)</h2><h3 id=13--216247-infinite-horizon-optimal-scheduling-for-feedback-control-siyi-wang-et-al-2024>(1/3 | 216/247) Infinite-horizon optimal scheduling for feedback control (Siyi Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Siyi Wang, Sandra Hirche. (2024)<br><strong>Infinite-horizon optimal scheduling for feedback control</strong><br><button class=copy-to-clipboard title="Infinite-horizon optimal scheduling for feedback control" index=216>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-216 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08819v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08819v1.pdf filename=2402.08819v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Emerging cyber-physical systems impel the development of communication protocols to efficiently utilize resources. This paper investigates the optimal co-design of control and scheduling in networked control systems. The objective is to co-design the control law and the scheduling mechanism that jointly optimize the tradeoff between regulation performance and communication resource consumption in the long run. The concept of the value of information (VoI) is employed to evaluate the importance of data being transmitted. The optimal solution includes a certainty equivalent control law and a stationary scheduling policy based on the VoI function. The closed-loop system under the designed scheduling policy is shown to be stochastically stable. By analyzing the property of the VoI function, we show that the optimal scheduling policy is symmetric and is a monotone function when the system matrix is diagonal. Moreover, by the diagonal system matrix assumption, the optimal scheduling policy is shown to be of threshold type. Then we provide a simplified yet equivalent form of the threshold-based optimal scheduling policy. The threshold value searching region is also given. Finally, the numerical <b>simulation</b> illustrates the theoretical result of the VoI-based scheduling.</p></p class="citation"></blockquote><h3 id=23--217247-ant-colony-optimization-for-cooperative-inspection-path-planning-using-multiple-unmanned-aerial-vehicles-duy-nam-bui-et-al-2024>(2/3 | 217/247) Ant Colony Optimization for Cooperative Inspection Path Planning Using Multiple Unmanned Aerial Vehicles (Duy Nam Bui et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Duy Nam Bui, Thuy Ngan Duong, Manh Duong Phung. (2024)<br><strong>Ant Colony Optimization for Cooperative Inspection Path Planning Using Multiple Unmanned Aerial Vehicles</strong><br><button class=copy-to-clipboard title="Ant Colony Optimization for Cooperative Inspection Path Planning Using Multiple Unmanned Aerial Vehicles" index=217>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-217 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-AI, cs-SY, eess-SY, eess.SY<br>Keyword Score: 10<br>Keywords: Heuristic Approach<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08246v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08246v1.pdf filename=2402.08246v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents a new swarm intelligence-based approach to deal with the cooperative path planning problem of unmanned aerial vehicles (UAVs), which is essential for the automatic inspection of infrastructure. The approach uses a 3D model of the structure to generate viewpoints for the UAVs. The calculation of the viewpoints considers the constraints related to the UAV formation model, camera parameters, and requirements for data post-processing. The viewpoints are then used as input to formulate the path planning as an extended traveling salesman problem and the definition of a new cost function. Ant colony optimization is finally used to solve the problem to yield optimal inspection paths. Experiments with 3D models of real structures have been conducted to evaluate the performance of the proposed approach. The results show that our system is not only capable of generating feasible inspection paths for UAVs but also reducing the path length by 29.47% for complex structures when compared with another <b>heuristic</b> <b>approach.</b> The source code of the algorithm can be found at <a href=https://github.com/duynamrcv/aco_3d_ipp>https://github.com/duynamrcv/aco_3d_ipp</a>.</p></p class="citation"></blockquote><h3 id=33--218247-an-adaptive-system-architecture-for-multimodal-intelligent-transportation-systems-muhammad-farooq-et-al-2024>(3/3 | 218/247) An Adaptive System Architecture for Multimodal Intelligent Transportation Systems (Muhammad Farooq et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Muhammad Farooq, Nima Afraz, Fatemeh Golpayegani. (2024)<br><strong>An Adaptive System Architecture for Multimodal Intelligent Transportation Systems</strong><br><button class=copy-to-clipboard title="An Adaptive System Architecture for Multimodal Intelligent Transportation Systems" index=218>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-218 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 6<br>Keywords: Multi-modal, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08817v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08817v1.pdf filename=2402.08817v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Multimodal</b> intelligent transportation systems (M-ITS) encompass a range of transportation services that utilise various modes of transport and incorporate intelligent technologies for enhanced efficiency and user experience. There are several challenges in M-ITS including data integration, Interoperability, scalability, user experience, etc. To address these challenges, such a system requires an adaptive system architecture that enables M-ITS to operate as an integrated ecosystem. In this paper, we provide an adaptive, user-centric, and layered architecture for <b>multimodal</b> transportation systems. The proposed architecture ensures scalability for seamless interactions of various subcomponents, that are often managed by different stakeholders. Concurrently, the data architecture is detailed, covering diverse data sources, advanced analytics, and stringent governance, providing a robust basis for intelligent decision-making. We provide two example use cases of the proposed architecture, showing how the data architecture and the system architecture can be fused and serve <b>multimodal</b> intelligent transport services.</p></p class="citation"></blockquote><h2 id=mathoc-1>math.OC (1)</h2><h3 id=11--219247-model-approximation-in-mdps-with-unbounded-per-step-cost-berk-bozkurt-et-al-2024>(1/1 | 219/247) Model approximation in MDPs with unbounded per-step cost (Berk Bozkurt et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Berk Bozkurt, Aditya Mahajan, Ashutosh Nayyar, Yi Ouyang. (2024)<br><strong>Model approximation in MDPs with unbounded per-step cost</strong><br><button class=copy-to-clipboard title="Model approximation in MDPs with unbounded per-step cost" index=219>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-219 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.OC<br>Categories: cs-LG, cs-SY, eess-SY, math-OC, math.OC<br>Keyword Score: 20<br>Keywords: Markov Decision Process, Markov Decision Process<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08813v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08813v1.pdf filename=2402.08813v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We consider the problem of designing a control policy for an infinite-horizon discounted cost <b>Markov</b> <b>decision</b> <b>process</b> $\mathcal{M}$ when we only have access to an approximate model $\hat{\mathcal{M}}$. How well does an optimal policy $\hat{\pi}^{\star}$ of the approximate model perform when used in the original model $\mathcal{M}$? We answer this question by bounding a weighted norm of the difference between the value function of $\hat{\pi}^\star $ when used in $\mathcal{M}$ and the optimal value function of $\mathcal{M}$. We then extend our results and obtain potentially tighter upper bounds by considering affine transformations of the per-step cost. We further provide upper bounds that explicitly depend on the weighted distance between cost functions and weighted distance between transition kernels of the original and approximate models. We present examples to illustrate our results.</p></p class="citation"></blockquote><h2 id=csit-7>cs.IT (7)</h2><h3 id=17--220247-introducing-rsess-an-open-source-enumerative-sphere-shaping-implementation-coded-in-rust-frederik-ritter-et-al-2024>(1/7 | 220/247) Introducing RSESS: An Open Source Enumerative Sphere Shaping Implementation Coded in Rust (Frederik Ritter et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Frederik Ritter, Andrej Rode, Laurent Schmalen. (2024)<br><strong>Introducing RSESS: An Open Source Enumerative Sphere Shaping Implementation Coded in Rust</strong><br><button class=copy-to-clipboard title="Introducing RSESS: An Open Source Enumerative Sphere Shaping Implementation Coded in Rust" index=220>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-220 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, eess-SP, math-IT<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08771v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08771v1.pdf filename=2402.08771v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this work, we present an open-source implementation of the enumerative sphere shaping (ESS) algorithm used for probabilistic constellation shaping (PCS). PCS aims at closing the shaping gap caused by using uniformly distributed modulation symbols in channels for which information theory shows non-uniformly distributed signaling to be optimal. ESS is one such PCS algorithm that sets itself apart as it operates on a trellis representation of a subset of the possible symbol sequences. ESS leads to an empirical distribution of the symbols that closely approximates the optimal distribution for the additive white Gaussian noise (AWGN) channel. We provide an open-source implementation of this algorithm in the compiled language Rust, as well as Python bindings with which our Rust code can be called in a regular Python script. We also compare <b>simulation</b> results on the AWGN channel using our implementation with previous works on this topic.</p></p class="citation"></blockquote><h3 id=27--221247-a-precise-bare-simulation-approach-to-the-minimization-of-some-distances-ii-further-foundations-michel-broniatowski-et-al-2024>(2/7 | 221/247) A precise bare simulation approach to the minimization of some distances. II. Further Foundations (Michel Broniatowski et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Michel Broniatowski, Wolfgang Stummer. (2024)<br><strong>A precise bare simulation approach to the minimization of some distances. II. Further Foundations</strong><br><button class=copy-to-clipboard title="A precise bare simulation approach to the minimization of some distances. II. Further Foundations" index=221>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-221 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, math-IT<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08478v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08478v1.pdf filename=2402.08478v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The constrained minimization (respectively maximization) of directed distances and of related generalized entropies is a fundamental task in information theory as well as in the adjacent fields of statistics, machine learning, artificial intelligence, signal processing and pattern recognition. In our previous paper &ldquo;A precise bare <b>simulation</b> approach to the minimization of some distances. I. Foundations&rdquo;, we obtained such kind of constrained optima by a new dimension-free precise bare (pure) <b>simulation</b> method, provided basically that (i) the underlying directed distance is of f-divergence type, and that (ii) this can be connected to a light-tailed probability distribution in a certain manner. In the present paper, we extend this approach such that constrained optimization problems of a very huge amount of directed distances and generalized entropies &ndash; and beyond &ndash; can be tackled by a newly developed dimension-free extended bare <b>simulation</b> method, for obtaining both optima as well as optimizers. Almost no assumptions (like convexity) on the set of constraints are needed, within our discrete setup of arbitrary dimension, and our method is precise (i.e., converges in the limit). For instance, we cover constrained optimizations of arbitrary f-divergences, Bregman distances, scaled Bregman distances and weighted Euclidean distances. The potential for wide-spread applicability is indicated, too; in particular, we deliver many recent references for uses of the involved distances/divergences in various different research fields (which may also serve as an interdisciplinary interface).</p></p class="citation"></blockquote><h3 id=37--222247-opportunistic-scheduling-using-statistical-information-of-wireless-channels-zhouyou-gu-et-al-2024>(3/7 | 222/247) Opportunistic Scheduling Using Statistical Information of Wireless Channels (Zhouyou Gu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhouyou Gu, Wibowo Hardjawana, Branka Vucetic. (2024)<br><strong>Opportunistic Scheduling Using Statistical Information of Wireless Channels</strong><br><button class=copy-to-clipboard title="Opportunistic Scheduling Using Statistical Information of Wireless Channels" index=222>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-222 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs-NI, cs.IT, eess-SP, math-IT<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08238v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08238v1.pdf filename=2402.08238v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper considers opportunistic scheduler (OS) design using statistical channel state information~(CSI). We apply max-weight schedulers (MWSs) to maximize a utility function of users&rsquo; average data rates. MWSs schedule the user with the highest weighted instantaneous data rate every time slot. Existing methods require hundreds of time slots to adjust the MWS&rsquo;s weights according to the instantaneous CSI before finding the optimal weights that maximize the utility function. In contrast, our MWS design requires few slots for estimating the statistical CSI. Specifically, we formulate a weight optimization problem using the mean and variance of users&rsquo; signal-to-noise ratios (SNRs) to construct constraints bounding users&rsquo; feasible average rates. Here, the utility function is the formulated objective, and the MWS&rsquo;s weights are optimization variables. We develop an iterative solver for the problem and prove that it finds the optimal weights. We also design an online architecture where the solver adaptively generates optimal weights for networks with varying mean and variance of the SNRs. <b>Simulations</b> show that our methods effectively require $4\sim10$ times fewer slots to find the optimal weights and achieve $5\sim15%$ better average rates than the existing methods.</p></p class="citation"></blockquote><h3 id=47--223247-asynchronous-distributed-coordinated-hybrid-precoding-in-multi-cell-mmwave-wireless-networks-meesam-jafri-et-al-2024>(4/7 | 223/247) Asynchronous Distributed Coordinated Hybrid Precoding in Multi-cell mmWave Wireless Networks (Meesam Jafri et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Meesam Jafri, Suraj Srivastava, Sunil Kumar, Aditya K. Jagannatham, Lajos Hanzo. (2024)<br><strong>Asynchronous Distributed Coordinated Hybrid Precoding in Multi-cell mmWave Wireless Networks</strong><br><button class=copy-to-clipboard title="Asynchronous Distributed Coordinated Hybrid Precoding in Multi-cell mmWave Wireless Networks" index=223>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-223 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, eess-SP, math-IT<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08231v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08231v1.pdf filename=2402.08231v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Asynchronous distributed hybrid beamformers (ADBF) are conceived for minimizing the total transmit power subject to signal-to-interference-plus-noise ratio (SINR) constraints at the users. Our design requires only limited information exchange between the base stations (BSs) of the mmWave multi-cell coordinated (MCC) networks considered. To begin with, a semidefinite relaxation (SDR)-based fully-digital (FD) beamformer is designed for a centralized MCC system. Subsequently, a Bayesian learning (BL) technique is harnessed for decomposing the FD beamformer into its analog and baseband components and construct a hybrid transmit precoder (TPC). However, the centralized TPC design requires global channel state information (CSI), hence it results in a high signaling overhead. An alternating direction based method of multipliers (ADMM) technique is developed for a synchronous distributed beamformer (SDBF) design, which relies only on limited information exchange among the BSs, thus reducing the signaling overheads required by the centralized TPC design procedure. However, the SDBF design is challenging, since it requires the updates from the BSs to be strictly synchronized. As a remedy, an ADBF framework is developed that mitigates the inter-cell interference (ICI) and also control the asynchrony in the system. Furthermore, the above ADBF framework is also extended to the robust ADBF (R-ADBF) algorithm that incorporates the CSI uncertainty into the design procedure for minimizing the the worst-case transmit power. Our <b>simulation</b> results illustrate both the enhanced performance and the improved convergence properties of the ADMM-based ADBF and R-ADBF schemes.</p></p class="citation"></blockquote><h3 id=57--224247-two-dimensional-direction-of-arrival-estimation-using-stacked-intelligent-metasurfaces-jiancheng-an-et-al-2024>(5/7 | 224/247) Two-Dimensional Direction-of-Arrival Estimation Using Stacked Intelligent Metasurfaces (Jiancheng An et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiancheng An, Chau Yuen, Yong Liang Guan, Marco Di Renzo, Mérouane Debbah, H. Vincent Poor, Lajos Hanzo. (2024)<br><strong>Two-Dimensional Direction-of-Arrival Estimation Using Stacked Intelligent Metasurfaces</strong><br><button class=copy-to-clipboard title="Two-Dimensional Direction-of-Arrival Estimation Using Stacked Intelligent Metasurfaces" index=224>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-224 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, eess-SP, math-IT<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08224v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08224v1.pdf filename=2402.08224v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Stacked intelligent metasurfaces (SIM) are capable of emulating reconfigurable physical neural networks by relying on electromagnetic (EM) waves as carriers. They can also perform various complex computational and signal processing tasks. A SIM is fabricated by densely integrating multiple metasurface layers, each consisting of a large number of small meta-atoms that can control the EM waves passing through it. In this paper, we harness a SIM for two-dimensional (2D) direction-of-arrival (DOA) estimation. In contrast to the conventional designs, an advanced SIM in front of the receiver array automatically carries out the 2D discrete Fourier transform (DFT) as the incident waves propagate through it. As a result, the receiver array directly observes the angular spectrum of the incoming signal. In this context, the DOA estimates can be readily obtained by using probes to detect the energy distribution on the receiver array. This avoids the need for power-thirsty radio frequency (RF) chains. To enable SIM to perform the 2D DFT, we formulate the optimization problem of minimizing the fitting error between the SIM&rsquo;s EM response and the 2D DFT matrix. Furthermore, a gradient descent algorithm is customized for iteratively updating the phase shift of each meta-atom in SIM. To further improve the DOA estimation accuracy, we configure the phase shift pattern in the zeroth layer of the SIM to generate a set of 2D DFT matrices associated with orthogonal spatial frequency bins. Additionally, we analytically evaluate the performance of the proposed SIM-based DOA estimator by deriving a tight upper bound for the mean square error (MSE). Our numerical <b>simulations</b> verify the capability of a well-trained SIM to perform DOA estimation and corroborate our theoretical analysis. It is demonstrated that a SIM having an optical computational speed achieves an MSE of $10^{-4}$ for DOA estimation.</p></p class="citation"></blockquote><h3 id=67--225247-an-information-theoretic-lower-bound-in-time-uniform-estimation-john-c-duchi-et-al-2024>(6/7 | 225/247) An information-theoretic lower bound in time-uniform estimation (John C. Duchi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>John C. Duchi, Saminul Haque. (2024)<br><strong>An information-theoretic lower bound in time-uniform estimation</strong><br><button class=copy-to-clipboard title="An information-theoretic lower bound in time-uniform estimation" index=225>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-225 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, math-IT, math-ST, stat-TH<br>Keyword Score: 10<br>Keywords: Logistic Regression<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08794v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08794v1.pdf filename=2402.08794v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present an information-theoretic lower bound for the problem of parameter estimation with time-uniform coverage guarantees. Via a new a reduction to sequential testing, we obtain stronger lower bounds that capture the hardness of the time-uniform setting. In the case of location model estimation, <b>logistic</b> <b>regression,</b> and exponential family models, our $\Omega(\sqrt{n^{-1}\log \log n})$ lower bound is sharp to within constant factors in typical settings.</p></p class="citation"></blockquote><h3 id=77--226247-a-scalable-synergy-first-backbone-decomposition-of-higher-order-structures-in-complex-systems-thomas-f-varley-2024>(7/7 | 226/247) A scalable, synergy-first backbone decomposition of higher-order structures in complex systems (Thomas F. Varley, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Thomas F. Varley. (2024)<br><strong>A scalable, synergy-first backbone decomposition of higher-order structures in complex systems</strong><br><button class=copy-to-clipboard title="A scalable, synergy-first backbone decomposition of higher-order structures in complex systems" index=226>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-226 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, math-IT, stat-OT<br>Keyword Score: 10<br>Keywords: Mutual Information<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08135v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08135v1.pdf filename=2402.08135v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Since its introduction in 2011, the partial information decomposition (PID) has triggered an explosion of interest in the field of multivariate information theory and the study of emergent, higher-order (&ldquo;synergistic&rdquo;) interactions in complex systems. Despite its power, however, the PID has a number of limitations that restrict its general applicability: it scales poorly with system size and the standard approach to decomposition hinges on a definition of &ldquo;redundancy&rdquo;, leaving synergy only vaguely defined as &ldquo;that information not redundant.&rdquo; Other heuristic measures, such as the O-information, have been introduced, although these measures typically only provided a summary statistic of redundancy/synergy dominance, rather than direct insight into the synergy itself. To address this issue, we present an alternative decomposition that is synergy-first, scales much more gracefully than the PID, and has a straightforward interpretation. Our approach defines synergy as that information in a set that would be lost following the minimally invasive perturbation on any single element. By generalizing this idea to sets of elements, we construct a totally ordered &ldquo;backbone&rdquo; of partial synergy atoms that sweeps systems scales. Our approach starts with entropy, but can be generalized to the Kullback-Leibler divergence, and by extension, to the total correlation and the single-target <b>mutual</b> <b>information.</b> Finally, we show that this approach can be used to decompose higher-order interactions beyond just information theory: we demonstrate this by showing how synergistic combinations of pairwise edges in a complex network supports signal communicability and global integration. We conclude by discussing how this perspective on synergistic structure (information-based or otherwise) can deepen our understanding of part-whole relationships in complex systems.</p></p class="citation"></blockquote><h2 id=csni-1>cs.NI (1)</h2><h3 id=11--227247-energy-aware-dynamic-resource-allocation-in-virtual-sensor-networks-carmen-delgado-et-al-2024>(1/1 | 227/247) Energy-aware Dynamic Resource Allocation in Virtual Sensor Networks (Carmen Delgado et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Carmen Delgado, María Canales, Jorge Ortín, José Ramón Gállego, Alessandro Redondi, Sonda Bousnina, Matteo Cesana. (2024)<br><strong>Energy-aware Dynamic Resource Allocation in Virtual Sensor Networks</strong><br><button class=copy-to-clipboard title="Energy-aware Dynamic Resource Allocation in Virtual Sensor Networks" index=227>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-227 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NI<br>Categories: cs-NI, cs.NI<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08443v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08443v1.pdf filename=2402.08443v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Sensor network virtualization enables the possibility of sharing common physical resources to multiple stakeholder applications. This paper focuses on addressing the dynamic adaptation of already assigned virtual sensor network resources to respond to time varying application demands. We propose an optimization framework that dynamically allocate applications into sensor nodes while accounting for the characteristics and limitations of the wireless sensor environment. It takes also into account the additional energy consumption related to activating new nodes and/or moving already active applications. Different objective functions related to the available energy in the nodes are analyzed. The proposed framework is evaluated by <b>simulation</b> considering realistic parameters from actual sensor nodes and deployed applications to assess the efficiency of the proposals.</p></p class="citation"></blockquote><h2 id=econgn-1>econ.GN (1)</h2><h3 id=11--228247-artificial-intelligence-and-the-transformation-of-higher-education-institutions-evangelos-katsamakas-et-al-2024>(1/1 | 228/247) Artificial intelligence and the transformation of higher education institutions (Evangelos Katsamakas et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Evangelos Katsamakas, Oleg V. Pavlov, Ryan Saklad. (2024)<br><strong>Artificial intelligence and the transformation of higher education institutions</strong><br><button class=copy-to-clipboard title="Artificial intelligence and the transformation of higher education institutions" index=228>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-228 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: econ.GN<br>Categories: cs-AI, cs-CY, econ-GN, econ.GN, q-fin-EC<br>Keyword Score: 20<br>Keywords: Generative AI, ChatGPT<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08143v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08143v1.pdf filename=2402.08143v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Artificial intelligence (AI) advances and the rapid adoption of <b>generative</b> <b>AI</b> tools like <b>ChatGPT</b> present new opportunities and challenges for higher education. While substantial literature discusses AI in higher education, there is a lack of a systemic approach that captures a holistic view of the AI transformation of higher education institutions (HEIs). To fill this gap, this article, taking a complex systems approach, develops a causal loop diagram (CLD) to map the causal feedback mechanisms of AI transformation in a typical HEI. Our model accounts for the forces that drive the AI transformation and the consequences of the AI transformation on value creation in a typical HEI. The article identifies and analyzes several reinforcing and balancing feedback loops, showing how, motivated by AI technology advances, the HEI invests in AI to improve student learning, research, and administration. The HEI must take measures to deal with academic integrity problems and adapt to changes in available jobs due to AI, emphasizing AI-complementary skills for its students. However, HEIs face a competitive threat and several policy traps that may lead to decline. HEI leaders need to become systems thinkers to manage the complexity of the AI transformation and benefit from the AI feedback loops while avoiding the associated pitfalls. We also discuss long-term scenarios, the notion of HEIs influencing the direction of AI, and directions for future research on AI transformation.</p></p class="citation"></blockquote><h2 id=q-biobm-1>q-bio.BM (1)</h2><h3 id=11--229247-a-survey-of-generative-ai-for-de-novo-drug-design-new-frontiers-in-molecule-and-protein-generation-xiangru-tang-et-al-2024>(1/1 | 229/247) A Survey of Generative AI for De Novo Drug Design: New Frontiers in Molecule and Protein Generation (Xiangru Tang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiangru Tang, Howard Dai, Elizabeth Knight, Fang Wu, Yunyang Li, Tianxiao Li, Mark Gerstein. (2024)<br><strong>A Survey of Generative AI for De Novo Drug Design: New Frontiers in Molecule and Protein Generation</strong><br><button class=copy-to-clipboard title="A Survey of Generative AI for De Novo Drug Design: New Frontiers in Molecule and Protein Generation" index=229>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-229 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: q-bio.BM<br>Categories: cs-AI, cs-LG, q-bio-BM, q-bio.BM<br>Keyword Score: 13<br>Keywords: Benchmarking, Generative AI<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08703v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08703v1.pdf filename=2402.08703v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Artificial intelligence (AI)-driven methods can vastly improve the historically costly drug design process, with various <b>generative</b> <b>models</b> already in widespread use. <b>Generative</b> <b>models</b> for de novo drug design, in particular, focus on the creation of novel biological compounds entirely from scratch, representing a promising future direction. Rapid development in the field, combined with the inherent complexity of the drug design process, creates a difficult landscape for new researchers to enter. In this survey, we organize de novo drug design into two overarching themes: small molecule and protein generation. Within each theme, we identify a variety of subtasks and applications, highlighting important datasets, <b>benchmarks,</b> and model architectures and comparing the performance of top models. We take a broad approach to AI-driven drug design, allowing for both micro-level comparisons of various methods within each subtask and macro-level observations across different fields. We discuss parallel challenges and approaches between the two applications and highlight future directions for AI-driven de novo drug design as a whole. An organized repository of all covered sources is available at <a href=https://github.com/gersteinlab/GenAI4Drug>https://github.com/gersteinlab/GenAI4Drug</a>.</p></p class="citation"></blockquote><h2 id=csdb-2>cs.DB (2)</h2><h3 id=12--230247-evaluating-the-data-model-robustness-of-text-to-sql-systems-based-on-real-user-queries-jonathan-fürst-et-al-2024>(1/2 | 230/247) Evaluating the Data Model Robustness of Text-to-SQL Systems Based on Real User Queries (Jonathan Fürst et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jonathan Fürst, Catherine Kosten, Farhard Nooralahzadeh, Yi Zhang, Kurt Stockinger. (2024)<br><strong>Evaluating the Data Model Robustness of Text-to-SQL Systems Based on Real User Queries</strong><br><button class=copy-to-clipboard title="Evaluating the Data Model Robustness of Text-to-SQL Systems Based on Real User Queries" index=230>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-230 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DB<br>Categories: cs-AI, cs-CL, cs-DB, cs.DB<br>Keyword Score: 13<br>Keywords: Benchmarking, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08349v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08349v1.pdf filename=2402.08349v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Text-to-SQL systems (also known as NL-to-SQL systems) have become an increasingly popular solution for bridging the gap between user capabilities and SQL-based data access. These systems translate user requests in natural language to valid SQL statements for a specific database. Recent Text-to-SQL systems have benefited from the rapid improvement of <b>transformer-based</b> language models. However, while Text-to-SQL systems that incorporate such models continuously reach new high scores on &ndash; often synthetic &ndash; <b>benchmark</b> datasets, a systematic exploration of their robustness towards different data models in a real-world, realistic scenario is notably missing. This paper provides the first in-depth evaluation of the data model robustness of Text-to-SQL systems in practice based on a multi-year international project focused on Text-to-SQL interfaces. Our evaluation is based on a real-world deployment of FootballDB, a system that was deployed over a 9 month period in the context of the FIFA World Cup 2022, during which about 6K natural language questions were asked and executed. All of our data is based on real user questions that were asked live to the system. We manually labeled and translated a subset of these questions for three different data models. For each data model, we explore the performance of representative Text-to-SQL systems and language models. We further quantify the impact of training data size, pre-, and post-processing steps as well as language model inference time. Our comprehensive evaluation sheds light on the design choices of real-world Text-to-SQL systems and their impact on moving from research prototypes to real deployments. Last, we provide a new <b>benchmark</b> dataset to the community, which is the first to enable the evaluation of different data models for the same dataset and is substantially more challenging than most previous datasets in terms of query complexity.</p></p class="citation"></blockquote><h3 id=22--231247-from-shapes-to-shapes-inferring-shacl-shapes-for-results-of-sparql-construct-queries-extended-version-philipp-seifer-et-al-2024>(2/2 | 231/247) From Shapes to Shapes: Inferring SHACL Shapes for Results of SPARQL CONSTRUCT Queries (Extended Version) (Philipp Seifer et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Philipp Seifer, Daniel Hernández, Ralf Lämmel, Steffen Staab. (2024)<br><strong>From Shapes to Shapes: Inferring SHACL Shapes for Results of SPARQL CONSTRUCT Queries (Extended Version)</strong><br><button class=copy-to-clipboard title="From Shapes to Shapes: Inferring SHACL Shapes for Results of SPARQL CONSTRUCT Queries (Extended Version)" index=231>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-231 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DB<br>Categories: cs-AI, cs-DB, cs-LO, cs.DB<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08509v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08509v1.pdf filename=2402.08509v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>SPARQL CONSTRUCT queries allow for the specification of data processing pipelines that transform given input <b>graphs</b> into new output <b>graphs.</b> It is now common to constrain <b>graphs</b> through SHACL shapes allowing users to understand which data they can expect and which not. However, it becomes challenging to understand what <b>graph</b> data can be expected at the end of a data processing pipeline without knowing the particular input data: Shape constraints on the input <b>graph</b> may affect the output <b>graph,</b> but may no longer apply literally, and new shapes may be imposed by the query template. In this paper, we study the derivation of shape constraints that hold on all possible output <b>graphs</b> of a given SPARQL CONSTRUCT query. We assume that the SPARQL CONSTRUCT query is fixed, e.g., being part of a program, whereas the input <b>graphs</b> adhere to input shape constraints but may otherwise vary over time and, thus, are mostly unknown. We study a fragment of SPARQL CONSTRUCT queries (SCCQ) and a fragment of SHACL (Simple SHACL). We formally define the problem of deriving the most restrictive set of Simple SHACL shapes that constrain the results from evaluating a SCCQ over any input <b>graph</b> restricted by a given set of Simple SHACL shapes. We propose and implement an algorithm that statically analyses input SHACL shapes and CONSTRUCT queries and prove its soundness and complexity.</p></p class="citation"></blockquote><h2 id=csdl-2>cs.DL (2)</h2><h3 id=12--232247-interleaved-snowballing-reducing-the-workload-of-literature-curators-ralf-stephan-2024>(1/2 | 232/247) Interleaved snowballing: Reducing the workload of literature curators (Ralf Stephan, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ralf Stephan. (2024)<br><strong>Interleaved snowballing: Reducing the workload of literature curators</strong><br><button class=copy-to-clipboard title="Interleaved snowballing: Reducing the workload of literature curators" index=232>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-232 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DL<br>Categories: H-3-7, cs-DL, cs.DL<br>Keyword Score: 13<br>Keywords: Graph, Text Classification<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08339v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08339v1.pdf filename=2402.08339v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We formally define the literature (reference) snowballing method and present a refined version of it. We show that the improved algorithm can substantially reduce curator work, even before application of <b>text</b> <b>classification,</b> by reducing the number of candidates to classify. We also present a desktop application named LitBall that implements this and other literature collection methods, through access to the Semantic Scholar academic <b>graph</b> (S2AG).</p></p class="citation"></blockquote><h3 id=22--233247-forecasting-high-impact-research-topics-via-machine-learning-on-evolving-knowledge-graphs-xuemei-gu-et-al-2024>(2/2 | 233/247) Forecasting high-impact research topics via machine learning on evolving knowledge graphs (Xuemei Gu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xuemei Gu, Mario Krenn. (2024)<br><strong>Forecasting high-impact research topics via machine learning on evolving knowledge graphs</strong><br><button class=copy-to-clipboard title="Forecasting high-impact research topics via machine learning on evolving knowledge graphs" index=233>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-233 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DL<br>Categories: cs-AI, cs-DL, cs-LG, cs.DL<br>Keyword Score: 8<br>Keywords: Graph, Knowledge Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08640v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08640v1.pdf filename=2402.08640v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The exponential growth in scientific publications poses a severe challenge for human researchers. It forces attention to more narrow sub-fields, which makes it challenging to discover new impactful research ideas and collaborations outside one&rsquo;s own field. While there are ways to predict a scientific paper&rsquo;s future citation counts, they need the research to be finished and the paper written, usually assessing impact long after the idea was conceived. Here we show how to predict the impact of onsets of ideas that have never been published by researchers. For that, we developed a large evolving <b>knowledge</b> <b>graph</b> built from more than 21 million scientific papers. It combines a semantic network created from the content of the papers and an impact network created from the historic citations of papers. Using machine learning, we can predict the dynamic of the evolving network into the future with high accuracy, and thereby the impact of new research directions. We envision that the ability to predict the impact of new ideas will be a crucial component of future artificial muses that can inspire new impactful and interesting scientific ideas.</p></p class="citation"></blockquote><h2 id=physicschem-ph-1>physics.chem-ph (1)</h2><h3 id=11--234247-zero-shot-molecular-generation-via-similarity-kernels-rokas-elijošius-et-al-2024>(1/1 | 234/247) Zero Shot Molecular Generation via Similarity Kernels (Rokas Elijošius et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rokas Elijošius, Fabian Zills, Ilyes Batatia, Sam Walton Norwood, Dávid Péter Kovács, Christian Holm, Gábor Csányi. (2024)<br><strong>Zero Shot Molecular Generation via Similarity Kernels</strong><br><button class=copy-to-clipboard title="Zero Shot Molecular Generation via Similarity Kernels" index=234>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-234 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: physics.chem-ph<br>Categories: cs-LG, physics-chem-ph, physics.chem-ph<br>Keyword Score: 10<br>Keywords: Zero-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08708v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08708v1.pdf filename=2402.08708v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Generative modelling aims to accelerate the discovery of novel chemicals by directly proposing structures with desirable properties. Recently, score-based, or diffusion, generative models have significantly outperformed previous approaches. Key to their success is the close relationship between the score and physical force, allowing the use of powerful equivariant neural networks. However, the behaviour of the learnt score is not yet well understood. Here, we analyse the score by training an energy-based diffusion model for molecular generation. We find that during the generation the score resembles a restorative potential initially and a quantum-mechanical force at the end. In between the two endpoints, it exhibits special properties that enable the building of large molecules. Using insights from the trained model, we present Similarity-based Molecular Generation (SiMGen), a new method for zero shot molecular generation. SiMGen combines a time-dependent similarity kernel with descriptors from a pretrained machine learning force field to generate molecules without any further training. Our approach allows full control over the molecular shape through point cloud priors and supports conditional generation. We also release an interactive web tool that allows users to generate structures with SiMGen online (<a href=https://zndraw.icp.uni-stuttgart.de>https://zndraw.icp.uni-stuttgart.de</a>).</p></p class="citation"></blockquote><h2 id=csma-1>cs.MA (1)</h2><h3 id=11--235247-logic-of-awareness-for-nested-knowledge-yudai-kubono-2024>(1/1 | 235/247) Logic of Awareness for Nested Knowledge (Yudai Kubono, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yudai Kubono. (2024)<br><strong>Logic of Awareness for Nested Knowledge</strong><br><button class=copy-to-clipboard title="Logic of Awareness for Nested Knowledge" index=235>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-235 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.MA<br>Categories: cs-LO, cs-MA, cs.MA<br>Keyword Score: 10<br>Keywords: Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08282v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08282v1.pdf filename=2402.08282v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Reasoning</b> abilities of human beings are limited. Logics that treat logical inference for human knowledge should reflect these limited abilities. Logic of awareness is one of those logics. In the logic, what an agent with a limited <b>reasoning</b> ability actually knows at a given moment (explicit knowledge) is distinguished from the ideal knowledge that an agent obtains by performing all possible inferences with what she already knows (implicit knowledge). This paper proposes a logic for explicit knowledge. In particular, we focus more on nested explicit knowledge, which means another agent&rsquo;s knowledge that an agent actually knows at a given moment. We develope a new formalization of two ideas and propose Kripke-style semantics. The first idea is the effect on an agent&rsquo;s <b>reasoning</b> ability by a state of an agent&rsquo;s awareness. We incorporate a relation on possible worlds called an indistinguishable relation to represent ignorance due to lack of awareness. The second idea is a state of each agent&rsquo;s awareness in the other agent&rsquo;s mind. We incorporate a non-empty finite sequence of agents called \textit{a chain of belief for awareness}. Our logic is called Awareness Logic with Partitions and Chains (ALPC). Employing an example, we show how nested explicit knowledge is formalized with our logic. Thereafter, we propose the proof system and prove the completeness. Finally, we discuss directions for extending and applying our logic and conclude. Our logic offers a foundation for a formal representation of human knowledge. We expect that the logic can be applied to computer science and game theory by describing and analyzing strategic behavior in a game and practical agent communication.</p></p class="citation"></blockquote><h2 id=q-fintr-1>q-fin.TR (1)</h2><h3 id=11--236247-end-to-end-policy-learning-of-a-statistical-arbitrage-autoencoder-architecture-fabian-krause-et-al-2024>(1/1 | 236/247) End-to-End Policy Learning of a Statistical Arbitrage Autoencoder Architecture (Fabian Krause et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fabian Krause, Jan-Peter Calliess. (2024)<br><strong>End-to-End Policy Learning of a Statistical Arbitrage Autoencoder Architecture</strong><br><button class=copy-to-clipboard title="End-to-End Policy Learning of a Statistical Arbitrage Autoencoder Architecture" index=236>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-236 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: q-fin.TR<br>Categories: cs-LG, q-fin-TR, q-fin.TR<br>Keyword Score: 10<br>Keywords: Autoencoder<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08233v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08233v1.pdf filename=2402.08233v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In Statistical Arbitrage (StatArb), classical mean reversion trading strategies typically hinge on asset-pricing or PCA based models to identify the mean of a synthetic asset. Once such a (linear) model is identified, a separate mean reversion strategy is then devised to generate a trading signal. With a view of generalising such an approach and turning it truly data-driven, we study the utility of <b>Autoencoder</b> architectures in StatArb. As a first approach, we employ a standard <b>Autoencoder</b> trained on US stock returns to derive trading strategies based on the Ornstein-Uhlenbeck (OU) process. To further enhance this model, we take a policy-learning approach and embed the <b>Autoencoder</b> network into a neural network representation of a space of portfolio trading policies. This integration outputs portfolio allocations directly and is end-to-end trainable by backpropagation of the risk-adjusted returns of the neural policy. Our findings demonstrate that this innovative end-to-end policy learning approach not only simplifies the strategy development process, but also yields superior gross returns over its competitors illustrating the potential of end-to-end training over classical two-stage approaches.</p></p class="citation"></blockquote><h2 id=statme-1>stat.ME (1)</h2><h3 id=11--237247-gradient-flow-adaptive-importance-sampling-for-bayesian-leave-one-out-cross-validation-for-sigmoidal-classification-models-joshua-c-chang-et-al-2024>(1/1 | 237/247) Gradient-flow adaptive importance sampling for Bayesian leave one out cross-validation for sigmoidal classification models (Joshua C Chang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Joshua C Chang, Xiangting Li, Shixin Xu, Hao-Ren Yao, Julia Porcino, Carson Chow. (2024)<br><strong>Gradient-flow adaptive importance sampling for Bayesian leave one out cross-validation for sigmoidal classification models</strong><br><button class=copy-to-clipboard title="Gradient-flow adaptive importance sampling for Bayesian leave one out cross-validation for sigmoidal classification models" index=237>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-237 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ME<br>Categories: cs-AI, cs-LG, math-SP, math-ST, stat-ME, stat-TH, stat.ME<br>Keyword Score: 10<br>Keywords: Logistic Regression<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08151v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08151v1.pdf filename=2402.08151v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce a set of gradient-flow-guided adaptive importance sampling (IS) transformations to stabilize Monte-Carlo approximations of point-wise leave one out cross-validated (LOO) predictions for Bayesian classification models. One can leverage this methodology for assessing model generalizability by for instance computing a LOO analogue to the AIC or computing LOO ROC/PRC curves and derived metrics like the AUROC and AUPRC. By the calculus of variations and gradient flow, we derive two simple nonlinear single-step transformations that utilize gradient information to shift a model&rsquo;s pre-trained full-data posterior closer to the target LOO posterior predictive distributions. In doing so, the transformations stabilize importance weights. Because the transformations involve the gradient of the likelihood function, the resulting Monte Carlo integral depends on Jacobian determinants with respect to the model Hessian. We derive closed-form exact formulae for these Jacobian determinants in the cases of <b>logistic</b> <b>regression</b> and shallow ReLU-activated artificial neural networks, and provide a simple approximation that sidesteps the need to compute full Hessian matrices and their spectra. We test the methodology on an $n\ll p$ dataset that is known to produce unstable LOO IS weights.</p></p class="citation"></blockquote><h2 id=eesssp-1>eess.SP (1)</h2><h3 id=11--238247-benchmarking-multi-component-signal-processing-methods-in-the-time-frequency-plane-juan-m-miramont-et-al-2024>(1/1 | 238/247) Benchmarking multi-component signal processing methods in the time-frequency plane (Juan M. Miramont et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Juan M. Miramont, Rémi Bardenet, Pierre Chainais, Francois Auger. (2024)<br><strong>Benchmarking multi-component signal processing methods in the time-frequency plane</strong><br><button class=copy-to-clipboard title="Benchmarking multi-component signal processing methods in the time-frequency plane" index=238>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-238 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SP<br>Categories: cs-SD, eess-AS, eess-SP, eess.SP<br>Keyword Score: 6<br>Keywords: Benchmarking, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08521v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08521v1.pdf filename=2402.08521v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Signal processing in the time-frequency plane has a long history and remains a field of methodological innovation. For instance, detection and denoising based on the zeros of the spectrogram have been proposed since 2015, contrasting with a long history of focusing on larger values of the spectrogram. Yet, unlike neighboring fields like optimization and machine learning, time-frequency signal processing lacks widely-adopted <b>benchmarking</b> tools. In this work, we contribute an open-source, Python-based toolbox termed MCSM-Benchs for <b>benchmarking</b> multi-component signal analysis methods, and we demonstrate our toolbox on three time-frequency <b>benchmarks.</b> First, we compare different methods for signal detection based on the zeros of the spectrogram, including unexplored variations of previously proposed detection tests. Second, we compare zero-based denoising methods to both classical and novel methods based on large values and ridges of the spectrogram. Finally, we compare the denoising performance of these methods against typical spectrogram thresholding strategies, in terms of post-processing artifacts commonly referred to as musical noise. At a low level, the obtained results provide new insight on the assessed approaches, and in particular research directions to further develop zero-based methods. At a higher level, our <b>benchmarks</b> exemplify the benefits of using a public, collaborative, common framework for <b>benchmarking.</b></p></p class="citation"></blockquote><h2 id=csds-5>cs.DS (5)</h2><h3 id=15--239247-sequence-graphs-realizations-and-ambiguity-in-language-models-sammy-khalife-et-al-2024>(1/5 | 239/247) Sequence graphs realizations and ambiguity in language models (Sammy Khalife et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sammy Khalife, Yann Ponty, Laurent Bulteau. (2024)<br><strong>Sequence graphs realizations and ambiguity in language models</strong><br><button class=copy-to-clipboard title="Sequence graphs realizations and ambiguity in language models" index=239>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-239 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DS<br>Categories: cs-CC, cs-CL, cs-DS, cs.DS<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08830v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08830v1.pdf filename=2402.08830v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Several popular language models represent local contexts in an input text as bags of words. Such representations are naturally encoded by a sequence <b>graph</b> whose vertices are the distinct words occurring in x, with edges representing the (ordered) co-occurrence of two words within a sliding window of size w. However, this compressed representation is not generally bijective, and may introduce some degree of ambiguity. Some sequence <b>graphs</b> may admit several realizations as a sequence, while others may not admit any realization. In this paper, we study the realizability and ambiguity of sequence <b>graphs</b> from a combinatorial and computational point of view. We consider the existence and enumeration of realizations of a sequence <b>graph</b> under multiple settings: window size w, presence/absence of <b>graph</b> orientation, and presence/absence of weights (multiplicities). When w = 2, we provide polynomial time algorithms for realizability and enumeration in all cases except the undirected/weighted setting, where we show the #P-hardness of enumeration. For a window of size at least 3, we prove hardness of all variants, even when w is considered as a constant, with the notable exception of the undirected/unweighted case for which we propose an XP algorithms for both (realizability and enumeration) problems, tight due to a corresponding W[1]-hardness result. We conclude with an integer program formulation to solve the realizability problem, and with dynamic programming to solve the enumeration problem. This work leaves open the membership to NP for both problems, a non-trivial question due to the existence of minimum realizations having exponential size on the instance encoding.</p></p class="citation"></blockquote><h3 id=25--240247-parameterized-dynamic-data-structure-for-split-completion-konrad-majewski-et-al-2024>(2/5 | 240/247) Parameterized dynamic data structure for Split Completion (Konrad Majewski et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Konrad Majewski, Michał Pilipczuk, Anna Zych-Pawlewicz. (2024)<br><strong>Parameterized dynamic data structure for Split Completion</strong><br><button class=copy-to-clipboard title="Parameterized dynamic data structure for Split Completion" index=240>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-240 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DS<br>Categories: cs-DS, cs.DS<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08816v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08816v1.pdf filename=2402.08816v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We design a randomized data structure that, for a fully dynamic <b>graph</b> $G$ updated by edge insertions and deletions and integers $k, d$ fixed upon initialization, maintains the answer to the Split Completion problem: whether one can add $k$ edges to $G$ to obtain a split <b>graph.</b> The data structure can be initialized on an edgeless $n$-vertex <b>graph</b> in time $n \cdot (k d \cdot \log n)^{\mathcal{O}(1)}$, and the amortized time complexity of an update is $5^k \cdot (k d \cdot \log n)^{\mathcal{O}(1)}$. The answer provided by the data structure is correct with probability $1-\mathcal{O}(n^{-d})$.</p></p class="citation"></blockquote><h3 id=35--241247-tight-double-exponential-bounds-for-identification-problems-locating-dominating-set-and-test-cover-dipayan-chakraborty-et-al-2024>(3/5 | 241/247) Tight (Double) Exponential Bounds for Identification Problems: Locating-Dominating Set and Test Cover (Dipayan Chakraborty et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dipayan Chakraborty, Florent Foucaud, Diptapriyo Majumdar, Prafullkumar Tale. (2024)<br><strong>Tight (Double) Exponential Bounds for Identification Problems: Locating-Dominating Set and Test Cover</strong><br><button class=copy-to-clipboard title="Tight (Double) Exponential Bounds for Identification Problems: Locating-Dominating Set and Test Cover" index=241>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-241 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DS<br>Categories: cs-CC, cs-DM, cs-DS, cs.DS<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08346v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08346v1.pdf filename=2402.08346v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We investigate fine-grained algorithmic aspects of identification problems in <b>graphs</b> and set systems, with a focus on Locating-Dominating Set and Test Cover. We prove, among other things, the following three (tight) conditional lower bounds. \begin{enumerate} \item \textsc{Locating-Dominating Set} does not admit an algorithm running in time $2^{o(k^2)} \cdot poly(n)$, nor a polynomial time kernelization algorithm that reduces the solution size and outputs a kernel with $2^{o(k)}$ vertices, unless the \ETH\ fails. \end{enumerate} To the best of our knowledge, \textsc{Locating-Dominating Set} is the first problem that admits such an algorithmic lower-bound (with a quadratic function in the exponent) when parameterized by the solution size. \begin{enumerate}[resume] \item \textsc{Test Cover} does not admit an algorithm running in time $2^{2^{o(k)}} \cdot poly(|U| + |\calF|)$. \end{enumerate} After \textsc{Edge Clique Cover} and \textsc{BiClique Cover}, this is the only example that we know of that admits a double exponential lower bound when parameterized by the solution size. \begin{enumerate}[resume] \item \textsc{Locating-Dominating Set} (respectively, \textsc{Test Cover}) parameterized by the treewidth of the input <b>graph</b> (respectively, of the natural auxiliary <b>graph)</b> does not admit an algorithm running in time $2^{2^{o(\tw)}} \cdot poly(n)$ (respectively, $2^{2^{o(\tw)}} \cdot poly(|U| + |\calF|))$. \end{enumerate} This result augments the small list of NP-Complete problems that admit double exponential lower bounds when parameterized by treewidth. We also present algorithms whose running times match the above lower bounds. We also investigate the parameterizations by several other structural <b>graph</b> parameters, answering some open problems from the literature.</p></p class="citation"></blockquote><h3 id=45--242247-integrating-high-dimensional-functions-deterministically-david-gamarnik-et-al-2024>(4/5 | 242/247) Integrating High-Dimensional Functions Deterministically (David Gamarnik et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>David Gamarnik, Devin Smedira. (2024)<br><strong>Integrating High-Dimensional Functions Deterministically</strong><br><button class=copy-to-clipboard title="Integrating High-Dimensional Functions Deterministically" index=242>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-242 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DS<br>Categories: cs-DM, cs-DS, cs.DS, math-CO, math-PR<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08232v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08232v1.pdf filename=2402.08232v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We design a Quasi-Polynomial time deterministic approximation algorithm for computing the integral of a multi-dimensional separable function, supported by some underlying hyper-graph structure, appropriately defined. Equivalently, our integral is the partition function of a graphical model with continuous potentials. While randomized algorithms for high-dimensional integration are widely known, deterministic counterparts generally do not exist. We use the correlation decay method applied to the Riemann sum of the function to produce our algorithm. For our method to work, we require that the domain is bounded and the hyper-edge potentials are positive and bounded on the domain. We further assume that upper and lower bounds on the potentials separated by a multiplicative factor of $1 + O(1/\Delta^2)$, where $\Delta$ is the maximum degree of the <b>graph.</b> When $\Delta = 3$, our method works provided the upper and lower bounds are separated by a factor of at most $1.0479$. To the best of our knowledge, our algorithm is the first deterministic algorithm for high-dimensional integration of a continuous function, apart from the case of trivial product form distributions.</p></p class="citation"></blockquote><h3 id=55--243247-an-improved-approximation-algorithm-for-metric-triangle-packing-jingyang-zhao-et-al-2024>(5/5 | 243/247) An Improved Approximation Algorithm for Metric Triangle Packing (Jingyang Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jingyang Zhao, Mingyu Xiao. (2024)<br><strong>An Improved Approximation Algorithm for Metric Triangle Packing</strong><br><button class=copy-to-clipboard title="An Improved Approximation Algorithm for Metric Triangle Packing" index=243>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-243 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DS<br>Categories: cs-DS, cs.DS<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08216v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08216v1.pdf filename=2402.08216v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Given an edge-weighted metric complete <b>graph</b> with $n$ vertices, the maximum weight metric triangle packing problem is to find a set of $n/3$ vertex-disjoint triangles with the total weight of all triangles in the packing maximized. Several simple methods can lead to a 2/3-approximation ratio. However, this barrier is not easy to break. Chen et al. proposed a randomized approximation algorithm with an expected ratio of $(0.66768-\varepsilon)$ for any constant $\varepsilon>0$. In this paper, we improve the approximation ratio to $(0.66835-\varepsilon)$. Furthermore, we can derandomize our algorithm.</p></p class="citation"></blockquote><h2 id=csdc-1>cs.DC (1)</h2><h3 id=11--244247-byzantine-fault-tolerant-distributed-set-intersection-with-redundancy-shuo-liu-et-al-2024>(1/1 | 244/247) Byzantine fault-tolerant distributed set intersection with redundancy (Shuo Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shuo Liu, Nitin H. Vaidya. (2024)<br><strong>Byzantine fault-tolerant distributed set intersection with redundancy</strong><br><button class=copy-to-clipboard title="Byzantine fault-tolerant distributed set intersection with redundancy" index=244>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-244 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DC<br>Categories: cs-DC, cs.DC<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08809v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08809v1.pdf filename=2402.08809v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this report, we study the problem of Byzantine fault-tolerant distributed set intersection and the importance of redundancy in solving this problem. Specifically, consider a distributed system with $n$ agents, each of which has a local set. There are up to $f$ agents that are Byzantine faulty. The goal is to find the intersection of the sets of the non-faulty agents. We derive the Byzantine set intersection problem from the Byzantine optimization problem. We present the definition of $2f$-redundancy, and identify the necessary and sufficient condition if the Byzantine set intersection problem can be solved if a certain redundancy property is satisfied, and then present an equivalent condition. We further extend our results to arbitrary communication <b>graphs</b> in a decentralized setting. Finally, we present solvability results for the Byzantine optimization problem, inspired by our findings on Byzantine set intersection. The results we provide are for synchronous and asynchronous systems both.</p></p class="citation"></blockquote><h2 id=mathco-1>math.CO (1)</h2><h3 id=11--245247-detecting-k_23-as-an-induced-minor-clément-dallard-et-al-2024>(1/1 | 245/247) Detecting $K_{2,3}$ as an induced minor (Clément Dallard et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Clément Dallard, Maël Dumas, Claire Hilaire, Martin Milanič, Anthony Perez, Nicolas Trotignon. (2024)<br><strong>Detecting $K_{2,3}$ as an induced minor</strong><br><button class=copy-to-clipboard title="Detecting $K_{2,3}$ as an induced minor" index=245>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-245 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.CO<br>Categories: 05C75 (Primary), 05C85, 05C83, 05C40, 05C69 (Secondary), cs-DM, cs-DS, math-CO, math.CO<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08332v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08332v1.pdf filename=2402.08332v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We consider a natural generalization of chordal <b>graphs,</b> in which every minimal separator induces a subgraph with independence number at most $2$. Such <b>graphs</b> can be equivalently defined as <b>graphs</b> that do not contain the complete bipartite <b>graph</b> $K_{2,3}$ as an induced minor, that is, <b>graphs</b> from which $K_{2,3}$ cannot be obtained by a sequence of edge contractions and vertex deletions. We develop a polynomial-time algorithm for recognizing these <b>graphs.</b> Our algorithm relies on a characterization of $K_{2,3}$-induced minor-free <b>graphs</b> in terms of excluding particular induced subgraphs, called Truemper configurations.</p></p class="citation"></blockquote><h2 id=cscg-1>cs.CG (1)</h2><h3 id=11--246247-nearly-orthogonal-sets-over-finite-fields-dror-chawin-et-al-2024>(1/1 | 246/247) Nearly Orthogonal Sets over Finite Fields (Dror Chawin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dror Chawin, Ishay Haviv. (2024)<br><strong>Nearly Orthogonal Sets over Finite Fields</strong><br><button class=copy-to-clipboard title="Nearly Orthogonal Sets over Finite Fields" index=246>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-246 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CG<br>Categories: cs-CG, cs-DM, cs-IT, cs.CG, math-CO, math-IT<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08274v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08274v1.pdf filename=2402.08274v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>For a field $\mathbb{F}$ and integers $d$ and $k$, a set of vectors of $\mathbb{F}^d$ is called $k$-nearly orthogonal if its members are non-self-orthogonal and every $k+1$ of them include an orthogonal pair. We prove that for every prime $p$ there exists a positive constant $\delta = \delta (p)$, such that for every field $\mathbb{F}$ of characteristic $p$ and for all integers $k \geq 2$ and $d \geq k^{1/(p-1)}$, there exists a $k$-nearly orthogonal set of at least $d^{\delta \cdot k^{1/(p-1)}/ \log k}$ vectors of $\mathbb{F}^d$. In particular, for the binary field we obtain a set of $d^{\Omega( k /\log k)}$ vectors, and this is tight up to the $\log k$ term in the exponent. For comparison, the best known lower bound over the reals is $d^{\Omega( \log k / \log \log k)}$ (Alon and Szegedy, <b>Graphs</b> and Combin., 1999). The proof combines probabilistic and spectral arguments.</p></p class="citation"></blockquote><h2 id=csdm-1>cs.DM (1)</h2><h3 id=11--247247-iiro-honkalas-contributions-to-identifying-codes-olivier-hudry-et-al-2024>(1/1 | 247/247) Iiro Honkala&rsquo;s contributions to identifying codes (Olivier Hudry et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Olivier Hudry, Ville Junnila, Antoine Lobstein. (2024)<br><strong>Iiro Honkala&rsquo;s contributions to identifying codes</strong><br><button class=copy-to-clipboard title="Iiro Honkala's contributions to identifying codes" index=247>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-247 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DM<br>Categories: cs-DM, cs.DM, math-CO<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08264v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08264v1.pdf filename=2402.08264v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A set $C$ of vertices in a <b>graph</b> $G=(V,E)$ is an identifying code if it is dominating and any two vertices of $V$ are dominated by distinct sets of codewords. This paper presents a survey of Iiro Honkala&rsquo;s contributions to the study of identifying codes with respect to several aspects: complexity of computing an identifying code, combinatorics in binary Hamming spaces, infinite grids, relationships between identifying codes and usual parameters in <b>graphs,</b> structural properties of <b>graphs</b> admitting identifying codes, and number of optimal identifying codes.</p></p class="citation"></blockquote></div><div class="row pl-3 pr-3"><div class="col-md-6 share-buttons"></div></div><hr><div class="row next-prev-navigator"><div class="col-md-6 previous-article"><a href=/akitenkrad-blog/posts/arxiv/202402/20240214000000/ title="arXiv @ 2024.02.14" class="btn btn-outline-info"><div><i class="fas fa-chevron-circle-left"></i> Prev</div><div class=next-prev-text>arXiv @ 2024.02.14</div></a></div><div class="col-md-6 next-article"><a href=/akitenkrad-blog/posts/arxiv/202402/20240216000000/ title="arXiv @ 2024.02.16" class="btn btn-outline-info"><div>Next <i class="fas fa-chevron-circle-right"></i></div><div class=next-prev-text>arXiv @ 2024.02.16</div></a></div></div><hr></div></div></div><a id=scroll-to-top class=btn><i class="fas fa-chevron-circle-up"></i></a></section><section class=toc-section id=toc-section><div class=toc-holder><h5 class="text-center pl-3">Table of Contents</h5><hr><div class=toc><nav id=TableOfContents><ul><li><a href=#primary-categories>Primary Categories</a></li><li><a href=#keywords>Keywords</a></li><li><a href=#cscl-34>cs.CL (34)</a><ul><li><a href=#134--1247-learning-how-to-ask-cycle-consistency-refines-prompts-in-multimodal-foundation-models-maurice-diesendruck-et-al-2024>(1/34 | 1/247) Learning How To Ask: Cycle-Consistency Refines Prompts in Multimodal Foundation Models (Maurice Diesendruck et al., 2024)</a></li><li><a href=#234--2247-eliciting-personality-traits-in-large-language-models-airlie-hilliard-et-al-2024>(2/34 | 2/247) Eliciting Personality Traits in Large Language Models (Airlie Hilliard et al., 2024)</a></li><li><a href=#334--3247-punctuation-restoration-improves-structure-understanding-without-supervision-junghyun-min-et-al-2024>(3/34 | 3/247) Punctuation Restoration Improves Structure Understanding without Supervision (Junghyun Min et al., 2024)</a></li><li><a href=#434--4247-prompt-optimization-in-multi-step-tasks-promst-integrating-human-feedback-and-preference-alignment-yongchao-chen-et-al-2024>(4/34 | 4/247) PRompt Optimization in Multi-Step Tasks (PROMST): Integrating Human Feedback and Preference Alignment (Yongchao Chen et al., 2024)</a></li><li><a href=#534--5247-towards-faithful-and-robust-llm-specialists-for-evidence-based-question-answering-tobias-schimanski-et-al-2024>(5/34 | 5/247) Towards Faithful and Robust LLM Specialists for Evidence-Based Question-Answering (Tobias Schimanski et al., 2024)</a></li><li><a href=#634--6247-auditing-counterfire-evaluating-advanced-counterargument-generation-with-evidence-and-style-preetika-verma-et-al-2024>(6/34 | 6/247) Auditing Counterfire: Evaluating Advanced Counterargument Generation with Evidence and Style (Preetika Verma et al., 2024)</a></li><li><a href=#734--7247-plausible-extractive-rationalization-through-semi-supervised-entailment-signal-yeo-wei-jie-et-al-2024>(7/34 | 7/247) Plausible Extractive Rationalization through Semi-Supervised Entailment Signal (Yeo Wei Jie et al., 2024)</a></li><li><a href=#834--8247-bbox-adapter-lightweight-adapting-for-black-box-large-language-models-haotian-sun-et-al-2024>(8/34 | 8/247) BBox-Adapter: Lightweight Adapting for Black-Box Large Language Models (Haotian Sun et al., 2024)</a></li><li><a href=#934--9247-an-embarrassingly-simple-approach-for-llm-with-strong-asr-capacity-ziyang-ma-et-al-2024>(9/34 | 9/247) An Embarrassingly Simple Approach for LLM with Strong ASR Capacity (Ziyang Ma et al., 2024)</a></li><li><a href=#1034--10247-ecellm-generalizing-large-language-models-for-e-commerce-from-large-scale-high-quality-instruction-data-bo-peng-et-al-2024>(10/34 | 10/247) eCeLLM: Generalizing Large Language Models for E-commerce from Large-scale, High-quality Instruction Data (Bo Peng et al., 2024)</a></li><li><a href=#1134--11247-instructgraph-boosting-large-language-models-via-graph-centric-instruction-tuning-and-preference-alignment-jianing-wang-et-al-2024>(11/34 | 11/247) InstructGraph: Boosting Large Language Models via Graph-centric Instruction Tuning and Preference Alignment (Jianing Wang et al., 2024)</a></li><li><a href=#1234--12247-jamdec-unsupervised-authorship-obfuscation-using-constrained-decoding-over-small-language-models-jillian-fisher-et-al-2024>(12/34 | 12/247) JAMDEC: Unsupervised Authorship Obfuscation using Constrained Decoding over Small Language Models (Jillian Fisher et al., 2024)</a></li><li><a href=#1334--13247-pixel-sentence-representation-learning-chenghao-xiao-et-al-2024>(13/34 | 13/247) Pixel Sentence Representation Learning (Chenghao Xiao et al., 2024)</a></li><li><a href=#1434--14247-improving-generalization-in-semantic-parsing-by-increasing-natural-language-variation-irina-saparina-et-al-2024>(14/34 | 14/247) Improving Generalization in Semantic Parsing by Increasing Natural Language Variation (Irina Saparina et al., 2024)</a></li><li><a href=#1534--15247-bayesian-multi-task-transfer-learning-for-soft-prompt-tuning-haeju-lee-et-al-2024>(15/34 | 15/247) Bayesian Multi-Task Transfer Learning for Soft Prompt Tuning (Haeju Lee et al., 2024)</a></li><li><a href=#1634--16247-glore-when-where-and-how-to-improve-llm-reasoning-via-global-and-local-refinements-alex-havrilla-et-al-2024>(16/34 | 16/247) GLoRe: When, Where, and How to Improve LLM Reasoning via Global and Local Refinements (Alex Havrilla et al., 2024)</a></li><li><a href=#1734--17247-preflmr-scaling-up-fine-grained-late-interaction-multi-modal-retrievers-weizhe-lin-et-al-2024>(17/34 | 17/247) PreFLMR: Scaling Up Fine-Grained Late-Interaction Multi-modal Retrievers (Weizhe Lin et al., 2024)</a></li><li><a href=#1834--18247-measuring-and-controlling-persona-drift-in-language-model-dialogs-kenneth-li-et-al-2024>(18/34 | 18/247) Measuring and Controlling Persona Drift in Language Model Dialogs (Kenneth Li et al., 2024)</a></li><li><a href=#1934--19247-higher-layers-need-more-lora-experts-chongyang-gao-et-al-2024>(19/34 | 19/247) Higher Layers Need More LoRA Experts (Chongyang Gao et al., 2024)</a></li><li><a href=#2034--20247-privacy-preserving-language-model-inference-with-instance-obfuscation-yixiang-yao-et-al-2024>(20/34 | 20/247) Privacy-Preserving Language Model Inference with Instance Obfuscation (Yixiang Yao et al., 2024)</a></li><li><a href=#2134--21247-lying-blindly-bypassing-chatgpts-safeguards-to-generate-hard-to-detect-disinformation-claims-at-scale-freddy-heppell-et-al-2024>(21/34 | 21/247) Lying Blindly: Bypassing ChatGPT&rsquo;s Safeguards to Generate Hard-to-Detect Disinformation Claims at Scale (Freddy Heppell et al., 2024)</a></li><li><a href=#2234--22247-a-survey-of-table-reasoning-with-large-language-models-xuanliang-zhang-et-al-2024>(22/34 | 22/247) A Survey of Table Reasoning with Large Language Models (Xuanliang Zhang et al., 2024)</a></li><li><a href=#2334--23247-knowledge-editing-on-black-box-large-language-models-xiaoshuai-song-et-al-2024>(23/34 | 23/247) Knowledge Editing on Black-box Large Language Models (Xiaoshuai Song et al., 2024)</a></li><li><a href=#2434--24247-test-time-backdoor-attacks-on-multimodal-large-language-models-dong-lu-et-al-2024>(24/34 | 24/247) Test-Time Backdoor Attacks on Multimodal Large Language Models (Dong Lu et al., 2024)</a></li><li><a href=#2534--25247-agent-smith-a-single-image-can-jailbreak-one-million-multimodal-llm-agents-exponentially-fast-xiangming-gu-et-al-2024>(25/34 | 25/247) Agent Smith: A Single Image Can Jailbreak One Million Multimodal LLM Agents Exponentially Fast (Xiangming Gu et al., 2024)</a></li><li><a href=#2634--26247-syllable-based-dnn-hmm-cantonese-speech-to-text-system-timothy-wong-et-al-2024>(26/34 | 26/247) Syllable based DNN-HMM Cantonese Speech to Text System (Timothy Wong et al., 2024)</a></li><li><a href=#2734--27247-semrel2024-a-collection-of-semantic-textual-relatedness-datasets-for-14-languages-nedjma-ousidhoum-et-al-2024>(27/34 | 27/247) SemRel2024: A Collection of Semantic Textual Relatedness Datasets for 14 Languages (Nedjma Ousidhoum et al., 2024)</a></li><li><a href=#2834--28247-improving-factual-error-correction-for-abstractive-summarization-via-data-distillation-and-conditional-generation-cloze-yiyang-li-et-al-2024>(28/34 | 28/247) Improving Factual Error Correction for Abstractive Summarization via Data Distillation and Conditional-generation Cloze (Yiyang Li et al., 2024)</a></li><li><a href=#2934--29247-llms-and-the-human-condition-peter-wallis-2024>(29/34 | 29/247) LLMs and the Human Condition (Peter Wallis, 2024)</a></li><li><a href=#3034--30247-large-language-models-as-minecraft-agents-chris-madge-et-al-2024>(30/34 | 30/247) Large Language Models as Minecraft Agents (Chris Madge et al., 2024)</a></li><li><a href=#3134--31247-explicit-references-to-social-values-in-fairy-tales-a-comparison-between-three-european-cultures-alba-morollon-diaz-faes-et-al-2024>(31/34 | 31/247) Explicit References to Social Values in Fairy Tales: A Comparison between Three European Cultures (Alba Morollon Diaz-Faes et al., 2024)</a></li><li><a href=#3234--32247-chatcell-facilitating-single-cell-analysis-with-natural-language-yin-fang-et-al-2024>(32/34 | 32/247) ChatCell: Facilitating Single-Cell Analysis with Natural Language (Yin Fang et al., 2024)</a></li><li><a href=#3334--33247-a-systematic-review-of-data-to-text-nlg-chinonso-cynthia-osuji-et-al-2024>(33/34 | 33/247) A Systematic Review of Data-to-Text NLG (Chinonso Cynthia Osuji et al., 2024)</a></li><li><a href=#3434--34247-cma-rcausal-mediation-analysis-for-explaining-rumour-detection-lin-tian-et-al-2024>(34/34 | 34/247) CMA-R:Causal Mediation Analysis for Explaining Rumour Detection (Lin Tian et al., 2024)</a></li></ul></li><li><a href=#cslg-52>cs.LG (52)</a><ul><li><a href=#152--35247-cold-attack-jailbreaking-llms-with-stealthiness-and-controllability-xingang-guo-et-al-2024>(1/52 | 35/247) COLD-Attack: Jailbreaking LLMs with Stealthiness and Controllability (Xingang Guo et al., 2024)</a></li><li><a href=#252--36247-llaga-large-language-and-graph-assistant-runjin-chen-et-al-2024>(2/52 | 36/247) LLaGA: Large Language and Graph Assistant (Runjin Chen et al., 2024)</a></li><li><a href=#352--37247-mitigating-object-hallucination-in-large-vision-language-models-via-classifier-free-guidance-linxi-zhao-et-al-2024>(3/52 | 37/247) Mitigating Object Hallucination in Large Vision-Language Models via Classifier-Free Guidance (Linxi Zhao et al., 2024)</a></li><li><a href=#452--38247-disambiguated-node-classification-with-graph-neural-networks-tianxiang-zhao-et-al-2024>(4/52 | 38/247) Disambiguated Node Classification with Graph Neural Networks (Tianxiang Zhao et al., 2024)</a></li><li><a href=#552--39247-improving-black-box-robustness-with-in-context-rewriting-kyle-obrien-et-al-2024>(5/52 | 39/247) Improving Black-box Robustness with In-Context Rewriting (Kyle O&rsquo;Brien et al., 2024)</a></li><li><a href=#652--40247-graph-mamba-towards-learning-on-graphs-with-state-space-models-ali-behrouz-et-al-2024>(6/52 | 40/247) Graph Mamba: Towards Learning on Graphs with State Space Models (Ali Behrouz et al., 2024)</a></li><li><a href=#752--41247-rethinking-machine-unlearning-for-large-language-models-sijia-liu-et-al-2024>(7/52 | 41/247) Rethinking Machine Unlearning for Large Language Models (Sijia Liu et al., 2024)</a></li><li><a href=#852--42247-prompted-contextual-vectors-for-spear-phishing-detection-daniel-nahmias-et-al-2024>(8/52 | 42/247) Prompted Contextual Vectors for Spear-Phishing Detection (Daniel Nahmias et al., 2024)</a></li><li><a href=#952--43247-prdp-proximal-reward-difference-prediction-for-large-scale-reward-finetuning-of-diffusion-models-fei-deng-et-al-2024>(9/52 | 43/247) PRDP: Proximal Reward Difference Prediction for Large-Scale Reward Finetuning of Diffusion Models (Fei Deng et al., 2024)</a></li><li><a href=#1052--44247-fedlps-heterogeneous-federated-learning-for-multiple-tasks-with-local-parameter-sharing-yongzhe-jia-et-al-2024>(10/52 | 44/247) FedLPS: Heterogeneous Federated Learning for Multiple Tasks with Local Parameter Sharing (Yongzhe Jia et al., 2024)</a></li><li><a href=#1152--45247-apalu-a-trainable-adaptive-activation-function-for-deep-learning-networks-barathi-subramanian-et-al-2024>(11/52 | 45/247) APALU: A Trainable, Adaptive Activation Function for Deep Learning Networks (Barathi Subramanian et al., 2024)</a></li><li><a href=#1252--46247-world-model-on-million-length-video-and-language-with-ringattention-hao-liu-et-al-2024>(12/52 | 46/247) World Model on Million-Length Video And Language With RingAttention (Hao Liu et al., 2024)</a></li><li><a href=#1352--47247-sagman-stability-analysis-of-graph-neural-networks-on-the-manifolds-wuxinlin-cheng-et-al-2024>(13/52 | 47/247) SAGMAN: Stability Analysis of Graph Neural Networks on the Manifolds (Wuxinlin Cheng et al., 2024)</a></li><li><a href=#1452--48247-subgraphormer-unifying-subgraph-gnns-and-graph-transformers-via-graph-products-guy-bar-shalom-et-al-2024>(14/52 | 48/247) Subgraphormer: Unifying Subgraph GNNs and Graph Transformers via Graph Products (Guy Bar-Shalom et al., 2024)</a></li><li><a href=#1552--49247-loss-gat-label-propagation-and-one-class-semi-supervised-graph-attention-network-for-fake-news-detection-batool-lakzaei-et-al-2024>(15/52 | 49/247) LOSS-GAT: Label Propagation and One-Class Semi-Supervised Graph Attention Network for Fake News Detection (Batool Lakzaei et al., 2024)</a></li><li><a href=#1652--50247-multi-level-gnn-preconditioner-for-solving-large-scale-problems-matthieu-nastorg-et-al-2024>(16/52 | 50/247) Multi-Level GNN Preconditioner for Solving Large Scale Problems (Matthieu Nastorg et al., 2024)</a></li><li><a href=#1752--51247-investigating-out-of-distribution-generalization-of-gnns-an-architecture-perspective-kai-guo-et-al-2024>(17/52 | 51/247) Investigating Out-of-Distribution Generalization of GNNs: An Architecture Perspective (Kai Guo et al., 2024)</a></li><li><a href=#1852--52247-intelligent-agricultural-management-considering-n_2o-emission-and-climate-variability-with-uncertainties-zhaoan-wang-et-al-2024>(18/52 | 52/247) Intelligent Agricultural Management Considering N$_2$O Emission and Climate Variability with Uncertainties (Zhaoan Wang et al., 2024)</a></li><li><a href=#1952--53247-mixtures-of-experts-unlock-parameter-scaling-for-deep-rl-johan-obando-ceron-et-al-2024>(19/52 | 53/247) Mixtures of Experts Unlock Parameter Scaling for Deep RL (Johan Obando-Ceron et al., 2024)</a></li><li><a href=#2052--54247-a-distributional-analogue-to-the-successor-representation-harley-wiltzer-et-al-2024>(20/52 | 54/247) A Distributional Analogue to the Successor Representation (Harley Wiltzer et al., 2024)</a></li><li><a href=#2152--55247-mixture-of-link-predictors-li-ma-et-al-2024>(21/52 | 55/247) Mixture of Link Predictors (Li Ma et al., 2024)</a></li><li><a href=#2252--56247-concept-1k-a-novel-benchmark-for-instance-incremental-learning-junhao-zheng-et-al-2024>(22/52 | 56/247) Concept-1K: A Novel Benchmark for Instance Incremental Learning (Junhao Zheng et al., 2024)</a></li><li><a href=#2352--57247-revealing-decurve-flows-for-generalized-graph-propagation-chen-lin-et-al-2024>(23/52 | 57/247) Revealing Decurve Flows for Generalized Graph Propagation (Chen Lin et al., 2024)</a></li><li><a href=#2452--58247-parallel-friendly-spatio-temporal-graph-learning-for-photovoltaic-degradation-analysis-at-scale-yangxin-fan-et-al-2024>(24/52 | 58/247) Parallel-friendly Spatio-Temporal Graph Learning for Photovoltaic Degradation Analysis at Scale (Yangxin Fan et al., 2024)</a></li><li><a href=#2552--59247-learning-time-dependent-pde-via-graph-neural-networks-and-deep-operator-network-for-robust-accuracy-on-irregular-grids-sung-woong-cho-et-al-2024>(25/52 | 59/247) Learning time-dependent PDE via graph neural networks and deep operator network for robust accuracy on irregular grids (Sung Woong Cho et al., 2024)</a></li><li><a href=#2652--60247-improving-molecule-generation-and-drug-discovery-with-a-knowledge-enhanced-generative-model-aditya-malusare-et-al-2024>(26/52 | 60/247) Improving Molecule Generation and Drug Discovery with a Knowledge-enhanced Generative Model (Aditya Malusare et al., 2024)</a></li><li><a href=#2752--61247-flash-federated-learning-across-simultaneous-heterogeneities-xiangyu-chang-et-al-2024>(27/52 | 61/247) FLASH: Federated Learning Across Simultaneous Heterogeneities (Xiangyu Chang et al., 2024)</a></li><li><a href=#2852--62247-generative-vs-non-generative-models-in-engineering-shape-optimization-muhammad-usama-et-al-2024>(28/52 | 62/247) Generative VS non-Generative Models in Engineering Shape Optimization (Muhammad Usama et al., 2024)</a></li><li><a href=#2952--63247-uncertainty-quantification-via-stable-distribution-propagation-felix-petersen-et-al-2024>(29/52 | 63/247) Uncertainty Quantification via Stable Distribution Propagation (Felix Petersen et al., 2024)</a></li><li><a href=#3052--64247-thresholding-data-shapley-for-data-cleansing-using-multi-armed-bandits-hiroyuki-namba-et-al-2024>(30/52 | 64/247) Thresholding Data Shapley for Data Cleansing Using Multi-Armed Bandits (Hiroyuki Namba et al., 2024)</a></li><li><a href=#3152--65247-homomorphism-counts-for-graph-neural-networks-all-about-that-basis-emily-jin-et-al-2024>(31/52 | 65/247) Homomorphism Counts for Graph Neural Networks: All About That Basis (Emily Jin et al., 2024)</a></li><li><a href=#3252--66247-transition-constrained-bayesian-optimization-via-markov-decision-processes-jose-pablo-folch-et-al-2024>(32/52 | 66/247) Transition Constrained Bayesian Optimization via Markov Decision Processes (Jose Pablo Folch et al., 2024)</a></li><li><a href=#3352--67247-the-effect-of-data-poisoning-on-counterfactual-explanations-andré-artelt-et-al-2024>(33/52 | 67/247) The Effect of Data Poisoning on Counterfactual Explanations (André Artelt et al., 2024)</a></li><li><a href=#3452--68247-feature-attribution-with-necessity-and-sufficiency-via-dual-stage-perturbation-test-for-causal-explanation-xuexin-chen-et-al-2024>(34/52 | 68/247) Feature Attribution with Necessity and Sufficiency via Dual-stage Perturbation Test for Causal Explanation (Xuexin Chen et al., 2024)</a></li><li><a href=#3552--69247-graph-feature-preprocessor-real-time-extraction-of-subgraph-based-features-from-transaction-graphs-jovan-blanuša-et-al-2024>(35/52 | 69/247) Graph Feature Preprocessor: Real-time Extraction of Subgraph-based Features from Transaction Graphs (Jovan Blanuša et al., 2024)</a></li><li><a href=#3652--70247-neures-learning-proofs-of-propositional-satisfiability-mohamed-ghanem-et-al-2024>(36/52 | 70/247) NeuRes: Learning Proofs of Propositional Satisfiability (Mohamed Ghanem et al., 2024)</a></li><li><a href=#3752--71247-distal-interference-exploring-the-limits-of-model-based-continual-learning-heinrich-van-deventer-et-al-2024>(37/52 | 71/247) Distal Interference: Exploring the Limits of Model-Based Continual Learning (Heinrich van Deventer et al., 2024)</a></li><li><a href=#3852--72247-approximation-of-relation-functions-and-attention-mechanisms-awni-altabaa-et-al-2024>(38/52 | 72/247) Approximation of relation functions and attention mechanisms (Awni Altabaa et al., 2024)</a></li><li><a href=#3952--73247-hybrid-inverse-reinforcement-learning-juntao-ren-et-al-2024>(39/52 | 73/247) Hybrid Inverse Reinforcement Learning (Juntao Ren et al., 2024)</a></li><li><a href=#4052--74247-projection-free-online-convex-optimization-with-time-varying-constraints-dan-garber-et-al-2024>(40/52 | 74/247) Projection-Free Online Convex Optimization with Time-Varying Constraints (Dan Garber et al., 2024)</a></li><li><a href=#4152--75247-bayesian-strategic-classification-lee-cohen-et-al-2024>(41/52 | 75/247) Bayesian Strategic Classification (Lee Cohen et al., 2024)</a></li><li><a href=#4252--76247-a-convergence-analysis-of-approximate-message-passing-with-non-separable-functions-and-applications-to-multi-class-classification-burak-çakmak-et-al-2024>(42/52 | 76/247) A Convergence Analysis of Approximate Message Passing with Non-Separable Functions and Applications to Multi-Class Classification (Burak Çakmak et al., 2024)</a></li><li><a href=#4352--77247-generating-universal-adversarial-perturbations-for-quantum-classifiers-gautham-anil-et-al-2024>(43/52 | 77/247) Generating Universal Adversarial Perturbations for Quantum Classifiers (Gautham Anil et al., 2024)</a></li><li><a href=#4452--78247-a-generalized-approach-to-online-convex-optimization-mohammad-pedramfar-et-al-2024>(44/52 | 78/247) A Generalized Approach to Online Convex Optimization (Mohammad Pedramfar et al., 2024)</a></li><li><a href=#4552--79247-fairness-auditing-with-multi-agent-collaboration-martijn-de-vos-et-al-2024>(45/52 | 79/247) Fairness Auditing with Multi-Agent Collaboration (Martijn de Vos et al., 2024)</a></li><li><a href=#4652--80247-provable-traffic-rule-compliance-in-safe-reinforcement-learning-on-the-open-sea-hanna-krasowski-et-al-2024>(46/52 | 80/247) Provable Traffic Rule Compliance in Safe Reinforcement Learning on the Open Sea (Hanna Krasowski et al., 2024)</a></li><li><a href=#4752--81247-deep-reinforcement-learning-for-controlled-traversing-of-the-attractor-landscape-of-boolean-models-in-the-context-of-cellular-reprogramming-andrzej-mizera-et-al-2024>(47/52 | 81/247) Deep Reinforcement Learning for Controlled Traversing of the Attractor Landscape of Boolean Models in the Context of Cellular Reprogramming (Andrzej Mizera et al., 2024)</a></li><li><a href=#4852--82247-conservative-and-risk-aware-offline-multi-agent-reinforcement-learning-for-digital-twins-eslam-eldeeb-et-al-2024>(48/52 | 82/247) Conservative and Risk-Aware Offline Multi-Agent Reinforcement Learning for Digital Twins (Eslam Eldeeb et al., 2024)</a></li><li><a href=#4952--83247-helping-university-students-to-choose-elective-courses-by-using-a-hybrid-multi-criteria-recommendation-system-with-genetic-optimization-a-esteban-et-al-2024>(49/52 | 83/247) Helping university students to choose elective courses by using a hybrid multi-criteria recommendation system with genetic optimization (A. Esteban et al., 2024)</a></li><li><a href=#5052--84247-randomized-algorithms-for-symmetric-nonnegative-matrix-factorization-koby-hayashi-et-al-2024>(50/52 | 84/247) Randomized Algorithms for Symmetric Nonnegative Matrix Factorization (Koby Hayashi et al., 2024)</a></li><li><a href=#5152--85247-gaussian-ensemble-belief-propagation-for-efficient-inference-in-high-dimensional-systems-dan-mackinlay-et-al-2024>(51/52 | 85/247) Gaussian Ensemble Belief Propagation for Efficient Inference in High-Dimensional Systems (Dan MacKinlay et al., 2024)</a></li><li><a href=#5252--86247-causal-discovery-under-off-target-interventions-davin-choo-et-al-2024>(52/52 | 86/247) Causal Discovery under Off-Target Interventions (Davin Choo et al., 2024)</a></li></ul></li><li><a href=#cscr-5>cs.CR (5)</a><ul><li><a href=#15--87247-pandora-jailbreak-gpts-by-retrieval-augmented-generation-poisoning-gelei-deng-et-al-2024>(1/5 | 87/247) Pandora: Jailbreak GPTs by Retrieval Augmented Generation Poisoning (Gelei Deng et al., 2024)</a></li><li><a href=#25--88247-data-reconstruction-attacks-and-defenses-a-systematic-evaluation-sheng-liu-et-al-2024>(2/5 | 88/247) Data Reconstruction Attacks and Defenses: A Systematic Evaluation (Sheng Liu et al., 2024)</a></li><li><a href=#35--89247-cryptomite-a-versatile-and-user-friendly-library-of-randomness-extractors-cameron-foreman-et-al-2024>(3/5 | 89/247) Cryptomite: A versatile and user-friendly library of randomness extractors (Cameron Foreman et al., 2024)</a></li><li><a href=#45--90247-zero-trust-score-based-network-level-access-control-in-enterprise-networks-leonard-bradatsch-et-al-2024>(4/5 | 90/247) Zero Trust Score-based Network-level Access Control in Enterprise Networks (Leonard Bradatsch et al., 2024)</a></li><li><a href=#55--91247-neurobench-an-open-source-benchmark-framework-for-the-standardization-of-methodology-in-brainwave-based-authentication-research-avinash-kumar-chaurasia-et-al-2024>(5/5 | 91/247) NeuroBench: An Open-Source Benchmark Framework for the Standardization of Methodology in Brainwave-based Authentication Research (Avinash Kumar Chaurasia et al., 2024)</a></li></ul></li><li><a href=#csro-9>cs.RO (9)</a><ul><li><a href=#19--92247-grounding-llms-for-robot-task-planning-using-closed-loop-state-feedback-vineet-bhat-et-al-2024>(1/9 | 92/247) Grounding LLMs For Robot Task Planning Using Closed-loop State Feedback (Vineet Bhat et al., 2024)</a></li><li><a href=#29--93247-online-foundation-model-selection-in-robotics-po-han-li-et-al-2024>(2/9 | 93/247) Online Foundation Model Selection in Robotics (Po-han Li et al., 2024)</a></li><li><a href=#39--94247-the-colosseum-a-benchmark-for-evaluating-generalization-for-robotic-manipulation-wilbert-pumacay-et-al-2024>(3/9 | 94/247) THE COLOSSEUM: A Benchmark for Evaluating Generalization for Robotic Manipulation (Wilbert Pumacay et al., 2024)</a></li><li><a href=#49--95247-gaussian-sum-filter-for-range-based-3d-relative-pose-estimation-in-the-presence-of-ambiguities-syed-s-ahmed-et-al-2024>(4/9 | 95/247) Gaussian-Sum Filter for Range-based 3D Relative Pose Estimation in the Presence of Ambiguities (Syed S. Ahmed et al., 2024)</a></li><li><a href=#59--96247-mavrl-learn-to-fly-in-cluttered-environments-with-varying-speed-hang-yu-et-al-2024>(5/9 | 96/247) MAVRL: Learn to Fly in Cluttered Environments with Varying Speed (Hang Yu et al., 2024)</a></li><li><a href=#69--97247-self-reconfigurable-v-shape-formation-of-multiple-uavs-in-narrow-space-environments-duy-nam-bui-et-al-2024>(6/9 | 97/247) Self-Reconfigurable V-shape Formation of Multiple UAVs in Narrow Space Environments (Duy Nam Bui et al., 2024)</a></li><li><a href=#79--98247-metatra-meta-learning-for-generalized-trajectory-prediction-in-unseen-domain-xiaohe-li-et-al-2024>(7/9 | 98/247) MetaTra: Meta-Learning for Generalized Trajectory Prediction in Unseen Domain (Xiaohe Li et al., 2024)</a></li><li><a href=#89--99247-bbsea-an-exploration-of-brain-body-synchronization-for-embodied-agents-sizhe-yang-et-al-2024>(8/9 | 99/247) BBSEA: An Exploration of Brain-Body Synchronization for Embodied Agents (Sizhe Yang et al., 2024)</a></li><li><a href=#99--100247-approximate-sequential-optimization-for-informative-path-planning-joshua-ott-et-al-2024>(9/9 | 100/247) Approximate Sequential Optimization for Informative Path Planning (Joshua Ott et al., 2024)</a></li></ul></li><li><a href=#cscv-37>cs.CV (37)</a><ul><li><a href=#137--101247-pin-positional-insert-unlocks-object-localisation-abilities-in-vlms-michael-dorkenwald-et-al-2024>(1/37 | 101/247) PIN: Positional Insert Unlocks Object Localisation Abilities in VLMs (Michael Dorkenwald et al., 2024)</a></li><li><a href=#237--102247-visual-question-answering-instruction-unlocking-multimodal-large-language-model-to-domain-specific-visual-multitasks-jusung-lee-et-al-2024>(2/37 | 102/247) Visual Question Answering Instruction: Unlocking Multimodal Large Language Model To Domain-Specific Visual Multitasks (Jusung Lee et al., 2024)</a></li><li><a href=#337--103247-befunet-a-hybrid-cnn-transformer-architecture-for-precise-medical-image-segmentation-omid-nejati-manzari-et-al-2024>(3/37 | 103/247) BEFUnet: A Hybrid CNN-Transformer Architecture for Precise Medical Image Segmentation (Omid Nejati Manzari et al., 2024)</a></li><li><a href=#437--104247-visually-dehallucinative-instruction-generation-sungguk-cha-et-al-2024>(4/37 | 104/247) Visually Dehallucinative Instruction Generation (Sungguk Cha et al., 2024)</a></li><li><a href=#537--105247-leveraging-self-supervised-instance-contrastive-learning-for-radar-object-detection-colin-decourt-et-al-2024>(5/37 | 105/247) Leveraging Self-Supervised Instance Contrastive Learning for Radar Object Detection (Colin Decourt et al., 2024)</a></li><li><a href=#637--106247-peeking-behind-the-curtains-of-residual-learning-tunhou-zhang-et-al-2024>(6/37 | 106/247) Peeking Behind the Curtains of Residual Learning (Tunhou Zhang et al., 2024)</a></li><li><a href=#737--107247-towards-the-detection-of-ai-synthesized-human-face-images-yuhang-lu-et-al-2024>(7/37 | 107/247) Towards the Detection of AI-Synthesized Human Face Images (Yuhang Lu et al., 2024)</a></li><li><a href=#837--108247-intriguing-differences-between-zero-shot-and-systematic-evaluations-of-vision-language-transformer-models-shaeke-salman-et-al-2024>(8/37 | 108/247) Intriguing Differences Between Zero-Shot and Systematic Evaluations of Vision-Language Transformer Models (Shaeke Salman et al., 2024)</a></li><li><a href=#937--109247-automated-detection-of-motion-artifacts-in-brain-mr-images-using-deep-learning-and-explainable-artificial-intelligence-marina-manso-jimeno-et-al-2024>(9/37 | 109/247) Automated detection of motion artifacts in brain MR images using deep learning and explainable artificial intelligence (Marina Manso Jimeno et al., 2024)</a></li><li><a href=#1037--110247-im-3d-iterative-multiview-diffusion-and-reconstruction-for-high-quality-3d-generation-luke-melas-kyriazi-et-al-2024>(10/37 | 110/247) IM-3D: Iterative Multiview Diffusion and Reconstruction for High-Quality 3D Generation (Luke Melas-Kyriazi et al., 2024)</a></li><li><a href=#1137--111247-learning-continuous-3d-words-for-text-to-image-generation-ta-ying-cheng-et-al-2024>(11/37 | 111/247) Learning Continuous 3D Words for Text-to-Image Generation (Ta-Ying Cheng et al., 2024)</a></li><li><a href=#1237--112247-p-mamba-marrying-perona-malik-diffusion-with-mamba-for-efficient-pediatric-echocardiographic-left-ventricular-segmentation-zi-ye-et-al-2024>(12/37 | 112/247) P-Mamba: Marrying Perona Malik Diffusion with Mamba for Efficient Pediatric Echocardiographic Left Ventricular Segmentation (Zi Ye et al., 2024)</a></li><li><a href=#1337--113247-latent-space-configuration-for-improved-generalization-in-supervised-autoencoder-neural-networks-nikita-gabdullin-2024>(13/37 | 113/247) Latent space configuration for improved generalization in supervised autoencoder neural networks (Nikita Gabdullin, 2024)</a></li><li><a href=#1437--114247-conditional-information-gain-trellis-ufuk-can-bicici-et-al-2024>(14/37 | 114/247) Conditional Information Gain Trellis (Ufuk Can Bicici et al., 2024)</a></li><li><a href=#1537--115247-scribble-based-fast-weak-supervision-and-interactive-corrections-for-segmenting-whole-slide-images-antoine-habis-et-al-2024>(15/37 | 115/247) Scribble-based fast weak-supervision and interactive corrections for segmenting whole slide images (Antoine Habis et al., 2024)</a></li><li><a href=#1637--116247-a-dense-reward-view-on-aligning-text-to-image-diffusion-with-preference-shentao-yang-et-al-2024>(16/37 | 116/247) A Dense Reward View on Aligning Text-to-Image Diffusion with Preference (Shentao Yang et al., 2024)</a></li><li><a href=#1737--117247-fine-tuning-text-to-image-diffusion-models-for-class-wise-spurious-feature-generation-aprilpyone-maungmaung-et-al-2024>(17/37 | 117/247) Fine-Tuning Text-To-Image Diffusion Models for Class-Wise Spurious Feature Generation (AprilPyone MaungMaung et al., 2024)</a></li><li><a href=#1837--118247-ads-approximate-densest-subgraph-for-novel-image-discovery-shanfeng-hu-2024>(18/37 | 118/247) ADS: Approximate Densest Subgraph for Novel Image Discovery (Shanfeng Hu, 2024)</a></li><li><a href=#1937--119247-the-paradox-of-motion-evidence-for-spurious-correlations-in-skeleton-based-gait-recognition-models-andy-cătrună-et-al-2024>(19/37 | 119/247) The Paradox of Motion: Evidence for Spurious Correlations in Skeleton-based Gait Recognition Models (Andy Cătrună et al., 2024)</a></li><li><a href=#2037--120247-enhancing-robustness-of-indoor-robotic-navigation-with-free-space-segmentation-models-against-adversarial-attacks-qiyuan-an-et-al-2024>(20/37 | 120/247) Enhancing Robustness of Indoor Robotic Navigation with Free-Space Segmentation Models Against Adversarial Attacks (Qiyuan An et al., 2024)</a></li><li><a href=#2137--121247-latent-inversion-with-timestep-aware-sampling-for-training-free-non-rigid-editing-yunji-jung-et-al-2024>(21/37 | 121/247) Latent Inversion with Timestep-aware Sampling for Training-free Non-rigid Editing (Yunji Jung et al., 2024)</a></li><li><a href=#2237--122247-object-detection-in-thermal-images-using-deep-learning-for-unmanned-aerial-vehicles-minh-dang-tu-et-al-2024>(22/37 | 122/247) Object Detection in Thermal Images Using Deep Learning for Unmanned Aerial Vehicles (Minh Dang Tu et al., 2024)</a></li><li><a href=#2337--123247-randumb-a-simple-approach-that-questions-the-efficacy-of-continual-representation-learning-ameya-prabhu-et-al-2024>(23/37 | 123/247) RanDumb: A Simple Approach that Questions the Efficacy of Continual Representation Learning (Ameya Prabhu et al., 2024)</a></li><li><a href=#2437--124247-one-to-many-reconstruction-of-3d-geometry-of-cultural-artifacts-using-a-synthetically-trained-generative-model-thomas-pöllabauer-et-al-2024>(24/37 | 124/247) One-to-many Reconstruction of 3D Geometry of cultural Artifacts using a synthetically trained Generative Model (Thomas Pöllabauer et al., 2024)</a></li><li><a href=#2537--125247-bdslw60-a-word-level-bangla-sign-language-dataset-husne-ara-rubaiyeat-et-al-2024>(25/37 | 125/247) BdSLW60: A Word-Level Bangla Sign Language Dataset (Husne Ara Rubaiyeat et al., 2024)</a></li><li><a href=#2637--126247-camera-calibration-through-geometric-constraints-from-rotation-and-projection-matrices-muhammad-waleed-et-al-2024>(26/37 | 126/247) Camera Calibration through Geometric Constraints from Rotation and Projection Matrices (Muhammad Waleed et al., 2024)</a></li><li><a href=#2737--127247-seprep-net-multi-source-free-domain-adaptation-via-model-separation-and-reparameterization-ying-jin-et-al-2024>(27/37 | 127/247) SepRep-Net: Multi-source Free Domain Adaptation via Model Separation And Reparameterization (Ying Jin et al., 2024)</a></li><li><a href=#2837--128247-optimized-information-flow-for-transformer-tracking-janani-kugarajeevan-et-al-2024>(28/37 | 128/247) Optimized Information Flow for Transformer Tracking (Janani Kugarajeevan et al., 2024)</a></li><li><a href=#2937--129247-fess-loss-feature-enhanced-spatial-segmentation-loss-for-optimizing-medical-image-analysis-charulkumar-chodvadiya-et-al-2024>(29/37 | 129/247) FESS Loss: Feature-Enhanced Spatial Segmentation Loss for Optimizing Medical Image Analysis (Charulkumar Chodvadiya et al., 2024)</a></li><li><a href=#3037--130247-an-order-complexity-aesthetic-assessment-model-for-aesthetic-aware-music-recommendation-xin-jin-et-al-2024>(30/37 | 130/247) An Order-Complexity Aesthetic Assessment Model for Aesthetic-aware Music Recommendation (Xin Jin et al., 2024)</a></li><li><a href=#3137--131247-improving-image-coding-for-machines-through-optimizing-encoder-via-auxiliary-loss-kei-iino-et-al-2024>(31/37 | 131/247) Improving Image Coding for Machines through Optimizing Encoder via Auxiliary Loss (Kei Iino et al., 2024)</a></li><li><a href=#3237--132247-translating-images-to-road-networka-non-autoregressive-sequence-to-sequence-approach-jiachen-lu-et-al-2024>(32/37 | 132/247) Translating Images to Road Network:A Non-Autoregressive Sequence-to-Sequence Approach (Jiachen Lu et al., 2024)</a></li><li><a href=#3337--133247-amend-a-mixture-of-experts-framework-for-long-tailed-trajectory-prediction-ray-coden-mercurius-et-al-2024>(33/37 | 133/247) AMEND: A Mixture of Experts Framework for Long-tailed Trajectory Prediction (Ray Coden Mercurius et al., 2024)</a></li><li><a href=#3437--134247-preconditioners-for-the-stochastic-training-of-implicit-neural-representations-shin-fang-chng-et-al-2024>(34/37 | 134/247) Preconditioners for the Stochastic Training of Implicit Neural Representations (Shin-Fang Chng et al., 2024)</a></li><li><a href=#3537--135247-nerf-analogies-example-based-visual-attribute-transfer-for-nerfs-michael-fischer-et-al-2024>(35/37 | 135/247) NeRF Analogies: Example-Based Visual Attribute Transfer for NeRFs (Michael Fischer et al., 2024)</a></li><li><a href=#3637--136247-learning-to-produce-semi-dense-correspondences-for-visual-localization-khang-truong-giang-et-al-2024>(36/37 | 136/247) Learning to Produce Semi-dense Correspondences for Visual Localization (Khang Truong Giang et al., 2024)</a></li><li><a href=#3737--137247-crossgaze-a-strong-method-for-3d-gaze-estimation-in-the-wild-andy-cătrună-et-al-2024>(37/37 | 137/247) CrossGaze: A Strong Method for 3D Gaze Estimation in the Wild (Andy Cătrună et al., 2024)</a></li></ul></li><li><a href=#csai-27>cs.AI (27)</a><ul><li><a href=#127--138247-combining-insights-from-multiple-large-language-models-improves-diagnostic-accuracy-gioele-barabucci-et-al-2024>(1/27 | 138/247) Combining Insights From Multiple Large Language Models Improves Diagnostic Accuracy (Gioele Barabucci et al., 2024)</a></li><li><a href=#227--139247-rec-gpt4v-multimodal-recommendation-with-large-vision-language-models-yuqing-liu-et-al-2024>(2/27 | 139/247) Rec-GPT4V: Multimodal Recommendation with Large Vision-Language Models (Yuqing Liu et al., 2024)</a></li><li><a href=#327--140247-llm-driven-imitation-of-subrational-behavior--illusion-or-reality-andrea-coletta-et-al-2024>(3/27 | 140/247) LLM-driven Imitation of Subrational Behavior : Illusion or Reality? (Andrea Coletta et al., 2024)</a></li><li><a href=#427--141247-large-language-models-for-the-automated-analysis-of-optimization-algorithms-camilo-chacón-sartori-et-al-2024>(4/27 | 141/247) Large Language Models for the Automated Analysis of Optimization Algorithms (Camilo Chacón Sartori et al., 2024)</a></li><li><a href=#527--142247-lota-bench-benchmarking-language-oriented-task-planners-for-embodied-agents-jae-woo-choi-et-al-2024>(5/27 | 142/247) LoTa-Bench: Benchmarking Language-oriented Task Planners for Embodied Agents (Jae-Woo Choi et al., 2024)</a></li><li><a href=#627--143247-one-shot-imitation-in-a-non-stationary-environment-via-multi-modal-skill-sangwoo-shin-et-al-2024>(6/27 | 143/247) One-shot Imitation in a Non-Stationary Environment via Multi-Modal Skill (Sangwoo Shin et al., 2024)</a></li><li><a href=#727--144247-tandem-transformers-for-inference-efficient-llms-aishwarya-p-s-et-al-2024>(7/27 | 144/247) Tandem Transformers for Inference Efficient LLMs (Aishwarya P S et al., 2024)</a></li><li><a href=#827--145247-the-application-of-chatgpt-in-responding-to-questions-related-to-the-boston-bowel-preparation-scale-xiaoqiang-liu-et-al-2024>(8/27 | 145/247) The Application of ChatGPT in Responding to Questions Related to the Boston Bowel Preparation Scale (Xiaoqiang Liu et al., 2024)</a></li><li><a href=#927--146247-transformer-mechanisms-mimic-frontostriatal-gating-operations-when-trained-on-human-working-memory-tasks-aaron-traylor-et-al-2024>(9/27 | 146/247) Transformer Mechanisms Mimic Frontostriatal Gating Operations When Trained on Human Working Memory Tasks (Aaron Traylor et al., 2024)</a></li><li><a href=#1027--147247-enabling-multi-agent-transfer-reinforcement-learning-via-scenario-independent-representation-ayesha-siddika-nipu-et-al-2024>(10/27 | 147/247) Enabling Multi-Agent Transfer Reinforcement Learning via Scenario Independent Representation (Ayesha Siddika Nipu et al., 2024)</a></li><li><a href=#1127--148247-counterfactual-influence-in-markov-decision-processes-milad-kazemi-et-al-2024>(11/27 | 148/247) Counterfactual Influence in Markov Decision Processes (Milad Kazemi et al., 2024)</a></li><li><a href=#1227--149247-pix2code-learning-to-compose-neural-visual-concepts-as-programs-antonia-wüst-et-al-2024>(12/27 | 149/247) Pix2Code: Learning to Compose Neural Visual Concepts as Programs (Antonia Wüst et al., 2024)</a></li><li><a href=#1327--150247-bert4fca-a-method-for-bipartite-link-prediction-using-formal-concept-analysis-and-bert-siqi-peng-et-al-2024>(13/27 | 150/247) BERT4FCA: A Method for Bipartite Link Prediction using Formal Concept Analysis and BERT (Siqi Peng et al., 2024)</a></li><li><a href=#1427--151247-vehicle-behavior-prediction-by-episodic-memory-implanted-ndt-peining-shen-et-al-2024>(14/27 | 151/247) Vehicle Behavior Prediction by Episodic-Memory Implanted NDT (Peining Shen et al., 2024)</a></li><li><a href=#1527--152247-a-logical-approach-to-criminal-case-investigation-takanori-ugai-et-al-2024>(15/27 | 152/247) A Logical Approach to Criminal Case Investigation (Takanori Ugai et al., 2024)</a></li><li><a href=#1627--153247-advancing-data-driven-weather-forecasting-time-sliding-data-augmentation-of-era5-minjong-cheon-et-al-2024>(16/27 | 153/247) Advancing Data-driven Weather Forecasting: Time-Sliding Data Augmentation of ERA5 (Minjong Cheon et al., 2024)</a></li><li><a href=#1727--154247-epistemic-exploration-for-generalizable-planning-and-learning-in-non-stationary-settings-rushang-karia-et-al-2024>(17/27 | 154/247) Epistemic Exploration for Generalizable Planning and Learning in Non-Stationary Settings (Rushang Karia et al., 2024)</a></li><li><a href=#1827--155247-optimal-task-assignment-and-path-planning-using-conflict-based-search-with-precedence-and-temporal-constraints-yu-quan-chong-et-al-2024>(18/27 | 155/247) Optimal Task Assignment and Path Planning using Conflict-Based Search with Precedence and Temporal Constraints (Yu Quan Chong et al., 2024)</a></li><li><a href=#1927--156247-inference-of-abstraction-for-a-unified-account-of-symbolic-reasoning-from-data-hiroyuki-kido-2024>(19/27 | 156/247) Inference of Abstraction for a Unified Account of Symbolic Reasoning from Data (Hiroyuki Kido, 2024)</a></li><li><a href=#2027--157247-artificial-intelligence-for-literature-reviews-opportunities-and-challenges-francisco-bolanos-et-al-2024>(20/27 | 157/247) Artificial Intelligence for Literature Reviews: Opportunities and Challenges (Francisco Bolanos et al., 2024)</a></li><li><a href=#2127--158247-taking-training-seriously-human-guidance-and-management-based-regulation-of-artificial-intelligence-cary-coglianese-et-al-2024>(21/27 | 158/247) Taking Training Seriously: Human Guidance and Management-Based Regulation of Artificial Intelligence (Cary Coglianese et al., 2024)</a></li><li><a href=#2227--159247-a-survey-of-recent-methods-for-addressing-ai-fairness-and-bias-in-biomedicine-yifan-yang-et-al-2024>(22/27 | 159/247) A survey of recent methods for addressing AI fairness and bias in biomedicine (Yifan Yang et al., 2024)</a></li><li><a href=#2327--160247-towards-equitable-agile-research-and-development-of-ai-and-robotics-andrew-hundt-et-al-2024>(23/27 | 160/247) Towards Equitable Agile Research and Development of AI and Robotics (Andrew Hundt et al., 2024)</a></li><li><a href=#2427--161247-hierarchical-position-embedding-of-graphs-with-landmarks-and-clustering-for-link-prediction-minsang-kim-et-al-2024>(24/27 | 161/247) Hierarchical Position Embedding of Graphs with Landmarks and Clustering for Link Prediction (Minsang Kim et al., 2024)</a></li><li><a href=#2527--162247-geometry-induced-implicit-regularization-in-deep-relu-neural-networks-joachim-bona-pellissier-et-al-2024>(25/27 | 162/247) Geometry-induced Implicit Regularization in Deep ReLU Neural Networks (Joachim Bona-Pellissier et al., 2024)</a></li><li><a href=#2627--163247-time-to-stop-and-think-what-kind-of-research-do-we-want-to-do-josu-ceberio-et-al-2024>(26/27 | 163/247) Time to Stop and Think: What kind of research do we want to do? (Josu Ceberio et al., 2024)</a></li><li><a href=#2727--164247-inherent-diverse-redundant-safety-mechanisms-for-ai-based-software-elements-in-automotive-applications-mandar-pitale-et-al-2024>(27/27 | 164/247) Inherent Diverse Redundant Safety Mechanisms for AI-based Software Elements in Automotive Applications (Mandar Pitale et al., 2024)</a></li></ul></li><li><a href=#csne-1>cs.NE (1)</a><ul><li><a href=#11--165247-human-curriculum-effects-emerge-with-in-context-learning-in-neural-networks-jacob-russin-et-al-2024>(1/1 | 165/247) Human Curriculum Effects Emerge with In-Context Learning in Neural Networks (Jacob Russin et al., 2024)</a></li></ul></li><li><a href=#csir-6>cs.IR (6)</a><ul><li><a href=#16--166247-modeling-balanced-explicit-and-implicit-relations-with-contrastive-learning-for-knowledge-concept-recommendation-in-moocs-hengnian-gu-et-al-2024>(1/6 | 166/247) Modeling Balanced Explicit and Implicit Relations with Contrastive Learning for Knowledge Concept Recommendation in MOOCs (Hengnian Gu et al., 2024)</a></li><li><a href=#26--167247-multi-label-zero-shot-product-attribute-value-extraction-jiaying-gong-et-al-2024>(2/6 | 167/247) Multi-Label Zero-Shot Product Attribute-Value Extraction (Jiaying Gong et al., 2024)</a></li><li><a href=#36--168247-causal-learning-for-trustworthy-recommender-systems-a-survey-jin-li-et-al-2024>(3/6 | 168/247) Causal Learning for Trustworthy Recommender Systems: A Survey (Jin Li et al., 2024)</a></li><li><a href=#46--169247-captions-are-worth-a-thousand-words-enhancing-product-retrieval-with-pretrained-image-to-text-models-jason-tang-et-al-2024>(4/6 | 169/247) Captions Are Worth a Thousand Words: Enhancing Product Retrieval with Pretrained Image-to-Text Models (Jason Tang et al., 2024)</a></li><li><a href=#56--170247-frequency-aware-graph-signal-processing-for-collaborative-filtering-jiafeng-xia-et-al-2024>(5/6 | 170/247) Frequency-aware Graph Signal Processing for Collaborative Filtering (Jiafeng Xia et al., 2024)</a></li><li><a href=#66--171247-implementation-of-recommendation-algorithm-based-on-recommendation-sessions-in-e-commerce-it-system-michał-malinowski-2024>(6/6 | 171/247) Implementation of Recommendation Algorithm based on Recommendation Sessions in E-commerce IT System (Michał Malinowski, 2024)</a></li></ul></li><li><a href=#cshc-8>cs.HC (8)</a><ul><li><a href=#18--172247-ghostwriter-augmenting-collaborative-human-ai-writing-experiences-through-personalization-and-agency-catherine-yeh-et-al-2024>(1/8 | 172/247) GhostWriter: Augmenting Collaborative Human-AI Writing Experiences Through Personalization and Agency (Catherine Yeh et al., 2024)</a></li><li><a href=#28--173247-the-last-jitai-the-unreasonable-effectiveness-of-large-language-models-in-issuing-just-in-time-adaptive-interventions-fostering-physical-activity-in-a-prospective-cardiac-rehabilitation-setting-david-haag-et-al-2024>(2/8 | 173/247) The Last JITAI? The Unreasonable Effectiveness of Large Language Models in Issuing Just-in-Time Adaptive Interventions: Fostering Physical Activity in a Prospective Cardiac Rehabilitation Setting (David Haag et al., 2024)</a></li><li><a href=#38--174247-vision-based-hand-gesture-customization-from-a-single-demonstration-soroush-shahi-et-al-2024>(3/8 | 174/247) Vision-Based Hand Gesture Customization from a Single Demonstration (Soroush Shahi et al., 2024)</a></li><li><a href=#48--175247-simulating-human-strategic-behavior-comparing-single-and-multi-agent-llms-karthik-sreedhar-et-al-2024>(4/8 | 175/247) Simulating Human Strategic Behavior: Comparing Single and Multi-agent LLMs (Karthik Sreedhar et al., 2024)</a></li><li><a href=#58--176247-intelligent-canvas-enabling-design-like-exploratory-visual-data-analysis-with-generative-ai-through-rapid-prototyping-iteration-and-curation-zijian-ding-et-al-2024>(5/8 | 176/247) Intelligent Canvas: Enabling Design-Like Exploratory Visual Data Analysis with Generative AI through Rapid Prototyping, Iteration and Curation (Zijian Ding et al., 2024)</a></li><li><a href=#68--177247-exploring-diversity-perceptions-in-a-community-through-a-qa-chatbot-peter-kun-et-al-2024>(6/8 | 177/247) Exploring diversity perceptions in a community through a Q&amp;A chatbot (Peter Kun et al., 2024)</a></li><li><a href=#78--178247-moonwalk-advancing-gait-based-user-recognition-on-wearable-devices-with-metric-learning-asaf-liberman-et-al-2024>(7/8 | 178/247) Moonwalk: Advancing Gait-Based User Recognition on Wearable Devices with Metric Learning (Asaf Liberman et al., 2024)</a></li><li><a href=#88--179247-mood-as-a-contextual-cue-for-improved-emotion-inference-soujanya-narayana-et-al-2024>(8/8 | 179/247) Mood as a Contextual Cue for Improved Emotion Inference (Soujanya Narayana et al., 2024)</a></li></ul></li><li><a href=#csse-9>cs.SE (9)</a><ul><li><a href=#19--180247-chatgpt-vs-llama-impact-reliability-and-challenges-in-stack-overflow-discussions-leuson-da-silva-et-al-2024>(1/9 | 180/247) ChatGPT vs LLaMA: Impact, Reliability, and Challenges in Stack Overflow Discussions (Leuson Da Silva et al., 2024)</a></li><li><a href=#29--181247-unsupervised-evaluation-of-code-llms-with-round-trip-correctness-miltiadis-allamanis-et-al-2024>(2/9 | 181/247) Unsupervised Evaluation of Code LLMs with Round-Trip Correctness (Miltiadis Allamanis et al., 2024)</a></li><li><a href=#39--182247-on-the-fly-syntax-highlighting-generalisation-and-speed-ups-marco-edoardo-palma-et-al-2024>(3/9 | 182/247) On-the-Fly Syntax Highlighting: Generalisation and Speed-ups (Marco Edoardo Palma et al., 2024)</a></li><li><a href=#49--183247-verified-multi-step-synthesis-using-large-language-models-and-monte-carlo-tree-search-david-brandfonbrener-et-al-2024>(4/9 | 183/247) Verified Multi-Step Synthesis using Large Language Models and Monte Carlo Tree Search (David Brandfonbrener et al., 2024)</a></li><li><a href=#59--184247-generating-java-methods-an-empirical-assessment-of-four-ai-based-code-assistants-vincenzo-corso-et-al-2024>(5/9 | 184/247) Generating Java Methods: An Empirical Assessment of Four AI-Based Code Assistants (Vincenzo Corso et al., 2024)</a></li><li><a href=#69--185247-analyzing-prompt-influence-on-automated-method-generation-an-empirical-study-with-copilot-ionut-daniel-fagadau-et-al-2024>(6/9 | 185/247) Analyzing Prompt Influence on Automated Method Generation: An Empirical Study with Copilot (Ionut Daniel Fagadau et al., 2024)</a></li><li><a href=#79--186247-migration-to-microservices-a-comparative-study-of-decomposition-strategies-and-analysis-metrics-meryam-chaieb-et-al-2024>(7/9 | 186/247) Migration to Microservices: A Comparative Study of Decomposition Strategies and Analysis Metrics (Meryam chaieb et al., 2024)</a></li><li><a href=#89--187247-insights-towards-better-case-study-reporting-in-software-engineering-sergio-rico-2024>(8/9 | 187/247) Insights Towards Better Case Study Reporting in Software Engineering (Sergio Rico, 2024)</a></li><li><a href=#99--188247-what-the-fix-a-study-of-asats-rule-documentation-corentin-latappy-et-al-2024>(9/9 | 188/247) What the Fix? A Study of ASATs Rule Documentation (Corentin Latappy et al., 2024)</a></li></ul></li><li><a href=#cscy-2>cs.CY (2)</a><ul><li><a href=#12--189247-mapping-the-ethics-of-generative-ai-a-comprehensive-scoping-review-thilo-hagendorff-2024>(1/2 | 189/247) Mapping the Ethics of Generative AI: A Comprehensive Scoping Review (Thilo Hagendorff, 2024)</a></li><li><a href=#22--190247-unveiling-hidden-energy-anomalies-harnessing-deep-learning-to-optimize-energy-management-in-sports-facilities-fodil-fadli-et-al-2024>(2/2 | 190/247) Unveiling Hidden Energy Anomalies: Harnessing Deep Learning to Optimize Energy Management in Sports Facilities (Fodil Fadli et al., 2024)</a></li></ul></li><li><a href=#eessiv-5>eess.IV (5)</a><ul><li><a href=#15--191247-poisson-flow-consistency-models-for-low-dose-ct-image-denoising-dennis-hein-et-al-2024>(1/5 | 191/247) Poisson flow consistency models for low-dose CT image denoising (Dennis Hein et al., 2024)</a></li><li><a href=#25--192247-convolutional-neural-networks-towards-facial-skin-lesions-detection-reza-sarshar-et-al-2024>(2/5 | 192/247) Convolutional Neural Networks Towards Facial Skin Lesions Detection (Reza Sarshar et al., 2024)</a></li><li><a href=#35--193247-adversarially-robust-feature-learning-for-breast-cancer-diagnosis-degan-hao-et-al-2024>(3/5 | 193/247) Adversarially Robust Feature Learning for Breast Cancer Diagnosis (Degan Hao et al., 2024)</a></li><li><a href=#45--194247-rethinking-u-net-skip-connections-for-biomedical-image-segmentation-frauke-wilm-et-al-2024>(4/5 | 194/247) Rethinking U-net Skip Connections for Biomedical Image Segmentation (Frauke Wilm et al., 2024)</a></li><li><a href=#55--195247-deep-and-shallow-data-science-for-multi-scale-optical-neuroscience-gal-mishne-et-al-2024>(5/5 | 195/247) Deep and shallow data science for multi-scale optical neuroscience (Gal Mishne et al., 2024)</a></li></ul></li><li><a href=#quant-ph-4>quant-ph (4)</a><ul><li><a href=#14--196247-trained-quantum-neural-networks-are-gaussian-processes-filippo-girardi-et-al-2024>(1/4 | 196/247) Trained quantum neural networks are Gaussian processes (Filippo Girardi et al., 2024)</a></li><li><a href=#24--197247-edge-coloring-lattice-graphs-joris-kattemölle-2024>(2/4 | 197/247) Edge coloring lattice graphs (Joris Kattemölle, 2024)</a></li><li><a href=#34--198247-arbitrary-polynomial-separations-in-trainable-quantum-machine-learning-eric-r-anschuetz-et-al-2024>(3/4 | 198/247) Arbitrary Polynomial Separations in Trainable Quantum Machine Learning (Eric R. Anschuetz et al., 2024)</a></li><li><a href=#44--199247-on-black-box-separations-of-quantum-digital-signatures-from-pseudorandom-states-andrea-coladangelo-et-al-2024>(4/4 | 199/247) On black-box separations of quantum digital signatures from pseudorandom states (Andrea Coladangelo et al., 2024)</a></li></ul></li><li><a href=#eessas-3>eess.AS (3)</a><ul><li><a href=#13--200247-leveraging-cough-sounds-to-optimize-chest-x-ray-usage-in-low-resource-settings-alexander-philip-et-al-2024>(1/3 | 200/247) Leveraging cough sounds to optimize chest x-ray usage in low-resource settings (Alexander Philip et al., 2024)</a></li><li><a href=#23--201247-channel-combination-algorithms-for-robust-distant-voice-activity-and-overlapped-speech-detection-théo-mariotte-et-al-2024>(2/3 | 201/247) Channel-Combination Algorithms for Robust Distant Voice Activity and Overlapped Speech Detection (Théo Mariotte et al., 2024)</a></li><li><a href=#33--202247-unrestricted-global-phase-bias-aware-single-channel-speech-enhancement-with-conformer-based-metric-gan-shiqi-zhang-et-al-2024>(3/3 | 202/247) Unrestricted Global Phase Bias-Aware Single-channel Speech Enhancement with Conformer-based Metric GAN (Shiqi Zhang et al., 2024)</a></li></ul></li><li><a href=#q-biogn-1>q-bio.GN (1)</a><ul><li><a href=#11--203247-dnabert-s-learning-species-aware-dna-embedding-with-genome-foundation-models-zhihan-zhou-et-al-2024>(1/1 | 203/247) DNABERT-S: Learning Species-Aware DNA Embedding with Genome Foundation Models (Zhihan Zhou et al., 2024)</a></li></ul></li><li><a href=#statml-6>stat.ML (6)</a><ul><li><a href=#16--204247-space-time-bridge-diffusion-hamidreza-behjoo-et-al-2024>(1/6 | 204/247) Space-Time Bridge-Diffusion (Hamidreza Behjoo et al., 2024)</a></li><li><a href=#26--205247-on-limitations-of-the-transformer-architecture-binghui-peng-et-al-2024>(2/6 | 205/247) On Limitations of the Transformer Architecture (Binghui Peng et al., 2024)</a></li><li><a href=#36--206247-implicit-bias-in-noisy-sgd-with-applications-to-differentially-private-training-tom-sander-et-al-2024>(3/6 | 206/247) Implicit Bias in Noisy-SGD: With Applications to Differentially Private Training (Tom Sander et al., 2024)</a></li><li><a href=#46--207247-off-policy-evaluation-in-markov-decision-processes-under-weak-distributional-overlap-mohammad-mehrabi-et-al-2024>(4/6 | 207/247) Off-Policy Evaluation in Markov Decision Processes under Weak Distributional Overlap (Mohammad Mehrabi et al., 2024)</a></li><li><a href=#56--208247-corridor-geometry-in-gradient-based-optimization-benoit-dherin-et-al-2024>(5/6 | 208/247) Corridor Geometry in Gradient-Based Optimization (Benoit Dherin et al., 2024)</a></li><li><a href=#66--209247-adjustment-identification-distance-a-gadjid-for-causal-structure-learning-leonard-henckel-et-al-2024>(6/6 | 209/247) Adjustment Identification Distance: A gadjid for Causal Structure Learning (Leonard Henckel et al., 2024)</a></li></ul></li><li><a href=#csce-1>cs.CE (1)</a><ul><li><a href=#11--210247-a-projection-based-time-segmented-reduced-order-model-for-fluid-structure-interactions-qijia-zhai-et-al-2024>(1/1 | 210/247) A Projection-Based Time-Segmented Reduced Order Model for Fluid-Structure Interactions (Qijia Zhai et al., 2024)</a></li></ul></li><li><a href=#csgt-5>cs.GT (5)</a><ul><li><a href=#15--211247-equilibria-of-data-marketplaces-with-privacy-aware-sellers-under-endogenous-privacy-costs-diptangshu-sen-et-al-2024>(1/5 | 211/247) Equilibria of Data Marketplaces with Privacy-Aware Sellers under Endogenous Privacy Costs (Diptangshu Sen et al., 2024)</a></li><li><a href=#25--212247-nfgtransformer-equivariant-representation-learning-for-normal-form-games-siqi-liu-et-al-2024>(2/5 | 212/247) NfgTransformer: Equivariant Representation Learning for Normal-form Games (Siqi Liu et al., 2024)</a></li><li><a href=#35--213247-grace-period-is-all-you-need-individual-fairness-without-revenue-loss-in-revenue-management-patrick-jaillet-et-al-2024>(3/5 | 213/247) Grace Period is All You Need: Individual Fairness without Revenue Loss in Revenue Management (Patrick Jaillet et al., 2024)</a></li><li><a href=#45--214247-willy-wonka-mechanisms-thomas-archbold-et-al-2024>(4/5 | 214/247) Willy Wonka Mechanisms (Thomas Archbold et al., 2024)</a></li><li><a href=#55--215247-strategizing-against-no-regret-learners-in-first-price-auctions-aviad-rubinstein-et-al-2024>(5/5 | 215/247) Strategizing against No-Regret Learners in First-Price Auctions (Aviad Rubinstein et al., 2024)</a></li></ul></li><li><a href=#eesssy-3>eess.SY (3)</a><ul><li><a href=#13--216247-infinite-horizon-optimal-scheduling-for-feedback-control-siyi-wang-et-al-2024>(1/3 | 216/247) Infinite-horizon optimal scheduling for feedback control (Siyi Wang et al., 2024)</a></li><li><a href=#23--217247-ant-colony-optimization-for-cooperative-inspection-path-planning-using-multiple-unmanned-aerial-vehicles-duy-nam-bui-et-al-2024>(2/3 | 217/247) Ant Colony Optimization for Cooperative Inspection Path Planning Using Multiple Unmanned Aerial Vehicles (Duy Nam Bui et al., 2024)</a></li><li><a href=#33--218247-an-adaptive-system-architecture-for-multimodal-intelligent-transportation-systems-muhammad-farooq-et-al-2024>(3/3 | 218/247) An Adaptive System Architecture for Multimodal Intelligent Transportation Systems (Muhammad Farooq et al., 2024)</a></li></ul></li><li><a href=#mathoc-1>math.OC (1)</a><ul><li><a href=#11--219247-model-approximation-in-mdps-with-unbounded-per-step-cost-berk-bozkurt-et-al-2024>(1/1 | 219/247) Model approximation in MDPs with unbounded per-step cost (Berk Bozkurt et al., 2024)</a></li></ul></li><li><a href=#csit-7>cs.IT (7)</a><ul><li><a href=#17--220247-introducing-rsess-an-open-source-enumerative-sphere-shaping-implementation-coded-in-rust-frederik-ritter-et-al-2024>(1/7 | 220/247) Introducing RSESS: An Open Source Enumerative Sphere Shaping Implementation Coded in Rust (Frederik Ritter et al., 2024)</a></li><li><a href=#27--221247-a-precise-bare-simulation-approach-to-the-minimization-of-some-distances-ii-further-foundations-michel-broniatowski-et-al-2024>(2/7 | 221/247) A precise bare simulation approach to the minimization of some distances. II. Further Foundations (Michel Broniatowski et al., 2024)</a></li><li><a href=#37--222247-opportunistic-scheduling-using-statistical-information-of-wireless-channels-zhouyou-gu-et-al-2024>(3/7 | 222/247) Opportunistic Scheduling Using Statistical Information of Wireless Channels (Zhouyou Gu et al., 2024)</a></li><li><a href=#47--223247-asynchronous-distributed-coordinated-hybrid-precoding-in-multi-cell-mmwave-wireless-networks-meesam-jafri-et-al-2024>(4/7 | 223/247) Asynchronous Distributed Coordinated Hybrid Precoding in Multi-cell mmWave Wireless Networks (Meesam Jafri et al., 2024)</a></li><li><a href=#57--224247-two-dimensional-direction-of-arrival-estimation-using-stacked-intelligent-metasurfaces-jiancheng-an-et-al-2024>(5/7 | 224/247) Two-Dimensional Direction-of-Arrival Estimation Using Stacked Intelligent Metasurfaces (Jiancheng An et al., 2024)</a></li><li><a href=#67--225247-an-information-theoretic-lower-bound-in-time-uniform-estimation-john-c-duchi-et-al-2024>(6/7 | 225/247) An information-theoretic lower bound in time-uniform estimation (John C. Duchi et al., 2024)</a></li><li><a href=#77--226247-a-scalable-synergy-first-backbone-decomposition-of-higher-order-structures-in-complex-systems-thomas-f-varley-2024>(7/7 | 226/247) A scalable, synergy-first backbone decomposition of higher-order structures in complex systems (Thomas F. Varley, 2024)</a></li></ul></li><li><a href=#csni-1>cs.NI (1)</a><ul><li><a href=#11--227247-energy-aware-dynamic-resource-allocation-in-virtual-sensor-networks-carmen-delgado-et-al-2024>(1/1 | 227/247) Energy-aware Dynamic Resource Allocation in Virtual Sensor Networks (Carmen Delgado et al., 2024)</a></li></ul></li><li><a href=#econgn-1>econ.GN (1)</a><ul><li><a href=#11--228247-artificial-intelligence-and-the-transformation-of-higher-education-institutions-evangelos-katsamakas-et-al-2024>(1/1 | 228/247) Artificial intelligence and the transformation of higher education institutions (Evangelos Katsamakas et al., 2024)</a></li></ul></li><li><a href=#q-biobm-1>q-bio.BM (1)</a><ul><li><a href=#11--229247-a-survey-of-generative-ai-for-de-novo-drug-design-new-frontiers-in-molecule-and-protein-generation-xiangru-tang-et-al-2024>(1/1 | 229/247) A Survey of Generative AI for De Novo Drug Design: New Frontiers in Molecule and Protein Generation (Xiangru Tang et al., 2024)</a></li></ul></li><li><a href=#csdb-2>cs.DB (2)</a><ul><li><a href=#12--230247-evaluating-the-data-model-robustness-of-text-to-sql-systems-based-on-real-user-queries-jonathan-fürst-et-al-2024>(1/2 | 230/247) Evaluating the Data Model Robustness of Text-to-SQL Systems Based on Real User Queries (Jonathan Fürst et al., 2024)</a></li><li><a href=#22--231247-from-shapes-to-shapes-inferring-shacl-shapes-for-results-of-sparql-construct-queries-extended-version-philipp-seifer-et-al-2024>(2/2 | 231/247) From Shapes to Shapes: Inferring SHACL Shapes for Results of SPARQL CONSTRUCT Queries (Extended Version) (Philipp Seifer et al., 2024)</a></li></ul></li><li><a href=#csdl-2>cs.DL (2)</a><ul><li><a href=#12--232247-interleaved-snowballing-reducing-the-workload-of-literature-curators-ralf-stephan-2024>(1/2 | 232/247) Interleaved snowballing: Reducing the workload of literature curators (Ralf Stephan, 2024)</a></li><li><a href=#22--233247-forecasting-high-impact-research-topics-via-machine-learning-on-evolving-knowledge-graphs-xuemei-gu-et-al-2024>(2/2 | 233/247) Forecasting high-impact research topics via machine learning on evolving knowledge graphs (Xuemei Gu et al., 2024)</a></li></ul></li><li><a href=#physicschem-ph-1>physics.chem-ph (1)</a><ul><li><a href=#11--234247-zero-shot-molecular-generation-via-similarity-kernels-rokas-elijošius-et-al-2024>(1/1 | 234/247) Zero Shot Molecular Generation via Similarity Kernels (Rokas Elijošius et al., 2024)</a></li></ul></li><li><a href=#csma-1>cs.MA (1)</a><ul><li><a href=#11--235247-logic-of-awareness-for-nested-knowledge-yudai-kubono-2024>(1/1 | 235/247) Logic of Awareness for Nested Knowledge (Yudai Kubono, 2024)</a></li></ul></li><li><a href=#q-fintr-1>q-fin.TR (1)</a><ul><li><a href=#11--236247-end-to-end-policy-learning-of-a-statistical-arbitrage-autoencoder-architecture-fabian-krause-et-al-2024>(1/1 | 236/247) End-to-End Policy Learning of a Statistical Arbitrage Autoencoder Architecture (Fabian Krause et al., 2024)</a></li></ul></li><li><a href=#statme-1>stat.ME (1)</a><ul><li><a href=#11--237247-gradient-flow-adaptive-importance-sampling-for-bayesian-leave-one-out-cross-validation-for-sigmoidal-classification-models-joshua-c-chang-et-al-2024>(1/1 | 237/247) Gradient-flow adaptive importance sampling for Bayesian leave one out cross-validation for sigmoidal classification models (Joshua C Chang et al., 2024)</a></li></ul></li><li><a href=#eesssp-1>eess.SP (1)</a><ul><li><a href=#11--238247-benchmarking-multi-component-signal-processing-methods-in-the-time-frequency-plane-juan-m-miramont-et-al-2024>(1/1 | 238/247) Benchmarking multi-component signal processing methods in the time-frequency plane (Juan M. Miramont et al., 2024)</a></li></ul></li><li><a href=#csds-5>cs.DS (5)</a><ul><li><a href=#15--239247-sequence-graphs-realizations-and-ambiguity-in-language-models-sammy-khalife-et-al-2024>(1/5 | 239/247) Sequence graphs realizations and ambiguity in language models (Sammy Khalife et al., 2024)</a></li><li><a href=#25--240247-parameterized-dynamic-data-structure-for-split-completion-konrad-majewski-et-al-2024>(2/5 | 240/247) Parameterized dynamic data structure for Split Completion (Konrad Majewski et al., 2024)</a></li><li><a href=#35--241247-tight-double-exponential-bounds-for-identification-problems-locating-dominating-set-and-test-cover-dipayan-chakraborty-et-al-2024>(3/5 | 241/247) Tight (Double) Exponential Bounds for Identification Problems: Locating-Dominating Set and Test Cover (Dipayan Chakraborty et al., 2024)</a></li><li><a href=#45--242247-integrating-high-dimensional-functions-deterministically-david-gamarnik-et-al-2024>(4/5 | 242/247) Integrating High-Dimensional Functions Deterministically (David Gamarnik et al., 2024)</a></li><li><a href=#55--243247-an-improved-approximation-algorithm-for-metric-triangle-packing-jingyang-zhao-et-al-2024>(5/5 | 243/247) An Improved Approximation Algorithm for Metric Triangle Packing (Jingyang Zhao et al., 2024)</a></li></ul></li><li><a href=#csdc-1>cs.DC (1)</a><ul><li><a href=#11--244247-byzantine-fault-tolerant-distributed-set-intersection-with-redundancy-shuo-liu-et-al-2024>(1/1 | 244/247) Byzantine fault-tolerant distributed set intersection with redundancy (Shuo Liu et al., 2024)</a></li></ul></li><li><a href=#mathco-1>math.CO (1)</a><ul><li><a href=#11--245247-detecting-k_23-as-an-induced-minor-clément-dallard-et-al-2024>(1/1 | 245/247) Detecting $K_{2,3}$ as an induced minor (Clément Dallard et al., 2024)</a></li></ul></li><li><a href=#cscg-1>cs.CG (1)</a><ul><li><a href=#11--246247-nearly-orthogonal-sets-over-finite-fields-dror-chawin-et-al-2024>(1/1 | 246/247) Nearly Orthogonal Sets over Finite Fields (Dror Chawin et al., 2024)</a></li></ul></li><li><a href=#csdm-1>cs.DM (1)</a><ul><li><a href=#11--247247-iiro-honkalas-contributions-to-identifying-codes-olivier-hudry-et-al-2024>(1/1 | 247/247) Iiro Honkala&rsquo;s contributions to identifying codes (Olivier Hudry et al., 2024)</a></li></ul></li></ul></nav></div></div></section></div><footer id=footer class="container-fluid text-center align-content-center footer pb-2"><div class="container pt-5"><div class="row text-left"><div class="col-md-4 col-sm-12"><h5>Navigation</h5><ul><li class=nav-item><a class=smooth-scroll href=https://akitenkrad.github.io/akitenkrad-blog/#about>About</a></li><li class=nav-item><a class=smooth-scroll href=https://akitenkrad.github.io/akitenkrad-blog/#recent-posts>Recent Posts</a></li></ul></div><div class="col-md-4 col-sm-12"><h5>Contact me:</h5><ul><li><a href=mailto:contact.to.akitenkrad@gmail.com target=_blank rel=noopener><span><i class="fas fa-envelope"></i></span> <span>contact.to.akitenkrad@gmail.com</span></a></li></ul></div></div></div><hr><div class=container><div class="row text-left"><div class=col-md-4><a id=theme href=https://github.com/hossainemruz/toha target=_blank rel=noopener><img src=/akitenkrad-blog/images/theme-logo_hu8376fd15465fef26ffe66b6bcf0ca686_13669_32x0_resize_box_3.png alt="Toha Theme Logo">
Toha</a></div><div class="col-md-4 text-center">© 2020 Akitenkrad.</div><div class="col-md-4 text-right"><a id=hugo href=https://gohugo.io/ target=_blank rel=noopener>Powered by
<img src=/akitenkrad-blog/images/hugo-logo.svg alt="Hugo Logo" height=18></a></div></div></div></footer><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/jquery-3.4.1.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/popper.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/bootstrap.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/navbar.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/plyr.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/main.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js></script><script src=https://akitenkrad.github.io/akitenkrad-blog/js/single.js></script><script>hljs.initHighlightingOnLoad()</script><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/katex/katex.min.css><script type=text/javascript defer src=https://akitenkrad.github.io/akitenkrad-blog/katex/katex.min.js></script><script type=text/javascript defer src=https://akitenkrad.github.io/akitenkrad-blog/katex/auto-render.min.js></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\[",right:"\\]",display:!0},{left:"\\(",right:"\\)",display:!1}],throwOnError:!0})})</script></body></html>