<!doctype html><html><head><title>arXiv @ 2024.02.25</title>
<meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/bootstrap.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/layouts/main.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/navigators/navbar.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/plyr.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/flag-icon.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/custom/style.css><link href=https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css rel=stylesheet integrity=sha384-9ndCyUaIbzAi2FUVXJi0CjmCapSmO7SnpJef0486qhLnuZ2cdeRhO02iuK6FUUVM crossorigin=anonymous><script async src=https://cdn.jsdelivr.net/npm/es-module-shims@1/dist/es-module-shims.min.js crossorigin=anonymous></script><script type=importmap>
{
  "imports": {
    "@popperjs/core": "https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.8/dist/esm/popper.min.js",
    "bootstrap": "https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.esm.min.js"
  }
}
</script><script src=https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js integrity=sha384-geWF76RCwLtnZ8qwWowPQNguL3RmwHVBC9FhGdlKrxdiJJigb/j/68SIy3Te4Bkz crossorigin=anonymous></script><script type=module src=https://akitenkrad.github.io/akitenkrad-blog/js/custom/custom.js></script><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Muli:wght@300;400;500;600"><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/fontawesome/css/all.min.css><link rel=icon type=image/png href=https://akitenkrad.github.io/akitenkrad-blog/images/favicons/favicon-96x96_huf1ee13f0caf27d1547f91fb46207d708_13005_42x0_resize_box_3.png><meta property="og:title" content="arXiv @ 2024.02.25"><meta property="og:description" content="Primary Categories astro-ph.IM (1) cs.AI (2) cs.AR (1) cs.CE (1) cs.CL (36) cs.CR (7) cs.CV (25) cs.DC (2) cs.DM (1) cs.DS (5) cs.GT (3) cs.HC (8) cs.IR (5) cs.IT (2) cs.LG (56) cs.LO (1) cs.MA (2) cs.RO (9) cs.SD (1) cs.SE (3) cs.SI (1) eess.AS (2) eess.IV (2) eess.SY (1) math.CO (1) math.NA (2) math.OC (1) math.ST (2) q-bio.QM (1) quant-ph (1) stat.ML (5) Keywords keyword cs.CL cs.CV cs.LG Active Learning 1 Adversarial Attack 1 Adversarial Learning 1 Anomaly Detection 1 Aspect-based Sentiment Analysis 1 Autoencoder 1 1 1 Automatic Evaluation 1 Automatic Speech Recognition 1 BART 1 BERT 1 BLEU 1 Bandit Algorithm 4 Benchmarking 13 8 6 Black Box 2 Clustering 1 Continual Learning 1 Continuous Time 2 Contrastive Learning 2 2 1 Convolution 5 3 Convolutional Neural Network 8 3 Counter-factual 1 1 Data Augmentation 1 Dense Retrieval 1 Diffusion Model 1 3 1 Domain Adaptation 1 2 Federated Learning 2 Few-shot 3 1 Fine-tuning 21 4 9 Foundation Model 1 GPT 7 1 GPT-2 1 GPT-3 1 GPT-3."><meta property="og:type" content="article"><meta property="og:url" content="https://akitenkrad.github.io/akitenkrad-blog/posts/arxiv/202402/20240225000000/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-02-25T00:00:00+00:00"><meta property="article:modified_time" content="2024-02-25T00:00:00+00:00"><meta name=description content="arXiv @ 2024.02.25"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/atom-one-dark.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/layouts/single.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/navigators/sidebar.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/style.css><script type=text/javascript src=//ajax.googleapis.com/ajax/libs/jquery/3.1.1/jquery.min.js></script><script language=JavaScript>$(document).ready(function(){$("a[href^='http']:not([href*='"+location.hostname+"'])").attr("target","_blank")})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-1MYYZQG0WE"></script><script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-1MYYZQG0WE",{anonymize_ip:!1})}</script></head><body data-spy=scroll data-target=#TableOfContents data-offset=80><div class="container-fluid bg-dimmed wrapper"><nav class="navbar navbar-expand-xl top-navbar final-navbar shadow" style=position:fixed><div class=container><button class="navbar-toggler navbar-light navbar-for-sidebar" id=sidebar-toggler type=button onclick=toggleSidebar()>
<span class=navbar-toggler-icon></span>
</button>
<button class="navbar-toggler navbar-light" id=toc-toggler type=button onclick=toggleTOC()>
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse lang-selector" id=top-nav-items><ul class="navbar-nav mr-auto"><li class=nav-item><a class=navbar-brand href=/akitenkrad-blog><img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png alt=Logo>
Akitenkrad's Blog</a></li></ul><ul class="navbar-nav ml-auto"><li class=nav-item><a class=nav-link href=/akitenkrad-blog#home>Home</a></li><li class=nav-item><a class=nav-link href=/akitenkrad-blog#about>About</a></li><li class=nav-item><a class=nav-link href=/akitenkrad-blog#recent-posts>Recent Posts</a></li><li class=nav-item><a class=nav-link id=blog-link href=https://akitenkrad.github.io/akitenkrad-blog/posts>Posts</a></li><li class=nav-item><a class=nav-link id=tag-link href=https://akitenkrad.github.io/akitenkrad-blog/tags>Tags</a></li></ul></div></div><img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png class=d-none id=main-logo alt=Logo>
<img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png class=d-none id=inverted-logo alt="Inverted Logo"></nav><section class=sidebar-section id=sidebar-section><div class=sidebar-holder><div class=sidebar id=sidebar><form class=mx-auto method=get action=/akitenkrad-blog/search><input type=text name=keyword placeholder=Search data-search id=search-box></form><div class=sidebar-tree><ul class=tree id=tree><li id=list-heading><a href=/posts data-filter=all>Posts</a></li><div class=subtree><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/>Papers</a><ul><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202205/>2022.05</a><ul><li><a href=/akitenkrad-blog/posts/papers/202205/20220518224923/ title="A Context-Aware Citation Recommendation Model with BERT and Graph Convolutional Networks">A Context-Aware Citation Recommendation Model with BERT and Graph Convolutional Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220508162318/ title="A Deep Cascade Model for Multi-Document Reading Comprehension">A Deep Cascade Model for Multi-Document Reading Comprehension</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220506021208/ title="A Primer in BERTology: What We Know About How BERT Works">A Primer in BERTology: What We Know About How BERT Works</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220529131339/ title="Attention Is All You Need">Attention Is All You Need</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220503010000/ title="DeBERTa: Decoding-Enhanced BERT with Disentangled Attention">DeBERTa: Decoding-Enhanced BERT with Disentangled Attention</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220505222900/ title="Dense Passage Retrieval for Open-Domain Question Answering">Dense Passage Retrieval for Open-Domain Question Answering</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220511010217/ title="Multi-Style Generative Reading Comprehension">Multi-Style Generative Reading Comprehension</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220530102936/ title="Neural Machine Translation of Rare Words with Subword Units">Neural Machine Translation of Rare Words with Subword Units</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220523223206/ title="RoBERTa: A Robustly Optimized BERT Pretraining Approach">RoBERTa: A Robustly Optimized BERT Pretraining Approach</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220520124748/ title="Semi-Supervised Classification with Graph Convolutional Networks">Semi-Supervised Classification with Graph Convolutional Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220509110738/ title="Survey on graph embeddings and their applications to machine learning problems on graphs">Survey on graph embeddings and their applications to machine learning problems on graphs</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220514151839/ title="UnitedQA: A Hybrid Approach for Open Domain Question Answering">UnitedQA: A Hybrid Approach for Open Domain Question Answering</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202206/>2022.06</a><ul><li><a href=/akitenkrad-blog/posts/papers/202206/20220612105422/ title="Attributed Network Embedding for Learning in a Dynamic Environment">Attributed Network Embedding for Learning in a Dynamic Environment</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220608085622/ title="CodeBERT: A Pre-Trained Model for Programming and Natural Languages">CodeBERT: A Pre-Trained Model for Programming and Natural Languages</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220618223844/ title="High-order Proximity Preserved Embedding for Dynamic Networks">High-order Proximity Preserved Embedding for Dynamic Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220602171700/ title="S-Net: From Answer Extraction to Answer Generation for Machine Reading Comprehension">S-Net: From Answer Extraction to Answer Generation for Machine Reading Comprehension</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202207/>2022.07</a><ul><li><a href=/akitenkrad-blog/posts/papers/202207/20220727145036/ title="Dynamic Heterogeneous Graph Embedding Using Hierarchical Attentions">Dynamic Heterogeneous Graph Embedding Using Hierarchical Attentions</a></li><li><a href=/akitenkrad-blog/posts/papers/202207/20220726163444/ title="Dynamic Network Embedding Survey">Dynamic Network Embedding Survey</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202208/>2022.08</a><ul><li><a href=/akitenkrad-blog/posts/papers/202208/20220802103319/ title="Modeling Dynamic Heterogeneous Network for Link Prediction using Hierarchical Attention with Temporal RNN">Modeling Dynamic Heterogeneous Network for Link Prediction using Hierarchical Attention with Temporal RNN</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202209/>2022.09</a><ul><li><a href=/akitenkrad-blog/posts/papers/202209/20220909180827/ title="Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting">Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202305/>2023.05</a><ul><li><a href=/akitenkrad-blog/posts/papers/202305/20230513094804/ title="Improving Language Understanding by Generative Pre-Training">Improving Language Understanding by Generative Pre-Training</a></li><li><a href=/akitenkrad-blog/posts/papers/202305/20230514200241/ title="Language Models are Unsupervised Multitask Learners">Language Models are Unsupervised Multitask Learners</a></li></ul></li><li><a href=/akitenkrad-blog/posts/papers/202306/ title=2023.06>2023.06</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/akitenkrad-blog/posts/arxiv/>arXiv</a><ul class=active><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202307/>2023.07</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230701000000/ title="arXiv @ 2023.07.01">arXiv @ 2023.07.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230702000000/ title="arXiv @ 2023.07.02">arXiv @ 2023.07.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230703000000/ title="arXiv @ 2023.07.03">arXiv @ 2023.07.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230704000000/ title="arXiv @ 2023.07.04">arXiv @ 2023.07.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230705000000/ title="arXiv @ 2023.07.05">arXiv @ 2023.07.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230706000000/ title="arXiv @ 2023.07.06">arXiv @ 2023.07.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230707000000/ title="arXiv @ 2023.07.07">arXiv @ 2023.07.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230708000000/ title="arXiv @ 2023.07.08">arXiv @ 2023.07.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230709000000/ title="arXiv @ 2023.07.09">arXiv @ 2023.07.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230710000000/ title="arXiv @ 2023.07.10">arXiv @ 2023.07.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230711000000/ title="arXiv @ 2023.07.11">arXiv @ 2023.07.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230712000000/ title="arXiv @ 2023.07.12">arXiv @ 2023.07.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230713000000/ title="arXiv @ 2023.07.13">arXiv @ 2023.07.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230714000000/ title="arXiv @ 2023.07.14">arXiv @ 2023.07.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230715000000/ title="arXiv @ 2023.07.15">arXiv @ 2023.07.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230716000000/ title="arXiv @ 2023.07.16">arXiv @ 2023.07.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230717000000/ title="arXiv @ 2023.07.17">arXiv @ 2023.07.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230718000000/ title="arXiv @ 2023.07.18">arXiv @ 2023.07.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230719000000/ title="arXiv @ 2023.07.19">arXiv @ 2023.07.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230720000000/ title="arXiv @ 2023.07.20">arXiv @ 2023.07.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230721000000/ title="arXiv @ 2023.07.21">arXiv @ 2023.07.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230722000000/ title="arXiv @ 2023.07.22">arXiv @ 2023.07.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230723000000/ title="arXiv @ 2023.07.23">arXiv @ 2023.07.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230724000000/ title="arXiv @ 2023.07.24">arXiv @ 2023.07.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230725000000/ title="arXiv @ 2023.07.25">arXiv @ 2023.07.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230726000000/ title="arXiv @ 2023.07.26">arXiv @ 2023.07.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230727000000/ title="arXiv @ 2023.07.27">arXiv @ 2023.07.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230728000000/ title="arXiv @ 2023.07.28">arXiv @ 2023.07.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230729000000/ title="arXiv @ 2023.07.29">arXiv @ 2023.07.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230730000000/ title="arXiv @ 2023.07.30">arXiv @ 2023.07.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230731000000/ title="arXiv @ 2023.07.31">arXiv @ 2023.07.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202308/>2023.08</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230801000000/ title="arXiv @ 2023.08.01">arXiv @ 2023.08.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230802000000/ title="arXiv @ 2023.08.02">arXiv @ 2023.08.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230803000000/ title="arXiv @ 2023.08.03">arXiv @ 2023.08.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230804000000/ title="arXiv @ 2023.08.04">arXiv @ 2023.08.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230805000000/ title="arXiv @ 2023.08.05">arXiv @ 2023.08.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230806000000/ title="arXiv @ 2023.08.06">arXiv @ 2023.08.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230807000000/ title="arXiv @ 2023.08.07">arXiv @ 2023.08.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230808000000/ title="arXiv @ 2023.08.08">arXiv @ 2023.08.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230809000000/ title="arXiv @ 2023.08.09">arXiv @ 2023.08.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230810000000/ title="arXiv @ 2023.08.10">arXiv @ 2023.08.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230811000000/ title="arXiv @ 2023.08.11">arXiv @ 2023.08.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230812000000/ title="arXiv @ 2023.08.12">arXiv @ 2023.08.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230813000000/ title="arXiv @ 2023.08.13">arXiv @ 2023.08.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230814000000/ title="arXiv @ 2023.08.14">arXiv @ 2023.08.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230815000000/ title="arXiv @ 2023.08.15">arXiv @ 2023.08.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230816000000/ title="arXiv @ 2023.08.16">arXiv @ 2023.08.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230817000000/ title="arXiv @ 2023.08.17">arXiv @ 2023.08.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230818000000/ title="arXiv @ 2023.08.18">arXiv @ 2023.08.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230819000000/ title="arXiv @ 2023.08.19">arXiv @ 2023.08.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230820000000/ title="arXiv @ 2023.08.20">arXiv @ 2023.08.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230821000000/ title="arXiv @ 2023.08.21">arXiv @ 2023.08.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230822000000/ title="arXiv @ 2023.08.22">arXiv @ 2023.08.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230823000000/ title="arXiv @ 2023.08.23">arXiv @ 2023.08.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230824000000/ title="arXiv @ 2023.08.24">arXiv @ 2023.08.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230825000000/ title="arXiv @ 2023.08.25">arXiv @ 2023.08.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230826000000/ title="arXiv @ 2023.08.26">arXiv @ 2023.08.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230827000000/ title="arXiv @ 2023.08.27">arXiv @ 2023.08.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230828000000/ title="arXiv @ 2023.08.28">arXiv @ 2023.08.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230829000000/ title="arXiv @ 2023.08.29">arXiv @ 2023.08.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230830000000/ title="arXiv @ 2023.08.30">arXiv @ 2023.08.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230831000000/ title="arXiv @ 2023.08.31">arXiv @ 2023.08.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202309/>2023.09</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230901000000/ title="arXiv @ 2023.09.01">arXiv @ 2023.09.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230902000000/ title="arXiv @ 2023.09.02">arXiv @ 2023.09.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230903000000/ title="arXiv @ 2023.09.03">arXiv @ 2023.09.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230904000000/ title="arXiv @ 2023.09.04">arXiv @ 2023.09.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230905000000/ title="arXiv @ 2023.09.05">arXiv @ 2023.09.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230906000000/ title="arXiv @ 2023.09.06">arXiv @ 2023.09.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230907000000/ title="arXiv @ 2023.09.07">arXiv @ 2023.09.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230908000000/ title="arXiv @ 2023.09.08">arXiv @ 2023.09.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230909000000/ title="arXiv @ 2023.09.09">arXiv @ 2023.09.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230910000000/ title="arXiv @ 2023.09.10">arXiv @ 2023.09.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230911000000/ title="arXiv @ 2023.09.11">arXiv @ 2023.09.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230912000000/ title="arXiv @ 2023.09.12">arXiv @ 2023.09.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230913000000/ title="arXiv @ 2023.09.13">arXiv @ 2023.09.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230914000000/ title="arXiv @ 2023.09.14">arXiv @ 2023.09.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230915000000/ title="arXiv @ 2023.09.15">arXiv @ 2023.09.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230916000000/ title="arXiv @ 2023.09.16">arXiv @ 2023.09.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230917000000/ title="arXiv @ 2023.09.17">arXiv @ 2023.09.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230918000000/ title="arXiv @ 2023.09.18">arXiv @ 2023.09.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230919000000/ title="arXiv @ 2023.09.19">arXiv @ 2023.09.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230920000000/ title="arXiv @ 2023.09.20">arXiv @ 2023.09.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230921000000/ title="arXiv @ 2023.09.21">arXiv @ 2023.09.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230922000000/ title="arXiv @ 2023.09.22">arXiv @ 2023.09.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230923000000/ title="arXiv @ 2023.09.23">arXiv @ 2023.09.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230924000000/ title="arXiv @ 2023.09.24">arXiv @ 2023.09.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230925000000/ title="arXiv @ 2023.09.25">arXiv @ 2023.09.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230926000000/ title="arXiv @ 2023.09.26">arXiv @ 2023.09.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230927000000/ title="arXiv @ 2023.09.27">arXiv @ 2023.09.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230928000000/ title="arXiv @ 2023.09.28">arXiv @ 2023.09.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230929000000/ title="arXiv @ 2023.09.29">arXiv @ 2023.09.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230930000000/ title="arXiv @ 2023.09.30">arXiv @ 2023.09.30</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202310/>2023.1</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231001000000/ title="arXiv @ 2023.10.01">arXiv @ 2023.10.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231002000000/ title="arXiv @ 2023.10.02">arXiv @ 2023.10.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231003000000/ title="arXiv @ 2023.10.03">arXiv @ 2023.10.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231004000000/ title="arXiv @ 2023.10.04">arXiv @ 2023.10.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231005000000/ title="arXiv @ 2023.10.05">arXiv @ 2023.10.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231006000000/ title="arXiv @ 2023.10.06">arXiv @ 2023.10.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231007000000/ title="arXiv @ 2023.10.07">arXiv @ 2023.10.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231008000000/ title="arXiv @ 2023.10.08">arXiv @ 2023.10.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231009000000/ title="arXiv @ 2023.10.09">arXiv @ 2023.10.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231010000000/ title="arXiv @ 2023.10.10">arXiv @ 2023.10.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231011000000/ title="arXiv @ 2023.10.11">arXiv @ 2023.10.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231012000000/ title="arXiv @ 2023.10.12">arXiv @ 2023.10.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231013000000/ title="arXiv @ 2023.10.13">arXiv @ 2023.10.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231014000000/ title="arXiv @ 2023.10.14">arXiv @ 2023.10.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231015000000/ title="arXiv @ 2023.10.15">arXiv @ 2023.10.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231016000000/ title="arXiv @ 2023.10.16">arXiv @ 2023.10.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231017000000/ title="arXiv @ 2023.10.17">arXiv @ 2023.10.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231018000000/ title="arXiv @ 2023.10.18">arXiv @ 2023.10.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231019000000/ title="arXiv @ 2023.10.19">arXiv @ 2023.10.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231020000000/ title="arXiv @ 2023.10.20">arXiv @ 2023.10.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231021000000/ title="arXiv @ 2023.10.21">arXiv @ 2023.10.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231022000000/ title="arXiv @ 2023.10.22">arXiv @ 2023.10.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231023000000/ title="arXiv @ 2023.10.23">arXiv @ 2023.10.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231024000000/ title="arXiv @ 2023.10.24">arXiv @ 2023.10.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231025000000/ title="arXiv @ 2023.10.25">arXiv @ 2023.10.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231026000000/ title="arXiv @ 2023.10.26">arXiv @ 2023.10.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231027000000/ title="arXiv @ 2023.10.27">arXiv @ 2023.10.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231028000000/ title="arXiv @ 2023.10.28">arXiv @ 2023.10.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231029000000/ title="arXiv @ 2023.10.29">arXiv @ 2023.10.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231030000000/ title="arXiv @ 2023.10.30">arXiv @ 2023.10.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231031000000/ title="arXiv @ 2023.10.31">arXiv @ 2023.10.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202311/>2023.11</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231101000000/ title="arXiv @ 2023.11.01">arXiv @ 2023.11.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231102000000/ title="arXiv @ 2023.11.02">arXiv @ 2023.11.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231103000000/ title="arXiv @ 2023.11.03">arXiv @ 2023.11.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231104000000/ title="arXiv @ 2023.11.04">arXiv @ 2023.11.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231105000000/ title="arXiv @ 2023.11.05">arXiv @ 2023.11.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231106000000/ title="arXiv @ 2023.11.06">arXiv @ 2023.11.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231107000000/ title="arXiv @ 2023.11.07">arXiv @ 2023.11.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231108000000/ title="arXiv @ 2023.11.08">arXiv @ 2023.11.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231109000000/ title="arXiv @ 2023.11.09">arXiv @ 2023.11.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231110000000/ title="arXiv @ 2023.11.10">arXiv @ 2023.11.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231111000000/ title="arXiv @ 2023.11.11">arXiv @ 2023.11.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231112000000/ title="arXiv @ 2023.11.12">arXiv @ 2023.11.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231113000000/ title="arXiv @ 2023.11.13">arXiv @ 2023.11.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231114000000/ title="arXiv @ 2023.11.14">arXiv @ 2023.11.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231115000000/ title="arXiv @ 2023.11.15">arXiv @ 2023.11.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231116000000/ title="arXiv @ 2023.11.16">arXiv @ 2023.11.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231117000000/ title="arXiv @ 2023.11.17">arXiv @ 2023.11.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231118000000/ title="arXiv @ 2023.11.18">arXiv @ 2023.11.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231119000000/ title="arXiv @ 2023.11.19">arXiv @ 2023.11.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231120000000/ title="arXiv @ 2023.11.20">arXiv @ 2023.11.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231121000000/ title="arXiv @ 2023.11.21">arXiv @ 2023.11.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231122000000/ title="arXiv @ 2023.11.22">arXiv @ 2023.11.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231123000000/ title="arXiv @ 2023.11.23">arXiv @ 2023.11.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231124000000/ title="arXiv @ 2023.11.24">arXiv @ 2023.11.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231125000000/ title="arXiv @ 2023.11.25">arXiv @ 2023.11.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231126000000/ title="arXiv @ 2023.11.26">arXiv @ 2023.11.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231127000000/ title="arXiv @ 2023.11.27">arXiv @ 2023.11.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231128000000/ title="arXiv @ 2023.11.28">arXiv @ 2023.11.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231129000000/ title="arXiv @ 2023.11.29">arXiv @ 2023.11.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231130000000/ title="arXiv @ 2023.11.30">arXiv @ 2023.11.30</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202312/>2023.12</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231201000000/ title="arXiv @ 2023.12.01">arXiv @ 2023.12.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231202000000/ title="arXiv @ 2023.12.02">arXiv @ 2023.12.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231203000000/ title="arXiv @ 2023.12.03">arXiv @ 2023.12.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231204000000/ title="arXiv @ 2023.12.04">arXiv @ 2023.12.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231205000000/ title="arXiv @ 2023.12.05">arXiv @ 2023.12.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231206000000/ title="arXiv @ 2023.12.06">arXiv @ 2023.12.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231207000000/ title="arXiv @ 2023.12.07">arXiv @ 2023.12.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231208000000/ title="arXiv @ 2023.12.08">arXiv @ 2023.12.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231209000000/ title="arXiv @ 2023.12.09">arXiv @ 2023.12.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231210000000/ title="arXiv @ 2023.12.10">arXiv @ 2023.12.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231211000000/ title="arXiv @ 2023.12.11">arXiv @ 2023.12.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231212000000/ title="arXiv @ 2023.12.12">arXiv @ 2023.12.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231213000000/ title="arXiv @ 2023.12.13">arXiv @ 2023.12.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231214000000/ title="arXiv @ 2023.12.14">arXiv @ 2023.12.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231215000000/ title="arXiv @ 2023.12.15">arXiv @ 2023.12.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231216000000/ title="arXiv @ 2023.12.16">arXiv @ 2023.12.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231217000000/ title="arXiv @ 2023.12.17">arXiv @ 2023.12.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231218000000/ title="arXiv @ 2023.12.18">arXiv @ 2023.12.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231219000000/ title="arXiv @ 2023.12.19">arXiv @ 2023.12.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231220000000/ title="arXiv @ 2023.12.20">arXiv @ 2023.12.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231221000000/ title="arXiv @ 2023.12.21">arXiv @ 2023.12.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231222000000/ title="arXiv @ 2023.12.22">arXiv @ 2023.12.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231223000000/ title="arXiv @ 2023.12.23">arXiv @ 2023.12.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231224000000/ title="arXiv @ 2023.12.24">arXiv @ 2023.12.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231225000000/ title="arXiv @ 2023.12.25">arXiv @ 2023.12.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231226000000/ title="arXiv @ 2023.12.26">arXiv @ 2023.12.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231227000000/ title="arXiv @ 2023.12.27">arXiv @ 2023.12.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231228000000/ title="arXiv @ 2023.12.28">arXiv @ 2023.12.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231229000000/ title="arXiv @ 2023.12.29">arXiv @ 2023.12.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231230000000/ title="arXiv @ 2023.12.30">arXiv @ 2023.12.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231231000000/ title="arXiv @ 2023.12.31">arXiv @ 2023.12.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202401/>2024.01</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240101000000/ title="arXiv @ 2024.01.01">arXiv @ 2024.01.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240102000000/ title="arXiv @ 2024.01.02">arXiv @ 2024.01.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240103000000/ title="arXiv @ 2024.01.03">arXiv @ 2024.01.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240104000000/ title="arXiv @ 2024.01.04">arXiv @ 2024.01.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240105000000/ title="arXiv @ 2024.01.05">arXiv @ 2024.01.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240106000000/ title="arXiv @ 2024.01.06">arXiv @ 2024.01.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240107000000/ title="arXiv @ 2024.01.07">arXiv @ 2024.01.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240108000000/ title="arXiv @ 2024.01.08">arXiv @ 2024.01.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240109000000/ title="arXiv @ 2024.01.09">arXiv @ 2024.01.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240110000000/ title="arXiv @ 2024.01.10">arXiv @ 2024.01.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240111000000/ title="arXiv @ 2024.01.11">arXiv @ 2024.01.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240112000000/ title="arXiv @ 2024.01.12">arXiv @ 2024.01.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240113000000/ title="arXiv @ 2024.01.13">arXiv @ 2024.01.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240114000000/ title="arXiv @ 2024.01.14">arXiv @ 2024.01.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240115000000/ title="arXiv @ 2024.01.15">arXiv @ 2024.01.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240116000000/ title="arXiv @ 2024.01.16">arXiv @ 2024.01.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240117000000/ title="arXiv @ 2024.01.17">arXiv @ 2024.01.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240118000000/ title="arXiv @ 2024.01.18">arXiv @ 2024.01.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240119000000/ title="arXiv @ 2024.01.19">arXiv @ 2024.01.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240120000000/ title="arXiv @ 2024.01.20">arXiv @ 2024.01.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240121000000/ title="arXiv @ 2024.01.21">arXiv @ 2024.01.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240122000000/ title="arXiv @ 2024.01.22">arXiv @ 2024.01.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240123000000/ title="arXiv @ 2024.01.23">arXiv @ 2024.01.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240124000000/ title="arXiv @ 2024.01.24">arXiv @ 2024.01.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240125000000/ title="arXiv @ 2024.01.25">arXiv @ 2024.01.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240126000000/ title="arXiv @ 2024.01.26">arXiv @ 2024.01.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240127000000/ title="arXiv @ 2024.01.27">arXiv @ 2024.01.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240128000000/ title="arXiv @ 2024.01.28">arXiv @ 2024.01.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240129000000/ title="arXiv @ 2024.01.29">arXiv @ 2024.01.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240130000000/ title="arXiv @ 2024.01.30">arXiv @ 2024.01.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240131000000/ title="arXiv @ 2024.01.31">arXiv @ 2024.01.31</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/akitenkrad-blog/posts/arxiv/202402/>2024.02</a><ul class=active><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240201000000/ title="arXiv @ 2024.02.01">arXiv @ 2024.02.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240202000000/ title="arXiv @ 2024.02.02">arXiv @ 2024.02.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240203000000/ title="arXiv @ 2024.02.03">arXiv @ 2024.02.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240204000000/ title="arXiv @ 2024.02.04">arXiv @ 2024.02.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240205000000/ title="arXiv @ 2024.02.05">arXiv @ 2024.02.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240206000000/ title="arXiv @ 2024.02.06">arXiv @ 2024.02.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240207000000/ title="arXiv @ 2024.02.07">arXiv @ 2024.02.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240208000000/ title="arXiv @ 2024.02.08">arXiv @ 2024.02.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240209000000/ title="arXiv @ 2024.02.09">arXiv @ 2024.02.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240210000000/ title="arXiv @ 2024.02.10">arXiv @ 2024.02.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240211000000/ title="arXiv @ 2024.02.11">arXiv @ 2024.02.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240212000000/ title="arXiv @ 2024.02.12">arXiv @ 2024.02.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240213000000/ title="arXiv @ 2024.02.13">arXiv @ 2024.02.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240214000000/ title="arXiv @ 2024.02.14">arXiv @ 2024.02.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240215000000/ title="arXiv @ 2024.02.15">arXiv @ 2024.02.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240216000000/ title="arXiv @ 2024.02.16">arXiv @ 2024.02.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240217000000/ title="arXiv @ 2024.02.17">arXiv @ 2024.02.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240218000000/ title="arXiv @ 2024.02.18">arXiv @ 2024.02.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240219000000/ title="arXiv @ 2024.02.19">arXiv @ 2024.02.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240220000000/ title="arXiv @ 2024.02.20">arXiv @ 2024.02.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240221000000/ title="arXiv @ 2024.02.21">arXiv @ 2024.02.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240222000000/ title="arXiv @ 2024.02.22">arXiv @ 2024.02.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240223000000/ title="arXiv @ 2024.02.23">arXiv @ 2024.02.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240224000000/ title="arXiv @ 2024.02.24">arXiv @ 2024.02.24</a></li><li><a class=active href=/akitenkrad-blog/posts/arxiv/202402/20240225000000/ title="arXiv @ 2024.02.25">arXiv @ 2024.02.25</a></li></ul></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/algorithms/>Algorithms</a><ul><li><a href=/akitenkrad-blog/posts/algorithms/bandit-algorithm-basic/ title="Bandit Algorithm Basic">Bandit Algorithm Basic</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/conference/>Conference</a><ul><li><a href=/akitenkrad-blog/posts/conference/acl/ title="Annual Meeting of the Association for Computational Linguistics">Annual Meeting of the Association for Computational Linguistics</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/>Figures</a><ul><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/202208/>2022.08</a><ul><li><a href=/akitenkrad-blog/posts/figures/202208/20220822092748/ title=1ヶ月当たりの実労働時間の推移>1ヶ月当たりの実労働時間の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220813115200/ title=コンビニエンスストアの店舗数の推移>コンビニエンスストアの店舗数の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220821113322/ title=地域別1世帯当たり1ヶ月間の支出額の推移>地域別1世帯当たり1ヶ月間の支出額の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220828111836/ title="夫婦別子供の有無別生活時間 (2016)">夫婦別子供の有無別生活時間 (2016)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220827103615/ title=家族類型の変遷>家族類型の変遷</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220815131016/ title="年齢別大学院入学者数 (2021)">年齢別大学院入学者数 (2021)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220818122628/ title=情報サービス業における企業特殊的人的資本（名目）の推移>情報サービス業における企業特殊的人的資本（名目）の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220824095127/ title=有業者の社会生活における活動の変化>有業者の社会生活における活動の変化</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220814175022/ title=東京23区の乗用車保有台数>東京23区の乗用車保有台数</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220817121117/ title="業種別企業特殊的人的資本 (2018)">業種別企業特殊的人的資本 (2018)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220825094111/ title=無業者の社会生活における活動の変化>無業者の社会生活における活動の変化</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220823104100/ title=睡眠時間の推移>睡眠時間の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220826102652/ title=総人口の推移>総人口の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220820225528/ title=都道府県別国公立別学校数・学生数>都道府県別国公立別学校数・学生数</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220816120845/ title="都道府県別消費者物価指数/教育（全国平均=100）">都道府県別消費者物価指数/教育（全国平均=100）</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220812173814/ title=金沢市の夏の月平均気温の遷移>金沢市の夏の月平均気温の遷移</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/202209/>2022.09</a><ul><li><a href=/akitenkrad-blog/posts/figures/202209/20220907122906/ title="ACL-2022 WordCloud">ACL-2022 WordCloud</a></li></ul></li></ul></li><li><a href=/akitenkrad-blog/posts/latex/ title="Latex Mathematics Syntax Guide">Latex Mathematics Syntax Guide</a></li><li><a href=/akitenkrad-blog/posts/markdown/ title="Markdown Sample">Markdown Sample</a></li></div></ul></div></div></div></section><section class=content-section id=content-section><div class=content><div class="container p-0 read-area"><div class="hero-area col-sm-12" id=hero-area style=background-image:url(/akitenkrad-blog/posts/arxiv/202402/20240225000000/hero.png)></div><div class=page-content><div class="author-profile ml-auto align-self-lg-center"><img class=rounded-circle src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_120x120_fit_box_3.png alt="Author Image"><h5 class=author-name></h5><p>Sunday, Feb 25, 2024</p></div><div class=title><h1>arXiv @ 2024.02.25</h1></div><div class=taxonomy-terms><ul><li class=rounded><a href=/akitenkrad-blog/tags/arxiv class="btn, btn-sm">arXiv</a></li><li class=rounded><a href=/akitenkrad-blog/tags/published2024 class="btn, btn-sm">2024</a></li></ul></div><div class=post-content id=post-content><figure style=border:none;width:100%;display:flex;justify-content:center><iframe src=pie.html width=900 height=620 style=border:none></iframe></figure><h2 id=primary-categories>Primary Categories</h2><ul><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240225000000/#astro-phim-1>astro-ph.IM (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240225000000/#csai-2>cs.AI (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240225000000/#csar-1>cs.AR (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240225000000/#csce-1>cs.CE (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240225000000/#cscl-36>cs.CL (36)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240225000000/#cscr-7>cs.CR (7)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240225000000/#cscv-25>cs.CV (25)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240225000000/#csdc-2>cs.DC (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240225000000/#csdm-1>cs.DM (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240225000000/#csds-5>cs.DS (5)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240225000000/#csgt-3>cs.GT (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240225000000/#cshc-8>cs.HC (8)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240225000000/#csir-5>cs.IR (5)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240225000000/#csit-2>cs.IT (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240225000000/#cslg-56>cs.LG (56)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240225000000/#cslo-1>cs.LO (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240225000000/#csma-2>cs.MA (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240225000000/#csro-9>cs.RO (9)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240225000000/#cssd-1>cs.SD (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240225000000/#csse-3>cs.SE (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240225000000/#cssi-1>cs.SI (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240225000000/#eessas-2>eess.AS (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240225000000/#eessiv-2>eess.IV (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240225000000/#eesssy-1>eess.SY (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240225000000/#mathco-1>math.CO (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240225000000/#mathna-2>math.NA (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240225000000/#mathoc-1>math.OC (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240225000000/#mathst-2>math.ST (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240225000000/#q-bioqm-1>q-bio.QM (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240225000000/#quant-ph-1>quant-ph (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240225000000/#statml-5>stat.ML (5)</a></li></ul><h2 id=keywords>Keywords</h2><table border=1 class=dataframe><thead><tr style=text-align:right><th>keyword</th><th>cs.CL</th><th>cs.CV</th><th>cs.LG</th></tr></thead><tbody><tr><td>Active Learning</td><td></td><td></td><td>1</td></tr><tr><td>Adversarial Attack</td><td></td><td></td><td>1</td></tr><tr><td>Adversarial Learning</td><td></td><td></td><td>1</td></tr><tr><td>Anomaly Detection</td><td></td><td>1</td><td></td></tr><tr><td>Aspect-based Sentiment Analysis</td><td>1</td><td></td><td></td></tr><tr><td>Autoencoder</td><td>1</td><td>1</td><td>1</td></tr><tr><td>Automatic Evaluation</td><td>1</td><td></td><td></td></tr><tr><td>Automatic Speech Recognition</td><td></td><td>1</td><td></td></tr><tr><td>BART</td><td>1</td><td></td><td></td></tr><tr><td>BERT</td><td>1</td><td></td><td></td></tr><tr><td>BLEU</td><td></td><td>1</td><td></td></tr><tr><td>Bandit Algorithm</td><td></td><td></td><td>4</td></tr><tr><td>Benchmarking</td><td>13</td><td>8</td><td>6</td></tr><tr><td>Black Box</td><td></td><td></td><td>2</td></tr><tr><td>Clustering</td><td></td><td></td><td>1</td></tr><tr><td>Continual Learning</td><td></td><td></td><td>1</td></tr><tr><td>Continuous Time</td><td></td><td></td><td>2</td></tr><tr><td>Contrastive Learning</td><td>2</td><td>2</td><td>1</td></tr><tr><td>Convolution</td><td></td><td>5</td><td>3</td></tr><tr><td>Convolutional Neural Network</td><td></td><td>8</td><td>3</td></tr><tr><td>Counter-factual</td><td>1</td><td></td><td>1</td></tr><tr><td>Data Augmentation</td><td>1</td><td></td><td></td></tr><tr><td>Dense Retrieval</td><td>1</td><td></td><td></td></tr><tr><td>Diffusion Model</td><td>1</td><td>3</td><td>1</td></tr><tr><td>Domain Adaptation</td><td>1</td><td>2</td><td></td></tr><tr><td>Federated Learning</td><td></td><td></td><td>2</td></tr><tr><td>Few-shot</td><td>3</td><td></td><td>1</td></tr><tr><td>Fine-tuning</td><td>21</td><td>4</td><td>9</td></tr><tr><td>Foundation Model</td><td>1</td><td></td><td></td></tr><tr><td>GPT</td><td>7</td><td></td><td>1</td></tr><tr><td>GPT-2</td><td></td><td></td><td>1</td></tr><tr><td>GPT-3</td><td>1</td><td></td><td></td></tr><tr><td>GPT-3.5</td><td>1</td><td></td><td></td></tr><tr><td>GPT-4</td><td>4</td><td></td><td></td></tr><tr><td>Gaussian Process</td><td></td><td></td><td>1</td></tr><tr><td>Generative Adversarial Network</td><td></td><td>2</td><td></td></tr><tr><td>Graph</td><td>3</td><td></td><td>7</td></tr><tr><td>Graph Attention Networks</td><td>1</td><td></td><td></td></tr><tr><td>Graph Contrastive Learning</td><td></td><td></td><td>2</td></tr><tr><td>Graph Convolutional Network</td><td></td><td></td><td>1</td></tr><tr><td>Graph Neural Network</td><td>2</td><td></td><td>7</td></tr><tr><td>Grounding</td><td></td><td>1</td><td></td></tr><tr><td>Hallucination Detection</td><td>1</td><td></td><td></td></tr><tr><td>High-Resource</td><td>1</td><td></td><td></td></tr><tr><td>Human Intervention</td><td></td><td></td><td>1</td></tr><tr><td>Image2text</td><td></td><td></td><td>1</td></tr><tr><td>In-context Learning</td><td>5</td><td></td><td></td></tr><tr><td>Information Retrieval</td><td>1</td><td></td><td></td></tr><tr><td>Instruction Following</td><td>1</td><td></td><td></td></tr><tr><td>Instruction Tuning</td><td></td><td>1</td><td>1</td></tr><tr><td>Knowledge Base Question Answering</td><td>1</td><td></td><td></td></tr><tr><td>Knowledge Based Question Answering</td><td>1</td><td></td><td></td></tr><tr><td>Knowledge Distillation</td><td>2</td><td></td><td>2</td></tr><tr><td>Knowledge Graph</td><td>2</td><td></td><td></td></tr><tr><td>LLaMA</td><td>3</td><td></td><td>2</td></tr><tr><td>LSTM</td><td>1</td><td></td><td></td></tr><tr><td>Large Language Model</td><td>46</td><td>4</td><td>10</td></tr><tr><td>Logistic Regression</td><td></td><td></td><td>1</td></tr><tr><td>Low-Resource</td><td>1</td><td></td><td></td></tr><tr><td>Machine Unlearning</td><td>1</td><td></td><td>1</td></tr><tr><td>Message-Passing</td><td></td><td></td><td>2</td></tr><tr><td>Meta Learning</td><td></td><td></td><td>1</td></tr><tr><td>Mistral</td><td>2</td><td></td><td>1</td></tr><tr><td>Multi-modal</td><td></td><td>2</td><td>3</td></tr><tr><td>Mutual Information</td><td></td><td></td><td>1</td></tr><tr><td>Named Entity Recognition</td><td>2</td><td></td><td></td></tr><tr><td>Natural Language Inference</td><td>4</td><td></td><td></td></tr><tr><td>Neural Machine Translation</td><td>6</td><td></td><td>2</td></tr><tr><td>Node Classification</td><td></td><td></td><td>1</td></tr><tr><td>Object Detection</td><td></td><td>1</td><td>1</td></tr><tr><td>Opinion Summarization</td><td>1</td><td></td><td></td></tr><tr><td>Optical Character Recognition</td><td></td><td>2</td><td></td></tr><tr><td>Out-of-distribution</td><td>1</td><td>1</td><td></td></tr><tr><td>Outlier Detection</td><td></td><td>1</td><td></td></tr><tr><td>Pre-trained Language Model</td><td>10</td><td></td><td></td></tr><tr><td>Probabilistic Model</td><td></td><td></td><td>1</td></tr><tr><td>Prompt</td><td>9</td><td>1</td><td>5</td></tr><tr><td>Quantization</td><td></td><td></td><td>1</td></tr><tr><td>Question Answering</td><td>3</td><td></td><td>2</td></tr><tr><td>Reasoning</td><td>3</td><td>1</td><td>4</td></tr><tr><td>Recommendation</td><td></td><td></td><td>1</td></tr><tr><td>Reconstruction Loss</td><td>1</td><td></td><td></td></tr><tr><td>Recurrent Neural Network</td><td></td><td></td><td>1</td></tr><tr><td>Reinforcement Learning</td><td>1</td><td></td><td>5</td></tr><tr><td>Reinforcement Learning from Human Feedback</td><td>2</td><td></td><td></td></tr><tr><td>Relation Extraction</td><td>1</td><td></td><td></td></tr><tr><td>Representation Learning</td><td>1</td><td></td><td>1</td></tr><tr><td>Retrieval-Augmented Generation</td><td>3</td><td></td><td></td></tr><tr><td>RoBERTa</td><td></td><td></td><td>1</td></tr><tr><td>Self-Attention</td><td></td><td></td><td>1</td></tr><tr><td>Self-supervised Learning</td><td></td><td>3</td><td>1</td></tr><tr><td>Self-supervised Pre-training</td><td></td><td></td><td>1</td></tr><tr><td>Semantic Parsing</td><td>1</td><td></td><td></td></tr><tr><td>Semi-Supervised Learning</td><td></td><td>1</td><td></td></tr><tr><td>Sentence Embedding</td><td>2</td><td></td><td></td></tr><tr><td>Sentiment Analysis</td><td>4</td><td></td><td></td></tr><tr><td>Simulation</td><td></td><td></td><td>5</td></tr><tr><td>Simulator</td><td></td><td></td><td>5</td></tr><tr><td>Slot Filling</td><td>1</td><td></td><td></td></tr><tr><td>Stance Detection</td><td>1</td><td></td><td></td></tr><tr><td>Stemming</td><td>1</td><td></td><td></td></tr><tr><td>Stochastic Gradient Descent</td><td></td><td></td><td>1</td></tr><tr><td>Style Transfer</td><td></td><td></td><td>1</td></tr><tr><td>Summarization</td><td>3</td><td></td><td>2</td></tr><tr><td>Supervised Learning</td><td>1</td><td>7</td><td>3</td></tr><tr><td>T5</td><td></td><td></td><td>1</td></tr><tr><td>Text Classification</td><td></td><td></td><td>1</td></tr><tr><td>Text Embedding</td><td>1</td><td></td><td></td></tr><tr><td>Text Generation</td><td>1</td><td>1</td><td>1</td></tr><tr><td>Text2image</td><td></td><td>3</td><td></td></tr><tr><td>Transfer Learning</td><td>1</td><td></td><td>1</td></tr><tr><td>Transformer</td><td>3</td><td>4</td><td>5</td></tr><tr><td>Unsupervised Learning</td><td>2</td><td>5</td><td>3</td></tr><tr><td>Variational Autoencoder</td><td></td><td></td><td>1</td></tr><tr><td>Vision Transformer</td><td></td><td>2</td><td></td></tr><tr><td>Vision-and-Language</td><td></td><td>3</td><td>1</td></tr><tr><td>Visual Question Answering</td><td></td><td></td><td>1</td></tr><tr><td>Weakly Supervised Learning</td><td></td><td></td><td>2</td></tr><tr><td>Weakly-supervised Learning</td><td></td><td></td><td>2</td></tr><tr><td>Zero-shot</td><td>5</td><td></td><td></td></tr></tbody></table><script>$(function(){$("table").addClass("keyword-table table-bordered border-success"),$("table thead").addClass("sticky-top"),$("table tbody td").css("text-align","")})</script><h2 id=cscl-36>cs.CL (36)</h2><h3 id=136--1190-interactive-kbqa-multi-turn-interactions-for-knowledge-base-question-answering-with-large-language-models-guanming-xiong-et-al-2024>(1/36 | 1/190) Interactive-KBQA: Multi-Turn Interactions for Knowledge Base Question Answering with Large Language Models (Guanming Xiong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Guanming Xiong, Junwei Bao, Wen Zhao. (2024)<br><strong>Interactive-KBQA: Multi-Turn Interactions for Knowledge Base Question Answering with Large Language Models</strong><br><button class=copy-to-clipboard title="Interactive-KBQA: Multi-Turn Interactions for Knowledge Base Question Answering with Large Language Models" index=1>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-1 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: I-2-7, cs-AI, cs-CL, cs.CL<br>Keyword Score: 110<br>Keywords: Few-shot, Low-Resource, Knowledge Base Question Answering, Knowledge Based Question Answering, Question Answering, Reasoning, Semantic Parsing, In-context Learning, In-context Learning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15131v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15131v1.pdf filename=2402.15131v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This study explores the realm of <b>knowledge-base</b> <b>question</b> <b>answering</b> <b>(KBQA).</b> <b>KBQA</b> is considered a challenging task, particularly in parsing intricate <b>questions</b> <b>into</b> executable logical forms. Traditional <b>semantic</b> <b>parsing</b> (SP)-based methods require extensive data annotations, which result in significant costs. Recently, the advent of <b>few-shot</b> <b>in-context</b> <b>learning,</b> powered by <b>large</b> <b>language</b> <b>models</b> <b>(LLMs),</b> has showcased promising capabilities. Yet, fully leveraging <b>LLMs</b> to parse <b>questions</b> <b>into</b> logical forms in <b>low-resource</b> scenarios poses a substantial challenge. To tackle these hurdles, we introduce Interactive-KBQA, a framework designed to generate logical forms through direct interaction with <b>knowledge</b> <b>bases</b> <b>(KBs).</b> <b>Within</b> this framework, we have developed three generic APIs for KB interaction. For each category of complex <b>question,</b> <b>we</b> devised exemplars to guide <b>LLMs</b> through the <b>reasoning</b> processes. Our method achieves competitive results on the WebQuestionsSP, ComplexWebQuestions, KQA Pro, and MetaQA datasets with a minimal number of examples (shots). Importantly, our approach supports manual intervention, allowing for the iterative refinement of <b>LLM</b> outputs. By annotating a dataset with step-wise <b>reasoning</b> processes, we showcase our model&rsquo;s adaptability and highlight its potential for contributing significant enhancements to the field.</p></p class="citation"></blockquote><h3 id=236--2190-fine-tuning-large-language-models-for-domain-specific-machine-translation-jiawei-zheng-et-al-2024>(2/36 | 2/190) Fine-tuning Large Language Models for Domain-specific Machine Translation (Jiawei Zheng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiawei Zheng, Hanghai Hong, Xiaoli Wang, Jingsong Su, Yonggui Liang, Shikai Wu. (2024)<br><strong>Fine-tuning Large Language Models for Domain-specific Machine Translation</strong><br><button class=copy-to-clipboard title="Fine-tuning Large Language Models for Domain-specific Machine Translation" index=2>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-2 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keyword Score: 110<br>Keywords: Fine-tuning, Fine-tuning, Zero-shot, Neural Machine Translation, Neural Machine Translation, Domain Adaptation, In-context Learning, In-context Learning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15061v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15061v1.pdf filename=2402.15061v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have made significant progress in <b>machine</b> <b>translation</b> <b>(MT).</b> However, their potential in <b>domain-specific</b> <b>MT</b> remains under-explored. Current <b>LLM-based</b> <b>MT</b> systems still face several challenges. First, for <b>LLMs</b> with <b>in-context</b> <b>learning,</b> their effectiveness is highly sensitive to input translation examples, and processing them can increase inference costs. They often require extra post-processing due to over-generation. Second, <b>LLMs</b> with <b>fine-tuning</b> on <b>domain-specific</b> <b>data</b> often require high training costs for <b>domain</b> <b>adaptation,</b> and may weaken the <b>zero-shot</b> <b>MT</b> capabilities of <b>LLMs</b> due to over-specialization. The aforementioned methods can struggle to translate rare words in <b>domain</b> <b>transfer</b> scenarios. To address these challenges, this paper proposes a <b>prompt-oriented</b> <b>fine-tuning</b> method, denoted as LlamaIT, to effectively and efficiently <b>fine-tune</b> a general-purpose <b>LLM</b> for <b>domain-specific</b> <b>MT</b> tasks. First, we construct a task-specific mix-domain dataset, which is then used to <b>fine-tune</b> the <b>LLM</b> with LoRA. This can eliminate the need for input translation examples, post-processing, or over-specialization. By <b>zero-shot</b> <b>prompting</b> with instructions, we adapt the <b>MT</b> tasks to the target <b>domain</b> <b>at</b> inference time. To further elicit the <b>MT</b> capability for rare words, we construct new <b>prompts</b> by incorporating <b>domain-specific</b> <b>bilingual</b> vocabulary. We also conduct extensive experiments on both publicly available and self-constructed datasets. The results show that our LlamaIT can significantly enhance the <b>domain-specific</b> <b>MT</b> capabilities of the <b>LLM,</b> meanwhile preserving its <b>zero-shot</b> <b>MT</b> capabilities.</p></p class="citation"></blockquote><h3 id=336--3190-arabiangpt-native-arabic-gpt-based-large-language-anis-koubaa-et-al-2024>(3/36 | 3/190) ArabianGPT: Native Arabic GPT-based Large Language (Anis Koubaa et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Anis Koubaa, Adel Ammar, Lahouari Ghouti, Omar Najar, Serry Sibaee. (2024)<br><strong>ArabianGPT: Native Arabic GPT-based Large Language</strong><br><button class=copy-to-clipboard title="ArabianGPT: Native Arabic GPT-based Large Language" index=3>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-3 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 93<br>Keywords: Benchmarking, Fine-tuning, Fine-tuning, GPT, Transformer, Question Answering, Sentiment Analysis, Large Language Model, Large Language Model, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15313v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15313v1.pdf filename=2402.15313v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The predominance of English and Latin-based <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> has led to a notable deficit in native Arabic <b>LLMs.</b> This discrepancy is accentuated by the prevalent inclusion of English tokens in existing Arabic models, detracting from their efficacy in processing native Arabic&rsquo;s intricate morphology and syntax. Consequently, there is a theoretical and practical imperative for developing <b>LLMs</b> predominantly focused on Arabic linguistic elements. To address this gap, this paper proposes ArabianGPT, a series of <b>transformer-based</b> models within the ArabianLLM suite designed explicitly for Arabic. These models, including ArabianGPT-0.1B and ArabianGPT-0.3B, vary in size and complexity, aligning with the nuanced linguistic characteristics of Arabic. The AraNizer tokenizer, integral to these models, addresses the unique morphological aspects of Arabic script, ensuring more accurate text processing. Empirical results from <b>fine-tuning</b> the models on tasks like <b>sentiment</b> <b>analysis</b> and <b>summarization</b> demonstrate significant improvements. For <b>sentiment</b> <b>analysis,</b> the <b>fine-tuned</b> ArabianGPT-0.1B model achieved a remarkable accuracy of 95%, a substantial increase from the base model&rsquo;s 56%. Similarly, in <b>summarization</b> tasks, <b>fine-tuned</b> models showed enhanced F1 scores, indicating improved precision and recall in generating concise summaries. Comparative analysis of <b>fine-tuned</b> ArabianGPT models against their base versions across various <b>benchmarks</b> reveals nuanced differences in performance, with <b>fine-tuning</b> positively impacting specific tasks like <b>question</b> <b>answering</b> and <b>summarization.</b> These findings underscore the efficacy of <b>fine-tuning</b> in aligning ArabianGPT models more closely with specific NLP tasks, highlighting the potential of tailored <b>transformer</b> architectures in advancing Arabic NLP.</p></p class="citation"></blockquote><h3 id=436--4190-repetition-improves-language-model-embeddings-jacob-mitchell-springer-et-al-2024>(4/36 | 4/190) Repetition Improves Language Model Embeddings (Jacob Mitchell Springer et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jacob Mitchell Springer, Suhas Kotha, Daniel Fried, Graham Neubig, Aditi Raghunathan. (2024)<br><strong>Repetition Improves Language Model Embeddings</strong><br><button class=copy-to-clipboard title="Repetition Improves Language Model Embeddings" index=4>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-4 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keyword Score: 90<br>Keywords: Fine-tuning, Fine-tuning, Zero-shot, Mistral, In-context Learning, Large Language Model, Large Language Model, Pre-trained Language Model, Text Embedding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15449v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15449v1.pdf filename=2402.15449v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent approaches to improving the extraction of <b>text</b> <b>embeddings</b> from autoregressive <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have largely focused on improvements to data, backbone <b>pretrained</b> <b>language</b> <b>models,</b> or improving task-differentiation via instructions. In this work, we address an architectural limitation of autoregressive models: token embeddings cannot contain information from tokens that appear later in the input. To address this limitation, we propose a simple approach, &ldquo;echo embeddings,&rdquo; in which we repeat the input twice in context and extract embeddings from the second occurrence. We show that echo embeddings of early tokens can encode information about later tokens, allowing us to maximally leverage high-quality <b>LLMs</b> for embeddings. On the MTEB leaderboard, echo embeddings improve over classical embeddings by over 9% <b>zero-shot</b> and by around 0.7% when <b>fine-tuned.</b> Echo embeddings with a <b>Mistral-7B</b> model achieve state-of-the-art compared to prior open source models that do not leverage synthetic <b>fine-tuning</b> data.</p></p class="citation"></blockquote><h3 id=536--5190-a-data-centric-approach-to-generate-faithful-and-high-quality-patient-summaries-with-large-language-models-stefan-hegselmann-et-al-2024>(5/36 | 5/190) A Data-Centric Approach To Generate Faithful and High Quality Patient Summaries with Large Language Models (Stefan Hegselmann et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Stefan Hegselmann, Shannon Zejiang Shen, Florian Gierse, Monica Agrawal, David Sontag, Xiaoyi Jiang. (2024)<br><strong>A Data-Centric Approach To Generate Faithful and High Quality Patient Summaries with Large Language Models</strong><br><button class=copy-to-clipboard title="A Data-Centric Approach To Generate Faithful and High Quality Patient Summaries with Large Language Models" index=5>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-5 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 80<br>Keywords: Fine-tuning, Zero-shot, GPT, GPT-4, LLaMA, Hallucination Detection, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15422v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15422v1.pdf filename=2402.15422v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Patients often face difficulties in understanding their hospitalizations, while healthcare workers have limited resources to provide explanations. In this work, we investigate the potential of <b>large</b> <b>language</b> <b>models</b> to generate patient summaries based on doctors&rsquo; notes and study the effect of training data on the faithfulness and quality of the generated summaries. To this end, we develop a rigorous labeling protocol for <b>hallucinations,</b> <b>and</b> have two medical experts annotate 100 real-world summaries and 100 generated summaries. We show that <b>fine-tuning</b> on <b>hallucination-free</b> <b>data</b> effectively reduces <b>hallucinations</b> <b>from</b> 2.60 to 1.55 per summary for <b>Llama</b> 2, while preserving relevant information. Although the effect is still present, it is much smaller for <b>GPT-4</b> when <b>prompted</b> with five examples (0.70 to 0.40). We also conduct a qualitative evaluation using <b>hallucination-free</b> <b>and</b> improved training data. <b>GPT-4</b> shows very good results even in the <b>zero-shot</b> setting. We find that common quantitative metrics do not correlate well with faithfulness and quality. Finally, we test <b>GPT-4</b> for automatic <b>hallucination</b> <b>detection,</b> which yields promising results.</p></p class="citation"></blockquote><h3 id=636--6190-improving-sentence-embeddings-with-an-automatically-generated-nli-dataset-soma-sato-et-al-2024>(6/36 | 6/190) Improving Sentence Embeddings with an Automatically Generated NLI Dataset (Soma Sato et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Soma Sato, Hayato Tsukagoshi, Ryohei Sasano, Koichi Takeda. (2024)<br><strong>Improving Sentence Embeddings with an Automatically Generated NLI Dataset</strong><br><button class=copy-to-clipboard title="Improving Sentence Embeddings with an Automatically Generated NLI Dataset" index=6>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-6 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keyword Score: 80<br>Keywords: Fine-tuning, Fine-tuning, Unsupervised Learning, Natural Language Inference, Natural Language Inference, Sentence Embedding, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15132v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15132v1.pdf filename=2402.15132v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Decoder-based <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have shown high performance on many tasks in <b>natural</b> <b>language</b> <b>processing.</b> This is also true for <b>sentence</b> <b>embedding</b> learning, where a decoder-based model, PromptEOL, has achieved the best performance on semantic textual similarity (STS) tasks. However, PromptEOL makes great use of <b>fine-tuning</b> with a manually annotated <b>natural</b> <b>language</b> <b>inference</b> <b>(NLI)</b> dataset. We aim to improve <b>sentence</b> <b>embeddings</b> learned in an <b>unsupervised</b> setting by automatically generating an <b>NLI</b> dataset with an <b>LLM</b> and using it to <b>fine-tune</b> PromptEOL. In experiments on STS tasks, the proposed method achieved an average Spearman&rsquo;s rank correlation coefficient of 82.21 with respect to human evaluation, thus outperforming existing methods without using <b>large,</b> <b>manually</b> <b>annotated</b> datasets.</p></p class="citation"></blockquote><h3 id=736--7190-nuner-entity-recognition-encoder-pre-training-via-llm-annotated-data-sergei-bogdanov-et-al-2024>(7/36 | 7/190) NuNER: Entity Recognition Encoder Pre-training via LLM-Annotated Data (Sergei Bogdanov et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sergei Bogdanov, Alexandre Constantin, Timothée Bernard, Benoit Crabbé, Etienne Bernard. (2024)<br><strong>NuNER: Entity Recognition Encoder Pre-training via LLM-Annotated Data</strong><br><button class=copy-to-clipboard title="NuNER: Entity Recognition Encoder Pre-training via LLM-Annotated Data" index=7>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-7 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 70<br>Keywords: Few-shot, Fine-tuning, Foundation Model, Named Entity Recognition, Named Entity Recognition, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15343v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15343v1.pdf filename=2402.15343v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have shown impressive abilities in data annotation, opening the way for new approaches to solve classic NLP problems. In this paper, we show how to use <b>LLMs</b> to create NuNER, a compact language representation model specialized in the <b>Named</b> <b>Entity</b> <b>Recognition</b> <b>(NER)</b> task. NuNER can be <b>fine-tuned</b> to solve downstream <b>NER</b> problems in a data-efficient way, outperforming similar-sized <b>foundation</b> <b>models</b> in the <b>few-shot</b> regime and competing with much larger <b>LLMs.</b> We find that the size and entity-type diversity of the pre-training dataset are key to achieving good performance. We view NuNER as a member of the broader family of task-specific <b>foundation</b> <b>models,</b> recently unlocked by <b>LLMs.</b></p></p class="citation"></blockquote><h3 id=836--8190-entity-level-factual-adaptiveness-of-fine-tuning-based-abstractive-summarization-models-jongyoon-song-et-al-2024>(8/36 | 8/190) Entity-level Factual Adaptiveness of Fine-tuning based Abstractive Summarization Models (Jongyoon Song et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jongyoon Song, Nohil Park, Bongkyu Hwang, Jaewoong Yun, Seongho Joe, Youngjune L. Gwon, Sungroh Yoon. (2024)<br><strong>Entity-level Factual Adaptiveness of Fine-tuning based Abstractive Summarization Models</strong><br><button class=copy-to-clipboard title="Entity-level Factual Adaptiveness of Fine-tuning based Abstractive Summarization Models" index=8>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-8 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 70<br>Keywords: Contrastive Learning, Counter-factual, Data Augmentation, Fine-tuning, BART, Pre-trained Language Model, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15162v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15162v1.pdf filename=2402.15162v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Abstractive <b>summarization</b> models often generate factually inconsistent content particularly when the parametric knowledge of the model conflicts with the knowledge in the input document. In this paper, we analyze the robustness of <b>fine-tuning</b> based <b>summarization</b> models to the knowledge conflict, which we call factual adaptiveness. We utilize <b>pre-trained</b> <b>language</b> <b>models</b> to construct evaluation sets and find that factual adaptiveness is not strongly correlated with factual consistency on original datasets. Furthermore, we introduce a controllable <b>counterfactual</b> <b>data</b> <b>augmentation</b> method where the degree of knowledge conflict within the augmented <b>data</b> <b>can</b> be adjustable. Our experimental results on two <b>pre-trained</b> <b>language</b> <b>models</b> (PEGASUS and <b>BART)</b> and two <b>fine-tuning</b> datasets (XSum and CNN/DailyMail) demonstrate that our method enhances factual adaptiveness while achieving factual consistency on original datasets on par with the <b>contrastive</b> <b>learning</b> baseline.</p></p class="citation"></blockquote><h3 id=936--9190-colbert-xm-a-modular-multi-vector-representation-model-for-zero-shot-multilingual-information-retrieval-antoine-louis-et-al-2024>(9/36 | 9/190) ColBERT-XM: A Modular Multi-Vector Representation Model for Zero-Shot Multilingual Information Retrieval (Antoine Louis et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Antoine Louis, Vageesh Saxena, Gijs van Dijck, Gerasimos Spanakis. (2024)<br><strong>ColBERT-XM: A Modular Multi-Vector Representation Model for Zero-Shot Multilingual Information Retrieval</strong><br><button class=copy-to-clipboard title="ColBERT-XM: A Modular Multi-Vector Representation Model for Zero-Shot Multilingual Information Retrieval" index=9>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-9 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-IR, cs.CL<br>Keyword Score: 70<br>Keywords: Dense Retrieval, Fine-tuning, High-Resource, Out-of-distribution, Zero-shot, Information Retrieval, Pre-trained Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15059v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15059v1.pdf filename=2402.15059v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>State-of-the-art neural retrievers predominantly focus on <b>high-resource</b> languages like English, which impedes their adoption in retrieval scenarios involving other languages. Current approaches circumvent the lack of high-quality labeled data in non-English languages by leveraging multilingual <b>pretrained</b> <b>language</b> <b>models</b> capable of cross-lingual transfer. However, these models require substantial task-specific <b>fine-tuning</b> across multiple languages, often perform poorly in languages with minimal representation in the pretraining corpus, and struggle to incorporate new languages after the pretraining phase. In this work, we present a novel modular <b>dense</b> <b>retrieval</b> model that learns from the rich data of a single <b>high-resource</b> language and effectively <b>zero-shot</b> transfers to a wide array of languages, thereby eliminating the need for language-specific labeled data. Our model, ColBERT-XM, demonstrates competitive performance against existing state-of-the-art multilingual retrievers trained on more extensive datasets in various languages. Further analysis reveals that our modular approach is highly data-efficient, effectively adapts to <b>out-of-distribution</b> data, and significantly reduces energy consumption and carbon emissions. By demonstrating its proficiency in <b>zero-shot</b> scenarios, ColBERT-XM marks a shift towards more sustainable and inclusive retrieval systems, enabling effective <b>information</b> <b>accessibility</b> in numerous languages. We publicly release our code and models for the community.</p></p class="citation"></blockquote><h3 id=1036--10190-attributionbench-how-hard-is-automatic-attribution-evaluation-yifei-li-et-al-2024>(10/36 | 10/190) AttributionBench: How Hard is Automatic Attribution Evaluation? (Yifei Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yifei Li, Xiang Yue, Zeyi Liao, Huan Sun. (2024)<br><strong>AttributionBench: How Hard is Automatic Attribution Evaluation?</strong><br><button class=copy-to-clipboard title="AttributionBench: How Hard is Automatic Attribution Evaluation?" index=10>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-10 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 63<br>Keywords: Benchmarking, Fine-tuning, GPT, GPT-3, GPT-3.5, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15089v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15089v1.pdf filename=2402.15089v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Modern generative search engines enhance the reliability of <b>large</b> <b>language</b> <b>model</b> <b>(LLM)</b> responses by providing cited evidence. However, evaluating the answer&rsquo;s attribution, i.e., whether every claim within the generated responses is fully supported by its cited evidence, remains an open problem. This verification, traditionally dependent on costly human evaluation, underscores the urgent need for automatic attribution evaluation methods. To bridge the gap in the absence of standardized <b>benchmarks</b> for these methods, we present AttributionBench, a comprehensive <b>benchmark</b> compiled from various existing attribution datasets. Our extensive experiments on AttributionBench reveal the challenges of automatic attribution evaluation, even for state-of-the-art <b>LLMs.</b> Specifically, our findings show that even a <b>fine-tuned</b> <b>GPT-3.5</b> only achieves around 80% macro-F1 under a binary classification formulation. A detailed analysis of more than 300 error cases indicates that a majority of failures stem from the model&rsquo;s inability to process nuanced information, and the discrepancy between the information the model has access to and that human annotators do.</p></p class="citation"></blockquote><h3 id=1136--11190-ranking-entities-along-conceptual-space-dimensions-with-llms-an-analysis-of-fine-tuning-strategies-nitesh-kumar-et-al-2024>(11/36 | 11/190) Ranking Entities along Conceptual Space Dimensions with LLMs: An Analysis of Fine-Tuning Strategies (Nitesh Kumar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nitesh Kumar, Usashi Chatterjee, Steven Schockaert. (2024)<br><strong>Ranking Entities along Conceptual Space Dimensions with LLMs: An Analysis of Fine-Tuning Strategies</strong><br><button class=copy-to-clipboard title="Ranking Entities along Conceptual Space Dimensions with LLMs: An Analysis of Fine-Tuning Strategies" index=11>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-11 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keyword Score: 60<br>Keywords: Fine-tuning, Fine-tuning, Knowledge Distillation, Zero-shot, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15337v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15337v1.pdf filename=2402.15337v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Conceptual spaces represent entities in terms of their primitive semantic features. Such representations are highly valuable but they are notoriously difficult to learn, especially when it comes to modelling perceptual and subjective features. <b>Distilling</b> conceptual spaces from <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> has recently emerged as a promising strategy. However, existing work has been limited to probing pre-trained <b>LLMs</b> using relatively simple <b>zero-shot</b> strategies. We focus in particular on the task of ranking entities according to a given conceptual space dimension. Unfortunately, we cannot directly <b>fine-tune</b> <b>LLMs</b> on this task, because ground truth rankings for conceptual space dimensions are rare. We therefore use more readily available features as training data and analyse whether the ranking capabilities of the resulting models transfer to perceptual and subjective features. We find that this is indeed the case, to some extent, but having perceptual and subjective features in the training data seems essential for achieving the best results. We furthermore find that pointwise ranking strategies are competitive against pairwise approaches, in defiance of common wisdom.</p></p class="citation"></blockquote><h3 id=1236--12190-how-unethical-are-instruction-centric-responses-of-llms-unveiling-the-vulnerabilities-of-safety-guardrails-to-harmful-queries-somnath-banerjee-et-al-2024>(12/36 | 12/190) How (un)ethical are instruction-centric responses of LLMs? Unveiling the vulnerabilities of safety guardrails to harmful queries (Somnath Banerjee et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Somnath Banerjee, Sayan Layek, Rima Hazra, Animesh Mukherjee. (2024)<br><strong>How (un)ethical are instruction-centric responses of LLMs? Unveiling the vulnerabilities of safety guardrails to harmful queries</strong><br><button class=copy-to-clipboard title="How (un)ethical are instruction-centric responses of LLMs? Unveiling the vulnerabilities of safety guardrails to harmful queries" index=12>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-12 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-CR, cs.CL<br>Keyword Score: 60<br>Keywords: GPT, GPT-4, LLaMA, Mistral, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15302v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15302v1.pdf filename=2402.15302v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this study, we tackle a growing concern around the safety and ethical use of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs).</b> Despite their potential, these models can be tricked into producing harmful or unethical content through various sophisticated methods, including &lsquo;jailbreaking&rsquo; techniques and targeted manipulation. Our work zeroes in on a specific issue: to what extent <b>LLMs</b> can be led astray by asking them to generate responses that are instruction-centric such as a pseudocode, a program or a software snippet as opposed to vanilla text. To investigate this question, we introduce TechHazardQA, a dataset containing complex queries which should be answered in both text and instruction-centric formats (e.g., pseudocodes), aimed at identifying triggers for unethical responses. We query a series of <b>LLMs</b> &ndash; <b>Llama-2-13b,</b> <b>Llama-2-7b,</b> <b>Mistral-V2</b> and <b>Mistral</b> 8X7B &ndash; and ask them to generate both text and instruction-centric responses. For evaluation we report the harmfulness score metric as well as judgements from <b>GPT-4</b> and humans. Overall, we observe that asking <b>LLMs</b> to produce instruction-centric responses enhances the unethical response generation by ~2-38% across the models. As an additional objective, we investigate the impact of model editing using the ROME technique, which further increases the propensity for generating undesirable content. In particular, asking edited <b>LLMs</b> to generate instruction-centric responses further increases the unethical response generation by ~3-16% across the different models.</p></p class="citation"></blockquote><h3 id=1336--13190-dempt-decoding-enhanced-multi-phase-prompt-tuning-for-making-llms-be-better-context-aware-translators-xinglin-lyu-et-al-2024>(13/36 | 13/190) DeMPT: Decoding-enhanced Multi-phase Prompt Tuning for Making LLMs Be Better Context-aware Translators (Xinglin Lyu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xinglin Lyu, Junhui Li, Yanqing Zhao, Min Zhang, Daimeng Wei, Shimin Tao, Hao Yang, Min Zhang. (2024)<br><strong>DeMPT: Decoding-enhanced Multi-phase Prompt Tuning for Making LLMs Be Better Context-aware Translators</strong><br><button class=copy-to-clipboard title="DeMPT: Decoding-enhanced Multi-phase Prompt Tuning for Making LLMs Be Better Context-aware Translators" index=13>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-13 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 60<br>Keywords: Neural Machine Translation, Neural Machine Translation, Neural Machine Translation, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15200v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15200v1.pdf filename=2402.15200v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Generally, the decoder-only <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> are adapted to context-aware <b>neural</b> <b>machine</b> <b>translation</b> <b>(NMT)</b> in a concatenating way, where <b>LLMs</b> take the concatenation of the source sentence (i.e., intra-sentence context) and the inter-sentence context as the input, and then to generate the target tokens sequentially. This adaptation strategy, i.e., concatenation mode, considers intra-sentence and inter-sentence contexts with the same priority, despite an apparent difference between the two kinds of contexts. In this paper, we propose an alternative adaptation approach, named Decoding-enhanced Multi-phase <b>Prompt</b> Tuning (DeMPT), to make <b>LLMs</b> discriminately model and utilize the inter- and intra-sentence context and more effectively adapt <b>LLMs</b> to context-aware <b>NMT.</b> First, DeMPT divides the context-aware <b>NMT</b> process into three separate phases. During each phase, different continuous <b>prompts</b> are introduced to make <b>LLMs</b> discriminately model various information. Second, DeMPT employs a heuristic way to further discriminately enhance the utilization of the source-side inter- and intra-sentence information at the final decoding phase. Experiments show that our approach significantly outperforms the concatenation method, and further improves the performance of <b>LLMs</b> in discourse modeling.</p></p class="citation"></blockquote><h3 id=1436--14190-self-adaptive-reconstruction-with-contrastive-learning-for-unsupervised-sentence-embeddings-junlong-liu-et-al-2024>(14/36 | 14/190) Self-Adaptive Reconstruction with Contrastive Learning for Unsupervised Sentence Embeddings (Junlong Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Junlong Liu, Xichen Shang, Huawen Feng, Junhao Zheng, Qianli Ma. (2024)<br><strong>Self-Adaptive Reconstruction with Contrastive Learning for Unsupervised Sentence Embeddings</strong><br><button class=copy-to-clipboard title="Self-Adaptive Reconstruction with Contrastive Learning for Unsupervised Sentence Embeddings" index=14>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-14 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keyword Score: 60<br>Keywords: Autoencoder, Contrastive Learning, Reconstruction Loss, Unsupervised Learning, Sentence Embedding, Pre-trained Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15153v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15153v1.pdf filename=2402.15153v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Unsupervised</b> <b>sentence</b> <b>embeddings</b> task aims to convert <b>sentences</b> <b>to</b> semantic vector representations. Most previous works directly use the <b>sentence</b> <b>representations</b> derived from <b>pretrained</b> <b>language</b> <b>models.</b> However, due to the token bias in <b>pretrained</b> <b>language</b> <b>models,</b> the models can not capture the fine-grained semantics in <b>sentences,</b> <b>which</b> leads to poor predictions. To address this issue, we propose a novel Self-Adaptive <b>Reconstruction</b> <b>Contrastive</b> <b>Sentence</b> <b>Embeddings</b> (SARCSE) framework, which reconstructs all tokens in <b>sentences</b> <b>with</b> an <b>AutoEncoder</b> to help the model to preserve more fine-grained semantics during tokens aggregating. In addition, we proposed a self-adaptive <b>reconstruction</b> <b>loss</b> to alleviate the token bias towards frequency. Experimental results show that SARCSE gains significant improvements compared with the strong baseline SimCSE on the 7 STS tasks.</p></p class="citation"></blockquote><h3 id=1536--15190-pemt-multi-task-correlation-guided-mixture-of-experts-enables-parameter-efficient-transfer-learning-zhisheng-lin-et-al-2024>(15/36 | 15/190) PEMT: Multi-Task Correlation Guided Mixture-of-Experts Enables Parameter-Efficient Transfer Learning (Zhisheng Lin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhisheng Lin, Han Fu, Chenghao Liu, Zhuo Li, Jianling Sun. (2024)<br><strong>PEMT: Multi-Task Correlation Guided Mixture-of-Experts Enables Parameter-Efficient Transfer Learning</strong><br><button class=copy-to-clipboard title="PEMT: Multi-Task Correlation Guided Mixture-of-Experts Enables Parameter-Efficient Transfer Learning" index=15>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-15 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keyword Score: 60<br>Keywords: Graph Attention Networks, Fine-tuning, Knowledge Distillation, Transfer Learning, Pre-trained Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15082v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15082v1.pdf filename=2402.15082v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Parameter-efficient <b>fine-tuning</b> (PEFT) has emerged as an effective method for adapting <b>pre-trained</b> <b>language</b> <b>models</b> to various tasks efficiently. Recently, there has been a growing interest in transferring knowledge from one or multiple tasks to the downstream target task to achieve performance improvements. However, current approaches typically either train adapters on individual tasks or <b>distill</b> shared knowledge from source tasks, failing to fully exploit task-specific knowledge and the correlation between source and target tasks. To overcome these limitations, we propose PEMT, a novel parameter-efficient <b>fine-tuning</b> framework based on multi-task <b>transfer</b> <b>learning.</b> PEMT extends the mixture-of-experts (MoE) framework to capture the transferable knowledge as a weighted combination of adapters trained on source tasks. These weights are determined by a <b>gated</b> unit, measuring the correlation between the target and each source task using task description <b>prompt</b> vectors. To fully exploit the task-specific knowledge, we also propose the Task Sparsity Loss to improve the sparsity of the <b>gated</b> unit. We conduct experiments on a broad range of tasks over 17 datasets. The experimental results demonstrate our PEMT yields stable improvements over full <b>fine-tuning,</b> and state-of-the-art PEFT and knowledge transferring methods on various tasks. The results highlight the effectiveness of our method which is capable of sufficiently exploiting the knowledge and correlation features across multiple tasks.</p></p class="citation"></blockquote><h3 id=1636--16190-dual-encoder-exploiting-the-potential-of-syntactic-and-semantic-for-aspect-sentiment-triplet-extraction-xiaowei-zhao-et-al-2024>(16/36 | 16/190) Dual Encoder: Exploiting the Potential of Syntactic and Semantic for Aspect Sentiment Triplet Extraction (Xiaowei Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiaowei Zhao, Yong Zhou, Xiujuan Xu. (2024)<br><strong>Dual Encoder: Exploiting the Potential of Syntactic and Semantic for Aspect Sentiment Triplet Extraction</strong><br><button class=copy-to-clipboard title="Dual Encoder: Exploiting the Potential of Syntactic and Semantic for Aspect Sentiment Triplet Extraction" index=16>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-16 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 56<br>Keywords: Graph, Graph Neural Network, Graph Neural Network, Benchmarking, BERT, LSTM, Sentiment Analysis<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15370v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15370v1.pdf filename=2402.15370v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Aspect <b>Sentiment</b> <b>Triple</b> Extraction (ASTE) is an emerging task in fine-grained <b>sentiment</b> <b>analysis.</b> Recent studies have employed <b>Graph</b> <b>Neural</b> <b>Networks</b> <b>(GNN)</b> to model the syntax-semantic relationships inherent in triplet elements. However, they have yet to fully tap into the vast potential of syntactic and semantic information within the ASTE task. In this work, we propose a \emph{Dual Encoder: Exploiting the potential of Syntactic and Semantic} model (D2E2S), which maximizes the syntactic and semantic relationships among words. Specifically, our model utilizes a dual-channel encoder with a <b>BERT</b> channel to capture semantic information, and an enhanced <b>LSTM</b> channel for comprehensive syntactic information capture. Subsequently, we introduce the heterogeneous feature interaction module to capture intricate interactions between dependency syntax and attention semantics, and to dynamically select vital nodes. We leverage the synergy of these modules to harness the significant potential of syntactic and semantic information in ASTE tasks. Testing on public <b>benchmarks,</b> our D2E2S model surpasses the current state-of-the-art(SOTA), demonstrating its effectiveness.</p></p class="citation"></blockquote><h3 id=1736--17190-causal-graph-discovery-with-retrieval-augmented-generation-based-large-language-models-yuzhe-zhang-et-al-2024>(17/36 | 17/190) Causal Graph Discovery with Retrieval-Augmented Generation based Large Language Models (Yuzhe Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuzhe Zhang, Yipeng Zhang, Yidong Gan, Lina Yao, Chen Wang. (2024)<br><strong>Causal Graph Discovery with Retrieval-Augmented Generation based Large Language Models</strong><br><button class=copy-to-clipboard title="Causal Graph Discovery with Retrieval-Augmented Generation based Large Language Models" index=17>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-17 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL, stat-ME<br>Keyword Score: 53<br>Keywords: Graph, Retrieval-Augmented Generation, Retrieval-Augmented Generation, Retrieval-Augmented Generation, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15301v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15301v1.pdf filename=2402.15301v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Causal <b>graph</b> recovery is essential in the field of causal inference. Traditional methods are typically knowledge-based or statistical estimation-based, which are limited by data collection biases and individuals&rsquo; knowledge about factors affecting the relations between variables of interests. The advance of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> provides opportunities to address these problems. We propose a novel method that utilizes the extensive knowledge contained within a <b>large</b> <b>corpus</b> <b>of</b> scientific literature to deduce causal relationships in general causal <b>graph</b> recovery tasks. This method leverages <b>Retrieval</b> <b>Augmented-Generation</b> <b>(RAG)</b> based <b>LLMs</b> to systematically analyze and extract pertinent information from a comprehensive collection of research papers. Our method first retrieves relevant text chunks from the aggregated literature. Then, the <b>LLM</b> is tasked with identifying and labelling potential associations between factors. Finally, we give a method to aggregate the associational relationships to build a causal <b>graph.</b> We demonstrate our method is able to construct high quality causal <b>graphs</b> on the well-known SACHS dataset solely from literature.</p></p class="citation"></blockquote><h3 id=1836--18190-kieval-a-knowledge-grounded-interactive-evaluation-framework-for-large-language-models-zhuohao-yu-et-al-2024>(18/36 | 18/190) KIEval: A Knowledge-grounded Interactive Evaluation Framework for Large Language Models (Zhuohao Yu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhuohao Yu, Chang Gao, Wenjin Yao, Yidong Wang, Wei Ye, Jindong Wang, Xing Xie, Yue Zhang, Shikun Zhang. (2024)<br><strong>KIEval: A Knowledge-grounded Interactive Evaluation Framework for Large Language Models</strong><br><button class=copy-to-clipboard title="KIEval: A Knowledge-grounded Interactive Evaluation Framework for Large Language Models" index=18>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-18 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 53<br>Keywords: Automatic Evaluation, Benchmarking, Fine-tuning, Supervised Learning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15043v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15043v1.pdf filename=2402.15043v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Automatic</b> <b>evaluation</b> methods for <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> are hindered by data contamination, leading to inflated assessments of their effectiveness. Existing strategies, which aim to detect contaminated texts, focus on quantifying contamination status instead of accurately gauging model performance. In this paper, we introduce KIEval, a Knowledge-grounded Interactive Evaluation framework, which incorporates an <b>LLM-powered</b> &ldquo;interactor&rdquo; role for the first time to accomplish a dynamic contamination-resilient evaluation. Starting with a question in a conventional <b>LLM</b> <b>benchmark</b> involving domain-specific knowledge, KIEval utilizes dynamically generated, multi-round, and knowledge-focused dialogues to determine whether a model&rsquo;s response is merely a recall of <b>benchmark</b> answers or demonstrates a deep comprehension to apply knowledge in more complex conversations. Extensive experiments on seven leading <b>LLMs</b> across five datasets validate KIEval&rsquo;s effectiveness and generalization. We also reveal that data contamination brings no contribution or even negative effect to models&rsquo; real-world applicability and understanding, and existing contamination detection methods for <b>LLMs</b> can only identify contamination in pre-training but not during <b>supervised</b> <b>fine-tuning.</b></p></p class="citation"></blockquote><h3 id=1936--19190-leveraging-domain-knowledge-for-efficient-reward-modelling-in-rlhf-a-case-study-in-e-commerce-opinion-summarization-swaroop-nath-et-al-2024>(19/36 | 19/190) Leveraging Domain Knowledge for Efficient Reward Modelling in RLHF: A Case-Study in E-Commerce Opinion Summarization (Swaroop Nath et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Swaroop Nath, Tejpalsingh Siledar, Sankara Sri Raghava Ravindra Muddu, Rupasai Rangaraju, Harshad Khadilkar, Pushpak Bhattacharyya, Suman Banerjee, Amey Patil, Sudhanshu Shekhar Singh, Muthusamy Chelliah, Nikesh Garera. (2024)<br><strong>Leveraging Domain Knowledge for Efficient Reward Modelling in RLHF: A Case-Study in E-Commerce Opinion Summarization</strong><br><button class=copy-to-clipboard title="Leveraging Domain Knowledge for Efficient Reward Modelling in RLHF: A Case-Study in E-Commerce Opinion Summarization" index=19>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-19 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keyword Score: 50<br>Keywords: Reinforcement Learning, Reinforcement Learning from Human Feedback, Reinforcement Learning from Human Feedback, Opinion Summarization, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15473v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15473v1.pdf filename=2402.15473v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Reinforcement</b> <b>Learning</b> <b>from</b> <b>Human</b> <b>Feedback</b> <b>(RLHF)</b> has become a dominating strategy in steering Language Models (LMs) towards human values/goals. The key to the strategy is employing a reward model ({$\varphi$}) which can reflect a latent reward model with humans. While this strategy has proven to be effective, the training methodology requires a lot of human preference annotation (usually of the order of tens of thousands) to train {$\varphi$}. Such large-scale preference annotations can be achievable if the reward model can be ubiquitously used. However, human values/goals are subjective and depend on the nature of the task. This poses a challenge in collecting diverse preferences for downstream applications. To address this, we propose a novel methodology to infuse domain knowledge into {$\varphi$}, which reduces the size of preference annotation required. We validate our approach in E-Commerce <b>Opinion</b> <b>Summarization,</b> with a significant reduction in dataset size (just $940$ samples) while advancing the state-of-the-art. Our contributions include a novel Reward Modelling technique, a new dataset (PromptOpinSumm) for <b>Opinion</b> <b>Summarization,</b> and a human preference dataset (OpinPref). The proposed methodology opens avenues for efficient <b>RLHF,</b> making it more adaptable to diverse applications with varying human values. We release the artifacts for usage under MIT License.</p></p class="citation"></blockquote><h3 id=2036--20190-gpt-hatecheck-can-llms-write-better-functional-tests-for-hate-speech-detection-yiping-jin-et-al-2024>(20/36 | 20/190) GPT-HateCheck: Can LLMs Write Better Functional Tests for Hate Speech Detection? (Yiping Jin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yiping Jin, Leo Wanner, Alexander Shvets. (2024)<br><strong>GPT-HateCheck: Can LLMs Write Better Functional Tests for Hate Speech Detection?</strong><br><button class=copy-to-clipboard title="GPT-HateCheck: Can LLMs Write Better Functional Tests for Hate Speech Detection?" index=20>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-20 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-CY, cs.CL<br>Keyword Score: 50<br>Keywords: GPT, Natural Language Inference, Natural Language Inference, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15238v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15238v1.pdf filename=2402.15238v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Online hate detection suffers from biases incurred in data sampling, annotation, and model pre-training. Therefore, measuring the averaged performance over all examples in held-out test data is inadequate. Instead, we must identify specific model weaknesses and be informed when it is more likely to fail. A recent proposal in this direction is HateCheck, a suite for testing fine-grained model functionalities on synthesized data generated using templates of the kind &ldquo;You are just a [slur] to me.&rdquo; However, despite enabling more detailed diagnostic insights, the HateCheck test cases are often generic and have simplistic sentence structures that do not match the real-world data. To address this limitation, we propose <b>GPT-HateCheck,</b> a framework to generate more diverse and realistic functional tests from scratch by instructing <b>large</b> <b>language</b> <b>models</b> <b>(LLMs).</b> We employ an additional <b>natural</b> <b>language</b> <b>inference</b> <b>(NLI)</b> model to verify the generations. Crowd-sourced annotation demonstrates that the generated test cases are of high quality. Using the new functional tests, we can uncover model weaknesses that would be overlooked using the original HateCheck dataset.</p></p class="citation"></blockquote><h3 id=2136--21190-fine-grained-detoxification-via-instance-level-prefixes-for-large-language-models-xin-yi-et-al-2024>(21/36 | 21/190) Fine-Grained Detoxification via Instance-Level Prefixes for Large Language Models (Xin Yi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xin Yi, Linlin Wang, Xiaoling Wang, Liang He. (2024)<br><strong>Fine-Grained Detoxification via Instance-Level Prefixes for Large Language Models</strong><br><button class=copy-to-clipboard title="Fine-Grained Detoxification via Instance-Level Prefixes for Large Language Models" index=21>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-21 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: Fine-tuning, Text Generation, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15202v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15202v1.pdf filename=2402.15202v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Impressive results have been achieved in natural language processing (NLP) tasks through the training of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs).</b> However, these models occasionally produce toxic content such as insults, threats, and profanity in response to certain <b>prompts,</b> thereby constraining their practical utility. To tackle this issue, various <b>finetuning-based</b> and decoding-based approaches have been utilized to mitigate toxicity. However, these methods typically necessitate additional costs such as high-quality training data or auxiliary models. In this paper, we propose fine-grained detoxification via instance-level prefixes (FGDILP) to mitigate toxic <b>text</b> <b>without</b> additional cost. Specifically, FGDILP contrasts the contextualized representation in attention space using a positive prefix-prepended <b>prompt</b> against multiple negative prefix-prepended <b>prompts</b> at the instance level. This allows for constructing fine-grained subtoxicity vectors, which enables collaborative detoxification by fusing them to correct the normal generation process when provided with a raw <b>prompt.</b> We validate that FGDILP enables controlled <b>text</b> <b>generation</b> with regard to toxicity at both the utterance and context levels. Our method surpasses <b>prompt-based</b> baselines in detoxification, although at a slight cost to generation fluency and diversity.</p></p class="citation"></blockquote><h3 id=2236--22190-interpreting-context-look-ups-in-transformers-investigating-attention-mlp-interactions-clement-neo-et-al-2024>(22/36 | 22/190) Interpreting Context Look-ups in Transformers: Investigating Attention-MLP Interactions (Clement Neo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Clement Neo, Shay B. Cohen, Fazl Barez. (2024)<br><strong>Interpreting Context Look-ups in Transformers: Investigating Attention-MLP Interactions</strong><br><button class=copy-to-clipboard title="Interpreting Context Look-ups in Transformers: Investigating Attention-MLP Interactions" index=22>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-22 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 50<br>Keywords: GPT, GPT-4, Transformer, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15055v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15055v1.pdf filename=2402.15055v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we investigate the interplay between attention heads and specialized &ldquo;next-token&rdquo; neurons in the Multilayer Perceptron that predict specific tokens. By <b>prompting</b> an <b>LLM</b> like <b>GPT-4</b> to explain these model internals, we can elucidate attention mechanisms that activate certain next-token neurons. Our analysis identifies attention heads that recognize contexts relevant to predicting a particular token, activating the associated neuron through the residual connection. We focus specifically on heads in earlier layers consistently activating the same next-token neuron across similar <b>prompts.</b> Exploring these differential activation patterns reveals that heads that specialize for distinct linguistic contexts are tied to generating certain tokens. Overall, our method combines neural explanations and probing isolated components to illuminate how attention enables context-dependent, specialized processing in <b>LLMs.</b></p></p class="citation"></blockquote><h3 id=2336--23190-unlocking-the-power-of-large-language-models-for-entity-alignment-xuhui-jiang-et-al-2024>(23/36 | 23/190) Unlocking the Power of Large Language Models for Entity Alignment (Xuhui Jiang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xuhui Jiang, Yinghan Shen, Zhichao Shi, Chengjin Xu, Wei Li, Zixuan Li, Jian Guo, Huawei Shen, Yuanzhuo Wang. (2024)<br><strong>Unlocking the Power of Large Language Models for Entity Alignment</strong><br><button class=copy-to-clipboard title="Unlocking the Power of Large Language Models for Entity Alignment" index=23>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-23 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 48<br>Keywords: Graph, Knowledge Graph, Knowledge Graph, Representation Learning, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15048v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15048v1.pdf filename=2402.15048v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Entity Alignment (EA) is vital for integrating diverse <b>knowledge</b> <b>graph</b> <b>(KG)</b> data, playing a crucial role in data-driven AI applications. Traditional EA methods primarily rely on comparing entity embeddings, but their effectiveness is constrained by the limited input <b>KG</b> data and the capabilities of the <b>representation</b> <b>learning</b> techniques. Against this backdrop, we introduce ChatEA, an innovative framework that incorporates <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> to improve EA. To address the constraints of limited input <b>KG</b> data, ChatEA introduces a <b>KG-code</b> translation module that translates <b>KG</b> structures into a format understandable by <b>LLMs,</b> thereby allowing <b>LLMs</b> to utilize their extensive background <b>knowledge</b> <b>to</b> improve EA accuracy. To overcome the over-reliance on entity embedding comparisons, ChatEA implements a two-stage EA strategy that capitalizes on <b>LLMs&rsquo;</b> capability for multi-step <b>reasoning</b> in a dialogue format, thereby enhancing accuracy while preserving efficiency. Our experimental results affirm ChatEA&rsquo;s superior performance, highlighting <b>LLMs&rsquo;</b> potential in facilitating EA tasks.</p></p class="citation"></blockquote><h3 id=2436--24190-tombench-benchmarking-theory-of-mind-in-large-language-models-zhuang-chen-et-al-2024>(24/36 | 24/190) ToMBench: Benchmarking Theory of Mind in Large Language Models (Zhuang Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhuang Chen, Jincenzi Wu, Jinfeng Zhou, Bosi Wen, Guanqun Bi, Gongyao Jiang, Yaru Cao, Mengting Hu, Yunghwei Lai, Zexuan Xiong, Minlie Huang. (2024)<br><strong>ToMBench: Benchmarking Theory of Mind in Large Language Models</strong><br><button class=copy-to-clipboard title="ToMBench: Benchmarking Theory of Mind in Large Language Models" index=24>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-24 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 46<br>Keywords: Benchmarking, Benchmarking, GPT, GPT-4, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15052v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15052v1.pdf filename=2402.15052v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Theory of Mind (ToM) is the cognitive capability to perceive and ascribe mental states to oneself and others. Recent research has sparked a debate over whether <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> exhibit a form of ToM. However, existing ToM evaluations are hindered by challenges such as constrained scope, subjective judgment, and unintended contamination, yielding inadequate assessments. To address this gap, we introduce ToMBench with three key characteristics: a systematic evaluation framework encompassing 8 tasks and 31 abilities in social cognition, a multiple-choice question format to support automated and unbiased evaluation, and a build-from-scratch bilingual inventory to strictly avoid data leakage. Based on ToMBench, we conduct extensive experiments to evaluate the ToM performance of 10 popular <b>LLMs</b> across tasks and abilities. We find that even the most advanced <b>LLMs</b> like <b>GPT-4</b> lag behind human performance by over 10% points, indicating that <b>LLMs</b> have not achieved a human-level theory of mind yet. Our aim with ToMBench is to enable an efficient and effective evaluation of <b>LLMs&rsquo;</b> ToM capabilities, thereby facilitating the development of <b>LLMs</b> with inherent social intelligence.</p></p class="citation"></blockquote><h3 id=2536--25190-deem-dynamic-experienced-expert-modeling-for-stance-detection-xiaolong-wang-et-al-2024>(25/36 | 25/190) DEEM: Dynamic Experienced Expert Modeling for Stance Detection (Xiaolong Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiaolong Wang, Yile Wang, Sijie Cheng, Peng Li, Yang Liu. (2024)<br><strong>DEEM: Dynamic Experienced Expert Modeling for Stance Detection</strong><br><button class=copy-to-clipboard title="DEEM: Dynamic Experienced Expert Modeling for Stance Detection" index=25>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-25 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 43<br>Keywords: Benchmarking, Reasoning, Stance Detection, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15264v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15264v1.pdf filename=2402.15264v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent work has made a preliminary attempt to use <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> to solve the <b>stance</b> <b>detection</b> task, showing promising results. However, considering that <b>stance</b> <b>detection</b> usually requires detailed background knowledge, the vanilla <b>reasoning</b> method may neglect the domain knowledge to make a professional and accurate analysis. Thus, there is still room for improvement of <b>LLMs</b> <b>reasoning,</b> especially in leveraging the generation capability of <b>LLMs</b> to simulate specific experts (i.e., multi-agents) to detect the <b>stance.</b> <b>In</b> this paper, different from existing multi-agent works that require detailed descriptions and use fixed experts, we propose a Dynamic Experienced Expert Modeling (DEEM) method which can leverage the generated experienced experts and let <b>LLMs</b> reason in a semi-parametric way, making the experts more generalizable and reliable. Experimental results demonstrate that DEEM consistently achieves the best results on three standard <b>benchmarks,</b> outperforms methods with self-consistency <b>reasoning,</b> and reduces the bias of <b>LLMs.</b></p></p class="citation"></blockquote><h3 id=2636--26190-on-the-multi-turn-instruction-following-for-conversational-web-agents-yang-deng-et-al-2024>(26/36 | 26/190) On the Multi-turn Instruction Following for Conversational Web Agents (Yang Deng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yang Deng, Xuan Zhang, Wenxuan Zhang, Yifei Yuan, See-Kiong Ng, Tat-Seng Chua. (2024)<br><strong>On the Multi-turn Instruction Following for Conversational Web Agents</strong><br><button class=copy-to-clipboard title="On the Multi-turn Instruction Following for Conversational Web Agents" index=26>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-26 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 43<br>Keywords: Benchmarking, Instruction Following, Neural Machine Translation, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15057v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15057v1.pdf filename=2402.15057v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Web agents powered by <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have demonstrated remarkable abilities in planning and executing multi-step interactions within complex web-based environments, fulfilling a wide range of web navigation tasks. Despite these advancements, the potential for <b>LLM-powered</b> agents to effectively engage with sequential user <b>instructions</b> <b>in</b> real-world scenarios has not been fully explored. In this work, we introduce a new task of Conversational Web Navigation, which necessitates sophisticated interactions that span multiple turns with both the users and the environment, supported by a specially developed dataset named Multi-Turn Mind2Web <b>(MT-Mind2Web).</b> To tackle the limited context length of <b>LLMs</b> and the context-dependency issue of the conversational tasks, we further propose a novel framework, named self-reflective memory-augmented planning (Self-MAP), which employs memory utilization and self-reflection techniques. Extensive experiments are conducted to <b>benchmark</b> the <b>MT-Mind2Web</b> dataset, and validate the effectiveness of the proposed method.</p></p class="citation"></blockquote><h3 id=2736--27190-memoryprompt-a-light-wrapper-to-improve-context-tracking-in-pre-trained-language-models-nathanaël-carraz-rakotonirina-et-al-2024>(27/36 | 27/190) MemoryPrompt: A Light Wrapper to Improve Context Tracking in Pre-trained Language Models (Nathanaël Carraz Rakotonirina et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nathanaël Carraz Rakotonirina, Marco Baroni. (2024)<br><strong>MemoryPrompt: A Light Wrapper to Improve Context Tracking in Pre-trained Language Models</strong><br><button class=copy-to-clipboard title="MemoryPrompt: A Light Wrapper to Improve Context Tracking in Pre-trained Language Models" index=27>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-27 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 40<br>Keywords: Fine-tuning, Transformer, Pre-trained Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15268v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15268v1.pdf filename=2402.15268v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Transformer-based</b> language models (LMs) track contextual information through large, hard-coded input windows. We introduce MemoryPrompt, a leaner approach in which the LM is complemented by a small auxiliary recurrent network that passes information to the LM by prefixing its regular input with a sequence of vectors, akin to soft <b>prompts,</b> without requiring LM <b>finetuning.</b> Tested on a task designed to probe a LM&rsquo;s ability to keep track of multiple fact updates, a MemoryPrompt-augmented LM outperforms much larger LMs that have access to the full input history. We also test MemoryPrompt on a long-distance dialogue dataset, where its performance is comparable to that of a model conditioned on the entire conversation history. In both experiments we also observe that, unlike full-finetuning approaches, MemoryPrompt does not suffer from catastrophic forgetting when adapted to new tasks, thus not disrupting the generalist capabilities of the underlying LM.</p></p class="citation"></blockquote><h3 id=2836--28190-infusing-hierarchical-guidance-into-prompt-tuning-a-parameter-efficient-framework-for-multi-level-implicit-discourse-relation-recognition-haodong-zhao-et-al-2024>(28/36 | 28/190) Infusing Hierarchical Guidance into Prompt Tuning: A Parameter-Efficient Framework for Multi-level Implicit Discourse Relation Recognition (Haodong Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haodong Zhao, Ruifang He, Mengnan Xiao, Jing Xu. (2024)<br><strong>Infusing Hierarchical Guidance into Prompt Tuning: A Parameter-Efficient Framework for Multi-level Implicit Discourse Relation Recognition</strong><br><button class=copy-to-clipboard title="Infusing Hierarchical Guidance into Prompt Tuning: A Parameter-Efficient Framework for Multi-level Implicit Discourse Relation Recognition" index=28>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-28 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Fine-tuning, Relation Extraction, Pre-trained Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15080v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15080v1.pdf filename=2402.15080v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Multi-level implicit discourse <b>relation</b> <b>recognition</b> (MIDRR) aims at identifying hierarchical discourse <b>relations</b> <b>among</b> arguments. Previous methods achieve the promotion through <b>fine-tuning</b> <b>PLMs.</b> However, due to the data scarcity and the task gap, the pre-trained feature space cannot be accurately tuned to the task-specific space, which even aggravates the collapse of the vanilla space. Besides, the comprehension of hierarchical semantics for MIDRR makes the conversion much harder. In this paper, we propose a <b>prompt-based</b> Parameter-Efficient Multi-level IDRR (PEMI) framework to solve the above problems. First, we leverage parameter-efficient <b>prompt</b> tuning to drive the inputted arguments to match the pre-trained space and realize the approximation with few parameters. Furthermore, we propose a hierarchical label refining (HLR) method for the <b>prompt</b> verbalizer to deeply integrate hierarchical guidance into the <b>prompt</b> tuning. Finally, our model achieves comparable results on PDTB 2.0 and 3.0 using about 0.1% trainable parameters compared with baselines and the visualization demonstrates the effectiveness of our HLR method.</p></p class="citation"></blockquote><h3 id=2936--29190-api-blend-a-comprehensive-corpora-for-training-and-benchmarking-api-llms-kinjal-basu-et-al-2024>(29/36 | 29/190) API-BLEND: A Comprehensive Corpora for Training and Benchmarking API LLMs (Kinjal Basu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kinjal Basu, Ibrahim Abdelaziz, Subhajit Chaudhury, Soham Dan, Maxwell Crouse, Asim Munawar, Sadhana Kumaravel, Vinod Muthusamy, Pavan Kapanipathi, Luis A. Lastras. (2024)<br><strong>API-BLEND: A Comprehensive Corpora for Training and Benchmarking API LLMs</strong><br><button class=copy-to-clipboard title="API-BLEND: A Comprehensive Corpora for Training and Benchmarking API LLMs" index=29>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-29 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 36<br>Keywords: Benchmarking, Benchmarking, Slot Filling, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15491v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15491v1.pdf filename=2402.15491v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>There is a growing need for <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> to effectively use tools and external Application Programming Interfaces (APIs) to plan and complete tasks. As such, there is tremendous interest in methods that can acquire sufficient quantities of train and test data that involve calls to tools / APIs. Two lines of research have emerged as the predominant strategies for addressing this challenge. The first has focused on synthetic data generation techniques, while the second has involved curating task-adjacent datasets which can be transformed into API / Tool-based tasks. In this paper, we focus on the task of identifying, curating, and transforming existing datasets and, in turn, introduce API-BLEND, a <b>large</b> <b>corpora</b> <b>for</b> training and systematic testing of tool-augmented <b>LLMs.</b> The datasets mimic real-world scenarios involving API-tasks such as API / tool detection, <b>slot</b> <b>filling,</b> and sequencing of the detected APIs. We demonstrate the utility of the API-BLEND dataset for both training and <b>benchmarking</b> purposes.</p></p class="citation"></blockquote><h3 id=3036--30190-lets-rectify-step-by-step-improving-aspect-based-sentiment-analysis-with-diffusion-models-shunyu-liu-et-al-2024>(30/36 | 30/190) Let&rsquo;s Rectify Step by Step: Improving Aspect-based Sentiment Analysis with Diffusion Models (Shunyu Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shunyu Liu, Jie Zhou, Qunxi Zhu, Qin Chen, Qingchun Bai, Jun Xiao, Liang He. (2024)<br><strong>Let&rsquo;s Rectify Step by Step: Improving Aspect-based Sentiment Analysis with Diffusion Models</strong><br><button class=copy-to-clipboard title="Let's Rectify Step by Step: Improving Aspect-based Sentiment Analysis with Diffusion Models" index=30>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-30 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keyword Score: 33<br>Keywords: Diffusion Model, Benchmarking, Aspect-based Sentiment Analysis, Sentiment Analysis<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15289v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15289v1.pdf filename=2402.15289v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Aspect-Based</b> <b>Sentiment</b> <b>Analysis</b> (ABSA) stands as a crucial task in predicting the <b>sentiment</b> <b>polarity</b> associated with identified aspects within text. However, a notable challenge in ABSA lies in precisely determining the aspects&rsquo; boundaries (start and end indices), especially for long ones, due to users&rsquo; colloquial expressions. We propose DiffusionABSA, a novel <b>diffusion</b> <b>model</b> tailored for ABSA, which extracts the aspects progressively step by step. Particularly, DiffusionABSA gradually adds noise to the aspect terms in the training process, subsequently learning a denoising process that progressively restores these terms in a reverse manner. To estimate the boundaries, we design a denoising neural network enhanced by a syntax-aware temporal attention mechanism to chronologically capture the interplay between aspects and surrounding text. Empirical evaluations conducted on eight <b>benchmark</b> datasets underscore the compelling advantages offered by DiffusionABSA when compared against robust baseline models. Our code is publicly available at <a href=https://github.com/Qlb6x/DiffusionABSA>https://github.com/Qlb6x/DiffusionABSA</a>.</p></p class="citation"></blockquote><h3 id=3136--31190-machine-unlearning-of-pre-trained-large-language-models-jin-yao-et-al-2024>(31/36 | 31/190) Machine Unlearning of Pre-trained Large Language Models (Jin Yao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jin Yao, Eli Chien, Minxin Du, Xinyao Niu, Tianhao Wang, Zezhou Cheng, Xiang Yue. (2024)<br><strong>Machine Unlearning of Pre-trained Large Language Models</strong><br><button class=copy-to-clipboard title="Machine Unlearning of Pre-trained Large Language Models" index=31>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-31 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-CR, cs-LG, cs.CL<br>Keyword Score: 33<br>Keywords: Benchmarking, Machine Unlearning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15159v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15159v1.pdf filename=2402.15159v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This study investigates the concept of the `right to be forgotten&rsquo; within the context of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs).</b> We explore <b>machine</b> <b>unlearning</b> as a pivotal solution, with a focus on pre-trained models&ndash;a notably under-researched area. Our research delineates a comprehensive framework for <b>machine</b> <b>unlearning</b> in pre-trained <b>LLMs,</b> encompassing a critical analysis of seven diverse unlearning methods. Through rigorous evaluation using curated datasets from arXiv, books, and GitHub, we establish a robust <b>benchmark</b> for unlearning performance, demonstrating that these methods are over $10^5$ times more computationally efficient than retraining. Our results show that integrating gradient ascent with gradient descent on in-distribution data improves hyperparameter robustness. We also provide detailed guidelines for efficient hyperparameter tuning in the unlearning process. Our findings advance the discourse on ethical AI practices, offering substantive insights into the mechanics of <b>machine</b> <b>unlearning</b> for pre-trained <b>LLMs</b> and underscoring the potential for responsible AI development.</p></p class="citation"></blockquote><h3 id=3236--32190-carbd-ko-a-contextually-annotated-review-benchmark-dataset-for-aspect-level-sentiment-classification-in-korean-dongjun-jang-et-al-2024>(32/36 | 32/190) CARBD-Ko: A Contextually Annotated Review Benchmark Dataset for Aspect-Level Sentiment Classification in Korean (Dongjun Jang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dongjun Jang, Jean Seo, Sungjoo Byun, Taekyoung Kim, Minseok Kim, Hyopil Shin. (2024)<br><strong>CARBD-Ko: A Contextually Annotated Review Benchmark Dataset for Aspect-Level Sentiment Classification in Korean</strong><br><button class=copy-to-clipboard title="CARBD-Ko: A Contextually Annotated Review Benchmark Dataset for Aspect-Level Sentiment Classification in Korean" index=32>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-32 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 33<br>Keywords: Benchmarking, Sentiment Analysis, Pre-trained Language Model, Pre-trained Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15046v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15046v1.pdf filename=2402.15046v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper explores the challenges posed by aspect-based <b>sentiment</b> <b>classification</b> (ABSC) within <b>pretrained</b> <b>language</b> <b>models</b> <b>(PLMs),</b> with a particular focus on contextualization and hallucination issues. In order to tackle these challenges, we introduce CARBD-Ko (a Contextually Annotated Review <b>Benchmark</b> Dataset for Aspect-Based <b>Sentiment</b> <b>Classification</b> in Korean), a <b>benchmark</b> dataset that incorporates aspects and dual-tagged polarities to distinguish between aspect-specific and aspect-agnostic <b>sentiment</b> <b>classification.</b> The dataset consists of sentences annotated with specific aspects, aspect polarity, aspect-agnostic polarity, and the intensity of aspects. To address the issue of dual-tagged aspect polarities, we propose a novel approach employing a Siamese Network. Our experimental findings highlight the inherent difficulties in accurately predicting dual-polarities and underscore the significance of contextualized <b>sentiment</b> <b>analysis</b> models. The CARBD-Ko dataset serves as a valuable resource for future research endeavors in aspect-level <b>sentiment</b> <b>classification.</b></p></p class="citation"></blockquote><h3 id=3336--33190-prejudice-and-caprice-a-statistical-framework-for-measuring-social-discrimination-in-large-language-models-yiran-liu-et-al-2024>(33/36 | 33/190) Prejudice and Caprice: A Statistical Framework for Measuring Social Discrimination in Large Language Models (Yiran Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yiran Liu, Ke Yang, Zehan Qi, Xiao Liu, Yang Yu, Chengxiang Zhai. (2024)<br><strong>Prejudice and Caprice: A Statistical Framework for Measuring Social Discrimination in Large Language Models</strong><br><button class=copy-to-clipboard title="Prejudice and Caprice: A Statistical Framework for Measuring Social Discrimination in Large Language Models" index=33>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-33 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-CY, cs.CL<br>Keyword Score: 30<br>Keywords: Stemming, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15481v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15481v1.pdf filename=2402.15481v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The growing integration of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> into social operations amplifies their impact on decisions in crucial areas such as economics, law, education, and healthcare, raising public concerns about these models&rsquo; discrimination-related safety and reliability. However, prior discrimination measuring frameworks solely assess the average discriminatory behavior of <b>LLMs,</b> often proving inadequate due to the overlook of an additional discrimination-leading factor, i.e., the <b>LLMs&rsquo;</b> prediction variation across diverse contexts. In this work, we present the Prejudice-Caprice Framework (PCF) that comprehensively measures discrimination in <b>LLMs</b> by considering both their consistently biased preference and preference variation across diverse contexts. Specifically, we mathematically dissect the aggregated contextualized discrimination risk of <b>LLMs</b> into prejudice risk, originating from <b>LLMs&rsquo;</b> persistent prejudice, and caprice risk, <b>stemming</b> from their generation inconsistency. In addition, we utilize a data-mining approach to gather preference-detecting probes from sentence skeletons, devoid of attribute indications, to approximate <b>LLMs&rsquo;</b> applied contexts. While initially intended for assessing discrimination in <b>LLMs,</b> our proposed PCF facilitates the comprehensive and flexible measurement of any inductive biases, including knowledge alongside prejudice, across various modality models. We apply our discrimination-measuring framework to 12 common <b>LLMs,</b> yielding intriguing findings: i) modern <b>LLMs</b> demonstrate significant pro-male stereotypes, ii) <b>LLMs&rsquo;</b> exhibited discrimination correlates with several social and economic factors, iii) prejudice risk dominates the overall discrimination risk and follows a normal distribution, and iv) caprice risk contributes minimally to the overall risk but follows a fat-tailed distribution, suggesting that it is wild risk requiring enhanced surveillance.</p></p class="citation"></blockquote><h3 id=3436--34190-chitchat-as-interference-adding-user-backstories-to-task-oriented-dialogues-armand-stricker-et-al-2024>(34/36 | 34/190) Chitchat as Interference: Adding User Backstories to Task-Oriented Dialogues (Armand Stricker et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Armand Stricker, Patrick Paroubek. (2024)<br><strong>Chitchat as Interference: Adding User Backstories to Task-Oriented Dialogues</strong><br><button class=copy-to-clipboard title="Chitchat as Interference: Adding User Backstories to Task-Oriented Dialogues" index=34>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-34 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Few-shot, LLaMA, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15248v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15248v1.pdf filename=2402.15248v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>During task-oriented dialogues (TODs), human users naturally introduce chitchat that is beyond the immediate scope of the task, interfering with the flow of the conversation. To address this issue without the need for expensive manual data creation, we use <b>few-shot</b> <b>prompting</b> with <b>Llama-2-70B</b> to enhance the MultiWOZ dataset with user backstories, a typical example of chitchat interference in TODs. We assess the impact of this addition by testing two models: one trained solely on TODs and another trained on TODs with a preliminary chitchat interaction. Our analysis reveals that our enriched dataset poses a significant challenge to these systems. Moreover, we demonstrate that our dataset can be effectively used for training purposes, enabling a system to consistently acknowledge the user&rsquo;s backstory while also successfully moving the task forward in the same turn, as confirmed by human evaluation. These findings highlight the benefits of generating novel chitchat-TOD scenarios to test TOD systems more thoroughly and improve their resilience to natural user interferences.</p></p class="citation"></blockquote><h3 id=3536--35190-gotcha-dont-trick-me-with-unanswerable-questions-self-aligning-large-language-models-for-responding-to-unknown-questions-yang-deng-et-al-2024>(35/36 | 35/190) Gotcha! Don&rsquo;t trick me with unanswerable questions! Self-aligning Large Language Models for Responding to Unknown Questions (Yang Deng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yang Deng, Yong Zhao, Moxin Li, See-Kiong Ng, Tat-Seng Chua. (2024)<br><strong>Gotcha! Don&rsquo;t trick me with unanswerable questions! Self-aligning Large Language Models for Responding to Unknown Questions</strong><br><button class=copy-to-clipboard title="Gotcha! Don't trick me with unanswerable questions! Self-aligning Large Language Models for Responding to Unknown Questions" index=35>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-35 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keyword Score: 30<br>Keywords: Fine-tuning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15062v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15062v1.pdf filename=2402.15062v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Despite the remarkable abilities of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> to answer questions, they often display a considerable level of overconfidence even when the question does not have a definitive answer. To avoid providing hallucinated answers to these unknown questions, existing studies typically investigate approaches to refusing to answer these questions. In this work, we propose a novel and scalable self-alignment method to utilize the <b>LLM</b> itself to enhance its response-ability to different types of unknown questions, being capable of not only refusing to answer but also providing explanation to the unanswerability of unknown questions. Specifically, the Self-Align method first employ a two-stage class-aware self-augmentation approach to generate a <b>large</b> <b>amount</b> <b>of</b> unknown question-response data. Then we conduct disparity-driven self-curation to select qualified data for <b>fine-tuning</b> the <b>LLM</b> itself for aligning the responses to unknown questions as desired. Experimental results on two datasets across four types of unknown questions validate the superiority of the Self-Align method over existing baselines in terms of three types of task formulation.</p></p class="citation"></blockquote><h3 id=3636--36190-biomedical-entity-linking-as-multiple-choice-question-answering-zhenxi-lin-et-al-2024>(36/36 | 36/190) Biomedical Entity Linking as Multiple Choice Question Answering (Zhenxi Lin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhenxi Lin, Ziheng Zhang, Xian Wu, Yefeng Zheng. (2024)<br><strong>Biomedical Entity Linking as Multiple Choice Question Answering</strong><br><button class=copy-to-clipboard title="Biomedical Entity Linking as Multiple Choice Question Answering" index=36>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-36 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 20<br>Keywords: Question Answering, Pre-trained Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15189v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15189v1.pdf filename=2402.15189v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Although biomedical entity linking (BioEL) has made significant progress with <b>pre-trained</b> <b>language</b> <b>models,</b> challenges still exist for fine-grained and long-tailed entities. To address these challenges, we present BioELQA, a novel model that treats Biomedical Entity Linking as Multiple Choice <b>Question</b> <b>Answering.</b> BioELQA first obtains candidate entities with a fast retriever, jointly presents the mention and candidate entities to a generator, and then outputs the predicted symbol associated with its chosen entity. This formulation enables explicit comparison of different candidate entities, thus capturing fine-grained interactions between mentions and entities, as well as among entities themselves. To improve generalization for long-tailed entities, we retrieve similar labeled training instances as clues and concatenate the input with retrieved instances for the generator. Extensive experimental results show that BioELQA outperforms state-of-the-art baselines on several datasets.</p></p class="citation"></blockquote><h2 id=cslg-56>cs.LG (56)</h2><h3 id=156--37190-graphedit-large-language-models-for-graph-structure-learning-zirui-guo-et-al-2024>(1/56 | 37/190) GraphEdit: Large Language Models for Graph Structure Learning (Zirui Guo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zirui Guo, Lianghao Xia, Yanhua Yu, Yuling Wang, Zixuan Yang, Wei Wei, Liang Pang, Tat-Seng Chua, Chao Huang. (2024)<br><strong>GraphEdit: Large Language Models for Graph Structure Learning</strong><br><button class=copy-to-clipboard title="GraphEdit: Large Language Models for Graph Structure Learning" index=37>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-37 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 76<br>Keywords: Message-Passing, Graph, Graph Neural Network, Graph Neural Network, Benchmarking, Reasoning, Instruction Tuning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15183v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15183v1.pdf filename=2402.15183v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Graph</b> <b>Structure</b> <b>Learning</b> (GSL) focuses on capturing intrinsic dependencies and interactions among nodes in <b>graph-structured</b> <b>data</b> <b>by</b> generating novel <b>graph</b> <b>structures.</b> <b>Graph</b> <b>Neural</b> <b>Networks</b> <b>(GNNs)</b> have emerged as promising GSL solutions, utilizing recursive message passing to encode node-wise inter-dependencies. However, many existing GSL methods heavily depend on explicit <b>graph</b> <b>structural</b> <b>information</b> as supervision signals, leaving them susceptible to challenges such as data noise and sparsity. In this work, we propose GraphEdit, an approach that leverages <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> to learn complex node relationships in <b>graph-structured</b> <b>data.</b> <b>By</b> enhancing the <b>reasoning</b> capabilities of <b>LLMs</b> through <b>instruction-tuning</b> <b>over</b> <b>graph</b> <b>structures,</b> <b>we</b> aim to overcome the limitations associated with explicit <b>graph</b> <b>structural</b> <b>information</b> and enhance the reliability of <b>graph</b> <b>structure</b> <b>learning.</b> Our approach not only effectively denoises noisy connections but also identifies node-wise dependencies from a global perspective, providing a comprehensive understanding of the <b>graph</b> <b>structure.</b> <b>We</b> conduct extensive experiments on multiple <b>benchmark</b> datasets to demonstrate the effectiveness and robustness of GraphEdit across various settings. We have made our model implementation available at: <a href=https://github.com/HKUDS/GraphEdit>https://github.com/HKUDS/GraphEdit</a>.</p></p class="citation"></blockquote><h3 id=256--38190-sampling-based-distributed-training-with-message-passing-neural-network-priyesh-kakka-et-al-2024>(2/56 | 38/190) Sampling-based Distributed Training with Message Passing Neural Network (Priyesh Kakka et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Priyesh Kakka, Sheel Nidhan, Rishikesh Ranade, Jonathan F. MacArt. (2024)<br><strong>Sampling-based Distributed Training with Message Passing Neural Network</strong><br><button class=copy-to-clipboard title="Sampling-based Distributed Training with Message Passing Neural Network" index=38>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-38 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-DC, cs-LG, cs.LG, physics-flu-dyn<br>Keyword Score: 73<br>Keywords: Graph Convolutional Network, Message-Passing, Graph, Graph Neural Network, Convolution, Convolutional Neural Network, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15106v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15106v1.pdf filename=2402.15106v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this study, we introduce a domain-decomposition-based distributed training and inference approach for <b>message-passing</b> neural networks (MPNN). Our objective is to address the challenge of scaling edge-based <b>graph</b> <b>neural</b> <b>networks</b> as the number of nodes increases. Through our distributed training approach, coupled with Nystr"om-approximation sampling techniques, we present a scalable <b>graph</b> <b>neural</b> <b>network,</b> referred to as DS-MPNN (D and S standing for distributed and sampled, respectively), capable of scaling up to $O(10^5)$ nodes. We validate our sampling and distributed training approach on two cases: (a) a Darcy flow dataset and (b) steady RANS <b>simulations</b> of 2-D airfoils, providing comparisons with both single-GPU implementation and node-based <b>graph</b> <b>convolution</b> <b>networks</b> <b>(GCNs).</b> The DS-MPNN model demonstrates comparable accuracy to single-GPU implementation, can accommodate a significantly larger number of nodes compared to the single-GPU variant (S-MPNN), and significantly outperforms the node-based <b>GCN.</b></p></p class="citation"></blockquote><h3 id=356--39190-advancing-parameter-efficiency-in-fine-tuning-via-representation-editing-muling-wu-et-al-2024>(3/56 | 39/190) Advancing Parameter Efficiency in Fine-tuning via Representation Editing (Muling Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Muling Wu, Wenhao Liu, Xiaohua Wang, Tianlong Li, Changze Lv, Zixuan Ling, Jianhao Zhu, Cenyuan Zhang, Xiaoqing Zheng, Xuanjing Huang. (2024)<br><strong>Advancing Parameter Efficiency in Fine-tuning via Representation Editing</strong><br><button class=copy-to-clipboard title="Advancing Parameter Efficiency in Fine-tuning via Representation Editing" index=39>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-39 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CL, cs-LG, cs.LG<br>Keyword Score: 70<br>Keywords: Fine-tuning, GPT, GPT-2, LLaMA, RoBERTa, T5, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15179v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15179v1.pdf filename=2402.15179v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Parameter Efficient <b>Fine-Tuning</b> (PEFT) has gained significant attention for its ability to achieve competitive results while updating only a small subset of trainable parameters. Despite the promising performance of current PEFT methods, they present challenges in hyperparameter selection, such as determining the rank of LoRA or Adapter, or specifying the length of soft <b>prompts.</b> In addressing these challenges, we propose a novel approach to <b>fine-tuning</b> neural models, termed Representation EDiting (RED), which scales and biases the representation produced at each layer. RED substantially reduces the number of trainable parameters by a factor of $25,700$ compared to full parameter <b>fine-tuning,</b> and by a factor of $32$ compared to LoRA. Remarkably, RED achieves comparable or superior results to full parameter <b>fine-tuning</b> and other PEFT methods. Extensive experiments were conducted across models of varying architectures and scales, including <b>RoBERTa,</b> <b>GPT-2,</b> <b>T5,</b> and <b>Llama-2,</b> and the results demonstrate the efficiency and efficacy of RED, positioning it as a promising PEFT approach for large neural models.</p></p class="citation"></blockquote><h3 id=456--40190-mechanics-informed-autoencoder-enables-automated-detection-and-localization-of-unforeseen-structural-damage-xuyang-li-et-al-2024>(4/56 | 40/190) Mechanics-Informed Autoencoder Enables Automated Detection and Localization of Unforeseen Structural Damage (Xuyang Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xuyang Li, Hamed Bolandi, Mahdi Masmoudi, Talal Salem, Nizar Lajnef, Vishnu Naresh Boddeti. (2024)<br><strong>Mechanics-Informed Autoencoder Enables Automated Detection and Localization of Unforeseen Structural Damage</strong><br><button class=copy-to-clipboard title="Mechanics-Informed Autoencoder Enables Automated Detection and Localization of Unforeseen Structural Damage" index=40>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-40 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, eess-SP<br>Keyword Score: 50<br>Keywords: Autoencoder, Human Intervention, Simulation, Simulator, Variational Autoencoder<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15492v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15492v1.pdf filename=2402.15492v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Structural health monitoring (SHM) is vital for ensuring the safety and longevity of structures like buildings and bridges. As the volume and scale of structures and the impact of their failure continue to grow, there is a dire need for SHM techniques that are scalable, inexpensive, operate passively without <b>human</b> <b>intervention,</b> and customized for each mechanical structure without the need for complex baseline models. We present a novel &ldquo;deploy-and-forget&rdquo; approach for automated detection and localization of damages in structures. It is based on a synergistic combination of fully passive measurements from inexpensive sensors and a mechanics-informed <b>autoencoder.</b> Once deployed, our solution continuously learns and adapts a bespoke baseline model for each structure, learning from its undamaged state&rsquo;s response characteristics. After learning from just 3 hours of data, it can autonomously detect and localize different types of unforeseen damage. Results from numerical <b>simulations</b> and experiments indicate that incorporating the mechanical characteristics into the <b>variational</b> <b>autoencoder</b> allows for up to 35% earlier detection and localization of damage over a standard <b>autoencoder.</b> Our approach holds substantial promise for a significant reduction in <b>human</b> <b>intervention</b> and inspection costs and enables proactive and preventive maintenance strategies, thus extending the lifespan, reliability, and sustainability of civil infrastructures.</p></p class="citation"></blockquote><h3 id=556--41190-a-comprehensive-survey-of-convolutions-in-deep-learning-applications-challenges-and-future-trends-abolfazl-younesi-et-al-2024>(5/56 | 41/190) A Comprehensive Survey of Convolutions in Deep Learning: Applications, Challenges, and Future Trends (Abolfazl Younesi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Abolfazl Younesi, Mohsen Ansari, MohammadAmin Fazli, Alireza Ejlali, Muhammad Shafique, Jörg Henkel. (2024)<br><strong>A Comprehensive Survey of Convolutions in Deep Learning: Applications, Challenges, and Future Trends</strong><br><button class=copy-to-clipboard title="A Comprehensive Survey of Convolutions in Deep Learning: Applications, Challenges, and Future Trends" index=41>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-41 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs-NE, cs.LG<br>Keyword Score: 50<br>Keywords: Object Detection, Convolution, Convolutional Neural Network, Convolutional Neural Network, Meta Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15490v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15490v1.pdf filename=2402.15490v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In today&rsquo;s digital age, <b>Convolutional</b> <b>Neural</b> <b>Networks</b> <b>(CNNs),</b> a subset of Deep Learning (DL), are widely used for various computer vision tasks such as image classification, <b>object</b> <b>detection,</b> and image segmentation. There are numerous types of <b>CNNs</b> designed to meet specific needs and requirements, including 1D, 2D, and 3D <b>CNNs,</b> as well as dilated, grouped, attention, depthwise <b>convolutions,</b> and NAS, among others. Each type of <b>CNN</b> has its unique structure and characteristics, making it suitable for specific tasks. It&rsquo;s crucial to gain a thorough understanding and perform a comparative analysis of these different <b>CNN</b> types to understand their strengths and weaknesses. Furthermore, studying the performance, limitations, and practical applications of each type of <b>CNN</b> can aid in the development of new and improved architectures in the future. We also dive into the platforms and frameworks that researchers utilize for their research or development from various perspectives. Additionally, we explore the main research fields of <b>CNN</b> like 6D vision, generative models, and <b>meta-learning.</b> <b>This</b> survey paper provides a comprehensive examination and comparison of various <b>CNN</b> architectures, highlighting their architectural differences and emphasizing their respective advantages, disadvantages, applications, challenges, and future trends.</p></p class="citation"></blockquote><h3 id=656--42190-debiasing-machine-learning-models-by-using-weakly-supervised-learning-renan-d-b-brotto-et-al-2024>(6/56 | 42/190) Debiasing Machine Learning Models by Using Weakly Supervised Learning (Renan D. B. Brotto et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Renan D. B. Brotto, Jean-Michel Loubes, Laurent Risser, Jean-Pierre Florens, Kenji Nose-Filho, João M. T. Romano. (2024)<br><strong>Debiasing Machine Learning Models by Using Weakly Supervised Learning</strong><br><button class=copy-to-clipboard title="Debiasing Machine Learning Models by Using Weakly Supervised Learning" index=42>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-42 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: 68T05, cs-CY, cs-LG, cs.LG<br>Keyword Score: 50<br>Keywords: Supervised Learning, Supervised Learning, Weakly-supervised Learning, Weakly-supervised Learning, Weakly Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15477v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15477v1.pdf filename=2402.15477v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We tackle the problem of bias mitigation of algorithmic decisions in a setting where both the output of the algorithm and the sensitive variable are continuous. Most of prior work deals with discrete sensitive variables, meaning that the biases are measured for subgroups of persons defined by a label, leaving out important algorithmic bias cases, where the sensitive variable is continuous. Typical examples are unfair decisions made with respect to the age or the financial status. In our work, we then propose a bias mitigation strategy for continuous sensitive variables, based on the notion of endogeneity which comes from the field of econometrics. In addition to solve this new problem, our bias mitigation strategy is a <b>weakly</b> <b>supervised</b> <b>learning</b> method which requires that a small portion of the data can be measured in a fair manner. It is model agnostic, in the sense that it does not make any hypothesis on the prediction model. It also makes use of a reasonably large amount of input observations and their corresponding predictions. Only a small fraction of the true output predictions should be known. This therefore limits the need for expert interventions. Results obtained on synthetic data show the effectiveness of our approach for examples as close as possible to real-life applications in econometrics.</p></p class="citation"></blockquote><h3 id=756--43190-gptvq-the-blessing-of-dimensionality-for-llm-quantization-mart-van-baalen-et-al-2024>(7/56 | 43/190) GPTVQ: The Blessing of Dimensionality for LLM Quantization (Mart van Baalen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mart van Baalen, Andrey Kuzmin, Markus Nagel, Peter Couperus, Cedric Bastoul, Eric Mahurin, Tijmen Blankevoort, Paul Whatmough. (2024)<br><strong>GPTVQ: The Blessing of Dimensionality for LLM Quantization</strong><br><button class=copy-to-clipboard title="GPTVQ: The Blessing of Dimensionality for LLM Quantization" index=43>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-43 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CL, cs-LG, cs.LG<br>Keyword Score: 50<br>Keywords: Quantization, LLaMA, Mistral, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15319v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15319v1.pdf filename=2402.15319v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this work we show that the size versus accuracy trade-off of neural network <b>quantization</b> can be significantly improved by increasing the <b>quantization</b> dimensionality. We propose the GPTVQ method, a new fast method for post-training vector <b>quantization</b> (VQ) that scales well to <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs).</b> Our method interleaves <b>quantization</b> of one or more columns with updates to the remaining unquantized weights, using information from the Hessian of the per-layer output reconstruction MSE. <b>Quantization</b> codebooks are initialized using an efficient data-aware version of the EM algorithm. The codebooks are then updated, and further compressed by using integer <b>quantization</b> and SVD-based compression. GPTVQ establishes a new state-of-the art in the size vs accuracy trade-offs on a wide range of <b>LLMs</b> such as <b>Llama-v2</b> and <b>Mistral.</b> Furthermore, our method is efficient: on a single H100 it takes between 3 and 11 hours to process a Llamav2-70B model, depending on <b>quantization</b> setting. Lastly, with on-device timings for VQ decompression on a mobile CPU we show that VQ leads to improved latency compared to using a 4-bit integer format.</p></p class="citation"></blockquote><h3 id=856--44190-counterfactual-generation-with-identifiability-guarantees-hanqi-yan-et-al-2024>(8/56 | 44/190) Counterfactual Generation with Identifiability Guarantees (Hanqi Yan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hanqi Yan, Lingjing Kong, Lin Gui, Yuejie Chi, Eric Xing, Yulan He, Kun Zhang. (2024)<br><strong>Counterfactual Generation with Identifiability Guarantees</strong><br><button class=copy-to-clipboard title="Counterfactual Generation with Identifiability Guarantees" index=44>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-44 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CL, cs-LG, cs.LG<br>Keyword Score: 50<br>Keywords: Counter-factual, Unsupervised Learning, Image2text, Style Transfer, Text Generation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15309v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15309v1.pdf filename=2402.15309v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Counterfactual</b> generation lies at the core of various machine learning tasks, including <b>image</b> <b>translation</b> and controllable <b>text</b> <b>generation.</b> This generation process usually requires the identification of the disentangled latent representations, such as content and <b>style,</b> <b>that</b> underlie the observed data. However, it becomes more challenging when faced with a scarcity of paired data and labeling information. Existing disentangled methods crucially rely on oversimplified assumptions, such as assuming independent content and <b>style</b> <b>variables,</b> to identify the latent variables, even though such assumptions may not hold for complex data distributions. For instance, food reviews tend to involve words like tasty, whereas movie reviews commonly contain words such as thrilling for the same positive sentiment. This problem is exacerbated when data are sampled from multiple domains since the dependence between content and <b>style</b> <b>may</b> vary significantly over domains. In this work, we tackle the domain-varying dependence between the content and the <b>style</b> <b>variables</b> inherent in the <b>counterfactual</b> generation task. We provide identification guarantees for such latent-variable models by leveraging the relative sparsity of the influences from different latent variables. Our theoretical insights enable the development of a doMain AdapTive <b>counTerfactual</b> gEneration model, called (MATTE). Our theoretically grounded framework achieves state-of-the-art performance in <b>unsupervised</b> <b>style</b> <b>transfer</b> tasks, where neither paired data nor <b>style</b> <b>labels</b> are utilized, across four large-scale datasets. Code is available at <a href=https://github.com/hanqi-qi/Matte.git>https://github.com/hanqi-qi/Matte.git</a></p></p class="citation"></blockquote><h3 id=956--45190-smoothed-graph-contrastive-learning-via-seamless-proximity-integration-maysam-behmanesh-et-al-2024>(9/56 | 45/190) Smoothed Graph Contrastive Learning via Seamless Proximity Integration (Maysam Behmanesh et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Maysam Behmanesh, Maks Ovsjanikov. (2024)<br><strong>Smoothed Graph Contrastive Learning via Seamless Proximity Integration</strong><br><button class=copy-to-clipboard title="Smoothed Graph Contrastive Learning via Seamless Proximity Integration" index=45>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-45 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 46<br>Keywords: Graph, Graph Contrastive Learning, Graph Contrastive Learning, Benchmarking, Contrastive Learning, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15270v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15270v1.pdf filename=2402.15270v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Graph</b> <b>contrastive</b> <b>learning</b> <b>(GCL)</b> aligns node representations by classifying node pairs into positives and negatives using a selection process that typically relies on establishing correspondences within two augmented <b>graphs.</b> <b>The</b> <b>conventional</b> <b>GCL</b> approaches incorporate negative samples uniformly in the <b>contrastive</b> <b>loss,</b> resulting in the equal treatment negative nodes, regardless of their proximity to the true positive. In this paper, we present a Smoothed <b>Graph</b> <b>Contrastive</b> <b>Learning</b> model (SGCL), which leverages the geometric structure of augmented <b>graphs</b> <b>to</b> <b>inject</b> proximity information associated with positive/negative pairs in the <b>contrastive</b> <b>loss,</b> thus significantly regularizing the learning process. The proposed SGCL adjusts the penalties associated with node pairs in the <b>contrastive</b> <b>loss</b> by incorporating three distinct smoothing techniques that result in proximity aware positives and negatives. To enhance scalability for large-scale <b>graphs,</b> <b>the</b> <b>proposed</b> framework incorporates a <b>graph</b> <b>batch-generating</b> <b>strategy</b> that partitions the given <b>graphs</b> <b>into</b> <b>multiple</b> subgraphs, facilitating efficient training in separate batches. Through extensive experimentation in the <b>unsupervised</b> setting on various <b>benchmarks,</b> particularly those of large scale, we demonstrate the superiority of our proposed framework against recent baselines.</p></p class="citation"></blockquote><h3 id=1056--46190-united-we-pretrain-divided-we-fail-representation-learning-for-time-series-by-pretraining-on-75-datasets-at-once-maurice-kraus-et-al-2024>(10/56 | 46/190) United We Pretrain, Divided We Fail! Representation Learning for Time Series by Pretraining on 75 Datasets at Once (Maurice Kraus et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Maurice Kraus, Felix Divo, David Steinmann, Devendra Singh Dhami, Kristian Kersting. (2024)<br><strong>United We Pretrain, Divided We Fail! Representation Learning for Time Series by Pretraining on 75 Datasets at Once</strong><br><button class=copy-to-clipboard title="United We Pretrain, Divided We Fail! Representation Learning for Time Series by Pretraining on 75 Datasets at Once" index=46>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-46 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 45<br>Keywords: Fine-tuning, Representation Learning, Self-supervised Learning, Self-supervised Pre-training, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15404v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15404v1.pdf filename=2402.15404v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In natural language processing and vision, pretraining is utilized to learn effective <b>representations.</b> <b>Unfortunately,</b> the success of pretraining does not easily carry over to time series due to potential mismatch between sources and target. Actually, common belief is that multi-dataset pretraining does not work for time series! Au contraire, we introduce a new <b>self-supervised</b> <b>contrastive</b> pretraining approach to learn one encoding from many unlabeled and diverse time series datasets, so that the single learned <b>representation</b> <b>can</b> then be reused in several target domains for, say, classification. Specifically, we propose the XD-MixUp interpolation method and the Soft Interpolation Contextual Contrasting (SICC) loss. Empirically, this outperforms both <b>supervised</b> training and other <b>self-supervised</b> <b>pretraining</b> methods when <b>finetuning</b> on low-data regimes. This disproves the common belief: We can actually learn from multiple time series datasets, even from 75 at once.</p></p class="citation"></blockquote><h3 id=1156--47190-fine-tuning-of-continuous-time-diffusion-models-as-entropy-regularized-control-masatoshi-uehara-et-al-2024>(11/56 | 47/190) Fine-Tuning of Continuous-Time Diffusion Models as Entropy-Regularized Control (Masatoshi Uehara et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Masatoshi Uehara, Yulai Zhao, Kevin Black, Ehsan Hajiramezanali, Gabriele Scalia, Nathaniel Lee Diamant, Alex M Tseng, Tommaso Biancalani, Sergey Levine. (2024)<br><strong>Fine-Tuning of Continuous-Time Diffusion Models as Entropy-Regularized Control</strong><br><button class=copy-to-clipboard title="Fine-Tuning of Continuous-Time Diffusion Models as Entropy-Regularized Control" index=47>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-47 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG, stat-ML<br>Keyword Score: 40<br>Keywords: Diffusion Model, Continuous Time, Continuous Time, Fine-tuning, Fine-tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15194v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15194v1.pdf filename=2402.15194v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Diffusion</b> <b>models</b> excel at capturing complex data distributions, such as those of natural images and proteins. While <b>diffusion</b> <b>models</b> are trained to represent the distribution in the training dataset, we often are more concerned with other properties, such as the aesthetic quality of the generated images or the functional properties of generated proteins. <b>Diffusion</b> <b>models</b> can be <b>finetuned</b> in a goal-directed way by maximizing the value of some reward function (e.g., the aesthetic quality of an image). However, these approaches may lead to reduced sample diversity, significant deviations from the training data distribution, and even poor sample quality due to the exploitation of an imperfect reward function. The last issue often occurs when the reward function is a learned model meant to approximate a ground-truth &ldquo;genuine&rdquo; reward, as is the case in many practical applications. These challenges, collectively termed &ldquo;reward collapse,&rdquo; pose a substantial obstacle. To address this reward collapse, we frame the <b>finetuning</b> problem as entropy-regularized control against the pretrained <b>diffusion</b> <b>model,</b> i.e., directly optimizing entropy-enhanced rewards with neural SDEs. We present theoretical and empirical evidence that demonstrates our framework is capable of efficiently generating diverse samples with high genuine rewards, mitigating the overoptimization of imperfect reward models.</p></p class="citation"></blockquote><h3 id=1256--48190-multimodal-transformer-with-a-low-computational-cost-guarantee-sungjin-park-et-al-2024>(12/56 | 48/190) Multimodal Transformer With a Low-Computational-Cost Guarantee (Sungjin Park et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sungjin Park, Edward Choi. (2024)<br><strong>Multimodal Transformer With a Low-Computational-Cost Guarantee</strong><br><button class=copy-to-clipboard title="Multimodal Transformer With a Low-Computational-Cost Guarantee" index=48>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-48 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CV, cs-LG, cs-MM, cs.LG<br>Keyword Score: 36<br>Keywords: Multi-modal, Multi-modal, Transformer, Question Answering, Visual Question Answering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15096v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15096v1.pdf filename=2402.15096v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Transformer-based</b> models have significantly improved performance across a range of <b>multimodal</b> understanding tasks, such as <b>visual</b> <b>question</b> <b>answering</b> and action recognition. However, <b>multimodal</b> <b>Transformers</b> significantly suffer from a quadratic complexity of the multi-head attention with the input sequence length, especially as the number of modalities increases. To address this, we introduce Low-Cost <b>Multimodal</b> <b>Transformer</b> (LoCoMT), a novel <b>multimodal</b> attention mechanism that aims to reduce computational cost during training and inference with minimal performance loss. Specifically, by assigning different <b>multimodal</b> attention patterns to each attention head, LoCoMT can flexibly control <b>multimodal</b> signals and theoretically ensures a reduced computational cost compared to existing <b>multimodal</b> <b>Transformer</b> variants. Experimental results on two <b>multimodal</b> datasets, namely Audioset and MedVidCL demonstrate that LoCoMT not only reduces GFLOPs but also matches or even outperforms established models.</p></p class="citation"></blockquote><h3 id=1356--49190-understanding-oversmoothing-in-diffusion-based-gnns-from-the-perspective-of-operator-semigroup-theory-weichen-zhao-et-al-2024>(13/56 | 49/190) Understanding Oversmoothing in Diffusion-Based GNNs From the Perspective of Operator Semigroup Theory (Weichen Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Weichen Zhao, Chenguang Wang, Xinyan Wang, Congying Han, Tiande Guo, Tianshu Yu. (2024)<br><strong>Understanding Oversmoothing in Diffusion-Based GNNs From the Perspective of Operator Semigroup Theory</strong><br><button class=copy-to-clipboard title="Understanding Oversmoothing in Diffusion-Based GNNs From the Perspective of Operator Semigroup Theory" index=49>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-49 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 33<br>Keywords: Node Classification, Graph, Graph Neural Network, Graph Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15326v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15326v1.pdf filename=2402.15326v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents a novel study of the oversmoothing issue in diffusion-based <b>Graph</b> <b>Neural</b> <b>Networks</b> <b>(GNNs).</b> Diverging from extant approaches grounded in random walk analysis or particle systems, we approach this problem through operator semigroup theory. This theoretical framework allows us to rigorously prove that oversmoothing is intrinsically linked to the ergodicity of the diffusion operator. This finding further poses a general and mild ergodicity-breaking condition, encompassing the various specific solutions previously offered, thereby presenting a more universal and theoretically grounded approach to mitigating oversmoothing in diffusion-based <b>GNNs.</b> Additionally, we offer a probabilistic interpretation of our theory, forging a link with prior works and broadening the theoretical horizon. Our experimental results reveal that this ergodicity-breaking term effectively mitigates oversmoothing measured by Dirichlet energy, and simultaneously enhances performance in <b>node</b> <b>classification</b> tasks.</p></p class="citation"></blockquote><h3 id=1456--50190-transformers-are-expressive-but-are-they-expressive-enough-for-regression-swaroop-nath-et-al-2024>(14/56 | 50/190) Transformers are Expressive, But Are They Expressive Enough for Regression? (Swaroop Nath et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Swaroop Nath, Harshad Khadilkar, Pushpak Bhattacharyya. (2024)<br><strong>Transformers are Expressive, But Are They Expressive Enough for Regression?</strong><br><button class=copy-to-clipboard title="Transformers are Expressive, But Are They Expressive Enough for Regression?" index=50>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-50 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-ML<br>Keyword Score: 30<br>Keywords: Transformer, Neural Machine Translation, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15478v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15478v1.pdf filename=2402.15478v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Transformers</b> have become pivotal in Natural Language Processing, demonstrating remarkable success in applications like <b>Machine</b> <b>Translation</b> and <b>Summarization.</b> Given their widespread adoption, several works have attempted to analyze the expressivity of <b>Transformers.</b> Expressivity of a neural network is the class of functions it can approximate. A neural network is fully expressive if it can act as a universal function approximator. We attempt to analyze the same for <b>Transformers.</b> Contrary to existing claims, our findings reveal that <b>Transformers</b> struggle to reliably approximate continuous functions, relying on piecewise constant approximations with sizable intervals. The central question emerges as: &ldquo;\textit{Are <b>Transformers</b> truly Universal Function Approximators}?&rdquo; To address this, we conduct a thorough investigation, providing theoretical insights and supporting evidence through experiments. Our contributions include a theoretical analysis pinpointing the root of <b>Transformers&rsquo;</b> limitation in function approximation and extensive experiments to verify the limitation. By shedding light on these challenges, we advocate a refined understanding of <b>Transformers&rsquo;</b> capabilities.</p></p class="citation"></blockquote><h3 id=1556--51190-does-combining-parameter-efficient-modules-improve-few-shot-transfer-accuracy-nader-asadi-et-al-2024>(15/56 | 51/190) Does Combining Parameter-efficient Modules Improve Few-shot Transfer Accuracy? (Nader Asadi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nader Asadi, Mahdi Beitollahi, Yasser Khalil, Yinchuan Li, Guojun Zhang, Xi Chen. (2024)<br><strong>Does Combining Parameter-efficient Modules Improve Few-shot Transfer Accuracy?</strong><br><button class=copy-to-clipboard title="Does Combining Parameter-efficient Modules Improve Few-shot Transfer Accuracy?" index=51>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-51 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CV, cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Few-shot, Fine-tuning, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15414v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15414v1.pdf filename=2402.15414v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Parameter-efficient <b>fine-tuning</b> stands as the standard for efficiently <b>fine-tuning</b> large language and vision models on downstream tasks. Specifically, the efficiency of low-rank adaptation has facilitated the creation and sharing of hundreds of custom LoRA modules, each trained on distinct data from various downstream tasks. In this paper, we explore the composability of LoRA modules, examining if combining these pre-trained modules enhances generalization to unseen downstream tasks. Our investigation involves evaluating two approaches: (a) uniform composition, involving averaging upstream LoRA modules with equal weights, and (b) learned composition, where we learn the weights for each upstream module and perform weighted averaging. Our experimental results on both vision and language models reveal that in <b>few-shot</b> settings, where only a limited number of samples are available for the downstream task, both uniform and learned composition methods result in better transfer accuracy; outperforming full <b>fine-tuning</b> and training a LoRA from scratch. Moreover, in full-shot settings, learned composition performs comparably to regular LoRA training with significantly fewer number of trainable parameters. Our research unveils the potential of uniform composition for enhancing transferability in low-shot settings, without introducing additional learnable parameters.</p></p class="citation"></blockquote><h3 id=1656--52190-when-in-doubt-think-slow-iterative-reasoning-with-latent-imagination-martin-benfeghoul-et-al-2024>(16/56 | 52/190) When in Doubt, Think Slow: Iterative Reasoning with Latent Imagination (Martin Benfeghoul et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Martin Benfeghoul, Umais Zahid, Qinghai Guo, Zafeirios Fountas. (2024)<br><strong>When in Doubt, Think Slow: Iterative Reasoning with Latent Imagination</strong><br><button class=copy-to-clipboard title="When in Doubt, Think Slow: Iterative Reasoning with Latent Imagination" index=52>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-52 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: I-2-0; I-2-8; I-2-10; I-4-5; I-4-10, cs-AI, cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Fine-tuning, Reinforcement Learning, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15283v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15283v1.pdf filename=2402.15283v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In an unfamiliar setting, a model-based <b>reinforcement</b> <b>learning</b> agent can be limited by the accuracy of its world model. In this work, we present a novel, training-free approach to improving the performance of such agents separately from planning and learning. We do so by applying iterative inference at decision-time, to <b>fine-tune</b> the inferred agent states based on the coherence of future state representations. Our approach achieves a consistent improvement in both reconstruction accuracy and task performance when applied to visual 3D navigation tasks. We go on to show that considering more future states further improves the performance of the agent in partially-observable environments, but not in a fully-observable one. Finally, we demonstrate that agents with less training pre-evaluation benefit most from our approach.</p></p class="citation"></blockquote><h3 id=1756--53190-chunkattention-efficient-self-attention-with-prefix-aware-kv-cache-and-two-phase-partition-lu-ye-et-al-2024>(17/56 | 53/190) ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and Two-Phase Partition (Lu Ye et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lu Ye, Ze Tao, Yong Huang, Yang Li. (2024)<br><strong>ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and Two-Phase Partition</strong><br><button class=copy-to-clipboard title="ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and Two-Phase Partition" index=53>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-53 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CL, cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Large Language Model, Prompt, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15220v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15220v1.pdf filename=2402.15220v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Self-attention</b> is an essential component of large language models(LLMs) but a significant source of inference latency for long sequences. In multi-tenant <b>LLMs</b> serving scenarios, the compute and memory operation cost of <b>self-attention</b> can be optimized by using the probability that multiple <b>LLM</b> requests have shared system <b>prompts</b> in prefixes. In this paper, we introduce ChunkAttention, a prefix-aware <b>self-attention</b> module that can detect matching <b>prompt</b> prefixes across multiple requests and share their key/value tensors in memory at runtime to improve the memory utilization of KV cache. This is achieved by breaking monolithic key/value tensors into smaller chunks and structuring them into the auxiliary prefix tree. Consequently, on top of the prefix-tree based KV cache, we design an efficient <b>self-attention</b> kernel, where a two-phase partition algorithm is implemented to improve the data locality during <b>self-attention</b> computation in the presence of shared system <b>prompts.</b> Experiments show that ChunkAttention can speed up the <b>self-attention</b> kernel by 3.2-4.8$\times$ compared to the start-of-the-art implementation, with the length of the system <b>prompt</b> ranging from 1024 to 4096.</p></p class="citation"></blockquote><h3 id=1856--54190-second-order-fine-tuning-without-pain-for-llmsa-hessian-informed-zeroth-order-optimizer-yanjun-zhao-et-al-2024>(18/56 | 54/190) Second-Order Fine-Tuning without Pain for LLMs:A Hessian Informed Zeroth-Order Optimizer (Yanjun Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yanjun Zhao, Sizhe Dang, Haishan Ye, Guang Dai, Yi Qian, Ivor W. Tsang. (2024)<br><strong>Second-Order Fine-Tuning without Pain for LLMs:A Hessian Informed Zeroth-Order Optimizer</strong><br><button class=copy-to-clipboard title="Second-Order Fine-Tuning without Pain for LLMs:A Hessian Informed Zeroth-Order Optimizer" index=54>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-54 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Fine-tuning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15173v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15173v1.pdf filename=2402.15173v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Fine-tuning</b> <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> with classic first-order optimizers entails prohibitive GPU memory due to the backpropagation process. Recent works have turned to zeroth-order optimizers for <b>fine-tuning,</b> which save substantial memory by using two forward passes. However, these optimizers are plagued by the heterogeneity of parameter curvatures across different dimensions. In this work, we propose HiZOO, a diagonal Hessian informed zeroth-order optimizer which is the first work to leverage the diagonal Hessian to enhance zeroth-order optimizer for <b>fine-tuning</b> <b>LLMs.</b> What&rsquo;s more, HiZOO avoids the expensive memory cost and only increases one forward pass per step. Extensive experiments on various models (350M~66B parameters) indicate that HiZOO improves model convergence, significantly reducing training steps and effectively enhancing model accuracy. Moreover, we visualize the optimization trajectories of HiZOO on test functions, illustrating its effectiveness in handling heterogeneous curvatures. Lastly, we provide theoretical proofs of convergence for HiZOO. Code is publicly available at <a href=https://anonymous.4open.science/r/HiZOO27F8>https://anonymous.4open.science/r/HiZOO27F8</a>.</p></p class="citation"></blockquote><h3 id=1956--55190-spatially-aware-transformer-memory-for-embodied-agents-junmo-cho-et-al-2024>(19/56 | 55/190) Spatially-Aware Transformer Memory for Embodied Agents (Junmo Cho et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Junmo Cho, Jaesik Yoon, Sungjin Ahn. (2024)<br><strong>Spatially-Aware Transformer Memory for Embodied Agents</strong><br><button class=copy-to-clipboard title="Spatially-Aware Transformer Memory for Embodied Agents" index=55>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-55 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Reinforcement Learning, Transformer, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15160v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15160v1.pdf filename=2402.15160v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Episodic memory plays a crucial role in various cognitive processes, such as the ability to mentally recall past events. While cognitive science emphasizes the significance of spatial context in the formation and retrieval of episodic memory, the current primary approach to implementing episodic memory in AI systems is through <b>transformers</b> that store temporally ordered experiences, which overlooks the spatial dimension. As a result, it is unclear how the underlying structure could be extended to incorporate the spatial axis beyond temporal order alone and thereby what benefits can be obtained. To address this, this paper explores the use of Spatially-Aware <b>Transformer</b> models that incorporate spatial information. These models enable the creation of place-centric episodic memory that considers both temporal and spatial dimensions. Adopting this approach, we demonstrate that memory utilization efficiency can be improved, leading to enhanced accuracy in various place-centric downstream tasks. Additionally, we propose the Adaptive Memory Allocator, a memory management method based on <b>reinforcement</b> <b>learning</b> that aims to optimize efficiency of memory utilization. Our experiments demonstrate the advantages of our proposed model in various environments and across multiple downstream tasks, including prediction, generation, <b>reasoning,</b> and <b>reinforcement</b> <b>learning.</b> The source code for our models and experiments will be available at <a href=https://github.com/junmokane/spatially-aware-transformer>https://github.com/junmokane/spatially-aware-transformer</a>.</p></p class="citation"></blockquote><h3 id=2056--56190-accelerating-convergence-of-stein-variational-gradient-descent-via-deep-unfolding-yuya-kawamura-et-al-2024>(20/56 | 56/190) Accelerating Convergence of Stein Variational Gradient Descent via Deep Unfolding (Yuya Kawamura et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuya Kawamura, Satoshi Takabe. (2024)<br><strong>Accelerating Convergence of Stein Variational Gradient Descent via Deep Unfolding</strong><br><button class=copy-to-clipboard title="Accelerating Convergence of Stein Variational Gradient Descent via Deep Unfolding" index=56>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-56 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-ML<br>Keyword Score: 30<br>Keywords: Logistic Regression, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15125v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15125v1.pdf filename=2402.15125v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Stein variational gradient descent (SVGD) is a prominent particle-based variational inference method used for sampling a target distribution. SVGD has attracted interest for application in machine-learning techniques such as Bayesian inference. In this paper, we propose novel trainable algorithms that incorporate a deep-learning technique called deep unfolding,into SVGD. This approach facilitates the learning of the internal parameters of SVGD, thereby accelerating its convergence speed. To evaluate the proposed trainable SVGD algorithms, we conducted numerical <b>simulations</b> of three tasks: sampling a one-dimensional Gaussian mixture, performing Bayesian <b>logistic</b> <b>regression,</b> and learning Bayesian neural networks. The results show that our proposed algorithms exhibit faster convergence than the conventional variants of SVGD.</p></p class="citation"></blockquote><h3 id=2156--57190-trajectory-wise-iterative-reinforcement-learning-framework-for-auto-bidding-haoming-li-et-al-2024>(21/56 | 57/190) Trajectory-wise Iterative Reinforcement Learning Framework for Auto-bidding (Haoming Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haoming Li, Yusen Huo, Shuai Dou, Zhenzhe Zheng, Zhilin Zhang, Chuan Yu, Jian Xu, Fan Wu. (2024)<br><strong>Trajectory-wise Iterative Reinforcement Learning Framework for Auto-bidding</strong><br><button class=copy-to-clipboard title="Trajectory-wise Iterative Reinforcement Learning Framework for Auto-bidding" index=57>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-57 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-GT, cs-IR, cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Reinforcement Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15102v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15102v1.pdf filename=2402.15102v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In online advertising, advertisers participate in ad auctions to acquire ad opportunities, often by utilizing auto-bidding tools provided by demand-side platforms (DSPs). The current auto-bidding algorithms typically employ <b>reinforcement</b> <b>learning</b> (RL). However, due to safety concerns, most RL-based auto-bidding policies are trained in <b>simulation,</b> leading to a performance degradation when deployed in online environments. To narrow this gap, we can deploy multiple auto-bidding agents in parallel to collect a large interaction dataset. Offline RL algorithms can then be utilized to train a new policy. The trained policy can subsequently be deployed for further data collection, resulting in an iterative training framework, which we refer to as iterative offline RL. In this work, we identify the performance bottleneck of this iterative offline RL framework, which originates from the ineffective exploration and exploitation caused by the inherent conservatism of offline RL algorithms. To overcome this bottleneck, we propose Trajectory-wise Exploration and Exploitation (TEE), which introduces a novel data collecting and data utilization method for iterative offline RL from a trajectory perspective. Furthermore, to ensure the safety of online exploration while preserving the dataset quality for TEE, we propose Safe Exploration by Adaptive Action Selection (SEAS). Both offline experiments and real-world experiments on Alibaba display advertising platform demonstrate the effectiveness of our proposed method.</p></p class="citation"></blockquote><h3 id=2256--58190-enhancing-one-shot-federated-learning-through-data-and-ensemble-co-boosting-rong-dai-et-al-2024>(22/56 | 58/190) Enhancing One-Shot Federated Learning Through Data and Ensemble Co-Boosting (Rong Dai et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rong Dai, Yonggang Zhang, Ang Li, Tongliang Liu, Xun Yang, Bo Han. (2024)<br><strong>Enhancing One-Shot Federated Learning Through Data and Ensemble Co-Boosting</strong><br><button class=copy-to-clipboard title="Enhancing One-Shot Federated Learning Through Data and Ensemble Co-Boosting" index=58>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-58 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Federated Learning, Knowledge Distillation, Knowledge Distillation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15070v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15070v1.pdf filename=2402.15070v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>One-shot <b>Federated</b> <b>Learning</b> (OFL) has become a promising learning paradigm, enabling the training of a global server model via a single communication round. In OFL, the server model is aggregated by <b>distilling</b> knowledge from all client models (the ensemble), which are also responsible for synthesizing samples for <b>distillation.</b> In this regard, advanced works show that the performance of the server model is intrinsically related to the quality of the synthesized data and the ensemble model. To promote OFL, we introduce a novel framework, Co-Boosting, in which synthesized data and the ensemble model mutually enhance each other progressively. Specifically, Co-Boosting leverages the current ensemble model to synthesize higher-quality samples in an adversarial manner. These hard samples are then employed to promote the quality of the ensemble model by adjusting the ensembling weights for each client model. Consequently, Co-Boosting periodically achieves high-quality data and ensemble models. Extensive experiments demonstrate that Co-Boosting can substantially outperform existing baselines under various settings. Moreover, Co-Boosting eliminates the need for adjustments to the client&rsquo;s local training, requires no additional data or model transmission, and allows client models to have heterogeneous architectures.</p></p class="citation"></blockquote><h3 id=2356--59190-autommlab-automatically-generating-deployable-models-from-language-instructions-for-computer-vision-tasks-zekang-yang-et-al-2024>(23/56 | 59/190) AutoMMLab: Automatically Generating Deployable Models from Language Instructions for Computer Vision Tasks (Zekang Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zekang Yang, Wang Zeng, Sheng Jin, Chen Qian, Ping Luo, Wentao Liu. (2024)<br><strong>AutoMMLab: Automatically Generating Deployable Models from Language Instructions for Computer Vision Tasks</strong><br><button class=copy-to-clipboard title="AutoMMLab: Automatically Generating Deployable Models from Language Instructions for Computer Vision Tasks" index=59>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-59 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CV, cs-LG, cs.LG<br>Keyword Score: 23<br>Keywords: Benchmarking, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15351v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15351v1.pdf filename=2402.15351v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Automated machine learning (AutoML) is a collection of techniques designed to automate the machine learning development process. While traditional AutoML approaches have been successfully applied in several critical steps of model development (e.g. hyperparameter optimization), there lacks a AutoML system that automates the entire end-to-end model production workflow. To fill this blank, we present AutoMMLab, a general-purpose <b>LLM-empowered</b> AutoML system that follows user&rsquo;s language instructions to automate the whole model production workflow for computer vision tasks. The proposed AutoMMLab system effectively employs <b>LLMs</b> as the bridge to connect AutoML and OpenMMLab community, empowering non-expert individuals to easily build task-specific models via a user-friendly language interface. Specifically, we propose RU-LLaMA to understand users&rsquo; request and schedule the whole pipeline, and propose a novel <b>LLM-based</b> hyperparameter optimizer called HPO-LLaMA to effectively search for the optimal hyperparameters. Experiments show that our AutoMMLab system is versatile and covers a wide range of mainstream tasks, including classification, detection, segmentation and keypoint estimation. We further develop a new <b>benchmark,</b> called LAMP, for studying key components in the end-to-end <b>prompt-based</b> model training pipeline. Code, model, and data will be released.</p></p class="citation"></blockquote><h3 id=2456--60190-optimal-transport-for-structure-learning-under-missing-data-vy-vo-et-al-2024>(24/56 | 60/190) Optimal Transport for Structure Learning Under Missing Data (Vy Vo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Vy Vo, He Zhao, Trung Le, Edwin V. Bonilla, Dinh Phung. (2024)<br><strong>Optimal Transport for Structure Learning Under Missing Data</strong><br><button class=copy-to-clipboard title="Optimal Transport for Structure Learning Under Missing Data" index=60>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-60 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 23<br>Keywords: Graph, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15255v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15255v1.pdf filename=2402.15255v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Causal discovery in the presence of missing data introduces a chicken-and-egg dilemma. While the goal is to recover the true causal structure, robust imputation requires considering the dependencies or preferably causal relations among variables. Merely filling in missing values with existing imputation methods and subsequently applying structure learning on the complete data is empirical shown to be sub-optimal. To this end, we propose in this paper a score-based algorithm, based on optimal transport, for learning causal structure from missing data. This optimal transport viewpoint diverges from existing score-based approaches that are dominantly based on EM. We project structure learning as a density fitting problem, where the goal is to find the causal model that induces a distribution of minimum Wasserstein distance with the distribution over the observed data. Through extensive <b>simulations</b> and real-data experiments, our framework is shown to recover the true causal <b>graphs</b> more effectively than the baselines in various <b>simulations</b> and real-data experiments. Empirical evidences also demonstrate the superior scalability of our approach, along with the flexibility to incorporate any off-the-shelf causal discovery methods for complete data.</p></p class="citation"></blockquote><h3 id=2556--61190-mspipe-efficient-temporal-gnn-training-via-staleness-aware-pipeline-guangming-sheng-et-al-2024>(25/56 | 61/190) MSPipe: Efficient Temporal GNN Training via Staleness-aware Pipeline (Guangming Sheng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Guangming Sheng, Junwei Su, Chao Huang, Chuan Wu. (2024)<br><strong>MSPipe: Efficient Temporal GNN Training via Staleness-aware Pipeline</strong><br><button class=copy-to-clipboard title="MSPipe: Efficient Temporal GNN Training via Staleness-aware Pipeline" index=61>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-61 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-DC, cs-LG, cs.LG<br>Keyword Score: 23<br>Keywords: Graph, Graph Neural Network, Graph Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15113v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15113v1.pdf filename=2402.15113v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Memory-based Temporal <b>Graph</b> <b>Neural</b> <b>Networks</b> (MTGNNs) are a class of temporal <b>graph</b> <b>neural</b> <b>networks</b> that utilize a node memory module to capture and retain long-term temporal dependencies, leading to superior performance compared to memory-less counterparts. However, the iterative reading and updating process of the memory module in MTGNNs to obtain up-to-date information needs to follow the temporal dependencies. This introduces significant overhead and limits training throughput. Existing optimizations for static <b>GNNs</b> are not directly applicable to MTGNNs due to differences in training paradigm, model architecture, and the absence of a memory module. Moreover, they do not effectively address the challenges posed by temporal dependencies, making them ineffective for MTGNN training. In this paper, we propose MSPipe, a general and efficient framework for MTGNNs that maximizes training throughput while maintaining model accuracy. Our design addresses the unique challenges associated with fetching and updating node memory states in MTGNNs by integrating staleness into the memory module. However, simply introducing a predefined staleness bound in the memory module to break temporal dependencies may lead to suboptimal performance and lack of generalizability across different models and datasets. To solve this, we introduce an online pipeline scheduling algorithm in MSPipe that strategically breaks temporal dependencies with minimal staleness and delays memory fetching to obtain fresher memory states. Moreover, we design a staleness mitigation mechanism to enhance training convergence and model accuracy. We provide convergence analysis and prove that MSPipe maintains the same convergence rate as vanilla sample-based <b>GNN</b> training. Experimental results show that MSPipe achieves up to 2.45x speed-up without sacrificing accuracy, making it a promising solution for efficient MTGNN training.</p></p class="citation"></blockquote><h3 id=2656--62190-cost-adaptive-recourse-recommendation-by-adaptive-preference-elicitation-duy-nguyen-et-al-2024>(26/56 | 62/190) Cost-Adaptive Recourse Recommendation by Adaptive Preference Elicitation (Duy Nguyen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Duy Nguyen, Bao Nguyen, Viet Anh Nguyen. (2024)<br><strong>Cost-Adaptive Recourse Recommendation by Adaptive Preference Elicitation</strong><br><button class=copy-to-clipboard title="Cost-Adaptive Recourse Recommendation by Adaptive Preference Elicitation" index=62>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-62 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 23<br>Keywords: Graph, Recommendation, Question Answering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15073v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15073v1.pdf filename=2402.15073v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Algorithmic recourse recommends a cost-efficient action to a subject to reverse an unfavorable machine learning classification decision. Most existing methods in the literature generate recourse under the assumption of complete knowledge about the cost function. In real-world practice, subjects could have distinct preferences, leading to incomplete information about the underlying cost function of the subject. This paper proposes a two-step approach integrating preference learning into the recourse generation problem. In the first step, we design a <b>question-answering</b> <b>framework</b> to refine the confidence set of the Mahalanobis matrix cost of the subject sequentially. Then, we generate recourse by utilizing two methods: gradient-based and <b>graph-based</b> cost-adaptive recourse that ensures validity while considering the whole confidence set of the cost matrix. The numerical evaluation demonstrates the benefits of our approach over state-of-the-art baselines in delivering cost-efficient recourse <b>recommendations.</b></p></p class="citation"></blockquote><h3 id=2756--63190-fair-filtering-of-automatically-induced-rules-divya-jyoti-bajpai-et-al-2024>(27/56 | 63/190) FAIR: Filtering of Automatically Induced Rules (Divya Jyoti Bajpai et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Divya Jyoti Bajpai, Ayush Maheshwari, Manjesh Kumar Hanawal, Ganesh Ramakrishnan. (2024)<br><strong>FAIR: Filtering of Automatically Induced Rules</strong><br><button class=copy-to-clipboard title="FAIR: Filtering of Automatically Induced Rules" index=63>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-63 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Text Classification, Weakly Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15472v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15472v1.pdf filename=2402.15472v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The availability of large annotated data can be a critical bottleneck in training machine learning algorithms successfully, especially when applied to diverse domains. <b>Weak</b> <b>supervision</b> offers a promising alternative by accelerating the creation of labeled training data using domain-specific rules. However, it requires users to write a diverse set of high-quality rules to assign labels to the unlabeled data. Automatic Rule Induction (ARI) approaches circumvent this problem by automatically creating rules from features on a small labeled set and filtering a final set of rules from them. In the ARI approach, the crucial step is to filter out a set of a high-quality useful subset of rules from the large set of automatically created rules. In this paper, we propose an algorithm (Filtering of Automatically Induced Rules) to filter rules from a large number of automatically induced rules using submodular objective functions that account for the collective precision, coverage, and conflicts of the rule set. We experiment with three ARI approaches and five <b>text</b> <b>classification</b> datasets to validate the superior performance of our algorithm with respect to several semi-supervised label aggregation approaches. Further, we show that achieves statistically significant results in comparison to existing rule-filtering approaches.</p></p class="citation"></blockquote><h3 id=2856--64190-the-impact-of-lora-on-the-emergence-of-clusters-in-transformers-hugo-koubbi-et-al-2024>(28/56 | 64/190) The Impact of LoRA on the Emergence of Clusters in Transformers (Hugo Koubbi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hugo Koubbi, Matthieu Boussard, Louis Hernandez. (2024)<br><strong>The Impact of LoRA on the Emergence of Clusters in Transformers</strong><br><button class=copy-to-clipboard title="The Impact of LoRA on the Emergence of Clusters in Transformers" index=64>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-64 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, math-DS, stat-ML<br>Keyword Score: 20<br>Keywords: Fine-tuning, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15415v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15415v1.pdf filename=2402.15415v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we employ the mathematical framework on <b>Transformers</b> developed by \citet{sander2022sinkformers,geshkovski2023emergence,geshkovski2023mathematical} to explore how variations in attention parameters and initial token values impact the structural dynamics of token clusters. Our analysis demonstrates that while the clusters within a modified attention matrix dynamics can exhibit significant divergence from the original over extended periods, they maintain close similarities over shorter intervals, depending on the parameter differences. This work contributes to the <b>fine-tuning</b> field through practical applications to the LoRA algorithm \cite{hu2021lora,peft}, enhancing our understanding of the behavior of LoRA-enhanced <b>Transformer</b> models.</p></p class="citation"></blockquote><h3 id=2956--65190-genie-generative-interactive-environments-jake-bruce-et-al-2024>(29/56 | 65/190) Genie: Generative Interactive Environments (Jake Bruce et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jake Bruce, Michael Dennis, Ashley Edwards, Jack Parker-Holder, Yuge Shi, Edward Hughes, Matthew Lai, Aditi Mavalankar, Richie Steigerwald, Chris Apps, Yusuf Aytar, Sarah Bechtle, Feryal Behbahani, Stephanie Chan, Nicolas Heess, Lucy Gonzalez, Simon Osindero, Sherjil Ozair, Scott Reed, Jingwei Zhang, Konrad Zolna, Jeff Clune, Nando de Freitas, Satinder Singh, Tim Rocktäschel. (2024)<br><strong>Genie: Generative Interactive Environments</strong><br><button class=copy-to-clipboard title="Genie: Generative Interactive Environments" index=65>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-65 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CV, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Unsupervised Learning, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15391v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15391v1.pdf filename=2402.15391v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce Genie, the first generative interactive environment trained in an <b>unsupervised</b> manner from unlabelled Internet videos. The model can be <b>prompted</b> to generate an endless variety of action-controllable virtual worlds described through text, synthetic images, photographs, and even sketches. At 11B parameters, Genie can be considered a foundation world model. It is comprised of a spatiotemporal video tokenizer, an autoregressive dynamics model, and a simple and scalable latent action model. Genie enables users to act in the generated environments on a frame-by-frame basis despite training without any ground-truth action labels or other domain-specific requirements typically found in the world model literature. Further the resulting learned latent action space facilitates training agents to imitate behaviors from unseen videos, opening the path for training generalist agents of the future.</p></p class="citation"></blockquote><h3 id=3056--66190-explorations-of-self-repair-in-language-models-cody-rushing-et-al-2024>(30/56 | 66/190) Explorations of Self-Repair in Language Models (Cody Rushing et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Cody Rushing, Neel Nanda. (2024)<br><strong>Explorations of Self-Repair in Language Models</strong><br><button class=copy-to-clipboard title="Explorations of Self-Repair in Language Models" index=66>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-66 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CL, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15390v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15390v1.pdf filename=2402.15390v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Prior interpretability research studying narrow distributions has preliminarily identified self-repair, a phenomena where if components in <b>large</b> <b>language</b> <b>models</b> are ablated, later components will change their behavior to compensate. Our work builds off this past literature, demonstrating that self-repair exists on a variety of models families and sizes when ablating individual attention heads on the full training distribution. We further show that on the full training distribution self-repair is imperfect, as the original direct effect of the head is not fully restored, and noisy, since the degree of self-repair varies significantly across different <b>prompts</b> (sometimes overcorrecting beyond the original effect). We highlight two different mechanisms that contribute to self-repair, including changes in the final LayerNorm scaling factor (which can repair up to 30% of the direct effect) and sparse sets of neurons implementing Anti-Erasure. We additionally discuss the implications of these results for interpretability practitioners and close with a more speculative discussion on the mystery of why self-repair occurs in these models at all, highlighting evidence for the Iterative Inference hypothesis in language models, a framework that predicts self-repair.</p></p class="citation"></blockquote><h3 id=3156--67190-on-the-duality-between-sharpness-aware-minimization-and-adversarial-training-yihao-zhang-et-al-2024>(31/56 | 67/190) On the Duality Between Sharpness-Aware Minimization and Adversarial Training (Yihao Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yihao Zhang, Hangzhou He, Jingyu Zhu, Huanran Chen, Yifei Wang, Zeming Wei. (2024)<br><strong>On the Duality Between Sharpness-Aware Minimization and Adversarial Training</strong><br><button class=copy-to-clipboard title="On the Duality Between Sharpness-Aware Minimization and Adversarial Training" index=67>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-67 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CR, cs-LG, cs.LG, math-OC<br>Keyword Score: 20<br>Keywords: Adversarial Learning, Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15152v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15152v1.pdf filename=2402.15152v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Adversarial</b> <b>Training</b> (AT), which adversarially perturb the input samples during training, has been acknowledged as one of the most effective defenses against <b>adversarial</b> <b>attacks,</b> yet suffers from a fundamental tradeoff that inevitably decreases clean accuracy. Instead of perturbing the samples, Sharpness-Aware Minimization (SAM) perturbs the model weights during training to find a more flat loss landscape and improve generalization. However, as SAM is designed for better clean accuracy, its effectiveness in enhancing <b>adversarial</b> <b>robustness</b> remains unexplored. In this work, considering the duality between SAM and AT, we investigate the <b>adversarial</b> <b>robustness</b> derived from SAM. Intriguingly, we find that using SAM alone can improve <b>adversarial</b> <b>robustness.</b> To understand this unexpected property of SAM, we first provide empirical and theoretical insights into how SAM can implicitly learn more robust features, and conduct comprehensive experiments to show that SAM can improve <b>adversarial</b> <b>robustness</b> notably without sacrificing any clean accuracy, shedding light on the potential of SAM to be a substitute for AT when accuracy comes at a higher priority. Code is available at <a href=https://github.com/weizeming/SAM_AT>https://github.com/weizeming/SAM_AT</a>.</p></p class="citation"></blockquote><h3 id=3256--68190-deep-coupling-network-for-multivariate-time-series-forecasting-kun-yi-et-al-2024>(32/56 | 68/190) Deep Coupling Network For Multivariate Time Series Forecasting (Kun Yi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kun Yi, Qi Zhang, Hui He, Kaize Shi, Liang Hu, Ning An, Zhendong Niu. (2024)<br><strong>Deep Coupling Network For Multivariate Time Series Forecasting</strong><br><button class=copy-to-clipboard title="Deep Coupling Network For Multivariate Time Series Forecasting" index=68>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-68 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Mutual Information, Neural Machine Translation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15134v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15134v1.pdf filename=2402.15134v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Multivariate time series <b>(MTS)</b> forecasting is crucial in many real-world applications. To achieve accurate <b>MTS</b> forecasting, it is essential to simultaneously consider both intra- and inter-series relationships among time series data. However, previous work has typically modeled intra- and inter-series relationships separately and has disregarded multi-order interactions present within and between time series data, which can seriously degrade forecasting accuracy. In this paper, we reexamine intra- and inter-series relationships from the perspective of <b>mutual</b> <b>information</b> and accordingly construct a comprehensive relationship learning mechanism tailored to simultaneously capture the intricate multi-order intra- and inter-series couplings. Based on the mechanism, we propose a novel deep coupling network for <b>MTS</b> forecasting, named DeepCN, which consists of a coupling mechanism dedicated to explicitly exploring the multi-order intra- and inter-series relationships among time series data concurrently, a coupled variable representation module aimed at encoding diverse variable patterns, and an inference module facilitating predictions through one forward step. Extensive experiments conducted on seven real-world datasets demonstrate that our proposed DeepCN achieves superior performance compared with the state-of-the-art baselines.</p></p class="citation"></blockquote><h3 id=3356--69190-transflower-an-explainable-transformer-based-model-with-flow-to-flow-attention-for-commuting-flow-prediction-yan-luo-et-al-2024>(33/56 | 69/190) TransFlower: An Explainable Transformer-Based Model with Flow-to-Flow Attention for Commuting Flow Prediction (Yan Luo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yan Luo, Zhuoyue Wan, Yuzhong Chen, Gengchen Mai, Fu-lai Chung, Kent Larson. (2024)<br><strong>TransFlower: An Explainable Transformer-Based Model with Flow-to-Flow Attention for Commuting Flow Prediction</strong><br><button class=copy-to-clipboard title="TransFlower: An Explainable Transformer-Based Model with Flow-to-Flow Attention for Commuting Flow Prediction" index=69>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-69 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CY, cs-LG, cs.LG<br>Keyword Score: 15<br>Keywords: Black Box, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15398v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15398v1.pdf filename=2402.15398v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Understanding the link between urban planning and commuting flows is crucial for guiding urban development and policymaking. This research, bridging computer science and urban studies, addresses the challenge of integrating these fields with their distinct focuses. Traditional urban studies methods, like the gravity and radiation models, often underperform in complex scenarios due to their limited handling of multiple variables and reliance on overly simplistic and unrealistic assumptions, such as spatial isotropy. While deep learning models offer improved accuracy, their <b>black-box</b> <b>nature</b> poses a trade-off between performance and explainability &ndash; both vital for analyzing complex societal phenomena like commuting flows. To address this, we introduce TransFlower, an explainable, <b>transformer-based</b> model employing flow-to-flow attention to predict urban commuting patterns. It features a geospatial encoder with an anisotropy-aware relative location encoder for nuanced flow representation. Following this, the <b>transformer-based</b> flow predictor enhances this by leveraging attention mechanisms to efficiently capture flow interactions. Our model outperforms existing methods by up to 30.8% Common Part of Commuters, offering insights into mobility dynamics crucial for urban planning and policy decisions.</p></p class="citation"></blockquote><h3 id=3456--70190-parameter-free-algorithms-for-performative-regret-minimization-under-decision-dependent-distributions-sungwoo-park-et-al-2024>(34/56 | 70/190) Parameter-Free Algorithms for Performative Regret Minimization under Decision-Dependent Distributions (Sungwoo Park et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sungwoo Park, Junyeop Kwon, Byeongnoh Kim, Suhyun Chae, Jeeyong Lee, Dabeen Lee. (2024)<br><strong>Parameter-Free Algorithms for Performative Regret Minimization under Decision-Dependent Distributions</strong><br><button class=copy-to-clipboard title="Parameter-Free Algorithms for Performative Regret Minimization under Decision-Dependent Distributions" index=70>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-70 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, math-OC<br>Keyword Score: 15<br>Keywords: Bandit Algorithm, Black Box<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15188v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15188v1.pdf filename=2402.15188v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper studies performative risk minimization, a formulation of stochastic optimization under decision-dependent distributions. We consider the general case where the performative risk can be non-convex, for which we develop efficient parameter-free optimistic optimization-based methods. Our algorithms significantly improve upon the existing Lipschitz <b>bandit-based</b> method in many aspects. In particular, our framework does not require knowledge about the sensitivity parameter of the distribution map and the Lipshitz constant of the loss function. This makes our framework practically favorable, together with the efficient optimistic optimization-based tree-search mechanism. We provide experimental results that demonstrate the numerical superiority of our algorithms over the existing method and other <b>black-box</b> <b>optimistic</b> optimization methods.</p></p class="citation"></blockquote><h3 id=3556--71190-co-supervised-learning-improving-weak-to-strong-generalization-with-hierarchical-mixture-of-experts-yuejiang-liu-et-al-2024>(35/56 | 71/190) Co-Supervised Learning: Improving Weak-to-Strong Generalization with Hierarchical Mixture of Experts (Yuejiang Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuejiang Liu, Alexandre Alahi. (2024)<br><strong>Co-Supervised Learning: Improving Weak-to-Strong Generalization with Hierarchical Mixture of Experts</strong><br><button class=copy-to-clipboard title="Co-Supervised Learning: Improving Weak-to-Strong Generalization with Hierarchical Mixture of Experts" index=71>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-71 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CV, cs-LG, cs.LG<br>Keyword Score: 13<br>Keywords: Benchmarking, Fine-tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15505v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15505v1.pdf filename=2402.15505v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Steering the behavior of a strong model pre-trained on internet-scale data can be difficult due to the scarcity of competent supervisors. Recent studies reveal that, despite supervisory noises, a strong student model may surpass its weak teacher when <b>fine-tuned</b> on specific objectives. Yet, the effectiveness of such weak-to-strong generalization remains limited, especially in the presence of large capability gaps. In this paper, we propose to address this challenge by harnessing a diverse set of specialized teachers, instead of a single generalist one, that collectively supervises the strong student. Our approach resembles the classical hierarchical mixture of experts, with two components tailored for co-supervision: (i) we progressively alternate student training and teacher assignment, leveraging the growth of the strong student to identify plausible supervisions; (ii) we conservatively enforce teacher-student and local-global consistency, leveraging their dependencies to reject potential annotation noises. We validate the proposed method through visual recognition tasks on the OpenAI weak-to-strong <b>benchmark</b> and additional multi-domain datasets. Our code is available at \url{https://github.com/yuejiangliu/csl}.</p></p class="citation"></blockquote><h3 id=3656--72190-neuralthink-algorithm-synthesis-that-extrapolates-in-general-tasks-bernardo-esteves-et-al-2024>(36/56 | 72/190) NeuralThink: Algorithm Synthesis that Extrapolates in General Tasks (Bernardo Esteves et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bernardo Esteves, Miguel Vasco, Francisco S. Melo. (2024)<br><strong>NeuralThink: Algorithm Synthesis that Extrapolates in General Tasks</strong><br><button class=copy-to-clipboard title="NeuralThink: Algorithm Synthesis that Extrapolates in General Tasks" index=72>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-72 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 13<br>Keywords: Benchmarking, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15393v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15393v1.pdf filename=2402.15393v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>While machine learning methods excel at pattern recognition, they struggle with complex <b>reasoning</b> tasks in a scalable, algorithmic manner. Recent Deep Thinking methods show promise in learning algorithms that extrapolate: learning in smaller environments and executing the learned algorithm in larger environments. However, these works are limited to symmetrical tasks, where the input and output dimensionalities are the same. To address this gap, we propose NeuralThink, a new recurrent architecture that can consistently extrapolate to both symmetrical and asymmetrical tasks, where the dimensionality of the input and output are different. We contribute with a novel <b>benchmark</b> of asymmetrical tasks for extrapolation. We show that NeuralThink consistently outperforms the prior state-of-the-art Deep Thinking architectures, in regards to stable extrapolation to large observations from smaller training sizes.</p></p class="citation"></blockquote><h3 id=3756--73190-optimisic-information-directed-sampling-gergely-neu-et-al-2024>(37/56 | 73/190) Optimisic Information Directed Sampling (Gergely Neu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Gergely Neu, Matteo Papini, Ludovic Schwartz. (2024)<br><strong>Optimisic Information Directed Sampling</strong><br><button class=copy-to-clipboard title="Optimisic Information Directed Sampling" index=73>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-73 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Bandit Algorithm<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15411v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15411v1.pdf filename=2402.15411v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We study the problem of online learning in contextual <b>bandit</b> problems where the loss function is assumed to belong to a known parametric function class. We propose a new analytic framework for this setting that bridges the Bayesian theory of information-directed sampling due to Russo and Van Roy (2018) and the worst-case theory of Foster, Kakade, Qian, and Rakhlin (2021) based on the decision-estimation coefficient. Drawing from both lines of work, we propose a algorithmic template called Optimistic Information-Directed Sampling and show that it can achieve instance-dependent regret guarantees similar to the ones achievable by the classic Bayesian IDS method, but with the major advantage of not requiring any Bayesian assumptions. The key technical innovation of our analysis is introducing an optimistic surrogate model for the regret and using it to define a frequentist version of the Information Ratio of Russo and Van Roy (2018), and a less conservative version of the Decision Estimation Coefficient of Foster et al. (2021). Keywords: Contextual <b>bandits,</b> information-directed sampling, decision estimation coefficient, first-order regret bounds.</p></p class="citation"></blockquote><h3 id=3856--74190-distributionally-robust-off-dynamics-reinforcement-learning-provable-efficiency-with-linear-function-approximation-zhishuai-liu-et-al-2024>(38/56 | 74/190) Distributionally Robust Off-Dynamics Reinforcement Learning: Provable Efficiency with Linear Function Approximation (Zhishuai Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhishuai Liu, Pan Xu. (2024)<br><strong>Distributionally Robust Off-Dynamics Reinforcement Learning: Provable Efficiency with Linear Function Approximation</strong><br><button class=copy-to-clipboard title="Distributionally Robust Off-Dynamics Reinforcement Learning: Provable Efficiency with Linear Function Approximation" index=74>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-74 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15399v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15399v1.pdf filename=2402.15399v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We study off-dynamics <b>Reinforcement</b> <b>Learning</b> (RL), where the policy is trained on a source domain and deployed to a distinct target domain. We aim to solve this problem via online distributionally robust Markov decision processes (DRMDPs), where the learning algorithm actively interacts with the source domain while seeking the optimal performance under the worst possible dynamics that is within an uncertainty set of the source domain&rsquo;s transition kernel. We provide the first study on online DRMDPs with function approximation for off-dynamics RL. We find that DRMDPs&rsquo; dual formulation can induce nonlinearity, even when the nominal transition kernel is linear, leading to error propagation. By designing a $d$-rectangular uncertainty set using the total variation distance, we remove this additional nonlinearity and bypass the error propagation. We then introduce DR-LSVI-UCB, the first provably efficient online DRMDP algorithm for off-dynamics RL with function approximation, and establish a polynomial suboptimality bound that is independent of the state and action space sizes. Our work makes the first step towards a deeper understanding of the provable efficiency of online DRMDPs with linear function approximation. Finally, we substantiate the performance and robustness of DR-LSVI-UCB through different numerical experiments.</p></p class="citation"></blockquote><h3 id=3956--75190-offline-inverse-rl-new-solution-concepts-and-provably-efficient-algorithms-filippo-lazzati-et-al-2024>(39/56 | 75/190) Offline Inverse RL: New Solution Concepts and Provably Efficient Algorithms (Filippo Lazzati et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Filippo Lazzati, Mirco Mutti, Alberto Maria Metelli. (2024)<br><strong>Offline Inverse RL: New Solution Concepts and Provably Efficient Algorithms</strong><br><button class=copy-to-clipboard title="Offline Inverse RL: New Solution Concepts and Provably Efficient Algorithms" index=75>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-75 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15392v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15392v1.pdf filename=2402.15392v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Inverse <b>reinforcement</b> <b>learning</b> (IRL) aims to recover the reward function of an expert agent from demonstrations of behavior. It is well known that the IRL problem is fundamentally ill-posed, i.e., many reward functions can explain the demonstrations. For this reason, IRL has been recently reframed in terms of estimating the feasible reward set, thus, postponing the selection of a single reward. However, so far, the available formulations and algorithmic solutions have been proposed and analyzed mainly for the online setting, where the learner can interact with the environment and query the expert at will. This is clearly unrealistic in most practical applications, where the availability of an offline dataset is a much more common scenario. In this paper, we introduce a novel notion of feasible reward set capturing the opportunities and limitations of the offline setting and we analyze the complexity of its estimation. This requires the introduction an original learning framework that copes with the intrinsic difficulty of the setting, for which the data coverage is not under control. Then, we propose two computationally and statistically efficient algorithms, IRLO and PIRLO, for addressing the problem. In particular, the latter adopts a specific form of pessimism to enforce the novel desirable property of inclusion monotonicity of the delivered feasible set. With this work, we aim to provide a panorama of the challenges of the offline IRL problem and how they can be fruitfully addressed.</p></p class="citation"></blockquote><h3 id=4056--76190-information-theoretic-safe-bayesian-optimization-alessandro-g-bottero-et-al-2024>(40/56 | 76/190) Information-Theoretic Safe Bayesian Optimization (Alessandro G. Bottero et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Alessandro G. Bottero, Carlos E. Luis, Julia Vinogradska, Felix Berkenkamp, Jan Peters. (2024)<br><strong>Information-Theoretic Safe Bayesian Optimization</strong><br><button class=copy-to-clipboard title="Information-Theoretic Safe Bayesian Optimization" index=76>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-76 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG, stat-ML<br>Keyword Score: 10<br>Keywords: Gaussian Process<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15347v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15347v1.pdf filename=2402.15347v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We consider a sequential decision making task, where the goal is to optimize an unknown function without evaluating parameters that violate an a~priori unknown (safety) constraint. A common approach is to place a <b>Gaussian</b> <b>process</b> prior on the unknown functions and allow evaluations only in regions that are safe with high probability. Most current methods rely on a discretization of the domain and cannot be directly extended to the continuous case. Moreover, the way in which they exploit regularity assumptions about the constraint introduces an additional critical hyperparameter. In this paper, we propose an information-theoretic safe exploration criterion that directly exploits the GP posterior to identify the most informative safe parameters to evaluate. The combination of this exploration criterion with a well known Bayesian optimization acquisition function yields a novel safe Bayesian optimization selection criterion. Our approach is naturally applicable to continuous domains and does not require additional explicit hyperparameters. We theoretically analyze the method and show that we do not violate the safety constraint with high probability and that we learn about the value of the safe optimum up to arbitrary precision. Empirical evaluations demonstrate improved data-efficiency and scalability.</p></p class="citation"></blockquote><h3 id=4156--77190-categorical-deep-learning-an-algebraic-theory-of-architectures-bruno-gavranović-et-al-2024>(41/56 | 77/190) Categorical Deep Learning: An Algebraic Theory of Architectures (Bruno Gavranović et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bruno Gavranović, Paul Lessard, Andrew Dudzik, Tamara von Glehn, João G. M. Araújo, Petar Veličković. (2024)<br><strong>Categorical Deep Learning: An Algebraic Theory of Architectures</strong><br><button class=copy-to-clipboard title="Categorical Deep Learning: An Algebraic Theory of Architectures" index=77>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-77 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG, math-CT, math-RA, stat-ML<br>Keyword Score: 10<br>Keywords: Recurrent Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15332v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15332v1.pdf filename=2402.15332v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present our position on the elusive quest for a general-purpose framework for specifying and studying deep learning architectures. Our opinion is that the key attempts made so far lack a coherent bridge between specifying constraints which models must satisfy and specifying their implementations. Focusing on building a such a bridge, we propose to apply category theory &ndash; precisely, the universal algebra of monads valued in a 2-category of parametric maps &ndash; as a single theory elegantly subsuming both of these flavours of neural network design. To defend our position, we show how this theory recovers constraints induced by geometric deep learning, as well as implementations of many architectures drawn from the diverse landscape of neural networks, such as <b>RNNs.</b> We also illustrate how the theory naturally encodes many standard constructs in computer science and automata theory.</p></p class="citation"></blockquote><h3 id=4256--78190-linear-dynamics-embedded-neural-network-for-long-sequence-modeling-tongyi-liang-et-al-2024>(42/56 | 78/190) Linear Dynamics-embedded Neural Network for Long-Sequence Modeling (Tongyi Liang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tongyi Liang, Han-Xiong Li. (2024)<br><strong>Linear Dynamics-embedded Neural Network for Long-Sequence Modeling</strong><br><button class=copy-to-clipboard title="Linear Dynamics-embedded Neural Network for Long-Sequence Modeling" index=78>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-78 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Convolution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15290v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15290v1.pdf filename=2402.15290v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The trade-off between performance and computational efficiency in long-sequence modeling becomes a bottleneck for existing models. Inspired by the continuous state space models (SSMs) with multi-input and multi-output in control theory, we propose a new neural network called Linear Dynamics-embedded Neural Network (LDNN). SSMs&rsquo; continuous, discrete, and <b>convolutional</b> properties enable LDNN to have few parameters, flexible inference, and efficient training in long-sequence tasks. Two efficient strategies, diagonalization and $&rsquo;\text{Disentanglement then Fast Fourier Transform (FFT)}&rsquo;$, are developed to reduce the time complexity of <b>convolution</b> from $O(LNH\max{L, N})$ to $O(LN\max {H, \log L})$. We further improve LDNN through bidirectional noncausal and multi-head settings to accommodate a broader range of applications. Extensive experiments on the Long Range Arena (LRA) demonstrate the effectiveness and state-of-the-art performance of LDNN.</p></p class="citation"></blockquote><h3 id=4356--79190-calibration-of-deep-learning-classification-models-in-fnirs-zhihao-cao-et-al-2024>(43/56 | 79/190) Calibration of Deep Learning Classification Models in fNIRS (Zhihao Cao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhihao Cao, Zizhou Luo. (2024)<br><strong>Calibration of Deep Learning Classification Models in fNIRS</strong><br><button class=copy-to-clipboard title="Calibration of Deep Learning Classification Models in fNIRS" index=79>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-79 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, eess-SP<br>Keyword Score: 10<br>Keywords: Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15266v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15266v1.pdf filename=2402.15266v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Functional near-infrared spectroscopy (fNIRS) is a valuable non-invasive tool for monitoring brain activity. The classification of fNIRS data in relation to conscious activity holds significance for advancing our understanding of the brain and facilitating the development of brain-computer interfaces (BCI). Many researchers have turned to deep learning to tackle the classification challenges inherent in fNIRS data due to its strong generalization and robustness. In the application of fNIRS, reliability is really important, and one mathematical formulation of the reliability of confidence is calibration. However, many researchers overlook the important issue of calibration. To address this gap, we propose integrating calibration into fNIRS field and assess the reliability of existing models. Surprisingly, our results indicate poor calibration performance in many proposed models. To advance calibration development in the fNIRS field, we <b>summarize</b> three practical tips. Through this letter, we hope to emphasize the critical role of calibration in fNIRS research and argue for enhancing the reliability of deep learning-based predictions in fNIRS classification tasks. All data from our experimental process are openly available on GitHub.</p></p class="citation"></blockquote><h3 id=4456--80190-dynamic-memory-based-adaptive-optimization-balázs-szegedy-et-al-2024>(44/56 | 80/190) Dynamic Memory Based Adaptive Optimization (Balázs Szegedy et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Balázs Szegedy, Domonkos Czifra, Péter Kőrösi-Szabó. (2024)<br><strong>Dynamic Memory Based Adaptive Optimization</strong><br><button class=copy-to-clipboard title="Dynamic Memory Based Adaptive Optimization" index=80>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-80 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG, math-OC<br>Keyword Score: 10<br>Keywords: Stochastic Gradient Descent<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15262v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15262v1.pdf filename=2402.15262v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Define an optimizer as having memory $k$ if it stores $k$ dynamically changing vectors in the parameter space. Classical <b>SGD</b> has memory $0$, momentum <b>SGD</b> optimizer has $1$ and Adam optimizer has $2$. We address the following questions: How can optimizers make use of more memory units? What information should be stored in them? How to use them for the learning steps? As an approach to the last question, we introduce a general method called &ldquo;Retrospective Learning Law Correction&rdquo; or shortly RLLC. This method is designed to calculate a dynamically varying linear combination (called learning law) of memory units, which themselves may evolve arbitrarily. We demonstrate RLLC on optimizers whose memory units have linear update rules and small memory ($\leq 4$ memory units). Our experiments show that in a variety of standard problems, these optimizers outperform the above mentioned three classical optimizers. We conclude that RLLC is a promising framework for boosting the performance of known optimizers by adding more memory units and by making them more adaptive.</p></p class="citation"></blockquote><h3 id=4556--81190-a-bargaining-based-approach-for-feature-trading-in-vertical-federated-learning-yue-cui-et-al-2024>(45/56 | 81/190) A Bargaining-based Approach for Feature Trading in Vertical Federated Learning (Yue Cui et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yue Cui, Liuyi Yao, Zitao Li, Yaliang Li, Bolin Ding, Xiaofang Zhou. (2024)<br><strong>A Bargaining-based Approach for Feature Trading in Vertical Federated Learning</strong><br><button class=copy-to-clipboard title="A Bargaining-based Approach for Feature Trading in Vertical Federated Learning" index=81>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-81 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs-MA, cs.LG<br>Keyword Score: 10<br>Keywords: Federated Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15247v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15247v1.pdf filename=2402.15247v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Vertical <b>Federated</b> <b>Learning</b> (VFL) has emerged as a popular machine learning paradigm, enabling model training across the data and the task parties with different features about the same user set while preserving data privacy. In production environment, VFL usually involves one task party and one data party. Fair and economically efficient feature trading is crucial to the commercialization of VFL, where the task party is considered as the data consumer who buys the data party&rsquo;s features. However, current VFL feature trading practices often price the data party&rsquo;s data as a whole and assume transactions occur prior to the performing VFL. Neglecting the performance gains resulting from traded features may lead to underpayment and overpayment issues. In this study, we propose a bargaining-based feature trading approach in VFL to encourage economically efficient transactions. Our model incorporates performance gain-based pricing, taking into account the revenue-based optimization objectives of both parties. We analyze the proposed bargaining model under perfect and imperfect performance information settings, proving the existence of an equilibrium that optimizes the parties&rsquo; objectives. Moreover, we develop performance gain estimation-based bargaining strategies for imperfect performance information scenarios and discuss potential security issues and solutions. Experiments on three real-world datasets demonstrate the effectiveness of the proposed bargaining model.</p></p class="citation"></blockquote><h3 id=4656--82190-which-model-to-transfer-a-survey-on-transferability-estimation-yuhe-ding-et-al-2024>(46/56 | 82/190) Which Model to Transfer? A Survey on Transferability Estimation (Yuhe Ding et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuhe Ding, Bo Jiang, Aijing Yu, Aihua Zheng, Jian Liang. (2024)<br><strong>Which Model to Transfer? A Survey on Transferability Estimation</strong><br><button class=copy-to-clipboard title="Which Model to Transfer? A Survey on Transferability Estimation" index=82>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-82 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CV, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Transfer Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15231v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15231v1.pdf filename=2402.15231v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Transfer</b> <b>learning</b> methods endeavor to leverage relevant knowledge from existing source pre-trained models or datasets to solve downstream target tasks. With the increase in the scale and quantity of available pre-trained models nowadays, it becomes critical to assess in advance whether they are suitable for a specific target task. Model transferability estimation is an emerging and growing area of interest, aiming to propose a metric to quantify this suitability without training them individually, which is computationally prohibitive. Despite extensive recent advances already devoted to this area, they have custom terminological definitions and experimental settings. In this survey, we present the first review of existing advances in this area and categorize them into two separate realms: source-free model transferability estimation and source-dependent model transferability estimation. Each category is systematically defined, accompanied by a comprehensive taxonomy. Besides, we address challenges and outline future research directions, intending to provide a comprehensive guide to aid researchers and practitioners.</p></p class="citation"></blockquote><h3 id=4756--83190-fixed-random-classifier-rearrangement-for-continual-learning-shengyang-huang-et-al-2024>(47/56 | 83/190) Fixed Random Classifier Rearrangement for Continual Learning (Shengyang Huang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shengyang Huang, Jianwen Mo. (2024)<br><strong>Fixed Random Classifier Rearrangement for Continual Learning</strong><br><button class=copy-to-clipboard title="Fixed Random Classifier Rearrangement for Continual Learning" index=83>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-83 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Continual Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15227v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15227v1.pdf filename=2402.15227v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the explosive growth of data, <b>continual</b> <b>learning</b> capability is increasingly important for neural networks. Due to catastrophic forgetting, neural networks inevitably forget the knowledge of old tasks after learning new ones. In visual classification scenario, a common practice of alleviating the forgetting is to constrain the backbone. However, the impact of classifiers is underestimated. In this paper, we analyze the variation of model predictions in sequential binary classification tasks and find that the norm of the equivalent one-class classifiers significantly affects the forgetting level. Based on this conclusion, we propose a two-stage <b>continual</b> <b>learning</b> algorithm named Fixed Random Classifier Rearrangement (FRCR). In first stage, FRCR replaces the learnable classifiers with fixed random classifiers, constraining the norm of the equivalent one-class classifiers without affecting the performance of the network. In second stage, FRCR rearranges the entries of new classifiers to implicitly reduce the drift of old latent representations. The experimental results on multiple datasets show that FRCR significantly mitigates the model forgetting; subsequent experimental analyses further validate the effectiveness of the algorithm.</p></p class="citation"></blockquote><h3 id=4856--84190-bidirectional-uncertainty-based-active-learning-for-open-set-annotation-chen-chen-zong-et-al-2024>(48/56 | 84/190) Bidirectional Uncertainty-Based Active Learning for Open Set Annotation (Chen-Chen Zong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chen-Chen Zong, Ye-Wen Wang, Kun-Peng Ning, Haibo Ye, Sheng-Jun Huang. (2024)<br><strong>Bidirectional Uncertainty-Based Active Learning for Open Set Annotation</strong><br><button class=copy-to-clipboard title="Bidirectional Uncertainty-Based Active Learning for Open Set Annotation" index=84>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-84 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Active Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15198v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15198v1.pdf filename=2402.15198v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Active</b> <b>learning</b> (AL) in open set scenarios presents a novel challenge of identifying the most valuable examples in an unlabeled data pool that comprises data from both known and unknown classes. Traditional methods prioritize selecting informative examples with low confidence, with the risk of mistakenly selecting unknown-class examples with similarly low confidence. Recent methods favor the most probable known-class examples, with the risk of picking simple already mastered examples. In this paper, we attempt to query examples that are both likely from known classes and highly informative, and propose a \textit{Bidirectional Uncertainty-based <b>Active</b> <b>Learning}</b> (BUAL) framework. Specifically, we achieve this by first pushing the unknown class examples toward regions with high-confidence predictions with our proposed \textit{Random Label Negative Learning} method. Then, we propose a \textit{Bidirectional Uncertainty sampling} strategy by jointly estimating uncertainty posed by both positive and negative learning to perform consistent and stable sampling. BUAL successfully extends existing uncertainty-based AL methods to complex open-set scenarios. Extensive experiments on multiple datasets with varying openness demonstrate that BUAL achieves state-of-the-art performance.</p></p class="citation"></blockquote><h3 id=4956--85190-unified-view-of-grokking-double-descent-and-emergent-abilities-a-perspective-from-circuits-competition-yufei-huang-et-al-2024>(49/56 | 85/190) Unified View of Grokking, Double Descent and Emergent Abilities: A Perspective from Circuits Competition (Yufei Huang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yufei Huang, Shengding Hu, Xu Han, Zhiyuan Liu, Maosong Sun. (2024)<br><strong>Unified View of Grokking, Double Descent and Emergent Abilities: A Perspective from Circuits Competition</strong><br><button class=copy-to-clipboard title="Unified View of Grokking, Double Descent and Emergent Abilities: A Perspective from Circuits Competition" index=85>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-85 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15175v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15175v1.pdf filename=2402.15175v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent studies have uncovered intriguing phenomena in deep learning, such as grokking, double descent, and emergent abilities in <b>large</b> <b>language</b> <b>models,</b> which challenge human intuition and are crucial for a deeper understanding of neural models. In this paper, we present a comprehensive framework that provides a unified view of these three phenomena, focusing on the competition between memorization and generalization circuits. This approach, initially employed to explain grokking, is extended in our work to encompass a wider range of model sizes and training data volumes. Our framework delineates four distinct training dynamics, each depending on varying combinations of model size and training data quantity. Utilizing this framework, we provide a detailed analysis of the double descent phenomenon and propose two verifiable predictions regarding its occurrence, both substantiated by our experimental results. Moreover, we expand our framework to the multi-task learning paradigm, demonstrating how algorithm tasks can be turned into emergent abilities. This offers a novel perspective to understand emergent abilities in <b>Large</b> <b>Language</b> <b>Models.</b></p></p class="citation"></blockquote><h3 id=5056--86190-covariance-adaptive-least-squares-algorithm-for-stochastic-combinatorial-semi-bandits-julien-zhou-et-al-2024>(50/56 | 86/190) Covariance-Adaptive Least-Squares Algorithm for Stochastic Combinatorial Semi-Bandits (Julien Zhou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Julien Zhou, Pierre Gaillard, Thibaud Rahier, Houssam Zenati, Julyan Arbel. (2024)<br><strong>Covariance-Adaptive Least-Squares Algorithm for Stochastic Combinatorial Semi-Bandits</strong><br><button class=copy-to-clipboard title="Covariance-Adaptive Least-Squares Algorithm for Stochastic Combinatorial Semi-Bandits" index=86>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-86 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, math-ST, stat-ML, stat-TH<br>Keyword Score: 10<br>Keywords: Bandit Algorithm<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15171v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15171v1.pdf filename=2402.15171v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We address the problem of stochastic combinatorial semi-bandits, where a player can select from P subsets of a set containing d base items. Most existing algorithms (e.g. CUCB, ESCB, OLS-UCB) require prior knowledge on the reward distribution, like an upper bound on a sub-Gaussian proxy-variance, which is hard to estimate tightly. In this work, we design a variance-adaptive version of OLS-UCB, relying on an online estimation of the covariance structure. Estimating the coefficients of a covariance matrix is much more manageable in practical settings and results in improved regret upper bounds compared to proxy variance-based algorithms. When covariance coefficients are all non-negative, we show that our approach efficiently leverages the semi-bandit feedback and provably outperforms <b>bandit</b> feedback approaches, not only in exponential regimes where P $\gg$ d but also when P $\le$ d, which is not straightforward from most existing analyses.</p></p class="citation"></blockquote><h3 id=5156--87190-the-surprising-effectiveness-of-skip-tuning-in-diffusion-sampling-jiajun-ma-et-al-2024>(51/56 | 87/190) The Surprising Effectiveness of Skip-Tuning in Diffusion Sampling (Jiajun Ma et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiajun Ma, Shuchen Xue, Tianyang Hu, Wenjia Wang, Zhaoqiang Liu, Zhenguo Li, Zhi-Ming Ma, Kenji Kawaguchi. (2024)<br><strong>The Surprising Effectiveness of Skip-Tuning in Diffusion Sampling</strong><br><button class=copy-to-clipboard title="The Surprising Effectiveness of Skip-Tuning in Diffusion Sampling" index=87>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-87 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Probabilistic Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15170v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15170v1.pdf filename=2402.15170v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the incorporation of the UNet architecture, diffusion <b>probabilistic</b> <b>models</b> have become a dominant force in image generation tasks. One key design in UNet is the skip connections between the encoder and decoder blocks. Although skip connections have been shown to improve training stability and model performance, we reveal that such shortcuts can be a limiting factor for the complexity of the transformation. As the sampling steps decrease, the generation process and the role of the UNet get closer to the push-forward transformations from Gaussian distribution to the target, posing a challenge for the network&rsquo;s complexity. To address this challenge, we propose Skip-Tuning, a simple yet surprisingly effective training-free tuning method on the skip connections. Our method can achieve 100% FID improvement for pretrained EDM on ImageNet 64 with only 19 NFEs (1.75), breaking the limit of ODE samplers regardless of sampling steps. Surprisingly, the improvement persists when we increase the number of sampling steps and can even surpass the best result from EDM-2 (1.58) with only 39 NFEs (1.57). Comprehensive exploratory experiments are conducted to shed light on the surprising effectiveness. We observe that while Skip-Tuning increases the score-matching losses in the pixel space, the losses in the feature space are reduced, particularly at intermediate noise levels, which coincide with the most effective range accounting for image quality improvement.</p></p class="citation"></blockquote><h3 id=5256--88190-multi-armed-bandits-with-abstention-junwen-yang-et-al-2024>(52/56 | 88/190) Multi-Armed Bandits with Abstention (Junwen Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Junwen Yang, Tianyuan Jin, Vincent Y. F. Tan. (2024)<br><strong>Multi-Armed Bandits with Abstention</strong><br><button class=copy-to-clipboard title="Multi-Armed Bandits with Abstention" index=88>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-88 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-IT, cs-LG, cs.LG, math-IT, stat-ML<br>Keyword Score: 10<br>Keywords: Bandit Algorithm<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15127v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15127v1.pdf filename=2402.15127v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce a novel extension of the canonical multi-armed <b>bandit</b> problem that incorporates an additional strategic element: abstention. In this enhanced framework, the agent is not only tasked with selecting an arm at each time step, but also has the option to abstain from accepting the stochastic instantaneous reward before observing it. When opting for abstention, the agent either suffers a fixed regret or gains a guaranteed reward. Given this added layer of complexity, we ask whether we can develop efficient algorithms that are both asymptotically and minimax optimal. We answer this question affirmatively by designing and analyzing algorithms whose regrets meet their corresponding information-theoretic lower bounds. Our results offer valuable quantitative insights into the benefits of the abstention option, laying the groundwork for further exploration in other online decision-making problems with such an option. Numerical results further corroborate our theoretical findings.</p></p class="citation"></blockquote><h3 id=5356--89190-machine-unlearning-by-suppressing-sample-contribution-xinwen-cheng-et-al-2024>(53/56 | 89/190) Machine Unlearning by Suppressing Sample Contribution (Xinwen Cheng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xinwen Cheng, Zhehao Huang, Xiaolin Huang. (2024)<br><strong>Machine Unlearning by Suppressing Sample Contribution</strong><br><button class=copy-to-clipboard title="Machine Unlearning by Suppressing Sample Contribution" index=89>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-89 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Machine Unlearning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15109v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15109v1.pdf filename=2402.15109v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Machine</b> <b>Unlearning</b> (MU) is to forget data from a well-trained model, which is practically important due to the &ldquo;right to be forgotten&rdquo;. In this paper, we start from the fundamental distinction between training data and unseen data on their contribution to the model: the training data contributes to the final model while the unseen data does not. We theoretically discover that the input sensitivity can approximately measure the contribution and practically design an algorithm, called MU-Mis <b>(machine</b> <b>unlearning</b> via minimizing input sensitivity), to suppress the contribution of the forgetting data. Experimental results demonstrate that MU-Mis outperforms state-of-the-art MU methods significantly. Additionally, MU-Mis aligns more closely with the application of MU as it does not require the use of remaining data.</p></p class="citation"></blockquote><h3 id=5456--90190-fourier-basis-density-model-alfredo-de-la-fuente-et-al-2024>(54/56 | 90/190) Fourier Basis Density Model (Alfredo De la Fuente et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Alfredo De la Fuente, Saurabh Singh, Johannes Ballé. (2024)<br><strong>Fourier Basis Density Model</strong><br><button class=copy-to-clipboard title="Fourier Basis Density Model" index=90>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-90 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-ML<br>Keyword Score: 3<br>Keywords: Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15345v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15345v1.pdf filename=2402.15345v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce a lightweight, flexible and end-to-end trainable probability density model parameterized by a constrained Fourier basis. We assess its performance at approximating a range of <b>multi-modal</b> 1D densities, which are generally difficult to fit. In comparison to the deep factorized model introduced in [1], our model achieves a lower cross entropy at a similar computational budget. In addition, we also evaluate our method on a toy compression task, demonstrating its utility in learned compression.</p></p class="citation"></blockquote><h3 id=5556--91190-towards-principled-task-grouping-for-multi-task-learning-chenguang-wang-et-al-2024>(55/56 | 91/190) Towards Principled Task Grouping for Multi-Task Learning (Chenguang Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chenguang Wang, Xuanhao Pan, Tianshu Yu. (2024)<br><strong>Towards Principled Task Grouping for Multi-Task Learning</strong><br><button class=copy-to-clipboard title="Towards Principled Task Grouping for Multi-Task Learning" index=91>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-91 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15328v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15328v1.pdf filename=2402.15328v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents a novel approach to task grouping in Multitask Learning (MTL), advancing beyond existing methods by addressing key theoretical and practical limitations. Unlike prior studies, our approach offers a more theoretically grounded method that does not rely on restrictive assumptions for constructing transfer gains. We also propose a flexible mathematical programming formulation which can accommodate a wide spectrum of resource constraints, thus enhancing its versatility. Experimental results across diverse domains, including computer vision datasets, combinatorial optimization <b>benchmarks</b> and time series tasks, demonstrate the superiority of our method over extensive baselines, validating its effectiveness and general applicability in MTL.</p></p class="citation"></blockquote><h3 id=5656--92190-convergence-analysis-of-blurring-mean-shift-ryoya-yamasaki-et-al-2024>(56/56 | 92/190) Convergence Analysis of Blurring Mean Shift (Ryoya Yamasaki et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ryoya Yamasaki, Toshiyuki Tanaka. (2024)<br><strong>Convergence Analysis of Blurring Mean Shift</strong><br><button class=copy-to-clipboard title="Convergence Analysis of Blurring Mean Shift" index=92>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-92 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CV, cs-LG, cs.LG<br>Keyword Score: 3<br>Keywords: Clustering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15146v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15146v1.pdf filename=2402.15146v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Blurring mean shift (BMS) algorithm, a variant of the mean shift algorithm, is a kernel-based iterative method for data <b>clustering,</b> where data points are clustered according to their convergent points via iterative blurring. In this paper, we analyze convergence properties of the BMS algorithm by leveraging its interpretation as an optimization procedure, which is known but has been underutilized in existing convergence studies. Whereas existing results on convergence properties applicable to multi-dimensional data only cover the case where all the blurred data point sequences converge to a single point, this study provides a convergence guarantee even when those sequences can converge to multiple points, yielding multiple clusters. This study also shows that the convergence of the BMS algorithm is fast by further leveraging geometrical characterization of the convergent points.</p></p class="citation"></blockquote><h2 id=csro-9>cs.RO (9)</h2><h3 id=19--93190-predilect-preferences-delineated-with-zero-shot-language-based-reasoning-in-reinforcement-learning-simon-holk-et-al-2024>(1/9 | 93/190) PREDILECT: Preferences Delineated with Zero-Shot Language-based Reasoning in Reinforcement Learning (Simon Holk et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Simon Holk, Daniel Marta, Iolanda Leite. (2024)<br><strong>PREDILECT: Preferences Delineated with Zero-Shot Language-based Reasoning in Reinforcement Learning</strong><br><button class=copy-to-clipboard title="PREDILECT: Preferences Delineated with Zero-Shot Language-based Reasoning in Reinforcement Learning" index=93>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-93 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-CL, cs-LG, cs-RO, cs.RO<br>Keyword Score: 60<br>Keywords: Reinforcement Learning, Zero-shot, Reasoning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15420v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15420v1.pdf filename=2402.15420v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Preference-based <b>reinforcement</b> <b>learning</b> (RL) has emerged as a new field in robot learning, where humans play a pivotal role in shaping robot behavior by expressing preferences on different sequences of state-action pairs. However, formulating realistic policies for robots demands responses from humans to an extensive array of queries. In this work, we approach the sample-efficiency challenge by expanding the information collected per query to contain both preferences and optional text <b>prompting.</b> To accomplish this, we leverage the <b>zero-shot</b> capabilities of a <b>large</b> <b>language</b> <b>model</b> <b>(LLM)</b> to reason from the text provided by humans. To accommodate the additional query information, we reformulate the reward learning objectives to contain flexible highlights &ndash; state-action pairs that contain relatively high information and are related to the features processed in a <b>zero-shot</b> fashion from a pretrained <b>LLM.</b> In both a simulated scenario and a user study, we reveal the effectiveness of our work by analyzing the feedback and its implications. Additionally, the collective feedback collected serves to train a robot on socially compliant trajectories in a simulated social navigation landscape. We provide video examples of the trained policies at <a href=https://sites.google.com/view/rl-predilect>https://sites.google.com/view/rl-predilect</a></p></p class="citation"></blockquote><h3 id=29--94190-safe-task-planning-for-language-instructed-multi-robot-systems-using-conformal-prediction-jun-wang-et-al-2024>(2/9 | 94/190) Safe Task Planning for Language-Instructed Multi-Robot Systems using Conformal Prediction (Jun Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jun Wang, Guocheng He, Yiannis Kantaros. (2024)<br><strong>Safe Task Planning for Language-Instructed Multi-Robot Systems using Conformal Prediction</strong><br><button class=copy-to-clipboard title="Safe Task Planning for Language-Instructed Multi-Robot Systems using Conformal Prediction" index=94>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-94 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-RO, cs.RO<br>Keyword Score: 25<br>Keywords: Black Box, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15368v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15368v1.pdf filename=2402.15368v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper addresses task planning problems for language-instructed robot teams. Tasks are expressed in natural language (NL), requiring the robots to apply their capabilities (e.g., mobility, manipulation, and sensing) at various locations and semantic objects. Several recent works have addressed similar planning problems by leveraging pre-trained <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> to design effective multi-robot plans. However, these approaches lack mission performance and safety guarantees. To address this challenge, we introduce a new decentralized <b>LLM-based</b> planner that is capable of achieving high mission success rates. This is accomplished by leveraging conformal prediction (CP), a distribution-free uncertainty quantification tool in <b>black-box</b> <b>models.</b> CP allows the proposed multi-robot planner to reason about its inherent uncertainty in a decentralized fashion, enabling robots to make individual decisions when they are sufficiently certain and seek help otherwise. We show, both theoretically and empirically, that the proposed planner can achieve user-specified task success rates while minimizing the overall number of help requests. We demonstrate the performance of our approach on multi-robot home service applications. We also show through comparative experiments, that our method outperforms recent centralized and decentralized multi-robot <b>LLM-based</b> planners in terms of in terms of its ability to design correct plans. The advantage of our algorithm over baselines becomes more pronounced with increasing mission complexity and robot team size.</p></p class="citation"></blockquote><h3 id=39--95190-roboexp-action-conditioned-scene-graph-via-interactive-exploration-for-robotic-manipulation-hanxiao-jiang-et-al-2024>(3/9 | 95/190) RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for Robotic Manipulation (Hanxiao Jiang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hanxiao Jiang, Binghao Huang, Ruihai Wu, Zhuoran Li, Shubham Garg, Hooshang Nayyeri, Shenlong Wang, Yunzhu Li. (2024)<br><strong>RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for Robotic Manipulation</strong><br><button class=copy-to-clipboard title="RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for Robotic Manipulation" index=95>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-95 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-CV, cs-LG, cs-RO, cs.RO<br>Keyword Score: 24<br>Keywords: Graph, Geometry, Multi-modal, Multi-modal, Zero-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15487v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15487v1.pdf filename=2402.15487v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Robots need to explore their surroundings to adapt to and tackle tasks in unknown environments. Prior work has proposed building scene <b>graphs</b> of the environment but typically assumes that the environment is static, omitting regions that require active interactions. This severely limits their ability to handle more complex tasks in household and office environments: before setting up a table, robots must explore drawers and cabinets to locate all utensils and condiments. In this work, we introduce the novel task of interactive scene exploration, wherein robots autonomously explore environments and produce an action-conditioned scene <b>graph</b> (ACSG) that captures the structure of the underlying environment. The ACSG accounts for both low-level information, such as <b>geometry</b> and semantics, and high-level information, such as the action-conditioned relationships between different entities in the scene. To this end, we present the Robotic Exploration (RoboEXP) system, which incorporates the Large <b>Multimodal</b> Model (LMM) and an explicit memory design to enhance our system&rsquo;s capabilities. The robot reasons about what and how to explore an object, accumulating new information through the interaction process and incrementally constructing the ACSG. We apply our system across various real-world settings in a <b>zero-shot</b> manner, demonstrating its effectiveness in exploring and modeling environments it has never seen before. Leveraging the constructed ACSG, we illustrate the effectiveness and efficiency of our RoboEXP system in facilitating a wide range of real-world manipulation tasks involving rigid, articulated objects, nested objects like Matryoshka dolls, and deformable objects like cloth.</p></p class="citation"></blockquote><h3 id=49--96190-dynamics-guided-diffusion-model-for-robot-manipulator-design-xiaomeng-xu-et-al-2024>(4/9 | 96/190) Dynamics-Guided Diffusion Model for Robot Manipulator Design (Xiaomeng Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiaomeng Xu, Huy Ha, Shuran Song. (2024)<br><strong>Dynamics-Guided Diffusion Model for Robot Manipulator Design</strong><br><button class=copy-to-clipboard title="Dynamics-Guided Diffusion Model for Robot Manipulator Design" index=96>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-96 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-LG, cs-RO, cs.RO<br>Keyword Score: 15<br>Keywords: Diffusion Model, Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15038v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15038v1.pdf filename=2402.15038v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present Dynamics-Guided <b>Diffusion</b> <b>Model,</b> a data-driven framework for generating manipulator <b>geometry</b> designs for a given manipulation task. Instead of training different design models for each task, our approach employs a learned dynamics network shared across tasks. For a new manipulation task, we first decompose it into a collection of individual motion targets which we call target interaction profile, where each individual motion can be modeled by the shared dynamics network. The design objective constructed from the target and predicted interaction profiles provides a gradient to guide the refinement of finger <b>geometry</b> for the task. This refinement process is executed as a classifier-guided <b>diffusion</b> <b>process,</b> where the design objective acts as the classifier guidance. We evaluate our framework on various manipulation tasks, under the sensor-less setting using only an open-loop parallel jaw motion. Our generated designs outperform optimization-based and unguided <b>diffusion</b> <b>baselines</b> relatively by 31.5% and 45.3% on average manipulation success rate. With the ability to generate a design within 0.8 seconds, our framework could facilitate rapid design iteration and enhance the adoption of data-driven approaches for robotic mechanism design.</p></p class="citation"></blockquote><h3 id=59--97190-grasp-see-and-place-efficient-unknown-object-rearrangement-with-policy-structure-prior-kechun-xu-et-al-2024>(5/9 | 97/190) Grasp, See and Place: Efficient Unknown Object Rearrangement with Policy Structure Prior (Kechun Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kechun Xu, Zhongxiang Zhou, Jun Wu, Haojian Lu, Rong Xiong, Yue Wang. (2024)<br><strong>Grasp, See and Place: Efficient Unknown Object Rearrangement with Policy Structure Prior</strong><br><button class=copy-to-clipboard title="Grasp, See and Place: Efficient Unknown Object Rearrangement with Policy Structure Prior" index=97>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-97 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-LG, cs-RO, cs.RO<br>Keyword Score: 10<br>Keywords: Foundation Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15402v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15402v1.pdf filename=2402.15402v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We focus on the task of unknown object rearrangement, where a robot is supposed to re-configure the objects into a desired goal configuration specified by an RGB-D image. Recent works explore unknown object rearrangement systems by incorporating learning-based perception modules. However, they are sensitive to perception error, and pay less attention to task-level performance. In this paper, we aim to develop an effective system for unknown object rearrangement amidst perception noise. We theoretically reveal the noisy perception impacts grasp and place in a decoupled way, and show such a decoupled structure is non-trivial to improve task optimality. We propose GSP, a dual-loop system with the decoupled structure as prior. For the inner loop, we learn an active seeing policy for self-confident object matching to improve the perception of place. For the outer loop, we learn a grasp policy aware of object matching and grasp capability guided by task-level rewards. We leverage the <b>foundation</b> <b>model</b> CLIP for object matching, policy learning and self-termination. A series of experiments indicate that GSP can conduct unknown object rearrangement with higher completion rate and less steps.</p></p class="citation"></blockquote><h3 id=69--98190-follow-the-footprints-self-supervised-traversability-estimation-for-off-road-vehicle-navigation-based-on-geometric-and-visual-cues-yurim-jeon-et-al-2024>(6/9 | 98/190) Follow the Footprints: Self-supervised Traversability Estimation for Off-road Vehicle Navigation based on Geometric and Visual Cues (Yurim Jeon et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yurim Jeon, E In Son, Seung-Woo Seo. (2024)<br><strong>Follow the Footprints: Self-supervised Traversability Estimation for Off-road Vehicle Navigation based on Geometric and Visual Cues</strong><br><button class=copy-to-clipboard title="Follow the Footprints: Self-supervised Traversability Estimation for Off-road Vehicle Navigation based on Geometric and Visual Cues" index=98>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-98 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 10<br>Keywords: Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15363v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15363v1.pdf filename=2402.15363v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this study, we address the off-road traversability estimation problem, that predicts areas where a robot can navigate in off-road environments. An off-road environment is an unstructured environment comprising a combination of traversable and non-traversable spaces, which presents a challenge for estimating traversability. This study highlights three primary factors that affect a robot&rsquo;s traversability in an off-road environment: surface slope, semantic information, and robot platform. We present two strategies for estimating traversability, using a guide filter network (GFN) and footprint supervision module (FSM). The first strategy involves building a novel GFN using a newly designed guide filter layer. The GFN interprets the surface and semantic information from the input data and integrates them to extract features optimized for traversability estimation. The second strategy involves developing an FSM, which is a self-supervision module that utilizes the path traversed by the robot in pre-driving, also known as a footprint. This enables the prediction of traversability that reflects the characteristics of the robot platform. Based on these two strategies, the proposed method overcomes the limitations of existing methods, which require laborious human supervision and lack scalability. Extensive experiments in diverse conditions, including automobiles and unmanned ground vehicles, herbfields, woodlands, and farmlands, demonstrate that the proposed method is compatible for various robot platforms and adaptable to a range of terrains. Code is available at <a href=https://github.com/yurimjeon1892/FtFoot>https://github.com/yurimjeon1892/FtFoot</a>.</p></p class="citation"></blockquote><h3 id=79--99190-streaming-gaussian-dirichlet-random-fields-for-spatial-predictions-of-high-dimensional-categorical-observations-j-e-san-soucie-et-al-2024>(7/9 | 99/190) Streaming Gaussian Dirichlet Random Fields for Spatial Predictions of High Dimensional Categorical Observations (J. E. San Soucie et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>J. E. San Soucie, H. M. Sosik, Y. Girdhar. (2024)<br><strong>Streaming Gaussian Dirichlet Random Fields for Spatial Predictions of High Dimensional Categorical Observations</strong><br><button class=copy-to-clipboard title="Streaming Gaussian Dirichlet Random Fields for Spatial Predictions of High Dimensional Categorical Observations" index=99>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-99 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-LG, cs-RO, cs.RO<br>Keyword Score: 10<br>Keywords: Gaussian Process<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15359v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15359v1.pdf filename=2402.15359v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present the Streaming <b>Gaussian</b> <b>Dirichlet</b> Random Field (S-GDRF) model, a novel approach for modeling a stream of spatiotemporally distributed, sparse, high-dimensional categorical observations. The proposed approach efficiently learns global and local patterns in spatiotemporal data, allowing for fast inference and querying with a bounded time complexity. Using a high-resolution data series of plankton images classified with a neural network, we demonstrate the ability of the approach to make more accurate predictions compared to a Variational <b>Gaussian</b> <b>Process</b> (VGP), and to learn a predictive distribution of observations from streaming categorical data. S-GDRFs open the door to enabling efficient informative path planning over high-dimensional categorical observations, which until now has not been feasible.</p></p class="citation"></blockquote><h3 id=89--100190-clipper-a-fast-maximal-clique-algorithm-for-robust-global-registration-kaveh-fathian-et-al-2024>(8/9 | 100/190) CLIPPER+: A Fast Maximal Clique Algorithm for Robust Global Registration (Kaveh Fathian et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kaveh Fathian, Tyler Summers. (2024)<br><strong>CLIPPER+: A Fast Maximal Clique Algorithm for Robust Global Registration</strong><br><button class=copy-to-clipboard title="CLIPPER+: A Fast Maximal Clique Algorithm for Robust Global Registration" index=100>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-100 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-CV, cs-RO, cs.RO<br>Keyword Score: 6<br>Keywords: Graph, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15464v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15464v1.pdf filename=2402.15464v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present CLIPPER+, an algorithm for finding maximal cliques in unweighted <b>graphs</b> for outlier-robust global registration. The registration problem can be formulated as a <b>graph</b> and solved by finding its maximum clique. This formulation leads to extreme robustness to outliers; however, finding the maximum clique is an NP-hard problem, and therefore approximation is required in practice for large-size problems. The performance of an approximation algorithm is evaluated by its computational complexity (the lower the runtime, the better) and solution accuracy (how close the solution is to the maximum clique). Accordingly, the main contribution of CLIPPER+ is outperforming the state-of-the-art in accuracy while maintaining a relatively low runtime. CLIPPER+ builds on prior work (CLIPPER [1] and PMC [2]) and prunes the <b>graph</b> by removing vertices that have a small core number and cannot be a part of the maximum clique. This will result in a smaller <b>graph,</b> on which the maximum clique can be estimated considerably faster. We evaluate the performance of CLIPPER+ on standard <b>graph</b> <b>benchmarks,</b> as well as synthetic and real-world point cloud registration problems. These evaluations demonstrate that CLIPPER+ has the highest accuracy and can register point clouds in scenarios where over $99%$ of associations are outliers. Our code and evaluation <b>benchmarks</b> are released at <a href=https://github.com/ariarobotics/clipperp>https://github.com/ariarobotics/clipperp</a>.</p></p class="citation"></blockquote><h3 id=99--101190-neural-implicit-swept-volume-models-for-fast-collision-detection-dominik-joho-et-al-2024>(9/9 | 101/190) Neural Implicit Swept Volume Models for Fast Collision Detection (Dominik Joho et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dominik Joho, Jonas Schwinn, Kirill Safronov. (2024)<br><strong>Neural Implicit Swept Volume Models for Fast Collision Detection</strong><br><button class=copy-to-clipboard title="Neural Implicit Swept Volume Models for Fast Collision Detection" index=101>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-101 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-LG, cs-RO, cs.RO<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15281v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15281v1.pdf filename=2402.15281v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Collision detection is one of the most time-consuming operations during motion planning. Thus, there is an increasing interest in exploring machine learning techniques to speed up collision detection and sampling-based motion planning. A recent line of research focuses on utilizing neural signed distance functions of either the robot <b>geometry</b> or the swept volume of the robot motion. Building on this, we present a novel neural implicit swept volume model that is the first to continuously represent arbitrary motions parameterized by their start and goal configurations. This allows to quickly compute signed distances for any point in the task space to the robot motion. Further, we present an algorithm combining the speed of the deep learning-based signed distance computations with the strong accuracy guarantees of geometric collision checkers. We validate our approach in simulated and real-world robotic experiments, and demonstrate that it is able to speed up a commercial bin picking application.</p></p class="citation"></blockquote><h2 id=cscv-25>cs.CV (25)</h2><h3 id=125--102190-on-normalization-equivariance-properties-of-supervised-and-unsupervised-denoising-methods-a-survey-sébastien-herbreteau-et-al-2024>(1/25 | 102/190) On normalization-equivariance properties of supervised and unsupervised denoising methods: a survey (Sébastien Herbreteau et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sébastien Herbreteau, Charles Kervrann. (2024)<br><strong>On normalization-equivariance properties of supervised and unsupervised denoising methods: a survey</strong><br><button class=copy-to-clipboard title="On normalization-equivariance properties of supervised and unsupervised denoising methods: a survey" index=102>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-102 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 60<br>Keywords: Convolution, Convolutional Neural Network, Supervised Learning, Supervised Learning, Unsupervised Learning, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15352v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15352v1.pdf filename=2402.15352v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Image denoising is probably the oldest and still one of the most active research topic in image processing. Many methodological concepts have been introduced in the past decades and have improved performances significantly in recent years, especially with the emergence of <b>convolutional</b> <b>neural</b> <b>networks</b> and <b>supervised</b> <b>deep</b> learning. In this paper, we propose a survey of guided tour of <b>supervised</b> <b>and</b> <b>unsupervised</b> <b>learning</b> methods for image denoising, classifying the main principles elaborated during this evolution, with a particular concern given to recent developments in <b>supervised</b> <b>learning.</b> It is conceived as a tutorial organizing in a comprehensive framework current approaches. We give insights on the rationales and limitations of the most performant methods in the literature, and we highlight the common features between many of them. Finally, we focus on on the normalization equivariance properties that is surprisingly not guaranteed with most of <b>supervised</b> <b>methods.</b> It is of paramount importance that intensity shifting or scaling applied to the input image results in a corresponding change in the denoiser output.</p></p class="citation"></blockquote><h3 id=225--103190-label-efficient-multi-organ-segmentation-method-with-diffusion-model-yongzhi-huang-et-al-2024>(2/25 | 103/190) Label-efficient Multi-organ Segmentation Method with Diffusion Model (Yongzhi Huang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yongzhi Huang, Jinxin Zhu, Haseeb Hassan, Liyilei Su, Jingyu Li, Binding Huang. (2024)<br><strong>Label-efficient Multi-organ Segmentation Method with Diffusion Model</strong><br><button class=copy-to-clipboard title="Label-efficient Multi-organ Segmentation Method with Diffusion Model" index=103>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-103 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 50<br>Keywords: Diffusion Model, Fine-tuning, Semi-Supervised Learning, Supervised Learning, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15216v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15216v1.pdf filename=2402.15216v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Accurate segmentation of multiple organs in Computed Tomography (CT) images plays a vital role in computer-aided diagnosis systems. Various <b>supervised-learning</b> <b>approaches</b> have been proposed recently. However, these methods heavily depend on a large amount of high-quality labeled data, which is expensive to obtain in practice. In this study, we present a label-efficient learning approach using a pre-trained <b>diffusion</b> <b>model</b> for multi-organ segmentation tasks in CT images. First, a denoising <b>diffusion</b> <b>model</b> was trained using unlabeled CT data, generating additional two-dimensional (2D) CT images. Then the pre-trained denoising <b>diffusion</b> <b>network</b> was transferred to the downstream multi-organ segmentation task, effectively creating a <b>semi-supervised</b> <b>learning</b> model that requires only a small amount of labeled data. Furthermore, linear classification and <b>fine-tuning</b> decoder strategies were employed to enhance the network&rsquo;s segmentation performance. Our generative model at 256x256 resolution achieves impressive performance in terms of Fr'echet inception distance, spatial Fr'echet inception distance, and F1-score, with values of 11.32, 46.93, and 73.1%, respectively. These results affirm the <b>diffusion</b> <b>model&rsquo;s</b> ability to generate diverse and realistic 2D CT images. Additionally, our method achieves competitive multi-organ segmentation performance compared to state-of-the-art methods on the FLARE 2022 dataset, particularly in limited labeled data scenarios. Remarkably, even with only 1% and 10% labeled data, our method achieves Dice similarity coefficients (DSCs) of 71.56% and 78.51% after <b>fine-tuning,</b> respectively. The method achieves a DSC score of 51.81% using just four labeled CT scans. These results demonstrate the efficacy of our approach in overcoming the limitations of <b>supervised</b> <b>learning</b> heavily reliant on large-scale labeled data.</p></p class="citation"></blockquote><h3 id=325--104190-fine-tuning-clip-text-encoders-with-two-step-paraphrasing-hyunjae-kim-et-al-2024>(3/25 | 104/190) Fine-tuning CLIP Text Encoders with Two-step Paraphrasing (Hyunjae Kim et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hyunjae Kim, Seunghyun Yoon, Trung Bui, Handong Zhao, Quan Tran, Franck Dernoncourt, Jaewoo Kang. (2024)<br><strong>Fine-tuning CLIP Text Encoders with Two-step Paraphrasing</strong><br><button class=copy-to-clipboard title="Fine-tuning CLIP Text Encoders with Two-step Paraphrasing" index=104>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-104 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-LG, cs.CV<br>Keyword Score: 50<br>Keywords: Fine-tuning, Fine-tuning, Text2image, Large Language Model, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15120v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15120v1.pdf filename=2402.15120v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Contrastive language-image pre-training (CLIP) models have demonstrated considerable success across various <b>vision-language</b> tasks, such as <b>text-to-image</b> retrieval, where the model is required to effectively process natural language input to produce an accurate visual output. However, current models still face limitations in dealing with linguistic variations in input queries, such as paraphrases, making it challenging to handle a broad range of user queries in real-world applications. In this study, we introduce a straightforward <b>fine-tuning</b> approach to enhance the representations of CLIP models for paraphrases. Our approach involves a two-step paraphrase generation process, where we automatically create two categories of paraphrases from web-scale image captions by leveraging <b>large</b> <b>language</b> <b>models.</b> Subsequently, we <b>fine-tune</b> the CLIP text encoder using these generated paraphrases while freezing the image encoder. Our resulting model, which we call ParaCLIP, exhibits significant improvements over baseline CLIP models across various tasks, including paraphrased retrieval (with rank similarity scores improved by up to 2.0% and 5.6%), Visual Genome Relation and Attribution, as well as seven semantic textual similarity tasks.</p></p class="citation"></blockquote><h3 id=425--105190-representing-online-handwriting-for-recognition-in-large-vision-language-models-anastasiia-fadeeva-et-al-2024>(4/25 | 105/190) Representing Online Handwriting for Recognition in Large Vision-Language Models (Anastasiia Fadeeva et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Anastasiia Fadeeva, Philippe Schlattner, Andrii Maksai, Mark Collier, Efi Kokiopoulou, Jesse Berent, Claudiu Musat. (2024)<br><strong>Representing Online Handwriting for Recognition in Large Vision-Language Models</strong><br><button class=copy-to-clipboard title="Representing Online Handwriting for Recognition in Large Vision-Language Models" index=105>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-105 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-LG, cs.CV<br>Keyword Score: 40<br>Keywords: Optical Character Recognition, Optical Character Recognition, Fine-tuning, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15307v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15307v1.pdf filename=2402.15307v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The adoption of tablets with touchscreens and styluses is increasing, and a key feature is converting handwriting to text, enabling search, indexing, and AI assistance. Meanwhile, <b>vision-language</b> models (VLMs) are now the go-to solution for image understanding, thanks to both their state-of-the-art performance across a variety of tasks and the simplicity of a unified approach to training, <b>fine-tuning,</b> and inference. While VLMs obtain high performance on image-based tasks, they perform poorly on handwriting recognition when applied naively, i.e., by rendering handwriting as an image and performing <b>optical</b> <b>character</b> <b>recognition</b> <b>(OCR).</b> In this paper, we study online handwriting recognition with VLMs, going beyond naive <b>OCR.</b> We propose a novel tokenized representation of digital ink (online handwriting) that includes both a time-ordered sequence of strokes as text, and as image. We show that this representation yields results comparable to or better than state-of-the-art online handwriting recognizers. Wide applicability is shown through results with two different VLM families, on multiple public datasets. Our approach can be applied to off-the-shelf VLMs, does not require any changes in their architecture, and can be used in both <b>fine-tuning</b> and parameter-efficient tuning. We perform a detailed ablation study to identify the key elements of the proposed representation.</p></p class="citation"></blockquote><h3 id=525--106190-seeing-is-believing-mitigating-hallucination-in-large-vision-language-models-via-clip-guided-decoding-ailin-deng-et-al-2024>(5/25 | 106/190) Seeing is Believing: Mitigating Hallucination in Large Vision-Language Models via CLIP-Guided Decoding (Ailin Deng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ailin Deng, Zhirui Chen, Bryan Hooi. (2024)<br><strong>Seeing is Believing: Mitigating Hallucination in Large Vision-Language Models via CLIP-Guided Decoding</strong><br><button class=copy-to-clipboard title="Seeing is Believing: Mitigating Hallucination in Large Vision-Language Models via CLIP-Guided Decoding" index=106>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-106 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CL, cs-CV, cs-LG, cs-MM, cs.CV<br>Keyword Score: 40<br>Keywords: Grounding, Text Generation, Instruction Tuning, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15300v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15300v1.pdf filename=2402.15300v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Large <b>Vision-Language</b> Models (LVLMs) are susceptible to object hallucinations, an issue in which their generated <b>text</b> <b>contains</b> non-existent objects, greatly limiting their reliability and practicality. Current approaches often rely on the model&rsquo;s token likelihoods or other internal information, <b>instruction</b> <b>tuning</b> on additional datasets, or incorporating complex external tools. We first perform empirical analysis on sentence-level LVLM hallucination, finding that CLIP similarity to the image acts as a stronger and more robust indicator of hallucination compared to token likelihoods. Motivated by this, we introduce our CLIP-Guided Decoding (CGD) approach, a straightforward but effective training-free approach to reduce object hallucination at decoding time. CGD uses CLIP to guide the model&rsquo;s decoding process by enhancing visual <b>grounding</b> of generated <b>text</b> <b>with</b> the image. Experiments demonstrate that CGD effectively mitigates object hallucination across multiple LVLM families while preserving the utility of <b>text</b> <b>generation.</b></p></p class="citation"></blockquote><h3 id=625--107190-modified-cyclegan-for-the-synthesization-of-samples-for-wheat-head-segmentation-jaden-myers-et-al-2024>(6/25 | 107/190) Modified CycleGAN for the synthesization of samples for wheat head segmentation (Jaden Myers et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jaden Myers, Keyhan Najafian, Farhad Maleki, Katie Ovens. (2024)<br><strong>Modified CycleGAN for the synthesization of samples for wheat head segmentation</strong><br><button class=copy-to-clipboard title="Modified CycleGAN for the synthesization of samples for wheat head segmentation" index=107>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-107 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Generative Adversarial Network, Generative Adversarial Network, Supervised Learning, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15135v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15135v1.pdf filename=2402.15135v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep learning models have been used for a variety of image processing tasks. However, most of these models are developed through <b>supervised</b> <b>learning</b> approaches, which rely heavily on the availability of large-scale annotated datasets. Developing such datasets is tedious and expensive. In the absence of an annotated dataset, synthetic data can be used for model development; however, due to the substantial differences between simulated and real data, a phenomenon referred to as domain gap, the resulting models often underperform when applied to real data. In this research, we aim to address this challenge by first computationally simulating a large-scale annotated dataset and then using a <b>generative</b> <b>adversarial</b> <b>network</b> <b>(GAN)</b> to fill the gap between simulated and real images. This approach results in a synthetic dataset that can be effectively utilized to train a deep-learning model. Using this approach, we developed a realistic annotated synthetic dataset for wheat head segmentation. This dataset was then used to develop a deep-learning model for semantic segmentation. The resulting model achieved a Dice score of 83.4% on an internal dataset and Dice scores of 79.6% and 83.6% on two external Global Wheat Head Detection datasets. While we proposed this approach in the context of wheat head segmentation, it can be generalized to other crop types or, more broadly, to images with dense, repeated patterns such as those found in cellular imagery.</p></p class="citation"></blockquote><h3 id=725--108190-benchmarking-the-robustness-of-panoptic-segmentation-for-automated-driving-yiting-wang-et-al-2024>(7/25 | 108/190) Benchmarking the Robustness of Panoptic Segmentation for Automated Driving (Yiting Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yiting Wang, Haonan Zhao, Daniel Gummadi, Mehrdad Dianati, Kurt Debattista, Valentina Donzella. (2024)<br><strong>Benchmarking the Robustness of Panoptic Segmentation for Automated Driving</strong><br><button class=copy-to-clipboard title="Benchmarking the Robustness of Panoptic Segmentation for Automated Driving" index=108>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-108 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 36<br>Keywords: Vision Transformer, Benchmarking, Benchmarking, Transformer, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15469v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15469v1.pdf filename=2402.15469v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Precise situational awareness is required for the safe decision-making of assisted and automated driving (AAD) functions. Panoptic segmentation is a promising perception technique to identify and categorise objects, impending hazards, and driveable space at a pixel level. While segmentation quality is generally associated with the quality of the camera data, a comprehensive understanding and modelling of this relationship are paramount for AAD system designers. Motivated by such a need, this work proposes a unifying pipeline to assess the robustness of panoptic segmentation models for AAD, correlating it with traditional image quality. The first step of the proposed pipeline involves generating degraded camera data that reflects real-world noise factors. To this end, 19 noise factors have been identified and implemented with 3 severity levels. Of these factors, this work proposes novel models for unfavourable light and snow. After applying the degradation models, three state-of-the-art CNN- and <b>vision</b> <b>transformers</b> (ViT)-based panoptic segmentation networks are used to analyse their robustness. The variations of the segmentation performance are then correlated to 8 selected image quality metrics. This research reveals that: 1) certain specific noise factors produce the highest impact on panoptic segmentation, i.e. droplets on lens and Gaussian noise; 2) the ViT-based panoptic segmentation backbones show better robustness to the considered noise factors; 3) some image quality metrics (i.e. LPIPS and CW-SSIM) correlate strongly with panoptic segmentation performance and therefore they can be used as predictive metrics for network performance.</p></p class="citation"></blockquote><h3 id=825--109190-large-multimodal-agents-a-survey-junlin-xie-et-al-2024>(8/25 | 109/190) Large Multimodal Agents: A Survey (Junlin Xie et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Junlin Xie, Zhihong Chen, Ruifei Zhang, Xiang Wan, Guanbin Li. (2024)<br><strong>Large Multimodal Agents: A Survey</strong><br><button class=copy-to-clipboard title="Large Multimodal Agents: A Survey" index=109>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-109 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CL, cs-CV, cs.CV<br>Keyword Score: 36<br>Keywords: Multi-modal, Multi-modal, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15116v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15116v1.pdf filename=2402.15116v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have achieved superior performance in powering text-based AI agents, endowing them with decision-making and <b>reasoning</b> abilities akin to humans. Concurrently, there is an emerging research trend focused on extending these <b>LLM-powered</b> AI agents into the <b>multimodal</b> domain. This extension enables AI agents to interpret and respond to diverse <b>multimodal</b> user queries, thereby handling more intricate and nuanced tasks. In this paper, we conduct a systematic review of <b>LLM-driven</b> <b>multimodal</b> agents, which we refer to as <b>large</b> <b>multimodal</b> <b>agents</b> ( LMAs for short). First, we introduce the essential components involved in developing LMAs and categorize the current body of research into four distinct types. Subsequently, we review the collaborative frameworks integrating multiple LMAs , enhancing collective efficacy. One of the critical challenges in this field is the diverse evaluation methods used across existing studies, hindering effective comparison among different LMAs . Therefore, we compile these evaluation methodologies and establish a comprehensive framework to bridge the gaps. This framework aims to standardize evaluations, facilitating more meaningful comparisons. Concluding our review, we highlight the extensive applications of LMAs and propose possible future research directions. Our discussion aims to provide valuable insights and guidelines for future research in this rapidly evolving field. An up-to-date resource list is available at <a href=https://github.com/jun0wanan/awesome-large-multimodal-agents>https://github.com/jun0wanan/awesome-large-multimodal-agents</a>.</p></p class="citation"></blockquote><h3 id=925--110190-gen4gen-generative-data-pipeline-for-generative-multi-concept-composition-chun-hsiao-yeh-et-al-2024>(9/25 | 110/190) Gen4Gen: Generative Data Pipeline for Generative Multi-Concept Composition (Chun-Hsiao Yeh et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chun-Hsiao Yeh, Ta-Ying Cheng, He-Yen Hsieh, Chuan-En Lin, Yi Ma, Andrew Markham, Niki Trigoni, H. T. Kung, Yubei Chen. (2024)<br><strong>Gen4Gen: Generative Data Pipeline for Generative Multi-Concept Composition</strong><br><button class=copy-to-clipboard title="Gen4Gen: Generative Data Pipeline for Generative Multi-Concept Composition" index=110>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-110 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 33<br>Keywords: Diffusion Model, Benchmarking, Text2image, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15504v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15504v1.pdf filename=2402.15504v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent <b>text-to-image</b> <b>diffusion</b> <b>models</b> are able to learn and synthesize images containing novel, personalized concepts (e.g., their own pets or specific items) with just a few examples for training. This paper tackles two interconnected issues within this realm of personalizing <b>text-to-image</b> <b>diffusion</b> <b>models.</b> First, current personalization techniques fail to reliably extend to multiple concepts &ndash; we hypothesize this to be due to the mismatch between complex scenes and simple text descriptions in the pre-training dataset (e.g., LAION). Second, given an image containing multiple personalized concepts, there lacks a holistic metric that evaluates performance on not just the degree of resemblance of personalized concepts, but also whether all concepts are present in the image and whether the image accurately reflects the overall text description. To address these issues, we introduce Gen4Gen, a semi-automated dataset creation pipeline utilizing generative models to combine personalized concepts into complex compositions along with text-descriptions. Using this, we create a dataset called MyCanvas, that can be used to <b>benchmark</b> the task of multi-concept personalization. In addition, we design a comprehensive metric comprising two scores (CP-CLIP and TI-CLIP) for better quantifying the performance of multi-concept, personalized <b>text-to-image</b> <b>diffusion</b> <b>methods.</b> We provide a simple baseline built on top of Custom <b>Diffusion</b> <b>with</b> empirical <b>prompting</b> strategies for future researchers to evaluate on MyCanvas. We show that by improving data quality and <b>prompting</b> strategies, we can significantly increase multi-concept personalized image generation quality, without requiring any modifications to model architecture or training algorithms.</p></p class="citation"></blockquote><h3 id=1025--111190-where-visual-speech-meets-language-vsp-llm-framework-for-efficient-and-context-aware-visual-speech-processing-jeong-hun-yeo-et-al-2024>(10/25 | 111/190) Where Visual Speech Meets Language: VSP-LLM Framework for Efficient and Context-Aware Visual Speech Processing (Jeong Hun Yeo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jeong Hun Yeo, Seunghee Han, Minsu Kim, Yong Man Ro. (2024)<br><strong>Where Visual Speech Meets Language: VSP-LLM Framework for Efficient and Context-Aware Visual Speech Processing</strong><br><button class=copy-to-clipboard title="Where Visual Speech Meets Language: VSP-LLM Framework for Efficient and Context-Aware Visual Speech Processing" index=111>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-111 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CL, cs-CV, cs.CV, eess-AS, eess-IV<br>Keyword Score: 33<br>Keywords: Benchmarking, Self-supervised Learning, Automatic Speech Recognition, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15151v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15151v1.pdf filename=2402.15151v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In visual <b>speech</b> <b>processing,</b> context modeling capability is one of the most important requirements due to the ambiguous nature of lip movements. For example, homophenes, words that share identical lip movements but produce different sounds, can be distinguished by considering the context. In this paper, we propose a novel framework, namely Visual <b>Speech</b> <b>Processing</b> incorporated with <b>LLMs</b> (VSP-LLM), to maximize the context modeling ability by bringing the overwhelming power of <b>LLMs.</b> Specifically, VSP-LLM is designed to perform multi-tasks of visual <b>speech</b> <b>recognition</b> and translation, where the given instructions control the type of task. The input video is mapped to the input latent space of a <b>LLM</b> by employing a <b>self-supervised</b> visual <b>speech</b> <b>model.</b> Focused on the fact that there is redundant information in input frames, we propose a novel deduplication method that reduces the embedded visual features by employing visual <b>speech</b> <b>units.</b> Through the proposed deduplication and Low Rank Adaptors (LoRA), VSP-LLM can be trained in a computationally efficient manner. In the translation dataset, the MuAViC <b>benchmark,</b> we demonstrate that VSP-LLM can more effectively recognize and translate lip movements with just 15 hours of labeled data, compared to the recent translation model trained with 433 hours of labeld data.</p></p class="citation"></blockquote><h3 id=1125--112190-retinotopic-mapping-enhances-the-robustness-of-convolutional-neural-networks-jean-nicolas-jérémie-et-al-2024>(11/25 | 112/190) Retinotopic Mapping Enhances the Robustness of Convolutional Neural Networks (Jean-Nicolas Jérémie et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jean-Nicolas Jérémie, Emmanuel Daucé, Laurent U Perrinet. (2024)<br><strong>Retinotopic Mapping Enhances the Robustness of Convolutional Neural Networks</strong><br><button class=copy-to-clipboard title="Retinotopic Mapping Enhances the Robustness of Convolutional Neural Networks" index=112>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-112 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV, q-bio-NC<br>Keyword Score: 30<br>Keywords: Convolution, Convolutional Neural Network, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15480v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15480v1.pdf filename=2402.15480v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Foveated vision, a trait shared by many animals, including humans, has not been fully utilized in machine learning applications, despite its significant contributions to biological visual function. This study investigates whether retinotopic mapping, a critical component of foveated vision, can enhance image categorization and localization performance when integrated into deep <b>convolutional</b> <b>neural</b> <b>networks</b> <b>(CNNs).</b> Retinotopic mapping was integrated into the inputs of standard off-the-shelf <b>convolutional</b> <b>neural</b> <b>networks</b> <b>(CNNs),</b> which were then retrained on the ImageNet task. As expected, the logarithmic-polar mapping improved the network&rsquo;s ability to handle arbitrary image zooms and rotations, particularly for isolated objects. Surprisingly, the retinotopically mapped network achieved comparable performance in classification. Furthermore, the network demonstrated improved classification localization when the foveated center of the transform was shifted. This replicates a crucial ability of the human visual system that is absent in typical <b>convolutional</b> <b>neural</b> <b>networks</b> <b>(CNNs).</b> These findings suggest that retinotopic mapping may be fundamental to significant preattentive visual processes.</p></p class="citation"></blockquote><h3 id=1225--113190-hierarchical-invariance-for-robust-and-interpretable-vision-tasks-at-larger-scales-shuren-qi-et-al-2024>(12/25 | 113/190) Hierarchical Invariance for Robust and Interpretable Vision Tasks at Larger Scales (Shuren Qi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shuren Qi, Yushu Zhang, Chao Wang, Zhihua Xia, Jian Weng, Xiaochun Cao. (2024)<br><strong>Hierarchical Invariance for Robust and Interpretable Vision Tasks at Larger Scales</strong><br><button class=copy-to-clipboard title="Hierarchical Invariance for Robust and Interpretable Vision Tasks at Larger Scales" index=113>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-113 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 30<br>Keywords: Convolution, Convolutional Neural Network, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15430v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15430v1.pdf filename=2402.15430v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Developing robust and interpretable vision systems is a crucial step towards trustworthy artificial intelligence. In this regard, a promising paradigm considers embedding task-required invariant structures, e.g., geometric invariance, in the fundamental image representation. However, such invariant representations typically exhibit limited discriminability, limiting their applications in larger-scale trustworthy vision tasks. For this open problem, we conduct a systematic investigation of hierarchical invariance, exploring this topic from theoretical, practical, and application perspectives. At the theoretical level, we show how to construct over-complete invariants with a <b>Convolutional</b> <b>Neural</b> <b>Networks</b> <b>(CNN)-like</b> hierarchical architecture yet in a fully interpretable manner. The general blueprint, specific definitions, invariant properties, and numerical implementations are provided. At the practical level, we discuss how to customize this theoretical framework into a given task. With the over-completeness, discriminative features w.r.t. the task can be adaptively formed in a Neural Architecture Search (NAS)-like manner. We demonstrate the above arguments with accuracy, invariance, and efficiency results on texture, digit, and parasite classification experiments. Furthermore, at the application level, our representations are explored in real-world forensics tasks on adversarial perturbations and Artificial Intelligence Generated Content (AIGC). Such applications reveal that the proposed strategy not only realizes the theoretically promised invariance, but also exhibits competitive discriminability even in the era of deep learning. For robust and interpretable vision tasks at larger scales, hierarchical invariant representation can be considered as an effective alternative to traditional <b>CNN</b> and invariants.</p></p class="citation"></blockquote><h3 id=1325--114190-semi-supervised-counting-via-pixel-by-pixel-density-distribution-modelling-hui-lin-et-al-2024>(13/25 | 114/190) Semi-supervised Counting via Pixel-by-pixel Density Distribution Modelling (Hui Lin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hui Lin, Zhiheng Ma, Rongrong Ji, Yaowei Wang, Zhou Su, Xiaopeng Hong, Deyu Meng. (2024)<br><strong>Semi-supervised Counting via Pixel-by-pixel Density Distribution Modelling</strong><br><button class=copy-to-clipboard title="Semi-supervised Counting via Pixel-by-pixel Density Distribution Modelling" index=114>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-114 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 30<br>Keywords: Self-supervised Learning, Self-supervised Learning, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15297v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15297v1.pdf filename=2402.15297v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper focuses on semi-supervised crowd counting, where only a small portion of the training data are labeled. We formulate the pixel-wise density value to regress as a probability distribution, instead of a single deterministic value. On this basis, we propose a semi-supervised crowd-counting model. Firstly, we design a pixel-wise distribution matching loss to measure the differences in the pixel-wise density distributions between the prediction and the ground truth; Secondly, we enhance the <b>transformer</b> decoder by using density tokens to specialize the forwards of decoders w.r.t. different density intervals; Thirdly, we design the interleaving consistency <b>self-supervised</b> <b>learning</b> mechanism to learn from unlabeled data efficiently. Extensive experiments on four datasets are performed to show that our method clearly outperforms the competitors by a large margin under various labeled ratio settings. Code will be released at <a href=https://github.com/LoraLinH/Semi-supervised-Counting-via-Pixel-by-pixel-Density-Distribution-Modelling>https://github.com/LoraLinH/Semi-supervised-Counting-via-Pixel-by-pixel-Density-Distribution-Modelling</a>.</p></p class="citation"></blockquote><h3 id=1425--115190-unsupervised-domain-adaptation-for-brain-vessel-segmentation-through-transwarp-contrastive-learning-fengming-lin-et-al-2024>(14/25 | 115/190) Unsupervised Domain Adaptation for Brain Vessel Segmentation through Transwarp Contrastive Learning (Fengming Lin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fengming Lin, Yan Xia, Michael MacRaild, Yash Deo, Haoran Dou, Qiongyao Liu, Kun Wu, Nishant Ravikumar, Alejandro F. Frangi. (2024)<br><strong>Unsupervised Domain Adaptation for Brain Vessel Segmentation through Transwarp Contrastive Learning</strong><br><button class=copy-to-clipboard title="Unsupervised Domain Adaptation for Brain Vessel Segmentation through Transwarp Contrastive Learning" index=115>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-115 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 30<br>Keywords: Contrastive Learning, Unsupervised Learning, Domain Adaptation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15237v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15237v1.pdf filename=2402.15237v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Unsupervised</b> <b>domain</b> <b>adaptation</b> (UDA) aims to align the labelled source distribution with the unlabelled target distribution to obtain <b>domain-invariant</b> <b>predictive</b> models. Since cross-modality medical data exhibit significant intra and inter-domain shifts and most are unlabelled, UDA is more important while challenging in medical image analysis. This paper proposes a simple yet potent <b>contrastive</b> <b>learning</b> framework for UDA to narrow the inter-domain gap between labelled source and unlabelled target distribution. Our method is validated on cerebral vessel datasets. Experimental results show that our approach can learn latent features from labelled 3DRA modality data and improve vessel segmentation performance in unlabelled MRA modality data.</p></p class="citation"></blockquote><h3 id=1525--116190-font-impression-estimation-in-the-wild-kazuki-kitajima-et-al-2024>(15/25 | 116/190) Font Impression Estimation in the Wild (Kazuki Kitajima et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kazuki Kitajima, Daichi Haraguchi, Seiichi Uchida. (2024)<br><strong>Font Impression Estimation in the Wild</strong><br><button class=copy-to-clipboard title="Font Impression Estimation in the Wild" index=116>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-116 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Convolution, Convolutional Neural Network, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15236v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15236v1.pdf filename=2402.15236v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper addresses the challenging task of estimating font impressions from real font images. We use a font dataset with annotation about font impressions and a <b>convolutional</b> <b>neural</b> <b>network</b> <b>(CNN)</b> framework for this task. However, impressions attached to individual fonts are often missing and noisy because of the subjective characteristic of font impression annotation. To realize stable impression estimation even with such a dataset, we propose an exemplar-based impression estimation approach, which relies on a strategy of ensembling impressions of exemplar fonts that are similar to the input image. In addition, we train <b>CNN</b> with synthetic font images that mimic scanned word images so that <b>CNN</b> estimates impressions of font images in the wild. We evaluate the basic performance of the proposed estimation method quantitatively and qualitatively. Then, we conduct a correlation analysis between book genres and font impressions on real book cover images; it is important to note that this analysis is only possible with our impression estimation method. The analysis reveals various trends in the correlation between them - this fact supports a hypothesis that book cover designers carefully choose a font for a book cover considering the impression given by the font.</p></p class="citation"></blockquote><h3 id=1625--117190-outlier-detection-by-ensembling-uncertainty-with-negative-objectness-anja-delić-et-al-2024>(16/25 | 117/190) Outlier detection by ensembling uncertainty with negative objectness (Anja Delić et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Anja Delić, Matej Grcić, Siniša Šegvić. (2024)<br><strong>Outlier detection by ensembling uncertainty with negative objectness</strong><br><button class=copy-to-clipboard title="Outlier detection by ensembling uncertainty with negative objectness" index=117>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-117 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 23<br>Keywords: Benchmarking, Outlier Detection, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15374v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15374v1.pdf filename=2402.15374v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Outlier</b> <b>detection</b> is an essential capability in safety-critical applications of <b>supervised</b> visual recognition. Most of the existing methods deliver best results by encouraging standard closed-set models to produce low-confidence predictions in negative training data. However, that approach conflates prediction uncertainty with recognition of the negative class. We therefore reconsider direct prediction of K+1 logits that correspond to K groundtruth classes and one <b>outlier</b> <b>class.</b> This setup allows us to formulate a novel anomaly score as an ensemble of in-distribution uncertainty and the posterior of the <b>outlier</b> <b>class</b> which we term negative objectness. Now <b>outliers</b> <b>can</b> be independently detected due to i) high prediction uncertainty or ii) similarity with negative data. We embed our method into a dense prediction architecture with mask-level recognition over K+2 classes. The training procedure encourages the novel K+2-th class to learn negative objectness at pasted negative instances. Our models outperform the current state-of-the art on standard <b>benchmarks</b> for image-wide and pixel-level <b>outlier</b> <b>detection</b> with and without training on real negative data.</p></p class="citation"></blockquote><h3 id=1725--118190-source-guided-similarity-preservation-for-online-person-re-identification-hamza-rami-et-al-2024>(17/25 | 118/190) Source-Guided Similarity Preservation for Online Person Re-Identification (Hamza Rami et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hamza Rami, Jhony H. Giraldo, Nicolas Winckler, Stéphane Lathuilière. (2024)<br><strong>Source-Guided Similarity Preservation for Online Person Re-Identification</strong><br><button class=copy-to-clipboard title="Source-Guided Similarity Preservation for Online Person Re-Identification" index=118>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-118 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 23<br>Keywords: Benchmarking, Unsupervised Learning, Domain Adaptation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15206v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15206v1.pdf filename=2402.15206v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Online <b>Unsupervised</b> <b>Domain</b> <b>Adaptation</b> (OUDA) for person Re-Identification (Re-ID) is the task of continuously adapting a model trained on a well-annotated source <b>domain</b> <b>dataset</b> to a target <b>domain</b> <b>observed</b> as a data stream. In OUDA, person Re-ID models face two main challenges: catastrophic forgetting and <b>domain</b> <b>shift.</b> In this work, we propose a new Source-guided Similarity Preservation (S2P) framework to alleviate these two problems. Our framework is based on the extraction of a support set composed of source images that maximizes the similarity with the target data. This support set is used to identify feature similarities that must be preserved during the learning process. S2P can incorporate multiple existing UDA methods to mitigate catastrophic forgetting. Our experiments show that S2P outperforms previous state-of-the-art methods on multiple real-to-real and synthetic-to-real challenging OUDA <b>benchmarks.</b></p></p class="citation"></blockquote><h3 id=1825--119190-attention-guided-masked-autoencoders-for-learning-image-representations-leon-sick-et-al-2024>(18/25 | 119/190) Attention-Guided Masked Autoencoders For Learning Image Representations (Leon Sick et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Leon Sick, Dominik Engel, Pedro Hermosilla, Timo Ropinski. (2024)<br><strong>Attention-Guided Masked Autoencoders For Learning Image Representations</strong><br><button class=copy-to-clipboard title="Attention-Guided Masked Autoencoders For Learning Image Representations" index=119>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-119 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 23<br>Keywords: Autoencoder, Benchmarking, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15172v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15172v1.pdf filename=2402.15172v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Masked <b>autoencoders</b> (MAEs) have established themselves as a powerful method for <b>unsupervised</b> pre-training for computer vision tasks. While vanilla MAEs put equal emphasis on reconstructing the individual parts of the image, we propose to inform the reconstruction process through an attention-guided loss function. By leveraging advances in <b>unsupervised</b> object discovery, we obtain an attention map of the scene which we employ in the loss function to put increased emphasis on reconstructing relevant objects, thus effectively incentivizing the model to learn more object-focused representations without compromising the established masking strategy. Our evaluations show that our pre-trained models learn better latent representations than the vanilla MAE, demonstrated by improved linear probing and k-NN classification results on several <b>benchmarks</b> while at the same time making ViTs more robust against varying backgrounds.</p></p class="citation"></blockquote><h3 id=1925--120190-fiducial-focus-augmentation-for-facial-landmark-detection-purbayan-kar-et-al-2024>(19/25 | 120/190) Fiducial Focus Augmentation for Facial Landmark Detection (Purbayan Kar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Purbayan Kar, Vishal Chudasama, Naoyuki Onoe, Pankaj Wasnik, Vineeth Balasubramanian. (2024)<br><strong>Fiducial Focus Augmentation for Facial Landmark Detection</strong><br><button class=copy-to-clipboard title="Fiducial Focus Augmentation for Facial Landmark Detection" index=120>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-120 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 23<br>Keywords: Benchmarking, Convolutional Neural Network, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15044v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15044v1.pdf filename=2402.15044v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep learning methods have led to significant improvements in the performance on the facial landmark detection (FLD) task. However, detecting landmarks in challenging settings, such as head pose changes, exaggerated expressions, or uneven illumination, continue to remain a challenge due to high variability and insufficient samples. This inadequacy can be attributed to the model&rsquo;s inability to effectively acquire appropriate facial structure information from the input images. To address this, we propose a novel image augmentation technique specifically designed for the FLD task to enhance the model&rsquo;s understanding of facial structures. To effectively utilize the newly proposed augmentation technique, we employ a Siamese architecture-based training mechanism with a Deep Canonical Correlation Analysis (DCCA)-based loss to achieve collective learning of high-level feature representations from two different views of the input images. Furthermore, we employ a <b>Transformer</b> + <b>CNN-based</b> network with a custom hourglass module as the robust backbone for the Siamese framework. Extensive experiments show that our approach outperforms multiple state-of-the-art approaches across various <b>benchmark</b> datasets.</p></p class="citation"></blockquote><h3 id=2025--121190-protip-probabilistic-robustness-verification-on-text-to-image-diffusion-models-against-stochastic-perturbation-yi-zhang-et-al-2024>(20/25 | 121/190) ProTIP: Probabilistic Robustness Verification on Text-to-Image Diffusion Models against Stochastic Perturbation (Yi Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yi Zhang, Yun Tang, Wenjie Ruan, Xiaowei Huang, Siddartha Khastgir, Paul Jennings, Xingyu Zhao. (2024)<br><strong>ProTIP: Probabilistic Robustness Verification on Text-to-Image Diffusion Models against Stochastic Perturbation</strong><br><button class=copy-to-clipboard title="ProTIP: Probabilistic Robustness Verification on Text-to-Image Diffusion Models against Stochastic Perturbation" index=121>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-121 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-LG, cs.CV<br>Keyword Score: 20<br>Keywords: Diffusion Model, Text2image<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15429v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15429v1.pdf filename=2402.15429v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Text-to-Image</b> (T2I) <b>Diffusion</b> <b>Models</b> (DMs) have shown impressive abilities in generating high-quality images based on simple text descriptions. However, as is common with many Deep Learning (DL) models, DMs are subject to a lack of robustness. While there are attempts to evaluate the robustness of T2I DMs as a binary or worst-case problem, they cannot answer how robust in general the model is whenever an adversarial example (AE) can be found. In this study, we first introduce a probabilistic notion of T2I DMs&rsquo; robustness; and then establish an efficient framework, ProTIP, to evaluate it with statistical guarantees. The main challenges stem from: i) the high computational cost of the generation process; and ii) determining if a perturbed input is an AE involves comparing two output distributions, which is fundamentally harder compared to other DL tasks like classification where an AE is identified upon misprediction of labels. To tackle the challenges, we employ sequential analysis with efficacy and futility early stopping rules in the statistical testing for identifying AEs, and adaptive concentration inequalities to dynamically determine the &ldquo;just-right&rdquo; number of stochastic perturbations whenever the verification target is met. Empirical experiments validate the effectiveness and efficiency of ProTIP over common T2I DMs. Finally, we demonstrate an application of ProTIP to rank commonly used defence methods.</p></p class="citation"></blockquote><h3 id=2125--122190-puad-frustratingly-simple-method-for-robust-anomaly-detection-shota-sugawara-et-al-2024>(21/25 | 122/190) PUAD: Frustratingly Simple Method for Robust Anomaly Detection (Shota Sugawara et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shota Sugawara, Ryuji Imamura. (2024)<br><strong>PUAD: Frustratingly Simple Method for Robust Anomaly Detection</strong><br><button class=copy-to-clipboard title="PUAD: Frustratingly Simple Method for Robust Anomaly Detection" index=122>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-122 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 20<br>Keywords: Anomaly Detection, Out-of-distribution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15143v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15143v1.pdf filename=2402.15143v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Developing an accurate and fast <b>anomaly</b> <b>detection</b> model is an important task in real-time computer vision applications. There has been much research to develop a single model that detects either structural or logical anomalies, which are inherently distinct. The majority of the existing approaches implicitly assume that the <b>anomaly</b> <b>can</b> be represented by identifying the anomalous location. However, we argue that logical anomalies, such as the wrong number of objects, can not be well-represented by the spatial feature maps and require an alternative approach. In addition, we focused on the possibility of detecting logical anomalies by using an <b>out-of-distribution</b> detection approach on the feature space, which aggregates the spatial information of the feature map. As a demonstration, we propose a method that incorporates a simple <b>out-of-distribution</b> detection method on the feature space against state-of-the-art reconstruction-based approaches. Despite the simplicity of our proposal, our method PUAD (Picturable and Unpicturable <b>Anomaly</b> <b>Detection)</b> achieves state-of-the-art performance on the MVTec LOCO AD dataset.</p></p class="citation"></blockquote><h3 id=2225--123190-descripción-automática-de-secciones-delgadas-de-rocas-una-aplicación-web-stalyn-paucar-et-al-2024>(22/25 | 123/190) Descripción automática de secciones delgadas de rocas: una aplicación Web (Stalyn Paucar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Stalyn Paucar, Christian Mejía-Escobar y Víctor Collaguazo. (2024)<br><strong>Descripción automática de secciones delgadas de rocas: una aplicación Web</strong><br><button class=copy-to-clipboard title="Descripción automática de secciones delgadas de rocas: una aplicación Web" index=123>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-123 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 20<br>Keywords: Transformer, BLEU<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15039v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15039v1.pdf filename=2402.15039v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The identification and characterization of various rock types is one of the fundamental activities for geology and related areas such as mining, petroleum, environment, industry and construction. Traditionally, a human specialist is responsible for analyzing and explaining details about the type, composition, texture, shape and other properties using rock samples collected in-situ or prepared in a laboratory. The results become subjective based on experience, in addition to consuming a large investment of time and effort. The present proposal uses artificial intelligence techniques combining computer vision and natural language processing to generate a textual and verbal description from a thin section image of rock. We build a dataset of images and their respective textual descriptions for the training of a model that associates the relevant features of the image extracted by EfficientNetB7 with the textual description generated by a <b>Transformer</b> network, reaching an accuracy value of 0.892 and a <b>BLEU</b> value of 0.71. This model can be a useful resource for research, professional and academic work, so it has been deployed through a Web application for public use.</p></p class="citation"></blockquote><h3 id=2325--124190-optimal-transport-on-the-lie-group-of-roto-translations-daan-bon-et-al-2024>(23/25 | 124/190) Optimal Transport on the Lie Group of Roto-translations (Daan Bon et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Daan Bon, Gautam Pai, Gijs Bellaard, Olga Mula, Remco Duits. (2024)<br><strong>Optimal Transport on the Lie Group of Roto-translations</strong><br><button class=copy-to-clipboard title="Optimal Transport on the Lie Group of Roto-translations" index=124>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-124 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: 62H35, 68U10, 90B06, 68T45, 68U99, cs-CV, cs.CV, math-DG, math-OC<br>Keyword Score: 10<br>Keywords: Convolution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15322v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15322v1.pdf filename=2402.15322v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The roto-translation group SE2 has been of active interest in image analysis due to methods that lift the image data to multi-orientation representations defined on this Lie group. This has led to impactful applications of crossing-preserving flows for image de-noising, geodesic tracking, and roto-translation equivariant deep learning. In this paper, we develop a computational framework for optimal transportation over Lie groups, with a special focus on SE2. We make several theoretical contributions (generalizable to matrix Lie groups) such as the non-optimality of group actions as transport maps, invariance and equivariance of optimal transport, and the quality of the entropic-regularized optimal transport plan using geodesic distance approximations. We develop a Sinkhorn like algorithm that can be efficiently implemented using fast and accurate distance approximations of the Lie group and GPU-friendly group <b>convolutions.</b> We report valuable advancements in the experiments on 1) image barycenters, 2) interpolation of planar orientation fields, and 3) Wasserstein gradient flows on SE2. We observe that our framework of lifting images to SE2 and optimal transport with left-invariant anisotropic metrics leads to equivariant transport along dominant contours and salient line structures in the image. This yields sharper and more meaningful interpolations compared to their counterparts on $\mathbb{R}^2$</p></p class="citation"></blockquote><h3 id=2425--125190-emiff-enhanced-multi-scale-image-feature-fusion-for-vehicle-infrastructure-cooperative-3d-object-detection-zhe-wang-et-al-2024>(24/25 | 125/190) EMIFF: Enhanced Multi-scale Image Feature Fusion for Vehicle-Infrastructure Cooperative 3D Object Detection (Zhe Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhe Wang, Siqi Fan, Xiaoliang Huo, Tongda Xu, Yan Wang, Jingjing Liu, Yilun Chen, Ya-Qin Zhang. (2024)<br><strong>EMIFF: Enhanced Multi-scale Image Feature Fusion for Vehicle-Infrastructure Cooperative 3D Object Detection</strong><br><button class=copy-to-clipboard title="EMIFF: Enhanced Multi-scale Image Feature Fusion for Vehicle-Infrastructure Cooperative 3D Object Detection" index=125>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-125 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Object Detection<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15272v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15272v1.pdf filename=2402.15272v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In autonomous driving, cooperative perception makes use of multi-view cameras from both vehicles and infrastructure, providing a global vantage point with rich semantic context of road conditions beyond a single vehicle viewpoint. Currently, two major challenges persist in vehicle-infrastructure cooperative 3D (VIC3D) <b>object</b> <b>detection:</b> $1)$ inherent pose errors when fusing multi-view images, caused by time asynchrony across cameras; $2)$ information loss in transmission process resulted from limited communication bandwidth. To address these issues, we propose a novel camera-based 3D detection framework for VIC3D task, Enhanced Multi-scale Image Feature Fusion (EMIFF). To fully exploit holistic perspectives from both vehicles and infrastructure, we propose Multi-scale Cross Attention (MCA) and Camera-aware Channel Masking (CCM) modules to enhance infrastructure and vehicle features at scale, spatial, and channel levels to correct the pose error introduced by camera asynchrony. We also introduce a Feature Compression (FC) module with channel and spatial compression blocks for transmission efficiency. Experiments show that EMIFF achieves SOTA on DAIR-V2X-C datasets, significantly outperforming previous early-fusion and late-fusion methods with comparable transmission costs.</p></p class="citation"></blockquote><h3 id=2525--126190-gs-ema-integrating-gradient-surgery-exponential-moving-average-with-boundary-aware-contrastive-learning-for-enhanced-domain-generalization-in-aneurysm-segmentation-fengming-lin-et-al-2024>(25/25 | 126/190) GS-EMA: Integrating Gradient Surgery Exponential Moving Average with Boundary-Aware Contrastive Learning for Enhanced Domain Generalization in Aneurysm Segmentation (Fengming Lin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fengming Lin, Yan Xia, Michael MacRaild, Yash Deo, Haoran Dou, Qiongyao Liu, Nina Cheng, Nishant Ravikumar, Alejandro F. Frangi. (2024)<br><strong>GS-EMA: Integrating Gradient Surgery Exponential Moving Average with Boundary-Aware Contrastive Learning for Enhanced Domain Generalization in Aneurysm Segmentation</strong><br><button class=copy-to-clipboard title="GS-EMA: Integrating Gradient Surgery Exponential Moving Average with Boundary-Aware Contrastive Learning for Enhanced Domain Generalization in Aneurysm Segmentation" index=126>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-126 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 10<br>Keywords: Contrastive Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15239v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15239v1.pdf filename=2402.15239v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The automated segmentation of cerebral aneurysms is pivotal for accurate diagnosis and treatment planning. Confronted with significant domain shifts and class imbalance in 3D Rotational Angiography (3DRA) data from various medical institutions, the task becomes challenging. These shifts include differences in image appearance, intensity distribution, resolution, and aneurysm size, all of which complicate the segmentation process. To tackle these issues, we propose a novel domain generalization strategy that employs gradient surgery exponential moving average (GS-EMA) optimization technique coupled with boundary-aware <b>contrastive</b> <b>learning</b> (BACL). Our approach is distinct in its ability to adapt to new, unseen domains by learning domain-invariant features, thereby improving the robustness and accuracy of aneurysm segmentation across diverse clinical datasets. The results demonstrate that our proposed approach can extract more domain-invariant features, minimizing over-segmentation and capturing more complete aneurysm structures.</p></p class="citation"></blockquote><h2 id=csir-5>cs.IR (5)</h2><h3 id=15--127190-item-side-fairness-of-large-language-model-based-recommendation-system-meng-jiang-et-al-2024>(1/5 | 127/190) Item-side Fairness of Large Language Model-based Recommendation System (Meng Jiang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Meng Jiang, Keqin Bao, Jizhi Zhang, Wenjie Wang, Zhengyi Yang, Fuli Feng, Xiangnan He. (2024)<br><strong>Item-side Fairness of Large Language Model-based Recommendation System</strong><br><button class=copy-to-clipboard title="Item-side Fairness of Large Language Model-based Recommendation System" index=127>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-127 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 60<br>Keywords: Fairness, Fine-tuning, Recommendation, LLaMA, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15215v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15215v1.pdf filename=2402.15215v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Recommendation</b> systems for Web content distribution intricately connect to the information access and exposure opportunities for vulnerable populations. The emergence of <b>Large</b> <b>Language</b> <b>Models-based</b> <b>Recommendation</b> System (LRS) may introduce additional societal challenges to <b>recommendation</b> systems due to the inherent biases in <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs).</b> From the perspective of item-side <b>fairness,</b> there remains a lack of comprehensive investigation into the item-side <b>fairness</b> of LRS given the unique characteristics of LRS compared to conventional <b>recommendation</b> systems. To bridge this gap, this study examines the property of LRS with respect to item-side <b>fairness</b> and reveals the influencing factors of both historical users&rsquo; interactions and inherent semantic biases of <b>LLMs,</b> shedding light on the need to extend conventional item-side <b>fairness</b> methods for LRS. Towards this goal, we develop a concise and effective framework called IFairLRS to enhance the item-side <b>fairness</b> of an LRS. IFairLRS covers the main stages of building an LRS with specifically adapted strategies to calibrate the <b>recommendations</b> of LRS. We utilize IFairLRS to <b>fine-tune</b> <b>LLaMA,</b> a representative <b>LLM,</b> on \textit{MovieLens} and \textit{Steam} datasets, and observe significant item-side <b>fairness</b> improvements. The code can be found in <a href=https://github.com/JiangM-C/IFairLRS.git>https://github.com/JiangM-C/IFairLRS.git</a>.</p></p class="citation"></blockquote><h3 id=25--128190-multi-agent-collaboration-framework-for-recommender-systems-zhefan-wang-et-al-2024>(2/5 | 128/190) Multi-Agent Collaboration Framework for Recommender Systems (Zhefan Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhefan Wang, Yuanqing Yu, Wendi Zheng, Weizhi Ma, Min Zhang. (2024)<br><strong>Multi-Agent Collaboration Framework for Recommender Systems</strong><br><button class=copy-to-clipboard title="Multi-Agent Collaboration Framework for Recommender Systems" index=128>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-128 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 50<br>Keywords: Recommendation, Recommender System, Simulation, Simulator, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15235v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15235v1.pdf filename=2402.15235v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>LLM-based</b> agents have gained considerable attention for their decision-making skills and ability to handle complex tasks. Recognizing the current gap in leveraging agent capabilities for multi-agent collaboration in <b>recommendation</b> systems, we introduce MACRec, a novel framework designed to enhance <b>recommendation</b> systems through multi-agent collaboration. Unlike existing work on using agents for user/item <b>simulation,</b> we aim to deploy multi-agents to tackle <b>recommendation</b> tasks directly. In our framework, <b>recommendation</b> tasks are addressed through the collaborative efforts of various specialized agents, including Manager, User/Item Analyst, Reflector, Searcher, and Task Interpreter, with different working flows. Furthermore, we provide application examples of how developers can easily use MACRec on various <b>recommendation</b> tasks, including rating prediction, sequential <b>recommendation,</b> conversational <b>recommendation,</b> and explanation generation of <b>recommendation</b> results. The framework and demonstration video are publicly available at <a href=https://github.com/wzf2000/MACRec>https://github.com/wzf2000/MACRec</a>.</p></p class="citation"></blockquote><h3 id=35--129190-text2pic-swift-enhancing-long-text-to-image-retrieval-for-large-scale-libraries-zijun-long-et-al-2024>(3/5 | 129/190) Text2Pic Swift: Enhancing Long-Text to Image Retrieval for Large-Scale Libraries (Zijun Long et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zijun Long, Xuri Ge, Richard Mccreadie, Joemon Jose. (2024)<br><strong>Text2Pic Swift: Enhancing Long-Text to Image Retrieval for Large-Scale Libraries</strong><br><button class=copy-to-clipboard title="Text2Pic Swift: Enhancing Long-Text to Image Retrieval for Large-Scale Libraries" index=129>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-129 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-AI, cs-CV, cs-IR, cs.IR<br>Keyword Score: 26<br>Keywords: Multi-modal, Multi-modal, Text2image, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15276v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15276v1.pdf filename=2402.15276v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Text-to-image</b> retrieval plays a crucial role across various applications, including digital libraries, e-commerce platforms, and multimedia databases, by enabling the search for images using text queries. Despite the advancements in <b>Multimodal</b> <b>Large</b> <b>Language</b> <b>Models</b> (MLLMs), which offer leading-edge performance, their applicability in <b>large-scale,</b> <b>varied,</b> <b>and</b> ambiguous retrieval scenarios is constrained by significant computational demands and the generation of injective embeddings. This paper introduces the Text2Pic Swift framework, tailored for efficient and robust retrieval of images corresponding to extensive textual descriptions in sizable datasets. The framework employs a two-tier approach: the initial Entity-based Ranking (ER) stage addresses the ambiguity inherent in lengthy text queries through a multiple-queries-to-multiple-targets strategy, effectively narrowing down potential candidates for subsequent analysis. Following this, the Summary-based Re-ranking (SR) stage further refines these selections based on concise query summaries. Additionally, we present a novel Decoupling-BEiT-3 encoder, specifically designed to tackle the challenges of ambiguous queries and to facilitate both stages of the retrieval process, thereby significantly improving computational efficiency via vector-based similarity assessments. Our evaluation, conducted on the AToMiC dataset, demonstrates that Text2Pic Swift outperforms current MLLMs by achieving up to an 11.06% increase in Recall@1000, alongside reductions in training and retrieval durations by 68.75% and 99.79%, respectively.</p></p class="citation"></blockquote><h3 id=45--130190-faithful-temporal-question-answering-over-heterogeneous-sources-zhen-jia-et-al-2024>(4/5 | 130/190) Faithful Temporal Question Answering over Heterogeneous Sources (Zhen Jia et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhen Jia, Philipp Christmann, Gerhard Weikum. (2024)<br><strong>Faithful Temporal Question Answering over Heterogeneous Sources</strong><br><button class=copy-to-clipboard title="Faithful Temporal Question Answering over Heterogeneous Sources" index=130>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-130 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-CL, cs-IR, cs.IR<br>Keyword Score: 23<br>Keywords: Benchmarking, Question Answering, Question Answering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15400v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15400v1.pdf filename=2402.15400v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Temporal <b>question</b> <b>answering</b> <b>(QA)</b> involves time constraints, with phrases such as &ldquo;&mldr; in 2019&rdquo; or &ldquo;&mldr; before COVID&rdquo;. In the former, time is an explicit condition, in the latter it is implicit. State-of-the-art methods have limitations along three dimensions. First, with neural inference, time constraints are merely soft-matched, giving room to invalid or inexplicable answers. Second, <b>questions</b> <b>with</b> implicit time are poorly supported. Third, answers come from a single source: either a knowledge base (KB) or a text corpus. We propose a temporal <b>QA</b> system that addresses these shortcomings. First, it enforces temporal constraints for faithful answering with tangible evidence. Second, it properly handles implicit <b>questions.</b> <b>Third,</b> it operates over heterogeneous sources, covering KB, text and web tables in a unified manner. The method has three stages: (i) understanding the <b>question</b> <b>and</b> its temporal conditions, (ii) retrieving evidence from all sources, and (iii) faithfully answering the <b>question.</b> <b>As</b> implicit <b>questions</b> <b>are</b> sparse in prior <b>benchmarks,</b> we introduce a principled method for generating diverse <b>questions.</b> <b>Experiments</b> show superior performance over a suite of baselines.</p></p class="citation"></blockquote><h3 id=55--131190-easyrl4rec-a-user-friendly-code-library-for-reinforcement-learning-based-recommender-systems-yuanqing-yu-et-al-2024>(5/5 | 131/190) EasyRL4Rec: A User-Friendly Code Library for Reinforcement Learning Based Recommender Systems (Yuanqing Yu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuanqing Yu, Chongming Gao, Jiawei Chen, Heng Tang, Yuefeng Sun, Qian Chen, Weizhi Ma, Min Zhang. (2024)<br><strong>EasyRL4Rec: A User-Friendly Code Library for Reinforcement Learning Based Recommender Systems</strong><br><button class=copy-to-clipboard title="EasyRL4Rec: A User-Friendly Code Library for Reinforcement Learning Based Recommender Systems" index=131>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-131 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs-LG, cs.IR<br>Keyword Score: 20<br>Keywords: Recommender System, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15164v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15164v1.pdf filename=2402.15164v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Reinforcement</b> <b>Learning</b> (RL)-Based <b>Recommender</b> <b>Systems</b> (RSs) are increasingly recognized for their ability to improve long-term user engagement. Yet, the field grapples with challenges such as the absence of accessible frameworks, inconsistent evaluation standards, and the complexity of replicating prior work. Addressing these obstacles, we present EasyRL4Rec, a user-friendly and efficient library tailored for RL-based RSs. EasyRL4Rec features lightweight, diverse RL environments built on five widely-used public datasets, and is equipped with comprehensive core modules that offer rich options to ease the development of models. It establishes consistent evaluation criteria with a focus on long-term impacts and introduces customized solutions for state modeling and action representation tailored to <b>recommender</b> <b>systems.</b> Additionally, we share valuable insights gained from extensive experiments with current methods. EasyRL4Rec aims to facilitate the model development and experimental process in the domain of RL-based RSs. The library is openly accessible at <a href=https://github.com/chongminggao/EasyRL4Rec>https://github.com/chongminggao/EasyRL4Rec</a>.</p></p class="citation"></blockquote><h2 id=q-bioqm-1>q-bio.QM (1)</h2><h3 id=11--132190-all-thresholds-barred-direct-estimation-of-call-density-in-bioacoustic-data-amanda-k-navine-et-al-2024>(1/1 | 132/190) All Thresholds Barred: Direct Estimation of Call Density in Bioacoustic Data (Amanda K. Navine et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Amanda K. Navine, Tom Denton, Matthew J. Weldy, Patrick J. Hart. (2024)<br><strong>All Thresholds Barred: Direct Estimation of Call Density in Bioacoustic Data</strong><br><button class=copy-to-clipboard title="All Thresholds Barred: Direct Estimation of Call Density in Bioacoustic Data" index=132>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-132 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: q-bio.QM<br>Categories: cs-LG, cs-SD, eess-AS, q-bio-QM, q-bio.QM<br>Keyword Score: 50<br>Keywords: Distribution Shift, Distribution Shift, Simulation, Simulator, Grounding, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15360v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15360v1.pdf filename=2402.15360v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Passive acoustic monitoring (PAM) studies generate thousands of hours of audio, which may be used to monitor specific animal populations, conduct broad biodiversity surveys, detect threats such as poachers, and more. Machine learning classifiers for species identification are increasingly being used to process the vast amount of audio generated by bioacoustic surveys, expediting analysis and increasing the utility of PAM as a management tool. In common practice, a threshold is applied to classifier output scores, and scores above the threshold are aggregated into a detection count. The choice of threshold produces biased counts of vocalizations, which are subject to false positive/negative rates that may vary across subsets of the dataset. In this work, we advocate for directly estimating call density: The proportion of detection windows containing the target vocalization, regardless of classifier score. Our approach targets a desirable ecological estimator and provides a more rigorous <b>grounding</b> for identifying the core problems caused by <b>distribution</b> <b>shifts</b> &ndash; when the defining characteristics of the data <b>distribution</b> <b>change</b> &ndash; and designing strategies to mitigate them. We propose a validation scheme for estimating call density in a body of data and obtain, through Bayesian <b>reasoning,</b> probability <b>distributions</b> <b>of</b> confidence scores for both the positive and negative classes. We use these <b>distributions</b> <b>to</b> predict site-level densities, which may be subject to <b>distribution</b> <b>shifts.</b> We test our proposed methods on a real-world study of Hawaiian birds and provide <b>simulation</b> results leveraging existing fully annotated datasets, demonstrating robustness to variations in call density and classifier model quality.</p></p class="citation"></blockquote><h2 id=statml-5>stat.ML (5)</h2><h3 id=15--133190-efficient-semi-supervised-inference-for-logistic-regression-under-case-control-studies-zhuojun-quan-et-al-2024>(1/5 | 133/190) Efficient semi-supervised inference for logistic regression under case-control studies (Zhuojun Quan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhuojun Quan, Yuanyuan Lin, Kani Chen, Wen Yu. (2024)<br><strong>Efficient semi-supervised inference for logistic regression under case-control studies</strong><br><button class=copy-to-clipboard title="Efficient semi-supervised inference for logistic regression under case-control studies" index=133>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-133 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-LG, stat-ML, stat.ML<br>Keyword Score: 40<br>Keywords: Logistic Regression, Semi-Supervised Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15365v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15365v1.pdf filename=2402.15365v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Semi-supervised</b> <b>learning</b> has received increasingly attention in statistics and machine learning. In <b>semi-supervised</b> <b>learning</b> settings, a labeled data set with both outcomes and covariates and an unlabeled data set with covariates only are collected. We consider an inference problem in <b>semi-supervised</b> <b>settings</b> where the outcome in the labeled data is binary and the labeled data is collected by case-control sampling. Case-control sampling is an effective sampling scheme for alleviating imbalance structure in binary data. Under the <b>logistic</b> <b>model</b> assumption, case-control data can still provide consistent estimator for the slope parameter of the regression model. However, the intercept parameter is not identifiable. Consequently, the marginal case proportion cannot be estimated from case-control data. We find out that with the availability of the unlabeled data, the intercept parameter can be identified in <b>semi-supervised</b> <b>learning</b> setting. We construct the likelihood function of the observed labeled and unlabeled data and obtain the maximum likelihood estimator via an iterative algorithm. The proposed estimator is shown to be consistent, asymptotically normal, and semiparametrically efficient. Extensive <b>simulation</b> studies are conducted to show the finite sample performance of the proposed method. The results imply that the unlabeled data not only helps to identify the intercept but also improves the estimation efficiency of the slope parameter. Meanwhile, the marginal case proportion can be estimated accurately by the proposed method.</p></p class="citation"></blockquote><h3 id=25--134190-iteration-and-stochastic-first-order-oracle-complexities-of-stochastic-gradient-descent-using-constant-and-decaying-learning-rates-kento-imaizumi-et-al-2024>(2/5 | 134/190) Iteration and Stochastic First-order Oracle Complexities of Stochastic Gradient Descent using Constant and Decaying Learning Rates (Kento Imaizumi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kento Imaizumi, Hideaki Iiduka. (2024)<br><strong>Iteration and Stochastic First-order Oracle Complexities of Stochastic Gradient Descent using Constant and Decaying Learning Rates</strong><br><button class=copy-to-clipboard title="Iteration and Stochastic First-order Oracle Complexities of Stochastic Gradient Descent using Constant and Decaying Learning Rates" index=134>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-134 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-LG, stat-ML, stat.ML<br>Keyword Score: 20<br>Keywords: Stochastic Gradient Descent, Stochastic Gradient Descent<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15344v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15344v1.pdf filename=2402.15344v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The performance of <b>stochastic</b> <b>gradient</b> <b>descent</b> <b>(SGD),</b> which is the simplest first-order optimizer for training deep neural networks, depends on not only the learning rate but also the batch size. They both affect the number of iterations and the <b>stochastic</b> <b>first-order</b> <b>oracle</b> (SFO) complexity needed for training. In particular, the previous numerical results indicated that, for <b>SGD</b> using a constant learning rate, the number of iterations needed for training decreases when the batch size increases, and the SFO complexity needed for training is minimized at a critical batch size and that it increases once the batch size exceeds that size. Here, we study the relationship between batch size and the iteration and SFO complexities needed for nonconvex optimization in deep learning with <b>SGD</b> using constant or decaying learning rates and show that <b>SGD</b> using the critical batch size minimizes the SFO complexity. We also provide numerical comparisons of <b>SGD</b> with the existing first-order optimizers and show the usefulness of <b>SGD</b> using a critical batch size. Moreover, we show that measured critical batch sizes are close to the sizes estimated from our theoretical results.</p></p class="citation"></blockquote><h3 id=35--135190-statistical-agnostic-regression-a-machine-learning-method-to-validate-regression-models-juan-m-gorriz-et-al-2024>(3/5 | 135/190) Statistical Agnostic Regression: a machine learning method to validate regression models (Juan M Gorriz et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Juan M Gorriz, J. Ramirez, F. Segovia, F. J. Martinez-Murcia, C. Jiménez-Mesa, J. Suckling. (2024)<br><strong>Statistical Agnostic Regression: a machine learning method to validate regression models</strong><br><button class=copy-to-clipboard title="Statistical Agnostic Regression: a machine learning method to validate regression models" index=135>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-135 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-LG, math-ST, stat-CO, stat-ML, stat-TH, stat.ML<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15213v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15213v1.pdf filename=2402.15213v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Regression analysis is a central topic in statistical modeling, aiming to estimate the relationships between a dependent variable, commonly referred to as the response variable, and one or more independent variables, i.e., explanatory variables. Linear regression is by far the most popular method for performing this task in several fields of research, such as prediction, forecasting, or causal inference. Beyond various classical methods to solve linear regression problems, such as Ordinary Least Squares, Ridge, or Lasso regressions - which are often the foundation for more advanced machine learning (ML) techniques - the latter have been successfully applied in this scenario without a formal definition of statistical significance. At most, permutation or classical analyses based on empirical measures (e.g., residuals or accuracy) have been conducted to reflect the greater ability of ML estimations for detection. In this paper, we introduce a method, named Statistical Agnostic Regression (SAR), for evaluating the statistical significance of an ML-based linear regression based on concentration inequalities of the actual risk using the analysis of the worst case. To achieve this goal, similar to the classification problem, we define a threshold to establish that there is sufficient evidence with a probability of at least 1-eta to conclude that there is a linear relationship in the population between the explanatory (feature) and the response (label) variables. <b>Simulations</b> in only two dimensions demonstrate the ability of the proposed agnostic test to provide a similar analysis of variance given by the classical $F$ test for the slope parameter.</p></p class="citation"></blockquote><h3 id=45--136190-generative-modelling-with-tensor-train-approximations-of-hamilton--jacobi--bellman-equations-david-sommer-et-al-2024>(4/5 | 136/190) Generative Modelling with Tensor Train approximations of Hamilton&ndash;Jacobi&ndash;Bellman equations (David Sommer et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>David Sommer, Robert Gruhlke, Max Kirstein, Martin Eigel, Claudia Schillings. (2024)<br><strong>Generative Modelling with Tensor Train approximations of Hamilton&ndash;Jacobi&ndash;Bellman equations</strong><br><button class=copy-to-clipboard title="Generative Modelling with Tensor Train approximations of Hamilton--Jacobi--Bellman equations" index=136>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-136 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: 35F21, 35Q84, 62F15, 65N75, 65C30, cs-LG, math-ST, stat-ML, stat-TH, stat.ML<br>Keyword Score: 15<br>Keywords: Black Box, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15285v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15285v1.pdf filename=2402.15285v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Sampling from probability densities is a common challenge in fields such as Uncertainty Quantification (UQ) and Generative Modelling (GM). In GM in particular, the use of reverse-time diffusion processes depending on the log-densities of Ornstein-Uhlenbeck forward processes are a popular sampling tool. In Berner et al. [2022] the authors point out that these log-densities can be obtained by solution of a \textit{Hamilton-Jacobi-Bellman} (HJB) equation known from stochastic optimal control. While this HJB equation is usually treated with indirect methods such as policy iteration and <b>unsupervised</b> training of <b>black-box</b> <b>architectures</b> like Neural Networks, we propose instead to solve the HJB equation by direct time integration, using compressed polynomials represented in the Tensor Train (TT) format for spatial discretization. Crucially, this method is sample-free, agnostic to normalization constants and can avoid the curse of dimensionality due to the TT compression. We provide a complete derivation of the HJB equation&rsquo;s action on Tensor Train polynomials and demonstrate the performance of the proposed time-step-, rank- and degree-adaptive integration method on a nonlinear sampling task in 20 dimensions.</p></p class="citation"></blockquote><h3 id=55--137190-nonlinear-bayesian-optimal-experimental-design-using-logarithmic-sobolev-inequalities-fengyi-li-et-al-2024>(5/5 | 137/190) Nonlinear Bayesian optimal experimental design using logarithmic Sobolev inequalities (Fengyi Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fengyi Li, Ayoub Belhadji, Youssef Marzouk. (2024)<br><strong>Nonlinear Bayesian optimal experimental design using logarithmic Sobolev inequalities</strong><br><button class=copy-to-clipboard title="Nonlinear Bayesian optimal experimental design using logarithmic Sobolev inequalities" index=137>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-137 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-LG, stat-ME, stat-ML, stat.ML<br>Keyword Score: 10<br>Keywords: Mutual Information<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15053v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15053v1.pdf filename=2402.15053v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We study the problem of selecting $k$ experiments from a larger candidate pool, where the goal is to maximize <b>mutual</b> <b>information</b> (MI) between the selected subset and the underlying parameters. Finding the exact solution is to this combinatorial optimization problem is computationally costly, not only due to the complexity of the combinatorial search but also the difficulty of evaluating MI in nonlinear/non-Gaussian settings. We propose greedy approaches based on new computationally inexpensive lower bounds for MI, constructed via log-Sobolev inequalities. We demonstrate that our method outperforms random selection strategies, Gaussian approximations, and nested Monte Carlo (NMC) estimators of MI in various settings, including optimal design for nonlinear models with non-additive noise.</p></p class="citation"></blockquote><h2 id=cssd-1>cs.SD (1)</h2><h3 id=11--138190-a-survey-of-music-generation-in-the-context-of-interaction-ismael-agchar-et-al-2024>(1/1 | 138/190) A Survey of Music Generation in the Context of Interaction (Ismael Agchar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ismael Agchar, Ilja Baumann, Franziska Braun, Paula Andrea Perez-Toro, Korbinian Riedhammer, Sebastian Trump, Martin Ullrich. (2024)<br><strong>A Survey of Music Generation in the Context of Interaction</strong><br><button class=copy-to-clipboard title="A Survey of Music Generation in the Context of Interaction" index=138>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-138 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SD<br>Categories: cs-AI, cs-LG, cs-SD, cs.SD, eess-AS<br>Keyword Score: 40<br>Keywords: Automatic Evaluation, Generative Adversarial Network, Transformer, Style Transfer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15294v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15294v1.pdf filename=2402.15294v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In recent years, machine learning, and in particular generative adversarial neural networks <b>(GANs)</b> and attention-based neural networks <b>(transformers),</b> have been successfully used to compose and generate music, both melodies and polyphonic pieces. Current research focuses foremost on <b>style</b> <b>replication</b> (eg. generating a Bach-style chorale) or <b>style</b> <b>transfer</b> (eg. classical to jazz) based on large amounts of recorded or transcribed music, which in turn also allows for fairly straight-forward &ldquo;performance&rdquo; evaluation. However, most of these models are not suitable for human-machine co-creation through live interaction, neither is clear, how such models and resulting creations would be evaluated. This article presents a thorough review of music representation, feature analysis, heuristic algorithms, statistical and parametric modelling, and human and <b>automatic</b> <b>evaluation</b> measures, along with a discussion of which approaches and models seem most suitable for live interaction.</p></p class="citation"></blockquote><h2 id=eessiv-2>eess.IV (2)</h2><h3 id=12--139190-artificial-bee-colony-optimization-of-deep-convolutional-neural-networks-in-the-context-of-biomedical-imaging-adri-gomez-martin-et-al-2024>(1/2 | 139/190) Artificial Bee Colony optimization of Deep Convolutional Neural Networks in the context of Biomedical Imaging (Adri Gomez Martin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Adri Gomez Martin, Carlos Fernandez del Cerro, Monica Abella Garcia, Manuel Desco Menendez. (2024)<br><strong>Artificial Bee Colony optimization of Deep Convolutional Neural Networks in the context of Biomedical Imaging</strong><br><button class=copy-to-clipboard title="Artificial Bee Colony optimization of Deep Convolutional Neural Networks in the context of Biomedical Imaging" index=139>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-139 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-AI, cs-NE, eess-IV, eess.IV<br>Keyword Score: 40<br>Keywords: Convolution, Convolutional Neural Network, Fine-tuning, Transfer Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15246v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15246v1.pdf filename=2402.15246v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Most efforts in Computer Vision focus on natural images or artwork, which differ significantly both in size and contents from the kind of data biomedical image processing deals with. Thus, <b>Transfer</b> <b>Learning</b> models often prove themselves suboptimal for these tasks, even after manual <b>finetuning.</b> The development of architectures from scratch is oftentimes unfeasible due to the vastness of the hyperparameter space and a shortage of time, computational resources and Deep Learning experts in most biomedical research laboratories. An alternative to manually defining the models is the use of Neuroevolution, which employs metaheuristic techniques to optimize Deep Learning architectures. However, many algorithms proposed in the neuroevolutive literature are either too unreliable or limited to a small, predefined region of the hyperparameter space. To overcome these shortcomings, we propose the Chimera Algorithm, a novel, hybrid neuroevolutive algorithm that integrates the Artificial Bee Colony Algorithm with Evolutionary Computation tools to generate models from scratch, as well as to refine a given previous architecture to better fit the task at hand. The Chimera Algorithm has been validated with two datasets of natural and medical images, producing models that surpassed the performance of those coming from <b>Transfer</b> <b>Learning.</b></p></p class="citation"></blockquote><h3 id=22--140190-low-rank-representations-meets-deep-unfolding-a-generalized-and-interpretable-network-for-hyperspectral-anomaly-detection-chenyu-li-et-al-2024>(2/2 | 140/190) Low-Rank Representations Meets Deep Unfolding: A Generalized and Interpretable Network for Hyperspectral Anomaly Detection (Chenyu Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chenyu Li, Bing Zhang, Danfeng Hong, Jing Yao, Jocelyn Chanussot. (2024)<br><strong>Low-Rank Representations Meets Deep Unfolding: A Generalized and Interpretable Network for Hyperspectral Anomaly Detection</strong><br><button class=copy-to-clipboard title="Low-Rank Representations Meets Deep Unfolding: A Generalized and Interpretable Network for Hyperspectral Anomaly Detection" index=140>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-140 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, cs-LG, eess-IV, eess.IV<br>Keyword Score: 13<br>Keywords: Anomaly Detection, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15335v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15335v1.pdf filename=2402.15335v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Current hyperspectral <b>anomaly</b> <b>detection</b> (HAD) <b>benchmark</b> datasets suffer from low resolution, simple background, and small size of the detection data. These factors also limit the performance of the well-known low-rank representation (LRR) models in terms of robustness on the separation of background and target features and the reliance on manual parameter selection. To this end, we build a new set of HAD <b>benchmark</b> datasets for improving the robustness of the HAD algorithm in complex scenarios, AIR-HAD for short. Accordingly, we propose a generalized and interpretable HAD network by deeply unfolding a dictionary-learnable LLR model, named LRR-Net$^+$, which is capable of spectrally decoupling the background structure and object properties in a more generalized fashion and eliminating the bias introduced by vital interference targets concurrently. In addition, LRR-Net$^+$ integrates the solution process of the Alternating Direction Method of Multipliers (ADMM) optimizer with the deep network, guiding its search process and imparting a level of interpretability to parameter optimization. Additionally, the integration of physical models with DL techniques eliminates the need for manual parameter tuning. The manually tuned parameters are seamlessly transformed into trainable parameters for deep neural networks, facilitating a more efficient and automated optimization process. Extensive experiments conducted on the AIR-HAD dataset show the superiority of our LRR-Net$^+$ in terms of detection performance and generalization ability, compared to top-performing rivals. Furthermore, the compilable codes and our AIR-HAD <b>benchmark</b> datasets in this paper will be made available freely and openly at \url{https://sites.google.com/view/danfeng-hong}.</p></p class="citation"></blockquote><h2 id=cscr-7>cs.CR (7)</h2><h3 id=17--141190-a-first-look-at-gpt-apps-landscape-and-vulnerability-zejun-zhang-et-al-2024>(1/7 | 141/190) A First Look at GPT Apps: Landscape and Vulnerability (Zejun Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zejun Zhang, Li Zhang, Xin Yuan, Anlan Zhang, Mengwei Xu, Feng Qian. (2024)<br><strong>A First Look at GPT Apps: Landscape and Vulnerability</strong><br><button class=copy-to-clipboard title="A First Look at GPT Apps: Landscape and Vulnerability" index=141>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-141 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CL, cs-CR, cs.CR<br>Keyword Score: 40<br>Keywords: GPT, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15105v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15105v1.pdf filename=2402.15105v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the advancement of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs),</b> increasingly sophisticated and powerful <b>GPTs</b> are entering the market. Despite their popularity, the <b>LLM</b> ecosystem still remains unexplored. Additionally, <b>LLMs&rsquo;</b> susceptibility to attacks raises concerns over safety and plagiarism. Thus, in this work, we conduct a pioneering exploration of <b>GPT</b> stores, aiming to study vulnerabilities and plagiarism within <b>GPT</b> applications. To begin with, we conduct, to our knowledge, the first <b>large-scale</b> <b>monitoring</b> <b>and</b> analysis of two stores, an unofficial GPTStore.AI, and an official OpenAI <b>GPT</b> Store. Then, we propose a TriLevel <b>GPT</b> Reversing (T-GR) strategy for extracting <b>GPT</b> internals. To complete these two tasks efficiently, we develop two automated tools: one for web scraping and another designed for programmatically interacting with <b>GPTs.</b> Our findings reveal a significant enthusiasm among users and developers for <b>GPT</b> interaction and creation, as evidenced by the rapid increase in <b>GPTs</b> and their creators. However, we also uncover a widespread failure to protect <b>GPT</b> internals, with nearly 90% of system <b>prompts</b> easily accessible, leading to considerable plagiarism and duplication among <b>GPTs.</b></p></p class="citation"></blockquote><h3 id=27--142190-bspa-exploring-black-box-stealthy-prompt-attacks-against-image-generators-yu-tian-et-al-2024>(2/7 | 142/190) BSPA: Exploring Black-box Stealthy Prompt Attacks against Image Generators (Yu Tian et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yu Tian, Xiao Yang, Yinpeng Dong, Heming Yang, Hang Su, Jun Zhu. (2024)<br><strong>BSPA: Exploring Black-box Stealthy Prompt Attacks against Image Generators</strong><br><button class=copy-to-clipboard title="BSPA: Exploring Black-box Stealthy Prompt Attacks against Image Generators" index=142>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-142 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CL, cs-CR, cs-CV, cs.CR<br>Keyword Score: 25<br>Keywords: Black Box, Recommendation, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15218v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15218v1.pdf filename=2402.15218v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Extremely large image generators offer significant transformative potential across diverse sectors. It allows users to design specific <b>prompts</b> to generate realistic images through some <b>black-box</b> <b>APIs.</b> However, some studies reveal that image generators are notably susceptible to attacks and generate Not Suitable For Work (NSFW) contents by manually designed toxin texts, especially imperceptible to human observers. We urgently need a multitude of universal and transferable <b>prompts</b> to improve the safety of image generators, especially <b>black-box-released</b> <b>APIs.</b> Nevertheless, they are constrained by labor-intensive design processes and heavily reliant on the quality of the given instructions. To achieve this, we introduce a <b>black-box</b> <b>stealthy</b> <b>prompt</b> attack (BSPA) that adopts a retriever to simulate attacks from API users. It can effectively harness filter scores to tune the retrieval space of sensitive words for matching the input <b>prompts,</b> thereby crafting stealthy <b>prompts</b> tailored for image generators. Significantly, this approach is model-agnostic and requires no internal access to the model&rsquo;s features, ensuring its applicability to a wide range of image generators. Building on BSPA, we have constructed an automated <b>prompt</b> tool and a comprehensive <b>prompt</b> attack dataset (NSFWeval). Extensive experiments demonstrate that BSPA effectively explores the security vulnerabilities in a variety of state-of-the-art available <b>black-box</b> <b>models,</b> including Stable Diffusion XL, Midjourney, and DALL-E 2/3. Furthermore, we develop a resilient text filter and offer targeted <b>recommendations</b> to ensure the security of image generators against <b>prompt</b> attacks in the future.</p></p class="citation"></blockquote><h3 id=37--143190-trec-apt-tactic--technique-recognition-via-few-shot-provenance-subgraph-learning-mingqi-lv-et-al-2024>(3/7 | 143/190) TREC: APT Tactic / Technique Recognition via Few-Shot Provenance Subgraph Learning (Mingqi Lv et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mingqi Lv, HongZhe Gao, Xuebo Qiu, Tieming Chen, Tiantian Zhu. (2024)<br><strong>TREC: APT Tactic / Technique Recognition via Few-Shot Provenance Subgraph Learning</strong><br><button class=copy-to-clipboard title="TREC: APT Tactic / Technique Recognition via Few-Shot Provenance Subgraph Learning" index=143>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-143 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs-LG, cs.CR<br>Keyword Score: 23<br>Keywords: Graph, Few-shot, Few-shot Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15147v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15147v1.pdf filename=2402.15147v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>APT (Advanced Persistent Threat) with the characteristics of persistence, stealth, and diversity is one of the greatest threats against cyber-infrastructure. As a countermeasure, existing studies leverage provenance <b>graphs</b> to capture the complex relations between system entities in a host for effective APT detection. In addition to detecting single attack events as most existing work does, understanding the tactics / techniques (e.g., Kill-Chain, ATT&amp;CK) applied to organize and accomplish the APT attack campaign is more important for security operations. Existing studies try to manually design a set of rules to map low-level system events to high-level APT tactics / techniques. However, the rule based methods are coarse-grained and lack generalization ability, thus they can only recognize APT tactics and cannot identify fine-grained APT techniques and mutant APT attacks. In this paper, we propose TREC, the first attempt to recognize APT tactics / techniques from provenance <b>graphs</b> by exploiting deep learning techniques. To address the &ldquo;needle in a haystack&rdquo; problem, TREC segments small and compact subgraphs covering individual APT technique instances from a large provenance <b>graph</b> based on a malicious node detection model and a subgraph sampling algorithm. To address the &ldquo;training sample scarcity&rdquo; problem, TREC trains the APT tactic / technique recognition model in a <b>few-shot</b> <b>learning</b> manner by adopting a Siamese neural network. We evaluate TREC based on a customized dataset collected and made public by our team. The experiment results show that TREC significantly outperforms state-of-the-art systems in APT tactic recognition and TREC can also effectively identify APT techniques.</p></p class="citation"></blockquote><h3 id=47--144190-a-blockchain-enabled-framework-of-uav-coordination-for-post-disaster-networks-sana-hafeez-et-al-2024>(4/7 | 144/190) A Blockchain-Enabled Framework of UAV Coordination for Post-Disaster Networks (Sana Hafeez et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sana Hafeez, Runze Cheng, Lina Mohjazi, Muhammad Ali Imran, Yao Sun. (2024)<br><strong>A Blockchain-Enabled Framework of UAV Coordination for Post-Disaster Networks</strong><br><button class=copy-to-clipboard title="A Blockchain-Enabled Framework of UAV Coordination for Post-Disaster Networks" index=144>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-144 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs-SY, cs.CR, eess-SY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15331v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15331v1.pdf filename=2402.15331v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Emergency communication is critical but challenging after natural disasters when ground infrastructure is devastated. Unmanned aerial vehicles (UAVs) offer enormous potential for agile relief coordination in these scenarios. However, effectively leveraging UAV fleets poses additional challenges around security, privacy, and efficient collaboration across response agencies. This paper presents a robust blockchain-enabled framework to address these challenges by integrating a consortium blockchain model, smart contracts, and cryptographic techniques to securely coordinate UAV fleets for disaster response. Specifically, we make two key contributions: a consortium blockchain architecture for secure and private multi-agency coordination; and an optimized consensus protocol balancing efficiency and fault tolerance using a delegated proof of stake practical byzantine fault tolerance (DPoS-PBFT). Comprehensive <b>simulations</b> showcase the framework&rsquo;s ability to enhance transparency, automation, scalability, and cyber-attack resilience for UAV coordination in post-disaster networks.</p></p class="citation"></blockquote><h3 id=57--145190-on-the-usability-of-next-generation-authentication-a-study-on-eye-movement-and-brainwave-based-mechanisms-matin-fallahi-et-al-2024>(5/7 | 145/190) On the Usability of Next-Generation Authentication: A Study on Eye Movement and Brainwave-based Mechanisms (Matin Fallahi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Matin Fallahi, Patricia Arias Cabarcos, Thorsten Strufe. (2024)<br><strong>On the Usability of Next-Generation Authentication: A Study on Eye Movement and Brainwave-based Mechanisms</strong><br><button class=copy-to-clipboard title="On the Usability of Next-Generation Authentication: A Study on Eye Movement and Brainwave-based Mechanisms" index=145>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-145 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs-HC, cs.CR<br>Keyword Score: 10<br>Keywords: Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15388v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15388v1.pdf filename=2402.15388v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Passwords remain a widely-used authentication mechanism, despite their well-known security and usability limitations. To improve on this situation, next-generation authentication mechanisms, based on behavioral biometric factors such as eye movement and brainwave have emerged. However, their usability remains relatively under-explored. To fill this gap, we conducted an empirical user study (n=32 participants) to evaluate three brain-based and three eye-based authentication mechanisms, using both qualitative and quantitative methods. Our findings show good overall usability according to the System Usability Scale for both categories of mechanisms, with average SUS scores in the range of 78.6-79.6 and the best mechanisms rated with an &ldquo;excellent&rdquo; score. Participants particularly identified brainwave authentication as more secure yet more privacy-invasive and effort-intensive compared to eye movement authentication. However, the significant number of neutral responses indicates participants&rsquo; need for more detailed information about the security and privacy implications of these authentication methods. Building on the collected evidence, we identify three key areas for improvement: privacy, authentication interface design, and verification time. We offer <b>recommendations</b> for designers and developers to improve the usability and security of next-generation authentication mechanisms.</p></p class="citation"></blockquote><h3 id=67--146190-sok-what-dont-we-know-understanding-security-vulnerabilities-in-snarks-stefanos-chaliasos-et-al-2024>(6/7 | 146/190) SoK: What don&rsquo;t we know? Understanding Security Vulnerabilities in SNARKs (Stefanos Chaliasos et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Stefanos Chaliasos, Jens Ernstberger, David Theodore, David Wong, Mohammad Jahanara, Benjamin Livshits. (2024)<br><strong>SoK: What don&rsquo;t we know? Understanding Security Vulnerabilities in SNARKs</strong><br><button class=copy-to-clipboard title="SoK: What don't we know? Understanding Security Vulnerabilities in SNARKs" index=146>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-146 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs.CR<br>Keyword Score: 10<br>Keywords: Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15293v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15293v1.pdf filename=2402.15293v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Zero-knowledge proofs (ZKPs) have evolved from being a theoretical concept providing privacy and verifiability to having practical, real-world implementations, with SNARKs (Succinct Non-Interactive Argument of Knowledge) emerging as one of the most significant innovations. Prior work has mainly focused on designing more efficient SNARK systems and providing security proofs for them. Many think of SNARKs as &ldquo;just math,&rdquo; implying that what is proven to be correct and secure is correct in practice. In contrast, this paper focuses on assessing end-to-end security properties of real-life SNARK implementations. We start by building foundations with a system model and by establishing threat models and defining adversarial roles for systems that use SNARKs. Our study encompasses an extensive analysis of 141 actual vulnerabilities in SNARK implementations, providing a detailed taxonomy to aid developers and security researchers in understanding the security threats in systems employing SNARKs. Finally, we evaluate existing defense mechanisms and offer <b>recommendations</b> for enhancing the security of SNARK-based systems, paving the way for more robust and reliable implementations in the future.</p></p class="citation"></blockquote><h3 id=77--147190-chu-ko-nu-a-reliable-efficient-and-anonymously-authentication-enabled-realization-for-multi-round-secure-aggregation-in-federated-learning-kaiping-cui-et-al-2024>(7/7 | 147/190) Chu-ko-nu: A Reliable, Efficient, and Anonymously Authentication-Enabled Realization for Multi-Round Secure Aggregation in Federated Learning (Kaiping Cui et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kaiping Cui, Xia Feng, Liangmin Wang, Haiqin Wu, Xiaoyu Zhang, Boris Düdder. (2024)<br><strong>Chu-ko-nu: A Reliable, Efficient, and Anonymously Authentication-Enabled Realization for Multi-Round Secure Aggregation in Federated Learning</strong><br><button class=copy-to-clipboard title="Chu-ko-nu: A Reliable, Efficient, and Anonymously Authentication-Enabled Realization for Multi-Round Secure Aggregation in Federated Learning" index=147>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-147 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs-DC, cs-LG, cs.CR<br>Keyword Score: 10<br>Keywords: Federated Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15111v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15111v1.pdf filename=2402.15111v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Secure aggregation enables <b>federated</b> <b>learning</b> (FL) to perform collaborative training of clients from local gradient updates without exposing raw data. However, existing secure aggregation schemes inevitably perform an expensive fresh setup per round because each client needs to establish fresh input-independent secrets over different rounds. The latest research, Flamingo (S&amp;P 2023), designed a share-transfer-based reusable secret key to support the server continuously performing multiple rounds of aggregation. Nevertheless, the share transfer mechanism it proposed can only be achieved with P probability, which has limited reliability. To tackle the aforementioned problems, we propose a more reliable and anonymously authenticated scheme called Chu-ko-nu for multi-round secure aggregation. Specifically, in terms of share transfer, Chu-ko-nu breaks the probability P barrier by supplementing a redistribution process of secret key components (the sum of all components is the secret key), thus ensuring the reusability of the secret key. Based on this reusable secret key, Chu-ko-nu can efficiently perform consecutive aggregation in the following rounds. Furthermore, considering the client identity authentication and privacy protection issue most approaches ignore, Chu-ko-nu introduces a zero-knowledge proof-based authentication mechanism. It can support clients anonymously participating in FL training and enables the server to authenticate clients effectively in the presence of various attacks. Rigorous security proofs and extensive experiments demonstrated that Chu-ko-nu can provide reliable and anonymously authenticated aggregation for FL with low aggregation costs, at least a 21.02% reduction compared to the state-of-the-art schemes.</p></p class="citation"></blockquote><h2 id=csse-3>cs.SE (3)</h2><h3 id=13--148190-studying-llm-performance-on-closed--and-open-source-data-toufique-ahmed-et-al-2024>(1/3 | 148/190) Studying LLM Performance on Closed- and Open-source Data (Toufique Ahmed et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Toufique Ahmed, Christian Bird, Premkumar Devanbu, Saikat Chakraborty. (2024)<br><strong>Studying LLM Performance on Closed- and Open-source Data</strong><br><button class=copy-to-clipboard title="Studying LLM Performance on Closed- and Open-source Data" index=148>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-148 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-LG, cs-SE, cs.SE<br>Keyword Score: 40<br>Keywords: In-context Learning, In-context Learning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15100v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15100v1.pdf filename=2402.15100v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>models</b> <b>(LLMs)</b> are finding wide use in software engineering practice. These models are extremely data-hungry, and are largely trained on open-source (OSS) code distributed with permissive licenses. In terms of actual use however, a great deal of software development still occurs in the for-profit/proprietary sphere, where the code under development is not, and never has been, in the public domain; thus, many developers, do their work, and use <b>LLMs,</b> in settings where the models may not be as familiar with the code under development. In such settings, do <b>LLMs</b> work as well as they do for OSS code? If not, what are the differences? When performance differs, what are the possible causes, and are there work-arounds? In this paper, we examine this issue using proprietary, closed-source software data from Microsoft, where most proprietary code is in C# and C++. We find that performance for C# changes little from OSS &ndash;> proprietary code, but does significantly reduce for C++; we find that this difference is attributable to differences in identifiers. We also find that some performance degradation, in some cases, can be ameliorated efficiently by <b>in-context</b> <b>learning.</b></p></p class="citation"></blockquote><h3 id=23--149190-llm-compdroid-repairing-configuration-compatibility-bugs-in-android-apps-with-pre-trained-large-language-models-zhijie-liu-et-al-2024>(2/3 | 149/190) LLM-CompDroid: Repairing Configuration Compatibility Bugs in Android Apps with Pre-trained Large Language Models (Zhijie Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhijie Liu, Yutian Tang, Meiyun Li, Xin Jin, Yunfei Long, Liang Feng Zhang, Xiapu Luo. (2024)<br><strong>LLM-CompDroid: Repairing Configuration Compatibility Bugs in Android Apps with Pre-trained Large Language Models</strong><br><button class=copy-to-clipboard title="LLM-CompDroid: Repairing Configuration Compatibility Bugs in Android Apps with Pre-trained Large Language Models" index=149>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-149 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-SE, cs.SE<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15078v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15078v1.pdf filename=2402.15078v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>XML configurations are integral to the Android development framework, particularly in the realm of UI display. However, these configurations can introduce compatibility issues (bugs), resulting in divergent visual outcomes and system crashes across various Android API versions (levels). In this study, we systematically investigate <b>LLM-based</b> approaches for detecting and repairing configuration compatibility bugs. Our findings highlight certain limitations of <b>LLMs</b> in effectively identifying and resolving these bugs, while also revealing their potential in addressing complex, hard-to-repair issues that traditional tools struggle with. Leveraging these insights, we introduce the <b>LLM-CompDroid</b> framework, which combines the strengths of <b>LLMs</b> and traditional tools for bug resolution. Our experimental results demonstrate a significant enhancement in bug resolution performance by <b>LLM-CompDroid,</b> with <b>LLM-CompDroid-GPT-3.5</b> and <b>LLM-CompDroid-GPT-4</b> surpassing the state-of-the-art tool, ConfFix, by at least 9.8% and 10.4% in both Correct and Correct@k metrics, respectively. This innovative approach holds promise for advancing the reliability and robustness of Android applications, making a valuable contribution to the field of software development.</p></p class="citation"></blockquote><h3 id=33--150190-towards-model-driven-dashboard-generation-for-systems-of-systems-maria-teresa-rossi-et-al-2024>(3/3 | 150/190) Towards Model-Driven Dashboard Generation for Systems-of-Systems (Maria Teresa Rossi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Maria Teresa Rossi, Alessandro Tundo, Leonardo Mariani. (2024)<br><strong>Towards Model-Driven Dashboard Generation for Systems-of-Systems</strong><br><button class=copy-to-clipboard title="Towards Model-Driven Dashboard Generation for Systems-of-Systems" index=150>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-150 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-SE, cs.SE<br>Keyword Score: 10<br>Keywords: Human Intervention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15257v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15257v1.pdf filename=2402.15257v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Configuring and evolving dashboards in complex and large-scale Systems-of-Systems (SoS) can be an expensive and cumbersome task due to the many Key Performance Indicators (KPIs) that are usually collected and have to be arranged in a number of visualizations. Unfortunately, setting up dashboards is still a largely manual and error-prone task requiring extensive <b>human</b> <b>intervention.</b> This short paper describes emerging results about the definition of a model-driven technology-agnostic approach that can automatically transform a simple list of KPIs into a dashboard model, and then translate the model into an actual dashboard for a target dashboard technology. Dashboard customization can be efficiently obtained by solely modifying the abstract model representation, freeing operators from expensive interactions with actual dashboards.</p></p class="citation"></blockquote><h2 id=csgt-3>cs.GT (3)</h2><h3 id=13--151190-human-vs-generative-ai-in-content-creation-competition-symbiosis-or-conflict-fan-yao-et-al-2024>(1/3 | 151/190) Human vs. Generative AI in Content Creation Competition: Symbiosis or Conflict? (Fan Yao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fan Yao, Chuanhao Li, Denis Nekipelov, Hongning Wang, Haifeng Xu. (2024)<br><strong>Human vs. Generative AI in Content Creation Competition: Symbiosis or Conflict?</strong><br><button class=copy-to-clipboard title="Human vs. Generative AI in Content Creation Competition: Symbiosis or Conflict?" index=151>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-151 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.GT<br>Categories: cs-GT, cs-HC, cs.GT<br>Keyword Score: 30<br>Keywords: Generative AI, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15467v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15467v1.pdf filename=2402.15467v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The advent of <b>generative</b> <b>AI</b> (GenAI) technology produces transformative impact on the content creation landscape, offering alternative approaches to produce diverse, high-quality content across media, thereby reshaping online ecosystems but also raising concerns about market over-saturation and the potential marginalization of human creativity. Our work introduces a competition model generalized from the Tullock contest to analyze the tension between human creators and GenAI. Our theory and <b>simulations</b> suggest that despite challenges, a stable equilibrium between human and AI-generated content is possible. Our work contributes to understanding the competitive dynamics in the content creation industry, offering insights into the future interplay between human creativity and technological advancements in GenAI.</p></p class="citation"></blockquote><h3 id=23--152190-platforms-for-efficient-and-incentive-aware-collaboration-nika-haghtalab-et-al-2024>(2/3 | 152/190) Platforms for Efficient and Incentive-Aware Collaboration (Nika Haghtalab et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nika Haghtalab, Mingda Qiao, Kunhe Yang. (2024)<br><strong>Platforms for Efficient and Incentive-Aware Collaboration</strong><br><button class=copy-to-clipboard title="Platforms for Efficient and Incentive-Aware Collaboration" index=152>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-152 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.GT<br>Categories: cs-DS, cs-GT, cs-MA, cs.GT<br>Keyword Score: 13<br>Keywords: Graph, Federated Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15169v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15169v1.pdf filename=2402.15169v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Collaboration is crucial for reaching collective goals. However, its effectiveness is often undermined by the strategic behavior of individual agents &ndash; a fact that is captured by a high Price of Stability (PoS) in recent literature [Blum et al., 2021]. Implicit in the traditional PoS analysis is the assumption that agents have full knowledge of how their tasks relate to one another. We offer a new perspective on bringing about efficient collaboration among strategic agents using information design. Inspired by the growing importance of collaboration in machine learning (such as platforms for collaborative <b>federated</b> <b>learning</b> and data cooperatives), we propose a framework where the platform has more information about how the agents&rsquo; tasks relate to each other than the agents themselves. We characterize how and to what degree such platforms can leverage their information advantage to steer strategic agents toward efficient collaboration. Concretely, we consider collaboration networks where each node is a task type held by one agent, and each task benefits from contributions made in their inclusive neighborhood of tasks. This network structure is known to the agents and the platform, but only the platform knows each agent&rsquo;s real location &ndash; from the agents&rsquo; perspective, their location is determined by a random permutation. We employ private Bayesian persuasion and design two families of persuasive signaling schemes that the platform can use to ensure a small total workload when agents follow the signal. The first family aims to achieve the minmax optimal approximation ratio compared to the optimal collaboration, which is shown to be $\Theta(\sqrt{n})$ for unit-weight <b>graphs,</b> $\Theta(n^{2/3})$ for <b>graphs</b> with constant minimum edge weights, and $O(n^{3/4})$ for general weighted <b>graphs.</b> The second family ensures per-instance strict improvement compared to full information disclosure.</p></p class="citation"></blockquote><h3 id=33--153190-analyzing-games-in-maker-protocol-part-one-a-multi-agent-influence-diagram-approach-towards-coordination-abhimanyu-nag-et-al-2024>(3/3 | 153/190) Analyzing Games in Maker Protocol Part One: A Multi-Agent Influence Diagram Approach Towards Coordination (Abhimanyu Nag et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Abhimanyu Nag, Samrat Gupta, Sudipan Sinha, Arka Datta. (2024)<br><strong>Analyzing Games in Maker Protocol Part One: A Multi-Agent Influence Diagram Approach Towards Coordination</strong><br><button class=copy-to-clipboard title="Analyzing Games in Maker Protocol Part One: A Multi-Agent Influence Diagram Approach Towards Coordination" index=153>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-153 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.GT<br>Categories: cs-GT, cs.GT, econ-GN, q-fin-EC<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15037v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15037v1.pdf filename=2402.15037v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Decentralized Finance (DeFi) ecosystems, exemplified by the Maker Protocol, rely on intricate games to maintain stability and security. Understanding the dynamics of these games is crucial for ensuring the robustness of the system. This motivating research proposes a novel methodology leveraging Multi-Agent Influence Diagrams (MAID), originally proposed by Koller and Milch, to dissect and analyze the games within the Maker stablecoin protocol. By representing users and governance of the Maker protocol as agents and their interactions as edges in a <b>graph,</b> we capture the complex network of influences governing agent behaviors. Furthermore in the upcoming papers, we will show a Nash Equilibrium model to elucidate strategies that promote coordination and enhance economic security within the ecosystem. Through this approach, we aim to motivate the use of this method to introduce a new method of formal verification of game theoretic security in DeFi platforms.</p></p class="citation"></blockquote><h2 id=cshc-8>cs.HC (8)</h2><h3 id=18--154190-farsight-fostering-responsible-ai-awareness-during-ai-application-prototyping-zijie-j-wang-et-al-2024>(1/8 | 154/190) Farsight: Fostering Responsible AI Awareness During AI Application Prototyping (Zijie J. Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zijie J. Wang, Chinmay Kulkarni, Lauren Wilcox, Michael Terry, Michael Madaio. (2024)<br><strong>Farsight: Fostering Responsible AI Awareness During AI Application Prototyping</strong><br><button class=copy-to-clipboard title="Farsight: Fostering Responsible AI Awareness During AI Application Prototyping" index=154>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-154 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-AI, cs-CY, cs-HC, cs-LG, cs.HC<br>Keyword Score: 30<br>Keywords: Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15350v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15350v1.pdf filename=2402.15350v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Prompt-based</b> interfaces for <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have made prototyping and building AI-powered applications easier than ever before. However, identifying potential harms that may arise from AI applications remains a challenge, particularly during <b>prompt-based</b> prototyping. To address this, we present Farsight, a novel in situ interactive tool that helps people identify potential harms from the AI applications they are prototyping. Based on a user&rsquo;s <b>prompt,</b> Farsight highlights news articles about relevant AI incidents and allows users to explore and edit <b>LLM-generated</b> use cases, stakeholders, and harms. We report design insights from a co-design study with 10 AI prototypers and findings from a user study with 42 AI prototypers. After using Farsight, AI prototypers in our user study are better able to independently identify potential harms associated with a <b>prompt</b> and find our tool more useful and usable than existing resources. Their qualitative feedback also highlights that Farsight encourages them to focus on end-users and think beyond immediate harms. We discuss these findings and reflect on their implications for designing AI prototyping experiences that meaningfully engage with AI harms. Farsight is publicly accessible at: <a href=https://PAIR-code.github.io/farsight>https://PAIR-code.github.io/farsight</a>.</p></p class="citation"></blockquote><h3 id=28--155190-economic-and-financial-learning-with-artificial-intelligence-a-mixed-methods-study-on-chatgpt-holger-arndt-2024>(2/8 | 155/190) Economic and Financial Learning with Artificial Intelligence: A Mixed-Methods Study on ChatGPT (Holger Arndt, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Holger Arndt. (2024)<br><strong>Economic and Financial Learning with Artificial Intelligence: A Mixed-Methods Study on ChatGPT</strong><br><button class=copy-to-clipboard title="Economic and Financial Learning with Artificial Intelligence: A Mixed-Methods Study on ChatGPT" index=155>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-155 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-CY, cs-HC, cs.HC<br>Keyword Score: 30<br>Keywords: ChatGPT, Chatbot, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15278v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15278v1.pdf filename=2402.15278v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the evolving landscape of digital education, <b>chatbots</b> have emerged as potential game-changers, promising personalized and adaptive learning experiences. This research undertook an in-depth exploration of <b>ChatGPT&rsquo;s</b> potential as an educational tool, focusing on user perceptions, experiences and learning outcomes. Through a mixed-methods approach, a diverse group of 102 participants engaged with <b>ChatGPT,</b> providing insights pre- and postinteraction. The study reveals a notable positive shift in perceptions after exposure, underscoring the efficacy of <b>ChatGPT.</b> However, challenges such as <b>prompting</b> effectiveness and information accuracy emerged as pivotal concerns. Introducing the concept of &lsquo;AI-learning-competence&rsquo;, this study lays the groundwork for future research, emphasizing the need for formal training and pedagogical integration of AI tools.</p></p class="citation"></blockquote><h3 id=38--156190-clochat-understanding-how-people-customize-interact-and-experience-personas-in-large-language-models-juhye-ha-et-al-2024>(3/8 | 156/190) CloChat: Understanding How People Customize, Interact, and Experience Personas in Large Language Models (Juhye Ha et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Juhye Ha, Hyeon Jeon, DaEun Han, Jinwook Seo, Changhoon Oh. (2024)<br><strong>CloChat: Understanding How People Customize, Interact, and Experience Personas in Large Language Models</strong><br><button class=copy-to-clipboard title="CloChat: Understanding How People Customize, Interact, and Experience Personas in Large Language Models" index=156>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-156 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-CL, cs-HC, cs.HC<br>Keyword Score: 30<br>Keywords: ChatGPT, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15265v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15265v1.pdf filename=2402.15265v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have facilitated significant strides in generating conversational agents, enabling seamless, contextually relevant dialogues across diverse topics. However, the existing <b>LLM-driven</b> conversational agents have fixed personalities and functionalities, limiting their adaptability to individual user needs. Creating personalized agent personas with distinct expertise or traits can address this issue. Nonetheless, we lack knowledge of how people customize and interact with agent personas. In this research, we investigated how users customize agent personas and their impact on interaction quality, diversity, and dynamics. To this end, we developed CloChat, an interface supporting easy and accurate customization of agent personas in <b>LLMs.</b> We conducted a study comparing how participants interact with CloChat and <b>ChatGPT.</b> The results indicate that participants formed emotional bonds with the customized agents, engaged in more dynamic dialogues, and showed interest in sustaining interactions. These findings contribute to design implications for future systems with conversational agents using <b>LLMs.</b></p></p class="citation"></blockquote><h3 id=48--157190-i-see-an-ic-a-mixed-methods-approach-to-study-human-problem-solving-processes-in-hardware-reverse-engineering-rené-walendy-et-al-2024>(4/8 | 157/190) I see an IC: A Mixed-Methods Approach to Study Human Problem-Solving Processes in Hardware Reverse Engineering (René Walendy et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>René Walendy, Markus Weber, Jingjie Li, Steffen Becker, Carina Wiesen, Malte Elson, Younghyun Kim, Kassem Fawaz, Nikol Rummel, Christof Paar. (2024)<br><strong>I see an IC: A Mixed-Methods Approach to Study Human Problem-Solving Processes in Hardware Reverse Engineering</strong><br><button class=copy-to-clipboard title="I see an IC: A Mixed-Methods Approach to Study Human Problem-Solving Processes in Hardware Reverse Engineering" index=157>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-157 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-HC, cs.HC<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15452v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15452v1.pdf filename=2402.15452v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Trust in digital systems depends on secure hardware, often assured through Hardware Reverse Engineering (HRE). This work develops methods for investigating human problem-solving processes in HRE, an underexplored yet critical aspect. Since reverse engineers rely heavily on visual information, eye tracking holds promise for studying their cognitive processes. To gain further insights, we additionally employ verbal thought protocols during and immediately after HRE tasks: Concurrent and Retrospective Think Aloud. We evaluate the combination of eye tracking and Think Aloud with 41 participants in an HRE <b>simulation.</b> Eye tracking accurately identifies fixations on individual circuit elements and highlights critical components. Based on two use cases, we demonstrate that eye tracking and Think Aloud can complement each other to improve data quality. Our methodological insights can inform future studies in HRE, a specific setting of human-computer interaction, and in other problem-solving settings involving misleading or missing information.</p></p class="citation"></blockquote><h3 id=58--158190-metastates-an-approach-for-representing-human-workers-psychophysiological-states-in-the-industrial-metaverse-aitor-toichoa-eyam-et-al-2024>(5/8 | 158/190) MetaStates: An Approach for Representing Human Workers Psychophysiological States in the Industrial Metaverse (Aitor Toichoa Eyam et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Aitor Toichoa Eyam, Jose L. Martinez Lastra. (2024)<br><strong>MetaStates: An Approach for Representing Human Workers Psychophysiological States in the Industrial Metaverse</strong><br><button class=copy-to-clipboard title="MetaStates: An Approach for Representing Human Workers Psychophysiological States in the Industrial Metaverse" index=158>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-158 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-GR, cs-HC, cs.HC<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15340v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15340v1.pdf filename=2402.15340v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Photo-realistic avatar is a modern term referring to the digital asset that represents a human in computer graphic advance systems such as video games and <b>simulation</b> tools. These avatars utilize the advances in graphic technologies on both software and hardware aspects. While photorealistic avatars are increasingly used in industrial <b>simulations,</b> representing human factors such as human workers internal states, remains a challenge. This article addresses this issue by introducing the concept of MetaStates which are the digitization and representation of the psychophysiological states of a human worker in the digital world. The MetaStates influence the physical representation and performance of a digital human worker while performing a task. To demonstrate this concept the study presents a development of a photorealistic avatar which is integrated into a simulated environment and enhanced with a multi-level graphical representation of different psychophysiological states. This approach represents a major step forward in the use of digital humans for industrial <b>simulations,</b> allowing companies to better leverage the benefits of the Industrial Metaverse in their daily operations and <b>simulations</b> while keeping human workers at the center of the system.</p></p class="citation"></blockquote><h3 id=68--159190-enhancing-icu-patient-recovery-using-llms-to-assist-nurses-in-diary-writing-samuel-kernan-freire-et-al-2024>(6/8 | 159/190) Enhancing ICU Patient Recovery: Using LLMs to Assist Nurses in Diary Writing (Samuel Kernan Freire et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Samuel Kernan Freire, Margo MC van Mol, Carola Schol, Elif Özcan Vieira. (2024)<br><strong>Enhancing ICU Patient Recovery: Using LLMs to Assist Nurses in Diary Writing</strong><br><button class=copy-to-clipboard title="Enhancing ICU Patient Recovery: Using LLMs to Assist Nurses in Diary Writing" index=159>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-159 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-AI, cs-HC, cs.HC<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15205v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15205v1.pdf filename=2402.15205v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Intensive care unit (ICU) patients often develop new health-related problems in their long-term recovery. Health care professionals keeping a diary of a patient&rsquo;s stay is a proven strategy to tackle this but faces several adoption barriers, such as lack of time and difficulty in knowing what to write. <b>Large</b> <b>language</b> <b>models</b> <b>(LLMs),</b> with their ability to generate human-like text and adaptability, could solve these challenges. However, realizing this vision involves addressing several socio-technical and practical research challenges. This paper discusses these challenges and proposes future research directions to utilize the potential of <b>LLMs</b> in ICU diary writing, ultimately improving the long-term recovery outcomes for ICU patients.</p></p class="citation"></blockquote><h3 id=78--160190-hands-free-vr-jorge-askur-vazquez-fernandez-et-al-2024>(7/8 | 160/190) Hands-Free VR (Jorge Askur Vazquez Fernandez et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jorge Askur Vazquez Fernandez, Jae Joong Lee, Santiago Andrés Serrano Vacca, Alejandra Magana, Bedrich Benes, Voicu Popescu. (2024)<br><strong>Hands-Free VR</strong><br><button class=copy-to-clipboard title="Hands-Free VR" index=160>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-160 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-AI, cs-CL, cs-HC, cs.HC<br>Keyword Score: 20<br>Keywords: Fine-tuning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15083v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15083v1.pdf filename=2402.15083v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The paper introduces Hands-Free VR, a voice-based natural-language interface for VR. The user gives a command using their voice, the speech audio data is converted to text using a speech-to-text deep learning model that is <b>fine-tuned</b> for robustness to word phonetic similarity and to spoken English accents, and the text is mapped to an executable VR command using a <b>large</b> <b>language</b> <b>model</b> that is robust to natural language diversity. Hands-Free VR was evaluated in a controlled within-subjects study (N = 22) that asked participants to find specific objects and to place them in various configurations. In the control condition participants used a conventional VR user interface to grab, carry, and position the objects using the handheld controllers. In the experimental condition participants used Hands-Free VR. The results confirm that: (1) Hands-Free VR is robust to spoken English accents, as for 20 of our participants English was not their first language, and to word phonetic similarity, correctly transcribing the voice command 96.71% of the time; (2) Hands-Free VR is robust to natural language diversity, correctly mapping the transcribed command to an executable command in 97.83% of the time; (3) Hands-Free VR had a significant efficiency advantage over the conventional VR interface in terms of task completion time, total viewpoint translation, total view direction rotation, and total left and right hand translations; (4) Hands-Free VR received high user preference ratings in terms of ease of use, intuitiveness, ergonomics, reliability, and desirability.</p></p class="citation"></blockquote><h3 id=88--161190-the-affecttoolbox-affect-analysis-for-everyone-silvan-mertes-et-al-2024>(8/8 | 161/190) The AffectToolbox: Affect Analysis for Everyone (Silvan Mertes et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Silvan Mertes, Dominik Schiller, Michael Dietz, Elisabeth André, Florian Lingenfelser. (2024)<br><strong>The AffectToolbox: Affect Analysis for Everyone</strong><br><button class=copy-to-clipboard title="The AffectToolbox: Affect Analysis for Everyone" index=161>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-161 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-AI, cs-HC, cs-LG, cs.HC<br>Keyword Score: 13<br>Keywords: Multi-modal, Emotion Recognition<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15195v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15195v1.pdf filename=2402.15195v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the field of affective computing, where research continually advances at a rapid pace, the demand for user-friendly tools has become increasingly apparent. In this paper, we present the AffectToolbox, a novel software system that aims to support researchers in developing affect-sensitive studies and prototypes. The proposed system addresses the challenges posed by existing frameworks, which often require profound programming knowledge and cater primarily to power-users or skilled developers. Aiming to facilitate ease of use, the AffectToolbox requires no programming knowledge and offers its functionality to reliably analyze the affective state of users through an accessible graphical user interface. The architecture encompasses a variety of models for <b>emotion</b> <b>recognition</b> on multiple affective channels and modalities, as well as an elaborate fusion system to merge <b>multi-modal</b> assessments into a unified result. The entire system is open-sourced and will be publicly available to ensure easy integration into more complex applications through a well-structured, Python-based code base - therefore marking a substantial contribution toward advancing affective computing research and fostering a more collaborative and inclusive environment within this interdisciplinary field.</p></p class="citation"></blockquote><h2 id=astro-phim-1>astro-ph.IM (1)</h2><h3 id=11--162190-classification-of-compact-radio-sources-in-the-galactic-plane-with-supervised-machine-learning-s-riggi-et-al-2024>(1/1 | 162/190) Classification of compact radio sources in the Galactic plane with supervised machine learning (S. Riggi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>S. Riggi, G. Umana, C. Trigilio, C. Bordiu, F. Bufano, A. Ingallinera, F. Cavallaro, Y. Gordon, R. P. Norris, G. Gürkan, P. Leto, C. Buemi, S. Loru, A. M. Hopkins, M. D. Filipović, T. Cecconello. (2024)<br><strong>Classification of compact radio sources in the Galactic plane with supervised machine learning</strong><br><button class=copy-to-clipboard title="Classification of compact radio sources in the Galactic plane with supervised machine learning" index=162>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-162 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: astro-ph.IM<br>Categories: astro-ph-IM, astro-ph.IM, cs-LG, stat-ML<br>Keyword Score: 30<br>Keywords: Convolution, Convolutional Neural Network, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15232v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15232v1.pdf filename=2402.15232v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Generation of science-ready data from processed data products is one of the major challenges in next-generation radio continuum surveys with the Square Kilometre Array (SKA) and its precursors, due to the expected data volume and the need to achieve a high degree of automated processing. Source extraction, characterization, and classification are the major stages involved in this process. In this work we focus on the classification of compact radio sources in the Galactic plane using both radio and infrared images as inputs. To this aim, we produced a curated dataset of ~20,000 images of compact sources of different astronomical classes, obtained from past radio and infrared surveys, and novel radio data from pilot surveys carried out with the Australian SKA Pathfinder (ASKAP). Radio spectral index information was also obtained for a subset of the data. We then trained two different classifiers on the produced dataset. The first model uses gradient-boosted decision trees and is trained on a set of pre-computed features derived from the data, which include radio-infrared colour indices and the radio spectral index. The second model is trained directly on multi-channel images, employing <b>convolutional</b> <b>neural</b> <b>networks.</b> Using a completely <b>supervised</b> procedure, we obtained a high classification accuracy (F1-score>90%) for separating Galactic objects from the extragalactic background. Individual class discrimination performances, ranging from 60% to 75%, increased by 10% when adding far-infrared and spectral index information, with extragalactic objects, PNe and HII regions identified with higher accuracies. The implemented tools and trained models were publicly released, and made available to the radioastronomical community for future application on new radio data.</p></p class="citation"></blockquote><h2 id=eessas-2>eess.AS (2)</h2><h3 id=12--163190-childaugment-data-augmentation-methods-for-zero-resource-childrens-speaker-verification-vishwanath-pratap-singh-et-al-2024>(1/2 | 163/190) ChildAugment: Data Augmentation Methods for Zero-Resource Children&rsquo;s Speaker Verification (Vishwanath Pratap Singh et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Vishwanath Pratap Singh, Md Sahidullah, Tomi Kinnunen. (2024)<br><strong>ChildAugment: Data Augmentation Methods for Zero-Resource Children&rsquo;s Speaker Verification</strong><br><button class=copy-to-clipboard title="ChildAugment: Data Augmentation Methods for Zero-Resource Children's Speaker Verification" index=163>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-163 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.AS<br>Categories: cs-SD, eess-AS, eess.AS<br>Keyword Score: 30<br>Keywords: Data Augmentation, Fine-tuning, Low-Resource<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15214v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15214v1.pdf filename=2402.15214v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The accuracy of modern automatic speaker verification (ASV) systems, when trained exclusively on adult <b>data,</b> <b>drops</b> substantially when applied to children&rsquo;s speech. The scarcity of children&rsquo;s speech corpora hinders <b>fine-tuning</b> ASV systems for children&rsquo;s speech. Hence, there is a timely need to explore more effective ways of reusing adults&rsquo; speech <b>data.</b> <b>One</b> promising approach is to align vocal-tract parameters between adults and children through children-specific <b>data</b> <b>augmentation,</b> referred here to as ChildAugment. Specifically, we modify the formant frequencies and formant bandwidths of adult speech to emulate children&rsquo;s speech. The modified spectra are used to train ECAPA-TDNN (emphasized channel attention, propagation, and aggregation in time-delay neural network) recognizer for children. We compare ChildAugment against various state-of-the-art <b>data</b> <b>augmentation</b> techniques for children&rsquo;s ASV. We also extensively compare different scoring methods, including cosine scoring, PLDA (probabilistic linear discriminant analysis), and NPLDA (neural PLDA). We also propose a low-complexity weighted cosine score for extremely <b>low-resource</b> children ASV. Our findings on the CSLU kids corpus indicate that ChildAugment holds promise as a simple, acoustics-motivated approach, for improving state-of-the-art deep learning based ASV for children. We achieve up to 12.45% (boys) and 11.96% (girls) relative improvement over the baseline.</p></p class="citation"></blockquote><h3 id=22--164190-high-resolution-guitar-transcription-via-domain-adaptation-xavier-riley-et-al-2024>(2/2 | 164/190) High Resolution Guitar Transcription via Domain Adaptation (Xavier Riley et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xavier Riley, Drew Edwards, Simon Dixon. (2024)<br><strong>High Resolution Guitar Transcription via Domain Adaptation</strong><br><button class=copy-to-clipboard title="High Resolution Guitar Transcription via Domain Adaptation" index=164>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-164 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.AS<br>Categories: cs-LG, cs-SD, eess-AS, eess.AS<br>Keyword Score: 20<br>Keywords: Zero-shot, Domain Adaptation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15258v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15258v1.pdf filename=2402.15258v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Automatic music transcription (AMT) has achieved high accuracy for piano due to the availability of large, high-quality datasets such as MAESTRO and MAPS, but comparable datasets are not yet available for other instruments. In recent work, however, it has been demonstrated that aligning scores to transcription model activations can produce high quality AMT training data for instruments other than piano. Focusing on the guitar, we refine this approach to training on score data using a dataset of commercially available score-audio pairs. We propose the use of a high-resolution piano transcription model to train a new guitar transcription model. The resulting model obtains state-of-the-art transcription results on GuitarSet in a <b>zero-shot</b> context, improving on previously published methods.</p></p class="citation"></blockquote><h2 id=csit-2>cs.IT (2)</h2><h3 id=12--165190-pre-chirp-domain-index-modulation-for-affine-frequency-division-multiplexing-guangyao-liu-et-al-2024>(1/2 | 165/190) Pre-Chirp-Domain Index Modulation for Affine Frequency Division Multiplexing (Guangyao Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Guangyao Liu, Tianqi Mao, Ruiqi Liu, Zhenyu Xiao. (2024)<br><strong>Pre-Chirp-Domain Index Modulation for Affine Frequency Division Multiplexing</strong><br><button class=copy-to-clipboard title="Pre-Chirp-Domain Index Modulation for Affine Frequency Division Multiplexing" index=165>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-165 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, eess-SP, math-IT<br>Keyword Score: 30<br>Keywords: Fine-tuning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15185v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15185v1.pdf filename=2402.15185v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Affine frequency division multiplexing (AFDM), tailored as a novel multicarrier technique utilizing chirp signals for high-mobility communications, exhibits marked advantages compared to traditional orthogonal frequency division multiplexing (OFDM). AFDM is based on the discrete affine Fourier transform (DAFT) with two modifiable parameters of the chirp signals, termed as the pre-chirp parameter and post-chirp parameter, respectively. These parameters can be <b>fine-tuned</b> to avoid overlapping channel paths with different delays or Doppler shifts, leading to performance enhancement especially for doubly dispersive channel. In this paper, we propose a novel AFDM structure with the pre-chirp index modulation (PIM) philosophy (AFDM-PIM), which can embed additional information bits into the pre-chirp parameter design for both spectral and energy efficiency enhancement. Specifically, we first demonstrate that the application of distinct pre-chirp parameters to various subcarriers in the AFDM modulation process maintains the orthogonality among these subcarriers. Then, different pre-chirp parameters are flexibly assigned to each AFDM subcarrier according to the incoming bits. By such arrangement, aside from classical phase/amplitude modulation, extra binary bits can be implicitly conveyed by the indices of selected pre-chirping parameters realizations without additional energy consumption. At the receiver, both a maximum likelihood (ML) detector and a reduced-complexity ML-minimum mean square error (ML-MMSE) detector are employed to recover the information bits. It has been shown via <b>simulations</b> that the proposed AFDM-PIM exhibits superior bit error rate (BER) performance compared to classical AFDM, OFDM and IM-aided OFDM algorithms.</p></p class="citation"></blockquote><h3 id=22--166190-iterative-inversion-of-elaa-mimo-channels-using-symmetric-rank-1-regularization-jinfei-wang-et-al-2024>(2/2 | 166/190) Iterative Inversion of (ELAA-)MIMO Channels Using Symmetric Rank-$1$ Regularization (Jinfei Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jinfei Wang, Yi Ma, Rahim Tafazolli. (2024)<br><strong>Iterative Inversion of (ELAA-)MIMO Channels Using Symmetric Rank-$1$ Regularization</strong><br><button class=copy-to-clipboard title="Iterative Inversion of (ELAA-)MIMO Channels Using Symmetric Rank-$1$ Regularization" index=166>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-166 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, eess-SP, math-IT<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15334v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15334v1.pdf filename=2402.15334v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>While iterative matrix inversion methods excel in computational efficiency, memory optimization, and support for parallel and distributed computing when managing large matrices, their limitations are also evident in multiple-input multiple-output (MIMO) fading channels. These methods encounter challenges related to slow convergence and diminished accuracy, especially in ill-conditioned scenarios, hindering their application in future MIMO networks such as extra-large aperture array (ELAA). To address these challenges, this paper proposes a novel matrix regularization method termed symmetric rank-$1$ regularization (SR-$1$R). The proposed method functions by augmenting the channel matrix with a symmetric rank-$1$ matrix, with the primary goal of minimizing the condition number of the resultant regularized matrix. This significantly improves the matrix condition, enabling fast and accurate iterative inversion of the regularized matrix. Then, the inverse of the original channel matrix is obtained by applying the Sherman-Morrison transform on the outcome of iterative inversions. Our eigenvalue analysis unveils the best channel condition that can be achieved by an optimized SR-$1$R matrix. Moreover, a power iteration-assisted (PIA) approach is proposed to find the optimum SR-$1$R matrix without need of eigenvalue decomposition. The proposed approach exhibits logarithmic algorithm-depth in parallel computing for MIMO precoding. Finally, computer <b>simulations</b> demonstrate that SR-$1$R has the potential to reduce iterative iterations by up to $33%$, while also significantly improve symbol error probability by approximately an order of magnitude.</p></p class="citation"></blockquote><h2 id=csar-1>cs.AR (1)</h2><h3 id=11--167190-toward-high-performance-programmable-extreme-edge-intelligence-for-neuromorphic-vision-sensors-utilizing-magnetic-domain-wall-motion-based-mtj-md-abdullah-al-kaiser-et-al-2024>(1/1 | 167/190) Toward High Performance, Programmable Extreme-Edge Intelligence for Neuromorphic Vision Sensors utilizing Magnetic Domain Wall Motion-based MTJ (Md Abdullah-Al Kaiser et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Md Abdullah-Al Kaiser, Gourav Datta, Peter A. Beerel, Akhilesh R. Jaiswal. (2024)<br><strong>Toward High Performance, Programmable Extreme-Edge Intelligence for Neuromorphic Vision Sensors utilizing Magnetic Domain Wall Motion-based MTJ</strong><br><button class=copy-to-clipboard title="Toward High Performance, Programmable Extreme-Edge Intelligence for Neuromorphic Vision Sensors utilizing Magnetic Domain Wall Motion-based MTJ" index=167>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-167 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AR<br>Categories: cs-AR, cs-ET, cs.AR, eess-IV<br>Keyword Score: 30<br>Keywords: Convolution, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15121v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15121v1.pdf filename=2402.15121v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The desire to empower resource-limited edge devices with computer vision (CV) must overcome the high energy consumption of collecting and processing vast sensory data. To address the challenge, this work proposes an energy-efficient non-von-Neumann in-pixel processing solution for neuromorphic vision sensors employing emerging (X) magnetic domain wall magnetic tunnel junction (MDWMTJ) for the first time, in conjunction with CMOS-based neuromorphic pixels. Our hybrid CMOS+X approach performs in-situ massively parallel asynchronous analog <b>convolution,</b> exhibiting low power consumption and high accuracy across various CV applications by leveraging the non-volatility and programmability of the MDWMTJ. Moreover, our developed device-circuit-algorithm co-design framework captures device constraints (low tunnel-magnetoresistance, low dynamic range) and circuit constraints (non-linearity, process variation, area consideration) based on monte-carlo <b>simulations</b> and device parameters utilizing GF22nm FD-SOI technology. Our experimental results suggest we can achieve an average of 45.3% reduction in backend-processor energy, maintaining similar front-end energy compared to the state-of-the-art and high accuracy of 79.17% and 95.99% on the DVS-CIFAR10 and IBM DVS128-Gesture datasets, respectively.</p></p class="citation"></blockquote><h2 id=csai-2>cs.AI (2)</h2><h3 id=12--168190-a-relation-interactive-approach-for-message-passing-in-hyper-relational-knowledge-graphs-yonglin-jing-2024>(1/2 | 168/190) A Relation-Interactive Approach for Message Passing in Hyper-relational Knowledge Graphs (Yonglin Jing, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yonglin Jing. (2024)<br><strong>A Relation-Interactive Approach for Message Passing in Hyper-relational Knowledge Graphs</strong><br><button class=copy-to-clipboard title="A Relation-Interactive Approach for Message Passing in Hyper-relational Knowledge Graphs" index=168>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-168 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 26<br>Keywords: Message-Passing, Graph, Benchmarking, Knowledge Graph, Knowledge Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15140v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15140v1.pdf filename=2402.15140v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Hyper-relational <b>knowledge</b> <b>graphs</b> <b>(KGs)</b> contain additional key-value pairs, providing more information about the relations. In many scenarios, the same relation can have distinct key-value pairs, making the original triple fact more recognizable and specific. Prior studies on hyper-relational <b>KGs</b> have established a solid standard method for hyper-relational <b>graph</b> encoding. In this work, we propose a <b>message-passing-based</b> <b>graph</b> encoder with global relation structure awareness ability, which we call ReSaE. Compared to the prior state-of-the-art approach, ReSaE emphasizes the interaction of relations during message passing process and optimizes the readout structure for link prediction tasks. Overall, ReSaE gives a encoding solution for hyper-relational <b>KGs</b> and ensures stronger performance on downstream link prediction tasks. Our experiments demonstrate that ReSaE achieves state-of-the-art performance on multiple link prediction <b>benchmarks.</b> Furthermore, we also analyze the influence of different model structures on model performance.</p></p class="citation"></blockquote><h3 id=22--169190-agentohana-design-unified-data-and-training-pipeline-for-effective-agent-learning-jianguo-zhang-et-al-2024>(2/2 | 169/190) AgentOhana: Design Unified Data and Training Pipeline for Effective Agent Learning (Jianguo Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jianguo Zhang, Tian Lan, Rithesh Murthy, Zhiwei Liu, Weiran Yao, Juntao Tan, Thai Hoang, Liangwei Yang, Yihao Feng, Zuxin Liu, Tulika Awalgaonkar, Juan Carlos Niebles, Silvio Savarese, Shelby Heinecke, Huan Wang, Caiming Xiong. (2024)<br><strong>AgentOhana: Design Unified Data and Training Pipeline for Effective Agent Learning</strong><br><button class=copy-to-clipboard title="AgentOhana: Design Unified Data and Training Pipeline for Effective Agent Learning" index=169>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-169 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CL, cs-LG, cs.AI<br>Keyword Score: 23<br>Keywords: Benchmarking, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15506v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15506v1.pdf filename=2402.15506v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Autonomous agents powered by <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have garnered significant research attention. However, fully harnessing the potential of <b>LLMs</b> for agent-based tasks presents inherent challenges due to the heterogeneous nature of diverse data sources featuring multi-turn trajectories. In this paper, we introduce \textbf{AgentOhana} as a comprehensive solution to address these challenges. \textit{AgentOhana} aggregates agent trajectories from distinct environments, spanning a wide array of scenarios. It meticulously standardizes and unifies these trajectories into a consistent format, streamlining the creation of a generic data loader optimized for agent training. Leveraging the data unification, our training pipeline maintains equilibrium across different data sources and preserves independent randomness across devices during dataset partitioning and model training. Additionally, we present \textbf{xLAM-v0.1}, a <b>large</b> <b>action</b> <b>model</b> tailored for AI agents, which demonstrates exceptional performance across various <b>benchmarks.</b></p></p class="citation"></blockquote><h2 id=csce-1>cs.CE (1)</h2><h3 id=11--170190-design-and-optimization-of-functionally-graded-triangular-lattices-for-multiple-loading-conditions-junpeng-wang-et-al-2024>(1/1 | 170/190) Design and Optimization of Functionally-graded Triangular Lattices for Multiple Loading Conditions (Junpeng Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Junpeng Wang, Rüdiger Westermann, Xifeng Gao, Jun Wu. (2024)<br><strong>Design and Optimization of Functionally-graded Triangular Lattices for Multiple Loading Conditions</strong><br><button class=copy-to-clipboard title="Design and Optimization of Functionally-graded Triangular Lattices for Multiple Loading Conditions" index=170>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-170 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CE<br>Categories: cs-CE, cs.CE<br>Keyword Score: 25<br>Keywords: Geometry, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15458v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15458v1.pdf filename=2402.15458v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Aligning lattices based on local stress distribution is crucial for achieving exceptional structural stiffness. However, this aspect has primarily been investigated under a single load condition, where stress in 2D can be described by two orthogonal principal stress directions. In this paper, we introduce a novel approach for designing and optimizing triangular lattice structures to accommodate multiple loading conditions, which means multiple stress fields. Our method comprises two main steps: homogenization-based topology optimization and <b>geometry-based</b> de-homogenization. To ensure the geometric regularity of triangular lattices, we propose a simplified version of the general rank-$3$ laminate and parameterize the design domain using equilateral triangles with unique thickness per edge. During optimization, the thicknesses and orientation of each equilateral triangle are adjusted based on the homogenized properties of triangular lattices. Our numerical findings demonstrate that this proposed simplification results in only a slight decrease in stiffness, while achieving triangular lattice structures with a compelling geometric regularity. In <b>geometry-based</b> de-homogenization, we adopt a field-aligned triangulation approach to generate a globally consistent triangle mesh, with each triangle oriented according to the optimized orientation field. Our approach for handling multiple loading conditions, akin to de-homogenization techniques for single loading conditions, yields highly detailed, optimized, spatially varying lattice structures. The method is computationally efficient, as <b>simulations</b> and optimizations are conducted at a low-resolution discretization of the design domain. Furthermore, since our approach is <b>geometry-based,</b> obtained structures are encoded into a compact geometric format that facilitates downstream operations such as editing and fabrication.</p></p class="citation"></blockquote><h2 id=mathna-2>math.NA (2)</h2><h3 id=12--171190-a-cweno-large-time-step-scheme-for-hamilton--jacobi-equations-e-carlini-et-al-2024>(1/2 | 171/190) A CWENO large time-step scheme for Hamilton&ndash;Jacobi equations (E. Carlini et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>E. Carlini, R. Ferretti, S. Preda, M. Semplice. (2024)<br><strong>A CWENO large time-step scheme for Hamilton&ndash;Jacobi equations</strong><br><button class=copy-to-clipboard title="A CWENO large time-step scheme for Hamilton--Jacobi equations" index=171>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-171 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: 65N12, 65M10, 49L25, cs-NA, math-NA, math-OC, math.NA<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15367v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15367v1.pdf filename=2402.15367v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We propose a high order numerical scheme for time-dependent first order Hamilton&ndash;Jacobi&ndash;Bellman equations. In particular we propose to combine a semi-Lagrangian scheme with a Central Weighted Non-Oscillatory reconstruction. We prove a convergence result in the case of state- and time-independent Hamiltonians. Numerical <b>simulations</b> are presented in space dimensions one and two, also for more general state- and time-dependent Hamiltonians, demonstrating superior performance in terms of CPU time gain compared with a semi-Lagrangian scheme coupled with Weighted Non-Oscillatory reconstructions.</p></p class="citation"></blockquote><h3 id=22--172190-a-unified-constraint-formulation-of-immersed-body-techniques-for-coupled-fluid-solid-motion-continuous-equations-and-numerical-algorithms-amneet-pal-singh-bhalla-et-al-2024>(2/2 | 172/190) A unified constraint formulation of immersed body techniques for coupled fluid-solid motion: continuous equations and numerical algorithms (Amneet Pal Singh Bhalla et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Amneet Pal Singh Bhalla, Neelesh A. Patankar. (2024)<br><strong>A unified constraint formulation of immersed body techniques for coupled fluid-solid motion: continuous equations and numerical algorithms</strong><br><button class=copy-to-clipboard title="A unified constraint formulation of immersed body techniques for coupled fluid-solid motion: continuous equations and numerical algorithms" index=172>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-172 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: cs-NA, math-NA, math.NA, physics-flu-dyn<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15161v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15161v1.pdf filename=2402.15161v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Numerical <b>simulation</b> of moving immersed solid bodies in fluids is now practiced routinely following pioneering work of Peskin and co-workers on immersed boundary method (IBM), Glowinski and co-workers on fictitious domain method (FDM), and others on related methods. A variety of variants of IBM and FDM approaches have been published, most of which rely on using a background mesh for the fluid equations and tracking the solid body using Lagrangian points. The key idea that is common to these methods is to assume that the entire fluid-solid domain is a fluid and then to constrain the fluid within the solid domain to move in accordance with the solid governing equations. The immersed solid body can be rigid or deforming. Thus, in all these methods the fluid domain is extended into the solid domain. In this review, we provide a mathemarical perspective of various immersed methods by recasting the governing equations in an extended domain form for the fluid. The solid equations are used to impose appropriate constraints on the fluid that is extended into the solid domain. This leads to extended domain constrained fluid-solid governing equations that provide a unified framework for various immersed body techniques. The unified constrained governing equations in the strong form are independent of the temporal or spatial discretization schemes. We show that particular choices of time stepping and spatial discretization lead to different techniques reported in literature ranging from freely moving rigid to elastic self-propelling bodies. These techniques have wide ranging applications including aquatic locomotion, underwater vehicles, car aerodynamics, and organ physiology (e.g. cardiac flow, esophageal transport, respiratory flows), wave energy convertors, among others. We conclude with comments on outstanding challenges and future directions.</p></p class="citation"></blockquote><h2 id=csma-2>cs.MA (2)</h2><h3 id=12--173190-shapley-value-based-multi-agent-reinforcement-learning-theory-method-and-its-application-to-energy-network-jianhong-wang-2024>(1/2 | 173/190) Shapley Value Based Multi-Agent Reinforcement Learning: Theory, Method and Its Application to Energy Network (Jianhong Wang, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jianhong Wang. (2024)<br><strong>Shapley Value Based Multi-Agent Reinforcement Learning: Theory, Method and Its Application to Energy Network</strong><br><button class=copy-to-clipboard title="Shapley Value Based Multi-Agent Reinforcement Learning: Theory, Method and Its Application to Energy Network" index=173>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-173 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.MA<br>Categories: cs-LG, cs-MA, cs.MA<br>Keyword Score: 20<br>Keywords: Markov Decision Process, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15324v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15324v1.pdf filename=2402.15324v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Multi-agent <b>reinforcement</b> <b>learning</b> is an area of rapid advancement in artificial intelligence and machine learning. One of the important questions to be answered is how to conduct credit assignment in a multi-agent system. There have been many schemes designed to conduct credit assignment by multi-agent <b>reinforcement</b> <b>learning</b> algorithms. Although these credit assignment schemes have been proved useful in improving the performance of multi-agent <b>reinforcement</b> <b>learning,</b> most of them are designed heuristically without a rigorous theoretic basis and therefore infeasible to understand how agents cooperate. In this thesis, we aim at investigating the foundation of credit assignment in multi-agent <b>reinforcement</b> <b>learning</b> via cooperative game theory. We first extend a game model called convex game and a payoff distribution scheme called Shapley value in cooperative game theory to <b>Markov</b> <b>decision</b> <b>process,</b> named as <b>Markov</b> <b>convex</b> <b>game</b> and <b>Markov</b> <b>Shapley</b> <b>value</b> respectively. We represent a global reward game as a <b>Markov</b> <b>convex</b> <b>game</b> under the grand coalition. As a result, <b>Markov</b> <b>Shapley</b> <b>value</b> can be reasonably used as a credit assignment scheme in the global reward game. <b>Markov</b> <b>Shapley</b> <b>value</b> possesses the following virtues: (i) efficiency; (ii) identifiability of dummy agents; (iii) reflecting the contribution and (iv) symmetry, which form the fair credit assignment. Based on <b>Markov</b> <b>Shapley</b> <b>value,</b> we propose three multi-agent <b>reinforcement</b> <b>learning</b> algorithms called SHAQ, SQDDPG and SMFPPO. Furthermore, we extend <b>Markov</b> <b>convex</b> <b>game</b> to partial observability to deal with the partially observable problems, named as partially observable <b>Markov</b> <b>convex</b> <b>game.</b> In application, we evaluate SQDDPG and SMFPPO on the real-world problem in energy networks.</p></p class="citation"></blockquote><h3 id=22--174190-open-ad-hoc-teamwork-with-cooperative-game-theory-jianhong-wang-et-al-2024>(2/2 | 174/190) Open Ad Hoc Teamwork with Cooperative Game Theory (Jianhong Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jianhong Wang, Yang Li, Yuan Zhang, Wei Pan, Samuel Kaski. (2024)<br><strong>Open Ad Hoc Teamwork with Cooperative Game Theory</strong><br><button class=copy-to-clipboard title="Open Ad Hoc Teamwork with Cooperative Game Theory" index=174>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-174 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.MA<br>Categories: cs-LG, cs-MA, cs.MA<br>Keyword Score: 13<br>Keywords: Graph, Graph Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15259v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15259v1.pdf filename=2402.15259v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Ad hoc teamwork poses a challenging problem, requiring the design of an agent to collaborate with teammates without prior coordination or joint training. Open ad hoc teamwork further complicates this challenge by considering environments with a changing number of teammates, referred to as open teams. The state-of-the-art solution to this problem is <b>graph-based</b> <b>policy</b> <b>learning</b> (GPL), leveraging the generalizability of <b>graph</b> <b>neural</b> <b>networks</b> to handle an unrestricted number of agents and effectively address open teams. GPL&rsquo;s performance is superior to other methods, but its joint Q-value representation presents challenges for interpretation, hindering further development of this research line and applicability. In this paper, we establish a new theory to give an interpretation for the joint Q-value representation employed in GPL, from the perspective of cooperative game theory. Building on our theory, we propose a novel algorithm based on GPL framework, to complement the critical features that facilitate learning, but overlooked in GPL. Through experiments, we demonstrate the correctness of our theory by comparing the performance of the resulting algorithm with GPL in dynamic team compositions.</p></p class="citation"></blockquote><h2 id=eesssy-1>eess.SY (1)</h2><h3 id=11--175190-safety-optimized-reinforcement-learning-via-multi-objective-policy-optimization-homayoun-honari-et-al-2024>(1/1 | 175/190) Safety Optimized Reinforcement Learning via Multi-Objective Policy Optimization (Homayoun Honari et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Homayoun Honari, Mehran Ghafarian Tamizi, Homayoun Najjaran. (2024)<br><strong>Safety Optimized Reinforcement Learning via Multi-Objective Policy Optimization</strong><br><button class=copy-to-clipboard title="Safety Optimized Reinforcement Learning via Multi-Objective Policy Optimization" index=175>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-175 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-AI, cs-LG, cs-RO, cs-SY, eess-SY, eess.SY<br>Keyword Score: 20<br>Keywords: Fine-tuning, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15197v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15197v1.pdf filename=2402.15197v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Safe <b>reinforcement</b> <b>learning</b> (Safe RL) refers to a class of techniques that aim to prevent RL algorithms from violating constraints in the process of decision-making and exploration during trial and error. In this paper, a novel model-free Safe RL algorithm, formulated based on the multi-objective policy optimization framework is introduced where the policy is optimized towards optimality and safety, simultaneously. The optimality is achieved by the environment reward function that is subsequently shaped using a safety critic. The advantage of the Safety Optimized RL (SORL) algorithm compared to the traditional Safe RL algorithms is that it omits the need to constrain the policy search space. This allows SORL to find a natural tradeoff between safety and optimality without compromising the performance in terms of either safety or optimality due to strict search space constraints. Through our theoretical analysis of SORL, we propose a condition for SORL&rsquo;s converged policy to guarantee safety and then use it to introduce an aggressiveness parameter that allows for <b>fine-tuning</b> the mentioned tradeoff. The experimental results obtained in seven different robotic environments indicate a considerable reduction in the number of safety violations along with higher, or competitive, policy returns, in comparison to six different state-of-the-art Safe RL methods. The results demonstrate the significant superiority of the proposed SORL algorithm in safety-critical applications.</p></p class="citation"></blockquote><h2 id=mathst-2>math.ST (2)</h2><h3 id=12--176190-universal-lower-bounds-and-optimal-rates-achieving-minimax-clustering-error-in-sub-exponential-mixture-models-maximilien-dreveton-et-al-2024>(1/2 | 176/190) Universal Lower Bounds and Optimal Rates: Achieving Minimax Clustering Error in Sub-Exponential Mixture Models (Maximilien Dreveton et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Maximilien Dreveton, Alperen Gözeten, Matthias Grossglauser, Patrick Thiran. (2024)<br><strong>Universal Lower Bounds and Optimal Rates: Achieving Minimax Clustering Error in Sub-Exponential Mixture Models</strong><br><button class=copy-to-clipboard title="Universal Lower Bounds and Optimal Rates: Achieving Minimax Clustering Error in Sub-Exponential Mixture Models" index=176>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-176 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.ST<br>Categories: 62H30, 62F12, 62B10, cs-LG, math-ST, math.ST, stat-ML, stat-TH<br>Keyword Score: 13<br>Keywords: Clustering, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15432v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15432v1.pdf filename=2402.15432v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Clustering</b> is a pivotal challenge in <b>unsupervised</b> machine learning and is often investigated through the lens of mixture models. The optimal error rate for recovering cluster labels in Gaussian and sub-Gaussian mixture models involves ad hoc signal-to-noise ratios. Simple iterative algorithms, such as Lloyd&rsquo;s algorithm, attain this optimal error rate. In this paper, we first establish a universal lower bound for the error rate in <b>clustering</b> any mixture model, expressed through a Chernoff divergence, a more versatile measure of model information than signal-to-noise ratios. We then demonstrate that iterative algorithms attain this lower bound in mixture models with sub-exponential tails, notably emphasizing location-scale mixtures featuring Laplace-distributed errors. Additionally, for datasets better modelled by Poisson or Negative Binomial mixtures, we study mixture models whose distributions belong to an exponential family. In such mixtures, we establish that Bregman hard <b>clustering,</b> a variant of Lloyd&rsquo;s algorithm employing a Bregman divergence, is rate optimal.</p></p class="citation"></blockquote><h3 id=22--177190-the-umeyama-algorithm-for-matching-correlated-gaussian-geometric-models-in-the-low-dimensional-regime-shuyang-gong-et-al-2024>(2/2 | 177/190) The Umeyama algorithm for matching correlated Gaussian geometric models in the low-dimensional regime (Shuyang Gong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shuyang Gong, Zhangsong Li. (2024)<br><strong>The Umeyama algorithm for matching correlated Gaussian geometric models in the low-dimensional regime</strong><br><button class=copy-to-clipboard title="The Umeyama algorithm for matching correlated Gaussian geometric models in the low-dimensional regime" index=177>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-177 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.ST<br>Categories: 68Q87 (Primary), 62M15 (Secondary), cs-DS, cs-LG, math-PR, math-ST, math.ST, stat-TH<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15095v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15095v1.pdf filename=2402.15095v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Motivated by the problem of matching two correlated random geometric <b>graphs,</b> we study the problem of matching two Gaussian geometric models correlated through a latent node permutation. Specifically, given an unknown permutation $\pi^<em>$ on ${1,\ldots,n}$ and given $n$ i.i.d. pairs of correlated Gaussian vectors ${X_{\pi^</em>(i)},Y_i}$ in $\mathbb{R}^d$ with noise parameter $\sigma$, we consider two types of (correlated) weighted complete <b>graphs</b> with edge weights given by $A_{i,j}=\langle X_i,X_j \rangle$, $B_{i,j}=\langle Y_i,Y_j \rangle$. The goal is to recover the hidden vertex correspondence $\pi^<em>$ based on the observed matrices $A$ and $B$. For the low-dimensional regime where $d=O(\log n)$, Wang, Wu, Xu, and Yolou [WWXY22+] established the information thresholds for exact and almost exact recovery in matching correlated Gaussian geometric models. They also conducted numerical experiments for the classical Umeyama algorithm. In our work, we prove that this algorithm achieves exact recovery of $\pi^</em>$ when the noise parameter $\sigma=o(d^{-3}n^{-2/d})$, and almost exact recovery when $\sigma=o(d^{-3}n^{-1/d})$. Our results approach the information thresholds up to a $\operatorname{poly}(d)$ factor in the low-dimensional regime.</p></p class="citation"></blockquote><h2 id=mathoc-1>math.OC (1)</h2><h3 id=11--178190-closed-loop-design-for-scalable-performance-of-vehicular-formations-jonas-hansson-et-al-2024>(1/1 | 178/190) Closed-loop design for scalable performance of vehicular formations (Jonas Hansson et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jonas Hansson, Emma Tegling. (2024)<br><strong>Closed-loop design for scalable performance of vehicular formations</strong><br><button class=copy-to-clipboard title="Closed-loop design for scalable performance of vehicular formations" index=178>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-178 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.OC<br>Categories: cs-SY, eess-SY, math-OC, math.OC<br>Keyword Score: 13<br>Keywords: Message-Passing, Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15208v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15208v1.pdf filename=2402.15208v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents a novel control design for vehicular formations, which is an alternative to the conventional second-order consensus protocol. The design is motivated by the closed-loop system, which we construct as first-order systems connected in series, and is therefore called serial consensus. The serial consensus design will guarantee stability of the closed-loop system under the minimum requirement of the underlying communication <b>graphs</b> each containing a connected spanning tree &ndash; something that is not true in general for the conventional consensus protocols. Here, we show that the serial consensus design also gives guarantees on the worst-case transient behavior of the formation, which are independent of the number of vehicles and the underlying <b>graph</b> structure. In particular this shows that the serial consensus design can be used to guarantee string stability of the formation, and is therefore suitable for directed formations. We show that it can be implemented through message passing or measurements to neighbors at most two hops away. The results are illustrated through numerical examples.</p></p class="citation"></blockquote><h2 id=cssi-1>cs.SI (1)</h2><h3 id=11--179190-decoding-the-pulse-of-community-during-disasters-resilience-analysis-based-on-fluctuations-in-latent-lifestyle-signatures-within-human-visitation-networks-junwei-ma-et-al-2024>(1/1 | 179/190) Decoding the Pulse of Community during Disasters: Resilience Analysis Based on Fluctuations in Latent Lifestyle Signatures within Human Visitation Networks (Junwei Ma et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Junwei Ma, Ali Mostafavi. (2024)<br><strong>Decoding the Pulse of Community during Disasters: Resilience Analysis Based on Fluctuations in Latent Lifestyle Signatures within Human Visitation Networks</strong><br><button class=copy-to-clipboard title="Decoding the Pulse of Community during Disasters: Resilience Analysis Based on Fluctuations in Latent Lifestyle Signatures within Human Visitation Networks" index=179>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-179 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SI<br>Categories: cs-SI, cs.SI, physics-soc-ph, stat-AP<br>Keyword Score: 10<br>Keywords: Knowledge Distillation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15434v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15434v1.pdf filename=2402.15434v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Examining the impact of disasters on life activities of populations is critical for understanding community resilience dynamics, yet it remains insufficiently studied in the existing literature. In this study, we leveraged data from more than 1.2 million anonymized human mobility communications across 30 parishes in Louisiana to construct a temporal network that tracks visitation to places from which we characterized human lifestyle signatures before, during, and after Hurricane Ida in 2021. Utilizing the motif model, we <b>distilled</b> complex human lifestyles into identifiable patterns and clustered them into classes: commute, healthcare, dining out, and youth-oriented lifestyle. We defined two metrics to evaluate disruption and recovery fluctuations in lifestyle patterns during the perturbation period compared to the steady period: 1) frequency (daily number of motifs), and 2) proximity (daily average distance of motifs). The results indicate significant dynamics in lifestyle patterns due to the hurricane, with essential facilities (e.g., healthcare) demonstrating a swift recovery. The study underscores the heterogeneity of locations visited and the necessity of integrating both essential and non-essential facilities into disaster response initiatives. Furthermore, our study reveals sustained changes in lifestyle patterns, highlighting the long-term impact of the hurricane on daily life. These insights demonstrate the significance of examining lifestyle signatures and their fluctuations in evaluating disaster resilience patterns for affected communities. The outcomes of this study are poised to aid emergency managers and public officials to more effectively evaluate and monitor disaster impacts and recovery based on changes in lifestyle patterns in the community.</p></p class="citation"></blockquote><h2 id=csdc-2>cs.DC (2)</h2><h3 id=12--180190-convergence-analysis-of-split-federated-learning-on-heterogeneous-data-pengchao-han-et-al-2024>(1/2 | 180/190) Convergence Analysis of Split Federated Learning on Heterogeneous Data (Pengchao Han et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Pengchao Han, Chao Huang, Geng Tian, Ming Tang, Xin Liu. (2024)<br><strong>Convergence Analysis of Split Federated Learning on Heterogeneous Data</strong><br><button class=copy-to-clipboard title="Convergence Analysis of Split Federated Learning on Heterogeneous Data" index=180>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-180 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DC<br>Categories: cs-DC, cs-LG, cs.DC<br>Keyword Score: 10<br>Keywords: Federated Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15166v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15166v1.pdf filename=2402.15166v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Split <b>federated</b> <b>learning</b> (SFL) is a recent distributed approach for collaborative model training among multiple clients. In SFL, a global model is typically split into two parts, where clients train one part in a parallel <b>federated</b> <b>manner,</b> and a main server trains the other. Despite the recent research on SFL algorithm development, the convergence analysis of SFL is missing in the literature, and this paper aims to fill this gap. The analysis of SFL can be more challenging than that of <b>federated</b> <b>learning</b> (FL), due to the potential dual-paced updates at the clients and the main server. We provide convergence analysis of SFL for strongly convex and general convex objectives on heterogeneous data. The convergence rates are $O(1/T)$ and $O(1/\sqrt[3]{T})$, respectively, where $T$ denotes the total number of rounds for SFL training. We further extend the analysis to non-convex objectives and where some clients may be unavailable during training. Numerical experiments validate our theoretical results and show that SFL outperforms FL and split learning (SL) when data is highly heterogeneous across a large number of clients.</p></p class="citation"></blockquote><h3 id=22--181190-pico-accelerating-all-k-core-paradigms-on-gpu-chen-zhao-et-al-2024>(2/2 | 181/190) PICO: Accelerating All k-Core Paradigms on GPU (Chen Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chen Zhao, Ting Yu, Zhigao Zheng, Song Jin, Jiawei Jiang, Bo Du, Dacheng Tao. (2024)<br><strong>PICO: Accelerating All k-Core Paradigms on GPU</strong><br><button class=copy-to-clipboard title="PICO: Accelerating All k-Core Paradigms on GPU" index=181>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-181 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DC<br>Categories: cs-DC, cs.DC<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15253v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15253v1.pdf filename=2402.15253v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Core decomposition is a well-established <b>graph</b> mining problem with various applications that involves partitioning the <b>graph</b> into hierarchical subgraphs. Solutions to this problem have been developed using both bottom-up and top-down approaches from the perspective of vertex convergence dependency. However, existing algorithms have not effectively harnessed GPU performance to expedite core decomposition, despite the growing need for enhanced performance. Moreover, approaching performance limitations of core decomposition from two different directions within a parallel synchronization structure has not been thoroughly explored. This paper introduces an efficient GPU acceleration framework, PICO, for the Peel and Index2core paradigms of k-core decomposition. We propose PeelOne, a Peel-based algorithm designed to simplify the parallel logic and minimize atomic operations by eliminating vertices that are &lsquo;under-core&rsquo;. We also propose an Index2core-based algorithm, named HistoCore, which addresses the issue of extensive redundant computations across both vertices and edges. Extensive experiments on NVIDIA RTX 3090 GPU show that PeelOne outperforms all other Peel-based algorithms, and HistoCore outperforms all other Index2core-based algorithms. Furthermore, HistoCore even outperforms PeelOne by 1.1x - 3.2x speedup on six datasets, which breaks the stereotype that the Index2core paradigm performs much worse than the Peel in a shared memory parallel setting.</p></p class="citation"></blockquote><h2 id=csds-5>cs.DS (5)</h2><h3 id=15--182190-algorithmically-fair-maximization-of-multiple-submodular-objective-functions-georgios-amanatidis-et-al-2024>(1/5 | 182/190) Algorithmically Fair Maximization of Multiple Submodular Objective Functions (Georgios Amanatidis et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Georgios Amanatidis, Georgios Birmpas, Philip Lazos, Stefano Leonardi, Rebecca Reiffenhäuser. (2024)<br><strong>Algorithmically Fair Maximization of Multiple Submodular Objective Functions</strong><br><button class=copy-to-clipboard title="Algorithmically Fair Maximization of Multiple Submodular Objective Functions" index=182>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-182 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DS<br>Categories: cs-DS, cs-GT, cs.DS, math-OC<br>Keyword Score: 10<br>Keywords: Fairness<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15155v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15155v1.pdf filename=2402.15155v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Constrained maximization of submodular functions poses a central problem in combinatorial optimization. In many realistic scenarios, a number of agents need to maximize multiple submodular objectives over the same ground set. We study such a setting, where the different solutions must be disjoint, and thus, questions of <b>fairness</b> arise. Inspired from the fair division literature, we suggest a simple round-robin protocol, where agents are allowed to build their solutions one item at a time by taking turns. Unlike what is typical in fair division, however, the prime goal here is to provide a fair algorithmic environment; each agent is allowed to use any algorithm for constructing their respective solutions. We show that just by following simple greedy policies, agents have solid guarantees for both monotone and non-monotone objectives, and for combinatorial constraints as general as $p$-systems (which capture cardinality and matroid intersection constraints). In the monotone case, our results include approximate EF1-type guarantees and their implications in fair division may be of independent interest. Further, although following a greedy policy may not be optimal in general, we show that consistently performing better than that is computationally hard.</p></p class="citation"></blockquote><h3 id=25--183190-on-the-complexity-of-community-aware-network-sparsification-emanuel-herrendorf-et-al-2024>(2/5 | 183/190) On the Complexity of Community-aware Network Sparsification (Emanuel Herrendorf et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Emanuel Herrendorf, Christian Komusiewicz, Nils Morawietz, Frank Sommer. (2024)<br><strong>On the Complexity of Community-aware Network Sparsification</strong><br><button class=copy-to-clipboard title="On the Complexity of Community-aware Network Sparsification" index=183>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-183 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DS<br>Categories: cs-DS, cs-SI, cs.DS<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15494v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15494v1.pdf filename=2402.15494v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Network sparsification is the task of reducing the number of edges of a given <b>graph</b> while preserving some crucial <b>graph</b> property. In community-aware network sparsification, the preserved property concerns the subgraphs that are induced by the communities of the <b>graph</b> which are given as vertex subsets. This is formalized in the $\Pi$-Network Sparsification problem: given an edge-weighted <b>graph</b> $G$, a collection $Z$ of $c$ subsets of $V(G)$ (communities), and two numbers $\ell, b$, the question is whether there exists a spanning subgraph $G&rsquo;$ of $G$ with at most $\ell$ edges of total weight at most $b$ such that $G&rsquo;[C]$ fulfills $\Pi$ for each community $C$. Here, we consider two <b>graph</b> properties $\Pi$: the connectivity property (Connectivity NWS) and the property of having a spanning star (Stars NWS). Since both problems are NP-hard, we study their parameterized and fine-grained complexity. We provide a tight $2^{\Omega(n^2+c)} poly(n+|Z|)$-time running time lower bound based on the ETH for both problems, where $n$ is the number of vertices in $G$. The lower bound holds even in the restricted case when all communities have size at most 4, $G$ is a clique, and every edge has unit weight. For the connectivity property, the unit weight case with $G$ being a clique is the well-studied problem of computing a hypergraph support with a minimum number of edges. We then study the complexity of both problems parameterized by the feedback edge number $t$ of the solution <b>graph</b> $G&rsquo;$. For Stars NWS, we present an XP-algorithm for $t$. This answers an open question by Korach and Stern [Disc. Appl. Math. &lsquo;08] who asked for the existence of polynomial-time algorithms for $t=0$. In contrast, we show for Connectivity NWS that known polynomial-time algorithms for $t=0$ [Korach and Stern, Math. Program. &lsquo;03; Klemz et al., SWAT &lsquo;14] cannot be extended by showing that Connectivity NWS is NP-hard for $t=1$.</p></p class="citation"></blockquote><h3 id=35--184190-graph-partitioning-with-limited-moves-majid-behbahani-et-al-2024>(3/5 | 184/190) Graph Partitioning With Limited Moves (Majid Behbahani et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Majid Behbahani, Mina Dalirrooyfard, Elaheh Fata, Yuriy Nevmyvaka. (2024)<br><strong>Graph Partitioning With Limited Moves</strong><br><button class=copy-to-clipboard title="Graph Partitioning With Limited Moves" index=184>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-184 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DS<br>Categories: cs-DS, cs.DS<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15485v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15485v1.pdf filename=2402.15485v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In many real world networks, there already exists a (not necessarily optimal) $k$-partitioning of the network. Oftentimes, one aims to find a $k$-partitioning with a smaller cut value for such networks by moving only a few nodes across partitions. The number of nodes that can be moved across partitions is often a constraint forced by budgetary limitations. Motivated by such real-world applications, we introduce and study the $r$-move $k$-partitioning~problem, a natural variant of the Multiway cut problem. Given a <b>graph,</b> a set of $k$ terminals and an initial partitioning of the <b>graph,</b> the $r$-move $k$-partitioning~problem aims to find a $k$-partitioning with the minimum-weighted cut among all the $k$-partitionings that can be obtained by moving at most $r$ non-terminal nodes to partitions different from their initial ones. Our main result is a polynomial time $3(r+1)$ approximation algorithm for this problem. We further show that this problem is $W[1]$-hard, and give an FPTAS for when $r$ is a small constant.</p></p class="citation"></blockquote><h3 id=45--185190-tight-approximation-and-kernelization-bounds-for-vertex-disjoint-shortest-paths-matthias-bentert-et-al-2024>(4/5 | 185/190) Tight Approximation and Kernelization Bounds for Vertex-Disjoint Shortest Paths (Matthias Bentert et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Matthias Bentert, Fedor V. Fomin, Petr A. Golovach. (2024)<br><strong>Tight Approximation and Kernelization Bounds for Vertex-Disjoint Shortest Paths</strong><br><button class=copy-to-clipboard title="Tight Approximation and Kernelization Bounds for Vertex-Disjoint Shortest Paths" index=185>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-185 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DS<br>Categories: cs-DS, cs.DS<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15348v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15348v1.pdf filename=2402.15348v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We examine the possibility of approximating Maximum Vertex-Disjoint Shortest Paths. In this problem, the input is an edge-weighted (directed or undirected) $n$-vertex <b>graph</b> $G$ along with $k$ terminal pairs $(s_1,t_1),(s_2,t_2),\ldots,(s_k,t_k)$. The task is to connect as many terminal pairs as possible by pairwise vertex-disjoint paths such that each path is a shortest path between the respective terminals. Our work is anchored in the recent breakthrough by Lochet [SODA &lsquo;21], which demonstrates the polynomial-time solvability of the problem for a fixed value of $k$. Lochet&rsquo;s result implies the existence of a polynomial-time $ck$-approximation for Maximum Vertex-Disjoint Shortest Paths, where $c \leq 1$ is a constant. Our first result suggests that this approximation algorithm is, in a sense, the best we can hope for. More precisely, assuming the gap-ETH, we exclude the existence of an $o(k)$-approximations within $f(k) \cdot $poly($n$) time for any function $f$ that only depends on $k$. Our second result demonstrates the infeasibility of achieving an approximation ratio of $n^{\frac{1}{2}-\varepsilon}$ in polynomial time, unless P = NP. It is not difficult to show that a greedy algorithm selecting a path with the minimum number of arcs results in a $\lceil\sqrt{\ell}\rceil$-approximation, where $\ell$ is the number of edges in all the paths of an optimal solution. Since $\ell \leq n$, this underscores the tightness of the $n^{\frac{1}{2}-\varepsilon}$-inapproximability bound. Additionally, we establish that Maximum Vertex-Disjoint Shortest Paths is fixed-parameter tractable when parameterized by $\ell$ but does not admit a polynomial kernel. Our hardness results hold for undirected <b>graphs</b> with unit weights, while our positive results extend to scenarios where the input <b>graph</b> is directed and features arbitrary (non-negative) edge weights.</p></p class="citation"></blockquote><h3 id=55--186190-tight-inapproximability-of-target-set-reconfiguration-naoto-ohsaka-2024>(5/5 | 186/190) Tight Inapproximability of Target Set Reconfiguration (Naoto Ohsaka, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Naoto Ohsaka. (2024)<br><strong>Tight Inapproximability of Target Set Reconfiguration</strong><br><button class=copy-to-clipboard title="Tight Inapproximability of Target Set Reconfiguration" index=186>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-186 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DS<br>Categories: cs-CC, cs-DM, cs-DS, cs.DS<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15076v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15076v1.pdf filename=2402.15076v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Given a <b>graph</b> $G$ with a vertex threshold function $\tau$, consider a dynamic process in which any inactive vertex $v$ becomes activated whenever at least $\tau(v)$ of its neighbors are activated. A vertex set $S$ is called a target set if all vertices of $G$ would be activated when initially activating vertices of $S$. In the Minmax Target Set Reconfiguration problem, for a <b>graph</b> $G$ and its two target sets $X$ and $Y$, we wish to transform $X$ into $Y$ by repeatedly adding or removing a single vertex, using only target sets of $G$, so as to minimize the maximum size of any intermediate target set. We prove that it is NP-hard to approximate Minmax Target Set Reconfiguration within a factor of $2-o\left(\frac{1}{\operatorname{polylog} n}\right)$, where $n$ is the number of vertices. Our result establishes a tight lower bound on approximability of Minmax Target Set Reconfiguration, which admits a $2$-factor approximation algorithm. The proof is based on a gap-preserving reduction from Target Set Selection to Minmax Target Set Reconfiguration, where NP-hardness of approximation for the former problem is proven by Chen (SIAM J. Discrete Math., 2009) and Charikar, Naamad, and Wirth (APPROX/RANDOM 2016).</p></p class="citation"></blockquote><h2 id=quant-ph-1>quant-ph (1)</h2><h3 id=11--187190-on-the-composable-security-of-weak-coin-flipping-jiawei-wu-et-al-2024>(1/1 | 187/190) On the composable security of weak coin flipping (Jiawei Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiawei Wu, Yanglin Hu, Akshay Bansal, Marco Tomamichel. (2024)<br><strong>On the composable security of weak coin flipping</strong><br><button class=copy-to-clipboard title="On the composable security of weak coin flipping" index=187>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-187 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: quant-ph<br>Categories: cs-CR, quant-ph, quant-ph<br>Keyword Score: 5<br>Keywords: Black Box<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15233v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15233v1.pdf filename=2402.15233v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Weak coin flipping is a cryptographic primitive in which two mutually distrustful parties generate a shared random bit to agree on a winner via remote communication. While a stand-alone secure weak coin flipping protocol can be constructed from noiseless communication channels, its composability has not been explored. In this work, we demonstrate that no weak coin flipping protocol can be abstracted into a <b>black</b> <b>box</b> resource with composable security. Despite this, we also establish the overall stand-alone security of weak coin flipping protocols under sequential composition.</p></p class="citation"></blockquote><h2 id=csdm-1>cs.DM (1)</h2><h3 id=11--188190-some-results-involving-the-a_α-eigenvalues-for-graphs-and-line-graphs-joao-domingos-gomes-da-silva-junior-et-al-2024>(1/1 | 188/190) Some results involving the $A_α$-eigenvalues for graphs and line graphs (Joao Domingos Gomes da Silva Junior et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Joao Domingos Gomes da Silva Junior, Carla Silva Oliveira, Liliana Manuela Gaspar C. da Costa. (2024)<br><strong>Some results involving the $A_α$-eigenvalues for graphs and line graphs</strong><br><button class=copy-to-clipboard title="Some results involving the $A_α$-eigenvalues for graphs and line graphs" index=188>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-188 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DM<br>Categories: 05C05, cs-DM, cs.DM, math-CO<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15470v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15470v1.pdf filename=2402.15470v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Let $G$ be a simple <b>graph</b> with adjacency matrix $A(G)$, signless Laplacian matrix $Q(G)$, degree diagonal matrix $D(G)$ and let $l(G)$ be the line <b>graph</b> of $G$. In 2017, Nikiforov defined the $A_\alpha$-matrix of $G$, $A_\alpha(G)$, as a linear convex combination of $A(G)$ and $D(G)$, the following way, $A_\alpha(G):=\alpha A(G)+(1-\alpha)D(G),$ where $\alpha\in[0,1]$. In this paper, we present some bounds for the eigenvalues of $A_\alpha(G)$ and for the largest and smallest eigenvalues of $A_\alpha(l(G))$. Extremal <b>graphs</b> attaining some of these bounds are characterized.</p></p class="citation"></blockquote><h2 id=cslo-1>cs.LO (1)</h2><h3 id=11--189190-the-flower-calculus-pablo-donato-2024>(1/1 | 189/190) The Flower Calculus (Pablo Donato, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Pablo Donato. (2024)<br><strong>The Flower Calculus</strong><br><button class=copy-to-clipboard title="The Flower Calculus" index=189>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-189 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LO<br>Categories: cs-LO, cs.LO<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15174v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15174v1.pdf filename=2402.15174v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce the flower calculus, a deep inference proof system for intuitionistic first-order logic inspired by Peirce&rsquo;s existential <b>graphs.</b> It works as a rewriting system over inductive objects called &ldquo;flowers&rdquo;, that enjoy both a graphical interpretation as topological diagrams, and a textual presentation as nested sequents akin to coherent formulas. Importantly, the calculus dispenses completely with the traditional notion of symbolic connective, operating solely on nested flowers containing atomic predicates. We prove both the soundness of the full calculus and the completeness of an analytic fragment with respect to Kripke semantics. This provides to our knowledge the first analyticity result for a proof system based on existential <b>graphs,</b> adapting semantic cut-elimination techniques to a deep inference setting. Furthermore, the kernel of rules targetted by completeness is fully invertible, a desirable property for both automated and interactive proof search.</p></p class="citation"></blockquote><h2 id=mathco-1>math.CO (1)</h2><h3 id=11--190190-rectilinear-crossing-number-of-graphs-excluding-single-crossing-graphs-as-minors-vida-dujmović-et-al-2024>(1/1 | 190/190) Rectilinear Crossing Number of Graphs Excluding Single-Crossing Graphs as Minors (Vida Dujmović et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Vida Dujmović, Camille La Rose. (2024)<br><strong>Rectilinear Crossing Number of Graphs Excluding Single-Crossing Graphs as Minors</strong><br><button class=copy-to-clipboard title="Rectilinear Crossing Number of Graphs Excluding Single-Crossing Graphs as Minors" index=190>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-190 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.CO<br>Categories: cs-CG, math-CO, math.CO<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15034v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15034v1.pdf filename=2402.15034v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The crossing number of a <b>graph</b> $G$ is the minimum number of crossings in a drawing of $G$ in the plane. A rectilinear drawing of a <b>graph</b> $G$ represents vertices of $G$ by a set of points in the plane and represents each edge of $G$ by a straight-line segment connecting its two endpoints. The rectilinear crossing number of $G$ is the minimum number of crossings in a rectilinear drawing of $G$. By the crossing lemma, the crossing number of an $n$-vertex <b>graph</b> $G$ can be $O(n)$ only if $|E(G)|\in O(n)$. <b>Graphs</b> of bounded genus and bounded degree (B"{o}r"{o}czky, Pach and T'{o}th, 2006) and in fact all bounded degree proper minor-closed families (Wood and Telle, 2007) have been shown to admit linear crossing number, with tight $\Theta(\Delta n)$ bound shown by Dujmovi'c, Kawarabayashi, Mohar and Wood, 2008. Much less is known about rectilinear crossing number. It is not bounded by any function of the crossing number. We prove that <b>graphs</b> that exclude a single-crossing <b>graph</b> as a minor have the rectilinear crossing number $O(\Delta n)$. This dependence on $n$ and $\Delta$ is best possible. A single-crossing <b>graph</b> is a <b>graph</b> whose crossing number is at most one. Thus the result applies to $K_5$-minor-free <b>graphs,</b> for example. It also applies to bounded treewidth <b>graphs,</b> since each family of bounded treewidth <b>graphs</b> excludes some fixed planar <b>graph</b> as a minor. Prior to our work, the only bounded degree minor-closed families known to have linear rectilinear crossing number were bounded degree <b>graphs</b> of bounded treewidth (Wood and Telle, 2007), as well as, bounded degree $K_{3,3}$-minor-free <b>graphs</b> (Dujmovi'c, Kawarabayashi, Mohar and Wood, 2008). In the case of bounded treewidth <b>graphs,</b> our $O(\Delta n)$ result is again tight and improves on the previous best known bound of $O(\Delta^2 n)$ by Wood and Telle, 2007 (obtained for convex geometric drawings).</p></p class="citation"></blockquote></div><div class="row pl-3 pr-3"><div class="col-md-6 share-buttons"></div></div><hr><div class="row next-prev-navigator"><div class="col-md-6 previous-article"><a href=/akitenkrad-blog/posts/arxiv/202402/20240224000000/ title="arXiv @ 2024.02.24" class="btn btn-outline-info"><div><i class="fas fa-chevron-circle-left"></i> Prev</div><div class=next-prev-text>arXiv @ 2024.02.24</div></a></div><div class="col-md-6 next-article"><a href=/akitenkrad-blog/posts/algorithms/bandit-algorithm-basic/ title="Bandit Algorithm Basic" class="btn btn-outline-info"><div>Next <i class="fas fa-chevron-circle-right"></i></div><div class=next-prev-text>Bandit Algorithm Basic</div></a></div></div><hr></div></div></div><a id=scroll-to-top class=btn><i class="fas fa-chevron-circle-up"></i></a></section><section class=toc-section id=toc-section><div class=toc-holder><h5 class="text-center pl-3">Table of Contents</h5><hr><div class=toc><nav id=TableOfContents><ul><li><a href=#primary-categories>Primary Categories</a></li><li><a href=#keywords>Keywords</a></li><li><a href=#cscl-36>cs.CL (36)</a><ul><li><a href=#136--1190-interactive-kbqa-multi-turn-interactions-for-knowledge-base-question-answering-with-large-language-models-guanming-xiong-et-al-2024>(1/36 | 1/190) Interactive-KBQA: Multi-Turn Interactions for Knowledge Base Question Answering with Large Language Models (Guanming Xiong et al., 2024)</a></li><li><a href=#236--2190-fine-tuning-large-language-models-for-domain-specific-machine-translation-jiawei-zheng-et-al-2024>(2/36 | 2/190) Fine-tuning Large Language Models for Domain-specific Machine Translation (Jiawei Zheng et al., 2024)</a></li><li><a href=#336--3190-arabiangpt-native-arabic-gpt-based-large-language-anis-koubaa-et-al-2024>(3/36 | 3/190) ArabianGPT: Native Arabic GPT-based Large Language (Anis Koubaa et al., 2024)</a></li><li><a href=#436--4190-repetition-improves-language-model-embeddings-jacob-mitchell-springer-et-al-2024>(4/36 | 4/190) Repetition Improves Language Model Embeddings (Jacob Mitchell Springer et al., 2024)</a></li><li><a href=#536--5190-a-data-centric-approach-to-generate-faithful-and-high-quality-patient-summaries-with-large-language-models-stefan-hegselmann-et-al-2024>(5/36 | 5/190) A Data-Centric Approach To Generate Faithful and High Quality Patient Summaries with Large Language Models (Stefan Hegselmann et al., 2024)</a></li><li><a href=#636--6190-improving-sentence-embeddings-with-an-automatically-generated-nli-dataset-soma-sato-et-al-2024>(6/36 | 6/190) Improving Sentence Embeddings with an Automatically Generated NLI Dataset (Soma Sato et al., 2024)</a></li><li><a href=#736--7190-nuner-entity-recognition-encoder-pre-training-via-llm-annotated-data-sergei-bogdanov-et-al-2024>(7/36 | 7/190) NuNER: Entity Recognition Encoder Pre-training via LLM-Annotated Data (Sergei Bogdanov et al., 2024)</a></li><li><a href=#836--8190-entity-level-factual-adaptiveness-of-fine-tuning-based-abstractive-summarization-models-jongyoon-song-et-al-2024>(8/36 | 8/190) Entity-level Factual Adaptiveness of Fine-tuning based Abstractive Summarization Models (Jongyoon Song et al., 2024)</a></li><li><a href=#936--9190-colbert-xm-a-modular-multi-vector-representation-model-for-zero-shot-multilingual-information-retrieval-antoine-louis-et-al-2024>(9/36 | 9/190) ColBERT-XM: A Modular Multi-Vector Representation Model for Zero-Shot Multilingual Information Retrieval (Antoine Louis et al., 2024)</a></li><li><a href=#1036--10190-attributionbench-how-hard-is-automatic-attribution-evaluation-yifei-li-et-al-2024>(10/36 | 10/190) AttributionBench: How Hard is Automatic Attribution Evaluation? (Yifei Li et al., 2024)</a></li><li><a href=#1136--11190-ranking-entities-along-conceptual-space-dimensions-with-llms-an-analysis-of-fine-tuning-strategies-nitesh-kumar-et-al-2024>(11/36 | 11/190) Ranking Entities along Conceptual Space Dimensions with LLMs: An Analysis of Fine-Tuning Strategies (Nitesh Kumar et al., 2024)</a></li><li><a href=#1236--12190-how-unethical-are-instruction-centric-responses-of-llms-unveiling-the-vulnerabilities-of-safety-guardrails-to-harmful-queries-somnath-banerjee-et-al-2024>(12/36 | 12/190) How (un)ethical are instruction-centric responses of LLMs? Unveiling the vulnerabilities of safety guardrails to harmful queries (Somnath Banerjee et al., 2024)</a></li><li><a href=#1336--13190-dempt-decoding-enhanced-multi-phase-prompt-tuning-for-making-llms-be-better-context-aware-translators-xinglin-lyu-et-al-2024>(13/36 | 13/190) DeMPT: Decoding-enhanced Multi-phase Prompt Tuning for Making LLMs Be Better Context-aware Translators (Xinglin Lyu et al., 2024)</a></li><li><a href=#1436--14190-self-adaptive-reconstruction-with-contrastive-learning-for-unsupervised-sentence-embeddings-junlong-liu-et-al-2024>(14/36 | 14/190) Self-Adaptive Reconstruction with Contrastive Learning for Unsupervised Sentence Embeddings (Junlong Liu et al., 2024)</a></li><li><a href=#1536--15190-pemt-multi-task-correlation-guided-mixture-of-experts-enables-parameter-efficient-transfer-learning-zhisheng-lin-et-al-2024>(15/36 | 15/190) PEMT: Multi-Task Correlation Guided Mixture-of-Experts Enables Parameter-Efficient Transfer Learning (Zhisheng Lin et al., 2024)</a></li><li><a href=#1636--16190-dual-encoder-exploiting-the-potential-of-syntactic-and-semantic-for-aspect-sentiment-triplet-extraction-xiaowei-zhao-et-al-2024>(16/36 | 16/190) Dual Encoder: Exploiting the Potential of Syntactic and Semantic for Aspect Sentiment Triplet Extraction (Xiaowei Zhao et al., 2024)</a></li><li><a href=#1736--17190-causal-graph-discovery-with-retrieval-augmented-generation-based-large-language-models-yuzhe-zhang-et-al-2024>(17/36 | 17/190) Causal Graph Discovery with Retrieval-Augmented Generation based Large Language Models (Yuzhe Zhang et al., 2024)</a></li><li><a href=#1836--18190-kieval-a-knowledge-grounded-interactive-evaluation-framework-for-large-language-models-zhuohao-yu-et-al-2024>(18/36 | 18/190) KIEval: A Knowledge-grounded Interactive Evaluation Framework for Large Language Models (Zhuohao Yu et al., 2024)</a></li><li><a href=#1936--19190-leveraging-domain-knowledge-for-efficient-reward-modelling-in-rlhf-a-case-study-in-e-commerce-opinion-summarization-swaroop-nath-et-al-2024>(19/36 | 19/190) Leveraging Domain Knowledge for Efficient Reward Modelling in RLHF: A Case-Study in E-Commerce Opinion Summarization (Swaroop Nath et al., 2024)</a></li><li><a href=#2036--20190-gpt-hatecheck-can-llms-write-better-functional-tests-for-hate-speech-detection-yiping-jin-et-al-2024>(20/36 | 20/190) GPT-HateCheck: Can LLMs Write Better Functional Tests for Hate Speech Detection? (Yiping Jin et al., 2024)</a></li><li><a href=#2136--21190-fine-grained-detoxification-via-instance-level-prefixes-for-large-language-models-xin-yi-et-al-2024>(21/36 | 21/190) Fine-Grained Detoxification via Instance-Level Prefixes for Large Language Models (Xin Yi et al., 2024)</a></li><li><a href=#2236--22190-interpreting-context-look-ups-in-transformers-investigating-attention-mlp-interactions-clement-neo-et-al-2024>(22/36 | 22/190) Interpreting Context Look-ups in Transformers: Investigating Attention-MLP Interactions (Clement Neo et al., 2024)</a></li><li><a href=#2336--23190-unlocking-the-power-of-large-language-models-for-entity-alignment-xuhui-jiang-et-al-2024>(23/36 | 23/190) Unlocking the Power of Large Language Models for Entity Alignment (Xuhui Jiang et al., 2024)</a></li><li><a href=#2436--24190-tombench-benchmarking-theory-of-mind-in-large-language-models-zhuang-chen-et-al-2024>(24/36 | 24/190) ToMBench: Benchmarking Theory of Mind in Large Language Models (Zhuang Chen et al., 2024)</a></li><li><a href=#2536--25190-deem-dynamic-experienced-expert-modeling-for-stance-detection-xiaolong-wang-et-al-2024>(25/36 | 25/190) DEEM: Dynamic Experienced Expert Modeling for Stance Detection (Xiaolong Wang et al., 2024)</a></li><li><a href=#2636--26190-on-the-multi-turn-instruction-following-for-conversational-web-agents-yang-deng-et-al-2024>(26/36 | 26/190) On the Multi-turn Instruction Following for Conversational Web Agents (Yang Deng et al., 2024)</a></li><li><a href=#2736--27190-memoryprompt-a-light-wrapper-to-improve-context-tracking-in-pre-trained-language-models-nathanaël-carraz-rakotonirina-et-al-2024>(27/36 | 27/190) MemoryPrompt: A Light Wrapper to Improve Context Tracking in Pre-trained Language Models (Nathanaël Carraz Rakotonirina et al., 2024)</a></li><li><a href=#2836--28190-infusing-hierarchical-guidance-into-prompt-tuning-a-parameter-efficient-framework-for-multi-level-implicit-discourse-relation-recognition-haodong-zhao-et-al-2024>(28/36 | 28/190) Infusing Hierarchical Guidance into Prompt Tuning: A Parameter-Efficient Framework for Multi-level Implicit Discourse Relation Recognition (Haodong Zhao et al., 2024)</a></li><li><a href=#2936--29190-api-blend-a-comprehensive-corpora-for-training-and-benchmarking-api-llms-kinjal-basu-et-al-2024>(29/36 | 29/190) API-BLEND: A Comprehensive Corpora for Training and Benchmarking API LLMs (Kinjal Basu et al., 2024)</a></li><li><a href=#3036--30190-lets-rectify-step-by-step-improving-aspect-based-sentiment-analysis-with-diffusion-models-shunyu-liu-et-al-2024>(30/36 | 30/190) Let&rsquo;s Rectify Step by Step: Improving Aspect-based Sentiment Analysis with Diffusion Models (Shunyu Liu et al., 2024)</a></li><li><a href=#3136--31190-machine-unlearning-of-pre-trained-large-language-models-jin-yao-et-al-2024>(31/36 | 31/190) Machine Unlearning of Pre-trained Large Language Models (Jin Yao et al., 2024)</a></li><li><a href=#3236--32190-carbd-ko-a-contextually-annotated-review-benchmark-dataset-for-aspect-level-sentiment-classification-in-korean-dongjun-jang-et-al-2024>(32/36 | 32/190) CARBD-Ko: A Contextually Annotated Review Benchmark Dataset for Aspect-Level Sentiment Classification in Korean (Dongjun Jang et al., 2024)</a></li><li><a href=#3336--33190-prejudice-and-caprice-a-statistical-framework-for-measuring-social-discrimination-in-large-language-models-yiran-liu-et-al-2024>(33/36 | 33/190) Prejudice and Caprice: A Statistical Framework for Measuring Social Discrimination in Large Language Models (Yiran Liu et al., 2024)</a></li><li><a href=#3436--34190-chitchat-as-interference-adding-user-backstories-to-task-oriented-dialogues-armand-stricker-et-al-2024>(34/36 | 34/190) Chitchat as Interference: Adding User Backstories to Task-Oriented Dialogues (Armand Stricker et al., 2024)</a></li><li><a href=#3536--35190-gotcha-dont-trick-me-with-unanswerable-questions-self-aligning-large-language-models-for-responding-to-unknown-questions-yang-deng-et-al-2024>(35/36 | 35/190) Gotcha! Don&rsquo;t trick me with unanswerable questions! Self-aligning Large Language Models for Responding to Unknown Questions (Yang Deng et al., 2024)</a></li><li><a href=#3636--36190-biomedical-entity-linking-as-multiple-choice-question-answering-zhenxi-lin-et-al-2024>(36/36 | 36/190) Biomedical Entity Linking as Multiple Choice Question Answering (Zhenxi Lin et al., 2024)</a></li></ul></li><li><a href=#cslg-56>cs.LG (56)</a><ul><li><a href=#156--37190-graphedit-large-language-models-for-graph-structure-learning-zirui-guo-et-al-2024>(1/56 | 37/190) GraphEdit: Large Language Models for Graph Structure Learning (Zirui Guo et al., 2024)</a></li><li><a href=#256--38190-sampling-based-distributed-training-with-message-passing-neural-network-priyesh-kakka-et-al-2024>(2/56 | 38/190) Sampling-based Distributed Training with Message Passing Neural Network (Priyesh Kakka et al., 2024)</a></li><li><a href=#356--39190-advancing-parameter-efficiency-in-fine-tuning-via-representation-editing-muling-wu-et-al-2024>(3/56 | 39/190) Advancing Parameter Efficiency in Fine-tuning via Representation Editing (Muling Wu et al., 2024)</a></li><li><a href=#456--40190-mechanics-informed-autoencoder-enables-automated-detection-and-localization-of-unforeseen-structural-damage-xuyang-li-et-al-2024>(4/56 | 40/190) Mechanics-Informed Autoencoder Enables Automated Detection and Localization of Unforeseen Structural Damage (Xuyang Li et al., 2024)</a></li><li><a href=#556--41190-a-comprehensive-survey-of-convolutions-in-deep-learning-applications-challenges-and-future-trends-abolfazl-younesi-et-al-2024>(5/56 | 41/190) A Comprehensive Survey of Convolutions in Deep Learning: Applications, Challenges, and Future Trends (Abolfazl Younesi et al., 2024)</a></li><li><a href=#656--42190-debiasing-machine-learning-models-by-using-weakly-supervised-learning-renan-d-b-brotto-et-al-2024>(6/56 | 42/190) Debiasing Machine Learning Models by Using Weakly Supervised Learning (Renan D. B. Brotto et al., 2024)</a></li><li><a href=#756--43190-gptvq-the-blessing-of-dimensionality-for-llm-quantization-mart-van-baalen-et-al-2024>(7/56 | 43/190) GPTVQ: The Blessing of Dimensionality for LLM Quantization (Mart van Baalen et al., 2024)</a></li><li><a href=#856--44190-counterfactual-generation-with-identifiability-guarantees-hanqi-yan-et-al-2024>(8/56 | 44/190) Counterfactual Generation with Identifiability Guarantees (Hanqi Yan et al., 2024)</a></li><li><a href=#956--45190-smoothed-graph-contrastive-learning-via-seamless-proximity-integration-maysam-behmanesh-et-al-2024>(9/56 | 45/190) Smoothed Graph Contrastive Learning via Seamless Proximity Integration (Maysam Behmanesh et al., 2024)</a></li><li><a href=#1056--46190-united-we-pretrain-divided-we-fail-representation-learning-for-time-series-by-pretraining-on-75-datasets-at-once-maurice-kraus-et-al-2024>(10/56 | 46/190) United We Pretrain, Divided We Fail! Representation Learning for Time Series by Pretraining on 75 Datasets at Once (Maurice Kraus et al., 2024)</a></li><li><a href=#1156--47190-fine-tuning-of-continuous-time-diffusion-models-as-entropy-regularized-control-masatoshi-uehara-et-al-2024>(11/56 | 47/190) Fine-Tuning of Continuous-Time Diffusion Models as Entropy-Regularized Control (Masatoshi Uehara et al., 2024)</a></li><li><a href=#1256--48190-multimodal-transformer-with-a-low-computational-cost-guarantee-sungjin-park-et-al-2024>(12/56 | 48/190) Multimodal Transformer With a Low-Computational-Cost Guarantee (Sungjin Park et al., 2024)</a></li><li><a href=#1356--49190-understanding-oversmoothing-in-diffusion-based-gnns-from-the-perspective-of-operator-semigroup-theory-weichen-zhao-et-al-2024>(13/56 | 49/190) Understanding Oversmoothing in Diffusion-Based GNNs From the Perspective of Operator Semigroup Theory (Weichen Zhao et al., 2024)</a></li><li><a href=#1456--50190-transformers-are-expressive-but-are-they-expressive-enough-for-regression-swaroop-nath-et-al-2024>(14/56 | 50/190) Transformers are Expressive, But Are They Expressive Enough for Regression? (Swaroop Nath et al., 2024)</a></li><li><a href=#1556--51190-does-combining-parameter-efficient-modules-improve-few-shot-transfer-accuracy-nader-asadi-et-al-2024>(15/56 | 51/190) Does Combining Parameter-efficient Modules Improve Few-shot Transfer Accuracy? (Nader Asadi et al., 2024)</a></li><li><a href=#1656--52190-when-in-doubt-think-slow-iterative-reasoning-with-latent-imagination-martin-benfeghoul-et-al-2024>(16/56 | 52/190) When in Doubt, Think Slow: Iterative Reasoning with Latent Imagination (Martin Benfeghoul et al., 2024)</a></li><li><a href=#1756--53190-chunkattention-efficient-self-attention-with-prefix-aware-kv-cache-and-two-phase-partition-lu-ye-et-al-2024>(17/56 | 53/190) ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and Two-Phase Partition (Lu Ye et al., 2024)</a></li><li><a href=#1856--54190-second-order-fine-tuning-without-pain-for-llmsa-hessian-informed-zeroth-order-optimizer-yanjun-zhao-et-al-2024>(18/56 | 54/190) Second-Order Fine-Tuning without Pain for LLMs:A Hessian Informed Zeroth-Order Optimizer (Yanjun Zhao et al., 2024)</a></li><li><a href=#1956--55190-spatially-aware-transformer-memory-for-embodied-agents-junmo-cho-et-al-2024>(19/56 | 55/190) Spatially-Aware Transformer Memory for Embodied Agents (Junmo Cho et al., 2024)</a></li><li><a href=#2056--56190-accelerating-convergence-of-stein-variational-gradient-descent-via-deep-unfolding-yuya-kawamura-et-al-2024>(20/56 | 56/190) Accelerating Convergence of Stein Variational Gradient Descent via Deep Unfolding (Yuya Kawamura et al., 2024)</a></li><li><a href=#2156--57190-trajectory-wise-iterative-reinforcement-learning-framework-for-auto-bidding-haoming-li-et-al-2024>(21/56 | 57/190) Trajectory-wise Iterative Reinforcement Learning Framework for Auto-bidding (Haoming Li et al., 2024)</a></li><li><a href=#2256--58190-enhancing-one-shot-federated-learning-through-data-and-ensemble-co-boosting-rong-dai-et-al-2024>(22/56 | 58/190) Enhancing One-Shot Federated Learning Through Data and Ensemble Co-Boosting (Rong Dai et al., 2024)</a></li><li><a href=#2356--59190-autommlab-automatically-generating-deployable-models-from-language-instructions-for-computer-vision-tasks-zekang-yang-et-al-2024>(23/56 | 59/190) AutoMMLab: Automatically Generating Deployable Models from Language Instructions for Computer Vision Tasks (Zekang Yang et al., 2024)</a></li><li><a href=#2456--60190-optimal-transport-for-structure-learning-under-missing-data-vy-vo-et-al-2024>(24/56 | 60/190) Optimal Transport for Structure Learning Under Missing Data (Vy Vo et al., 2024)</a></li><li><a href=#2556--61190-mspipe-efficient-temporal-gnn-training-via-staleness-aware-pipeline-guangming-sheng-et-al-2024>(25/56 | 61/190) MSPipe: Efficient Temporal GNN Training via Staleness-aware Pipeline (Guangming Sheng et al., 2024)</a></li><li><a href=#2656--62190-cost-adaptive-recourse-recommendation-by-adaptive-preference-elicitation-duy-nguyen-et-al-2024>(26/56 | 62/190) Cost-Adaptive Recourse Recommendation by Adaptive Preference Elicitation (Duy Nguyen et al., 2024)</a></li><li><a href=#2756--63190-fair-filtering-of-automatically-induced-rules-divya-jyoti-bajpai-et-al-2024>(27/56 | 63/190) FAIR: Filtering of Automatically Induced Rules (Divya Jyoti Bajpai et al., 2024)</a></li><li><a href=#2856--64190-the-impact-of-lora-on-the-emergence-of-clusters-in-transformers-hugo-koubbi-et-al-2024>(28/56 | 64/190) The Impact of LoRA on the Emergence of Clusters in Transformers (Hugo Koubbi et al., 2024)</a></li><li><a href=#2956--65190-genie-generative-interactive-environments-jake-bruce-et-al-2024>(29/56 | 65/190) Genie: Generative Interactive Environments (Jake Bruce et al., 2024)</a></li><li><a href=#3056--66190-explorations-of-self-repair-in-language-models-cody-rushing-et-al-2024>(30/56 | 66/190) Explorations of Self-Repair in Language Models (Cody Rushing et al., 2024)</a></li><li><a href=#3156--67190-on-the-duality-between-sharpness-aware-minimization-and-adversarial-training-yihao-zhang-et-al-2024>(31/56 | 67/190) On the Duality Between Sharpness-Aware Minimization and Adversarial Training (Yihao Zhang et al., 2024)</a></li><li><a href=#3256--68190-deep-coupling-network-for-multivariate-time-series-forecasting-kun-yi-et-al-2024>(32/56 | 68/190) Deep Coupling Network For Multivariate Time Series Forecasting (Kun Yi et al., 2024)</a></li><li><a href=#3356--69190-transflower-an-explainable-transformer-based-model-with-flow-to-flow-attention-for-commuting-flow-prediction-yan-luo-et-al-2024>(33/56 | 69/190) TransFlower: An Explainable Transformer-Based Model with Flow-to-Flow Attention for Commuting Flow Prediction (Yan Luo et al., 2024)</a></li><li><a href=#3456--70190-parameter-free-algorithms-for-performative-regret-minimization-under-decision-dependent-distributions-sungwoo-park-et-al-2024>(34/56 | 70/190) Parameter-Free Algorithms for Performative Regret Minimization under Decision-Dependent Distributions (Sungwoo Park et al., 2024)</a></li><li><a href=#3556--71190-co-supervised-learning-improving-weak-to-strong-generalization-with-hierarchical-mixture-of-experts-yuejiang-liu-et-al-2024>(35/56 | 71/190) Co-Supervised Learning: Improving Weak-to-Strong Generalization with Hierarchical Mixture of Experts (Yuejiang Liu et al., 2024)</a></li><li><a href=#3656--72190-neuralthink-algorithm-synthesis-that-extrapolates-in-general-tasks-bernardo-esteves-et-al-2024>(36/56 | 72/190) NeuralThink: Algorithm Synthesis that Extrapolates in General Tasks (Bernardo Esteves et al., 2024)</a></li><li><a href=#3756--73190-optimisic-information-directed-sampling-gergely-neu-et-al-2024>(37/56 | 73/190) Optimisic Information Directed Sampling (Gergely Neu et al., 2024)</a></li><li><a href=#3856--74190-distributionally-robust-off-dynamics-reinforcement-learning-provable-efficiency-with-linear-function-approximation-zhishuai-liu-et-al-2024>(38/56 | 74/190) Distributionally Robust Off-Dynamics Reinforcement Learning: Provable Efficiency with Linear Function Approximation (Zhishuai Liu et al., 2024)</a></li><li><a href=#3956--75190-offline-inverse-rl-new-solution-concepts-and-provably-efficient-algorithms-filippo-lazzati-et-al-2024>(39/56 | 75/190) Offline Inverse RL: New Solution Concepts and Provably Efficient Algorithms (Filippo Lazzati et al., 2024)</a></li><li><a href=#4056--76190-information-theoretic-safe-bayesian-optimization-alessandro-g-bottero-et-al-2024>(40/56 | 76/190) Information-Theoretic Safe Bayesian Optimization (Alessandro G. Bottero et al., 2024)</a></li><li><a href=#4156--77190-categorical-deep-learning-an-algebraic-theory-of-architectures-bruno-gavranović-et-al-2024>(41/56 | 77/190) Categorical Deep Learning: An Algebraic Theory of Architectures (Bruno Gavranović et al., 2024)</a></li><li><a href=#4256--78190-linear-dynamics-embedded-neural-network-for-long-sequence-modeling-tongyi-liang-et-al-2024>(42/56 | 78/190) Linear Dynamics-embedded Neural Network for Long-Sequence Modeling (Tongyi Liang et al., 2024)</a></li><li><a href=#4356--79190-calibration-of-deep-learning-classification-models-in-fnirs-zhihao-cao-et-al-2024>(43/56 | 79/190) Calibration of Deep Learning Classification Models in fNIRS (Zhihao Cao et al., 2024)</a></li><li><a href=#4456--80190-dynamic-memory-based-adaptive-optimization-balázs-szegedy-et-al-2024>(44/56 | 80/190) Dynamic Memory Based Adaptive Optimization (Balázs Szegedy et al., 2024)</a></li><li><a href=#4556--81190-a-bargaining-based-approach-for-feature-trading-in-vertical-federated-learning-yue-cui-et-al-2024>(45/56 | 81/190) A Bargaining-based Approach for Feature Trading in Vertical Federated Learning (Yue Cui et al., 2024)</a></li><li><a href=#4656--82190-which-model-to-transfer-a-survey-on-transferability-estimation-yuhe-ding-et-al-2024>(46/56 | 82/190) Which Model to Transfer? A Survey on Transferability Estimation (Yuhe Ding et al., 2024)</a></li><li><a href=#4756--83190-fixed-random-classifier-rearrangement-for-continual-learning-shengyang-huang-et-al-2024>(47/56 | 83/190) Fixed Random Classifier Rearrangement for Continual Learning (Shengyang Huang et al., 2024)</a></li><li><a href=#4856--84190-bidirectional-uncertainty-based-active-learning-for-open-set-annotation-chen-chen-zong-et-al-2024>(48/56 | 84/190) Bidirectional Uncertainty-Based Active Learning for Open Set Annotation (Chen-Chen Zong et al., 2024)</a></li><li><a href=#4956--85190-unified-view-of-grokking-double-descent-and-emergent-abilities-a-perspective-from-circuits-competition-yufei-huang-et-al-2024>(49/56 | 85/190) Unified View of Grokking, Double Descent and Emergent Abilities: A Perspective from Circuits Competition (Yufei Huang et al., 2024)</a></li><li><a href=#5056--86190-covariance-adaptive-least-squares-algorithm-for-stochastic-combinatorial-semi-bandits-julien-zhou-et-al-2024>(50/56 | 86/190) Covariance-Adaptive Least-Squares Algorithm for Stochastic Combinatorial Semi-Bandits (Julien Zhou et al., 2024)</a></li><li><a href=#5156--87190-the-surprising-effectiveness-of-skip-tuning-in-diffusion-sampling-jiajun-ma-et-al-2024>(51/56 | 87/190) The Surprising Effectiveness of Skip-Tuning in Diffusion Sampling (Jiajun Ma et al., 2024)</a></li><li><a href=#5256--88190-multi-armed-bandits-with-abstention-junwen-yang-et-al-2024>(52/56 | 88/190) Multi-Armed Bandits with Abstention (Junwen Yang et al., 2024)</a></li><li><a href=#5356--89190-machine-unlearning-by-suppressing-sample-contribution-xinwen-cheng-et-al-2024>(53/56 | 89/190) Machine Unlearning by Suppressing Sample Contribution (Xinwen Cheng et al., 2024)</a></li><li><a href=#5456--90190-fourier-basis-density-model-alfredo-de-la-fuente-et-al-2024>(54/56 | 90/190) Fourier Basis Density Model (Alfredo De la Fuente et al., 2024)</a></li><li><a href=#5556--91190-towards-principled-task-grouping-for-multi-task-learning-chenguang-wang-et-al-2024>(55/56 | 91/190) Towards Principled Task Grouping for Multi-Task Learning (Chenguang Wang et al., 2024)</a></li><li><a href=#5656--92190-convergence-analysis-of-blurring-mean-shift-ryoya-yamasaki-et-al-2024>(56/56 | 92/190) Convergence Analysis of Blurring Mean Shift (Ryoya Yamasaki et al., 2024)</a></li></ul></li><li><a href=#csro-9>cs.RO (9)</a><ul><li><a href=#19--93190-predilect-preferences-delineated-with-zero-shot-language-based-reasoning-in-reinforcement-learning-simon-holk-et-al-2024>(1/9 | 93/190) PREDILECT: Preferences Delineated with Zero-Shot Language-based Reasoning in Reinforcement Learning (Simon Holk et al., 2024)</a></li><li><a href=#29--94190-safe-task-planning-for-language-instructed-multi-robot-systems-using-conformal-prediction-jun-wang-et-al-2024>(2/9 | 94/190) Safe Task Planning for Language-Instructed Multi-Robot Systems using Conformal Prediction (Jun Wang et al., 2024)</a></li><li><a href=#39--95190-roboexp-action-conditioned-scene-graph-via-interactive-exploration-for-robotic-manipulation-hanxiao-jiang-et-al-2024>(3/9 | 95/190) RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for Robotic Manipulation (Hanxiao Jiang et al., 2024)</a></li><li><a href=#49--96190-dynamics-guided-diffusion-model-for-robot-manipulator-design-xiaomeng-xu-et-al-2024>(4/9 | 96/190) Dynamics-Guided Diffusion Model for Robot Manipulator Design (Xiaomeng Xu et al., 2024)</a></li><li><a href=#59--97190-grasp-see-and-place-efficient-unknown-object-rearrangement-with-policy-structure-prior-kechun-xu-et-al-2024>(5/9 | 97/190) Grasp, See and Place: Efficient Unknown Object Rearrangement with Policy Structure Prior (Kechun Xu et al., 2024)</a></li><li><a href=#69--98190-follow-the-footprints-self-supervised-traversability-estimation-for-off-road-vehicle-navigation-based-on-geometric-and-visual-cues-yurim-jeon-et-al-2024>(6/9 | 98/190) Follow the Footprints: Self-supervised Traversability Estimation for Off-road Vehicle Navigation based on Geometric and Visual Cues (Yurim Jeon et al., 2024)</a></li><li><a href=#79--99190-streaming-gaussian-dirichlet-random-fields-for-spatial-predictions-of-high-dimensional-categorical-observations-j-e-san-soucie-et-al-2024>(7/9 | 99/190) Streaming Gaussian Dirichlet Random Fields for Spatial Predictions of High Dimensional Categorical Observations (J. E. San Soucie et al., 2024)</a></li><li><a href=#89--100190-clipper-a-fast-maximal-clique-algorithm-for-robust-global-registration-kaveh-fathian-et-al-2024>(8/9 | 100/190) CLIPPER+: A Fast Maximal Clique Algorithm for Robust Global Registration (Kaveh Fathian et al., 2024)</a></li><li><a href=#99--101190-neural-implicit-swept-volume-models-for-fast-collision-detection-dominik-joho-et-al-2024>(9/9 | 101/190) Neural Implicit Swept Volume Models for Fast Collision Detection (Dominik Joho et al., 2024)</a></li></ul></li><li><a href=#cscv-25>cs.CV (25)</a><ul><li><a href=#125--102190-on-normalization-equivariance-properties-of-supervised-and-unsupervised-denoising-methods-a-survey-sébastien-herbreteau-et-al-2024>(1/25 | 102/190) On normalization-equivariance properties of supervised and unsupervised denoising methods: a survey (Sébastien Herbreteau et al., 2024)</a></li><li><a href=#225--103190-label-efficient-multi-organ-segmentation-method-with-diffusion-model-yongzhi-huang-et-al-2024>(2/25 | 103/190) Label-efficient Multi-organ Segmentation Method with Diffusion Model (Yongzhi Huang et al., 2024)</a></li><li><a href=#325--104190-fine-tuning-clip-text-encoders-with-two-step-paraphrasing-hyunjae-kim-et-al-2024>(3/25 | 104/190) Fine-tuning CLIP Text Encoders with Two-step Paraphrasing (Hyunjae Kim et al., 2024)</a></li><li><a href=#425--105190-representing-online-handwriting-for-recognition-in-large-vision-language-models-anastasiia-fadeeva-et-al-2024>(4/25 | 105/190) Representing Online Handwriting for Recognition in Large Vision-Language Models (Anastasiia Fadeeva et al., 2024)</a></li><li><a href=#525--106190-seeing-is-believing-mitigating-hallucination-in-large-vision-language-models-via-clip-guided-decoding-ailin-deng-et-al-2024>(5/25 | 106/190) Seeing is Believing: Mitigating Hallucination in Large Vision-Language Models via CLIP-Guided Decoding (Ailin Deng et al., 2024)</a></li><li><a href=#625--107190-modified-cyclegan-for-the-synthesization-of-samples-for-wheat-head-segmentation-jaden-myers-et-al-2024>(6/25 | 107/190) Modified CycleGAN for the synthesization of samples for wheat head segmentation (Jaden Myers et al., 2024)</a></li><li><a href=#725--108190-benchmarking-the-robustness-of-panoptic-segmentation-for-automated-driving-yiting-wang-et-al-2024>(7/25 | 108/190) Benchmarking the Robustness of Panoptic Segmentation for Automated Driving (Yiting Wang et al., 2024)</a></li><li><a href=#825--109190-large-multimodal-agents-a-survey-junlin-xie-et-al-2024>(8/25 | 109/190) Large Multimodal Agents: A Survey (Junlin Xie et al., 2024)</a></li><li><a href=#925--110190-gen4gen-generative-data-pipeline-for-generative-multi-concept-composition-chun-hsiao-yeh-et-al-2024>(9/25 | 110/190) Gen4Gen: Generative Data Pipeline for Generative Multi-Concept Composition (Chun-Hsiao Yeh et al., 2024)</a></li><li><a href=#1025--111190-where-visual-speech-meets-language-vsp-llm-framework-for-efficient-and-context-aware-visual-speech-processing-jeong-hun-yeo-et-al-2024>(10/25 | 111/190) Where Visual Speech Meets Language: VSP-LLM Framework for Efficient and Context-Aware Visual Speech Processing (Jeong Hun Yeo et al., 2024)</a></li><li><a href=#1125--112190-retinotopic-mapping-enhances-the-robustness-of-convolutional-neural-networks-jean-nicolas-jérémie-et-al-2024>(11/25 | 112/190) Retinotopic Mapping Enhances the Robustness of Convolutional Neural Networks (Jean-Nicolas Jérémie et al., 2024)</a></li><li><a href=#1225--113190-hierarchical-invariance-for-robust-and-interpretable-vision-tasks-at-larger-scales-shuren-qi-et-al-2024>(12/25 | 113/190) Hierarchical Invariance for Robust and Interpretable Vision Tasks at Larger Scales (Shuren Qi et al., 2024)</a></li><li><a href=#1325--114190-semi-supervised-counting-via-pixel-by-pixel-density-distribution-modelling-hui-lin-et-al-2024>(13/25 | 114/190) Semi-supervised Counting via Pixel-by-pixel Density Distribution Modelling (Hui Lin et al., 2024)</a></li><li><a href=#1425--115190-unsupervised-domain-adaptation-for-brain-vessel-segmentation-through-transwarp-contrastive-learning-fengming-lin-et-al-2024>(14/25 | 115/190) Unsupervised Domain Adaptation for Brain Vessel Segmentation through Transwarp Contrastive Learning (Fengming Lin et al., 2024)</a></li><li><a href=#1525--116190-font-impression-estimation-in-the-wild-kazuki-kitajima-et-al-2024>(15/25 | 116/190) Font Impression Estimation in the Wild (Kazuki Kitajima et al., 2024)</a></li><li><a href=#1625--117190-outlier-detection-by-ensembling-uncertainty-with-negative-objectness-anja-delić-et-al-2024>(16/25 | 117/190) Outlier detection by ensembling uncertainty with negative objectness (Anja Delić et al., 2024)</a></li><li><a href=#1725--118190-source-guided-similarity-preservation-for-online-person-re-identification-hamza-rami-et-al-2024>(17/25 | 118/190) Source-Guided Similarity Preservation for Online Person Re-Identification (Hamza Rami et al., 2024)</a></li><li><a href=#1825--119190-attention-guided-masked-autoencoders-for-learning-image-representations-leon-sick-et-al-2024>(18/25 | 119/190) Attention-Guided Masked Autoencoders For Learning Image Representations (Leon Sick et al., 2024)</a></li><li><a href=#1925--120190-fiducial-focus-augmentation-for-facial-landmark-detection-purbayan-kar-et-al-2024>(19/25 | 120/190) Fiducial Focus Augmentation for Facial Landmark Detection (Purbayan Kar et al., 2024)</a></li><li><a href=#2025--121190-protip-probabilistic-robustness-verification-on-text-to-image-diffusion-models-against-stochastic-perturbation-yi-zhang-et-al-2024>(20/25 | 121/190) ProTIP: Probabilistic Robustness Verification on Text-to-Image Diffusion Models against Stochastic Perturbation (Yi Zhang et al., 2024)</a></li><li><a href=#2125--122190-puad-frustratingly-simple-method-for-robust-anomaly-detection-shota-sugawara-et-al-2024>(21/25 | 122/190) PUAD: Frustratingly Simple Method for Robust Anomaly Detection (Shota Sugawara et al., 2024)</a></li><li><a href=#2225--123190-descripción-automática-de-secciones-delgadas-de-rocas-una-aplicación-web-stalyn-paucar-et-al-2024>(22/25 | 123/190) Descripción automática de secciones delgadas de rocas: una aplicación Web (Stalyn Paucar et al., 2024)</a></li><li><a href=#2325--124190-optimal-transport-on-the-lie-group-of-roto-translations-daan-bon-et-al-2024>(23/25 | 124/190) Optimal Transport on the Lie Group of Roto-translations (Daan Bon et al., 2024)</a></li><li><a href=#2425--125190-emiff-enhanced-multi-scale-image-feature-fusion-for-vehicle-infrastructure-cooperative-3d-object-detection-zhe-wang-et-al-2024>(24/25 | 125/190) EMIFF: Enhanced Multi-scale Image Feature Fusion for Vehicle-Infrastructure Cooperative 3D Object Detection (Zhe Wang et al., 2024)</a></li><li><a href=#2525--126190-gs-ema-integrating-gradient-surgery-exponential-moving-average-with-boundary-aware-contrastive-learning-for-enhanced-domain-generalization-in-aneurysm-segmentation-fengming-lin-et-al-2024>(25/25 | 126/190) GS-EMA: Integrating Gradient Surgery Exponential Moving Average with Boundary-Aware Contrastive Learning for Enhanced Domain Generalization in Aneurysm Segmentation (Fengming Lin et al., 2024)</a></li></ul></li><li><a href=#csir-5>cs.IR (5)</a><ul><li><a href=#15--127190-item-side-fairness-of-large-language-model-based-recommendation-system-meng-jiang-et-al-2024>(1/5 | 127/190) Item-side Fairness of Large Language Model-based Recommendation System (Meng Jiang et al., 2024)</a></li><li><a href=#25--128190-multi-agent-collaboration-framework-for-recommender-systems-zhefan-wang-et-al-2024>(2/5 | 128/190) Multi-Agent Collaboration Framework for Recommender Systems (Zhefan Wang et al., 2024)</a></li><li><a href=#35--129190-text2pic-swift-enhancing-long-text-to-image-retrieval-for-large-scale-libraries-zijun-long-et-al-2024>(3/5 | 129/190) Text2Pic Swift: Enhancing Long-Text to Image Retrieval for Large-Scale Libraries (Zijun Long et al., 2024)</a></li><li><a href=#45--130190-faithful-temporal-question-answering-over-heterogeneous-sources-zhen-jia-et-al-2024>(4/5 | 130/190) Faithful Temporal Question Answering over Heterogeneous Sources (Zhen Jia et al., 2024)</a></li><li><a href=#55--131190-easyrl4rec-a-user-friendly-code-library-for-reinforcement-learning-based-recommender-systems-yuanqing-yu-et-al-2024>(5/5 | 131/190) EasyRL4Rec: A User-Friendly Code Library for Reinforcement Learning Based Recommender Systems (Yuanqing Yu et al., 2024)</a></li></ul></li><li><a href=#q-bioqm-1>q-bio.QM (1)</a><ul><li><a href=#11--132190-all-thresholds-barred-direct-estimation-of-call-density-in-bioacoustic-data-amanda-k-navine-et-al-2024>(1/1 | 132/190) All Thresholds Barred: Direct Estimation of Call Density in Bioacoustic Data (Amanda K. Navine et al., 2024)</a></li></ul></li><li><a href=#statml-5>stat.ML (5)</a><ul><li><a href=#15--133190-efficient-semi-supervised-inference-for-logistic-regression-under-case-control-studies-zhuojun-quan-et-al-2024>(1/5 | 133/190) Efficient semi-supervised inference for logistic regression under case-control studies (Zhuojun Quan et al., 2024)</a></li><li><a href=#25--134190-iteration-and-stochastic-first-order-oracle-complexities-of-stochastic-gradient-descent-using-constant-and-decaying-learning-rates-kento-imaizumi-et-al-2024>(2/5 | 134/190) Iteration and Stochastic First-order Oracle Complexities of Stochastic Gradient Descent using Constant and Decaying Learning Rates (Kento Imaizumi et al., 2024)</a></li><li><a href=#35--135190-statistical-agnostic-regression-a-machine-learning-method-to-validate-regression-models-juan-m-gorriz-et-al-2024>(3/5 | 135/190) Statistical Agnostic Regression: a machine learning method to validate regression models (Juan M Gorriz et al., 2024)</a></li><li><a href=#45--136190-generative-modelling-with-tensor-train-approximations-of-hamilton--jacobi--bellman-equations-david-sommer-et-al-2024>(4/5 | 136/190) Generative Modelling with Tensor Train approximations of Hamilton&ndash;Jacobi&ndash;Bellman equations (David Sommer et al., 2024)</a></li><li><a href=#55--137190-nonlinear-bayesian-optimal-experimental-design-using-logarithmic-sobolev-inequalities-fengyi-li-et-al-2024>(5/5 | 137/190) Nonlinear Bayesian optimal experimental design using logarithmic Sobolev inequalities (Fengyi Li et al., 2024)</a></li></ul></li><li><a href=#cssd-1>cs.SD (1)</a><ul><li><a href=#11--138190-a-survey-of-music-generation-in-the-context-of-interaction-ismael-agchar-et-al-2024>(1/1 | 138/190) A Survey of Music Generation in the Context of Interaction (Ismael Agchar et al., 2024)</a></li></ul></li><li><a href=#eessiv-2>eess.IV (2)</a><ul><li><a href=#12--139190-artificial-bee-colony-optimization-of-deep-convolutional-neural-networks-in-the-context-of-biomedical-imaging-adri-gomez-martin-et-al-2024>(1/2 | 139/190) Artificial Bee Colony optimization of Deep Convolutional Neural Networks in the context of Biomedical Imaging (Adri Gomez Martin et al., 2024)</a></li><li><a href=#22--140190-low-rank-representations-meets-deep-unfolding-a-generalized-and-interpretable-network-for-hyperspectral-anomaly-detection-chenyu-li-et-al-2024>(2/2 | 140/190) Low-Rank Representations Meets Deep Unfolding: A Generalized and Interpretable Network for Hyperspectral Anomaly Detection (Chenyu Li et al., 2024)</a></li></ul></li><li><a href=#cscr-7>cs.CR (7)</a><ul><li><a href=#17--141190-a-first-look-at-gpt-apps-landscape-and-vulnerability-zejun-zhang-et-al-2024>(1/7 | 141/190) A First Look at GPT Apps: Landscape and Vulnerability (Zejun Zhang et al., 2024)</a></li><li><a href=#27--142190-bspa-exploring-black-box-stealthy-prompt-attacks-against-image-generators-yu-tian-et-al-2024>(2/7 | 142/190) BSPA: Exploring Black-box Stealthy Prompt Attacks against Image Generators (Yu Tian et al., 2024)</a></li><li><a href=#37--143190-trec-apt-tactic--technique-recognition-via-few-shot-provenance-subgraph-learning-mingqi-lv-et-al-2024>(3/7 | 143/190) TREC: APT Tactic / Technique Recognition via Few-Shot Provenance Subgraph Learning (Mingqi Lv et al., 2024)</a></li><li><a href=#47--144190-a-blockchain-enabled-framework-of-uav-coordination-for-post-disaster-networks-sana-hafeez-et-al-2024>(4/7 | 144/190) A Blockchain-Enabled Framework of UAV Coordination for Post-Disaster Networks (Sana Hafeez et al., 2024)</a></li><li><a href=#57--145190-on-the-usability-of-next-generation-authentication-a-study-on-eye-movement-and-brainwave-based-mechanisms-matin-fallahi-et-al-2024>(5/7 | 145/190) On the Usability of Next-Generation Authentication: A Study on Eye Movement and Brainwave-based Mechanisms (Matin Fallahi et al., 2024)</a></li><li><a href=#67--146190-sok-what-dont-we-know-understanding-security-vulnerabilities-in-snarks-stefanos-chaliasos-et-al-2024>(6/7 | 146/190) SoK: What don&rsquo;t we know? Understanding Security Vulnerabilities in SNARKs (Stefanos Chaliasos et al., 2024)</a></li><li><a href=#77--147190-chu-ko-nu-a-reliable-efficient-and-anonymously-authentication-enabled-realization-for-multi-round-secure-aggregation-in-federated-learning-kaiping-cui-et-al-2024>(7/7 | 147/190) Chu-ko-nu: A Reliable, Efficient, and Anonymously Authentication-Enabled Realization for Multi-Round Secure Aggregation in Federated Learning (Kaiping Cui et al., 2024)</a></li></ul></li><li><a href=#csse-3>cs.SE (3)</a><ul><li><a href=#13--148190-studying-llm-performance-on-closed--and-open-source-data-toufique-ahmed-et-al-2024>(1/3 | 148/190) Studying LLM Performance on Closed- and Open-source Data (Toufique Ahmed et al., 2024)</a></li><li><a href=#23--149190-llm-compdroid-repairing-configuration-compatibility-bugs-in-android-apps-with-pre-trained-large-language-models-zhijie-liu-et-al-2024>(2/3 | 149/190) LLM-CompDroid: Repairing Configuration Compatibility Bugs in Android Apps with Pre-trained Large Language Models (Zhijie Liu et al., 2024)</a></li><li><a href=#33--150190-towards-model-driven-dashboard-generation-for-systems-of-systems-maria-teresa-rossi-et-al-2024>(3/3 | 150/190) Towards Model-Driven Dashboard Generation for Systems-of-Systems (Maria Teresa Rossi et al., 2024)</a></li></ul></li><li><a href=#csgt-3>cs.GT (3)</a><ul><li><a href=#13--151190-human-vs-generative-ai-in-content-creation-competition-symbiosis-or-conflict-fan-yao-et-al-2024>(1/3 | 151/190) Human vs. Generative AI in Content Creation Competition: Symbiosis or Conflict? (Fan Yao et al., 2024)</a></li><li><a href=#23--152190-platforms-for-efficient-and-incentive-aware-collaboration-nika-haghtalab-et-al-2024>(2/3 | 152/190) Platforms for Efficient and Incentive-Aware Collaboration (Nika Haghtalab et al., 2024)</a></li><li><a href=#33--153190-analyzing-games-in-maker-protocol-part-one-a-multi-agent-influence-diagram-approach-towards-coordination-abhimanyu-nag-et-al-2024>(3/3 | 153/190) Analyzing Games in Maker Protocol Part One: A Multi-Agent Influence Diagram Approach Towards Coordination (Abhimanyu Nag et al., 2024)</a></li></ul></li><li><a href=#cshc-8>cs.HC (8)</a><ul><li><a href=#18--154190-farsight-fostering-responsible-ai-awareness-during-ai-application-prototyping-zijie-j-wang-et-al-2024>(1/8 | 154/190) Farsight: Fostering Responsible AI Awareness During AI Application Prototyping (Zijie J. Wang et al., 2024)</a></li><li><a href=#28--155190-economic-and-financial-learning-with-artificial-intelligence-a-mixed-methods-study-on-chatgpt-holger-arndt-2024>(2/8 | 155/190) Economic and Financial Learning with Artificial Intelligence: A Mixed-Methods Study on ChatGPT (Holger Arndt, 2024)</a></li><li><a href=#38--156190-clochat-understanding-how-people-customize-interact-and-experience-personas-in-large-language-models-juhye-ha-et-al-2024>(3/8 | 156/190) CloChat: Understanding How People Customize, Interact, and Experience Personas in Large Language Models (Juhye Ha et al., 2024)</a></li><li><a href=#48--157190-i-see-an-ic-a-mixed-methods-approach-to-study-human-problem-solving-processes-in-hardware-reverse-engineering-rené-walendy-et-al-2024>(4/8 | 157/190) I see an IC: A Mixed-Methods Approach to Study Human Problem-Solving Processes in Hardware Reverse Engineering (René Walendy et al., 2024)</a></li><li><a href=#58--158190-metastates-an-approach-for-representing-human-workers-psychophysiological-states-in-the-industrial-metaverse-aitor-toichoa-eyam-et-al-2024>(5/8 | 158/190) MetaStates: An Approach for Representing Human Workers Psychophysiological States in the Industrial Metaverse (Aitor Toichoa Eyam et al., 2024)</a></li><li><a href=#68--159190-enhancing-icu-patient-recovery-using-llms-to-assist-nurses-in-diary-writing-samuel-kernan-freire-et-al-2024>(6/8 | 159/190) Enhancing ICU Patient Recovery: Using LLMs to Assist Nurses in Diary Writing (Samuel Kernan Freire et al., 2024)</a></li><li><a href=#78--160190-hands-free-vr-jorge-askur-vazquez-fernandez-et-al-2024>(7/8 | 160/190) Hands-Free VR (Jorge Askur Vazquez Fernandez et al., 2024)</a></li><li><a href=#88--161190-the-affecttoolbox-affect-analysis-for-everyone-silvan-mertes-et-al-2024>(8/8 | 161/190) The AffectToolbox: Affect Analysis for Everyone (Silvan Mertes et al., 2024)</a></li></ul></li><li><a href=#astro-phim-1>astro-ph.IM (1)</a><ul><li><a href=#11--162190-classification-of-compact-radio-sources-in-the-galactic-plane-with-supervised-machine-learning-s-riggi-et-al-2024>(1/1 | 162/190) Classification of compact radio sources in the Galactic plane with supervised machine learning (S. Riggi et al., 2024)</a></li></ul></li><li><a href=#eessas-2>eess.AS (2)</a><ul><li><a href=#12--163190-childaugment-data-augmentation-methods-for-zero-resource-childrens-speaker-verification-vishwanath-pratap-singh-et-al-2024>(1/2 | 163/190) ChildAugment: Data Augmentation Methods for Zero-Resource Children&rsquo;s Speaker Verification (Vishwanath Pratap Singh et al., 2024)</a></li><li><a href=#22--164190-high-resolution-guitar-transcription-via-domain-adaptation-xavier-riley-et-al-2024>(2/2 | 164/190) High Resolution Guitar Transcription via Domain Adaptation (Xavier Riley et al., 2024)</a></li></ul></li><li><a href=#csit-2>cs.IT (2)</a><ul><li><a href=#12--165190-pre-chirp-domain-index-modulation-for-affine-frequency-division-multiplexing-guangyao-liu-et-al-2024>(1/2 | 165/190) Pre-Chirp-Domain Index Modulation for Affine Frequency Division Multiplexing (Guangyao Liu et al., 2024)</a></li><li><a href=#22--166190-iterative-inversion-of-elaa-mimo-channels-using-symmetric-rank-1-regularization-jinfei-wang-et-al-2024>(2/2 | 166/190) Iterative Inversion of (ELAA-)MIMO Channels Using Symmetric Rank-$1$ Regularization (Jinfei Wang et al., 2024)</a></li></ul></li><li><a href=#csar-1>cs.AR (1)</a><ul><li><a href=#11--167190-toward-high-performance-programmable-extreme-edge-intelligence-for-neuromorphic-vision-sensors-utilizing-magnetic-domain-wall-motion-based-mtj-md-abdullah-al-kaiser-et-al-2024>(1/1 | 167/190) Toward High Performance, Programmable Extreme-Edge Intelligence for Neuromorphic Vision Sensors utilizing Magnetic Domain Wall Motion-based MTJ (Md Abdullah-Al Kaiser et al., 2024)</a></li></ul></li><li><a href=#csai-2>cs.AI (2)</a><ul><li><a href=#12--168190-a-relation-interactive-approach-for-message-passing-in-hyper-relational-knowledge-graphs-yonglin-jing-2024>(1/2 | 168/190) A Relation-Interactive Approach for Message Passing in Hyper-relational Knowledge Graphs (Yonglin Jing, 2024)</a></li><li><a href=#22--169190-agentohana-design-unified-data-and-training-pipeline-for-effective-agent-learning-jianguo-zhang-et-al-2024>(2/2 | 169/190) AgentOhana: Design Unified Data and Training Pipeline for Effective Agent Learning (Jianguo Zhang et al., 2024)</a></li></ul></li><li><a href=#csce-1>cs.CE (1)</a><ul><li><a href=#11--170190-design-and-optimization-of-functionally-graded-triangular-lattices-for-multiple-loading-conditions-junpeng-wang-et-al-2024>(1/1 | 170/190) Design and Optimization of Functionally-graded Triangular Lattices for Multiple Loading Conditions (Junpeng Wang et al., 2024)</a></li></ul></li><li><a href=#mathna-2>math.NA (2)</a><ul><li><a href=#12--171190-a-cweno-large-time-step-scheme-for-hamilton--jacobi-equations-e-carlini-et-al-2024>(1/2 | 171/190) A CWENO large time-step scheme for Hamilton&ndash;Jacobi equations (E. Carlini et al., 2024)</a></li><li><a href=#22--172190-a-unified-constraint-formulation-of-immersed-body-techniques-for-coupled-fluid-solid-motion-continuous-equations-and-numerical-algorithms-amneet-pal-singh-bhalla-et-al-2024>(2/2 | 172/190) A unified constraint formulation of immersed body techniques for coupled fluid-solid motion: continuous equations and numerical algorithms (Amneet Pal Singh Bhalla et al., 2024)</a></li></ul></li><li><a href=#csma-2>cs.MA (2)</a><ul><li><a href=#12--173190-shapley-value-based-multi-agent-reinforcement-learning-theory-method-and-its-application-to-energy-network-jianhong-wang-2024>(1/2 | 173/190) Shapley Value Based Multi-Agent Reinforcement Learning: Theory, Method and Its Application to Energy Network (Jianhong Wang, 2024)</a></li><li><a href=#22--174190-open-ad-hoc-teamwork-with-cooperative-game-theory-jianhong-wang-et-al-2024>(2/2 | 174/190) Open Ad Hoc Teamwork with Cooperative Game Theory (Jianhong Wang et al., 2024)</a></li></ul></li><li><a href=#eesssy-1>eess.SY (1)</a><ul><li><a href=#11--175190-safety-optimized-reinforcement-learning-via-multi-objective-policy-optimization-homayoun-honari-et-al-2024>(1/1 | 175/190) Safety Optimized Reinforcement Learning via Multi-Objective Policy Optimization (Homayoun Honari et al., 2024)</a></li></ul></li><li><a href=#mathst-2>math.ST (2)</a><ul><li><a href=#12--176190-universal-lower-bounds-and-optimal-rates-achieving-minimax-clustering-error-in-sub-exponential-mixture-models-maximilien-dreveton-et-al-2024>(1/2 | 176/190) Universal Lower Bounds and Optimal Rates: Achieving Minimax Clustering Error in Sub-Exponential Mixture Models (Maximilien Dreveton et al., 2024)</a></li><li><a href=#22--177190-the-umeyama-algorithm-for-matching-correlated-gaussian-geometric-models-in-the-low-dimensional-regime-shuyang-gong-et-al-2024>(2/2 | 177/190) The Umeyama algorithm for matching correlated Gaussian geometric models in the low-dimensional regime (Shuyang Gong et al., 2024)</a></li></ul></li><li><a href=#mathoc-1>math.OC (1)</a><ul><li><a href=#11--178190-closed-loop-design-for-scalable-performance-of-vehicular-formations-jonas-hansson-et-al-2024>(1/1 | 178/190) Closed-loop design for scalable performance of vehicular formations (Jonas Hansson et al., 2024)</a></li></ul></li><li><a href=#cssi-1>cs.SI (1)</a><ul><li><a href=#11--179190-decoding-the-pulse-of-community-during-disasters-resilience-analysis-based-on-fluctuations-in-latent-lifestyle-signatures-within-human-visitation-networks-junwei-ma-et-al-2024>(1/1 | 179/190) Decoding the Pulse of Community during Disasters: Resilience Analysis Based on Fluctuations in Latent Lifestyle Signatures within Human Visitation Networks (Junwei Ma et al., 2024)</a></li></ul></li><li><a href=#csdc-2>cs.DC (2)</a><ul><li><a href=#12--180190-convergence-analysis-of-split-federated-learning-on-heterogeneous-data-pengchao-han-et-al-2024>(1/2 | 180/190) Convergence Analysis of Split Federated Learning on Heterogeneous Data (Pengchao Han et al., 2024)</a></li><li><a href=#22--181190-pico-accelerating-all-k-core-paradigms-on-gpu-chen-zhao-et-al-2024>(2/2 | 181/190) PICO: Accelerating All k-Core Paradigms on GPU (Chen Zhao et al., 2024)</a></li></ul></li><li><a href=#csds-5>cs.DS (5)</a><ul><li><a href=#15--182190-algorithmically-fair-maximization-of-multiple-submodular-objective-functions-georgios-amanatidis-et-al-2024>(1/5 | 182/190) Algorithmically Fair Maximization of Multiple Submodular Objective Functions (Georgios Amanatidis et al., 2024)</a></li><li><a href=#25--183190-on-the-complexity-of-community-aware-network-sparsification-emanuel-herrendorf-et-al-2024>(2/5 | 183/190) On the Complexity of Community-aware Network Sparsification (Emanuel Herrendorf et al., 2024)</a></li><li><a href=#35--184190-graph-partitioning-with-limited-moves-majid-behbahani-et-al-2024>(3/5 | 184/190) Graph Partitioning With Limited Moves (Majid Behbahani et al., 2024)</a></li><li><a href=#45--185190-tight-approximation-and-kernelization-bounds-for-vertex-disjoint-shortest-paths-matthias-bentert-et-al-2024>(4/5 | 185/190) Tight Approximation and Kernelization Bounds for Vertex-Disjoint Shortest Paths (Matthias Bentert et al., 2024)</a></li><li><a href=#55--186190-tight-inapproximability-of-target-set-reconfiguration-naoto-ohsaka-2024>(5/5 | 186/190) Tight Inapproximability of Target Set Reconfiguration (Naoto Ohsaka, 2024)</a></li></ul></li><li><a href=#quant-ph-1>quant-ph (1)</a><ul><li><a href=#11--187190-on-the-composable-security-of-weak-coin-flipping-jiawei-wu-et-al-2024>(1/1 | 187/190) On the composable security of weak coin flipping (Jiawei Wu et al., 2024)</a></li></ul></li><li><a href=#csdm-1>cs.DM (1)</a><ul><li><a href=#11--188190-some-results-involving-the-a_α-eigenvalues-for-graphs-and-line-graphs-joao-domingos-gomes-da-silva-junior-et-al-2024>(1/1 | 188/190) Some results involving the $A_α$-eigenvalues for graphs and line graphs (Joao Domingos Gomes da Silva Junior et al., 2024)</a></li></ul></li><li><a href=#cslo-1>cs.LO (1)</a><ul><li><a href=#11--189190-the-flower-calculus-pablo-donato-2024>(1/1 | 189/190) The Flower Calculus (Pablo Donato, 2024)</a></li></ul></li><li><a href=#mathco-1>math.CO (1)</a><ul><li><a href=#11--190190-rectilinear-crossing-number-of-graphs-excluding-single-crossing-graphs-as-minors-vida-dujmović-et-al-2024>(1/1 | 190/190) Rectilinear Crossing Number of Graphs Excluding Single-Crossing Graphs as Minors (Vida Dujmović et al., 2024)</a></li></ul></li></ul></nav></div></div></section></div><footer id=footer class="container-fluid text-center align-content-center footer pb-2"><div class="container pt-5"><div class="row text-left"><div class="col-md-4 col-sm-12"><h5>Navigation</h5><ul><li class=nav-item><a class=smooth-scroll href=https://akitenkrad.github.io/akitenkrad-blog/#about>About</a></li><li class=nav-item><a class=smooth-scroll href=https://akitenkrad.github.io/akitenkrad-blog/#recent-posts>Recent Posts</a></li></ul></div><div class="col-md-4 col-sm-12"><h5>Contact me:</h5><ul><li><a href=mailto:contact.to.akitenkrad@gmail.com target=_blank rel=noopener><span><i class="fas fa-envelope"></i></span> <span>contact.to.akitenkrad@gmail.com</span></a></li></ul></div></div></div><hr><div class=container><div class="row text-left"><div class=col-md-4><a id=theme href=https://github.com/hossainemruz/toha target=_blank rel=noopener><img src=/akitenkrad-blog/images/theme-logo_hu8376fd15465fef26ffe66b6bcf0ca686_13669_32x0_resize_box_3.png alt="Toha Theme Logo">
Toha</a></div><div class="col-md-4 text-center">© 2020 Akitenkrad.</div><div class="col-md-4 text-right"><a id=hugo href=https://gohugo.io/ target=_blank rel=noopener>Powered by
<img src=/akitenkrad-blog/images/hugo-logo.svg alt="Hugo Logo" height=18></a></div></div></div></footer><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/jquery-3.4.1.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/popper.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/bootstrap.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/navbar.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/plyr.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/main.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js></script><script src=https://akitenkrad.github.io/akitenkrad-blog/js/single.js></script><script>hljs.initHighlightingOnLoad()</script><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/katex/katex.min.css><script type=text/javascript defer src=https://akitenkrad.github.io/akitenkrad-blog/katex/katex.min.js></script><script type=text/javascript defer src=https://akitenkrad.github.io/akitenkrad-blog/katex/auto-render.min.js></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\[",right:"\\]",display:!0},{left:"\\(",right:"\\)",display:!1}],throwOnError:!0})})</script></body></html>