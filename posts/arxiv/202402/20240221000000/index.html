<!doctype html><html><head><title>arXiv @ 2024.02.21</title>
<meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/bootstrap.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/layouts/main.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/navigators/navbar.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/plyr.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/flag-icon.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/custom/style.css><link href=https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css rel=stylesheet integrity=sha384-9ndCyUaIbzAi2FUVXJi0CjmCapSmO7SnpJef0486qhLnuZ2cdeRhO02iuK6FUUVM crossorigin=anonymous><script async src=https://cdn.jsdelivr.net/npm/es-module-shims@1/dist/es-module-shims.min.js crossorigin=anonymous></script><script type=importmap>
{
  "imports": {
    "@popperjs/core": "https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.8/dist/esm/popper.min.js",
    "bootstrap": "https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.esm.min.js"
  }
}
</script><script src=https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js integrity=sha384-geWF76RCwLtnZ8qwWowPQNguL3RmwHVBC9FhGdlKrxdiJJigb/j/68SIy3Te4Bkz crossorigin=anonymous></script><script type=module src=https://akitenkrad.github.io/akitenkrad-blog/js/custom/custom.js></script><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Muli:wght@300;400;500;600"><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/fontawesome/css/all.min.css><link rel=icon type=image/png href=https://akitenkrad.github.io/akitenkrad-blog/images/favicons/favicon-96x96_huf1ee13f0caf27d1547f91fb46207d708_13005_42x0_resize_box_3.png><meta property="og:title" content="arXiv @ 2024.02.21"><meta property="og:description" content="Primary Categories astro-ph.EP (1) astro-ph.GA (1) astro-ph.SR (1) cs.AI (9) cs.AR (2) cs.CG (3) cs.CL (109) cs.CR (9) cs.CV (45) cs.CY (1) cs.DB (2) cs.DC (5) cs.DL (1) cs.DS (4) cs.GT (3) cs.HC (4) cs.IR (7) cs.IT (3) cs.LG (71) cs.LO (1) cs.MA (2) cs.NE (3) cs.NI (2) cs.PL (1) cs.RO (11) cs.SD (5) cs.SE (3) cs.SI (5) eess.AS (3) eess.IV (1) eess.SP (2) eess.SY (5) math.CO (2) math.NA (6) math."><meta property="og:type" content="article"><meta property="og:url" content="https://akitenkrad.github.io/akitenkrad-blog/posts/arxiv/202402/20240221000000/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-02-21T00:00:00+00:00"><meta property="article:modified_time" content="2024-02-21T00:00:00+00:00"><meta name=description content="arXiv @ 2024.02.21"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/atom-one-dark.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/layouts/single.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/navigators/sidebar.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/style.css><script type=text/javascript src=//ajax.googleapis.com/ajax/libs/jquery/3.1.1/jquery.min.js></script><script language=JavaScript>$(document).ready(function(){$("a[href^='http']:not([href*='"+location.hostname+"'])").attr("target","_blank")})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-1MYYZQG0WE"></script><script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-1MYYZQG0WE",{anonymize_ip:!1})}</script></head><body data-spy=scroll data-target=#TableOfContents data-offset=80><div class="container-fluid bg-dimmed wrapper"><nav class="navbar navbar-expand-xl top-navbar final-navbar shadow" style=position:fixed><div class=container><button class="navbar-toggler navbar-light navbar-for-sidebar" id=sidebar-toggler type=button onclick=toggleSidebar()>
<span class=navbar-toggler-icon></span>
</button>
<button class="navbar-toggler navbar-light" id=toc-toggler type=button onclick=toggleTOC()>
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse lang-selector" id=top-nav-items><ul class="navbar-nav mr-auto"><li class=nav-item><a class=navbar-brand href=/akitenkrad-blog><img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png alt=Logo>
Akitenkrad's Blog</a></li></ul><ul class="navbar-nav ml-auto"><li class=nav-item><a class=nav-link href=/akitenkrad-blog#home>Home</a></li><li class=nav-item><a class=nav-link href=/akitenkrad-blog#about>About</a></li><li class=nav-item><a class=nav-link href=/akitenkrad-blog#recent-posts>Recent Posts</a></li><li class=nav-item><a class=nav-link id=blog-link href=https://akitenkrad.github.io/akitenkrad-blog/posts>Posts</a></li><li class=nav-item><a class=nav-link id=tag-link href=https://akitenkrad.github.io/akitenkrad-blog/tags>Tags</a></li></ul></div></div><img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png class=d-none id=main-logo alt=Logo>
<img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png class=d-none id=inverted-logo alt="Inverted Logo"></nav><section class=sidebar-section id=sidebar-section><div class=sidebar-holder><div class=sidebar id=sidebar><form class=mx-auto method=get action=/akitenkrad-blog/search><input type=text name=keyword placeholder=Search data-search id=search-box></form><div class=sidebar-tree><ul class=tree id=tree><li id=list-heading><a href=/posts data-filter=all>Posts</a></li><div class=subtree><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/>Papers</a><ul><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202205/>2022.05</a><ul><li><a href=/akitenkrad-blog/posts/papers/202205/20220518224923/ title="A Context-Aware Citation Recommendation Model with BERT and Graph Convolutional Networks">A Context-Aware Citation Recommendation Model with BERT and Graph Convolutional Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220508162318/ title="A Deep Cascade Model for Multi-Document Reading Comprehension">A Deep Cascade Model for Multi-Document Reading Comprehension</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220506021208/ title="A Primer in BERTology: What We Know About How BERT Works">A Primer in BERTology: What We Know About How BERT Works</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220529131339/ title="Attention Is All You Need">Attention Is All You Need</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220503010000/ title="DeBERTa: Decoding-Enhanced BERT with Disentangled Attention">DeBERTa: Decoding-Enhanced BERT with Disentangled Attention</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220505222900/ title="Dense Passage Retrieval for Open-Domain Question Answering">Dense Passage Retrieval for Open-Domain Question Answering</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220511010217/ title="Multi-Style Generative Reading Comprehension">Multi-Style Generative Reading Comprehension</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220530102936/ title="Neural Machine Translation of Rare Words with Subword Units">Neural Machine Translation of Rare Words with Subword Units</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220523223206/ title="RoBERTa: A Robustly Optimized BERT Pretraining Approach">RoBERTa: A Robustly Optimized BERT Pretraining Approach</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220520124748/ title="Semi-Supervised Classification with Graph Convolutional Networks">Semi-Supervised Classification with Graph Convolutional Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220509110738/ title="Survey on graph embeddings and their applications to machine learning problems on graphs">Survey on graph embeddings and their applications to machine learning problems on graphs</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220514151839/ title="UnitedQA: A Hybrid Approach for Open Domain Question Answering">UnitedQA: A Hybrid Approach for Open Domain Question Answering</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202206/>2022.06</a><ul><li><a href=/akitenkrad-blog/posts/papers/202206/20220612105422/ title="Attributed Network Embedding for Learning in a Dynamic Environment">Attributed Network Embedding for Learning in a Dynamic Environment</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220608085622/ title="CodeBERT: A Pre-Trained Model for Programming and Natural Languages">CodeBERT: A Pre-Trained Model for Programming and Natural Languages</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220618223844/ title="High-order Proximity Preserved Embedding for Dynamic Networks">High-order Proximity Preserved Embedding for Dynamic Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220602171700/ title="S-Net: From Answer Extraction to Answer Generation for Machine Reading Comprehension">S-Net: From Answer Extraction to Answer Generation for Machine Reading Comprehension</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202207/>2022.07</a><ul><li><a href=/akitenkrad-blog/posts/papers/202207/20220727145036/ title="Dynamic Heterogeneous Graph Embedding Using Hierarchical Attentions">Dynamic Heterogeneous Graph Embedding Using Hierarchical Attentions</a></li><li><a href=/akitenkrad-blog/posts/papers/202207/20220726163444/ title="Dynamic Network Embedding Survey">Dynamic Network Embedding Survey</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202208/>2022.08</a><ul><li><a href=/akitenkrad-blog/posts/papers/202208/20220802103319/ title="Modeling Dynamic Heterogeneous Network for Link Prediction using Hierarchical Attention with Temporal RNN">Modeling Dynamic Heterogeneous Network for Link Prediction using Hierarchical Attention with Temporal RNN</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202209/>2022.09</a><ul><li><a href=/akitenkrad-blog/posts/papers/202209/20220909180827/ title="Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting">Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202305/>2023.05</a><ul><li><a href=/akitenkrad-blog/posts/papers/202305/20230513094804/ title="Improving Language Understanding by Generative Pre-Training">Improving Language Understanding by Generative Pre-Training</a></li><li><a href=/akitenkrad-blog/posts/papers/202305/20230514200241/ title="Language Models are Unsupervised Multitask Learners">Language Models are Unsupervised Multitask Learners</a></li></ul></li><li><a href=/akitenkrad-blog/posts/papers/202306/ title=2023.06>2023.06</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/akitenkrad-blog/posts/arxiv/>arXiv</a><ul class=active><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202307/>2023.07</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230701000000/ title="arXiv @ 2023.07.01">arXiv @ 2023.07.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230702000000/ title="arXiv @ 2023.07.02">arXiv @ 2023.07.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230703000000/ title="arXiv @ 2023.07.03">arXiv @ 2023.07.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230704000000/ title="arXiv @ 2023.07.04">arXiv @ 2023.07.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230705000000/ title="arXiv @ 2023.07.05">arXiv @ 2023.07.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230706000000/ title="arXiv @ 2023.07.06">arXiv @ 2023.07.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230707000000/ title="arXiv @ 2023.07.07">arXiv @ 2023.07.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230708000000/ title="arXiv @ 2023.07.08">arXiv @ 2023.07.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230709000000/ title="arXiv @ 2023.07.09">arXiv @ 2023.07.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230710000000/ title="arXiv @ 2023.07.10">arXiv @ 2023.07.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230711000000/ title="arXiv @ 2023.07.11">arXiv @ 2023.07.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230712000000/ title="arXiv @ 2023.07.12">arXiv @ 2023.07.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230713000000/ title="arXiv @ 2023.07.13">arXiv @ 2023.07.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230714000000/ title="arXiv @ 2023.07.14">arXiv @ 2023.07.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230715000000/ title="arXiv @ 2023.07.15">arXiv @ 2023.07.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230716000000/ title="arXiv @ 2023.07.16">arXiv @ 2023.07.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230717000000/ title="arXiv @ 2023.07.17">arXiv @ 2023.07.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230718000000/ title="arXiv @ 2023.07.18">arXiv @ 2023.07.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230719000000/ title="arXiv @ 2023.07.19">arXiv @ 2023.07.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230720000000/ title="arXiv @ 2023.07.20">arXiv @ 2023.07.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230721000000/ title="arXiv @ 2023.07.21">arXiv @ 2023.07.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230722000000/ title="arXiv @ 2023.07.22">arXiv @ 2023.07.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230723000000/ title="arXiv @ 2023.07.23">arXiv @ 2023.07.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230724000000/ title="arXiv @ 2023.07.24">arXiv @ 2023.07.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230725000000/ title="arXiv @ 2023.07.25">arXiv @ 2023.07.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230726000000/ title="arXiv @ 2023.07.26">arXiv @ 2023.07.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230727000000/ title="arXiv @ 2023.07.27">arXiv @ 2023.07.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230728000000/ title="arXiv @ 2023.07.28">arXiv @ 2023.07.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230729000000/ title="arXiv @ 2023.07.29">arXiv @ 2023.07.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230730000000/ title="arXiv @ 2023.07.30">arXiv @ 2023.07.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230731000000/ title="arXiv @ 2023.07.31">arXiv @ 2023.07.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202308/>2023.08</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230801000000/ title="arXiv @ 2023.08.01">arXiv @ 2023.08.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230802000000/ title="arXiv @ 2023.08.02">arXiv @ 2023.08.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230803000000/ title="arXiv @ 2023.08.03">arXiv @ 2023.08.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230804000000/ title="arXiv @ 2023.08.04">arXiv @ 2023.08.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230805000000/ title="arXiv @ 2023.08.05">arXiv @ 2023.08.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230806000000/ title="arXiv @ 2023.08.06">arXiv @ 2023.08.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230807000000/ title="arXiv @ 2023.08.07">arXiv @ 2023.08.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230808000000/ title="arXiv @ 2023.08.08">arXiv @ 2023.08.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230809000000/ title="arXiv @ 2023.08.09">arXiv @ 2023.08.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230810000000/ title="arXiv @ 2023.08.10">arXiv @ 2023.08.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230811000000/ title="arXiv @ 2023.08.11">arXiv @ 2023.08.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230812000000/ title="arXiv @ 2023.08.12">arXiv @ 2023.08.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230813000000/ title="arXiv @ 2023.08.13">arXiv @ 2023.08.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230814000000/ title="arXiv @ 2023.08.14">arXiv @ 2023.08.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230815000000/ title="arXiv @ 2023.08.15">arXiv @ 2023.08.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230816000000/ title="arXiv @ 2023.08.16">arXiv @ 2023.08.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230817000000/ title="arXiv @ 2023.08.17">arXiv @ 2023.08.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230818000000/ title="arXiv @ 2023.08.18">arXiv @ 2023.08.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230819000000/ title="arXiv @ 2023.08.19">arXiv @ 2023.08.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230820000000/ title="arXiv @ 2023.08.20">arXiv @ 2023.08.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230821000000/ title="arXiv @ 2023.08.21">arXiv @ 2023.08.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230822000000/ title="arXiv @ 2023.08.22">arXiv @ 2023.08.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230823000000/ title="arXiv @ 2023.08.23">arXiv @ 2023.08.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230824000000/ title="arXiv @ 2023.08.24">arXiv @ 2023.08.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230825000000/ title="arXiv @ 2023.08.25">arXiv @ 2023.08.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230826000000/ title="arXiv @ 2023.08.26">arXiv @ 2023.08.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230827000000/ title="arXiv @ 2023.08.27">arXiv @ 2023.08.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230828000000/ title="arXiv @ 2023.08.28">arXiv @ 2023.08.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230829000000/ title="arXiv @ 2023.08.29">arXiv @ 2023.08.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230830000000/ title="arXiv @ 2023.08.30">arXiv @ 2023.08.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230831000000/ title="arXiv @ 2023.08.31">arXiv @ 2023.08.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202309/>2023.09</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230901000000/ title="arXiv @ 2023.09.01">arXiv @ 2023.09.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230902000000/ title="arXiv @ 2023.09.02">arXiv @ 2023.09.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230903000000/ title="arXiv @ 2023.09.03">arXiv @ 2023.09.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230904000000/ title="arXiv @ 2023.09.04">arXiv @ 2023.09.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230905000000/ title="arXiv @ 2023.09.05">arXiv @ 2023.09.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230906000000/ title="arXiv @ 2023.09.06">arXiv @ 2023.09.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230907000000/ title="arXiv @ 2023.09.07">arXiv @ 2023.09.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230908000000/ title="arXiv @ 2023.09.08">arXiv @ 2023.09.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230909000000/ title="arXiv @ 2023.09.09">arXiv @ 2023.09.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230910000000/ title="arXiv @ 2023.09.10">arXiv @ 2023.09.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230911000000/ title="arXiv @ 2023.09.11">arXiv @ 2023.09.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230912000000/ title="arXiv @ 2023.09.12">arXiv @ 2023.09.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230913000000/ title="arXiv @ 2023.09.13">arXiv @ 2023.09.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230914000000/ title="arXiv @ 2023.09.14">arXiv @ 2023.09.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230915000000/ title="arXiv @ 2023.09.15">arXiv @ 2023.09.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230916000000/ title="arXiv @ 2023.09.16">arXiv @ 2023.09.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230917000000/ title="arXiv @ 2023.09.17">arXiv @ 2023.09.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230918000000/ title="arXiv @ 2023.09.18">arXiv @ 2023.09.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230919000000/ title="arXiv @ 2023.09.19">arXiv @ 2023.09.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230920000000/ title="arXiv @ 2023.09.20">arXiv @ 2023.09.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230921000000/ title="arXiv @ 2023.09.21">arXiv @ 2023.09.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230922000000/ title="arXiv @ 2023.09.22">arXiv @ 2023.09.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230923000000/ title="arXiv @ 2023.09.23">arXiv @ 2023.09.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230924000000/ title="arXiv @ 2023.09.24">arXiv @ 2023.09.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230925000000/ title="arXiv @ 2023.09.25">arXiv @ 2023.09.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230926000000/ title="arXiv @ 2023.09.26">arXiv @ 2023.09.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230927000000/ title="arXiv @ 2023.09.27">arXiv @ 2023.09.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230928000000/ title="arXiv @ 2023.09.28">arXiv @ 2023.09.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230929000000/ title="arXiv @ 2023.09.29">arXiv @ 2023.09.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230930000000/ title="arXiv @ 2023.09.30">arXiv @ 2023.09.30</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202310/>2023.1</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231001000000/ title="arXiv @ 2023.10.01">arXiv @ 2023.10.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231002000000/ title="arXiv @ 2023.10.02">arXiv @ 2023.10.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231003000000/ title="arXiv @ 2023.10.03">arXiv @ 2023.10.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231004000000/ title="arXiv @ 2023.10.04">arXiv @ 2023.10.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231005000000/ title="arXiv @ 2023.10.05">arXiv @ 2023.10.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231006000000/ title="arXiv @ 2023.10.06">arXiv @ 2023.10.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231007000000/ title="arXiv @ 2023.10.07">arXiv @ 2023.10.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231008000000/ title="arXiv @ 2023.10.08">arXiv @ 2023.10.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231009000000/ title="arXiv @ 2023.10.09">arXiv @ 2023.10.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231010000000/ title="arXiv @ 2023.10.10">arXiv @ 2023.10.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231011000000/ title="arXiv @ 2023.10.11">arXiv @ 2023.10.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231012000000/ title="arXiv @ 2023.10.12">arXiv @ 2023.10.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231013000000/ title="arXiv @ 2023.10.13">arXiv @ 2023.10.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231014000000/ title="arXiv @ 2023.10.14">arXiv @ 2023.10.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231015000000/ title="arXiv @ 2023.10.15">arXiv @ 2023.10.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231016000000/ title="arXiv @ 2023.10.16">arXiv @ 2023.10.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231017000000/ title="arXiv @ 2023.10.17">arXiv @ 2023.10.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231018000000/ title="arXiv @ 2023.10.18">arXiv @ 2023.10.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231019000000/ title="arXiv @ 2023.10.19">arXiv @ 2023.10.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231020000000/ title="arXiv @ 2023.10.20">arXiv @ 2023.10.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231021000000/ title="arXiv @ 2023.10.21">arXiv @ 2023.10.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231022000000/ title="arXiv @ 2023.10.22">arXiv @ 2023.10.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231023000000/ title="arXiv @ 2023.10.23">arXiv @ 2023.10.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231024000000/ title="arXiv @ 2023.10.24">arXiv @ 2023.10.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231025000000/ title="arXiv @ 2023.10.25">arXiv @ 2023.10.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231026000000/ title="arXiv @ 2023.10.26">arXiv @ 2023.10.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231027000000/ title="arXiv @ 2023.10.27">arXiv @ 2023.10.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231028000000/ title="arXiv @ 2023.10.28">arXiv @ 2023.10.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231029000000/ title="arXiv @ 2023.10.29">arXiv @ 2023.10.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231030000000/ title="arXiv @ 2023.10.30">arXiv @ 2023.10.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231031000000/ title="arXiv @ 2023.10.31">arXiv @ 2023.10.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202311/>2023.11</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231101000000/ title="arXiv @ 2023.11.01">arXiv @ 2023.11.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231102000000/ title="arXiv @ 2023.11.02">arXiv @ 2023.11.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231103000000/ title="arXiv @ 2023.11.03">arXiv @ 2023.11.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231104000000/ title="arXiv @ 2023.11.04">arXiv @ 2023.11.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231105000000/ title="arXiv @ 2023.11.05">arXiv @ 2023.11.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231106000000/ title="arXiv @ 2023.11.06">arXiv @ 2023.11.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231107000000/ title="arXiv @ 2023.11.07">arXiv @ 2023.11.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231108000000/ title="arXiv @ 2023.11.08">arXiv @ 2023.11.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231109000000/ title="arXiv @ 2023.11.09">arXiv @ 2023.11.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231110000000/ title="arXiv @ 2023.11.10">arXiv @ 2023.11.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231111000000/ title="arXiv @ 2023.11.11">arXiv @ 2023.11.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231112000000/ title="arXiv @ 2023.11.12">arXiv @ 2023.11.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231113000000/ title="arXiv @ 2023.11.13">arXiv @ 2023.11.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231114000000/ title="arXiv @ 2023.11.14">arXiv @ 2023.11.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231115000000/ title="arXiv @ 2023.11.15">arXiv @ 2023.11.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231116000000/ title="arXiv @ 2023.11.16">arXiv @ 2023.11.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231117000000/ title="arXiv @ 2023.11.17">arXiv @ 2023.11.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231118000000/ title="arXiv @ 2023.11.18">arXiv @ 2023.11.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231119000000/ title="arXiv @ 2023.11.19">arXiv @ 2023.11.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231120000000/ title="arXiv @ 2023.11.20">arXiv @ 2023.11.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231121000000/ title="arXiv @ 2023.11.21">arXiv @ 2023.11.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231122000000/ title="arXiv @ 2023.11.22">arXiv @ 2023.11.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231123000000/ title="arXiv @ 2023.11.23">arXiv @ 2023.11.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231124000000/ title="arXiv @ 2023.11.24">arXiv @ 2023.11.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231125000000/ title="arXiv @ 2023.11.25">arXiv @ 2023.11.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231126000000/ title="arXiv @ 2023.11.26">arXiv @ 2023.11.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231127000000/ title="arXiv @ 2023.11.27">arXiv @ 2023.11.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231128000000/ title="arXiv @ 2023.11.28">arXiv @ 2023.11.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231129000000/ title="arXiv @ 2023.11.29">arXiv @ 2023.11.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231130000000/ title="arXiv @ 2023.11.30">arXiv @ 2023.11.30</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202312/>2023.12</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231201000000/ title="arXiv @ 2023.12.01">arXiv @ 2023.12.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231202000000/ title="arXiv @ 2023.12.02">arXiv @ 2023.12.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231203000000/ title="arXiv @ 2023.12.03">arXiv @ 2023.12.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231204000000/ title="arXiv @ 2023.12.04">arXiv @ 2023.12.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231205000000/ title="arXiv @ 2023.12.05">arXiv @ 2023.12.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231206000000/ title="arXiv @ 2023.12.06">arXiv @ 2023.12.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231207000000/ title="arXiv @ 2023.12.07">arXiv @ 2023.12.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231208000000/ title="arXiv @ 2023.12.08">arXiv @ 2023.12.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231209000000/ title="arXiv @ 2023.12.09">arXiv @ 2023.12.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231210000000/ title="arXiv @ 2023.12.10">arXiv @ 2023.12.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231211000000/ title="arXiv @ 2023.12.11">arXiv @ 2023.12.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231212000000/ title="arXiv @ 2023.12.12">arXiv @ 2023.12.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231213000000/ title="arXiv @ 2023.12.13">arXiv @ 2023.12.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231214000000/ title="arXiv @ 2023.12.14">arXiv @ 2023.12.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231215000000/ title="arXiv @ 2023.12.15">arXiv @ 2023.12.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231216000000/ title="arXiv @ 2023.12.16">arXiv @ 2023.12.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231217000000/ title="arXiv @ 2023.12.17">arXiv @ 2023.12.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231218000000/ title="arXiv @ 2023.12.18">arXiv @ 2023.12.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231219000000/ title="arXiv @ 2023.12.19">arXiv @ 2023.12.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231220000000/ title="arXiv @ 2023.12.20">arXiv @ 2023.12.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231221000000/ title="arXiv @ 2023.12.21">arXiv @ 2023.12.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231222000000/ title="arXiv @ 2023.12.22">arXiv @ 2023.12.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231223000000/ title="arXiv @ 2023.12.23">arXiv @ 2023.12.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231224000000/ title="arXiv @ 2023.12.24">arXiv @ 2023.12.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231225000000/ title="arXiv @ 2023.12.25">arXiv @ 2023.12.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231226000000/ title="arXiv @ 2023.12.26">arXiv @ 2023.12.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231227000000/ title="arXiv @ 2023.12.27">arXiv @ 2023.12.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231228000000/ title="arXiv @ 2023.12.28">arXiv @ 2023.12.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231229000000/ title="arXiv @ 2023.12.29">arXiv @ 2023.12.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231230000000/ title="arXiv @ 2023.12.30">arXiv @ 2023.12.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231231000000/ title="arXiv @ 2023.12.31">arXiv @ 2023.12.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202401/>2024.01</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240101000000/ title="arXiv @ 2024.01.01">arXiv @ 2024.01.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240102000000/ title="arXiv @ 2024.01.02">arXiv @ 2024.01.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240103000000/ title="arXiv @ 2024.01.03">arXiv @ 2024.01.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240104000000/ title="arXiv @ 2024.01.04">arXiv @ 2024.01.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240105000000/ title="arXiv @ 2024.01.05">arXiv @ 2024.01.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240106000000/ title="arXiv @ 2024.01.06">arXiv @ 2024.01.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240107000000/ title="arXiv @ 2024.01.07">arXiv @ 2024.01.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240108000000/ title="arXiv @ 2024.01.08">arXiv @ 2024.01.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240109000000/ title="arXiv @ 2024.01.09">arXiv @ 2024.01.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240110000000/ title="arXiv @ 2024.01.10">arXiv @ 2024.01.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240111000000/ title="arXiv @ 2024.01.11">arXiv @ 2024.01.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240112000000/ title="arXiv @ 2024.01.12">arXiv @ 2024.01.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240113000000/ title="arXiv @ 2024.01.13">arXiv @ 2024.01.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240114000000/ title="arXiv @ 2024.01.14">arXiv @ 2024.01.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240115000000/ title="arXiv @ 2024.01.15">arXiv @ 2024.01.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240116000000/ title="arXiv @ 2024.01.16">arXiv @ 2024.01.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240117000000/ title="arXiv @ 2024.01.17">arXiv @ 2024.01.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240118000000/ title="arXiv @ 2024.01.18">arXiv @ 2024.01.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240119000000/ title="arXiv @ 2024.01.19">arXiv @ 2024.01.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240120000000/ title="arXiv @ 2024.01.20">arXiv @ 2024.01.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240121000000/ title="arXiv @ 2024.01.21">arXiv @ 2024.01.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240122000000/ title="arXiv @ 2024.01.22">arXiv @ 2024.01.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240123000000/ title="arXiv @ 2024.01.23">arXiv @ 2024.01.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240124000000/ title="arXiv @ 2024.01.24">arXiv @ 2024.01.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240125000000/ title="arXiv @ 2024.01.25">arXiv @ 2024.01.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240126000000/ title="arXiv @ 2024.01.26">arXiv @ 2024.01.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240127000000/ title="arXiv @ 2024.01.27">arXiv @ 2024.01.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240128000000/ title="arXiv @ 2024.01.28">arXiv @ 2024.01.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240129000000/ title="arXiv @ 2024.01.29">arXiv @ 2024.01.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240130000000/ title="arXiv @ 2024.01.30">arXiv @ 2024.01.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240131000000/ title="arXiv @ 2024.01.31">arXiv @ 2024.01.31</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/akitenkrad-blog/posts/arxiv/202402/>2024.02</a><ul class=active><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240201000000/ title="arXiv @ 2024.02.01">arXiv @ 2024.02.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240202000000/ title="arXiv @ 2024.02.02">arXiv @ 2024.02.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240203000000/ title="arXiv @ 2024.02.03">arXiv @ 2024.02.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240204000000/ title="arXiv @ 2024.02.04">arXiv @ 2024.02.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240205000000/ title="arXiv @ 2024.02.05">arXiv @ 2024.02.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240206000000/ title="arXiv @ 2024.02.06">arXiv @ 2024.02.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240207000000/ title="arXiv @ 2024.02.07">arXiv @ 2024.02.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240208000000/ title="arXiv @ 2024.02.08">arXiv @ 2024.02.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240209000000/ title="arXiv @ 2024.02.09">arXiv @ 2024.02.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240210000000/ title="arXiv @ 2024.02.10">arXiv @ 2024.02.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240211000000/ title="arXiv @ 2024.02.11">arXiv @ 2024.02.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240212000000/ title="arXiv @ 2024.02.12">arXiv @ 2024.02.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240213000000/ title="arXiv @ 2024.02.13">arXiv @ 2024.02.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240214000000/ title="arXiv @ 2024.02.14">arXiv @ 2024.02.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240215000000/ title="arXiv @ 2024.02.15">arXiv @ 2024.02.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240216000000/ title="arXiv @ 2024.02.16">arXiv @ 2024.02.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240217000000/ title="arXiv @ 2024.02.17">arXiv @ 2024.02.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240218000000/ title="arXiv @ 2024.02.18">arXiv @ 2024.02.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240219000000/ title="arXiv @ 2024.02.19">arXiv @ 2024.02.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240220000000/ title="arXiv @ 2024.02.20">arXiv @ 2024.02.20</a></li><li><a class=active href=/akitenkrad-blog/posts/arxiv/202402/20240221000000/ title="arXiv @ 2024.02.21">arXiv @ 2024.02.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240222000000/ title="arXiv @ 2024.02.22">arXiv @ 2024.02.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240223000000/ title="arXiv @ 2024.02.23">arXiv @ 2024.02.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240224000000/ title="arXiv @ 2024.02.24">arXiv @ 2024.02.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240225000000/ title="arXiv @ 2024.02.25">arXiv @ 2024.02.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240226000000/ title="arXiv @ 2024.02.26">arXiv @ 2024.02.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240227000000/ title="arXiv @ 2024.02.27">arXiv @ 2024.02.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240228000000/ title="arXiv @ 2024.02.28">arXiv @ 2024.02.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240229000000/ title="arXiv @ 2024.02.29">arXiv @ 2024.02.29</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202403/>2024.03</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240301000000/ title="arXiv @ 2024.03.01">arXiv @ 2024.03.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240302000000/ title="arXiv @ 2024.03.02">arXiv @ 2024.03.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240303000000/ title="arXiv @ 2024.03.03">arXiv @ 2024.03.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240304000000/ title="arXiv @ 2024.03.04">arXiv @ 2024.03.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240305000000/ title="arXiv @ 2024.03.05">arXiv @ 2024.03.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240306000000/ title="arXiv @ 2024.03.06">arXiv @ 2024.03.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240307000000/ title="arXiv @ 2024.03.07">arXiv @ 2024.03.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240308000000/ title="arXiv @ 2024.03.08">arXiv @ 2024.03.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240309000000/ title="arXiv @ 2024.03.09">arXiv @ 2024.03.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240310000000/ title="arXiv @ 2024.03.10">arXiv @ 2024.03.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240311000000/ title="arXiv @ 2024.03.11">arXiv @ 2024.03.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240312000000/ title="arXiv @ 2024.03.12">arXiv @ 2024.03.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240313000000/ title="arXiv @ 2024.03.13">arXiv @ 2024.03.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240314000000/ title="arXiv @ 2024.03.14">arXiv @ 2024.03.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240315000000/ title="arXiv @ 2024.03.15">arXiv @ 2024.03.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240316000000/ title="arXiv @ 2024.03.16">arXiv @ 2024.03.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240317000000/ title="arXiv @ 2024.03.17">arXiv @ 2024.03.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240318000000/ title="arXiv @ 2024.03.18">arXiv @ 2024.03.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240319000000/ title="arXiv @ 2024.03.19">arXiv @ 2024.03.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240320000000/ title="arXiv @ 2024.03.20">arXiv @ 2024.03.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240321000000/ title="arXiv @ 2024.03.21">arXiv @ 2024.03.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240322000000/ title="arXiv @ 2024.03.22">arXiv @ 2024.03.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240323000000/ title="arXiv @ 2024.03.23">arXiv @ 2024.03.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240324000000/ title="arXiv @ 2024.03.24">arXiv @ 2024.03.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240325000000/ title="arXiv @ 2024.03.25">arXiv @ 2024.03.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240326000000/ title="arXiv @ 2024.03.26">arXiv @ 2024.03.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240327000000/ title="arXiv @ 2024.03.27">arXiv @ 2024.03.27</a></li></ul></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/algorithms/>Algorithms</a><ul><li><a href=/akitenkrad-blog/posts/algorithms/bandit-algorithm-basic/ title="Bandit Algorithm Basic">Bandit Algorithm Basic</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/conference/>Conference</a><ul><li><a href=/akitenkrad-blog/posts/conference/acl/ title="Annual Meeting of the Association for Computational Linguistics">Annual Meeting of the Association for Computational Linguistics</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/>Figures</a><ul><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/202208/>2022.08</a><ul><li><a href=/akitenkrad-blog/posts/figures/202208/20220822092748/ title=1ヶ月当たりの実労働時間の推移>1ヶ月当たりの実労働時間の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220813115200/ title=コンビニエンスストアの店舗数の推移>コンビニエンスストアの店舗数の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220821113322/ title=地域別1世帯当たり1ヶ月間の支出額の推移>地域別1世帯当たり1ヶ月間の支出額の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220828111836/ title="夫婦別子供の有無別生活時間 (2016)">夫婦別子供の有無別生活時間 (2016)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220827103615/ title=家族類型の変遷>家族類型の変遷</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220815131016/ title="年齢別大学院入学者数 (2021)">年齢別大学院入学者数 (2021)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220818122628/ title=情報サービス業における企業特殊的人的資本（名目）の推移>情報サービス業における企業特殊的人的資本（名目）の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220824095127/ title=有業者の社会生活における活動の変化>有業者の社会生活における活動の変化</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220814175022/ title=東京23区の乗用車保有台数>東京23区の乗用車保有台数</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220817121117/ title="業種別企業特殊的人的資本 (2018)">業種別企業特殊的人的資本 (2018)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220825094111/ title=無業者の社会生活における活動の変化>無業者の社会生活における活動の変化</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220823104100/ title=睡眠時間の推移>睡眠時間の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220826102652/ title=総人口の推移>総人口の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220820225528/ title=都道府県別国公立別学校数・学生数>都道府県別国公立別学校数・学生数</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220816120845/ title="都道府県別消費者物価指数/教育（全国平均=100）">都道府県別消費者物価指数/教育（全国平均=100）</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220812173814/ title=金沢市の夏の月平均気温の遷移>金沢市の夏の月平均気温の遷移</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/202209/>2022.09</a><ul><li><a href=/akitenkrad-blog/posts/figures/202209/20220907122906/ title="ACL-2022 WordCloud">ACL-2022 WordCloud</a></li></ul></li></ul></li><li><a href=/akitenkrad-blog/posts/latex/ title="Latex Mathematics Syntax Guide">Latex Mathematics Syntax Guide</a></li><li><a href=/akitenkrad-blog/posts/markdown/ title="Markdown Sample">Markdown Sample</a></li></div></ul></div></div></div></section><section class=content-section id=content-section><div class=content><div class="container p-0 read-area"><div class="hero-area col-sm-12" id=hero-area style=background-image:url(/akitenkrad-blog/posts/arxiv/202402/20240221000000/hero.png)></div><div class=page-content><div class="author-profile ml-auto align-self-lg-center"><img class=rounded-circle src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_120x120_fit_box_3.png alt="Author Image"><h5 class=author-name></h5><p>Wednesday, Feb 21, 2024</p></div><div class=title><h1>arXiv @ 2024.02.21</h1></div><div class=taxonomy-terms><ul><li class=rounded><a href=/akitenkrad-blog/tags/arxiv class="btn, btn-sm">arXiv</a></li><li class=rounded><a href=/akitenkrad-blog/tags/published2024 class="btn, btn-sm">2024</a></li></ul></div><div class=post-content id=post-content><figure style=border:none;width:100%;display:flex;justify-content:center><iframe src=pie.html width=900 height=620 style=border:none></iframe></figure><h2 id=primary-categories>Primary Categories</h2><ul><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240221000000/#astro-phep-1>astro-ph.EP (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240221000000/#astro-phga-1>astro-ph.GA (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240221000000/#astro-phsr-1>astro-ph.SR (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240221000000/#csai-9>cs.AI (9)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240221000000/#csar-2>cs.AR (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240221000000/#cscg-3>cs.CG (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240221000000/#cscl-109>cs.CL (109)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240221000000/#cscr-9>cs.CR (9)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240221000000/#cscv-45>cs.CV (45)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240221000000/#cscy-1>cs.CY (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240221000000/#csdb-2>cs.DB (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240221000000/#csdc-5>cs.DC (5)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240221000000/#csdl-1>cs.DL (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240221000000/#csds-4>cs.DS (4)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240221000000/#csgt-3>cs.GT (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240221000000/#cshc-4>cs.HC (4)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240221000000/#csir-7>cs.IR (7)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240221000000/#csit-3>cs.IT (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240221000000/#cslg-71>cs.LG (71)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240221000000/#cslo-1>cs.LO (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240221000000/#csma-2>cs.MA (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240221000000/#csne-3>cs.NE (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240221000000/#csni-2>cs.NI (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240221000000/#cspl-1>cs.PL (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240221000000/#csro-11>cs.RO (11)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240221000000/#cssd-5>cs.SD (5)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240221000000/#csse-3>cs.SE (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240221000000/#cssi-5>cs.SI (5)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240221000000/#eessas-3>eess.AS (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240221000000/#eessiv-1>eess.IV (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240221000000/#eesssp-2>eess.SP (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240221000000/#eesssy-5>eess.SY (5)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240221000000/#mathco-2>math.CO (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240221000000/#mathna-6>math.NA (6)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240221000000/#mathoc-1>math.OC (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240221000000/#mathst-1>math.ST (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240221000000/#physicschem-ph-1>physics.chem-ph (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240221000000/#physicscomp-ph-2>physics.comp-ph (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240221000000/#q-biobm-1>q-bio.BM (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240221000000/#q-bionc-1>q-bio.NC (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240221000000/#statml-6>stat.ML (6)</a></li></ul><h2 id=keywords>Keywords</h2><table border=1 class=dataframe><thead><tr style=text-align:right><th>keyword</th><th>cs.CL</th><th>cs.CV</th><th>cs.LG</th><th>cs.RO</th></tr></thead><tbody><tr><td>AI-generated Text Detection</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Active Learning</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Adversarial Attack</td><td></td><td>2</td><td>2</td><td></td></tr><tr><td>Adversarial Learning</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Alpaca</td><td>3</td><td></td><td></td><td></td></tr><tr><td>Anomaly Detection</td><td></td><td></td><td>2</td><td></td></tr><tr><td>Autoencoder</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Automatic Evaluation</td><td>2</td><td></td><td></td><td></td></tr><tr><td>BART</td><td>1</td><td></td><td></td><td></td></tr><tr><td>BERT</td><td>6</td><td></td><td></td><td></td></tr><tr><td>BERTScore</td><td>1</td><td>1</td><td></td><td></td></tr><tr><td>BLEU</td><td>2</td><td></td><td></td><td></td></tr><tr><td>BLOOM</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Bandit Algorithm</td><td></td><td></td><td>2</td><td></td></tr><tr><td>Bard</td><td></td><td></td><td></td><td>1</td></tr><tr><td>Benchmarking</td><td>28</td><td>10</td><td>14</td><td>1</td></tr><tr><td>Black Box</td><td>3</td><td>1</td><td>3</td><td></td></tr><tr><td>Chain-of-thought Prompt</td><td>2</td><td></td><td></td><td></td></tr><tr><td>ChatGPT</td><td>4</td><td></td><td></td><td></td></tr><tr><td>Chatbot</td><td>1</td><td></td><td>1</td><td></td></tr><tr><td>Claude</td><td>2</td><td></td><td></td><td></td></tr><tr><td>Clustering</td><td>1</td><td>1</td><td>3</td><td></td></tr><tr><td>Code Generation</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Common-sense Reasoning</td><td>2</td><td></td><td></td><td></td></tr><tr><td>Continual Learning</td><td></td><td>1</td><td>2</td><td></td></tr><tr><td>Contrastive Learning</td><td>2</td><td>4</td><td></td><td></td></tr><tr><td>Convolution</td><td></td><td>4</td><td>2</td><td></td></tr><tr><td>Convolutional Neural Network</td><td></td><td>4</td><td>3</td><td></td></tr><tr><td>Counter-factual</td><td>3</td><td>1</td><td>2</td><td></td></tr><tr><td>Data Augmentation</td><td>3</td><td>2</td><td>1</td><td></td></tr><tr><td>Dialogue System</td><td>2</td><td></td><td></td><td></td></tr><tr><td>Diffusion Model</td><td>1</td><td>9</td><td>5</td><td></td></tr><tr><td>Direct Preference Optimization</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Disambiguation</td><td></td><td></td><td></td><td>1</td></tr><tr><td>Distribution Shift</td><td>2</td><td></td><td></td><td></td></tr><tr><td>Distributional Reinforcement Learning</td><td></td><td></td><td></td><td>1</td></tr><tr><td>Emotion Recognition</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Event Argument Extraction</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Explainable AI</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Fact Verification</td><td>3</td><td></td><td>1</td><td></td></tr><tr><td>Fairness</td><td>2</td><td></td><td>3</td><td></td></tr><tr><td>Federated Learning</td><td></td><td></td><td>3</td><td></td></tr><tr><td>Few-shot</td><td>10</td><td>1</td><td>1</td><td></td></tr><tr><td>Few-shot Learning</td><td>2</td><td></td><td></td><td></td></tr><tr><td>Fine-tuning</td><td>27</td><td>6</td><td>9</td><td></td></tr><tr><td>Foundation Model</td><td>1</td><td>5</td><td>2</td><td></td></tr><tr><td>GPT</td><td>13</td><td>2</td><td>3</td><td></td></tr><tr><td>GPT-3</td><td>5</td><td></td><td>2</td><td></td></tr><tr><td>GPT-3.5</td><td>5</td><td></td><td>2</td><td></td></tr><tr><td>GPT-4</td><td>11</td><td></td><td>3</td><td></td></tr><tr><td>GPT-4 turbo</td><td>2</td><td></td><td></td><td></td></tr><tr><td>Gaussian Process</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Gemini</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Generative AI</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Generative Adversarial Network</td><td></td><td>2</td><td></td><td></td></tr><tr><td>Geometry</td><td></td><td>3</td><td></td><td>1</td></tr><tr><td>Graph</td><td>6</td><td>2</td><td>9</td><td>3</td></tr><tr><td>Graph Anomaly Detection</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Graph Attention Networks</td><td></td><td>1</td><td>1</td><td></td></tr><tr><td>Graph Neural Network</td><td></td><td></td><td>4</td><td></td></tr><tr><td>Grounding</td><td>1</td><td>1</td><td></td><td>1</td></tr><tr><td>High-Resource</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Human Intervention</td><td>1</td><td></td><td></td><td>1</td></tr><tr><td>Image2text</td><td>1</td><td>1</td><td></td><td></td></tr><tr><td>In-context Learning</td><td>15</td><td></td><td>3</td><td></td></tr><tr><td>Information Retrieval</td><td>2</td><td></td><td></td><td></td></tr><tr><td>Instruction Following</td><td>3</td><td>1</td><td>1</td><td></td></tr><tr><td>Instruction Tuning</td><td>2</td><td></td><td></td><td></td></tr><tr><td>Knowledge Distillation</td><td>10</td><td></td><td>4</td><td></td></tr><tr><td>Knowledge Graph</td><td>3</td><td></td><td></td><td></td></tr><tr><td>Knowledge Transfer</td><td>2</td><td></td><td></td><td></td></tr><tr><td>LLaMA</td><td>5</td><td></td><td>1</td><td></td></tr><tr><td>LSTM</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Large Language Model</td><td>135</td><td>8</td><td>21</td><td>1</td></tr><tr><td>Logistic Regression</td><td></td><td></td><td>2</td><td></td></tr><tr><td>Low-Resource</td><td>2</td><td></td><td>2</td><td></td></tr><tr><td>MNIST</td><td></td><td>1</td><td>3</td><td></td></tr><tr><td>Machine Unlearning</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Markov Decision Process</td><td></td><td></td><td>1</td><td>1</td></tr><tr><td>Mathematical Reasoning</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Meta Learning</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Mistral</td><td>5</td><td></td><td>1</td><td></td></tr><tr><td>Model Compression</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Multi-modal</td><td>15</td><td>9</td><td>3</td><td></td></tr><tr><td>Multiple Instance Learning</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Mutual Information</td><td></td><td>1</td><td>2</td><td></td></tr><tr><td>N-gram</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Named Entity Recognition</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Natural Language Generation</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Natural Language Inference</td><td>2</td><td></td><td></td><td></td></tr><tr><td>Natural Language Understanding</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Neural Machine Translation</td><td>7</td><td></td><td></td><td></td></tr><tr><td>Node Classification</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Object Detection</td><td></td><td>4</td><td>1</td><td></td></tr><tr><td>Offline Reinforcement Learning</td><td></td><td></td><td>2</td><td></td></tr><tr><td>Online Reinforcement Learning</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Out-of-distribution</td><td></td><td>1</td><td>1</td><td></td></tr><tr><td>Out-of-domain</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Perplexity</td><td>4</td><td></td><td>2</td><td></td></tr><tr><td>Planning Domain Descrition Language</td><td></td><td></td><td></td><td>1</td></tr><tr><td>Pre-trained Language Model</td><td>4</td><td></td><td>2</td><td></td></tr><tr><td>Probabilistic Model</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Prompt</td><td>28</td><td>3</td><td>5</td><td></td></tr><tr><td>Pruning</td><td></td><td>1</td><td>1</td><td></td></tr><tr><td>Quantization</td><td>2</td><td></td><td>5</td><td></td></tr><tr><td>Question Answering</td><td>16</td><td>1</td><td>1</td><td></td></tr><tr><td>Reasoning</td><td>21</td><td>3</td><td>5</td><td></td></tr><tr><td>Recommendation</td><td>1</td><td></td><td>1</td><td></td></tr><tr><td>Recommender System</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Recurrent Neural Network</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Reinforcement Learning</td><td>1</td><td></td><td>12</td><td>2</td></tr><tr><td>Relation Extraction</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Representation Learning</td><td>1</td><td>2</td><td></td><td></td></tr><tr><td>Retrieval-Augmented Generation</td><td>9</td><td></td><td>3</td><td></td></tr><tr><td>RoBERTa</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Rouge</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Sample Size</td><td></td><td></td><td>2</td><td></td></tr><tr><td>Scaling Law</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Self-Attention</td><td></td><td>3</td><td></td><td></td></tr><tr><td>Self-Distillation</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Self-supervised Learning</td><td></td><td>7</td><td>2</td><td></td></tr><tr><td>Self-supervised Pre-training</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Sentence Embedding</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Simulation</td><td>2</td><td>1</td><td>3</td><td>4</td></tr><tr><td>Simulator</td><td>2</td><td>1</td><td>3</td><td>4</td></tr><tr><td>Speech-to-Speech Translation</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Stemming</td><td></td><td></td><td></td><td>1</td></tr><tr><td>Stochastic Gradient Descent</td><td>1</td><td></td><td>5</td><td></td></tr><tr><td>Style Transfer</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Summarization</td><td>4</td><td>1</td><td></td><td></td></tr><tr><td>Supervised Learning</td><td>3</td><td>5</td><td>8</td><td></td></tr><tr><td>T5</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Temporal Knowledge Graph</td><td>2</td><td></td><td></td><td></td></tr><tr><td>Text Analysis</td><td></td><td>1</td><td></td><td></td></tr><tr><td>Text Embedding</td><td>2</td><td>1</td><td></td><td></td></tr><tr><td>Text Generation</td><td>2</td><td></td><td>1</td><td></td></tr><tr><td>Text Mining</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Text Summarization</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Text2SQL</td><td>2</td><td></td><td></td><td></td></tr><tr><td>Text2image</td><td>1</td><td>2</td><td></td><td></td></tr><tr><td>Transfer Learning</td><td>1</td><td></td><td>2</td><td></td></tr><tr><td>Transformer</td><td>5</td><td>7</td><td>9</td><td></td></tr><tr><td>Unsupervised Learning</td><td></td><td>1</td><td>9</td><td></td></tr><tr><td>Variational Autoencoder</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Vision Transformer</td><td></td><td>4</td><td>2</td><td></td></tr><tr><td>Vision-and-Language</td><td>4</td><td>3</td><td>1</td><td>1</td></tr><tr><td>Visual Question Answering</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Weakly-supervised Learning</td><td></td><td>2</td><td></td><td></td></tr><tr><td>Word Embedding</td><td>2</td><td></td><td></td><td></td></tr><tr><td>Zero-shot</td><td>6</td><td>2</td><td>2</td><td></td></tr></tbody></table><script>$(function(){$("table").addClass("keyword-table table-bordered border-success"),$("table thead").addClass("sticky-top"),$("table tbody td").css("text-align","")})</script><h2 id=cscl-109>cs.CL (109)</h2><h3 id=1109--1346-artprompt-ascii-art-based-jailbreak-attacks-against-aligned-llms-fengqing-jiang-et-al-2024>(1/109 | 1/346) ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs (Fengqing Jiang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fengqing Jiang, Zhangchen Xu, Luyao Niu, Zhen Xiang, Bhaskar Ramasubramanian, Bo Li, Radha Poovendran. (2024)<br><strong>ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs</strong><br><button class=copy-to-clipboard title="ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs" index=1>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-1 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 118<br>Keywords: Benchmarking, Black Box, Fine-tuning, Supervised Learning, Claude, GPT, GPT-3, GPT-3.5, GPT-4, Gemini, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11753v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11753v2.pdf filename=2402.11753v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Safety is critical to the usage of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs).</b> Multiple techniques such as data filtering and <b>supervised</b> <b>fine-tuning</b> have been developed to strengthen <b>LLM</b> safety. However, currently known techniques presume that corpora used for safety alignment of <b>LLMs</b> are solely interpreted by semantics. This assumption, however, does not hold in real-world applications, which leads to severe vulnerabilities in <b>LLMs.</b> For example, users of forums often use ASCII art, a form of text-based art, to convey image information. In this paper, we propose a novel ASCII art-based jailbreak attack and introduce a comprehensive <b>benchmark</b> Vision-in-Text Challenge (ViTC) to evaluate the capabilities of <b>LLMs</b> in recognizing <b>prompts</b> that cannot be solely interpreted by semantics. We show that five SOTA <b>LLMs</b> <b>(GPT-3.5,</b> <b>GPT-4,</b> <b>Gemini,</b> <b>Claude,</b> and Llama2) struggle to recognize <b>prompts</b> provided in the form of ASCII art. Based on this observation, we develop the jailbreak attack ArtPrompt, which leverages the poor performance of <b>LLMs</b> in recognizing ASCII art to bypass safety measures and elicit undesired behaviors from <b>LLMs.</b> ArtPrompt only requires <b>black-box</b> <b>access</b> to the victim <b>LLMs,</b> making it a practical attack. We evaluate ArtPrompt on five SOTA <b>LLMs,</b> and show that ArtPrompt can effectively and efficiently induce undesired behaviors from all five <b>LLMs.</b></p></p class="citation"></blockquote><h3 id=2109--2346-graph-based-retriever-captures-the-long-tail-of-biomedical-knowledge-julien-delile-et-al-2024>(2/109 | 2/346) Graph-Based Retriever Captures the Long Tail of Biomedical Knowledge (Julien Delile et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Julien Delile, Srayanta Mukherjee, Anton Van Pamel, Leonid Zhukov. (2024)<br><strong>Graph-Based Retriever Captures the Long Tail of Biomedical Knowledge</strong><br><button class=copy-to-clipboard title="Graph-Based Retriever Captures the Long Tail of Biomedical Knowledge" index=2>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-2 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-IR, cs.CL<br>Keyword Score: 108<br>Keywords: Graph, Knowledge Graph, Retrieval-Augmented Generation, Retrieval-Augmented Generation, Retrieval-Augmented Generation, Information Retrieval, Question Answering, Large Language Model, Large Language Model, Prompt, Summarization, Text Embedding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12352v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12352v1.pdf filename=2402.12352v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> are transforming the way <b>information</b> <b>is</b> retrieved with vast amounts of <b>knowledge</b> <b>being</b> <b>summarized</b> and presented via natural language conversations. Yet, <b>LLMs</b> are prone to highlight the most frequently seen pieces of <b>information</b> <b>from</b> the training set and to neglect the rare ones. In the field of biomedical research, latest discoveries are key to academic and industrial actors and are obscured by the abundance of an ever-increasing literature corpus (the <b>information</b> <b>overload</b> problem). Surfacing new associations between biomedical entities, e.g., drugs, genes, diseases, with <b>LLMs</b> becomes a challenge of capturing the long-tail <b>knowledge</b> <b>of</b> the biomedical scientific production. To overcome this challenge, <b>Retrieval</b> <b>Augmented</b> <b>Generation</b> <b>(RAG)</b> has been proposed to alleviate some of the shortcomings of <b>LLMs</b> by augmenting the <b>prompts</b> with context retrieved from external datasets. <b>RAG</b> methods typically select the context via maximum similarity search over <b>text</b> <b>embeddings.</b> In this study, we show that <b>RAG</b> methods leave out a significant proportion of relevant <b>information</b> <b>due</b> to clusters of over-represented concepts in the biomedical literature. We introduce a novel <b>information-retrieval</b> <b>method</b> that leverages a <b>knowledge</b> <b>graph</b> to downsample these clusters and mitigate the <b>information</b> <b>overload</b> problem. Its <b>retrieval</b> <b>performance</b> <b>is</b> about twice better than embedding similarity alternatives on both precision and recall. Finally, we demonstrate that both embedding similarity and <b>knowledge</b> <b>graph</b> <b>retrieval</b> <b>methods</b> <b>can</b> be advantageously combined into a hybrid model that outperforms both, enabling potential improvements to biomedical <b>question-answering</b> <b>models.</b></p></p class="citation"></blockquote><h3 id=3109--3346-is-open-source-there-yet-a-comparative-study-on-commercial-and-open-source-llms-in-their-ability-to-label-chest-x-ray-reports-felix-j-dorfner-et-al-2024>(3/109 | 3/346) Is Open-Source There Yet? A Comparative Study on Commercial and Open-Source LLMs in Their Ability to Label Chest X-Ray Reports (Felix J. Dorfner et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Felix J. Dorfner, Liv Jürgensen, Leonhard Donle, Fares Al Mohamad, Tobias R. Bodenmann, Mason C. Cleveland, Felix Busch, Lisa C. Adams, James Sato, Thomas Schultz, Albert E. Kim, Jameson Merkow, Keno K. Bressem, Christopher P. Bridge. (2024)<br><strong>Is Open-Source There Yet? A Comparative Study on Commercial and Open-Source LLMs in Their Ability to Label Chest X-Ray Reports</strong><br><button class=copy-to-clipboard title="Is Open-Source There Yet? A Comparative Study on Commercial and Open-Source LLMs in Their Ability to Label Chest X-Ray Reports" index=3>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-3 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 100<br>Keywords: Few-shot, Zero-shot, GPT, GPT-3, GPT-3.5, GPT-4, Mistral, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12298v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12298v1.pdf filename=2402.12298v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Introduction: With the rapid advances in <b>large</b> <b>language</b> <b>models</b> <b>(LLMs),</b> there have been numerous new open source as well as commercial models. While recent publications have explored <b>GPT-4</b> in its application to extracting information of interest from radiology reports, there has not been a real-world comparison of <b>GPT-4</b> to different leading open-source models. Materials and Methods: Two different and independent datasets were used. The first dataset consists of 540 chest x-ray reports that were created at the Massachusetts General Hospital between July 2019 and July 2021. The second dataset consists of 500 chest x-ray reports from the ImaGenome dataset. We then compared the commercial models <b>GPT-3.5</b> Turbo and <b>GPT-4</b> from OpenAI to the open-source models <b>Mistral-7B,</b> Mixtral-8x7B, Llama2-13B, Llama2-70B, QWEN1.5-72B and CheXbert and CheXpert-labeler in their ability to accurately label the presence of multiple findings in x-ray text reports using different <b>prompting</b> techniques. Results: On the ImaGenome dataset, the best performing open-source model was Llama2-70B with micro F1-scores of 0.972 and 0.970 for zero- and <b>few-shot</b> <b>prompts,</b> respectively. <b>GPT-4</b> achieved micro F1-scores of 0.975 and 0.984, respectively. On the institutional dataset, the best performing open-source model was QWEN1.5-72B with micro F1-scores of 0.952 and 0.965 for zero- and <b>few-shot</b> <b>prompting,</b> respectively. <b>GPT-4</b> achieved micro F1-scores of 0.975 and 0.973, respectively. Conclusion: In this paper, we show that while <b>GPT-4</b> is superior to open-source models in <b>zero-shot</b> report labeling, the implementation of <b>few-shot</b> <b>prompting</b> can bring open-source models on par with <b>GPT-4.</b> This shows that open-source models could be a performant and privacy preserving alternative to <b>GPT-4</b> for the task of radiology report classification.</p></p class="citation"></blockquote><h3 id=4109--4346-bider-bridging-knowledge-inconsistency-for-efficient-retrieval-augmented-llms-via-key-supporting-evidence-jiajie-jin-et-al-2024>(4/109 | 4/346) BIDER: Bridging Knowledge Inconsistency for Efficient Retrieval-Augmented LLMs via Key Supporting Evidence (Jiajie Jin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiajie Jin, Yutao Zhu, Yujia Zhou, Zhicheng Dou. (2024)<br><strong>BIDER: Bridging Knowledge Inconsistency for Efficient Retrieval-Augmented LLMs via Key Supporting Evidence</strong><br><button class=copy-to-clipboard title="BIDER: Bridging Knowledge Inconsistency for Efficient Retrieval-Augmented LLMs via Key Supporting Evidence" index=4>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-4 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 90<br>Keywords: Fine-tuning, Reinforcement Learning, Simulation, Simulator, Supervised Learning, Question Answering, Question Answering, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12174v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12174v1.pdf filename=2402.12174v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Retrieval-augmented <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have demonstrated efficacy in knowledge-intensive tasks such as open-domain <b>QA,</b> addressing inherent challenges in knowledge update and factual inadequacy. However, inconsistencies between retrieval knowledge and the necessary knowledge for <b>LLMs,</b> leading to a decline in <b>LLM&rsquo;s</b> answer quality. This paper introduces BIDER, an approach that refines retrieval documents into Key Supporting Evidence (KSE) through knowledge synthesis, <b>supervised</b> <b>fine-tuning</b> (SFT), and preference alignment. We train BIDER by learning from crafting KSE, while maximizing its output to align with <b>LLM&rsquo;s</b> information acquisition preferences through <b>reinforcement</b> <b>learning.</b> Evaluations across five datasets show BIDER boosts <b>LLMs&rsquo;</b> answer quality by 7% while reducing input content length in retrieval documents by 80%, outperforming existing methods. The proposed KSE <b>simulation</b> effectively equips <b>LLMs</b> with essential information for accurate <b>question</b> <b>answering.</b></p></p class="citation"></blockquote><h3 id=5109--5346-your-large-language-model-is-secretly-a-fairness-proponent-and-you-should-prompt-it-like-one-tianlin-li-et-al-2024>(5/109 | 5/346) Your Large Language Model is Secretly a Fairness Proponent and You Should Prompt it Like One (Tianlin Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tianlin Li, Xiaoyu Zhang, Chao Du, Tianyu Pang, Qian Liu, Qing Guo, Chao Shen, Yang Liu. (2024)<br><strong>Your Large Language Model is Secretly a Fairness Proponent and You Should Prompt it Like One</strong><br><button class=copy-to-clipboard title="Your Large Language Model is Secretly a Fairness Proponent and You Should Prompt it Like One" index=5>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-5 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: I-2; J-4, cs-AI, cs-CL, cs.CL<br>Keyword Score: 90<br>Keywords: Fairness, GPT, GPT-3, GPT-3.5, GPT-4, Mistral, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12150v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12150v1.pdf filename=2402.12150v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The widespread adoption of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> underscores the urgent need to ensure their <b>fairness.</b> However, <b>LLMs</b> frequently present dominant viewpoints while ignoring alternative perspectives from minority parties, resulting in potential biases. We hypothesize that these <b>fairness-violating</b> behaviors occur because <b>LLMs</b> express their viewpoints using a human personality that represents the majority of training data. In response to this, we validate that <b>prompting</b> <b>LLMs</b> with specific roles can allow <b>LLMs</b> to express diverse viewpoints. Building on this insight and observation, we develop FairThinking, a pipeline designed to automatically generate roles that enable <b>LLMs</b> to articulate diverse perspectives for fair expressions. To evaluate FairThinking, we create a dataset with a thousand items covering three <b>fairness-related</b> topics and conduct experiments on <b>GPT-3.5,</b> <b>GPT-4,</b> Llama2, and <b>Mistral</b> to demonstrate its superior performance.</p></p class="citation"></blockquote><h3 id=6109--6346-meta-ranking-less-capable-language-models-are-capable-for-single-response-judgement-zijun-liu-et-al-2024>(6/109 | 6/346) Meta Ranking: Less Capable Language Models are Capable for Single Response Judgement (Zijun Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zijun Liu, Boqun Kou, Peng Li, Ming Yan, Ji Zhang, Fei Huang, Yang Liu. (2024)<br><strong>Meta Ranking: Less Capable Language Models are Capable for Single Response Judgement</strong><br><button class=copy-to-clipboard title="Meta Ranking: Less Capable Language Models are Capable for Single Response Judgement" index=6>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-6 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 90<br>Keywords: Fine-tuning, Alpaca, GPT, GPT-4, GPT-4 turbo, LLaMA, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12146v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12146v1.pdf filename=2402.12146v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Although <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have demonstrated strong performance on a wide range of tasks, they still face reliability challenges such as hallucination. Previous studies reveal that highly capable <b>LLMs</b> like <b>GPT-4</b> <b>are</b> effective in judging the reliability of individual responses, while less capable ones are often tuned to evaluate the relative reliability of responses to the same query. To enable less capable <b>LLMs</b> to effectively judge the reliability of individual responses, we propose a novel method named $\textit{Meta}$ $\textit{Ranking}$ (MR). Unlike previous methods, which assess the response directly, we achieve the judgement by comparing the target query-response pair with reference query-response pairs. We found its remarkable effectiveness in error detection for <b>LLM</b> responses on <b>reasoning</b> tasks, where less capable <b>LLMs</b> could outperform strong baselines, even without <b>fine-tuning.</b> We further demonstrate that MR can be used to enhance the performance of <b>LLMs</b> in two practical applications: query routing and iterative training data filtering. The former achieves <b>GPT-4-turbo</b> <b>comparable</b> performance with less than half the token consumption, while the latter makes the instruction-tuned <b>LLaMA-7B</b> and Phi-2, a 2.7B model, significantly surpass <b>Alpaca-13B</b> over fewer training samples, underscoring the high potential of our proposed method.</p></p class="citation"></blockquote><h3 id=7109--7346-modularized-networks-for-few-shot-hateful-meme-detection-rui-cao-et-al-2024>(7/109 | 7/346) Modularized Networks for Few-shot Hateful Meme Detection (Rui Cao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rui Cao, Roy Ka-Wei Lee, Jing Jiang. (2024)<br><strong>Modularized Networks for Few-shot Hateful Meme Detection</strong><br><button class=copy-to-clipboard title="Modularized Networks for Few-shot Hateful Meme Detection" index=7>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-7 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-CV, cs.CL<br>Keyword Score: 90<br>Keywords: Few-shot, Few-shot Learning, Fine-tuning, Low-Resource, Reasoning, In-context Learning, In-context Learning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11845v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11845v1.pdf filename=2402.11845v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we address the challenge of detecting hateful memes in the <b>low-resource</b> setting where only a few labeled examples are available. Our approach leverages the compositionality of Low-rank adaptation (LoRA), a widely used parameter-efficient tuning technique. We commence by <b>fine-tuning</b> <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> with LoRA on selected tasks pertinent to hateful meme detection, thereby generating a suite of LoRA modules. These modules are capable of essential <b>reasoning</b> skills for hateful meme detection. We then use the few available annotated samples to train a module composer, which assigns weights to the LoRA modules based on their relevance. The model&rsquo;s learnable parameters are directly proportional to the number of LoRA modules. This modularized network, underpinned by <b>LLMs</b> and augmented with LoRA modules, exhibits enhanced generalization in the context of hateful meme detection. Our evaluation spans three datasets designed for hateful meme detection in a <b>few-shot</b> <b>learning</b> context. The proposed method demonstrates superior performance to traditional <b>in-context</b> <b>learning,</b> which is also more computationally intensive during inference.We then use the few available annotated samples to train a module composer, which assigns weights to the LoRA modules based on their relevance. The model&rsquo;s learnable parameters are directly proportional to the number of LoRA modules. This modularized network, underpinned by <b>LLMs</b> and augmented with LoRA modules, exhibits enhanced generalization in the context of hateful meme detection. Our evaluation spans three datasets designed for hateful meme detection in a <b>few-shot</b> <b>learning</b> context. The proposed method demonstrates superior performance to traditional <b>in-context</b> <b>learning,</b> which is also more computationally intensive during inference.</p></p class="citation"></blockquote><h3 id=8109--8346-end-to-end-multilingual-fact-checking-at-scale-vinay-setty-2024>(8/109 | 8/346) End-to-end multilingual fact-checking at scale (Vinay Setty, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Vinay Setty. (2024)<br><strong>End-to-end multilingual fact-checking at scale</strong><br><button class=copy-to-clipboard title="End-to-end multilingual fact-checking at scale" index=8>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-8 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 83<br>Keywords: Benchmarking, Fine-tuning, GPT, GPT-3, GPT-3.5, GPT-4, Mistral, Fact Verification, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12147v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12147v1.pdf filename=2402.12147v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this article, we describe how you can perform end-to-end <b>fact-checking</b> <b>in</b> over 100 languages using Factiverse AI models. We also show through an experimental <b>benchmark</b> that <b>fine-tuned</b> models tailored for <b>fact-checking</b> <b>tasks</b> outperform <b>Large</b> <b>Language</b> <b>Models</b> such as <b>GPT-4,</b> <b>GPT-3.5-Turbo,</b> and <b>Mistral-7b.</b></p></p class="citation"></blockquote><h3 id=9109--9346-creating-a-fine-grained-entity-type-taxonomy-using-llms-michael-gunn-et-al-2024>(9/109 | 9/346) Creating a Fine Grained Entity Type Taxonomy Using LLMs (Michael Gunn et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Michael Gunn, Dohyun Park, Nidhish Kamath. (2024)<br><strong>Creating a Fine Grained Entity Type Taxonomy Using LLMs</strong><br><button class=copy-to-clipboard title="Creating a Fine Grained Entity Type Taxonomy Using LLMs" index=9>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-9 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 80<br>Keywords: GPT, GPT-4, GPT-4 turbo, Event Argument Extraction, Information Retrieval, Relation Extraction, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12557v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12557v1.pdf filename=2402.12557v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this study, we investigate the potential of <b>GPT-4</b> <b>and</b> its advanced iteration, <b>GPT-4</b> <b>Turbo,</b> in autonomously developing a detailed entity type taxonomy. Our objective is to construct a comprehensive taxonomy, starting from a broad classification of entity types - including objects, time, locations, organizations, <b>events,</b> <b>actions,</b> <b>and</b> subjects - similar to existing manually curated taxonomies. This classification is then progressively refined through iterative <b>prompting</b> techniques, leveraging <b>GPT-4&rsquo;s</b> <b>internal</b> knowledge base. The result is an extensive taxonomy comprising over 5000 nuanced entity types, which demonstrates remarkable quality upon subjective evaluation. We employed a straightforward yet effective <b>prompting</b> strategy, enabling the taxonomy to be dynamically expanded. The practical applications of this detailed taxonomy are diverse and significant. It facilitates the creation of new, more intricate branches through pattern-based combinations and notably enhances <b>information</b> <b>extraction</b> tasks, such as <b>relation</b> <b>extraction</b> and <b>event</b> <b>argument</b> <b>extraction.</b> Our methodology not only introduces an innovative approach to taxonomy creation but also opens new avenues for applying such taxonomies in various computational linguistics and AI-related fields.</p></p class="citation"></blockquote><h3 id=10109--10346-do-large-language-models-understand-logic-or-just-mimick-context-junbing-yan-et-al-2024>(10/109 | 10/346) Do Large Language Models Understand Logic or Just Mimick Context? (Junbing Yan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Junbing Yan, Chengyu Wang, Jun Huang, Wei Zhang. (2024)<br><strong>Do Large Language Models Understand Logic or Just Mimick Context?</strong><br><button class=copy-to-clipboard title="Do Large Language Models Understand Logic or Just Mimick Context?" index=10>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-10 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 80<br>Keywords: Counter-factual, Few-shot, Reasoning, In-context Learning, In-context Learning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12091v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12091v1.pdf filename=2402.12091v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Over the past few years, the abilities of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have received extensive attention, which have performed exceptionally well in complicated scenarios such as logical <b>reasoning</b> and symbolic inference. A significant factor contributing to this progress is the benefit of <b>in-context</b> <b>learning</b> and <b>few-shot</b> <b>prompting.</b> However, the reasons behind the success of such models using contextual <b>reasoning</b> have not been fully explored. Do <b>LLMs</b> have understand logical rules to draw inferences, or do they ``guess&rsquo;&rsquo; the answers by learning a type of probabilistic mapping through context? This paper investigates the <b>reasoning</b> capabilities of <b>LLMs</b> on two logical <b>reasoning</b> datasets by using <b>counterfactual</b> methods to replace context text and modify logical concepts. Based on our analysis, it is found that <b>LLMs</b> do not truly understand logical rules; rather, <b>in-context</b> <b>learning</b> has simply enhanced the likelihood of these models arriving at the correct answers. If one alters certain words in the context text or changes the concepts of logical terms, the outputs of <b>LLMs</b> can be significantly disrupted, leading to counter-intuitive responses. This work provides critical insights into the limitations of <b>LLMs,</b> underscoring the need for more robust mechanisms to ensure reliable logical <b>reasoning</b> in <b>LLMs.</b></p></p class="citation"></blockquote><h3 id=11109--11346-where-it-really-matters-few-shot-environmental-conservation-media-monitoring-for-low-resource-languages-sameer-jain-et-al-2024>(11/109 | 11/346) Where It Really Matters: Few-Shot Environmental Conservation Media Monitoring for Low-Resource Languages (Sameer Jain et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sameer Jain, Sedrick Scott Keh, Shova Chettri, Karun Dewan, Pablo Izquierdo, Johanna Prussman, Pooja Shreshtha, Cesar Suarez, Zheyuan Ryan Shi, Lei Li, Fei Fang. (2024)<br><strong>Where It Really Matters: Few-Shot Environmental Conservation Media Monitoring for Low-Resource Languages</strong><br><button class=copy-to-clipboard title="Where It Really Matters: Few-Shot Environmental Conservation Media Monitoring for Low-Resource Languages" index=11>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-11 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-CY, cs.CL<br>Keyword Score: 80<br>Keywords: Few-shot, Fine-tuning, High-Resource, Low-Resource, In-context Learning, Large Language Model, Large Language Model, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11818v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11818v1.pdf filename=2402.11818v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Environmental conservation organizations routinely monitor news content on conservation in protected areas to maintain situational awareness of developments that can have an environmental impact. Existing automated media monitoring systems require <b>large</b> <b>amounts</b> <b>of</b> data labeled by domain experts, which is only feasible at scale for <b>high-resource</b> languages like English. However, such tools are most needed in the global south where news of interest is mainly in local <b>low-resource</b> languages, and far fewer experts are available to annotate datasets sustainably. In this paper, we propose NewsSerow, a method to automatically recognize environmental conservation content in <b>low-resource</b> languages. NewsSerow is a pipeline of <b>summarization,</b> <b>in-context</b> <b>few-shot</b> classification, and self-reflection using <b>large</b> <b>language</b> <b>models</b> <b>(LLMs).</b> Using at most 10 demonstration example news articles in Nepali, NewsSerow significantly outperforms other <b>few-shot</b> methods and achieves comparable performance with models fully <b>fine-tuned</b> using thousands of examples. The World Wide Fund for Nature (WWF) has deployed NewsSerow for media monitoring in Nepal, significantly reducing their operational burden, and ensuring that AI tools for conservation actually reach the communities that need them the most. NewsSerow has also been deployed for countries with other languages like Colombia.</p></p class="citation"></blockquote><h3 id=12109--12346-structured-chain-of-thought-prompting-for-few-shot-generation-of-content-grounded-qa-conversations-md-arafat-sultan-et-al-2024>(12/109 | 12/346) Structured Chain-of-Thought Prompting for Few-Shot Generation of Content-Grounded QA Conversations (Md Arafat Sultan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Md Arafat Sultan, Jatin Ganhotra, Ramón Fernandez Astudillo. (2024)<br><strong>Structured Chain-of-Thought Prompting for Few-Shot Generation of Content-Grounded QA Conversations</strong><br><button class=copy-to-clipboard title="Structured Chain-of-Thought Prompting for Few-Shot Generation of Content-Grounded QA Conversations" index=12>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-12 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 80<br>Keywords: Few-shot, Out-of-domain, Grounding, Question Answering, Chain-of-thought Prompt, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11770v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11770v2.pdf filename=2402.11770v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce a structured <b>chain-of-thought</b> <b>(SCoT)</b> <b>prompting</b> approach to generating content-grounded multi-turn question-answer conversations using a pre-trained <b>large</b> <b>language</b> <b>model</b> <b>(LLM).</b> At the core of our proposal is a structured breakdown of the complex task into a number of states in a state machine, so that actions corresponding to various subtasks, e.g., content reading and utterance generation, can be executed in their own dedicated states. Each state leverages a unique set of resources including <b>prompts</b> and (optionally) additional tools to augment the generation process. Our experimental results show that SCoT <b>prompting</b> with designated states for hallucination mitigation increases agent faithfulness to <b>grounding</b> documents by up to 16.8%. When used as training data, our open-domain conversations synthesized from only 6 Wikipedia-based seed demonstrations train strong conversational <b>QA</b> agents; in <b>out-of-domain</b> evaluation, for example, we observe improvements of up to 13.9% over target domain gold data when the latter is augmented with our generated examples.</p></p class="citation"></blockquote><h3 id=13109--13346-in-context-learning-demonstration-selection-via-influence-analysis-vinay-m-s-et-al-2024>(13/109 | 13/346) In-Context Learning Demonstration Selection via Influence Analysis (Vinay M. S. et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Vinay M. S., Minh-Hao Van, Xintao Wu. (2024)<br><strong>In-Context Learning Demonstration Selection via Influence Analysis</strong><br><button class=copy-to-clipboard title="In-Context Learning Demonstration Selection via Influence Analysis" index=13>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-13 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 80<br>Keywords: Few-shot, Few-shot Learning, Fine-tuning, In-context Learning, In-context Learning, In-context Learning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11750v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11750v1.pdf filename=2402.11750v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have demonstrated their <b>In-Context</b> <b>Learning</b> <b>(ICL)</b> capabilities which provides an opportunity to perform few shot learning without any gradient update. Despite its multiple benefits, <b>ICL</b> generalization performance is sensitive to the selected demonstrations. Selecting effective demonstrations for <b>ICL</b> is still an open research challenge. To address this challenge, we propose a demonstration selection method called InfICL which analyzes influences of training samples through influence functions. Identifying highly influential training samples can potentially aid in uplifting the <b>ICL</b> generalization performance. To limit the running cost of InfICL, we only employ the <b>LLM</b> to generate sample embeddings, and don&rsquo;t perform any costly fine tuning. We perform empirical study on multiple real-world datasets and show merits of our InfICL against state-of-the-art baselines.</p></p class="citation"></blockquote><h3 id=14109--14346-rjua-meddqa-a-multimodal-benchmark-for-medical-document-question-answering-and-clinical-reasoning-congyun-jin-et-al-2024>(14/109 | 14/346) RJUA-MedDQA: A Multimodal Benchmark for Medical Document Question Answering and Clinical Reasoning (Congyun Jin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Congyun Jin, Ming Zhang, Xiaowei Ma, Li Yujiao, Yingbo Wang, Yabo Jia, Yuliang Du, Tao Sun, Haowen Wang, Cong Fan, Jinjie Gu, Chenfei Chi, Xiangguo Lv, Fangzhou Li, Wei Xue, Yiran Huang. (2024)<br><strong>RJUA-MedDQA: A Multimodal Benchmark for Medical Document Question Answering and Clinical Reasoning</strong><br><button class=copy-to-clipboard title="RJUA-MedDQA: A Multimodal Benchmark for Medical Document Question Answering and Clinical Reasoning" index=14>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-14 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL, stat-AP<br>Keyword Score: 79<br>Keywords: Benchmarking, Few-shot, Multi-modal, Multi-modal, Image2text, Question Answering, Question Answering, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14840v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14840v1.pdf filename=2402.14840v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advancements in <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> and <b>Large</b> <b>Multi-modal</b> <b>Models</b> (LMMs) have shown potential in various medical applications, such as Intelligent Medical Diagnosis. Although impressive results have been achieved, we find that existing <b>benchmarks</b> do not reflect the complexity of real medical reports and specialized in-depth <b>reasoning</b> capabilities. In this work, we introduced RJUA-MedDQA, a comprehensive <b>benchmark</b> in the field of medical specialization, which poses several challenges: comprehensively interpreting imgage content across diverse challenging layouts, possessing numerical <b>reasoning</b> ability to identify abnormal indicators and demonstrating clinical <b>reasoning</b> ability to provide statements of disease diagnosis, status and advice based on medical contexts. We carefully design the data generation pipeline and proposed the Efficient Structural Restoration Annotation (ESRA) Method, aimed at restoring textual and tabular content in medical report images. This method substantially enhances annotation efficiency, doubling the productivity of each annotator, and yields a 26.8% improvement in accuracy. We conduct extensive evaluations, including <b>few-shot</b> assessments of 5 LMMs which are capable of solving Chinese medical <b>QA</b> tasks. To further investigate the limitations and potential of current LMMs, we conduct comparative experiments on a set of strong <b>LLMs</b> by using <b>image-text</b> generated by ESRA method. We report the performance of baselines and offer several observations: (1) The overall performance of existing LMMs is still limited; however LMMs more robust to low-quality and diverse-structured images compared to <b>LLMs.</b> (3) <b>Reasoning</b> across context and image content present significant challenges. We hope this <b>benchmark</b> helps the community make progress on these challenging tasks in <b>multi-modal</b> medical document understanding and facilitate its application in healthcare.</p></p class="citation"></blockquote><h3 id=15109--15346-arks-active-retrieval-in-knowledge-soup-for-code-generation-hongjin-su-et-al-2024>(15/109 | 15/346) ARKS: Active Retrieval in Knowledge Soup for Code Generation (Hongjin Su et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hongjin Su, Shuyang Jiang, Yuhang Lai, Haoyuan Wu, Boao Shi, Che Liu, Qian Liu, Tao Yu. (2024)<br><strong>ARKS: Active Retrieval in Knowledge Soup for Code Generation</strong><br><button class=copy-to-clipboard title="ARKS: Active Retrieval in Knowledge Soup for Code Generation" index=15>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-15 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 73<br>Keywords: Benchmarking, Retrieval-Augmented Generation, Retrieval-Augmented Generation, Retrieval-Augmented Generation, ChatGPT, Code Generation, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12317v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12317v1.pdf filename=2402.12317v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recently the <b>retrieval-augmented</b> <b>generation</b> <b>(RAG)</b> paradigm has raised much attention for its potential in incorporating external knowledge into <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> without further training. While widely explored in natural language applications, its utilization in <b>code</b> <b>generation</b> remains under-explored. In this paper, we introduce Active <b>Retrieval</b> <b>in</b> <b>Knowledge</b> Soup (ARKS), an advanced strategy for generalizing <b>large</b> <b>language</b> <b>models</b> for <b>code.</b> <b>In</b> contrast to relying on a single source, we construct a knowledge soup integrating web search, documentation, execution feedback, and evolved <b>code</b> <b>snippets.</b> We employ an active <b>retrieval</b> <b>strategy</b> <b>that</b> iteratively refines the query and updates the knowledge soup. To assess the performance of ARKS, we compile a new <b>benchmark</b> comprising realistic coding problems associated with frequently updated libraries and long-tail programming languages. Experimental results on <b>ChatGPT</b> and CodeLlama demonstrate a substantial improvement in the average execution accuracy of ARKS on <b>LLMs.</b> The analysis confirms the effectiveness of our proposed knowledge soup and active <b>retrieval</b> <b>strategies,</b> <b>offering</b> rich insights into the construction of effective <b>retrieval-augmented</b> <b>code</b> <b>generation</b> (RACG) pipelines. Our model, <b>code,</b> <b>and</b> data are available at <a href=https://arks-codegen.github.io>https://arks-codegen.github.io</a>.</p></p class="citation"></blockquote><h3 id=16109--16346-mrke-the-multi-hop-reasoning-evaluation-of-llms-by-knowledge-edition-jian-wu-et-al-2024>(16/109 | 16/346) MRKE: The Multi-hop Reasoning Evaluation of LLMs by Knowledge Edition (Jian Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jian Wu, Linyi Yang, Manabu Okumura, Yue Zhang. (2024)<br><strong>MRKE: The Multi-hop Reasoning Evaluation of LLMs by Knowledge Edition</strong><br><button class=copy-to-clipboard title="MRKE: The Multi-hop Reasoning Evaluation of LLMs by Knowledge Edition" index=16>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-16 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 73<br>Keywords: Benchmarking, GPT, GPT-4, Question Answering, Question Answering, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11924v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11924v1.pdf filename=2402.11924v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Although <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have shown strong performance in Multi-hop <b>Question</b> <b>Answering</b> (MHQA) tasks, their real <b>reasoning</b> ability remains exploration. Current <b>LLM</b> <b>QA</b> evaluation <b>benchmarks</b> have shown limitations, including 1) data contamination, the evaluation data are potentially exposed to <b>LLMs</b> during the pretraining stage; and 2) ignoration of the <b>reasoning</b> chain evaluation. Thus we introduce an <b>LLM</b> MHQA evaluation <b>benchmark,</b> the first <b>QA</b> <b>benchmark</b> based on the new, unprecedented knowledge by editing the off-the-shelf HotpotQA dataset; Besides, we also annotate and evaluate the <b>reasoning</b> chain in the form of sub-questions and intermediate answers corresponding to the multi-hop <b>questions.</b> <b>Specifically,</b> based on the observation, 1) <b>LLMs</b> show a performance gap between the original HotpotQA and our edited data, deeming that current MHQA <b>benchmarks</b> have the potential risk of data contamination that hard to evaluate <b>LLMs&rsquo;</b> performance objectively and scientifically; 2) <b>LLMs</b> only get a small percentage of the right <b>reasoning</b> chain, e.g. <b>GPT-4</b> only gets 36.3% right <b>reasoning</b> chain. We believe this new Multi-hop <b>QA</b> evaluation <b>benchmark</b> and novel evaluation methods will facilitate the development of trustworthy <b>LLM</b> evaluation on the MHQA task.</p></p class="citation"></blockquote><h3 id=17109--17346-parallel-structures-in-pre-training-data-yield-in-context-learning-yanda-chen-et-al-2024>(17/109 | 17/346) Parallel Structures in Pre-training Data Yield In-Context Learning (Yanda Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yanda Chen, Chen Zhao, Zhou Yu, Kathleen McKeown, He He. (2024)<br><strong>Parallel Structures in Pre-training Data Yield In-Context Learning</strong><br><button class=copy-to-clipboard title="Parallel Structures in Pre-training Data Yield In-Context Learning" index=17>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-17 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 70<br>Keywords: Distribution Shift, Distribution Shift, N-gram, In-context Learning, In-context Learning, In-context Learning, Pre-trained Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12530v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12530v1.pdf filename=2402.12530v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Pre-trained</b> <b>language</b> <b>models</b> (LMs) are capable of <b>in-context</b> <b>learning</b> <b>(ICL):</b> they can adapt to a task with only a few examples given in the <b>prompt</b> without any parameter update. However, it is unclear where this capability comes from as there is a stark <b>distribution</b> <b>shift</b> between pre-training text and <b>ICL</b> <b>prompts.</b> In this work, we study what patterns of the pre-training data contribute to <b>ICL.</b> We find that LMs&rsquo; <b>ICL</b> ability depends on $\textit{parallel structures}$ in the pre-training data &ndash; pairs of phrases following similar templates in the same context window. Specifically, we detect parallel structures by checking whether training on one phrase improves prediction of the other, and conduct ablation experiments to study their effect on <b>ICL.</b> We show that removing parallel structures in the pre-training data reduces LMs&rsquo; <b>ICL</b> accuracy by 51% (vs 2% from random ablation). This drop persists even when excluding common patterns such as <b>n-gram</b> repetitions and long-range dependency, showing the diversity and generality of parallel structures. A closer look at the detected parallel structures indicates that they cover diverse linguistic tasks and span long distances in the data.</p></p class="citation"></blockquote><h3 id=18109--18346-reformatted-alignment-run-ze-fan-et-al-2024>(18/109 | 18/346) Reformatted Alignment (Run-Ze Fan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Run-Ze Fan, Xuefeng Li, Haoyang Zou, Junlong Li, Shwai He, Ethan Chern, Jiewen Hu, Pengfei Liu. (2024)<br><strong>Reformatted Alignment</strong><br><button class=copy-to-clipboard title="Reformatted Alignment" index=18>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-18 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 70<br>Keywords: Fine-tuning, Alpaca, LLaMA, Mathematical Reasoning, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12219v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12219v1.pdf filename=2402.12219v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The quality of <b>finetuning</b> data is crucial for aligning <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> with human values. Current methods to improve data quality are either labor-intensive or prone to factual errors caused by <b>LLM</b> hallucinations. This paper explores elevating the quality of existing instruction data to better align with human values, introducing a simple and effective approach named ReAlign, which reformats the responses of instruction data into a format that better aligns with pre-established criteria and the collated evidence. This approach minimizes human annotation, hallucination, and the difficulty in scaling, remaining orthogonal to existing alignment techniques. Experimentally, ReAlign significantly boosts the general alignment ability, math <b>reasoning,</b> factuality, and readability of the <b>LLMs.</b> Encouragingly, without introducing any additional data or advanced training techniques, and merely by reformatting the response, <b>LLaMA-2-13B&rsquo;s</b> <b>mathematical</b> <b>reasoning</b> ability on GSM8K can be improved from 46.77% to 56.63% in accuracy. Additionally, a mere 5% of ReAlign data yields a 67% boost in general alignment ability measured by the <b>Alpaca</b> dataset. This work highlights the need for further research into the science and mechanistic interpretability of <b>LLMs.</b> We have made the associated code and data publicly accessible to support future studies at <a href=https://github.com/GAIR-NLP/ReAlign>https://github.com/GAIR-NLP/ReAlign</a>.</p></p class="citation"></blockquote><h3 id=19109--19346-distilling-large-language-models-for-text-attributed-graph-learning-bo-pan-et-al-2024>(19/109 | 19/346) Distilling Large Language Models for Text-Attributed Graph Learning (Bo Pan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bo Pan, Zheng Zhang, Yifei Zhang, Yuntong Hu, Liang Zhao. (2024)<br><strong>Distilling Large Language Models for Text-Attributed Graph Learning</strong><br><button class=copy-to-clipboard title="Distilling Large Language Models for Text-Attributed Graph Learning" index=19>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-19 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keyword Score: 63<br>Keywords: Graph, Few-shot, Knowledge Distillation, Zero-shot, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12022v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12022v1.pdf filename=2402.12022v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Text-Attributed <b>Graphs</b> (TAGs) are <b>graphs</b> of connected textual documents. <b>Graph</b> models can efficiently learn TAGs, but their training heavily relies on human-annotated labels, which are scarce or even unavailable in many applications. <b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have recently demonstrated remarkable capabilities in <b>few-shot</b> and <b>zero-shot</b> TAG learning, but they suffer from scalability, cost, and privacy issues. Therefore, in this work, we focus on synergizing <b>LLMs</b> and <b>graph</b> models with their complementary strengths by <b>distilling</b> the power of <b>LLMs</b> to a local <b>graph</b> model on TAG learning. To address the inherent gaps between <b>LLMs</b> (generative models for texts) and <b>graph</b> models (discriminative models for <b>graphs),</b> we propose first to let <b>LLMs</b> teach an interpreter with rich textual rationale and then let a student model mimic the interpreter&rsquo;s <b>reasoning</b> without <b>LLMs&rsquo;</b> textual rationale. Extensive experiments validate the efficacy of our proposed framework.</p></p class="citation"></blockquote><h3 id=20109--20346-sibo-a-simple-booster-for-parameter-efficient-fine-tuning-zhihao-wen-et-al-2024>(20/109 | 20/346) SIBO: A Simple Booster for Parameter-Efficient Fine-Tuning (Zhihao Wen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhihao Wen, Jie Zhang, Yuan Fang. (2024)<br><strong>SIBO: A Simple Booster for Parameter-Efficient Fine-Tuning</strong><br><button class=copy-to-clipboard title="SIBO: A Simple Booster for Parameter-Efficient Fine-Tuning" index=20>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-20 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 63<br>Keywords: Benchmarking, Fine-tuning, Transformer, Common-sense Reasoning, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11896v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11896v1.pdf filename=2402.11896v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Fine-tuning</b> all parameters of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> necessitates substantial computational power and extended time. Latest advancements in parameter-efficient <b>fine-tuning</b> (PEFT) techniques, such as Adapter tuning and LoRA, allow for adjustments to only a minor fraction of the parameters of these <b>LLMs.</b> Concurrently, it has been noted that the issue of over-smoothing diminishes the effectiveness of these <b>Transformer-based</b> <b>LLMs,</b> resulting in suboptimal performances in downstream tasks. In this paper, we present SIBO, which is a SImple BOoster to enhance PEFT, by injecting an initial residual. SIBO is straight-forward and readily extensible to a range of state-of-the-art PEFT techniques to alleviate over-smoothing and enhance performance. Extensive experiments on 22 <b>benchmark</b> datasets demonstrate that SIBO significantly enhances the performance of various strong baselines, achieving up to 15.7% and 23.5% improvement over existing PEFT methods on the arithmetic and <b>commonsense</b> <b>reasoning</b> tasks, respectively.</p></p class="citation"></blockquote><h3 id=21109--21346-standardize-aligning-language-models-with-expert-defined-standards-for-content-generation-joseph-marvin-imperial-et-al-2024>(21/109 | 21/346) Standardize: Aligning Language Models with Expert-Defined Standards for Content Generation (Joseph Marvin Imperial et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Joseph Marvin Imperial, Gail Forey, Harish Tayyar Madabushi. (2024)<br><strong>Standardize: Aligning Language Models with Expert-Defined Standards for Content Generation</strong><br><button class=copy-to-clipboard title="Standardize: Aligning Language Models with Expert-Defined Standards for Content Generation" index=21>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-21 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 60<br>Keywords: GPT, GPT-4, Text Generation, In-context Learning, In-context Learning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12593v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12593v1.pdf filename=2402.12593v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Domain experts across engineering, healthcare, and education follow strict standards for producing quality content such as technical manuals, medication instructions, and children&rsquo;s reading materials. However, current works in controllable <b>text</b> <b>generation</b> have yet to explore using these standards as references for control. Towards this end, we introduce Standardize, a retrieval-style <b>in-context</b> <b>learning-based</b> framework to guide <b>large</b> <b>language</b> <b>models</b> to align with expert-defined standards. Focusing on English language standards in the education domain as a use case, we consider the Common European Framework of Reference for Languages (CEFR) and Common Core Standards (CCS) for the task of open-ended content generation. Our findings show that models can gain 40% to 100% increase in precise accuracy for Llama2 and <b>GPT-4,</b> respectively, demonstrating that the use of knowledge artifacts extracted from standards and integrating them in the generation process can effectively guide models to produce better standard-aligned content.</p></p class="citation"></blockquote><h3 id=22109--22346-task-oriented-dialogue-with-in-context-learning-tom-bocklisch-et-al-2024>(22/109 | 22/346) Task-Oriented Dialogue with In-Context Learning (Tom Bocklisch et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tom Bocklisch, Thomas Werkmeister, Daksh Varshneya, Alan Nichol. (2024)<br><strong>Task-Oriented Dialogue with In-Context Learning</strong><br><button class=copy-to-clipboard title="Task-Oriented Dialogue with In-Context Learning" index=22>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-22 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 60<br>Keywords: Chatbot, Dialogue System, In-context Learning, In-context Learning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12234v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12234v1.pdf filename=2402.12234v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We describe a system for building task-oriented <b>dialogue</b> <b>systems</b> combining the <b>in-context</b> <b>learning</b> abilities of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> with the deterministic execution of business logic. <b>LLMs</b> are used to translate between the surface form of the conversation and a domain-specific language (DSL) which is used to progress the business logic. We compare our approach to the intent-based NLU approach predominantly used in industry today. Our experiments show that developing <b>chatbots</b> with our system requires significantly less effort than established approaches, that these <b>chatbots</b> can successfully navigate complex <b>dialogues</b> <b>which</b> are extremely challenging for NLU-based systems, and that our system has desirable properties for scaling task-oriented <b>dialogue</b> <b>systems</b> to a <b>large</b> <b>number</b> <b>of</b> tasks. We make our implementation available for use and further study.</p></p class="citation"></blockquote><h3 id=23109--23346-analysis-of-multidomain-abstractive-summarization-using-salience-allocation-tohida-rehman-et-al-2024>(23/109 | 23/346) Analysis of Multidomain Abstractive Summarization Using Salience Allocation (Tohida Rehman et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tohida Rehman, Raghubir Bose, Soumik Dey, Samiran Chattopadhyay. (2024)<br><strong>Analysis of Multidomain Abstractive Summarization Using Salience Allocation</strong><br><button class=copy-to-clipboard title="Analysis of Multidomain Abstractive Summarization Using Salience Allocation" index=23>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-23 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 60<br>Keywords: Fine-tuning, BART, Text Summarization, BERTScore, Rouge, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11955v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11955v1.pdf filename=2402.11955v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper explores the realm of abstractive <b>text</b> <b>summarization</b> through the lens of the SEASON (Salience Allocation as Guidance for Abstractive <b>SummarizatiON)</b> technique, a model designed to enhance <b>summarization</b> by leveraging salience allocation techniques. The study evaluates SEASON&rsquo;s efficacy by comparing it with prominent models like <b>BART,</b> PEGASUS, and ProphetNet, all <b>fine-tuned</b> for various <b>text</b> <b>summarization</b> tasks. The assessment is conducted using diverse datasets including CNN/Dailymail, SAMSum, and Financial-news based Event-Driven Trading (EDT), with a specific focus on a financial dataset containing a substantial volume of news articles from 2020/03/01 to 2021/05/06. This paper employs various evaluation metrics such as <b>ROUGE,</b> METEOR, <b>BERTScore,</b> and MoverScore to evaluate the performance of these models <b>fine-tuned</b> for generating abstractive summaries. The analysis of these metrics offers a thorough insight into the strengths and weaknesses demonstrated by each model in summarizing news dataset, dialogue dataset and financial <b>text</b> <b>dataset.</b> The results presented in this paper not only contribute to the evaluation of the SEASON model&rsquo;s effectiveness but also illuminate the intricacies of salience allocation techniques across various types of datasets.</p></p class="citation"></blockquote><h3 id=24109--24346-investigating-multi-hop-factual-shortcuts-in-knowledge-editing-of-large-language-models-tianjie-ju-et-al-2024>(24/109 | 24/346) Investigating Multi-Hop Factual Shortcuts in Knowledge Editing of Large Language Models (Tianjie Ju et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tianjie Ju, Yijin Chen, Xinwei Yuan, Zhuosheng Zhang, Wei Du, Yubin Zheng, Gongshen Liu. (2024)<br><strong>Investigating Multi-Hop Factual Shortcuts in Knowledge Editing of Large Language Models</strong><br><button class=copy-to-clipboard title="Investigating Multi-Hop Factual Shortcuts in Knowledge Editing of Large Language Models" index=24>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-24 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 60<br>Keywords: Few-shot, Reasoning, Chain-of-thought Prompt, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11900v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11900v1.pdf filename=2402.11900v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent work has showcased the powerful capability of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> in recalling knowledge and <b>reasoning.</b> However, the reliability of <b>LLMs</b> in combining these two capabilities into <b>reasoning</b> through multi-hop facts has not been widely explored. This paper systematically investigates the possibilities for <b>LLMs</b> to utilize shortcuts based on direct connections between the initial and terminal entities of multi-hop knowledge. We first explore the existence of factual shortcuts through Knowledge Neurons, revealing that: (i) the strength of factual shortcuts is highly correlated with the frequency of co-occurrence of initial and terminal entities in the pre-training corpora; (ii) <b>few-shot</b> <b>prompting</b> leverage more shortcuts in answering multi-hop questions compared to <b>chain-of-thought</b> <b>prompting.</b> Then, we analyze the risks posed by factual shortcuts from the perspective of multi-hop knowledge editing. Analysis shows that approximately 20% of the failures are attributed to shortcuts, and the initial and terminal entities in these failure instances usually have higher co-occurrences in the pre-training corpus. Finally, we propose erasing shortcut neurons to mitigate the associated risks and find that this approach significantly reduces failures in multiple-hop knowledge editing caused by shortcuts.</p></p class="citation"></blockquote><h3 id=25109--25346-chatgpt-based-data-augmentation-for-improved-parameter-efficient-debiasing-of-llms-pengrui-han-et-al-2024>(25/109 | 25/346) ChatGPT Based Data Augmentation for Improved Parameter-Efficient Debiasing of LLMs (Pengrui Han et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Pengrui Han, Rafal Kocielnik, Adhithya Saravanan, Roy Jiang, Or Sharir, Anima Anandkumar. (2024)<br><strong>ChatGPT Based Data Augmentation for Improved Parameter-Efficient Debiasing of LLMs</strong><br><button class=copy-to-clipboard title="ChatGPT Based Data Augmentation for Improved Parameter-Efficient Debiasing of LLMs" index=25>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-25 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: 68T50, I-2-7; K-4-1, cs-AI, cs-CL, cs-CY, cs.CL<br>Keyword Score: 60<br>Keywords: Data Augmentation, Fairness, ChatGPT, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11764v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11764v1.pdf filename=2402.11764v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>models</b> <b>(LLMs),</b> while powerful, exhibit harmful social biases. Debiasing is often challenging due to computational costs, <b>data</b> <b>constraints,</b> and potential degradation of multi-task language capabilities. This work introduces a novel approach utilizing <b>ChatGPT</b> to generate synthetic training <b>data,</b> <b>aiming</b> to enhance the debiasing of <b>LLMs.</b> We propose two strategies: Targeted <b>Prompting,</b> which provides effective debiasing for known biases but necessitates prior specification of bias in question; and General <b>Prompting,</b> which, while slightly less effective, offers debiasing across various categories. We leverage resource-efficient <b>LLM</b> debiasing using adapter tuning and compare the effectiveness of our synthetic <b>data</b> <b>to</b> existing debiasing datasets. Our results reveal that: (1) <b>ChatGPT</b> can efficiently produce high-quality training <b>data</b> <b>for</b> debiasing other <b>LLMs;</b> (2) <b>data</b> <b>produced</b> via our approach surpasses existing datasets in debiasing performance while also preserving internal knowledge of a pre-trained <b>LLM;</b> and (3) synthetic <b>data</b> <b>exhibits</b> generalizability across categories, effectively mitigating various biases, including intersectional ones. These findings underscore the potential of synthetic <b>data</b> <b>in</b> advancing the <b>fairness</b> of <b>LLMs</b> with minimal retraining cost.</p></p class="citation"></blockquote><h3 id=26109--26346-speech-translation-with-speech-foundation-models-and-large-language-models-what-is-there-and-what-is-missing-marco-gaido-et-al-2024>(26/109 | 26/346) Speech Translation with Speech Foundation Models and Large Language Models: What is There and What is Missing? (Marco Gaido et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Marco Gaido, Sara Papi, Matteo Negri, Luisa Bentivogli. (2024)<br><strong>Speech Translation with Speech Foundation Models and Large Language Models: What is There and What is Missing?</strong><br><button class=copy-to-clipboard title="Speech Translation with Speech Foundation Models and Large Language Models: What is There and What is Missing?" index=26>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-26 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 56<br>Keywords: Foundation Model, Multi-modal, Multi-modal, Recommendation, Speech-to-Speech Translation, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12025v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12025v1.pdf filename=2402.12025v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The field of natural language processing (NLP) has recently witnessed a transformative shift with the emergence of <b>foundation</b> <b>models,</b> particularly <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> that have revolutionized text-based NLP. This paradigm has extended to other modalities, including <b>speech,</b> <b>where</b> researchers are actively exploring the combination of <b>Speech</b> <b>Foundation</b> <b>Models</b> (SFMs) and <b>LLMs</b> into single, unified models capable of addressing <b>multimodal</b> tasks. Among such tasks, this paper focuses on <b>speech-to-text</b> <b>translation</b> (ST). By examining the published papers on the topic, we propose a unified view of the architectural solutions and training strategies presented so far, highlighting similarities and differences among them. Based on this examination, we not only organize the lessons learned but also show how diverse settings and evaluation approaches hinder the identification of the best-performing solution for each architectural building block and training choice. Lastly, we outline <b>recommendations</b> for future works on the topic aimed at better understanding the strengths and weaknesses of the SFM+LLM solutions for ST.</p></p class="citation"></blockquote><h3 id=27109--27346-hu-at-semeval-2024-task-8a-can-contrastive-learning-learn-embeddings-to-detect-machine-generated-text-shubhashis-roy-dipta-et-al-2024>(27/109 | 27/346) HU at SemEval-2024 Task 8A: Can Contrastive Learning Learn Embeddings to Detect Machine-Generated Text? (Shubhashis Roy Dipta et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shubhashis Roy Dipta, Sadat Shahriar. (2024)<br><strong>HU at SemEval-2024 Task 8A: Can Contrastive Learning Learn Embeddings to Detect Machine-Generated Text?</strong><br><button class=copy-to-clipboard title="HU at SemEval-2024 Task 8A: Can Contrastive Learning Learn Embeddings to Detect Machine-Generated Text?" index=27>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-27 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 55<br>Keywords: Black Box, Contrastive Learning, Data Augmentation, Text Generation, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11815v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11815v1.pdf filename=2402.11815v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper describes our system developed for SemEval-2024 Task 8, &ldquo;Multigenerator, Multidomain, and Multilingual <b>Black-Box</b> <b>Machine-Generated</b> <b>Text</b> <b>Detection."</b> Machine-generated <b>texts</b> <b>have</b> been one of the main concerns due to the use of <b>large</b> <b>language</b> <b>models</b> <b>(LLM)</b> in fake <b>text</b> <b>generation,</b> phishing, cheating in exams, or even plagiarizing copyright materials. A lot of systems have been developed to detect machine-generated <b>text.</b> <b>Nonetheless,</b> the majority of these systems rely on the <b>text-generating</b> <b>model,</b> a limitation that is impractical in real-world scenarios, as it&rsquo;s often impossible to know which specific model the user has used for <b>text</b> <b>generation.</b> In this work, we propose a single model based on <b>contrastive</b> <b>learning,</b> which uses ~40% of the baseline&rsquo;s parameters (149M vs. 355M) but shows a comparable performance on the test dataset (21st out of 137 participants). Our key finding is that even without an ensemble of multiple models, a single base model can have comparable performance with the help of <b>data</b> <b>augmentation</b> and <b>contrastive</b> <b>learning.</b></p></p class="citation"></blockquote><h3 id=28109--28346-a-synthetic-data-approach-for-domain-generalization-of-nli-models-mohammad-javad-hosseini-et-al-2024>(28/109 | 28/346) A synthetic data approach for domain generalization of NLI models (Mohammad Javad Hosseini et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mohammad Javad Hosseini, Andrey Petrov, Alex Fabrikant, Annie Louis. (2024)<br><strong>A synthetic data approach for domain generalization of NLI models</strong><br><button class=copy-to-clipboard title="A synthetic data approach for domain generalization of NLI models" index=28>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-28 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 53<br>Keywords: Benchmarking, Transfer Learning, T5, Natural Language Inference, Natural Language Inference, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12368v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12368v1.pdf filename=2402.12368v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Natural</b> <b>Language</b> <b>Inference</b> <b>(NLI)</b> remains an important <b>benchmark</b> task for <b>LLMs.</b> <b>NLI</b> datasets are a springboard for <b>transfer</b> <b>learning</b> to other semantic tasks, and <b>NLI</b> models are standard tools for identifying the faithfulness of model-generated text. There are several large scale <b>NLI</b> datasets today, and models have improved greatly by hill-climbing on these collections. Yet their realistic performance on out-of-distribution/domain data is less well-understood. We present an in-depth exploration of the problem of domain generalization of <b>NLI</b> models. We demonstrate a new approach for generating synthetic <b>NLI</b> data in diverse domains and lengths, so far not covered by existing training sets. The resulting examples have meaningful premises, the hypotheses are formed in creative ways rather than simple edits to a few premise tokens, and the labels have high accuracy. We show that models trained on this data ($685$K synthetic examples) have the best generalization to completely new downstream test settings. On the TRUE <b>benchmark,</b> a <b>T5-small</b> model trained with our data improves around $7%$ on average compared to training on the best alternative dataset. The improvements are more pronounced for smaller models, while still meaningful on a <b>T5</b> XXL model. We also demonstrate gains on test sets when in-domain training data is augmented with our domain-general synthetic data.</p></p class="citation"></blockquote><h3 id=29109--29346-neo-bench-evaluating-robustness-of-large-language-models-with-neologisms-jonathan-zheng-et-al-2024>(29/109 | 29/346) NEO-BENCH: Evaluating Robustness of Large Language Models with Neologisms (Jonathan Zheng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jonathan Zheng, Alan Ritter, Wei Xu. (2024)<br><strong>NEO-BENCH: Evaluating Robustness of Large Language Models with Neologisms</strong><br><button class=copy-to-clipboard title="NEO-BENCH: Evaluating Robustness of Large Language Models with Neologisms" index=29>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-29 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 53<br>Keywords: Benchmarking, Natural Language Understanding, Neural Machine Translation, Large Language Model, Large Language Model, Perplexity<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12261v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12261v1.pdf filename=2402.12261v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The performance of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> degrades from the temporal drift between data used for model training and newer text seen during inference. One understudied avenue of language change causing data drift is the emergence of neologisms &ndash; new word forms &ndash; over time. We create a diverse resource of recent English neologisms by using several popular collection methods. We analyze temporal drift using neologisms by comparing sentences containing new words with near-identical sentences that replace neologisms with existing substitute words. Model performance is nearly halved in <b>machine</b> <b>translation</b> when a single neologism is introduced in a sentence. Motivated by these results, we construct a <b>benchmark</b> to evaluate <b>LLMs&rsquo;</b> ability to generalize to neologisms with various <b>natural</b> <b>language</b> <b>understanding</b> tasks and model <b>perplexity.</b> Models with later knowledge cutoff dates yield lower perplexities and perform better in downstream tasks. <b>LLMs</b> are also affected differently based on the linguistic origins of words, indicating that neologisms are complex for static <b>LLMs</b> to address. We will release our <b>benchmark</b> and code for reproducing our experiments.</p></p class="citation"></blockquote><h3 id=30109--30346-model-tailor-mitigating-catastrophic-forgetting-in-multi-modal-large-language-models-didi-zhu-et-al-2024>(30/109 | 30/346) Model Tailor: Mitigating Catastrophic Forgetting in Multi-modal Large Language Models (Didi Zhu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Didi Zhu, Zhongyi Sun, Zexi Li, Tao Shen, Ke Yan, Shouhong Ding, Kun Kuang, Chao Wu. (2024)<br><strong>Model Tailor: Mitigating Catastrophic Forgetting in Multi-modal Large Language Models</strong><br><button class=copy-to-clipboard title="Model Tailor: Mitigating Catastrophic Forgetting in Multi-modal Large Language Models" index=30>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-30 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 53<br>Keywords: Fine-tuning, Fine-tuning, Multi-modal, Question Answering, Visual Question Answering, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12048v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12048v1.pdf filename=2402.12048v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Catastrophic forgetting emerges as a critical challenge when <b>fine-tuning</b> <b>multi-modal</b> <b>large</b> <b>language</b> <b>models</b> (MLLMs), where improving performance on unseen tasks often leads to a significant performance drop on the original tasks. This paper presents a comprehensive analysis of catastrophic forgetting in MLLMs and introduces a post-training adjustment method called Model Tailor. Our method primarily preserves the pre-trained parameters while replacing a small number ($\leq$ 10%) of <b>fine-tuned</b> parameters, maintaining $\sim$ 99% effectiveness on original tasks versus pre-training, and achieving $\sim$ 97% on new tasks compared to standard <b>fine-tuning.</b> Specifically, we derive a sparse mask to identify the &ldquo;model patch&rdquo;, based on a fusion strategy that integrates salience and sensitivity analysis. Subsequently, a compensation mechanism is introduced to &ldquo;decorate the patch&rdquo;, enhancing the model&rsquo;s performance on both target and original tasks. Additionally, our method is adaptable to multi-task scenarios. Through extensive experiments on InstructBLIP and LLaVA-1.5 in both image captioning and <b>visual</b> <b>question</b> <b>answering</b> tasks, our approach demonstrates significant task adaptability while preserving inherent pre-trained capabilities.</p></p class="citation"></blockquote><h3 id=31109--31346-trustscore-reference-free-evaluation-of-llm-response-trustworthiness-danna-zheng-et-al-2024>(31/109 | 31/346) TrustScore: Reference-Free Evaluation of LLM Response Trustworthiness (Danna Zheng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Danna Zheng, Danyang Liu, Mirella Lapata, Jeff Z. Pan. (2024)<br><strong>TrustScore: Reference-Free Evaluation of LLM Response Trustworthiness</strong><br><button class=copy-to-clipboard title="TrustScore: Reference-Free Evaluation of LLM Response Trustworthiness" index=31>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-31 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: Fact Verification, Question Answering, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12545v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12545v1.pdf filename=2402.12545v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have demonstrated impressive capabilities across various domains, <b>prompting</b> a surge in their practical applications. However, concerns have arisen regarding the trustworthiness of <b>LLMs</b> outputs, particularly in closed-book <b>question-answering</b> <b>tasks,</b> where non-experts may struggle to identify inaccuracies due to the absence of contextual or ground truth information. This paper introduces TrustScore, a framework based on the concept of Behavioral Consistency, which evaluates whether an <b>LLMs</b> response aligns with its intrinsic knowledge. Additionally, TrustScore can seamlessly integrate with <b>fact-checking</b> <b>methods,</b> which assesses alignment with external knowledge sources. The experimental results show that TrustScore achieves strong correlations with human judgments, surpassing existing reference-free metrics, and achieving results on par with reference-based metrics.</p></p class="citation"></blockquote><h3 id=32109--32346-your-vision-language-model-itself-is-a-strong-filter-towards-high-quality-instruction-tuning-with-data-selection-ruibo-chen-et-al-2024>(32/109 | 32/346) Your Vision-Language Model Itself Is a Strong Filter: Towards High-Quality Instruction Tuning with Data Selection (Ruibo Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ruibo Chen, Yihan Wu, Lichang Chen, Guodong Liu, Qi He, Tianyi Xiong, Chenxi Liu, Junfeng Guo, Heng Huang. (2024)<br><strong>Your Vision-Language Model Itself Is a Strong Filter: Towards High-Quality Instruction Tuning with Data Selection</strong><br><button class=copy-to-clipboard title="Your Vision-Language Model Itself Is a Strong Filter: Towards High-Quality Instruction Tuning with Data Selection" index=32>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-32 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: Instruction Following, Instruction Tuning, Large Language Model, Large Language Model, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12501v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12501v1.pdf filename=2402.12501v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Data selection in <b>instruction</b> <b>tuning</b> emerges as a pivotal process for acquiring high-quality data and training <b>instruction-following</b> <b>large</b> <b>language</b> <b>models</b> <b>(LLMs),</b> but it is still a new and unexplored research area for <b>vision-language</b> models (VLMs). Existing data selection approaches on <b>LLMs</b> either rely on single unreliable scores, or use downstream tasks for selection, which is time-consuming and can lead to potential over-fitting on the chosen evaluation datasets. To address this challenge, we introduce a novel dataset selection method, Self-Filter, that utilizes the VLM itself as a filter. This approach is inspired by the observation that VLMs benefit from training with the most challenging <b>instructions.</b> <b>Self-Filter</b> operates in two stages. In the first stage, we devise a scoring network to evaluate the difficulty of training <b>instructions,</b> <b>which</b> is co-trained with the VLM. In the second stage, we use the trained score net to measure the difficulty of each <b>instruction,</b> <b>select</b> the most challenging samples, and penalize similar samples to encourage diversity. Comprehensive experiments on LLaVA and MiniGPT-4 show that Self-Filter can reach better results compared to full data settings with merely about 15% samples, and can achieve superior performance against competitive baselines.</p></p class="citation"></blockquote><h3 id=33109--33346-gtbench-uncovering-the-strategic-reasoning-limitations-of-llms-via-game-theoretic-evaluations-jinhao-duan-et-al-2024>(33/109 | 33/346) GTBench: Uncovering the Strategic Reasoning Limitations of LLMs via Game-Theoretic Evaluations (Jinhao Duan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jinhao Duan, Renming Zhang, James Diffenderfer, Bhavya Kailkhura, Lichao Sun, Elias Stengel-Eskin, Mohit Bansal, Tianlong Chen, Kaidi Xu. (2024)<br><strong>GTBench: Uncovering the Strategic Reasoning Limitations of LLMs via Game-Theoretic Evaluations</strong><br><button class=copy-to-clipboard title="GTBench: Uncovering the Strategic Reasoning Limitations of LLMs via Game-Theoretic Evaluations" index=33>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-33 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 50<br>Keywords: GPT, GPT-4, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12348v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12348v1.pdf filename=2402.12348v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>As <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> are integrated into critical real-world applications, their strategic and logical <b>reasoning</b> abilities are increasingly crucial. This paper evaluates <b>LLMs&rsquo;</b> <b>reasoning</b> abilities in competitive environments through game-theoretic tasks, e.g., board and card games that require pure logic and strategic <b>reasoning</b> to compete with opponents. We first propose GTBench, a language-driven environment composing 10 widely-recognized tasks, across a comprehensive game taxonomy: complete versus incomplete information, dynamic versus static, and probabilistic versus deterministic scenarios. Then, we investigate two key problems: (1) Characterizing game-theoretic <b>reasoning</b> of <b>LLMs;</b> (2) <b>LLM-vs-LLM</b> competitions as <b>reasoning</b> evaluation. We observe that (1) <b>LLMs</b> have distinct behaviors regarding various gaming scenarios; for example, <b>LLMs</b> fail in complete and deterministic games yet they are competitive in probabilistic gaming scenarios; (2) Open-source <b>LLMs,</b> e.g., CodeLlama-34b-Instruct, are less competitive than commercial <b>LLMs,</b> e.g., <b>GPT-4,</b> in complex games. In addition, code-pretraining greatly benefits strategic <b>reasoning,</b> while advanced <b>reasoning</b> methods such as Chain-of-Thought (CoT) and Tree-of-Thought (ToT) do not always help. Detailed error profiles are also provided for a better understanding of <b>LLMs&rsquo;</b> behavior.</p></p class="citation"></blockquote><h3 id=34109--34346-emulated-disalignment-safety-alignment-for-large-language-models-may-backfire-zhanhui-zhou-et-al-2024>(34/109 | 34/346) Emulated Disalignment: Safety Alignment for Large Language Models May Backfire! (Zhanhui Zhou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhanhui Zhou, Jie Liu, Zhichen Dong, Jiaheng Liu, Chao Yang, Wanli Ouyang, Yu Qiao. (2024)<br><strong>Emulated Disalignment: Safety Alignment for Large Language Models May Backfire!</strong><br><button class=copy-to-clipboard title="Emulated Disalignment: Safety Alignment for Large Language Models May Backfire!" index=34>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-34 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 50<br>Keywords: Alpaca, LLaMA, Mistral, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12343v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12343v2.pdf filename=2402.12343v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> need to undergo safety alignment to ensure safe conversations with humans. However, in this work, we introduce an inference-time attack framework, demonstrating that safety alignment can also unintentionally facilitate harmful outcomes under adversarial manipulation. This framework, named Emulated Disalignment (ED), adversely combines a pair of open-source pre-trained and safety-aligned language models in the output space to produce a harmful language model without additional training. Our experiments with ED across three datasets and four model families <b>(Llama-1,</b> <b>Llama-2,</b> <b>Mistral,</b> and <b>Alpaca)</b> show that ED doubles the harmfulness of pre-trained models and outperforms strong baselines, achieving the highest harmful rate in 43 out of 48 evaluation subsets by a <b>large</b> <b>margin.</b> <b>Crucially,</b> our findings highlight the importance of reevaluating the practice of open-sourcing language models even after safety alignment.</p></p class="citation"></blockquote><h3 id=35109--35346-same-task-more-tokens-the-impact-of-input-length-on-the-reasoning-performance-of-large-language-models-mosh-levy-et-al-2024>(35/109 | 35/346) Same Task, More Tokens: the Impact of Input Length on the Reasoning Performance of Large Language Models (Mosh Levy et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mosh Levy, Alon Jacoby, Yoav Goldberg. (2024)<br><strong>Same Task, More Tokens: the Impact of Input Length on the Reasoning Performance of Large Language Models</strong><br><button class=copy-to-clipboard title="Same Task, More Tokens: the Impact of Input Length on the Reasoning Performance of Large Language Models" index=35>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-35 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: Question Answering, Reasoning, Large Language Model, Large Language Model, Perplexity<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14848v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14848v1.pdf filename=2402.14848v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper explores the impact of extending input lengths on the capabilities of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs).</b> Despite <b>LLMs</b> advancements in recent times, their performance consistency across different input lengths is not well understood. We investigate this aspect by introducing a novel <b>QA</b> <b>reasoning</b> framework, specifically designed to assess the impact of input length. We isolate the effect of input length using multiple versions of the same sample, each being extended with padding of different lengths, types and locations. Our findings show a notable degradation in <b>LLMs&rsquo;</b> <b>reasoning</b> performance at much shorter input lengths than their technical maximum. We show that the degradation trend appears in every version of our dataset, although at different intensities. Additionally, our study reveals that traditional <b>perplexity</b> metrics do not correlate with performance of <b>LLMs&rsquo;</b> in long input <b>reasoning</b> tasks. We analyse our results and identify failure modes that can serve as useful guides for future research, potentially informing strategies to address the limitations observed in <b>LLMs.</b></p></p class="citation"></blockquote><h3 id=36109--36346-enhancing-multilingual-capabilities-of-large-language-models-through-self-distillation-from-resource-rich-languages-yuanchi-zhang-et-al-2024>(36/109 | 36/346) Enhancing Multilingual Capabilities of Large Language Models through Self-Distillation from Resource-Rich Languages (Yuanchi Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuanchi Zhang, Yile Wang, Zijun Liu, Shuo Wang, Xiaolong Wang, Peng Li, Maosong Sun, Yang Liu. (2024)<br><strong>Enhancing Multilingual Capabilities of Large Language Models through Self-Distillation from Resource-Rich Languages</strong><br><button class=copy-to-clipboard title="Enhancing Multilingual Capabilities of Large Language Models through Self-Distillation from Resource-Rich Languages" index=36>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-36 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: Knowledge Transfer, Self-Distillation, LLaMA, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12204v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12204v1.pdf filename=2402.12204v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>While <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have been pre-trained on multilingual corpora, their performance still lags behind in most languages compared to a few resource-rich languages. One common approach to mitigate this issue is to translate training data from resource-rich languages into other languages and then continue training. However, using the data obtained solely relying on translation while ignoring the original capabilities of <b>LLMs</b> across languages is not always effective, which we show will limit the performance of cross-lingual <b>knowledge</b> <b>transfer.</b> In this work, we propose SDRRL, a method based on <b>Self-Distillation</b> from Resource-Rich Languages that effectively improve multilingual performance by leveraging the internal capabilities of <b>LLMs</b> on resource-rich languages. We evaluate on different <b>LLMs</b> <b>(LLaMA-2</b> and SeaLLM) and source languages across various comprehension and generation tasks, experimental results demonstrate that SDRRL can significantly enhance multilingual capabilities while minimizing the impact on original performance in resource-rich languages.</p></p class="citation"></blockquote><h3 id=37109--37346-towards-cross-tokenizer-distillation-the-universal-logit-distillation-loss-for-llms-nicolas-boizard-et-al-2024>(37/109 | 37/346) Towards Cross-Tokenizer Distillation: the Universal Logit Distillation Loss for LLMs (Nicolas Boizard et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nicolas Boizard, Kevin El Haddad, Céline Hudelot, Pierre Colombo. (2024)<br><strong>Towards Cross-Tokenizer Distillation: the Universal Logit Distillation Loss for LLMs</strong><br><button class=copy-to-clipboard title="Towards Cross-Tokenizer Distillation: the Universal Logit Distillation Loss for LLMs" index=37>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-37 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: Knowledge Distillation, Knowledge Distillation, Knowledge Distillation, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12030v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12030v2.pdf filename=2402.12030v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deploying <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> of several billion parameters can be impractical in most industrial use cases due to constraints such as cost, latency limitations, and hardware accessibility. <b>Knowledge</b> <b>distillation</b> <b>(KD)</b> offers a solution by compressing <b>knowledge</b> <b>from</b> resource-intensive <b>large</b> <b>models</b> <b>to</b> smaller ones. Various strategies exist, some relying on the text generated by the teacher model and optionally utilizing his logits to enhance learning. However, these methods based on logits often require both teacher and student models to share the same tokenizer, limiting their applicability across different <b>LLM</b> families. In this paper, we introduce Universal Logit <b>Distillation</b> (ULD) loss, grounded in optimal transport, to address this limitation. Our experimental results demonstrate the effectiveness of ULD loss in enabling <b>distillation</b> across models with different architectures and tokenizers, paving the way to a more widespread use of <b>distillation</b> techniques.</p></p class="citation"></blockquote><h3 id=38109--38346-uncovering-latent-human-wellbeing-in-language-model-embeddings-pedro-freire-et-al-2024>(38/109 | 38/346) Uncovering Latent Human Wellbeing in Language Model Embeddings (Pedro Freire et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Pedro Freire, ChengCheng Tan, Adam Gleave, Dan Hendrycks, Scott Emmons. (2024)<br><strong>Uncovering Latent Human Wellbeing in Language Model Embeddings</strong><br><button class=copy-to-clipboard title="Uncovering Latent Human Wellbeing in Language Model Embeddings" index=38>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-38 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: I-2-7, cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 50<br>Keywords: Fine-tuning, Fine-tuning, BERT, Prompt, Text Embedding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11777v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11777v1.pdf filename=2402.11777v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Do language models implicitly learn a concept of human wellbeing? We explore this through the ETHICS Utilitarianism task, assessing if scaling enhances pretrained models&rsquo; representations. Our initial finding reveals that, without any <b>prompt</b> engineering or <b>finetuning,</b> the leading principal component from OpenAI&rsquo;s <b>text-embedding-ada-002</b> <b>achieves</b> 73.9% accuracy. This closely matches the 74.6% of <b>BERT-large</b> <b>finetuned</b> on the entire ETHICS dataset, suggesting pretraining conveys some understanding about human wellbeing. Next, we consider four language model families, observing how Utilitarianism accuracy varies with increased parameters. We find performance is nondecreasing with increased model size when using sufficient numbers of principal components.</p></p class="citation"></blockquote><h3 id=39109--39346-artifacts-or-abduction-how-do-llms-answer-multiple-choice-questions-without-the-question-nishant-balepur-et-al-2024>(39/109 | 39/346) Artifacts or Abduction: How Do LLMs Answer Multiple-Choice Questions Without the Question? (Nishant Balepur et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nishant Balepur, Abhilasha Ravichander, Rachel Rudinger. (2024)<br><strong>Artifacts or Abduction: How Do LLMs Answer Multiple-Choice Questions Without the Question?</strong><br><button class=copy-to-clipboard title="Artifacts or Abduction: How Do LLMs Answer Multiple-Choice Questions Without the Question?" index=39>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-39 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 48<br>Keywords: Benchmarking, Black Box, Question Answering, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12483v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12483v1.pdf filename=2402.12483v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Multiple-choice <b>question</b> <b>answering</b> (MCQA) is often used to evaluate <b>large</b> <b>language</b> <b>models</b> <b>(LLMs).</b> To see if MCQA assesses <b>LLMs</b> as intended, we probe if <b>LLMs</b> can perform MCQA with choices-only <b>prompts,</b> where models must select the correct answer only from the choices. In three MCQA datasets and four <b>LLMs,</b> this <b>prompt</b> bests a majority baseline in 11/12 cases, with up to 0.33 accuracy gain. To help explain this behavior, we conduct an in-depth, <b>black-box</b> <b>analysis</b> on memorization, choice dynamics, and <b>question</b> <b>inference.</b> Our key findings are threefold. First, we find no evidence that the choices-only accuracy stems from memorization alone. Second, priors over individual choices do not fully explain choices-only accuracy, hinting that <b>LLMs</b> use the group dynamics of choices. Third, <b>LLMs</b> have some ability to infer a relevant <b>question</b> <b>from</b> choices, and surprisingly can sometimes even match the original <b>question.</b> <b>We</b> hope to motivate the use of stronger baselines in MCQA <b>benchmarks,</b> the design of robust MCQA datasets, and further efforts to explain <b>LLM</b> decision-making.</p></p class="citation"></blockquote><h3 id=40109--40346-browse-and-concentrate-comprehending-multimodal-content-via-prior-llm-context-fusion-ziyue-wang-et-al-2024>(40/109 | 40/346) Browse and Concentrate: Comprehending Multimodal Content via prior-LLM Context Fusion (Ziyue Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ziyue Wang, Chi Chen, Yiqi Zhu, Fuwen Luo, Peng Li, Ming Yan, Ji Zhang, Fei Huang, Maosong Sun, Yang Liu. (2024)<br><strong>Browse and Concentrate: Comprehending Multimodal Content via prior-LLM Context Fusion</strong><br><button class=copy-to-clipboard title="Browse and Concentrate: Comprehending Multimodal Content via prior-LLM Context Fusion" index=40>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-40 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 46<br>Keywords: Multi-modal, Multi-modal, BLOOM, Large Language Model, Large Language Model, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12195v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12195v1.pdf filename=2402.12195v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the <b>bloom</b> of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs),</b> <b>Multimodal</b> <b>Large</b> <b>Language</b> <b>Models</b> (MLLMs) that incorporate <b>LLMs</b> with pre-trained vision models have recently demonstrated impressive performance across diverse <b>vision-language</b> tasks. However, they fall short to comprehend context involving multiple images. A primary reason for this shortcoming is that the visual features for each images are encoded individually by frozen encoders before feeding into the <b>LLM</b> backbone, lacking awareness of other images and the <b>multimodal</b> instructions. We term this issue as prior-LLM modality isolation and propose a two phase paradigm, browse-and-concentrate, to enable in-depth <b>multimodal</b> context fusion prior to feeding the features into <b>LLMs.</b> This paradigm initially &ldquo;browses&rdquo; through the inputs for essential insights, and then revisits the inputs to &ldquo;concentrate&rdquo; on crucial details, guided by these insights, to achieve a more comprehensive understanding of the <b>multimodal</b> inputs. Additionally, we develop training strategies specifically to enhance the understanding of multi-image inputs. Our method markedly boosts the performance on 7 multi-image scenarios, contributing to increments on average accuracy by 2.13% and 7.60% against strong MLLMs baselines with 3B and 11B <b>LLMs,</b> respectively.</p></p class="citation"></blockquote><h3 id=41109--41346-adaptive-skeleton-graph-decoding-shuowei-jin-et-al-2024>(41/109 | 41/346) Adaptive Skeleton Graph Decoding (Shuowei Jin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shuowei Jin, Yongji Wu, Haizhong Zheng, Qingzhao Zhang, Matthew Lentz, Z. Morley Mao, Atul Prakash, Feng Qian, Danyang Zhuo. (2024)<br><strong>Adaptive Skeleton Graph Decoding</strong><br><button class=copy-to-clipboard title="Adaptive Skeleton Graph Decoding" index=41>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-41 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 43<br>Keywords: Graph, Stochastic Gradient Descent, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12280v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12280v1.pdf filename=2402.12280v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have seen significant adoption for natural language tasks, owing their success to massive numbers of model parameters (e.g., 70B+); however, <b>LLM</b> inference incurs significant computation and memory costs. Recent approaches propose parallel decoding strategies, such as Skeleton-of-Thought (SoT), to improve performance by breaking <b>prompts</b> down into sub-problems that can be decoded in parallel; however, they often suffer from reduced response quality. Our key insight is that we can request additional information, specifically dependencies and difficulty, when generating the sub-problems to improve both response quality and performance. In this paper, we propose Skeleton <b>Graph</b> Decoding <b>(SGD),</b> which uses dependencies exposed between sub-problems to support information forwarding between dependent sub-problems for improved quality while exposing parallelization opportunities for decoding independent sub-problems. Additionally, we leverage difficulty estimates for each sub-problem to select an appropriately-sized model, improving performance without significantly reducing quality. Compared to standard autoregressive generation and SoT, <b>SGD</b> achieves a 1.69x speedup while improving quality by up to 51%.</p></p class="citation"></blockquote><h3 id=42109--42346-stick-to-your-role-stability-of-personal-values-expressed-in-large-language-models-grgur-kovač-et-al-2024>(42/109 | 42/346) Stick to your Role! Stability of Personal Values Expressed in Large Language Models (Grgur Kovač et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Grgur Kovač, Rémy Portelas, Masataka Sawayama, Peter Ford Dominey, Pierre-Yves Oudeyer. (2024)<br><strong>Stick to your Role! Stability of Personal Values Expressed in Large Language Models</strong><br><button class=copy-to-clipboard title="Stick to your Role! Stability of Personal Values Expressed in Large Language Models" index=42>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-42 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: 68T07, I-2-7, cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 43<br>Keywords: Benchmarking, LLaMA, Mistral, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14846v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14846v1.pdf filename=2402.14846v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The standard way to study <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> through <b>benchmarks</b> or psychology questionnaires is to provide many different queries from similar minimal contexts (e.g. multiple choice questions). However, due to <b>LLM&rsquo;s</b> highly context-dependent nature, conclusions from such minimal-context evaluations may be little informative about the model&rsquo;s behavior in deployment (where it will be exposed to many new contexts). We argue that context-dependence should be studied as another dimension of <b>LLM</b> comparison alongside others such as cognitive abilities, knowledge, or model size. In this paper, we present a case-study about the stability of value expression over different contexts (simulated conversations on different topics), and as measured using a standard psychology questionnaire (PVQ) and a behavioral downstream task. We consider 19 open-sourced <b>LLMs</b> from five families. Reusing methods from psychology, we study Rank-order stability on the population (interpersonal) level, and Ipsative stability on the individual (intrapersonal) level. We explore two settings: with and without instructing <b>LLMs</b> to simulate particular personalities. We observe similar trends in the stability of models and model families - Mixtral, <b>Mistral</b> and Qwen families being more stable than <b>LLaMa-2</b> and Phi - over those two settings, two different simulated populations, and even in the downstream behavioral task. When instructed to simulate particular personas, <b>LLMs</b> exhibit low Rank-Order stability, and this stability further diminishes with conversation length. This highlights the need for future research directions on <b>LLMs</b> that can coherently simulate a diversity of personas, as well as how context-dependence can be studied in more thorough and efficient ways. This paper provides a foundational step in that direction, and, to our knowledge, it is the first study of value stability in <b>LLMs.</b></p></p class="citation"></blockquote><h3 id=43109--43346-transformer-based-causal-language-models-perform-clustering-xinbo-wu-et-al-2024>(43/109 | 43/346) Transformer-based Causal Language Models Perform Clustering (Xinbo Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xinbo Wu, Lav R. Varshney. (2024)<br><strong>Transformer-based Causal Language Models Perform Clustering</strong><br><button class=copy-to-clipboard title="Transformer-based Causal Language Models Perform Clustering" index=43>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-43 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 43<br>Keywords: Clustering, Transformer, Instruction Following, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12151v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12151v1.pdf filename=2402.12151v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Even though <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have demonstrated remarkable capability in solving various natural language tasks, the capability of an <b>LLM</b> to follow human <b>instructions</b> <b>is</b> still a concern. Recent works have shown great improvements in the <b>instruction-following</b> <b>capability</b> via additional training for <b>instruction-following</b> <b>tasks.</b> However, the mechanisms responsible for effective <b>instruction-following</b> <b>capabilities</b> remain inadequately understood. Here, we introduce a simplified <b>instruction-following</b> <b>task</b> and use synthetic datasets to analyze a <b>Transformer-based</b> causal language model. Our findings suggest that the model learns task-specific information by <b>clustering</b> data within its hidden space, with this <b>clustering</b> process evolving dynamically during learning. We also demonstrate how this phenomenon assists the model in handling unseen instances and validate our results in a more realistic setting.</p></p class="citation"></blockquote><h3 id=44109--44346-emobench-evaluating-the-emotional-intelligence-of-large-language-models-sahand-sabour-et-al-2024>(44/109 | 44/346) EmoBench: Evaluating the Emotional Intelligence of Large Language Models (Sahand Sabour et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sahand Sabour, Siyang Liu, Zheyuan Zhang, June M. Liu, Jinfeng Zhou, Alvionna S. Sunaryo, Juanzi Li, Tatia M. C. Lee, Rada Mihalcea, Minlie Huang. (2024)<br><strong>EmoBench: Evaluating the Emotional Intelligence of Large Language Models</strong><br><button class=copy-to-clipboard title="EmoBench: Evaluating the Emotional Intelligence of Large Language Models" index=44>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-44 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 43<br>Keywords: Benchmarking, Emotion Recognition, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12071v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12071v1.pdf filename=2402.12071v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advances in <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have highlighted the need for robust, comprehensive, and challenging <b>benchmarks.</b> Yet, research on evaluating their <b>Emotional</b> <b>Intelligence</b> (EI) is considerably limited. Existing <b>benchmarks</b> have two major shortcomings: first, they mainly focus on <b>emotion</b> <b>recognition,</b> neglecting essential EI capabilities such as <b>emotion</b> <b>regulation</b> and thought facilitation through <b>emotion</b> <b>understanding;</b> second, they are primarily constructed from existing datasets, which include frequent patterns, explicit information, and annotation errors, leading to unreliable evaluation. We propose EmoBench, a <b>benchmark</b> that draws upon established psychological theories and proposes a comprehensive definition for machine EI, including <b>Emotional</b> <b>Understanding</b> and <b>Emotional</b> <b>Application.</b> EmoBench includes a set of 400 hand-crafted questions in English and Chinese, which are meticulously designed to require thorough <b>reasoning</b> and understanding. Our findings reveal a considerable gap between the EI of existing <b>LLMs</b> and the average human, highlighting a promising direction for future research. Our code and data will be publicly available from <a href=https://github.com/Sahandfer/EmoBench>https://github.com/Sahandfer/EmoBench</a>.</p></p class="citation"></blockquote><h3 id=45109--45346-how-interpretable-are-reasoning-explanations-from-prompting-large-language-models-yeo-wei-jie-et-al-2024>(45/109 | 45/346) How Interpretable are Reasoning Explanations from Prompting Large Language Models? (Yeo Wei Jie et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yeo Wei Jie, Ranjan Satapathy, Goh Siow Mong, Rick, Erik Cambria. (2024)<br><strong>How Interpretable are Reasoning Explanations from Prompting Large Language Models?</strong><br><button class=copy-to-clipboard title="How Interpretable are Reasoning Explanations from Prompting Large Language Models?" index=45>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-45 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 43<br>Keywords: Benchmarking, Common-sense Reasoning, Reasoning, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11863v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11863v1.pdf filename=2402.11863v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Prompt</b> Engineering has garnered significant attention for enhancing the performance of <b>large</b> <b>language</b> <b>models</b> across a multitude of tasks. Techniques such as the Chain-of-Thought not only bolster task performance but also delineate a clear trajectory of <b>reasoning</b> steps, offering a tangible form of explanation for the audience. Prior works on interpretability assess the <b>reasoning</b> chains yielded by Chain-of-Thought solely along a singular axis, namely faithfulness. We present a comprehensive and multifaceted evaluation of interpretability, examining not only faithfulness but also robustness and utility across multiple <b>commonsense</b> <b>reasoning</b> <b>benchmarks.</b> Likewise, our investigation is not confined to a single <b>prompting</b> technique; it expansively covers a multitude of prevalent <b>prompting</b> techniques employed in <b>large</b> <b>language</b> <b>models,</b> thereby ensuring a wide-ranging and exhaustive evaluation. In addition, we introduce a simple interpretability alignment technique, termed Self-Entailment-Alignment Chain-of-thought, that yields more than 70% improvements across multiple dimensions of interpretability. Code is available at <a href=https://github.com/wj210/CoT_interpretability>https://github.com/wj210/CoT_interpretability</a></p></p class="citation"></blockquote><h3 id=46109--46346-fipo-free-form-instruction-oriented-prompt-optimization-with-preference-dataset-and-modular-fine-tuning-schema-junru-lu-et-al-2024>(46/109 | 46/346) FIPO: Free-form Instruction-oriented Prompt Optimization with Preference Dataset and Modular Fine-tuning Schema (Junru Lu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Junru Lu, Siyu An, Min Zhang, Yulan He, Di Yin, Xing Sun. (2024)<br><strong>FIPO: Free-form Instruction-oriented Prompt Optimization with Preference Dataset and Modular Fine-tuning Schema</strong><br><button class=copy-to-clipboard title="FIPO: Free-form Instruction-oriented Prompt Optimization with Preference Dataset and Modular Fine-tuning Schema" index=46>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-46 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 43<br>Keywords: Benchmarking, Fine-tuning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11811v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11811v1.pdf filename=2402.11811v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the quest to facilitate the deep intelligence of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> accessible in final-end user-bot interactions, the art of <b>prompt</b> crafting emerges as a critical yet complex task for the average user. Contrast to previous model-oriented yet instruction-agnostic Automatic <b>Prompt</b> Optimization methodologies, yielding polished results for predefined target models while suffering rapid degradation with out-of-box models, we present Free-form Instruction-oriented <b>Prompt</b> Optimization (FIPO). This approach is supported by our <b>large-scale</b> <b>prompt</b> <b>preference</b> dataset and employs a modular <b>fine-tuning</b> schema. The FIPO schema reimagines the optimization process into manageable modules, anchored by a meta <b>prompt</b> that dynamically adapts content. This allows for the flexible integration of the raw task instruction, the optional instruction response, and the optional ground truth to produce finely optimized task <b>prompts.</b> The FIPO preference dataset is meticulously constructed using the optimal and suboptimal <b>LLMs,</b> undergoing rigorous cross-verification by human experts and analytical models. Applying the insights from the data with Tulu2 models and <b>fine-tuning</b> strategies, we validate the efficacy of FIPO schema across five public <b>benchmarks.</b> Codes, data and scripts are here: <a href=https://github.com/LuJunru/FIPO_Project>https://github.com/LuJunru/FIPO_Project</a>.</p></p class="citation"></blockquote><h3 id=47109--47346-language-models-are-homer-simpson-safety-re-alignment-of-fine-tuned-language-models-through-task-arithmetic-rishabh-bhardwaj-et-al-2024>(47/109 | 47/346) Language Models are Homer Simpson! Safety Re-Alignment of Fine-tuned Language Models through Task Arithmetic (Rishabh Bhardwaj et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rishabh Bhardwaj, Do Duc Anh, Soujanya Poria. (2024)<br><strong>Language Models are Homer Simpson! Safety Re-Alignment of Fine-tuned Language Models through Task Arithmetic</strong><br><button class=copy-to-clipboard title="Language Models are Homer Simpson! Safety Re-Alignment of Fine-tuned Language Models through Task Arithmetic" index=47>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-47 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 43<br>Keywords: Benchmarking, Fine-tuning, Fine-tuning, Instruction Following, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11746v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11746v1.pdf filename=2402.11746v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Aligned language models face a significant limitation as their <b>fine-tuning</b> often results in compromised safety. To tackle this, we propose a simple method RESTA that performs <b>LLM</b> safety realignment. RESTA stands for REstoring Safety through Task Arithmetic. At its core, it involves a simple arithmetic addition of a safety vector to the weights of the compromised model. We demonstrate the effectiveness of RESTA in both parameter-efficient and full <b>fine-tuning,</b> covering a wide range of downstream tasks, including <b>instruction</b> <b>following</b> in Chinese, English, and Hindi, as well as problem-solving capabilities in Code and Math. We also showcase the generalizability of RESTA on three existing safety evaluation <b>benchmarks</b> and a multilingual <b>benchmark</b> dataset proposed as a part of this work, consisting of 550 harmful questions covering 11 categories, each with 5 sub-categories of harm. Overall, RESTA decreases the harmfulness of the compromised model from 18.6% to 5.1% and from 9.2% to 1.5% in parameter-efficient and full <b>fine-tuning,</b> respectively, while maintaining most of the model&rsquo;s performance on the task. We release the source codes at: <a href=https://github.com/declare-lab/resta>https://github.com/declare-lab/resta</a>.</p></p class="citation"></blockquote><h3 id=48109--48346-tilp-differentiable-learning-of-temporal-logical-rules-on-knowledge-graphs-siheng-xiong-et-al-2024>(48/109 | 48/346) TILP: Differentiable Learning of Temporal Logical Rules on Knowledge Graphs (Siheng Xiong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Siheng Xiong, Yuan Yang, Faramarz Fekri, James Clayton Kerce. (2024)<br><strong>TILP: Differentiable Learning of Temporal Logical Rules on Knowledge Graphs</strong><br><button class=copy-to-clipboard title="TILP: Differentiable Learning of Temporal Logical Rules on Knowledge Graphs" index=48>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-48 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 41<br>Keywords: Graph, Benchmarking, Knowledge Graph, Reasoning, Temporal Knowledge Graph, Temporal Knowledge Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12309v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12309v1.pdf filename=2402.12309v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Compared with static <b>knowledge</b> <b>graphs,</b> <b>temporal</b> <b>knowledge</b> <b>graphs</b> <b>(tKG),</b> which can capture the evolution and change of information over time, are more realistic and general. However, due to the complexity that the notion of time introduces to the learning of the rules, an accurate <b>graph</b> <b>reasoning,</b> e.g., predicting new links between entities, is still a difficult problem. In this paper, we propose TILP, a differentiable framework for <b>temporal</b> <b>logical</b> <b>rules</b> learning. By designing a constrained random walk mechanism and the introduction of <b>temporal</b> <b>operators,</b> <b>we</b> ensure the efficiency of our model. We present <b>temporal</b> <b>features</b> <b>modeling</b> in <b>tKG,</b> e.g., recurrence, <b>temporal</b> <b>order,</b> <b>interval</b> between pair of relations, and duration, and incorporate it into our learning process. We compare TILP with state-of-the-art methods on two <b>benchmark</b> datasets. We show that our proposed framework can improve upon the performance of baseline methods while providing interpretable results. In particular, we consider various scenarios in which training samples are limited, data is biased, and the time range between training and inference are different. In all these cases, TILP works much better than the state-of-the-art methods.</p></p class="citation"></blockquote><h3 id=49109--49346-asynchronous-and-segmented-bidirectional-encoding-for-nmt-jingpu-yang-et-al-2024>(49/109 | 49/346) Asynchronous and Segmented Bidirectional Encoding for NMT (Jingpu Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jingpu Yang, Zehua Han, Mengyu Xiang, Helin Wang, Yuxiao Huang, Miao Fang. (2024)<br><strong>Asynchronous and Segmented Bidirectional Encoding for NMT</strong><br><button class=copy-to-clipboard title="Asynchronous and Segmented Bidirectional Encoding for NMT" index=49>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-49 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 40<br>Keywords: Transformer, Neural Machine Translation, Neural Machine Translation, Neural Machine Translation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14849v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14849v1.pdf filename=2402.14849v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the rapid advancement of <b>Neural</b> <b>Machine</b> <b>Translation</b> <b>(NMT),</b> enhancing translation efficiency and quality has become a focal point of research. Despite the commendable performance of general models such as the <b>Transformer</b> in various aspects, they still fall short in processing long sentences and fully leveraging bidirectional contextual information. This paper introduces an improved model based on the <b>Transformer,</b> implementing an asynchronous and segmented bidirectional decoding strategy aimed at elevating translation efficiency and accuracy. Compared to traditional unidirectional translations from left-to-right or right-to-left, our method demonstrates heightened efficiency and improved translation quality, particularly in handling long sentences. Experimental results on the IWSLT2017 dataset confirm the effectiveness of our approach in accelerating translation and increasing accuracy, especially surpassing traditional unidirectional strategies in long sentence translation. Furthermore, this study analyzes the impact of sentence length on decoding outcomes and explores the model&rsquo;s performance in various scenarios. The findings of this research not only provide an effective encoding strategy for the <b>NMT</b> field but also pave new avenues and directions for future studies.</p></p class="citation"></blockquote><h3 id=50109--50346-understanding-fine-grained-distortions-in-reports-of-scientific-findings-amelie-wührl-et-al-2024>(50/109 | 50/346) Understanding Fine-grained Distortions in Reports of Scientific Findings (Amelie Wührl et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Amelie Wührl, Dustin Wright, Roman Klinger, Isabelle Augenstein. (2024)<br><strong>Understanding Fine-grained Distortions in Reports of Scientific Findings</strong><br><button class=copy-to-clipboard title="Understanding Fine-grained Distortions in Reports of Scientific Findings" index=50>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-50 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Few-shot, Fine-tuning, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12431v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12431v1.pdf filename=2402.12431v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Distorted science communication harms individuals and society as it can lead to unhealthy behavior change and decrease trust in scientific institutions. Given the rapidly increasing volume of science communication in recent years, a fine-grained understanding of how findings from scientific publications are reported to the general public, and methods to detect distortions from the original work automatically, are crucial. Prior work focused on individual aspects of distortions or worked with unpaired data. In this work, we make three foundational contributions towards addressing this problem: (1) annotating 1,600 instances of scientific findings from academic papers paired with corresponding findings as reported in news articles and tweets wrt. four characteristics: causality, certainty, generality and sensationalism; (2) establishing baselines for automatically detecting these characteristics; and (3) analyzing the prevalence of changes in these characteristics in both human-annotated and large-scale unlabeled data. Our results show that scientific findings frequently undergo subtle distortions when reported. Tweets distort findings more often than science news reports. Detecting fine-grained distortions automatically poses a challenging task. In our experiments, <b>fine-tuned</b> task-specific models consistently outperform <b>few-shot</b> <b>LLM</b> <b>prompting.</b></p></p class="citation"></blockquote><h3 id=51109--51346-query-based-adversarial-prompt-generation-jonathan-hayase-et-al-2024>(51/109 | 51/346) Query-Based Adversarial Prompt Generation (Jonathan Hayase et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jonathan Hayase, Ema Borevkovic, Nicholas Carlini, Florian Tramèr, Milad Nasr. (2024)<br><strong>Query-Based Adversarial Prompt Generation</strong><br><button class=copy-to-clipboard title="Query-Based Adversarial Prompt Generation" index=51>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-51 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-CR, cs-LG, cs.CL<br>Keyword Score: 40<br>Keywords: GPT, GPT-3, GPT-3.5, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12329v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12329v1.pdf filename=2402.12329v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent work has shown it is possible to construct adversarial examples that cause an aligned language model to emit harmful strings or perform harmful behavior. Existing attacks work either in the white-box setting (with full access to the model weights), or through transferability: the phenomenon that adversarial examples crafted on one model often remain effective on other models. We improve on prior work with a query-based attack that leverages API access to a remote language model to construct adversarial examples that cause the model to emit harmful strings with (much) higher probability than with transfer-only attacks. We validate our attack on <b>GPT-3.5</b> and OpenAI&rsquo;s safety classifier; we can cause <b>GPT-3.5</b> to emit harmful strings that current transfer attacks fail at, and we can evade the safety classifier with nearly 100% probability.</p></p class="citation"></blockquote><h3 id=52109--52346-key-ingredients-for-effective-zero-shot-cross-lingual-knowledge-transfer-in-generative-tasks-nadezhda-chirkova-et-al-2024>(52/109 | 52/346) Key ingredients for effective zero-shot cross-lingual knowledge transfer in generative tasks (Nadezhda Chirkova et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nadezhda Chirkova, Vassilina Nikoulina. (2024)<br><strong>Key ingredients for effective zero-shot cross-lingual knowledge transfer in generative tasks</strong><br><button class=copy-to-clipboard title="Key ingredients for effective zero-shot cross-lingual knowledge transfer in generative tasks" index=52>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-52 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Fine-tuning, Knowledge Transfer, Zero-shot, Pre-trained Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12279v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12279v1.pdf filename=2402.12279v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Zero-shot</b> cross-lingual generation implies <b>finetuning</b> of the multilingual <b>pretrained</b> <b>language</b> <b>model</b> on a generation task in one language and then using it to make predictions for this task in other languages. Previous works notice a frequent problem of generation in a wrong language and propose approaches to address it, usually using mT5 as a backbone model. In this work we compare various approaches proposed from the literature in unified settings, also including alternative backbone models, namely mBART and NLLB-200. We first underline the importance of tuning learning rate used for <b>finetuning,</b> which helps to substantially alleviate the problem of generation in the wrong language. Then, we show that with careful learning rate tuning, the simple full <b>finetuning</b> of the model acts as a very strong baseline and alternative approaches bring only marginal improvements. Finally, we find that mBART performs similarly to mT5 of the same size, and NLLB-200 can be competitive in some cases. Our final models reach the performance of the approach based on data translation which is usually considered as an upper baseline for <b>zero-shot</b> cross-lingual generation.</p></p class="citation"></blockquote><h3 id=53109--53346-analysis-of-levenshtein-transformers-decoder-and-its-variants-ruiyang-zhou-2024>(53/109 | 53/346) Analysis of Levenshtein Transformer&rsquo;s Decoder and Its Variants (Ruiyang Zhou, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ruiyang Zhou. (2024)<br><strong>Analysis of Levenshtein Transformer&rsquo;s Decoder and Its Variants</strong><br><button class=copy-to-clipboard title="Analysis of Levenshtein Transformer's Decoder and Its Variants" index=53>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-53 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Knowledge Distillation, Transformer, Neural Machine Translation, BLEU<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12249v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12249v1.pdf filename=2402.12249v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Levenshtein <b>transformer</b> (LevT) is a non-autoregressive <b>machine</b> <b>translation</b> model with high decoding efficiency and comparable translation quality in terms of <b>bleu</b> score, due to its parallel decoding and iterative refinement procedure. Are there any deficiencies of its translations and what improvements could be made? In this report, we focus on LevT&rsquo;s decoder and analyse the decoding results length, subword generation, and deletion module&rsquo;s capability. We hope to identify weaknesses of the decoder for future improvements. We also compare translations of the original LevT, knowledge-distilled LevT, LevT with translation memory, and the <b>KD-LevT</b> with translation memory to see how <b>KD</b> and translation memory can help.</p></p class="citation"></blockquote><h3 id=54109--54346-polarization-of-autonomous-generative-ai-agents-under-echo-chambers-masaya-ohagi-2024>(54/109 | 54/346) Polarization of Autonomous Generative AI Agents Under Echo Chambers (Masaya Ohagi, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Masaya Ohagi. (2024)<br><strong>Polarization of Autonomous Generative AI Agents Under Echo Chambers</strong><br><button class=copy-to-clipboard title="Polarization of Autonomous Generative AI Agents Under Echo Chambers" index=54>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-54 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Generative AI, ChatGPT, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12212v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12212v1.pdf filename=2402.12212v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Online social networks often create echo chambers where people only hear opinions reinforcing their beliefs. An echo chamber often generates polarization, leading to conflicts caused by people with radical opinions, such as the January 6, 2021, attack on the US Capitol. The echo chamber has been viewed as a human-specific problem, but this implicit assumption is becoming less reasonable as <b>large</b> <b>language</b> <b>models,</b> such as <b>ChatGPT,</b> acquire social abilities. In response to this situation, we investigated the potential for polarization to occur among a group of autonomous AI agents based on <b>generative</b> <b>language</b> models in an echo chamber environment. We had AI agents discuss specific topics and analyzed how the group&rsquo;s opinions changed as the discussion progressed. As a result, we found that the group of agents based on <b>ChatGPT</b> tended to become polarized in echo chamber environments. The analysis of opinion transitions shows that this result is caused by <b>ChatGPT&rsquo;s</b> high <b>prompt</b> understanding ability to update its opinion by considering its own and surrounding agents&rsquo; opinions. We conducted additional experiments to investigate under what specific conditions AI agents tended to polarize. As a result, we identified factors that strongly influence polarization, such as the agent&rsquo;s persona. These factors should be monitored to prevent the polarization of AI agents.</p></p class="citation"></blockquote><h3 id=55109--55346-a-chinese-dataset-for-evaluating-the-safeguards-in-large-language-models-yuxia-wang-et-al-2024>(55/109 | 55/346) A Chinese Dataset for Evaluating the Safeguards in Large Language Models (Yuxia Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuxia Wang, Zenan Zhai, Haonan Li, Xudong Han, Lizhi Lin, Zhenxuan Zhang, Jingru Zhao, Preslav Nakov, Timothy Baldwin. (2024)<br><strong>A Chinese Dataset for Evaluating the Safeguards in Large Language Models</strong><br><button class=copy-to-clipboard title="A Chinese Dataset for Evaluating the Safeguards in Large Language Models" index=55>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-55 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Automatic Evaluation, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12193v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12193v1.pdf filename=2402.12193v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Many studies have demonstrated that <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> can produce harmful responses, exposing users to unexpected risks when <b>LLMs</b> are deployed. Previous studies have proposed comprehensive taxonomies of the risks posed by <b>LLMs,</b> as well as corresponding <b>prompts</b> that can be used to examine the safety mechanisms of <b>LLMs.</b> However, the focus has been almost exclusively on English, and little has been explored for other languages. Here we aim to bridge this gap. We first introduce a dataset for the safety evaluation of Chinese <b>LLMs,</b> and then extend it to two other scenarios that can be used to better identify false negative and false positive examples in terms of risky <b>prompt</b> rejections. We further present a set of fine-grained safety assessment criteria for each risk type, facilitating both manual annotation and <b>automatic</b> <b>evaluation</b> in terms of <b>LLM</b> response harmfulness. Our experiments on five <b>LLMs</b> show that region-specific risks are the prevalent type of risk, presenting the major issue with all Chinese <b>LLMs</b> we experimented with. Warning: this paper contains example data that may be offensive, harmful, or biased.</p></p class="citation"></blockquote><h3 id=56109--56346-is-it-a-free-lunch-for-removing-outliers-during-pretraining-baohao-liao-et-al-2024>(56/109 | 56/346) Is It a Free Lunch for Removing Outliers during Pretraining? (Baohao Liao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Baohao Liao, Christof Monz. (2024)<br><strong>Is It a Free Lunch for Removing Outliers during Pretraining?</strong><br><button class=copy-to-clipboard title="Is It a Free Lunch for Removing Outliers during Pretraining?" index=56>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-56 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Fine-tuning, Quantization, Quantization, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12102v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12102v1.pdf filename=2402.12102v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the growing size of <b>large</b> <b>language</b> <b>models,</b> the role of <b>quantization</b> becomes increasingly significant. However, outliers present in weights or activations notably influence the performance of <b>quantized</b> models. Recently, \citet{qtransformer} introduced a novel softmax function aimed at pretraining models in an outlier-free manner, thereby enhancing their suitability for <b>quantization.</b> Interestingly, we observed that such an approach leads to performance degradation in full precision. Building on this insight, we enhance the method by ensuring its normalization is invariant to sequence length, a crucial factor for bridging the gap between pretraining and <b>fine-tuning.</b> Moreover, this improved method also facilitates successful pretraining of causal language models.</p></p class="citation"></blockquote><h3 id=57109--57346-small-models-big-insights-leveraging-slim-proxy-models-to-decide-when-and-what-to-retrieve-for-llms-jiejun-tan-et-al-2024>(57/109 | 57/346) Small Models, Big Insights: Leveraging Slim Proxy Models To Decide When and What to Retrieve for LLMs (Jiejun Tan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiejun Tan, Zhicheng Dou, Yutao Zhu, Peidong Guo, Kun Fang, Ji-Rong Wen. (2024)<br><strong>Small Models, Big Insights: Leveraging Slim Proxy Models To Decide When and What to Retrieve for LLMs</strong><br><button class=copy-to-clipboard title="Small Models, Big Insights: Leveraging Slim Proxy Models To Decide When and What to Retrieve for LLMs" index=57>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-57 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Question Answering, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12052v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12052v2.pdf filename=2402.12052v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The integration of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> and search engines represents a significant evolution in knowledge acquisition methodologies. However, determining the knowledge that an <b>LLM</b> already possesses and the knowledge that requires the help of a search engine remains an unresolved issue. Most existing methods solve this problem through the results of preliminary answers or <b>reasoning</b> done by the <b>LLM</b> itself, but this incurs excessively high computational costs. This paper introduces a novel collaborative approach, namely SlimPLM, that detects missing knowledge in <b>LLMs</b> with a slim proxy model, to enhance the <b>LLM&rsquo;s</b> knowledge acquisition process. We employ a proxy model which has far fewer parameters, and take its answers as heuristic answers. Heuristic answers are then utilized to predict the knowledge required to answer the user <b>question,</b> <b>as</b> well as the known and unknown knowledge within the <b>LLM.</b> We only conduct retrieval for the missing knowledge in <b>questions</b> <b>that</b> the <b>LLM</b> does not know. Extensive experimental results on five datasets with two <b>LLMs</b> demonstrate a notable improvement in the end-to-end performance of <b>LLMs</b> in <b>question-answering</b> <b>tasks,</b> achieving or surpassing current state-of-the-art models with lower <b>LLM</b> inference costs.</p></p class="citation"></blockquote><h3 id=58109--58346-remember-this-event-that-year-assessing-temporal-information-and-reasoning-in-large-language-models-himanshu-beniwal-et-al-2024>(58/109 | 58/346) Remember This Event That Year? Assessing Temporal Information and Reasoning in Large Language Models (Himanshu Beniwal et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Himanshu Beniwal, Kowsik Nandagopan D, Mayank Singh. (2024)<br><strong>Remember This Event That Year? Assessing Temporal Information and Reasoning in Large Language Models</strong><br><button class=copy-to-clipboard title="Remember This Event That Year? Assessing Temporal Information and Reasoning in Large Language Models" index=58>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-58 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 40<br>Keywords: Fine-tuning, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11997v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11997v1.pdf filename=2402.11997v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> are increasingly becoming ubiquitous, yet their ability to reason about and retain temporal information remains limited. This hinders their application in real-world scenarios where understanding the sequential nature of events is crucial. This paper experiments with state-of-the-art models on a novel, <b>large-scale</b> <b>temporal</b> <b>dataset,</b> \textbf{TempUN}, to reveal significant limitations in temporal retention and <b>reasoning</b> abilities. Interestingly, closed-source models indicate knowledge gaps more frequently, potentially suggesting a trade-off between uncertainty awareness and incorrect responses. Further, exploring various <b>fine-tuning</b> approaches yielded no major performance improvements. The associated dataset and code are available at the following URL (<a href=https://github.com/lingoiitgn/TempUN)>https://github.com/lingoiitgn/TempUN)</a>.</p></p class="citation"></blockquote><h3 id=59109--59346-direct-large-language-model-alignment-through-self-rewarding-contrastive-prompt-distillation-aiwei-liu-et-al-2024>(59/109 | 59/346) Direct Large Language Model Alignment Through Self-Rewarding Contrastive Prompt Distillation (Aiwei Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Aiwei Liu, Haoping Bai, Zhiyun Lu, Xiang Kong, Simon Wang, Jiulong Shan, Meng Cao, Lijie Wen. (2024)<br><strong>Direct Large Language Model Alignment Through Self-Rewarding Contrastive Prompt Distillation</strong><br><button class=copy-to-clipboard title="Direct Large Language Model Alignment Through Self-Rewarding Contrastive Prompt Distillation" index=59>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-59 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: 68T50, I-2-7, cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Knowledge Distillation, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11907v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11907v1.pdf filename=2402.11907v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Aligning <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> with human expectations without human-annotated preference data is an important problem. In this paper, we propose a method to evaluate the response preference by using the output probabilities of response pairs under contrastive <b>prompt</b> pairs, which could achieve better performance on LLaMA2-7B and LLaMA2-13B compared to RLAIF. Based on this, we propose an automatic alignment method, Direct <b>Large</b> <b>Model</b> <b>Alignment</b> (DLMA). First, we use contrastive <b>prompt</b> pairs to automatically generate preference data. Then, we continue to evaluate the generated preference data using contrastive <b>prompt</b> pairs and calculate a self-rewarding score. Finally, we use the DPO algorithm to effectively align <b>LLMs</b> by combining this self-rewarding score. In the experimental stage, our DLMA method could surpass the \texttt{RLHF} method without relying on human-annotated preference data.</p></p class="citation"></blockquote><h3 id=60109--60346-the-colorful-future-of-llms-evaluating-and-improving-llms-as-emotional-supporters-for-queer-youth-shir-lissak-et-al-2024>(60/109 | 60/346) The Colorful Future of LLMs: Evaluating and Improving LLMs as Emotional Supporters for Queer Youth (Shir Lissak et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shir Lissak, Nitay Calderon, Geva Shenkman, Yaakov Ophir, Eyal Fruchter, Anat Brunstein Klomek, Roi Reichart. (2024)<br><strong>The Colorful Future of LLMs: Evaluating and Improving LLMs as Emotional Supporters for Queer Youth</strong><br><button class=copy-to-clipboard title="The Colorful Future of LLMs: Evaluating and Improving LLMs as Emotional Supporters for Queer Youth" index=60>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-60 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: ChatGPT, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11886v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11886v1.pdf filename=2402.11886v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Queer youth face increased mental health risks, such as depression, anxiety, and suicidal ideation. Hindered by negative stigma, they often avoid seeking help and rely on online resources, which may provide incompatible information. Although access to a supportive environment and reliable information is invaluable, many queer youth worldwide have no access to such support. However, this could soon change due to the rapid adoption of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> such as <b>ChatGPT.</b> This paper aims to comprehensively explore the potential of <b>LLMs</b> to revolutionize emotional support for queers. To this end, we conduct a qualitative and quantitative analysis of <b>LLM&rsquo;s</b> interactions with queer-related content. To evaluate response quality, we develop a novel ten-question scale that is inspired by psychological standards and expert input. We apply this scale to score several <b>LLMs</b> and human comments to posts where queer youth seek advice and share experiences. We find that <b>LLM</b> responses are supportive and inclusive, outscoring humans. However, they tend to be generic, not empathetic enough, and lack personalization, resulting in nonreliable and potentially harmful advice. We discuss these challenges, demonstrate that a dedicated <b>prompt</b> can improve the performance, and propose a blueprint of an <b>LLM-supporter</b> that actively (but sensitively) seeks user context to provide personalized, empathetic, and reliable responses. Our annotated dataset is available for further research.</p></p class="citation"></blockquote><h3 id=61109--61346-generation-meets-verification-accelerating-large-language-model-inference-with-smart-parallel-auto-correct-decoding-hanling-yi-et-al-2024>(61/109 | 61/346) Generation Meets Verification: Accelerating Large Language Model Inference with Smart Parallel Auto-Correct Decoding (Hanling Yi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hanling Yi, Feng Lin, Hongbin Li, Peiyang Ning, Xiaotian Yu, Rong Xiao. (2024)<br><strong>Generation Meets Verification: Accelerating Large Language Model Inference with Smart Parallel Auto-Correct Decoding</strong><br><button class=copy-to-clipboard title="Generation Meets Verification: Accelerating Large Language Model Inference with Smart Parallel Auto-Correct Decoding" index=61>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-61 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 40<br>Keywords: Fine-tuning, Supervised Learning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11809v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11809v1.pdf filename=2402.11809v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This research aims to accelerate the inference speed of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> with billions of parameters. We propose \textbf{S}mart \textbf{P}arallel \textbf{A}uto-\textbf{C}orrect d\textbf{E}coding (SPACE), an innovative approach designed for achieving lossless acceleration of <b>LLMs.</b> By integrating semi-autoregressive inference and speculative decoding capabilities, SPACE uniquely enables autoregressive <b>LLMs</b> to parallelize token generation and verification. This is realized through a specialized semi-autoregressive <b>supervised</b> <b>fine-tuning</b> process that equips existing <b>LLMs</b> with the ability to simultaneously predict multiple tokens. Additionally, an auto-correct decoding algorithm facilitates the simultaneous generation and verification of token sequences within a single model invocation. Through extensive experiments on a range of <b>LLMs,</b> SPACE has demonstrated inference speedup ranging from 2.7x-4.0x on HumanEval-X while maintaining output quality.</p></p class="citation"></blockquote><h3 id=62109--62346-unveiling-the-magic-investigating-attention-distillation-in-retrieval-augmented-generation-zizhong-li-et-al-2024>(62/109 | 62/346) Unveiling the Magic: Investigating Attention Distillation in Retrieval-augmented Generation (Zizhong Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zizhong Li, Haopeng Zhang, Jiawei Zhang. (2024)<br><strong>Unveiling the Magic: Investigating Attention Distillation in Retrieval-augmented Generation</strong><br><button class=copy-to-clipboard title="Unveiling the Magic: Investigating Attention Distillation in Retrieval-augmented Generation" index=62>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-62 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-IR, cs.CL<br>Keyword Score: 40<br>Keywords: Knowledge Distillation, Retrieval-Augmented Generation, Retrieval-Augmented Generation, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11794v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11794v1.pdf filename=2402.11794v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Retrieval-augmented</b> <b>generation</b> <b>framework</b> can address the limitations of <b>large</b> <b>language</b> <b>models</b> by enabling real-time knowledge updates for more accurate answers. An efficient way in the training phase of <b>retrieval-augmented</b> <b>models</b> <b>is</b> attention <b>distillation,</b> which uses attention scores as a supervision signal instead of manually annotated query-document pairs. Despite its growing popularity, the detailed mechanisms behind the success of attention <b>distillation</b> remain unexplored, particularly the specific patterns it leverages to benefit training. In this paper, we address this gap by conducting a comprehensive review of attention <b>distillation</b> workflow and identifying key factors influencing the learning quality of <b>retrieval-augmented</b> <b>language</b> <b>models.</b> We further propose indicators for optimizing models&rsquo; training methods and avoiding ineffective training.</p></p class="citation"></blockquote><h3 id=63109--63346-mars-meaning-aware-response-scoring-for-uncertainty-estimation-in-generative-llms-yavuz-faruk-bakman-et-al-2024>(63/109 | 63/346) MARS: Meaning-Aware Response Scoring for Uncertainty Estimation in Generative LLMs (Yavuz Faruk Bakman et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yavuz Faruk Bakman, Duygu Nur Yaldiz, Baturalp Buyukates, Chenyang Tao, Dimitrios Dimitriadis, Salman Avestimehr. (2024)<br><strong>MARS: Meaning-Aware Response Scoring for Uncertainty Estimation in Generative LLMs</strong><br><button class=copy-to-clipboard title="MARS: Meaning-Aware Response Scoring for Uncertainty Estimation in Generative LLMs" index=63>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-63 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keyword Score: 40<br>Keywords: Question Answering, Question Answering, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11756v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11756v2.pdf filename=2402.11756v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Generative <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> are widely utilized for their excellence in various tasks. However, their tendency to produce inaccurate or misleading outputs poses a potential risk, particularly in high-stakes environments. Therefore, estimating the correctness of generative <b>LLM</b> outputs is an important task for enhanced reliability. Uncertainty Estimation (UE) in generative <b>LLMs</b> is an evolving domain, where SOTA probability-based methods commonly employ length-normalized scoring. In this work, we propose Meaning-Aware Response Scoring (MARS) as an alternative to length-normalized scoring for UE methods. MARS is a novel scoring function that considers the semantic contribution of each token in the generated sequence in the context of the <b>question.</b> <b>We</b> demonstrate that integrating MARS into UE methods results in a universal and significant improvement in UE performance. We conduct experiments using three distinct closed-book <b>question-answering</b> <b>datasets</b> across five popular pre-trained <b>LLMs.</b> Lastly, we validate the efficacy of MARS on a Medical <b>QA</b> dataset. Code can be found <a href=https://github.com/Ybakman/LLM_Uncertainity>https://github.com/Ybakman/LLM_Uncertainity</a>.</p></p class="citation"></blockquote><h3 id=64109--64346-ontology-enhanced-claim-detection-zehra-melce-hüsünbeyi-et-al-2024>(64/109 | 64/346) Ontology Enhanced Claim Detection (Zehra Melce Hüsünbeyi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zehra Melce Hüsünbeyi, Tatjana Scheffler. (2024)<br><strong>Ontology Enhanced Claim Detection</strong><br><button class=copy-to-clipboard title="Ontology Enhanced Claim Detection" index=64>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-64 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 38<br>Keywords: Graph, Knowledge Graph, BERT, Sentence Embedding, Word Embedding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12282v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12282v1.pdf filename=2402.12282v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We propose an ontology enhanced model for <b>sentence</b> <b>based</b> claim detection. We fused ontology embeddings from a <b>knowledge</b> <b>base</b> with <b>BERT</b> <b>sentence</b> <b>embeddings</b> to perform claim detection for the ClaimBuster and the NewsClaims datasets. Our ontology enhanced approach showed the best results with these small-sized unbalanced datasets, compared to other statistical and neural machine learning models. The experiments demonstrate that adding domain specific features (either trained <b>word</b> <b>embeddings</b> or <b>knowledge</b> <b>graph</b> metadata) can improve traditional ML methods. In addition, adding domain <b>knowledge</b> <b>in</b> the form of ontology embeddings helps avoid the bias encountered in neural network based models, for example the pure <b>BERT</b> model bias towards larger classes in our small corpus.</p></p class="citation"></blockquote><h3 id=65109--65346-analobench-benchmarking-the-identification-of-abstract-and-long-context-analogies-xiao-ye-et-al-2024>(65/109 | 65/346) AnaloBench: Benchmarking the Identification of Abstract and Long-context Analogies (Xiao Ye et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiao Ye, Andrew Wang, Jacob Choi, Yining Lu, Shreya Sharma, Lingfeng Shen, Vijay Tiyyala, Nicholas Andrews, Daniel Khashabi. (2024)<br><strong>AnaloBench: Benchmarking the Identification of Abstract and Long-context Analogies</strong><br><button class=copy-to-clipboard title="AnaloBench: Benchmarking the Identification of Abstract and Long-context Analogies" index=65>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-65 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 36<br>Keywords: Benchmarking, Benchmarking, Claude, GPT, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12370v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12370v1.pdf filename=2402.12370v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Humans regularly engage in analogical thinking, relating personal experiences to current situations ($X$ is analogous to $Y$ because of $Z$). Analogical thinking allows humans to solve problems in creative ways, grasp difficult concepts, and articulate ideas more effectively. Can language models (LMs) do the same? To answer this question, we propose ANALOBENCH, a <b>benchmark</b> to determine analogical <b>reasoning</b> ability in LMs. Our <b>benchmarking</b> approach focuses on aspects of this ability that are common among humans: (i) recalling related experiences from a large amount of information, and (ii) applying analogical <b>reasoning</b> to complex and lengthy scenarios. We test a broad collection of proprietary models (e.g., <b>GPT</b> family, <b>Claude</b> V2) and open source models such as LLaMA2. As in prior results, scaling up LMs results in some performance boosts. Surprisingly, scale offers minimal gains when, (i) analogies involve lengthy scenarios, or (ii) recalling relevant scenarios from a large pool of information, a process analogous to finding a needle in a haystack. We hope these observations encourage further research in this field.</p></p class="citation"></blockquote><h3 id=66109--66346-lemma-towards-lvlm-enhanced-multimodal-misinformation-detection-with-external-knowledge-augmentation-keyang-xuan-et-al-2024>(66/109 | 66/346) LEMMA: Towards LVLM-Enhanced Multimodal Misinformation Detection with External Knowledge Augmentation (Keyang Xuan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Keyang Xuan, Li Yi, Fan Yang, Ruochen Wu, Yi R. Fung, Heng Ji. (2024)<br><strong>LEMMA: Towards LVLM-Enhanced Multimodal Misinformation Detection with External Knowledge Augmentation</strong><br><button class=copy-to-clipboard title="LEMMA: Towards LVLM-Enhanced Multimodal Misinformation Detection with External Knowledge Augmentation" index=66>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-66 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 36<br>Keywords: Multi-modal, Multi-modal, Reasoning, Large Language Model, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11943v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11943v1.pdf filename=2402.11943v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The rise of <b>multimodal</b> misinformation on social platforms poses significant challenges for individuals and societies. Its increased credibility and broader impact compared to textual misinformation make detection complex, requiring robust <b>reasoning</b> across diverse media types and profound knowledge for accurate verification. The emergence of Large Vision Language Model (LVLM) offers a potential solution to this problem. Leveraging their proficiency in processing visual and textual information, LVLM demonstrates promising capabilities in recognizing complex information and exhibiting strong <b>reasoning</b> skills. In this paper, we first investigate the potential of LVLM on <b>multimodal</b> misinformation detection. We find that even though LVLM has a superior performance compared to <b>LLMs,</b> its profound <b>reasoning</b> may present limited power with a lack of evidence. Based on these observations, we propose LEMMA: LVLM-Enhanced <b>Multimodal</b> Misinformation Detection with External Knowledge Augmentation. LEMMA leverages LVLM intuition and <b>reasoning</b> capabilities while augmenting them with external knowledge to enhance the accuracy of misinformation detection. Our method improves the accuracy over the top baseline LVLM by 7% and 13% on Twitter and Fakeddit datasets respectively.</p></p class="citation"></blockquote><h3 id=67109--67346-shallow-synthesis-of-knowledge-in-gpt-generated-texts-a-case-study-in-automatic-related-work-composition-anna-martin-boyle-et-al-2024>(67/109 | 67/346) Shallow Synthesis of Knowledge in GPT-Generated Texts: A Case Study in Automatic Related Work Composition (Anna Martin-Boyle et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Anna Martin-Boyle, Aahan Tyagi, Marti A. Hearst, Dongyeop Kang. (2024)<br><strong>Shallow Synthesis of Knowledge in GPT-Generated Texts: A Case Study in Automatic Related Work Composition</strong><br><button class=copy-to-clipboard title="Shallow Synthesis of Knowledge in GPT-Generated Texts: A Case Study in Automatic Related Work Composition" index=67>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-67 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 33<br>Keywords: Graph, Human Intervention, GPT, GPT-4<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12255v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12255v1.pdf filename=2402.12255v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Numerous AI-assisted scholarly applications have been developed to aid different stages of the research process. We present an analysis of AI-assisted scholarly writing generated with ScholaCite, a tool we built that is designed for organizing literature and composing Related Work sections for academic papers. Our evaluation method focuses on the analysis of citation <b>graphs</b> to assess the structural complexity and inter-connectedness of citations in texts and involves a three-way comparison between (1) original <b>human-written</b> <b>texts,</b> (2) purely <b>GPT-generated</b> texts, and (3) <b>human-AI</b> <b>collaborative</b> texts. We find that <b>GPT-4</b> can generate reasonable coarse-grained citation groupings to support <b>human</b> <b>users</b> in brainstorming, but fails to perform detailed synthesis of related works without <b>human</b> <b>intervention.</b> We suggest that future writing assistant tools should not be used to draft text independently of the <b>human</b> <b>author.</b></p></p class="citation"></blockquote><h3 id=68109--68346-understanding-the-effects-of-noise-in-text-to-sql-an-examination-of-the-bird-bench-benchmark-niklas-wretblad-et-al-2024>(68/109 | 68/346) Understanding the Effects of Noise in Text-to-SQL: An Examination of the BIRD-Bench Benchmark (Niklas Wretblad et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Niklas Wretblad, Fredrik Gordh Riseby, Rahul Biswas, Amin Ahmadi, Oskar Holmström. (2024)<br><strong>Understanding the Effects of Noise in Text-to-SQL: An Examination of the BIRD-Bench Benchmark</strong><br><button class=copy-to-clipboard title="Understanding the Effects of Noise in Text-to-SQL: An Examination of the BIRD-Bench Benchmark" index=68>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-68 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 33<br>Keywords: Benchmarking, Zero-shot, Text2SQL, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12243v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12243v2.pdf filename=2402.12243v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Text-to-SQL,</b> which involves translating natural language into Structured Query Language (SQL), is crucial for enabling broad access to structured databases without expert knowledge. However, designing models for such tasks is challenging due to numerous factors, including the presence of &rsquo;noise,&rsquo; such as ambiguous questions and syntactical errors. This study provides an in-depth analysis of the distribution and types of noise in the widely used BIRD-Bench <b>benchmark</b> and the impact of noise on models. While BIRD-Bench was created to model dirty and noisy database values, it was not created to contain noise and errors in the questions and gold queries. We found that noise in questions and gold queries are prevalent in the dataset, with varying amounts across domains, and with an uneven distribution between noise types. The presence of incorrect gold SQL queries, which then generate incorrect gold answers, has a significant impact on the <b>benchmark&rsquo;s</b> reliability. Surprisingly, when evaluating models on corrected SQL queries, <b>zero-shot</b> baselines surpassed the performance of state-of-the-art <b>prompting</b> methods. We conclude that informative noise labels and reliable <b>benchmarks</b> are crucial to developing new <b>Text-to-SQL</b> methods that can handle varying types of noise.</p></p class="citation"></blockquote><h3 id=69109--69346-a-systematic-comparison-of-contextualized-word-embeddings-for-lexical-semantic-change-francesco-periti-et-al-2024>(69/109 | 69/346) A Systematic Comparison of Contextualized Word Embeddings for Lexical Semantic Change (Francesco Periti et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Francesco Periti, Nina Tahmasebi. (2024)<br><strong>A Systematic Comparison of Contextualized Word Embeddings for Lexical Semantic Change</strong><br><button class=copy-to-clipboard title="A Systematic Comparison of Contextualized Word Embeddings for Lexical Semantic Change" index=69>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-69 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 33<br>Keywords: Benchmarking, GPT, GPT-4, Word Embedding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12011v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12011v2.pdf filename=2402.12011v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Contextualized embeddings are the preferred tool for modeling Lexical Semantic Change (LSC). Current evaluations typically focus on a specific task known as Graded Change Detection (GCD). However, performance comparison across work are often misleading due to their reliance on diverse settings. In this paper, we evaluate state-of-the-art models and approaches for GCD under equal conditions. We further break the LSC problem into <b>Word-in-Context</b> <b>(WiC)</b> and <b>Word</b> <b>Sense</b> Induction (WSI) tasks, and compare models across these different levels. Our evaluation is performed across different languages on eight available <b>benchmarks</b> for LSC, and shows that (i) APD outperforms other approaches for GCD; (ii) XL-LEXEME outperforms other contextualized models for WiC, WSI, and GCD, while being comparable to <b>GPT-4;</b> (iii) there is a clear need for improving the modeling of <b>word</b> <b>meanings,</b> as well as focus on how, when, and why these meanings change, rather than solely focusing on the extent of semantic change.</p></p class="citation"></blockquote><h3 id=70109--70346-learning-to-edit-aligning-llms-with-knowledge-editing-yuxin-jiang-et-al-2024>(70/109 | 70/346) Learning to Edit: Aligning LLMs with Knowledge Editing (Yuxin Jiang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuxin Jiang, Yufei Wang, Chuhan Wu, Wanjun Zhong, Xingshan Zeng, Jiahui Gao, Liangyou Li, Xin Jiang, Lifeng Shang, Ruiming Tang, Qun Liu, Wei Wang. (2024)<br><strong>Learning to Edit: Aligning LLMs with Knowledge Editing</strong><br><button class=copy-to-clipboard title="Learning to Edit: Aligning LLMs with Knowledge Editing" index=70>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-70 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 33<br>Keywords: Benchmarking, Fine-tuning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11905v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11905v1.pdf filename=2402.11905v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Knowledge editing techniques, aiming to efficiently modify a minor proportion of knowledge in <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> without negatively impacting performance across other inputs, have garnered widespread attention. However, existing methods predominantly rely on memorizing the updated knowledge, impeding <b>LLMs</b> from effectively combining the new knowledge with their inherent knowledge when answering questions. To this end, we propose a Learning to Edit (LTE) framework, focusing on teaching <b>LLMs</b> to apply updated knowledge into input questions, inspired by the philosophy of &ldquo;Teach a man to fish.&rdquo; LTE features a two-phase process: (i) the Alignment Phase, which <b>fine-tunes</b> <b>LLMs</b> on a meticulously curated parallel dataset to make reliable, in-scope edits while preserving out-of-scope information and linguistic proficiency; and (ii) the Inference Phase, which employs a retrieval-based mechanism for real-time and mass knowledge editing. By comparing our approach with seven advanced baselines across four popular knowledge editing <b>benchmarks</b> and two <b>LLM</b> architectures, we demonstrate LTE&rsquo;s superiority in knowledge editing performance, robustness in both batch and sequential editing, minimal interference on general tasks, and rapid editing speeds. The data and code are available at <a href=https://github.com/YJiangcm/LTE>https://github.com/YJiangcm/LTE</a>.</p></p class="citation"></blockquote><h3 id=71109--71346-genaudit-fixing-factual-errors-in-language-model-outputs-with-evidence-kundan-krishna-et-al-2024>(71/109 | 71/346) GenAudit: Fixing Factual Errors in Language Model Outputs with Evidence (Kundan Krishna et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kundan Krishna, Sanjana Ramprasad, Prakhar Gupta, Byron C. Wallace, Zachary C. Lipton, Jeffrey P. Bigham. (2024)<br><strong>GenAudit: Fixing Factual Errors in Language Model Outputs with Evidence</strong><br><button class=copy-to-clipboard title="GenAudit: Fixing Factual Errors in Language Model Outputs with Evidence" index=71>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-71 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keyword Score: 30<br>Keywords: Fact Verification, Question Answering, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12566v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12566v1.pdf filename=2402.12566v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>LLMs</b> can generate factually incorrect statements even when provided access to reference documents. Such errors can be dangerous in high-stakes applications (e.g., document-grounded <b>QA</b> for healthcare or finance). We present GenAudit &ndash; a tool intended to assist <b>fact-checking</b> <b>LLM</b> responses for document-grounded tasks. GenAudit suggests edits to the <b>LLM</b> response by revising or removing claims that are not supported by the reference document, and also presents evidence from the reference for <b>facts</b> <b>that</b> do appear to have support. We train models to execute these tasks, and design an interactive interface to present suggested edits and evidence to users. Comprehensive evaluation by human raters shows that GenAudit can detect errors in 8 different <b>LLM</b> outputs when summarizing documents from diverse domains. To ensure that most errors are flagged by the system, we propose a method that can increase the error recall while minimizing impact on precision. We will release our tool (GenAudit) and <b>fact-checking</b> <b>model</b> for public use.</p></p class="citation"></blockquote><h3 id=72109--72346-confidence-matters-revisiting-intrinsic-self-correction-capabilities-of-large-language-models-loka-li-et-al-2024>(72/109 | 72/346) Confidence Matters: Revisiting Intrinsic Self-Correction Capabilities of Large Language Models (Loka Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Loka Li, Guangyi Chen, Yusheng Su, Zhenhao Chen, Yixuan Zhang, Eric Xing, Kun Zhang. (2024)<br><strong>Confidence Matters: Revisiting Intrinsic Self-Correction Capabilities of Large Language Models</strong><br><button class=copy-to-clipboard title="Confidence Matters: Revisiting Intrinsic Self-Correction Capabilities of Large Language Models" index=72>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-72 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12563v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12563v1.pdf filename=2402.12563v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The recent success of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> has catalyzed an increasing interest in their self-correction capabilities. This paper presents a comprehensive investigation into the intrinsic self-correction of <b>LLMs,</b> attempting to address the ongoing debate about its feasibility. Our research has identified an important latent factor - the <code>confidence'' of &lt;b>LLMs&lt;/b> - during the self-correction process. Overlooking this factor may cause the models to over-criticize themselves, resulting in unreliable conclusions regarding the efficacy of self-correction. We have experimentally observed that &lt;b>LLMs&lt;/b> possess the capability to understand the </code>confidence&rsquo;&rsquo; in their own responses. It motivates us to develop an <code>If-or-Else'' (IoE) &lt;b>prompting&lt;/b> framework, designed to guide &lt;b>LLMs&lt;/b> in assessing their own </code>confidence&rsquo;&rsquo;, facilitating intrinsic self-corrections. We conduct extensive experiments and demonstrate that our IoE-based <b>Prompt</b> can achieve a consistent improvement regarding the accuracy of self-corrected responses over the initial answers. Our study not only sheds light on the underlying factors affecting self-correction in <b>LLMs,</b> but also introduces a practical framework that utilizes the IoE <b>prompting</b> principle to efficiently improve self-correction capabilities with ``confidence&rsquo;&rsquo;. The code is available at \url{https://github.com/MBZUAI-CLeaR/IoE-Prompting.git}.</p></p class="citation"></blockquote><h3 id=73109--73346-emergent-word-order-universals-from-cognitively-motivated-language-models-tatsuki-kuribayashi-et-al-2024>(73/109 | 73/346) Emergent Word Order Universals from Cognitively-Motivated Language Models (Tatsuki Kuribayashi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tatsuki Kuribayashi, Ryo Ueda, Ryo Yoshida, Yohei Oseki, Ted Briscoe, Timothy Baldwin. (2024)<br><strong>Emergent Word Order Universals from Cognitively-Motivated Language Models</strong><br><button class=copy-to-clipboard title="Emergent Word Order Universals from Cognitively-Motivated Language Models" index=73>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-73 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Simulation, Simulator, Perplexity<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12363v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12363v1.pdf filename=2402.12363v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The world&rsquo;s languages exhibit certain so-called typological or implicational universals; for example, Subject-Object-Verb (SOV) word order typically employs postpositions. Explaining the source of such biases is a key goal in linguistics. We study the word-order universals through a computational <b>simulation</b> with language models (LMs). Our experiments show that typologically typical word orders tend to have lower <b>perplexity</b> estimated by LMs with cognitively plausible biases: syntactic biases, specific parsing strategies, and memory limitations. This suggests that the interplay of these cognitive biases and predictability <b>(perplexity)</b> can explain many aspects of word-order universals. This also showcases the advantage of cognitively-motivated LMs, which are typically employed in cognitive modeling, in the computational <b>simulation</b> of language universals.</p></p class="citation"></blockquote><h3 id=74109--74346-high-quality-data-to-text-generation-for-severely-under-resourced-languages-with-out-of-the-box-large-language-models-michela-lorandi-et-al-2024>(74/109 | 74/346) High-quality Data-to-Text Generation for Severely Under-Resourced Languages with Out-of-the-box Large Language Models (Michela Lorandi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Michela Lorandi, Anya Belz. (2024)<br><strong>High-quality Data-to-Text Generation for Severely Under-Resourced Languages with Out-of-the-box Large Language Models</strong><br><button class=copy-to-clipboard title="High-quality Data-to-Text Generation for Severely Under-Resourced Languages with Out-of-the-box Large Language Models" index=74>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-74 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: BLEU, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12267v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12267v1.pdf filename=2402.12267v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The performance of NLP methods for severely under-resourced languages cannot currently hope to match the state of the art in NLP methods for well resourced languages. We explore the extent to which pretrained <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> can bridge this gap, via the example of data-to-text generation for Irish, Welsh, Breton and Maltese. We test <b>LLMs</b> on these under-resourced languages and English, in a range of scenarios. We find that <b>LLMs</b> easily set the state of the art for the under-resourced languages by substantial margins, as measured by both automatic and human evaluations. For all our languages, human evaluation shows on-a-par performance with humans for our best systems, but <b>BLEU</b> scores collapse compared to English, casting doubt on the metric&rsquo;s suitability for evaluating non-task-specific systems. Overall, our results demonstrate the great potential of <b>LLMs</b> to bridge the performance gap for under-resourced languages.</p></p class="citation"></blockquote><h3 id=75109--75346-empirical-study-on-updating-key-value-memories-in-transformer-feed-forward-layers-zihan-qiu-et-al-2024>(75/109 | 75/346) Empirical Study on Updating Key-Value Memories in Transformer Feed-forward Layers (Zihan Qiu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zihan Qiu, Zeyu Huang, Youcheng Huang, Jie Fu. (2024)<br><strong>Empirical Study on Updating Key-Value Memories in Transformer Feed-forward Layers</strong><br><button class=copy-to-clipboard title="Empirical Study on Updating Key-Value Memories in Transformer Feed-forward Layers" index=75>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-75 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Fine-tuning, Transformer, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12233v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12233v1.pdf filename=2402.12233v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The feed-forward networks (FFNs) in <b>transformers</b> are recognized as a group of key-value neural memories to restore abstract high-level knowledge. In this work, we conduct an empirical ablation study on updating keys (the 1st layer in the FFNs layer) or values (the 2nd layer in the FFNs layer). We compare those two methods in various knowledge editing and <b>fine-tuning</b> tasks of <b>large</b> <b>language</b> <b>models</b> to draw insights to understand FFNs further. Code is available at $\href{https://github.com/qiuzh20/Tuning-keys-v.s.-values}{this,repo}$.</p></p class="citation"></blockquote><h3 id=76109--76346-groot-adversarial-testing-for-generative-text-to-image-models-with-tree-based-semantic-transformation-yi-liu-et-al-2024>(76/109 | 76/346) Groot: Adversarial Testing for Generative Text-to-Image Models with Tree-based Semantic Transformation (Yi Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yi Liu, Guowei Yang, Gelei Deng, Feiyue Chen, Yuqi Chen, Ling Shi, Tianwei Zhang, Yang Liu. (2024)<br><strong>Groot: Adversarial Testing for Generative Text-to-Image Models with Tree-based Semantic Transformation</strong><br><button class=copy-to-clipboard title="Groot: Adversarial Testing for Generative Text-to-Image Models with Tree-based Semantic Transformation" index=76>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-76 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-CR, cs-SE, cs.CL<br>Keyword Score: 30<br>Keywords: Text2image, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12100v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12100v1.pdf filename=2402.12100v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the prevalence of <b>text-to-image</b> generative models, their safety becomes a critical concern. adversarial testing techniques have been developed to probe whether such models can be <b>prompted</b> to produce Not-Safe-For-Work (NSFW) content. However, existing solutions face several challenges, including low success rate and inefficiency. We introduce Groot, the first automated framework leveraging tree-based semantic transformation for adversarial testing of <b>text-to-image</b> models. Groot employs semantic decomposition and sensitive element drowning strategies in conjunction with <b>LLMs</b> to systematically refine adversarial <b>prompts.</b> Our comprehensive evaluation confirms the efficacy of Groot, which not only exceeds the performance of current state-of-the-art approaches but also achieves a remarkable success rate (93.66%) on leading <b>text-to-image</b> models such as DALL-E 3 and Midjourney.</p></p class="citation"></blockquote><h3 id=77109--77346-can-llms-compute-with-reasons-harshit-sandilya-et-al-2024>(77/109 | 77/346) Can LLMs Compute with Reasons? (Harshit Sandilya et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Harshit Sandilya, Peehu Raj, Jainit Sushil Bafna, Srija Mukhopadhyay, Shivansh Sharma, Ellwil Sharma, Arastu Sharma, Neeta Trivedi, Manish Shrivastava, Rajesh Kumar. (2024)<br><strong>Can LLMs Compute with Reasons?</strong><br><button class=copy-to-clipboard title="Can LLMs Compute with Reasons?" index=77>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-77 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: 68T50, I-2-7, cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12080v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12080v1.pdf filename=2402.12080v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> often struggle with complex mathematical tasks, prone to &ldquo;hallucinating&rdquo; incorrect answers due to their reliance on statistical patterns. This limitation is further amplified in average Small LangSLMs with limited context and training data. To address this challenge, we propose an &ldquo;Inductive Learning&rdquo; approach utilizing a distributed network of SLMs. This network leverages error-based learning and hint incorporation to refine the <b>reasoning</b> capabilities of SLMs. Our goal is to provide a framework that empowers SLMs to approach the level of logic-based applications achieved by high-parameter models, potentially benefiting any language model. Ultimately, this novel concept paves the way for bridging the logical gap between humans and <b>LLMs</b> across various fields.</p></p class="citation"></blockquote><h3 id=78109--78346-are-llm-based-evaluators-confusing-nlg-quality-criteria-xinyu-hu-et-al-2024>(78/109 | 78/346) Are LLM-based Evaluators Confusing NLG Quality Criteria? (Xinyu Hu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xinyu Hu, Mingqi Gao, Sen Hu, Yang Zhang, Yicheng Chen, Teng Xu, Xiaojun Wan. (2024)<br><strong>Are LLM-based Evaluators Confusing NLG Quality Criteria?</strong><br><button class=copy-to-clipboard title="Are LLM-based Evaluators Confusing NLG Quality Criteria?" index=78>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-78 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Natural Language Generation, Large Language Model, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12055v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12055v1.pdf filename=2402.12055v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Some prior work has shown that <b>LLMs</b> perform well in <b>NLG</b> evaluation for different tasks. However, we discover that <b>LLMs</b> seem to confuse different evaluation criteria, which reduces their reliability. For further verification, we first consider avoiding issues of inconsistent conceptualization and vague expression in existing <b>NLG</b> quality criteria themselves. So we <b>summarize</b> a clear hierarchical classification system for 11 common aspects with corresponding different criteria from previous studies involved. Inspired by behavioral testing, we elaborately design 18 types of aspect-targeted perturbation attacks for fine-grained analysis of the evaluation behaviors of different <b>LLMs.</b> We also conduct human annotations beyond the guidance of the classification system to validate the impact of the perturbations. Our experimental results reveal confusion issues inherent in <b>LLMs,</b> as well as other noteworthy phenomena, and necessitate further research and improvements for <b>LLM-based</b> evaluation.</p></p class="citation"></blockquote><h3 id=79109--79346-automatic-evaluation-for-mental-health-counseling-using-llms-anqi-li-et-al-2024>(79/109 | 79/346) Automatic Evaluation for Mental Health Counseling using LLMs (Anqi Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Anqi Li, Yu Lu, Nirui Song, Shuai Zhang, Lizhi Ma, Zhenzhong Lan. (2024)<br><strong>Automatic Evaluation for Mental Health Counseling using LLMs</strong><br><button class=copy-to-clipboard title="Automatic Evaluation for Mental Health Counseling using LLMs" index=79>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-79 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Automatic Evaluation, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11958v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11958v1.pdf filename=2402.11958v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>High-quality psychological counseling is crucial for mental health worldwide, and timely evaluation is vital for ensuring its effectiveness. However, obtaining professional evaluation for each counseling session is expensive and challenging. Existing methods that rely on self or third-party manual reports to assess the quality of counseling suffer from subjective biases and limitations of time-consuming. To address above challenges, this paper proposes an innovative and efficient <b>automatic</b> <b>approach</b> using <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> to evaluate the working alliance in counseling conversations. We collected a comprehensive counseling dataset and conducted multiple third-party evaluations based on therapeutic relationship theory. Our <b>LLM-based</b> evaluation, combined with our guidelines, shows high agreement with human evaluations and provides valuable insights into counseling scripts. This highlights the potential of <b>LLMs</b> as supervisory tools for psychotherapists. By integrating <b>LLMs</b> into the evaluation process, our approach offers a cost-effective and dependable means of assessing counseling quality, enhancing overall effectiveness.</p></p class="citation"></blockquote><h3 id=80109--80346-sola-solver-layer-adaption-of-llm-for-better-logic-reasoning-yu-zhang-et-al-2024>(80/109 | 80/346) SoLA: Solver-Layer Adaption of LLM for Better Logic Reasoning (Yu Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yu Zhang, Hui-Ling Zhen, Zehua Pei, Yingzhao Lian, Lihao Yin, Mingxuan Yuan, Bei Yu. (2024)<br><strong>SoLA: Solver-Layer Adaption of LLM for Better Logic Reasoning</strong><br><button class=copy-to-clipboard title="SoLA: Solver-Layer Adaption of LLM for Better Logic Reasoning" index=80>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-80 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11903v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11903v1.pdf filename=2402.11903v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Considering the challenges faced by <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> on logical <b>reasoning,</b> prior efforts have sought to transform problem-solving through tool learning. While progress has been made on small-scale problems, solving industrial cases remains difficult due to their <b>large</b> <b>scale</b> <b>and</b> intricate expressions. In this paper, we propose a novel solver-layer adaptation (SoLA) method, where we introduce a solver as a new layer of the <b>LLM</b> to differentially guide solutions towards satisfiability. In SoLA, <b>LLM</b> aims to comprehend the search space described in natural language and identify local solutions of the highest quality, while the solver layer focuses solely on constraints not satisfied by the initial solution. Leveraging MaxSAT as a bridge, we define forward and backward transfer gradients, enabling the final model to converge to a satisfied solution or prove unsatisfiability. The backdoor theory ensures that SoLA can obtain accurate solutions within polynomial loops. We evaluate the performance of SoLA on various datasets and empirically demonstrate its consistent outperformance against existing symbolic solvers (including Z3 and Kissat) and tool-learning methods in terms of efficiency in large-scale problem-solving.</p></p class="citation"></blockquote><h3 id=81109--81346-revisiting-knowledge-distillation-for-autoregressive-language-models-qihuang-zhong-et-al-2024>(81/109 | 81/346) Revisiting Knowledge Distillation for Autoregressive Language Models (Qihuang Zhong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qihuang Zhong, Liang Ding, Li Shen, Juhua Liu, Bo Du, Dacheng Tao. (2024)<br><strong>Revisiting Knowledge Distillation for Autoregressive Language Models</strong><br><button class=copy-to-clipboard title="Revisiting Knowledge Distillation for Autoregressive Language Models" index=81>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-81 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Knowledge Distillation, Knowledge Distillation, Knowledge Distillation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11890v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11890v1.pdf filename=2402.11890v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Knowledge</b> <b>distillation</b> <b>(KD)</b> is a common approach to compress a teacher model to reduce its inference cost and memory footprint, by training a smaller student model. However, in the context of autoregressive language models (LMs), we empirically find that larger teacher LMs might dramatically result in a poorer student. In response to this problem, we conduct a series of analyses and reveal that different tokens have different teaching modes, neglecting which will lead to performance degradation. Motivated by this, we propose a simple yet effective adaptive teaching approach (ATKD) to improve the <b>KD.</b> The core of ATKD is to reduce rote learning and make teaching more diverse and flexible. Extensive experiments on 8 LM tasks show that, with the help of ATKD, various baseline <b>KD</b> methods can achieve consistent and significant performance gains (up to +3.04% average score) across all model types and sizes. More encouragingly, ATKD can improve the student model generalization effectively.</p></p class="citation"></blockquote><h3 id=82109--82346-rose-doesnt-do-that-boosting-the-safety-of-instruction-tuned-large-language-models-with-reverse-prompt-contrastive-decoding-qihuang-zhong-et-al-2024>(82/109 | 82/346) ROSE Doesn&rsquo;t Do That: Boosting the Safety of Instruction-Tuned Large Language Models with Reverse Prompt Contrastive Decoding (Qihuang Zhong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qihuang Zhong, Liang Ding, Juhua Liu, Bo Du, Dacheng Tao. (2024)<br><strong>ROSE Doesn&rsquo;t Do That: Boosting the Safety of Instruction-Tuned Large Language Models with Reverse Prompt Contrastive Decoding</strong><br><button class=copy-to-clipboard title="ROSE Doesn't Do That: Boosting the Safety of Instruction-Tuned Large Language Models with Reverse Prompt Contrastive Decoding" index=82>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-82 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11889v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11889v1.pdf filename=2402.11889v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the development of instruction-tuned <b>large</b> <b>language</b> <b>models</b> <b>(LLMs),</b> improving the safety of <b>LLMs</b> has become more critical. However, the current approaches for aligning the <b>LLMs</b> output with expected safety usually require substantial training efforts, e.g., high-quality safety data and expensive computational resources, which are costly and inefficient. To this end, we present reverse <b>prompt</b> contrastive decoding (ROSE), a simple-yet-effective method to directly boost the safety of existing instruction-tuned <b>LLMs</b> without any additional training. The principle of ROSE is to improve the probability of desired safe output via suppressing the undesired output induced by the carefully-designed reverse <b>prompts.</b> Experiments on 6 safety and 2 general-purpose tasks show that, our ROSE not only brings consistent and significant safety improvements (up to +13.8% safety score) upon 5 types of instruction-tuned <b>LLMs,</b> but also benefits the general-purpose ability of <b>LLMs.</b> In-depth analyses explore the underlying mechanism of ROSE, and reveal when and where to use it.</p></p class="citation"></blockquote><h3 id=83109--83346-head-wise-shareable-attention-for-large-language-models-zouying-cao-et-al-2024>(83/109 | 83/346) Head-wise Shareable Attention for Large Language Models (Zouying Cao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zouying Cao, Yifei Yang, Hai Zhao. (2024)<br><strong>Head-wise Shareable Attention for Large Language Models</strong><br><button class=copy-to-clipboard title="Head-wise Shareable Attention for Large Language Models" index=83>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-83 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: BERT, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11819v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11819v1.pdf filename=2402.11819v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> suffer from huge number of parameters, which restricts their deployment on edge devices. Weight sharing is one promising solution that encourages weight reuse, effectively reducing memory usage with less performance drop. However, current weight sharing techniques primarily focus on small-scale models like <b>BERT</b> and employ coarse-grained sharing rules, e.g., layer-wise. This becomes limiting given the prevalence of <b>LLMs</b> and sharing an entire layer or block obviously diminishes the flexibility of weight sharing. In this paper, we present a perspective on $\textit{$\textbf{head-wise shareable attention for <b>large</b> <b>language</b> <b>models}$}$.</b> We further propose two memory-efficient methods that share parameters across attention heads, with a specific focus on <b>LLMs.</b> Both of them use the same dynamic strategy to select the shared weight matrices. The first method directly reuses the pre-trained weights without retraining, denoted as $\textbf{DirectShare}$. The second method first post-trains with constraint on weight matrix similarity and then shares, denoted as $\textbf{PostShare}$. Experimental results reveal our head-wise shared models still maintain satisfactory capabilities, demonstrating the feasibility of fine-grained weight sharing applied to <b>LLMs.</b></p></p class="citation"></blockquote><h3 id=84109--84346-what-evidence-do-language-models-find-convincing-alexander-wan-et-al-2024>(84/109 | 84/346) What Evidence Do Language Models Find Convincing? (Alexander Wan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Alexander Wan, Eric Wallace, Dan Klein. (2024)<br><strong>What Evidence Do Language Models Find Convincing?</strong><br><button class=copy-to-clipboard title="What Evidence Do Language Models Find Convincing?" index=84>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-84 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keyword Score: 30<br>Keywords: Counter-factual, Retrieval-Augmented Generation, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11782v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11782v1.pdf filename=2402.11782v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Retrieval-augmented language models are being increasingly tasked with subjective, contentious, and conflicting queries such as &ldquo;is aspartame linked to cancer&rdquo;. To resolve these ambiguous queries, one must search through a large range of websites and consider &ldquo;which, if any, of this evidence do I find convincing?&rdquo;. In this work, we study how <b>LLMs</b> answer this question. In particular, we construct ConflictingQA, a dataset that pairs controversial queries with a series of real-world evidence documents that contain different facts (e.g., quantitative results), argument styles (e.g., appeals to authority), and answers (Yes or No). We use this dataset to perform sensitivity and <b>counterfactual</b> analyses to explore which text features most affect <b>LLM</b> predictions. Overall, we find that current models rely heavily on the relevance of a website to the query, while largely ignoring stylistic features that humans find important such as whether a text contains scientific references or is written with a neutral tone. Taken together, these results highlight the importance of <b>RAG</b> corpus quality (e.g., the need to filter misinformation), and possibly even a shift in how <b>LLMs</b> are trained to better align with human judgements.</p></p class="citation"></blockquote><h3 id=85109--85346-rfbes-at-semeval-2024-task-8-investigating-syntactic-and-semantic-features-for-distinguishing-ai-generated-and-human-written-texts-mohammad-heydari-rad-et-al-2024>(85/109 | 85/346) RFBES at SemEval-2024 Task 8: Investigating Syntactic and Semantic Features for Distinguishing AI-Generated and Human-Written Texts (Mohammad Heydari Rad et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mohammad Heydari Rad, Farhan Farsi, Shayan Bali, Romina Etezadi, Mehrnoush Shamsfard. (2024)<br><strong>RFBES at SemEval-2024 Task 8: Investigating Syntactic and Semantic Features for Distinguishing AI-Generated and Human-Written Texts</strong><br><button class=copy-to-clipboard title="RFBES at SemEval-2024 Task 8: Investigating Syntactic and Semantic Features for Distinguishing AI-Generated and Human-Written Texts" index=85>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-85 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 30<br>Keywords: AI-generated Text Detection, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14838v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14838v1.pdf filename=2402.14838v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Nowadays, the usage of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> has increased, and <b>LLMs</b> have been used to generate texts in different languages and for different tasks. Additionally, due to the participation of remarkable companies such as Google and OpenAI, <b>LLMs</b> are now more accessible, and people can easily use them. However, an important issue is how we can detect <b>AI-generated</b> <b>texts</b> <b>from</b> human-written ones. In this article, we have investigated the problem of <b>AI-generated</b> <b>text</b> <b>detection</b> from two different aspects: semantics and syntax. Finally, we presented an AI model that can distinguish <b>AI-generated</b> <b>texts</b> <b>from</b> human-written ones with high accuracy on both multilingual and monolingual tasks using the M4 dataset. According to our results, using a semantic approach would be more helpful for detection. However, there is a lot of room for improvement in the syntactic approach, and it would be a good approach for future work.</p></p class="citation"></blockquote><h3 id=86109--86346-m2k-vdg-model-adaptive-multimodal-knowledge-anchor-enhanced-video-grounded-dialogue-generation-hongcheng-liu-et-al-2024>(86/109 | 86/346) M2K-VDG: Model-Adaptive Multimodal Knowledge Anchor Enhanced Video-grounded Dialogue Generation (Hongcheng Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hongcheng Liu, Pingjie Wang, Yu Wang, Yanfeng Wang. (2024)<br><strong>M2K-VDG: Model-Adaptive Multimodal Knowledge Anchor Enhanced Video-grounded Dialogue Generation</strong><br><button class=copy-to-clipboard title="M2K-VDG: Model-Adaptive Multimodal Knowledge Anchor Enhanced Video-grounded Dialogue Generation" index=86>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-86 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 29<br>Keywords: Benchmarking, Counter-factual, Multi-modal, Multi-modal, Perplexity<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11875v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11875v1.pdf filename=2402.11875v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Video-grounded dialogue generation (VDG) requires the system to generate a fluent and accurate answer based on <b>multimodal</b> knowledge. However, the difficulty in <b>multimodal</b> knowledge utilization brings serious hallucinations to VDG models in practice. Although previous works mitigate the hallucination in a variety of ways, they hardly take notice of the importance of the <b>multimodal</b> knowledge anchor answer tokens. In this paper, we reveal via <b>perplexity</b> that different VDG models experience varying hallucinations and exhibit diverse anchor tokens. Based on this observation, we propose M2K-VDG, a model-adaptive <b>multimodal</b> knowledge anchor enhancement framework for hallucination reduction. Furthermore, we introduce the <b>counterfactual</b> effect for more accurate anchor token detection. The experimental results on three popular <b>benchmarks</b> exhibit the superiority of our approach over state-of-the-art methods, demonstrating its effectiveness in reducing hallucinations.</p></p class="citation"></blockquote><h3 id=87109--87346-anygpt-unified-multimodal-llm-with-discrete-sequence-modeling-jun-zhan-et-al-2024>(87/109 | 87/346) AnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling (Jun Zhan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jun Zhan, Junqi Dai, Jiasheng Ye, Yunhua Zhou, Dong Zhang, Zhigeng Liu, Xin Zhang, Ruibin Yuan, Ge Zhang, Linyang Li, Hang Yan, Jie Fu, Tao Gui, Tianxiang Sun, Yugang Jiang, Xipeng Qiu. (2024)<br><strong>AnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling</strong><br><button class=copy-to-clipboard title="AnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling" index=87>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-87 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-CV, cs-LG, cs.CL<br>Keyword Score: 26<br>Keywords: Multi-modal, Multi-modal, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12226v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12226v1.pdf filename=2402.12226v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce AnyGPT, an any-to-any <b>multimodal</b> language model that utilizes discrete representations for the unified processing of various modalities, including speech, text, images, and music. AnyGPT can be trained stably without any alterations to the current <b>large</b> <b>language</b> <b>model</b> <b>(LLM)</b> architecture or training paradigms. Instead, it relies exclusively on data-level preprocessing, facilitating the seamless integration of new modalities into <b>LLMs,</b> akin to the incorporation of new languages. We build a <b>multimodal</b> text-centric dataset for <b>multimodal</b> alignment pre-training. Utilizing generative models, we synthesize the first <b>large-scale</b> <b>any-to-any</b> <b>multimodal</b> instruction dataset. It consists of 108k samples of multi-turn conversations that intricately interweave various modalities, thus equipping the model to handle arbitrary combinations of <b>multimodal</b> inputs and outputs. Experimental results demonstrate that AnyGPT is capable of facilitating any-to-any <b>multimodal</b> conversation while achieving performance comparable to specialized models across all modalities, proving that discrete representations can effectively and conveniently unify multiple modalities within a language model. Demos are shown in <a href=https://junzhan2000.github.io/AnyGPT.github.io/>https://junzhan2000.github.io/AnyGPT.github.io/</a></p></p class="citation"></blockquote><h3 id=88109--88346-triple-encoders-representations-that-fire-together-wire-together-justus-jonas-erker-et-al-2024>(88/109 | 88/346) Triple-Encoders: Representations That Fire Together, Wire Together (Justus-Jonas Erker et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Justus-Jonas Erker, Florian Mai, Nils Reimers, Gerasimos Spanakis, Iryna Gurevych. (2024)<br><strong>Triple-Encoders: Representations That Fire Together, Wire Together</strong><br><button class=copy-to-clipboard title="Triple-Encoders: Representations That Fire Together, Wire Together" index=88>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-88 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 25<br>Keywords: Contrastive Learning, Representation Learning, Zero-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12332v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12332v1.pdf filename=2402.12332v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Search-based dialog models typically re-encode the dialog history at every turn, incurring high cost. Curved <b>Contrastive</b> <b>Learning,</b> a <b>representation</b> <b>learning</b> method that encodes relative distances between utterances into the embedding space via a bi-encoder, has recently shown promising results for dialog modeling at far superior efficiency. While high efficiency is achieved through independently encoding utterances, this ignores the importance of contextualization. To overcome this issue, this study introduces triple-encoders, which efficiently compute distributed utterance mixtures from these independently encoded utterances through a novel hebbian inspired co-occurrence learning objective without using any weights. Empirically, we find that triple-encoders lead to a substantial improvement over bi-encoders, and even to better <b>zero-shot</b> generalization than single-vector <b>representation</b> <b>models</b> without requiring re-encoding. Our code/model is publicly available.</p></p class="citation"></blockquote><h3 id=89109--89346-hunflair2-in-a-cross-corpus-evaluation-of-biomedical-named-entity-recognition-and-normalization-tools-mario-sänger-et-al-2024>(89/109 | 89/346) HunFlair2 in a cross-corpus evaluation of biomedical named entity recognition and normalization tools (Mario Sänger et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mario Sänger, Samuele Garda, Xing David Wang, Leon Weber-Genzel, Pia Droop, Benedikt Fuchs, Alan Akbik, Ulf Leser. (2024)<br><strong>HunFlair2 in a cross-corpus evaluation of biomedical named entity recognition and normalization tools</strong><br><button class=copy-to-clipboard title="HunFlair2 in a cross-corpus evaluation of biomedical named entity recognition and normalization tools" index=89>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-89 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 23<br>Keywords: Benchmarking, Named Entity Recognition, Text Mining<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12372v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12372v2.pdf filename=2402.12372v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the exponential growth of the life science literature, biomedical <b>text</b> <b>mining</b> (BTM) has become an essential technology for accelerating the extraction of insights from publications. Identifying <b>named</b> <b>entities</b> <b>(e.g.,</b> diseases, drugs, or genes) in <b>texts</b> <b>and</b> their linkage to reference knowledge bases are crucial steps in BTM pipelines to enable information aggregation from different documents. However, tools for these two steps are rarely applied in the same context in which they were developed. Instead, they are applied in the wild, i.e., on application-dependent <b>text</b> <b>collections</b> different from those used for the tools&rsquo; training, varying, e.g., in focus, genre, style, and <b>text</b> <b>type.</b> This raises the question of whether the reported performance of BTM tools can be trusted for downstream applications. Here, we report on the results of a carefully designed cross-corpus <b>benchmark</b> for <b>named</b> <b>entity</b> <b>extraction,</b> where tools were applied systematically to corpora not used during their training. Based on a survey of 28 published systems, we selected five for an in-depth analysis on three publicly available corpora encompassing four different entity types. Comparison between tools results in a mixed picture and shows that, in a cross-corpus setting, the performance is significantly lower than the one reported in an in-corpus setting. HunFlair2 showed the best performance on average, being closely followed by PubTator. Our results indicate that users of BTM tools should expect diminishing performances when applying them in the wild compared to original publications and show that further research is necessary to make BTM tools more robust.</p></p class="citation"></blockquote><h3 id=90109--90346-language-model-adaptation-to-specialized-domains-through-selective-masking-based-on-genre-and-topical-characteristics-anas-belfathi-et-al-2024>(90/109 | 90/346) Language Model Adaptation to Specialized Domains through Selective Masking based on Genre and Topical Characteristics (Anas Belfathi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Anas Belfathi, Ygor Gallina, Nicolas Hernandez, Richard Dufour, Laura Monceaux. (2024)<br><strong>Language Model Adaptation to Specialized Domains through Selective Masking based on Genre and Topical Characteristics</strong><br><button class=copy-to-clipboard title="Language Model Adaptation to Specialized Domains through Selective Masking based on Genre and Topical Characteristics" index=90>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-90 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 23<br>Keywords: Benchmarking, BERT, Pre-trained Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12036v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12036v1.pdf filename=2402.12036v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advances in <b>pre-trained</b> <b>language</b> <b>modeling</b> have facilitated significant progress across various natural language processing (NLP) tasks. Word masking during model training constitutes a pivotal component of language modeling in architectures like <b>BERT.</b> However, the prevalent method of word masking relies on random selection, potentially disregarding domain-specific linguistic attributes. In this article, we introduce an innovative masking approach leveraging genre and topicality information to tailor language models to specialized domains. Our method incorporates a ranking process that prioritizes words based on their significance, subsequently guiding the masking procedure. Experiments conducted using continual pre-training within the legal domain have underscored the efficacy of our approach on the LegalGLUE <b>benchmark</b> in the English language. <b>Pre-trained</b> <b>language</b> <b>models</b> and code are freely available for use.</p></p class="citation"></blockquote><h3 id=91109--91346-comprehensive-cognitive-llm-agent-for-smartphone-gui-automation-xinbei-ma-et-al-2024>(91/109 | 91/346) Comprehensive Cognitive LLM Agent for Smartphone GUI Automation (Xinbei Ma et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xinbei Ma, Zhuosheng Zhang, Hai Zhao. (2024)<br><strong>Comprehensive Cognitive LLM Agent for Smartphone GUI Automation</strong><br><button class=copy-to-clipboard title="Comprehensive Cognitive LLM Agent for Smartphone GUI Automation" index=91>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-91 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 23<br>Keywords: Benchmarking, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11941v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11941v1.pdf filename=2402.11941v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have shown remarkable potential as human-like autonomous language agents to interact with real-world environments, especially for graphical user interface (GUI) automation. However, those GUI agents require comprehensive cognition ability including exhaustive perception and reliable action response. We propose \underline{Co}mprehensive \underline{Co}gnitive <b>LLM</b> \underline{Agent}, CoCo-Agent, with two novel approaches, comprehensive environment perception (CEP) and conditional action prediction (CAP), to systematically improve the GUI automation performance. First, CEP facilitates the GUI perception through different aspects and granularity, including screenshots and complementary detailed layouts for the visual channel and historical actions for the textual channel. Second, CAP decomposes the action prediction into sub-problems: action type prediction and action target conditioned on the action type. With our technical design, our agent achieves new state-of-the-art performance on AITW and META-GUI <b>benchmarks,</b> showing promising abilities in realistic scenarios.</p></p class="citation"></blockquote><h3 id=92109--92346-have-seen-me-before-automating-dataset-updates-towards-reliable-and-timely-evaluation-jiahao-ying-et-al-2024>(92/109 | 92/346) Have Seen Me Before? Automating Dataset Updates Towards Reliable and Timely Evaluation (Jiahao Ying et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiahao Ying, Yixin Cao, Bo Wang, Wei Tang, Yizhe Yang, Shuicheng Yan. (2024)<br><strong>Have Seen Me Before? Automating Dataset Updates Towards Reliable and Timely Evaluation</strong><br><button class=copy-to-clipboard title="Have Seen Me Before? Automating Dataset Updates Towards Reliable and Timely Evaluation" index=92>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-92 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 23<br>Keywords: Benchmarking, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11894v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11894v1.pdf filename=2402.11894v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Due to the expanding capabilities and pre-training data, <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> are facing increasingly serious evaluation challenges. On one hand, the data leakage issue cause over-estimation on existing <b>benchmarks.</b> On the other hand, periodically curating datasets manually is costly. In this paper, we propose to automate dataset updates for reliable and timely evaluation. The basic idea is to generate unseen and high-quality testing samples based on existing ones to mitigate leakage issues. In specific, we propose two strategies with systematically verification. First, the mimicking strategy employs <b>LLMs</b> to create new samples resembling existing ones, to the maximum extent preserving the stylistic of the original dataset. Our experiments demonstrate its evaluation stability across multiple instantiations and its effectiveness in dealing with data leakage issues in most cases. Second, for the cases that mimicking dataset works poorly, we design an extending strategy that adjusts the difficulty of the generated samples according to varying cognitive levels. This not only makes our evaluation more systematic, but also, with a balanced difficulty, even discern model capabilities better at fine-grained levels.</p></p class="citation"></blockquote><h3 id=93109--93346-archer-a-human-labeled-text-to-sql-dataset-with-arithmetic-commonsense-and-hypothetical-reasoning-danna-zheng-et-al-2024>(93/109 | 93/346) Archer: A Human-Labeled Text-to-SQL Dataset with Arithmetic, Commonsense and Hypothetical Reasoning (Danna Zheng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Danna Zheng, Mirella Lapata, Jeff Z. Pan. (2024)<br><strong>Archer: A Human-Labeled Text-to-SQL Dataset with Arithmetic, Commonsense and Hypothetical Reasoning</strong><br><button class=copy-to-clipboard title="Archer: A Human-Labeled Text-to-SQL Dataset with Arithmetic, Commonsense and Hypothetical Reasoning" index=93>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-93 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Reasoning, Text2SQL<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12554v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12554v1.pdf filename=2402.12554v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present Archer, a challenging bilingual <b>text-to-SQL</b> dataset specific to complex <b>reasoning,</b> including arithmetic, commonsense and hypothetical <b>reasoning.</b> It contains 1,042 English questions and 1,042 Chinese questions, along with 521 unique SQL queries, covering 20 English databases across 20 domains. Notably, this dataset demonstrates a significantly higher level of complexity compared to existing publicly available datasets. Our evaluation shows that Archer challenges the capabilities of current state-of-the-art models, with a high-ranked model on the Spider leaderboard achieving only 6.73% execution accuracy on Archer test set. Thus, Archer presents a significant challenge for future research in this field.</p></p class="citation"></blockquote><h3 id=94109--94346-do-pre-trained-language-models-detect-and-understand-semantic-underspecification-ask-the-dust-frank-wildenburg-et-al-2024>(94/109 | 94/346) Do Pre-Trained Language Models Detect and Understand Semantic Underspecification? Ask the DUST! (Frank Wildenburg et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Frank Wildenburg, Michael Hanna, Sandro Pezzelle. (2024)<br><strong>Do Pre-Trained Language Models Detect and Understand Semantic Underspecification? Ask the DUST!</strong><br><button class=copy-to-clipboard title="Do Pre-Trained Language Models Detect and Understand Semantic Underspecification? Ask the DUST!" index=94>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-94 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Pre-trained Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12486v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12486v1.pdf filename=2402.12486v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In everyday language use, speakers frequently utter and interpret sentences that are semantically underspecified, namely, whose content is insufficient to fully convey their message or interpret them univocally. For example, to interpret the underspecified sentence &ldquo;Don&rsquo;t spend too much&rdquo;, which leaves implicit what (not) to spend, additional linguistic context or outside knowledge is needed. In this work, we propose a novel Dataset of semantically Underspecified Sentences grouped by Type (DUST) and use it to study whether <b>pre-trained</b> <b>language</b> <b>models</b> (LMs) correctly identify and interpret underspecified sentences. We find that newer LMs are reasonably able to identify underspecified sentences when explicitly <b>prompted.</b> However, interpreting them correctly is much harder for any LMs. Our experiments show that when interpreting underspecified sentences, LMs exhibit little uncertainty, contrary to what theoretical accounts of underspecification would predict. Overall, our study reveals limitations in current models&rsquo; processing of sentence semantics and highlights the importance of using naturalistic data and communicative scenarios when evaluating LMs&rsquo; language capabilities.</p></p class="citation"></blockquote><h3 id=95109--95346-sequoia-scalable-robust-and-hardware-aware-speculative-decoding-zhuoming-chen-et-al-2024>(95/109 | 95/346) Sequoia: Scalable, Robust, and Hardware-aware Speculative Decoding (Zhuoming Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhuoming Chen, Avner May, Ruslan Svirschevski, Yuhsun Huang, Max Ryabinin, Zhihao Jia, Beidi Chen. (2024)<br><strong>Sequoia: Scalable, Robust, and Hardware-aware Speculative Decoding</strong><br><button class=copy-to-clipboard title="Sequoia: Scalable, Robust, and Hardware-aware Speculative Decoding" index=95>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-95 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12374v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12374v1.pdf filename=2402.12374v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>As the usage of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> grows, performing efficient inference with these models becomes increasingly important. While speculative decoding has recently emerged as a promising direction for speeding up inference, existing methods are limited in their ability to scale to larger speculation budgets, and adapt to different hyperparameters and hardware. This paper introduces Sequoia, a scalable, robust, and hardware-aware algorithm for speculative decoding. To attain better scalability, Sequoia introduces a dynamic programming algorithm to find the optimal tree structure for the speculated tokens. To achieve robust speculative performance, Sequoia uses a novel sampling and verification method that outperforms prior work across different decoding temperatures. Finally, Sequoia introduces a hardware-aware tree optimizer that maximizes speculative performance by automatically selecting the token tree size and depth for a given hardware platform. Evaluation shows that Sequoia improves the decoding speed of Llama2-7B, Llama2-13B, and Vicuna-33B on an A100 by up to $4.04\times$, $3.84\times$, and $2.37\times$, and Llama2-70B offloading by up to $10.33\times$ on L40.</p></p class="citation"></blockquote><h3 id=96109--96346-zero-shot-vlms-for-hate-meme-detection-are-we-there-yet-naquee-rizwan-et-al-2024>(96/109 | 96/346) Zero shot VLMs for hate meme detection: Are we there yet? (Naquee Rizwan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Naquee Rizwan, Paramananda Bhaskar, Mithun Das, Swadhin Satyaprakash Majhi, Punyajoy Saha, Animesh Mukherjee. (2024)<br><strong>Zero shot VLMs for hate meme detection: Are we there yet?</strong><br><button class=copy-to-clipboard title="Zero shot VLMs for hate meme detection: Are we there yet?" index=96>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-96 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-CV, cs-LG, cs.CL<br>Keyword Score: 20<br>Keywords: Zero-shot, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12198v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12198v1.pdf filename=2402.12198v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Multimedia content on social media is rapidly evolving, with memes gaining prominence as a distinctive form. Unfortunately, some malicious users exploit memes to target individuals or vulnerable communities, making it imperative to identify and address such instances of hateful memes. Extensive research has been conducted to address this issue by developing hate meme detection models. However, a notable limitation of traditional machine/deep learning models is the requirement for labeled datasets for accurate classification. Recently, the research community has witnessed the emergence of several visual language models that have exhibited outstanding performance across various tasks. In this study, we aim to investigate the efficacy of these visual language models in handling intricate tasks such as hate meme detection. We use various <b>prompt</b> settings to focus on <b>zero-shot</b> classification of hateful/harmful memes. Through our analysis, we observe that large VLMs are still vulnerable for <b>zero-shot</b> hate meme detection.</p></p class="citation"></blockquote><h3 id=97109--97346-amplifying-training-data-exposure-through-fine-tuning-with-pseudo-labeled-memberships-myung-gyo-oh-et-al-2024>(97/109 | 97/346) Amplifying Training Data Exposure through Fine-Tuning with Pseudo-Labeled Memberships (Myung Gyo Oh et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Myung Gyo Oh, Hong Eun Ahn, Leo Hyun Park, Taekyoung Kwon. (2024)<br><strong>Amplifying Training Data Exposure through Fine-Tuning with Pseudo-Labeled Memberships</strong><br><button class=copy-to-clipboard title="Amplifying Training Data Exposure through Fine-Tuning with Pseudo-Labeled Memberships" index=97>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-97 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: I-2-7; K-6-5, cs-CL, cs-CR, cs-LG, cs.CL<br>Keyword Score: 20<br>Keywords: Fine-tuning, Fine-tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12189v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12189v1.pdf filename=2402.12189v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Neural language models (LMs) are vulnerable to training data extraction attacks due to data memorization. This paper introduces a novel attack scenario wherein an attacker adversarially <b>fine-tunes</b> pre-trained LMs to amplify the exposure of the original training data. This strategy differs from prior studies by aiming to intensify the LM&rsquo;s retention of its pre-training dataset. To achieve this, the attacker needs to collect generated texts that are closely aligned with the pre-training data. However, without knowledge of the actual dataset, quantifying the amount of pre-training data within generated texts is challenging. To address this, we propose the use of pseudo-labels for these generated texts, leveraging membership approximations indicated by machine-generated probabilities from the target LM. We subsequently <b>fine-tune</b> the LM to favor generations with higher likelihoods of originating from the pre-training data, based on their membership probabilities. Our empirical findings indicate a remarkable outcome: LMs with over 1B parameters exhibit a four to eight-fold increase in training data exposure. We discuss potential mitigations and suggest future research directions.</p></p class="citation"></blockquote><h3 id=98109--98346-purifying-large-language-models-by-ensembling-a-small-language-model-tianlin-li-et-al-2024>(98/109 | 98/346) Purifying Large Language Models by Ensembling a Small Language Model (Tianlin Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tianlin Li, Qian Liu, Tianyu Pang, Chao Du, Qing Guo, Yang Liu, Min Lin. (2024)<br><strong>Purifying Large Language Models by Ensembling a Small Language Model</strong><br><button class=copy-to-clipboard title="Purifying Large Language Models by Ensembling a Small Language Model" index=98>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-98 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: I-2, cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14845v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14845v1.pdf filename=2402.14845v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The emerging success of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> heavily relies on collecting abundant training data from external (untrusted) sources. Despite substantial efforts devoted to data cleaning and curation, well-constructed <b>LLMs</b> have been reported to suffer from copyright infringement, data poisoning, and/or privacy violations, which would impede practical deployment of <b>LLMs.</b> In this study, we propose a simple and easily implementable method for purifying <b>LLMs</b> from the negative effects caused by uncurated data, namely, through ensembling <b>LLMs</b> with benign and small language models (SLMs). Aside from theoretical guarantees, we perform comprehensive experiments to empirically confirm the efficacy of ensembling <b>LLMs</b> with SLMs, which can effectively preserve the performance of <b>LLMs</b> while mitigating issues such as copyright infringement, data poisoning, and privacy violations.</p></p class="citation"></blockquote><h3 id=99109--99346-acquiring-clean-language-models-from-backdoor-poisoned-datasets-by-downscaling-frequency-space-zongru-wu-et-al-2024>(99/109 | 99/346) Acquiring Clean Language Models from Backdoor Poisoned Datasets by Downscaling Frequency Space (Zongru Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zongru Wu, Zhuosheng Zhang, Pengzhou Cheng, Gongshen Liu. (2024)<br><strong>Acquiring Clean Language Models from Backdoor Poisoned Datasets by Downscaling Frequency Space</strong><br><button class=copy-to-clipboard title="Acquiring Clean Language Models from Backdoor Poisoned Datasets by Downscaling Frequency Space" index=99>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-99 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-CR, cs.CL<br>Keyword Score: 20<br>Keywords: BERT, RoBERTa<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12026v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12026v1.pdf filename=2402.12026v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Despite the notable success of language models (LMs) in various natural language processing (NLP) tasks, the reliability of LMs is susceptible to backdoor attacks. Prior research attempts to mitigate backdoor learning while training the LMs on the poisoned dataset, yet struggles against complex backdoor attacks in real-world scenarios. In this paper, we investigate the learning mechanisms of backdoor LMs in the frequency space by Fourier analysis. Our findings indicate that the backdoor mapping presented on the poisoned datasets exhibits a more discernible inclination towards lower frequency compared to clean mapping, resulting in the faster convergence of backdoor mapping. To alleviate this dilemma, we propose Multi-Scale Low-Rank Adaptation (MuScleLoRA), which deploys multiple radial scalings in the frequency space with low-rank adaptation to the target model and further aligns the gradients when updating parameters. Through downscaling in the frequency space, MuScleLoRA encourages the model to prioritize the learning of relatively high-frequency clean mapping, consequently mitigating backdoor learning. Experimental results demonstrate that MuScleLoRA outperforms baselines significantly. Notably, MuScleLoRA reduces the average success rate of diverse backdoor attacks to below 15% across multiple datasets and generalizes to various backbone LMs, including <b>BERT,</b> <b>RoBERTa,</b> and Llama2. The codes are available at <a href=https://github.com/ZrW00/MuScleLoRA>https://github.com/ZrW00/MuScleLoRA</a>.</p></p class="citation"></blockquote><h3 id=100109--100346-compress-to-impress-unleashing-the-potential-of-compressive-memory-in-real-world-long-term-conversations-nuo-chen-et-al-2024>(100/109 | 100/346) Compress to Impress: Unleashing the Potential of Compressive Memory in Real-World Long-Term Conversations (Nuo Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nuo Chen, Hongguang Li, Juhua Huang, Baoyuan Wang, Jia Li. (2024)<br><strong>Compress to Impress: Unleashing the Potential of Compressive Memory in Real-World Long-Term Conversations</strong><br><button class=copy-to-clipboard title="Compress to Impress: Unleashing the Potential of Compressive Memory in Real-World Long-Term Conversations" index=100>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-100 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Dialogue System, Instruction Tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11975v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11975v1.pdf filename=2402.11975v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Existing retrieval-based methods have made significant strides in maintaining long-term conversations. However, these approaches face challenges in memory database management and accurate memory retrieval, hindering their efficacy in dynamic, real-world interactions. This study introduces a novel framework, COmpressive Memory-Enhanced <b>Dialogue</b> <b>sYstems</b> (COMEDY), which eschews traditional retrieval modules and memory databases. Instead, COMEDY adopts a &lsquo;&lsquo;One-for-All&rsquo;&rsquo; approach, utilizing a single language model to manage memory generation, compression, and response generation. Central to this framework is the concept of compressive memory, which intergrates session-specific summaries, user-bot dynamics, and past events into a concise memory format. To support COMEDY, we curated a large-scale Chinese <b>instruction-tuning</b> <b>dataset,</b> Dolphin, derived from real user-chatbot interactions. Comparative evaluations demonstrate COMEDY&rsquo;s superiority over traditional retrieval-based methods in producing more nuanced and human-like conversational experiences. Our codes are available at <a href=https://github.com/nuochenpku/COMEDY>https://github.com/nuochenpku/COMEDY</a>.</p></p class="citation"></blockquote><h3 id=101109--101346-team-qust-at-semeval-2024-task-8-a-comprehensive-study-of-monolingual-and-multilingual-approaches-for-detecting-ai-generated-text-xiaoman-xu-et-al-2024>(101/109 | 101/346) Team QUST at SemEval-2024 Task 8: A Comprehensive Study of Monolingual and Multilingual Approaches for Detecting AI-generated Text (Xiaoman Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiaoman Xu, Xiangrun Li, Taihang Wang, Jianxiang Tian, Ye Jiang. (2024)<br><strong>Team QUST at SemEval-2024 Task 8: A Comprehensive Study of Monolingual and Multilingual Approaches for Detecting AI-generated Text</strong><br><button class=copy-to-clipboard title="Team QUST at SemEval-2024 Task 8: A Comprehensive Study of Monolingual and Multilingual Approaches for Detecting AI-generated Text" index=101>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-101 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Data Augmentation, Fine-tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11934v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11934v1.pdf filename=2402.11934v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents the participation of team QUST in Task 8 SemEval 2024. We first performed <b>data</b> <b>augmentation</b> and cleaning on the dataset to enhance model training efficiency and accuracy. In the monolingual task, we evaluated traditional deep-learning methods, multiscale positive-unlabeled framework (MPU), <b>fine-tuning,</b> adapters and ensemble methods. Then, we selected the top-performing models based on their accuracy from the monolingual models and evaluated them in subtasks A and B. The final model construction employed a stacking ensemble that combined <b>fine-tuning</b> with MPU. Our system achieved 8th (scored 8th in terms of accuracy, officially ranked 13th) place in the official test set in multilingual settings of subtask A. We release our system code at:https://github.com/warmth27/SemEval2024_QUST</p></p class="citation"></blockquote><h3 id=102109--102346-semantic-textual-similarity-assessment-in-chest-x-ray-reports-using-a-domain-specific-cosine-based-metric-sayeh-gholipour-picha-et-al-2024>(102/109 | 102/346) Semantic Textual Similarity Assessment in Chest X-ray Reports Using a Domain-Specific Cosine-Based Metric (Sayeh Gholipour Picha et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sayeh Gholipour Picha, Dawood Al Chanti, Alice Caplier. (2024)<br><strong>Semantic Textual Similarity Assessment in Chest X-ray Reports Using a Domain-Specific Cosine-Based Metric</strong><br><button class=copy-to-clipboard title="Semantic Textual Similarity Assessment in Chest X-ray Reports Using a Domain-Specific Cosine-Based Metric" index=102>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-102 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-CV, cs.CL<br>Keyword Score: 16<br>Keywords: Multi-modal, Multi-modal, Neural Machine Translation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11908v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11908v1.pdf filename=2402.11908v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Medical language processing and deep learning techniques have emerged as critical tools for improving healthcare, particularly in the analysis of medical imaging and medical text data. These <b>multimodal</b> data fusion techniques help to improve the interpretation of medical imaging and lead to increased diagnostic accuracy, informed clinical decisions, and improved patient outcomes. The success of these models relies on the ability to extract and consolidate semantic information from clinical text. This paper addresses the need for more robust methods to evaluate the semantic content of medical reports. Conventional natural language processing approaches and metrics are initially designed for considering the semantic context in the natural language domain and <b>machine</b> <b>translation,</b> often failing to capture the complex semantic meanings inherent in medical content. In this study, we introduce a novel approach designed specifically for assessing the semantic similarity between generated medical reports and the ground truth. Our approach is validated, demonstrating its efficiency in assessing domain-specific semantic similarity within medical contexts. By applying our metric to state-of-the-art Chest X-ray report generation models, we obtain results that not only align with conventional metrics but also provide more contextually meaningful scores in the considered medical domain.</p></p class="citation"></blockquote><h3 id=103109--103346-evaluating-image-review-ability-of-vision-language-models-shigeki-saito-et-al-2024>(103/109 | 103/346) Evaluating Image Review Ability of Vision Language Models (Shigeki Saito et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shigeki Saito, Kazuki Hayashi, Yusuke Ide, Yusuke Sakai, Kazuma Onishi, Toma Suzuki, Seiji Gobara, Hidetaka Kamigaito, Katsuhiko Hayashi, Taro Watanabe. (2024)<br><strong>Evaluating Image Review Ability of Vision Language Models</strong><br><button class=copy-to-clipboard title="Evaluating Image Review Ability of Vision Language Models" index=103>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-103 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-CV, cs-MM, cs.CL<br>Keyword Score: 13<br>Keywords: Benchmarking, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12121v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12121v1.pdf filename=2402.12121v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Large-scale vision language models (LVLMs) are language models that are capable of processing images and text inputs by a single model. This paper explores the use of LVLMs to generate review texts for images. The ability of LVLMs to review images is not fully understood, highlighting the need for a methodical evaluation of their review abilities. Unlike image captions, review texts can be written from various perspectives such as image composition and exposure. This diversity of review perspectives makes it difficult to uniquely determine a single correct review for an image. To address this challenge, we introduce an evaluation method based on rank correlation analysis, in which review texts are ranked by humans and LVLMs, then, measures the correlation between these rankings. We further validate this approach by creating a <b>benchmark</b> dataset aimed at assessing the image review ability of recent LVLMs. Our experiments with the dataset reveal that LVLMs, particularly those with proven superiority in other evaluative contexts, excel at distinguishing between high-quality and substandard image reviews.</p></p class="citation"></blockquote><h3 id=104109--104346-evolving-ai-collectives-to-enhance-human-diversity-and-enable-self-regulation-shiyang-lai-et-al-2024>(104/109 | 104/346) Evolving AI Collectives to Enhance Human Diversity and Enable Self-Regulation (Shiyang Lai et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shiyang Lai, Yujin Potter, Junsol Kim, Richard Zhuang, Dawn Song, James Evans. (2024)<br><strong>Evolving AI Collectives to Enhance Human Diversity and Enable Self-Regulation</strong><br><button class=copy-to-clipboard title="Evolving AI Collectives to Enhance Human Diversity and Enable Self-Regulation" index=104>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-104 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-CY, cs.CL<br>Keyword Score: 10<br>Keywords: Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12590v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12590v1.pdf filename=2402.12590v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> steer their behaviors based on texts generated by others. This capacity and their increasing prevalence in online settings portend that they will intentionally or unintentionally &ldquo;program&rdquo; one another and form emergent AI subjectivities, relationships, and collectives. Here, we call upon the research community to investigate these &ldquo;society-like&rdquo; properties of interacting artificial intelligences to increase their rewards and reduce their risks for human society and the health of online environments. We use a simple model and its outputs to illustrate how such emergent, decentralized AI collectives can expand the bounds of human diversity and reduce the risk of toxic, anti-social behavior online. Finally, we discuss opportunities for AI self-moderation and address ethical issues and design challenges associated with creating and maintaining decentralized AI collectives.</p></p class="citation"></blockquote><h3 id=105109--105346-llm-agents-for-psychology-a-study-on-gamified-assessments-qisen-yang-et-al-2024>(105/109 | 105/346) LLM Agents for Psychology: A Study on Gamified Assessments (Qisen Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qisen Yang, Zekun Wang, Honghui Chen, Shenzhi Wang, Yifan Pu, Xin Gao, Wenhao Huang, Shiji Song, Gao Huang. (2024)<br><strong>LLM Agents for Psychology: A Study on Gamified Assessments</strong><br><button class=copy-to-clipboard title="LLM Agents for Psychology: A Study on Gamified Assessments" index=105>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-105 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-CY, cs-HC, cs-LG, cs-MA, cs.CL<br>Keyword Score: 10<br>Keywords: Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12326v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12326v1.pdf filename=2402.12326v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Psychological measurement is essential for mental health, self-understanding, and personal development. Traditional methods, such as self-report scales and psychologist interviews, often face challenges with engagement and accessibility. While game-based and <b>LLM-based</b> tools have been explored to improve user interest and automate assessment, they struggle to balance engagement with generalizability. In this work, we propose PsychoGAT (Psychological Game AgenTs) to achieve a generic gamification of psychological assessment. The main insight is that powerful <b>LLMs</b> can function both as adept psychologists and innovative game designers. By incorporating <b>LLM</b> agents into designated roles and carefully managing their interactions, PsychoGAT can transform any standardized scales into personalized and engaging interactive fiction games. To validate the proposed method, we conduct psychometric evaluations to assess its effectiveness and employ human evaluators to examine the generated content across various psychological constructs, including depression, cognitive distortions, and personality traits. Results demonstrate that PsychoGAT serves as an effective assessment tool, achieving statistically significant excellence in psychometric metrics such as reliability, convergent validity, and discriminant validity. Moreover, human evaluations confirm PsychoGAT&rsquo;s enhancements in content coherence, interactivity, interest, immersion, and satisfaction.</p></p class="citation"></blockquote><h3 id=106109--106346-karl-knowledge-aware-retrieval-and-representations-aid-retention-and-learning-in-students-matthew-shu-et-al-2024>(106/109 | 106/346) KARL: Knowledge-Aware Retrieval and Representations aid Retention and Learning in Students (Matthew Shu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Matthew Shu, Nishant Balepur, Shi Feng, Jordan Boyd-Graber. (2024)<br><strong>KARL: Knowledge-Aware Retrieval and Representations aid Retention and Learning in Students</strong><br><button class=copy-to-clipboard title="KARL: Knowledge-Aware Retrieval and Representations aid Retention and Learning in Students" index=106>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-106 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 10<br>Keywords: BERT<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12291v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12291v1.pdf filename=2402.12291v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Flashcard schedulers are tools that rely on 1) student models to predict the flashcards a student knows; and 2) teaching policies to schedule cards based on these predictions. Existing student models, however, only use flashcard-level features, like the student&rsquo;s past responses, ignoring the semantic ties of flashcards. Deep Knowledge Tracing (DKT) models can capture semantic relations with language models, but are inefficient, lack content-rich datasets for evaluation, and require robust teaching policies. To address these issues, we design KARL, a DKT-inspired student model that uses retrieval and <b>BERT</b> embeddings for efficient and accurate student recall predictions. To test KARL, we collect a new dataset of diverse study history on trivia questions. KARL bests existing student models in AUC and calibration error. Finally, we propose a novel teaching policy that exploits the predictive power of DKT models to deploy KARL online. Based on 27 learners and 32 6-day study trajectories, KARL shows the ability to enhance medium-term educational learning, proving its efficacy for scheduling.</p></p class="citation"></blockquote><h3 id=107109--107346-text-diffusion-with-reinforced-conditioning-yuxuan-liu-et-al-2024>(107/109 | 107/346) Text Diffusion with Reinforced Conditioning (Yuxuan Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuxuan Liu, Tianchi Yang, Shaohan Huang, Zihan Zhang, Haizhen Huang, Furu Wei, Weiwei Deng, Feng Sun, Qi Zhang. (2024)<br><strong>Text Diffusion with Reinforced Conditioning</strong><br><button class=copy-to-clipboard title="Text Diffusion with Reinforced Conditioning" index=107>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-107 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 10<br>Keywords: Diffusion Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14843v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14843v1.pdf filename=2402.14843v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Diffusion</b> <b>models</b> have demonstrated exceptional capability in generating high-quality images, videos, and audio. Due to their adaptiveness in iterative refinement, they provide a strong potential for achieving better non-autoregressive sequence generation. However, existing text <b>diffusion</b> <b>models</b> still fall short in their performance due to a challenge in handling the discreteness of language. This paper thoroughly analyzes text <b>diffusion</b> <b>models</b> and uncovers two significant limitations: degradation of self-conditioning during training and misalignment between training and sampling. Motivated by our findings, we propose a novel Text <b>Diffusion</b> <b>model</b> called TREC, which mitigates the degradation with Reinforced Conditioning and the misalignment by Time-Aware Variance Scaling. Our extensive experiments demonstrate the competitiveness of TREC against autoregressive, non-autoregressive, and <b>diffusion</b> <b>baselines.</b> Moreover, qualitative analysis shows its advanced ability to fully utilize the <b>diffusion</b> <b>process</b> in refining samples.</p></p class="citation"></blockquote><h3 id=108109--108346-what-do-dialect-speakers-want-a-survey-of-attitudes-towards-language-technology-for-german-dialects-verena-blaschke-et-al-2024>(108/109 | 108/346) What Do Dialect Speakers Want? A Survey of Attitudes Towards Language Technology for German Dialects (Verena Blaschke et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Verena Blaschke, Christoph Purschke, Hinrich Schütze, Barbara Plank. (2024)<br><strong>What Do Dialect Speakers Want? A Survey of Attitudes Towards Language Technology for German Dialects</strong><br><button class=copy-to-clipboard title="What Do Dialect Speakers Want? A Survey of Attitudes Towards Language Technology for German Dialects" index=108>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-108 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 10<br>Keywords: Neural Machine Translation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11968v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11968v1.pdf filename=2402.11968v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Natural language processing (NLP) has largely focused on modelling standardized languages. More recently, attention has increasingly shifted to local, non-standardized languages and dialects. However, the relevant speaker populations&rsquo; needs and wishes with respect to NLP tools are largely unknown. In this paper, we focus on dialects and regional languages related to German &ndash; a group of varieties that is heterogeneous in terms of prestige and standardization. We survey speakers of these varieties (N=327) and present their opinions on hypothetical language technologies for their dialects. Although attitudes vary among subgroups of our respondents, we find that respondents are especially in favour of potential NLP tools that work with dialectal input (especially audio input) such as virtual assistants, and less so for applications that produce dialectal output such as <b>machine</b> <b>translation</b> or spellcheckers.</p></p class="citation"></blockquote><h3 id=109109--109346-causalgym-benchmarking-causal-interpretability-methods-on-linguistic-tasks-aryaman-arora-et-al-2024>(109/109 | 109/346) CausalGym: Benchmarking causal interpretability methods on linguistic tasks (Aryaman Arora et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Aryaman Arora, Dan Jurafsky, Christopher Potts. (2024)<br><strong>CausalGym: Benchmarking causal interpretability methods on linguistic tasks</strong><br><button class=copy-to-clipboard title="CausalGym: Benchmarking causal interpretability methods on linguistic tasks" index=109>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-109 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: I-2-7, cs-AI, cs-CL, cs.CL<br>Keyword Score: 6<br>Keywords: Benchmarking, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12560v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12560v1.pdf filename=2402.12560v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Language models (LMs) have proven to be powerful tools for psycholinguistic research, but most prior work has focused on purely behavioural measures (e.g., surprisal comparisons). At the same time, research in model interpretability has begun to illuminate the abstract causal mechanisms shaping LM behavior. To help bring these strands of research closer together, we introduce CausalGym. We adapt and expand the SyntaxGym suite of tasks to <b>benchmark</b> the ability of interpretability methods to causally affect model behaviour. To illustrate how CausalGym can be used, we study the pythia models (14M&ndash;6.9B) and assess the causal efficacy of a wide range of interpretability methods, including linear probing and distributed alignment search (DAS). We find that DAS outperforms the other methods, and so we use it to study the learning trajectory of two difficult linguistic phenomena in pythia-1b: negative polarity item licensing and filler&ndash;gap dependencies. Our analysis shows that the mechanism implementing both of these tasks is learned in discrete stages, not gradually.</p></p class="citation"></blockquote><h2 id=cslg-71>cs.LG (71)</h2><h3 id=171--110346-a-critical-evaluation-of-ai-feedback-for-aligning-large-language-models-archit-sharma-et-al-2024>(1/71 | 110/346) A Critical Evaluation of AI Feedback for Aligning Large Language Models (Archit Sharma et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Archit Sharma, Sedrick Keh, Eric Mitchell, Chelsea Finn, Kushal Arora, Thomas Kollar. (2024)<br><strong>A Critical Evaluation of AI Feedback for Aligning Large Language Models</strong><br><button class=copy-to-clipboard title="A Critical Evaluation of AI Feedback for Aligning Large Language Models" index=110>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-110 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CL, cs-LG, cs.LG<br>Keyword Score: 110<br>Keywords: Fine-tuning, Fine-tuning, Reinforcement Learning, Supervised Learning, GPT, GPT-3, GPT-3.5, GPT-4, Instruction Following, Large Language Model, Pre-trained Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12366v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12366v1.pdf filename=2402.12366v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Reinforcement</b> <b>learning</b> with AI feedback (RLAIF) is a popular paradigm for improving the <b>instruction-following</b> <b>abilities</b> of powerful <b>pre-trained</b> <b>language</b> <b>models.</b> RLAIF first performs <b>supervised</b> <b>fine-tuning</b> (SFT) using demonstrations from a teacher model and then further <b>fine-tunes</b> the model with <b>reinforcement</b> <b>learning</b> (RL), using feedback from a critic model. While recent popular open-source models have demonstrated substantial improvements in performance from the RL step, in this paper we question whether the complexity of this RL step is truly warranted for AI feedback. We show that the improvements of the RL step are virtually entirely due to the widespread practice of using a weaker teacher model (e.g. <b>GPT-3.5)</b> for SFT data collection than the critic (e.g., <b>GPT-4)</b> used for AI feedback generation. Specifically, we show that simple <b>supervised</b> <b>fine-tuning</b> with <b>GPT-4</b> as the teacher outperforms existing RLAIF pipelines. More generally, we find that the gains from RLAIF vary substantially across base model families, test-time evaluation protocols, and critic models. Finally, we provide a mechanistic explanation for when SFT may outperform the full two-step RLAIF pipeline as well as suggestions for making RLAIF maximally useful in practice.</p></p class="citation"></blockquote><h3 id=271--111346-spml-a-dsl-for-defending-language-models-against-prompt-attacks-reshabh-k-sharma-et-al-2024>(2/71 | 111/346) SPML: A DSL for Defending Language Models Against Prompt Attacks (Reshabh K Sharma et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Reshabh K Sharma, Vinayak Gupta, Dan Grossman. (2024)<br><strong>SPML: A DSL for Defending Language Models Against Prompt Attacks</strong><br><button class=copy-to-clipboard title="SPML: A DSL for Defending Language Models Against Prompt Attacks" index=111>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-111 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CL, cs-CR, cs-LG, cs-PL, cs.LG<br>Keyword Score: 93<br>Keywords: Benchmarking, GPT, GPT-3, GPT-3.5, GPT-4, LLaMA, Chatbot, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11755v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11755v1.pdf filename=2402.11755v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have profoundly transformed natural language applications, with a growing reliance on instruction-based definitions for designing <b>chatbots.</b> However, post-deployment the <b>chatbot</b> definitions are fixed and are vulnerable to attacks by malicious users, emphasizing the need to prevent unethical applications and financial losses. Existing studies explore user <b>prompts&rsquo;</b> impact on <b>LLM-based</b> <b>chatbots,</b> yet practical methods to contain attacks on application-specific <b>chatbots</b> remain unexplored. This paper presents System <b>Prompt</b> Meta Language (SPML), a domain-specific language for refining <b>prompts</b> and monitoring the inputs to the <b>LLM-based</b> <b>chatbots.</b> SPML actively checks attack <b>prompts,</b> ensuring user inputs align with <b>chatbot</b> definitions to prevent malicious execution on the <b>LLM</b> backbone, optimizing costs. It also streamlines <b>chatbot</b> definition crafting with programming language capabilities, overcoming natural language design challenges. Additionally, we introduce a groundbreaking <b>benchmark</b> with 1.8k system <b>prompts</b> and 20k user inputs, offering the inaugural language and <b>benchmark</b> for <b>chatbot</b> definition evaluation. Experiments across datasets demonstrate SPML&rsquo;s proficiency in understanding attacker <b>prompts,</b> surpassing models like <b>GPT-4,</b> <b>GPT-3.5,</b> and <b>LLAMA.</b> Our data and codes are publicly available at: <a href=https://prompt-compiler.github.io/SPML/>https://prompt-compiler.github.io/SPML/</a>.</p></p class="citation"></blockquote><h3 id=371--112346-robust-clip-unsupervised-adversarial-fine-tuning-of-vision-embeddings-for-robust-large-vision-language-models-christian-schlarmann-et-al-2024>(3/71 | 112/346) Robust CLIP: Unsupervised Adversarial Fine-Tuning of Vision Embeddings for Robust Large Vision-Language Models (Christian Schlarmann et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Christian Schlarmann, Naman Deep Singh, Francesco Croce, Matthias Hein. (2024)<br><strong>Robust CLIP: Unsupervised Adversarial Fine-Tuning of Vision Embeddings for Robust Large Vision-Language Models</strong><br><button class=copy-to-clipboard title="Robust CLIP: Unsupervised Adversarial Fine-Tuning of Vision Embeddings for Robust Large Vision-Language Models" index=112>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-112 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CV, cs-LG, cs.LG, stat-ML<br>Keyword Score: 83<br>Keywords: Fine-tuning, Foundation Model, Multi-modal, Unsupervised Learning, Zero-shot, GPT, GPT-4, Vision-and-Language, Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12336v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12336v1.pdf filename=2402.12336v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Multi-modal</b> <b>foundation</b> <b>models</b> like OpenFlamingo, LLaVA, and <b>GPT-4</b> are increasingly used for various real-world tasks. Prior work has shown that these models are highly vulnerable to <b>adversarial</b> <b>attacks</b> on the vision modality. These attacks can be leveraged to spread fake information or defraud users, and thus pose a significant risk, which makes the robustness of large <b>multi-modal</b> <b>foundation</b> <b>models</b> a pressing problem. The CLIP model, or one of its variants, is used as a frozen vision encoder in many <b>vision-language</b> models (VLMs), e.g. LLaVA and OpenFlamingo. We propose an <b>unsupervised</b> <b>adversarial</b> <b>fine-tuning</b> scheme to obtain a robust CLIP vision encoder, which yields robustness on all vision down-stream tasks (VLMs, <b>zero-shot</b> classification) that rely on CLIP. In particular, we show that stealth-attacks on users of VLMs by a malicious third party providing manipulated images are no longer possible once one replaces the original CLIP model with our robust one. No retraining or <b>fine-tuning</b> of the VLM is required. The code and robust models are available at <a href=https://github.com/chs20/RobustVLM>https://github.com/chs20/RobustVLM</a></p></p class="citation"></blockquote><h3 id=471--113346-self-amplify-improving-small-language-models-with-self-post-hoc-explanations-milan-bhan-et-al-2024>(4/71 | 113/346) Self-AMPLIFY: Improving Small Language Models with Self Post Hoc Explanations (Milan Bhan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Milan Bhan, Jean-Noel Vittaut, Nicolas Chesneau, Marie-Jeanne Lesot. (2024)<br><strong>Self-AMPLIFY: Improving Small Language Models with Self Post Hoc Explanations</strong><br><button class=copy-to-clipboard title="Self-AMPLIFY: Improving Small Language Models with Self Post Hoc Explanations" index=113>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-113 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CL, cs-LG, cs.LG<br>Keyword Score: 70<br>Keywords: Reasoning, In-context Learning, In-context Learning, In-context Learning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12038v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12038v1.pdf filename=2402.12038v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Incorporating natural language rationales in the <b>prompt</b> and <b>In-Context</b> <b>Learning</b> <b>(ICL)</b> has led to a significant improvement of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> performance. However, rationales currently require human-annotation or the use of auxiliary proxy models to target promising samples or generate high-quality rationales. In this work, we propose Self-AMPLIFY to generate automatically rationales from post hoc explanation methods applied to Small Language Models (SLMs) to improve their own performance. Self-AMPLIFY is a 3-step method that targets samples, generates rationales and builds a final <b>prompt</b> to leverage <b>ICL.</b> Self-AMPLIFY performance is evaluated on two SLMs and two datasets requiring <b>reasoning</b> abilities: these experiments show that Self-AMPLIFY achieves good results against competitors. Self-AMPLIFY is the first method to apply post hoc explanation methods to SLM to generate rationales to improve their own performance in a fully automated manner.</p></p class="citation"></blockquote><h3 id=571--114346-mafin-enhancing-black-box-embeddings-with-model-augmented-fine-tuning-mingtian-zhang-et-al-2024>(5/71 | 114/346) Mafin: Enhancing Black-Box Embeddings with Model Augmented Fine-tuning (Mingtian Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mingtian Zhang, Shawn Lan, Peter Hayes, David Barber. (2024)<br><strong>Mafin: Enhancing Black-Box Embeddings with Model Augmented Fine-tuning</strong><br><button class=copy-to-clipboard title="Mafin: Enhancing Black-Box Embeddings with Model Augmented Fine-tuning" index=114>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-114 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CL, cs-LG, cs.LG<br>Keyword Score: 65<br>Keywords: Black Box, Fine-tuning, Retrieval-Augmented Generation, Retrieval-Augmented Generation, Retrieval-Augmented Generation, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12177v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12177v1.pdf filename=2402.12177v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Retrieval</b> <b>Augmented</b> <b>Generation</b> <b>(RAG)</b> has emerged as an effective solution for mitigating hallucinations in <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs).</b> The <b>retrieval</b> <b>stage</b> <b>in</b> <b>RAG</b> typically involves a pre-trained embedding model, which converts queries and passages into vectors to capture their semantics. However, a standard pre-trained embedding model may exhibit sub-optimal performance when applied to specific domain knowledge, necessitating <b>fine-tuning.</b> This paper addresses scenarios where the embeddings are only available from a <b>black-box</b> <b>model.</b> We introduce Model augmented <b>fine-tuning</b> (Mafin) &ndash; a novel approach for <b>fine-tuning</b> a <b>black-box</b> <b>embedding</b> model by augmenting it with a trainable embedding model. Our results demonstrate that Mafin significantly enhances the performance of the <b>black-box</b> <b>embeddings</b> by only requiring the training of a small augmented model. We validate the effectiveness of our method on both labeled and unlabeled datasets, illustrating its broad applicability and efficiency.</p></p class="citation"></blockquote><h3 id=671--115346-hierarchical-bayes-approach-to-personalized-federated-unsupervised-learning-kaan-ozkara-et-al-2024>(6/71 | 115/346) Hierarchical Bayes Approach to Personalized Federated Unsupervised Learning (Kaan Ozkara et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kaan Ozkara, Bruce Huang, Ruida Zhou, Suhas Diggavi. (2024)<br><strong>Hierarchical Bayes Approach to Personalized Federated Unsupervised Learning</strong><br><button class=copy-to-clipboard title="Hierarchical Bayes Approach to Personalized Federated Unsupervised Learning" index=115>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-115 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 63<br>Keywords: Diffusion Model, Federated Learning, Sample Size, Supervised Learning, Supervised Learning, Unsupervised Learning, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12537v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12537v1.pdf filename=2402.12537v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Statistical heterogeneity of clients&rsquo; local data is an important characteristic in <b>federated</b> <b>learning,</b> motivating personalized algorithms tailored to the local data statistics. Though there has been a plethora of algorithms proposed for personalized <b>supervised</b> <b>learning,</b> discovering the structure of local data through personalized <b>unsupervised</b> <b>learning</b> is less explored. We initiate a systematic study of such personalized <b>unsupervised</b> <b>learning</b> by developing algorithms based on optimization criteria inspired by a hierarchical Bayesian statistical framework. We develop adaptive algorithms that discover the balance between using limited local data and collaborative information. We do this in the context of two <b>unsupervised</b> <b>learning</b> tasks: personalized dimensionality reduction and personalized <b>diffusion</b> <b>models.</b> We develop convergence analyses for our adaptive algorithms which illustrate the dependence on problem parameters (e.g., heterogeneity, local <b>sample</b> <b>size).</b> We also develop a theoretical framework for personalized <b>diffusion</b> <b>models,</b> which shows the benefits of collaboration even under heterogeneity. We finally evaluate our proposed algorithms using synthetic and real data, demonstrating the effective <b>sample</b> <b>amplification</b> for personalized tasks, induced through collaboration, despite data heterogeneity.</p></p class="citation"></blockquote><h3 id=771--116346-db-llm-accurate-dual-binarization-for-efficient-llms-hong-chen-et-al-2024>(7/71 | 116/346) DB-LLM: Accurate Dual-Binarization for Efficient LLMs (Hong Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hong Chen, Chengtao Lv, Liang Ding, Haotong Qin, Xiabin Zhou, Yifu Ding, Xuebo Liu, Min Zhang, Jinyang Guo, Xianglong Liu, Dacheng Tao. (2024)<br><strong>DB-LLM: Accurate Dual-Binarization for Efficient LLMs</strong><br><button class=copy-to-clipboard title="DB-LLM: Accurate Dual-Binarization for Efficient LLMs" index=116>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-116 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CL, cs-LG, cs.LG<br>Keyword Score: 60<br>Keywords: Knowledge Distillation, Quantization, Quantization, Large Language Model, Large Language Model, Perplexity<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11960v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11960v1.pdf filename=2402.11960v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have significantly advanced the field of natural language processing, while the expensive memory and computation consumption impede their practical deployment. <b>Quantization</b> emerges as one of the most effective methods for improving the computational efficiency of <b>LLMs.</b> However, existing ultra-low-bit <b>quantization</b> always causes severe accuracy drops. In this paper, we empirically relieve the micro and macro characteristics of ultra-low bit <b>quantization</b> and present a novel Dual-Binarization method for <b>LLMs,</b> namely DB-LLM. For the micro-level, we take both the accuracy advantage of 2-bit-width and the efficiency advantage of binarization into account, introducing Flexible Dual Binarization (FDB). By splitting 2-bit <b>quantized</b> weights into two independent sets of binaries, FDB ensures the accuracy of representations and introduces flexibility, utilizing the efficient bitwise operations of binarization while retaining the inherent high sparsity of ultra-low bit <b>quantization.</b> For the macro-level, we find the distortion that exists in the prediction of <b>LLM</b> after <b>quantization,</b> which is specified as the deviations related to the ambiguity of samples. We propose the Deviation-Aware <b>Distillation</b> (DAD) method, enabling the model to focus differently on various samples. Comprehensive experiments show that our DB-LLM not only significantly surpasses the current State-of-The-Art (SoTA) in ultra-low bit <b>quantization</b> (eg, <b>perplexity</b> decreased from 9.64 to 7.23), but also achieves an additional 20% reduction in computational consumption compared to the SOTA method under the same bit-width. Our code will be released soon.</p></p class="citation"></blockquote><h3 id=871--117346-towards-cross-domain-continual-learning-marcus-de-carvalho-et-al-2024>(8/71 | 117/346) Towards Cross-Domain Continual Learning (Marcus de Carvalho et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Marcus de Carvalho, Mahardhika Pratama, Jie Zhang, Chua Haoyan, Edward Yapp. (2024)<br><strong>Towards Cross-Domain Continual Learning</strong><br><button class=copy-to-clipboard title="Towards Cross-Domain Continual Learning" index=117>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-117 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CV, cs-LG, cs.LG<br>Keyword Score: 50<br>Keywords: Continual Learning, Convolution, Convolutional Neural Network, Supervised Learning, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12490v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12490v1.pdf filename=2402.12490v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Continual</b> <b>learning</b> is a process that involves training learning agents to sequentially master a stream of tasks or classes without revisiting past data. The challenge lies in leveraging previously acquired knowledge to learn new tasks efficiently, while avoiding catastrophic forgetting. Existing methods primarily focus on single domains, restricting their applicability to specific problems. In this work, we introduce a novel approach called Cross-Domain <b>Continual</b> <b>Learning</b> (CDCL) that addresses the limitations of being limited to single <b>supervised</b> domains. Our method combines inter- and intra-task cross-attention mechanisms within a compact <b>convolutional</b> <b>network.</b> This integration enables the model to maintain alignment with features from previous tasks, thereby delaying the data drift that may occur between tasks, while performing <b>unsupervised</b> cross-domain (UDA) between related domains. By leveraging an intra-task-specific pseudo-labeling method, we ensure accurate input pairs for both labeled and unlabeled samples, enhancing the learning process. To validate our approach, we conduct extensive experiments on public UDA datasets, showcasing its positive performance on cross-domain <b>continual</b> <b>learning</b> challenges. Additionally, our work introduces incremental ideas that contribute to the advancement of this field. We make our code and models available to encourage further exploration and reproduction of our results: \url{https://github.com/Ivsucram/CDCL}</p></p class="citation"></blockquote><h3 id=971--118346-uncertainty-quantification-in-fine-tuned-llms-using-lora-ensembles-oleksandr-balabanov-et-al-2024>(9/71 | 118/346) Uncertainty quantification in fine-tuned LLMs using LoRA ensembles (Oleksandr Balabanov et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Oleksandr Balabanov, Hampus Linander. (2024)<br><strong>Uncertainty quantification in fine-tuned LLMs using LoRA ensembles</strong><br><button class=copy-to-clipboard title="Uncertainty quantification in fine-tuned LLMs using LoRA ensembles" index=118>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-118 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CL, cs-LG, cs.LG, stat-ML<br>Keyword Score: 50<br>Keywords: Fine-tuning, Fine-tuning, Mistral, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12264v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12264v1.pdf filename=2402.12264v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Fine-tuning</b> <b>large</b> <b>language</b> <b>models</b> can improve task specific performance, although a general understanding of what the <b>fine-tuned</b> model has learned, forgotten and how to trust its predictions is still missing. We derive principled uncertainty quantification for <b>fine-tuned</b> <b>LLMs</b> with posterior approximations using computationally efficient low-rank adaptation ensembles. We analyze three common multiple-choice datasets using low-rank adaptation ensembles based on <b>Mistral-7b,</b> and draw quantitative and qualitative conclusions on their perceived complexity and model efficacy on the different target domains during and after <b>fine-tuning.</b> In particular, backed by the numerical experiments, we hypothesise about signals from entropic uncertainty measures for data domains that are inherently difficult for a given architecture to learn.</p></p class="citation"></blockquote><h3 id=1071--119346-wkvquant-quantizing-weight-and-keyvalue-cache-for-large-language-models-gains-more-yuxuan-yue-et-al-2024>(10/71 | 119/346) WKVQuant: Quantizing Weight and Key/Value Cache for Large Language Models Gains More (Yuxuan Yue et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuxuan Yue, Zhihang Yuan, Haojie Duanmu, Sifan Zhou, Jianlong Wu, Liqiang Nie. (2024)<br><strong>WKVQuant: Quantizing Weight and Key/Value Cache for Large Language Models Gains More</strong><br><button class=copy-to-clipboard title="WKVQuant: Quantizing Weight and Key/Value Cache for Large Language Models Gains More" index=119>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-119 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CL, cs-LG, cs.LG<br>Keyword Score: 50<br>Keywords: Quantization, Quantization, Text Generation, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12065v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12065v2.pdf filename=2402.12065v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> face significant deployment challenges due to their substantial memory requirements and the computational demands of auto-regressive <b>text</b> <b>generation</b> process. This paper addresses these challenges by focusing on the <b>quantization</b> of <b>LLMs,</b> a technique that reduces memory consumption by converting model parameters and activations into low-bit integers. We critically analyze the existing <b>quantization</b> approaches, identifying their limitations in balancing the accuracy and efficiency of the <b>quantized</b> <b>LLMs.</b> To advance beyond these limitations, we propose WKVQuant, a PTQ framework especially designed for quantizing weights and the key/value (KV) cache of <b>LLMs.</b> Specifically, we incorporates past-only <b>quantization</b> to improve the computation of attention. Additionally, we introduce two-dimensional <b>quantization</b> strategy to handle the distribution of KV cache, along with a cross-block reconstruction regularization for parameter optimization. Experiments show that WKVQuant achieves almost comparable memory savings to weight-activation <b>quantization,</b> while also approaching the performance of weight-only <b>quantization.</b></p></p class="citation"></blockquote><h3 id=1171--120346-unist-a-prompt-empowered-universal-model-for-urban-spatio-temporal-prediction-yuan-yuan-et-al-2024>(11/71 | 120/346) UniST: A Prompt-Empowered Universal Model for Urban Spatio-Temporal Prediction (Yuan Yuan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuan Yuan, Jingtao Ding, Jie Feng, Depeng Jin, Yong Li. (2024)<br><strong>UniST: A Prompt-Empowered Universal Model for Urban Spatio-Temporal Prediction</strong><br><button class=copy-to-clipboard title="UniST: A Prompt-Empowered Universal Model for Urban Spatio-Temporal Prediction" index=120>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-120 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 50<br>Keywords: Few-shot, Foundation Model, Zero-shot, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11838v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11838v1.pdf filename=2402.11838v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Urban spatio-temporal prediction is crucial for informed decision-making, such as transportation management, resource optimization, and urban planning. Although pretrained <b>foundation</b> <b>models</b> for natural languages have experienced remarkable breakthroughs, wherein one general-purpose model can tackle multiple tasks across various domains, urban spatio-temporal modeling lags behind. Existing approaches for urban prediction are usually tailored for specific spatio-temporal scenarios, requiring task-specific model designs and extensive in-domain training data. In this work, we propose a universal model, UniST, for urban spatio-temporal prediction. Drawing inspiration from <b>large</b> <b>language</b> <b>models,</b> UniST achieves success through: (i) flexibility towards diverse spatio-temporal data characteristics, (ii) effective generative pre-training with elaborated masking strategies to capture complex spatio-temporal relationships, (iii) spatio-temporal knowledge-guided <b>prompts</b> that align and leverage intrinsic and shared knowledge across scenarios. These designs together unlock the potential of a one-for-all model for spatio-temporal prediction with powerful generalization capability. Extensive experiments on 15 cities and 6 domains demonstrate the universality of UniST in advancing state-of-the-art prediction performance, especially in <b>few-shot</b> and <b>zero-shot</b> scenarios.</p></p class="citation"></blockquote><h3 id=1271--121346-tables-as-images-exploring-the-strengths-and-limitations-of-llms-on-multimodal-representations-of-tabular-data-naihao-deng-et-al-2024>(12/71 | 121/346) Tables as Images? Exploring the Strengths and Limitations of LLMs on Multimodal Representations of Tabular Data (Naihao Deng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Naihao Deng, Zhenjie Sun, Ruiqi He, Aman Sikka, Yulong Chen, Lin Ma, Yue Zhang, Rada Mihalcea. (2024)<br><strong>Tables as Images? Exploring the Strengths and Limitations of LLMs on Multimodal Representations of Tabular Data</strong><br><button class=copy-to-clipboard title="Tables as Images? Exploring the Strengths and Limitations of LLMs on Multimodal Representations of Tabular Data" index=121>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-121 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CL, cs-CV, cs-LG, cs.LG<br>Keyword Score: 49<br>Keywords: Benchmarking, Multi-modal, Multi-modal, Fact Verification, Question Answering, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12424v3 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12424v3.pdf filename=2402.12424v3.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we investigate the effectiveness of various <b>LLMs</b> in interpreting tabular data through different <b>prompting</b> strategies and data formats. Our analysis extends across six <b>benchmarks</b> for table-related tasks such as <b>question-answering</b> <b>and</b> <b>fact-checking.</b> <b>We</b> introduce for the first time the assessment of <b>LLMs&rsquo;</b> performance on image-based table representations. Specifically, we compare five text-based and three image-based table representations, demonstrating the influence of representation and <b>prompting</b> on <b>LLM</b> performance. Our study provides insights into the effective use of <b>LLMs</b> on table-related tasks.</p></p class="citation"></blockquote><h3 id=1371--122346-neuro-mimetic-task-free-unsupervised-online-learning-with-continual-self-organizing-maps-hitesh-vaidya-et-al-2024>(13/71 | 122/346) Neuro-mimetic Task-free Unsupervised Online Learning with Continual Self-Organizing Maps (Hitesh Vaidya et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hitesh Vaidya, Travis Desell, Ankur Mali, Alexander Ororbia. (2024)<br><strong>Neuro-mimetic Task-free Unsupervised Online Learning with Continual Self-Organizing Maps</strong><br><button class=copy-to-clipboard title="Neuro-mimetic Task-free Unsupervised Online Learning with Continual Self-Organizing Maps" index=122>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-122 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs-NE, cs.LG<br>Keyword Score: 46<br>Keywords: MNIST, Benchmarking, Clustering, Continual Learning, Unsupervised Learning, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12465v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12465v1.pdf filename=2402.12465v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>An intelligent system capable of <b>continual</b> <b>learning</b> is one that can process and extract knowledge from potentially infinitely long streams of pattern vectors. The major challenge that makes crafting such a system difficult is known as catastrophic forgetting - an agent, such as one based on artificial neural networks (ANNs), struggles to retain previously acquired knowledge when learning from new samples. Furthermore, ensuring that knowledge is preserved for previous tasks becomes more challenging when input is not supplemented with task boundary information. Although forgetting in the context of ANNs has been studied extensively, there still exists far less work investigating it in terms of <b>unsupervised</b> <b>architectures</b> such as the venerable self-organizing map (SOM), a neural model often used in <b>clustering</b> and dimensionality reduction. While the internal mechanisms of SOMs could, in principle, yield sparse representations that improve memory retention, we observe that, when a fixed-size SOM processes continuous data streams, it experiences concept drift. In light of this, we propose a generalization of the SOM, the <b>continual</b> <b>SOM</b> (CSOM), which is capable of online <b>unsupervised</b> <b>learning</b> under a low memory budget. Our results, on <b>benchmarks</b> including <b>MNIST,</b> Kuzushiji-MNIST, and Fashion-MNIST, show almost a two times increase in accuracy, and CIFAR-10 demonstrates a state-of-the-art result when tested on (online) <b>unsupervised</b> <b>class</b> incremental learning setting.</p></p class="citation"></blockquote><h3 id=1471--123346-synthetic-location-trajectory-generation-using-categorical-diffusion-models-simon-dirmeier-et-al-2024>(14/71 | 123/346) Synthetic location trajectory generation using categorical diffusion models (Simon Dirmeier et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Simon Dirmeier, Ye Hong, Fernando Perez-Cruz. (2024)<br><strong>Synthetic location trajectory generation using categorical diffusion models</strong><br><button class=copy-to-clipboard title="Synthetic location trajectory generation using categorical diffusion models" index=123>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-123 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 46<br>Keywords: Diffusion Model, Benchmarking, Benchmarking, Probabilistic Model, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12242v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12242v1.pdf filename=2402.12242v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Diffusion</b> <b>probabilistic</b> <b>models</b> (DPMs) have rapidly evolved to be one of the predominant generative models for the <b>simulation</b> of synthetic data, for instance, for computer vision, audio, natural language processing, or biomolecule generation. Here, we propose using DPMs for the generation of synthetic individual location trajectories (ILTs) which are sequences of variables representing physical locations visited by individuals. ILTs are of major importance in mobility research to understand the mobility behavior of populations and to ultimately inform political decision-making. We represent ILTs as multi-dimensional categorical random variables and propose to model their joint distribution using a continuous DPM by first applying the <b>diffusion</b> <b>process</b> in a continuous unconstrained space and then mapping the continuous variables into a discrete space. We demonstrate that our model can synthesize realistic ILPs by comparing conditionally and unconditionally generated sequences to real-world ILPs from a GNSS tracking data set which suggests the potential use of our model for synthetic data generation, for example, for <b>benchmarking</b> models used in mobility research.</p></p class="citation"></blockquote><h3 id=1571--124346-endowing-pre-trained-graph-models-with-provable-fairness-zhongjian-zhang-et-al-2024>(15/71 | 124/346) Endowing Pre-trained Graph Models with Provable Fairness (Zhongjian Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhongjian Zhang, Mengmei Zhang, Yue Yu, Cheng Yang, Jiawei Liu, Chuan Shi. (2024)<br><strong>Endowing Pre-trained Graph Models with Provable Fairness</strong><br><button class=copy-to-clipboard title="Endowing Pre-trained Graph Models with Provable Fairness" index=124>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-124 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CY, cs-LG, cs-SI, cs.LG<br>Keyword Score: 43<br>Keywords: Node Classification, Graph, Graph Neural Network, Fairness, Pre-trained Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12161v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12161v2.pdf filename=2402.12161v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Pre-trained</b> <b>graph</b> <b>models</b> (PGMs) aim to capture transferable inherent structural properties and apply them to different downstream tasks. Similar to <b>pre-trained</b> <b>language</b> <b>models,</b> PGMs also inherit biases from human society, resulting in discriminatory behavior in downstream applications. The debiasing process of existing fair methods is generally coupled with parameter optimization of <b>GNNs.</b> However, different downstream tasks may be associated with different sensitive attributes in reality, directly employing existing methods to improve the <b>fairness</b> of PGMs is inflexible and inefficient. Moreover, most of them lack a theoretical guarantee, i.e., provable lower bounds on the <b>fairness</b> of model predictions, which directly provides assurance in a practical scenario. To overcome these limitations, we propose a novel adapter-tuning framework that endows <b>pre-trained</b> <b>graph</b> <b>models</b> with provable <b>fairness</b> (called GraphPAR). GraphPAR freezes the parameters of PGMs and trains a parameter-efficient adapter to flexibly improve the <b>fairness</b> of PGMs in downstream tasks. Specifically, we design a sensitive semantic augmenter on <b>node</b> <b>representations,</b> to extend the <b>node</b> <b>representations</b> with different sensitive attribute semantics for each <b>node.</b> <b>The</b> extended representations will be used to further train an adapter, to prevent the propagation of sensitive attribute semantics from PGMs to task predictions. Furthermore, with GraphPAR, we quantify whether the <b>fairness</b> of each <b>node</b> <b>is</b> provable, i.e., predictions are always fair within a certain range of sensitive attribute semantics. Experimental evaluations on real-world datasets demonstrate that GraphPAR achieves state-of-the-art prediction performance and <b>fairness</b> on <b>node</b> <b>classification</b> task. Furthermore, based on our GraphPAR, around 90% <b>nodes</b> <b>have</b> provable <b>fairness.</b></p></p class="citation"></blockquote><h3 id=1671--125346-a-generative-pre-training-framework-for-spatio-temporal-graph-transfer-learning-yuan-yuan-et-al-2024>(16/71 | 125/346) A Generative Pre-Training Framework for Spatio-Temporal Graph Transfer Learning (Yuan Yuan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuan Yuan, Chenyang Shao, Jingtao Ding, Depeng Jin, Yong Li. (2024)<br><strong>A Generative Pre-Training Framework for Spatio-Temporal Graph Transfer Learning</strong><br><button class=copy-to-clipboard title="A Generative Pre-Training Framework for Spatio-Temporal Graph Transfer Learning" index=125>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-125 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 43<br>Keywords: Diffusion Model, Graph, Transfer Learning, Transformer, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11922v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11922v2.pdf filename=2402.11922v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Spatio-temporal <b>graph</b> (STG) learning is foundational for smart city applications, yet it is often hindered by data scarcity in many cities and regions. To bridge this gap, we propose a novel generative pre-training framework, GPDiff, for STG <b>transfer</b> <b>learning.</b> Unlike conventional approaches that heavily rely on common feature extraction or intricate <b>transfer</b> <b>learning</b> designs, our solution takes a novel approach by performing generative pre-training on a collection of model parameters optimized with data from source cities. We recast STG <b>transfer</b> <b>learning</b> as pre-training a generative hypernetwork, which generates tailored model parameters guided by <b>prompts,</b> allowing for adaptability to diverse data distributions and city-specific characteristics. GPDiff employs a <b>diffusion</b> <b>model</b> with a <b>transformer-based</b> denoising network, which is model-agnostic to integrate with powerful STG models. By addressing challenges arising from data gaps and the complexity of generalizing knowledge across cities, our framework consistently outperforms state-of-the-art baselines on multiple real-world datasets for tasks such as traffic speed prediction and crowd flow prediction. The implementation of our approach is available: <a href=https://github.com/PLUTO-SCY/GPDiff>https://github.com/PLUTO-SCY/GPDiff</a>.</p></p class="citation"></blockquote><h3 id=1771--126346-on-the-byzantine-resilience-of-distillation-based-federated-learning-christophe-roux-et-al-2024>(17/71 | 126/346) On the Byzantine-Resilience of Distillation-Based Federated Learning (Christophe Roux et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Christophe Roux, Max Zimmer, Sebastian Pokutta. (2024)<br><strong>On the Byzantine-Resilience of Distillation-Based Federated Learning</strong><br><button class=copy-to-clipboard title="On the Byzantine-Resilience of Distillation-Based Federated Learning" index=126>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-126 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-DC, cs-LG, cs.LG<br>Keyword Score: 40<br>Keywords: Federated Learning, Knowledge Distillation, Knowledge Distillation, Knowledge Distillation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12265v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12265v1.pdf filename=2402.12265v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Federated</b> <b>Learning</b> (FL) algorithms using <b>Knowledge</b> <b>Distillation</b> <b>(KD)</b> have received increasing attention due to their favorable properties with respect to privacy, non-i.i.d. data and communication cost. These methods depart from transmitting model parameters and, instead, communicate information about a learning task by sharing predictions on a public dataset. In this work, we study the performance of such approaches in the byzantine setting, where a subset of the clients act in an adversarial manner aiming to disrupt the learning process. We show that <b>KD-based</b> FL algorithms are remarkably resilient and analyze how byzantine clients can influence the learning process compared to <b>Federated</b> <b>Averaging.</b> Based on these insights, we introduce two new byzantine attacks and demonstrate that they are effective against prior byzantine-resilient methods. Additionally, we propose FilterExp, a novel method designed to enhance the byzantine resilience of <b>KD-based</b> FL algorithms and demonstrate its efficacy. Finally, we provide a general method to make attacks harder to detect, improving their effectiveness.</p></p class="citation"></blockquote><h3 id=1871--127346-all-language-models-large-and-small-zhixun-chen-et-al-2024>(18/71 | 127/346) All Language Models Large and Small (Zhixun Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhixun Chen, Yali Du, David Mguni. (2024)<br><strong>All Language Models Large and Small</strong><br><button class=copy-to-clipboard title="All Language Models Large and Small" index=127>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-127 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CL, cs-LG, cs.LG<br>Keyword Score: 40<br>Keywords: Low-Resource, Reinforcement Learning, Reasoning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12061v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12061v1.pdf filename=2402.12061v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Many leading language models (LMs) use high-intensity computational resources both during training and execution. This poses the challenge of lowering resource costs for deployment and faster execution of decision-making tasks among others. We introduce a novel plug-and-play LM framework named Language Optimising Network Distribution (LONDI) framework. LONDI learns to selectively employ large LMs only where complex decision-making and <b>reasoning</b> are required while using <b>low-resource</b> LMs everywhere else. LONDI consists of a system of two (off-)policy networks, an LM, a large LM <b>(LLM),</b> and a <b>reinforcement</b> <b>learning</b> module that uses switching controls to quickly learn which system states to call the <b>LLM.</b> We then introduce a variant of LONDI that maintains budget constraints on <b>LLM</b> calls and hence its resource usage. Theoretically, we prove LONDI learns the subset of system states to activate the <b>LLM</b> required to solve the task. We then prove that LONDI converges to optimal solutions while also preserving budgetary constraints on <b>LLM</b> calls almost surely enabling it to solve various tasks while significantly lowering computational costs. We test LONDI&rsquo;s performance in a range of tasks in ScienceWorld and BabyAI-Text and demonstrate that LONDI can solve tasks only solvable by resource-intensive <b>LLMs</b> while reducing GPU usage by up to 30%.</p></p class="citation"></blockquote><h3 id=1971--128346-reinforcement-learning-as-a-parsimonious-alternative-to-prediction-cascades-a-case-study-on-image-segmentation-bharat-srikishan-et-al-2024>(19/71 | 128/346) Reinforcement Learning as a Parsimonious Alternative to Prediction Cascades: A Case Study on Image Segmentation (Bharat Srikishan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bharat Srikishan, Anika Tabassum, Srikanth Allu, Ramakrishnan Kannan, Nikhil Muralidhar. (2024)<br><strong>Reinforcement Learning as a Parsimonious Alternative to Prediction Cascades: A Case Study on Image Segmentation</strong><br><button class=copy-to-clipboard title="Reinforcement Learning as a Parsimonious Alternative to Prediction Cascades: A Case Study on Image Segmentation" index=128>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-128 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CV, cs-LG, cs.LG<br>Keyword Score: 40<br>Keywords: MNIST, Object Detection, Low-Resource, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11760v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11760v1.pdf filename=2402.11760v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep learning architectures have achieved state-of-the-art (SOTA) performance on computer vision tasks such as <b>object</b> <b>detection</b> and image segmentation. This may be attributed to the use of over-parameterized, monolithic deep learning architectures executed on large datasets. Although such architectures lead to increased accuracy, this is usually accompanied by a large increase in computation and memory requirements during inference. While this is a non-issue in traditional machine learning pipelines, the recent confluence of machine learning and fields like the Internet of Things has rendered such large architectures infeasible for execution in <b>low-resource</b> settings. In such settings, previous efforts have proposed decision cascades where inputs are passed through models of increasing complexity until desired performance is achieved. However, we argue that cascaded prediction leads to increased computational cost due to wasteful intermediate computations. To address this, we propose PaSeR (Parsimonious Segmentation with <b>Reinforcement</b> <b>Learning)</b> a non-cascading, cost-aware learning pipeline as an alternative to cascaded architectures. Through experimental evaluation on real-world and standard datasets, we demonstrate that PaSeR achieves better accuracy while minimizing computational cost relative to cascaded models. Further, we introduce a new metric IoU/GigaFlop to evaluate the balance between cost and performance. On the real-world task of battery material phase segmentation, PaSeR yields a minimum performance improvement of 174% on the IoU/GigaFlop metric with respect to baselines. We also demonstrate PaSeR&rsquo;s adaptability to complementary models trained on a noisy <b>MNIST</b> dataset, where it achieved a minimum performance improvement on IoU/GigaFlop of 13.4% over SOTA models. Code and data are available at <a href=https://github.com/scailab/paser>https://github.com/scailab/paser</a> .</p></p class="citation"></blockquote><h3 id=2071--129346-generative-semi-supervised-graph-anomaly-detection-hezhe-qiao-et-al-2024>(20/71 | 129/346) Generative Semi-supervised Graph Anomaly Detection (Hezhe Qiao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hezhe Qiao, Qingsong Wen, Xiaoli Li, Ee-Peng Lim, Guansong Pang. (2024)<br><strong>Generative Semi-supervised Graph Anomaly Detection</strong><br><button class=copy-to-clipboard title="Generative Semi-supervised Graph Anomaly Detection" index=129>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-129 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 36<br>Keywords: Graph Anomaly Detection, Graph, Anomaly Detection, Benchmarking, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11887v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11887v1.pdf filename=2402.11887v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This work considers a practical semi-supervised <b>graph</b> <b>anomaly</b> <b>detection</b> (GAD) scenario, where part of the nodes in a <b>graph</b> <b>are</b> <b>known</b> to be normal, contrasting to the <b>unsupervised</b> setting in most GAD studies with a fully unlabeled <b>graph.</b> <b>As</b> <b>expected,</b> we find that having access to these normal nodes helps enhance the detection performance of existing <b>unsupervised</b> GAD methods when they are adapted to the semi-supervised setting. However, their utilization of these normal nodes is limited. In this paper, we propose a novel Generative GAD approach (GGAD) for the semi-supervised scenario to better exploit the normal nodes. The key idea is to generate outlier nodes that assimilate <b>anomaly</b> <b>nodes</b> in both local structure and node representations for providing effective negative node samples in training a discriminative one-class classifier. There have been many generative <b>anomaly</b> <b>detection</b> approaches, but they are designed for non-graph data, and as a result, they fail to take account of the <b>graph</b> <b>structure</b> <b>information.</b> Our approach tackles this problem by generating <b>graph</b> <b>structure-aware</b> <b>outlier</b> nodes that have asymmetric affinity separability from normal nodes while being enforced to achieve egocentric closeness to normal nodes in the node representation space. Comprehensive experiments on four real-world datasets are performed to establish a <b>benchmark</b> for semi-supervised GAD and show that GGAD substantially outperforms state-of-the-art <b>unsupervised</b> and semi-supervised GAD methods with varying numbers of training normal nodes. Code will be made available at <a href=https://github.com/mala-lab/GGAD>https://github.com/mala-lab/GGAD</a>.</p></p class="citation"></blockquote><h3 id=2171--130346-locality-sensitive-hashing-based-efficient-point-transformer-with-applications-in-high-energy-physics-siqi-miao-et-al-2024>(21/71 | 130/346) Locality-Sensitive Hashing-Based Efficient Point Transformer with Applications in High-Energy Physics (Siqi Miao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Siqi Miao, Zhiyuan Lu, Mia Liu, Javier Duarte, Pan Li. (2024)<br><strong>Locality-Sensitive Hashing-Based Efficient Point Transformer with Applications in High-Energy Physics</strong><br><button class=copy-to-clipboard title="Locality-Sensitive Hashing-Based Efficient Point Transformer with Applications in High-Energy Physics" index=130>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-130 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, hep-ex<br>Keyword Score: 33<br>Keywords: Graph, Graph Neural Network, Graph Neural Network, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12535v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12535v1.pdf filename=2402.12535v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This study introduces a novel <b>transformer</b> model optimized for large-scale point cloud processing in scientific domains such as high-energy physics (HEP) and astrophysics. Addressing the limitations of <b>graph</b> <b>neural</b> <b>networks</b> and standard <b>transformers,</b> our model integrates local inductive bias and achieves near-linear complexity with hardware-friendly regular operations. One contribution of this work is the quantitative analysis of the error-complexity tradeoff of various sparsification techniques for building efficient <b>transformers.</b> Our findings highlight the superiority of using locality-sensitive hashing (LSH), especially OR & AND-construction LSH, in kernel approximation for large-scale point cloud data with local inductive bias. Based on this finding, we propose LSH-based Efficient Point <b>Transformer</b> (\textbf{HEPT}), which combines E$^2$LSH with OR & AND constructions and is built upon regular computations. HEPT demonstrates remarkable performance in two critical yet time-consuming HEP tasks, significantly outperforming existing <b>GNNs</b> and <b>transformers</b> in accuracy and computational speed, marking a significant advancement in geometric deep learning and large-scale scientific data processing. Our code is available at \url{https://github.com/Graph-COM/HEPT}.</p></p class="citation"></blockquote><h3 id=2271--131346-the-edge-of-reach-problem-in-offline-model-based-reinforcement-learning-anya-sims-et-al-2024>(22/71 | 131/346) The Edge-of-Reach Problem in Offline Model-Based Reinforcement Learning (Anya Sims et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Anya Sims, Cong Lu, Yee Whye Teh. (2024)<br><strong>The Edge-of-Reach Problem in Offline Model-Based Reinforcement Learning</strong><br><button class=copy-to-clipboard title="The Edge-of-Reach Problem in Offline Model-Based Reinforcement Learning" index=131>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-131 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 33<br>Keywords: Benchmarking, Offline Reinforcement Learning, Online Reinforcement Learning, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12527v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12527v1.pdf filename=2402.12527v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Offline</b> <b>reinforcement</b> <b>learning</b> aims to enable agents to be trained from pre-collected datasets, however, this comes with the added challenge of estimating the value of behavior not covered in the dataset. Model-based methods offer a solution by allowing agents to collect additional synthetic data via rollouts in a learned dynamics model. The prevailing theoretical understanding is that this can then be viewed as <b>online</b> <b>reinforcement</b> <b>learning</b> in an approximate dynamics model, and any remaining gap is therefore assumed to be due to the imperfect dynamics model. Surprisingly, however, we find that if the learned dynamics model is replaced by the true error-free dynamics, existing model-based methods completely fail. This reveals a major misconception. Our subsequent investigation finds that the general procedure used in model-based algorithms results in the existence of a set of edge-of-reach states which trigger pathological value overestimation and collapse in Bellman-based algorithms. We term this the edge-of-reach problem. Based on this, we fill some gaps in existing theory and also explain how prior model-based methods are inadvertently addressing the true underlying edge-of-reach problem. Finally, we propose Reach-Aware Value Learning (RAVL), a simple and robust method that directly addresses the edge-of-reach problem and achieves strong performance across both proprioceptive and pixel-based <b>benchmarks.</b> Code open-sourced at: <a href=https://github.com/anyasims/edge-of-reach>https://github.com/anyasims/edge-of-reach</a>.</p></p class="citation"></blockquote><h3 id=2371--132346-convergence-of-gradient-descent-for-recurrent-neural-networks-a-nonasymptotic-analysis-semih-cayci-et-al-2024>(23/71 | 132/346) Convergence of Gradient Descent for Recurrent Neural Networks: A Nonasymptotic Analysis (Semih Cayci et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Semih Cayci, Atilla Eryilmaz. (2024)<br><strong>Convergence of Gradient Descent for Recurrent Neural Networks: A Nonasymptotic Analysis</strong><br><button class=copy-to-clipboard title="Convergence of Gradient Descent for Recurrent Neural Networks: A Nonasymptotic Analysis" index=132>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-132 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, math-OC, stat-ML<br>Keyword Score: 33<br>Keywords: Sample Size, Supervised Learning, Supervised Learning, Recurrent Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12241v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12241v1.pdf filename=2402.12241v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We analyze <b>recurrent</b> <b>neural</b> <b>networks</b> trained with gradient descent in the <b>supervised</b> <b>learning</b> setting for dynamical systems, and prove that gradient descent can achieve optimality \emph{without} massive overparameterization. Our in-depth nonasymptotic analysis (i) provides sharp bounds on the network size $m$ and iteration complexity $\tau$ in terms of the sequence length $T$, <b>sample</b> <b>size</b> $n$ and ambient dimension $d$, and (ii) identifies the significant impact of long-term dependencies in the dynamical system on the convergence and network width bounds characterized by a cutoff point that depends on the Lipschitz continuity of the activation function. Remarkably, this analysis reveals that an appropriately-initialized <b>recurrent</b> <b>neural</b> <b>network</b> trained with $n$ <b>samples</b> <b>can</b> achieve optimality with a network size $m$ that scales only logarithmically with $n$. This sharply contrasts with the prior works that require high-order polynomial dependency of $m$ on $n$ to establish strong regularity conditions. Our results are based on an explicit characterization of the class of dynamical systems that can be approximated and learned by <b>recurrent</b> <b>neural</b> <b>networks</b> via norm-constrained transportation mappings, and establishing local smoothness properties of the hidden state with respect to the learnable parameters.</p></p class="citation"></blockquote><h3 id=2471--133346-cluster-metric-sensitivity-to-irrelevant-features-miles-mccrory-et-al-2024>(24/71 | 133/346) Cluster Metric Sensitivity to Irrelevant Features (Miles McCrory et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Miles McCrory, Spencer A. Thomas. (2024)<br><strong>Cluster Metric Sensitivity to Irrelevant Features</strong><br><button class=copy-to-clipboard title="Cluster Metric Sensitivity to Irrelevant Features" index=133>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-133 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG, stat-ML<br>Keyword Score: 33<br>Keywords: Clustering, Mutual Information, Supervised Learning, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12008v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12008v1.pdf filename=2402.12008v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Clustering</b> algorithms are used extensively in data analysis for data exploration and discovery. Technological advancements lead to continually growth of data in terms of volume, dimensionality and complexity. This provides great opportunities in data analytics as the data can be interrogated for many different purposes. This however leads challenges, such as identification of relevant features for a given task. In <b>supervised</b> tasks, one can utilise a number of methods to optimise the input features for the task objective (e.g. classification accuracy). In <b>unsupervised</b> problems, such tools are not readily available, in part due to an inability to quantify feature relevance in unlabeled tasks. In this paper, we investigate the sensitivity of <b>clustering</b> performance noisy uncorrelated variables iteratively added to baseline datasets with well defined clusters. We show how different types of irrelevant variables can impact the outcome of a <b>clustering</b> result from $k$-means in different ways. We observe a resilience to very high proportions of irrelevant features for adjusted rand index (ARI) and normalised <b>mutual</b> <b>information</b> (NMI) when the irrelevant features are Gaussian distributed. For Uniformly distributed irrelevant features, we notice the resilience of ARI and NMI is dependent on the dimensionality of the data and exhibits tipping points between high scores and near zero. Our results show that the Silhouette Coefficient and the Davies-Bouldin score are the most sensitive to irrelevant added features exhibiting large changes in score for comparably low proportions of irrelevant features regardless of underlying distribution or data scaling. As such the Silhouette Coefficient and the Davies-Bouldin score are good candidates for optimising feature selection in <b>unsupervised</b> <b>clustering</b> tasks.</p></p class="citation"></blockquote><h3 id=2571--134346-ebft-effective-and-block-wise-fine-tuning-for-sparse-llms-song-guo-et-al-2024>(25/71 | 134/346) EBFT: Effective and Block-Wise Fine-Tuning for Sparse LLMs (Song Guo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Song Guo, Fan Wu, Lei Zhang, Xiawu Zheng, Shengchuan Zhang, Fei Chao, Yiyu Shi, Rongrong Ji. (2024)<br><strong>EBFT: Effective and Block-Wise Fine-Tuning for Sparse LLMs</strong><br><button class=copy-to-clipboard title="EBFT: Effective and Block-Wise Fine-Tuning for Sparse LLMs" index=134>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-134 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CL, cs-LG, cs.LG<br>Keyword Score: 33<br>Keywords: Benchmarking, Fine-tuning, Large Language Model, Perplexity<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12419v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12419v1.pdf filename=2402.12419v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Existing methods for <b>fine-tuning</b> sparse <b>LLMs</b> often suffer from resource-intensive requirements and high retraining costs. Additionally, many <b>fine-tuning</b> methods often rely on approximations or heuristic optimization strategies, which may lead to suboptimal solutions. To address these issues, we propose an efficient and fast framework for <b>fine-tuning</b> sparse <b>LLMs</b> based on minimizing reconstruction error. Our approach involves sampling a small dataset for calibration and utilizing backpropagation to iteratively optimize block-wise reconstruction error, on a block-by-block basis, aiming for optimal solutions. Extensive experiments on various <b>benchmarks</b> consistently demonstrate the superiority of our method over other baselines. For instance, on the Wikitext2 dataset with LlamaV1-7B at 70% sparsity, our proposed EBFT achieves a <b>perplexity</b> of 16.88, surpassing the state-of-the-art DSnoT with a <b>perplexity</b> of 75.14. Moreover, with a structured sparsity ratio of 26%, EBFT achieves a <b>perplexity</b> of 16.27, outperforming LoRA <b>(perplexity</b> 16.44). Furthermore, the <b>fine-tuning</b> process of EBFT for LlamaV1-7B only takes approximately 30 minutes, and the entire framework can be executed on a single 16GB GPU. The source code is available at <a href=https://github.com/sunggo/EBFT>https://github.com/sunggo/EBFT</a>.</p></p class="citation"></blockquote><h3 id=2671--135346-slade-detecting-dynamic-anomalies-in-edge-streams-without-labels-via-self-supervised-learning-jongha-lee-et-al-2024>(26/71 | 135/346) SLADE: Detecting Dynamic Anomalies in Edge Streams without Labels via Self-Supervised Learning (Jongha Lee et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jongha Lee, Sunwoo Kim, Kijung Shin. (2024)<br><strong>SLADE: Detecting Dynamic Anomalies in Edge Streams without Labels via Self-Supervised Learning</strong><br><button class=copy-to-clipboard title="SLADE: Detecting Dynamic Anomalies in Edge Streams without Labels via Self-Supervised Learning" index=135>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-135 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs-SI, cs.LG<br>Keyword Score: 33<br>Keywords: Graph, Anomaly Detection, Self-supervised Learning, Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11933v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11933v1.pdf filename=2402.11933v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>To detect anomalies in real-world <b>graphs,</b> such as social, email, and financial networks, various approaches have been developed. While they typically assume static input <b>graphs,</b> most real-world <b>graphs</b> grow over time, naturally represented as edge streams. In this context, we aim to achieve three goals: (a) instantly detecting anomalies as they occur, (b) adapting to dynamically changing states, and (c) handling the scarcity of dynamic <b>anomaly</b> <b>labels.</b> In this paper, we propose SLADE <b>(Self-supervised</b> <b>Learning</b> for <b>Anomaly</b> <b>Detection</b> in Edge Streams) for rapid detection of dynamic anomalies in edge streams, without relying on labels. SLADE detects the shifts of nodes into abnormal states by observing deviations in their interaction patterns over time. To this end, it trains a deep neural network to perform two <b>self-supervised</b> <b>tasks:</b> (a) minimizing drift in node representations and (b) generating long-term interaction patterns from short-term ones. Failure in these tasks for a node signals its deviation from the norm. Notably, the neural network and tasks are carefully designed so that all required operations can be performed in constant time (w.r.t. the <b>graph</b> size) in response to each new edge in the input stream. In dynamic <b>anomaly</b> <b>detection</b> across four real-world datasets, SLADE outperforms nine competing methods, even those leveraging label supervision.</p></p class="citation"></blockquote><h3 id=2771--136346-microstructures-and-accuracy-of-graph-recall-by-large-language-models-yanbang-wang-et-al-2024>(27/71 | 136/346) Microstructures and Accuracy of Graph Recall by Large Language Models (Yanbang Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yanbang Wang, Hejie Cui, Jon Kleinberg. (2024)<br><strong>Microstructures and Accuracy of Graph Recall by Large Language Models</strong><br><button class=copy-to-clipboard title="Microstructures and Accuracy of Graph Recall by Large Language Models" index=136>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-136 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CL, cs-IR, cs-LG, cs-SI, cs.LG<br>Keyword Score: 33<br>Keywords: Graph, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11821v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11821v2.pdf filename=2402.11821v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Graphs</b> data is crucial for many applications, and much of it exists in the relations described in textual format. As a result, being able to accurately recall and encode a <b>graph</b> described in earlier text is a basic yet pivotal ability that <b>LLMs</b> need to demonstrate if they are to perform <b>reasoning</b> tasks that involve <b>graph-structured</b> information. Human performance at <b>graph</b> recall has been studied by cognitive scientists for decades, and has been found to often exhibit certain structural patterns of bias that align with human handling of social relationships. To date, however, we know little about how <b>LLMs</b> behave in analogous <b>graph</b> recall tasks: do their recalled <b>graphs</b> also exhibit certain biased patterns, and if so, how do they compare with humans and affect other <b>graph</b> <b>reasoning</b> tasks? In this work, we perform the first systematical study of <b>graph</b> recall by <b>LLMs,</b> investigating the accuracy and biased microstructures (local structural patterns) in their recall. We find that <b>LLMs</b> not only underperform often in <b>graph</b> recall, but also tend to favor more triangles and alternating 2-paths. Moreover, we find that more advanced <b>LLMs</b> have a striking dependence on the domain that a real-world <b>graph</b> comes from &ndash; by yielding the best recall accuracy when the <b>graph</b> is narrated in a language style consistent with its original domain.</p></p class="citation"></blockquote><h3 id=2871--137346-induced-model-matching-how-restricted-models-can-help-larger-ones-usama-muneeb-et-al-2024>(28/71 | 137/346) Induced Model Matching: How Restricted Models Can Help Larger Ones (Usama Muneeb et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Usama Muneeb, Mesrob I. Ohannessian. (2024)<br><strong>Induced Model Matching: How Restricted Models Can Help Larger Ones</strong><br><button class=copy-to-clipboard title="Induced Model Matching: How Restricted Models Can Help Larger Ones" index=137>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-137 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CL, cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Logistic Regression, LSTM, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12513v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12513v1.pdf filename=2402.12513v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We consider scenarios where a very accurate predictive model using restricted features is available at the time of training of a larger, full-featured, model. This restricted model may be thought of as &ldquo;side-information&rdquo;, derived either from an auxiliary exhaustive dataset or on the same dataset, by forcing the restriction. How can the restricted model be useful to the full model? We propose an approach for transferring the knowledge of the restricted model to the full model, by aligning the full model&rsquo;s context-restricted performance with that of the restricted model&rsquo;s. We call this methodology Induced Model Matching (IMM) and first illustrate its general applicability by using <b>logistic</b> <b>regression</b> as a toy example. We then explore IMM&rsquo;s use in language modeling, the application that initially inspired it, and where it offers an explicit foundation in contrast to the implicit use of restricted models in techniques such as noising. We demonstrate the methodology on both <b>LSTM</b> and <b>transformer</b> full models, using $N$-grams as restricted models. To further illustrate the potential of the principle whenever it is much cheaper to collect restricted rather than full information, we conclude with a simple RL example where POMDP policies can improve learned MDP policies via IMM.</p></p class="citation"></blockquote><h3 id=2971--138346-in-deep-reinforcement-learning-a-pruned-network-is-a-good-network-johan-obando-ceron-et-al-2024>(29/71 | 138/346) In deep reinforcement learning, a pruned network is a good network (Johan Obando-Ceron et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Johan Obando-Ceron, Aaron Courville, Pablo Samuel Castro. (2024)<br><strong>In deep reinforcement learning, a pruned network is a good network</strong><br><button class=copy-to-clipboard title="In deep reinforcement learning, a pruned network is a good network" index=138>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-138 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Pruning, Reinforcement Learning, Scaling Law<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12479v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12479v1.pdf filename=2402.12479v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent work has shown that deep <b>reinforcement</b> <b>learning</b> agents have difficulty in effectively using their network parameters. We leverage prior insights into the advantages of sparse training techniques and demonstrate that gradual magnitude <b>pruning</b> enables agents to maximize parameter effectiveness. This results in networks that yield dramatic performance improvements over traditional networks and exhibit a type of <b>&ldquo;scaling</b> <b>law&rdquo;,</b> using only a small fraction of the full network parameters.</p></p class="citation"></blockquote><h3 id=3071--139346-universal-physics-transformers-benedikt-alkin-et-al-2024>(30/71 | 139/346) Universal Physics Transformers (Benedikt Alkin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Benedikt Alkin, Andreas Fürst, Simon Schmid, Lukas Gruber, Markus Holzleitner, Johannes Brandstetter. (2024)<br><strong>Universal Physics Transformers</strong><br><button class=copy-to-clipboard title="Universal Physics Transformers" index=139>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-139 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG, physics-flu-dyn<br>Keyword Score: 30<br>Keywords: Simulation, Simulator, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12365v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12365v1.pdf filename=2402.12365v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep neural network based surrogates for partial differential equations have recently gained increased interest. However, akin to their numerical counterparts, different techniques are used across applications, even if the underlying dynamics of the systems are similar. A prominent example is the Lagrangian and Eulerian specification in computational fluid dynamics, posing a challenge for neural networks to effectively model particle- as opposed to grid-based dynamics. We introduce Universal Physics <b>Transformers</b> (UPTs), a novel learning paradigm which models a wide range of spatio-temporal problems - both for Lagrangian and Eulerian discretization schemes. UPTs operate without grid- or particle-based latent structures, enabling flexibility across meshes and particles. UPTs efficiently propagate dynamics in the latent space, emphasized by inverse encoding and decoding techniques. Finally, UPTs allow for queries of the latent space representation at any point in space-time. We demonstrate the efficacy of UPTs in mesh-based fluid <b>simulations,</b> steady-state Reynolds averaged Navier-Stokes <b>simulations,</b> and Lagrangian-based dynamics. Project page: <a href=https://ml-jku.github.io/UPT>https://ml-jku.github.io/UPT</a></p></p class="citation"></blockquote><h3 id=3171--140346-generating-survival-interpretable-trajectories-and-data-andrei-v-konstantinov-et-al-2024>(31/71 | 140/346) Generating Survival Interpretable Trajectories and Data (Andrei V. Konstantinov et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Andrei V. Konstantinov, Stanislav R. Kirpichenko, Lev V. Utkin. (2024)<br><strong>Generating Survival Interpretable Trajectories and Data</strong><br><button class=copy-to-clipboard title="Generating Survival Interpretable Trajectories and Data" index=140>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-140 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG, stat-ML<br>Keyword Score: 30<br>Keywords: Autoencoder, Counter-factual, Variational Autoencoder<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12331v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12331v1.pdf filename=2402.12331v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A new model for generating survival trajectories and data based on applying an <b>autoencoder</b> of a specific structure is proposed. It solves three tasks. First, it provides predictions in the form of the expected event time and the survival function for a new generated feature vector on the basis of the Beran estimator. Second, the model generates additional data based on a given training set that would supplement the original dataset. Third, the most important, it generates a prototype time-dependent trajectory for an object, which characterizes how features of the object could be changed to achieve a different time to an event. The trajectory can be viewed as a type of the <b>counterfactual</b> explanation. The proposed model is robust during training and inference due to a specific weighting scheme incorporating into the <b>variational</b> <b>autoencoder.</b> The model also determines the censored indicators of new generated data by solving a classification task. The paper demonstrates the efficiency and properties of the proposed model using numerical experiments on synthetic and real datasets. The code of the algorithm implementing the proposed model is publicly available.</p></p class="citation"></blockquote><h3 id=3271--141346-towards-a-tailored-mixed-precision-sub-8bit-quantization-scheme-for-gated-recurrent-units-using-genetic-algorithms-riccardo-miccini-et-al-2024>(32/71 | 141/346) Towards a tailored mixed-precision sub-8bit quantization scheme for Gated Recurrent Units using Genetic Algorithms (Riccardo Miccini et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Riccardo Miccini, Alessandro Cerioli, Clément Laroche, Tobias Piechowiak, Jens Sparsø, Luca Pezzarossa. (2024)<br><strong>Towards a tailored mixed-precision sub-8bit quantization scheme for Gated Recurrent Units using Genetic Algorithms</strong><br><button class=copy-to-clipboard title="Towards a tailored mixed-precision sub-8bit quantization scheme for Gated Recurrent Units using Genetic Algorithms" index=141>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-141 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs-NE, cs.LG, eess-SP<br>Keyword Score: 30<br>Keywords: Graph Attention Networks, Model Compression, Quantization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12263v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12263v1.pdf filename=2402.12263v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Despite the recent advances in <b>model</b> <b>compression</b> techniques for deep neural networks, deploying such <b>models</b> <b>on</b> ultra-low-power embedded devices still proves challenging. In particular, <b>quantization</b> schemes for <b>Gated</b> Recurrent Units (GRU) are difficult to tune due to their dependence on an internal state, preventing them from fully benefiting from sub-8bit <b>quantization.</b> In this work, we propose a modular integer <b>quantization</b> scheme for GRUs where the bit width of each operator can be selected independently. We then employ Genetic Algorithms (GA) to explore the vast search space of possible bit widths, simultaneously optimising for <b>model</b> <b>size</b> and accuracy. We evaluate our methods on four different sequential tasks and demonstrate that mixed-precision solutions exceed homogeneous-precision ones in terms of Pareto efficiency. In our results, we achieve a <b>model</b> <b>size</b> reduction between 25% and 55% while maintaining an accuracy comparable with the 8-bit homogeneous equivalent.</p></p class="citation"></blockquote><h3 id=3371--142346-revisiting-data-augmentation-in-deep-reinforcement-learning-jianshu-hu-et-al-2024>(33/71 | 142/346) Revisiting Data Augmentation in Deep Reinforcement Learning (Jianshu Hu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jianshu Hu, Yunpeng Jiang, Paul Weng. (2024)<br><strong>Revisiting Data Augmentation in Deep Reinforcement Learning</strong><br><button class=copy-to-clipboard title="Revisiting Data Augmentation in Deep Reinforcement Learning" index=142>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-142 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CV, cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Data Augmentation, Recommendation, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12181v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12181v1.pdf filename=2402.12181v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Various <b>data</b> <b>augmentation</b> techniques have been recently proposed in image-based deep <b>reinforcement</b> <b>learning</b> (DRL). Although they empirically demonstrate the effectiveness of <b>data</b> <b>augmentation</b> for improving sample efficiency or generalization, which technique should be preferred is not always clear. To tackle this question, we analyze existing methods to better understand them and to uncover how they are connected. Notably, by expressing the variance of the Q-targets and that of the empirical actor/critic losses of these methods, we can analyze the effects of their different components and compare them. We furthermore formulate an explanation about how these methods may be affected by choosing different <b>data</b> <b>augmentation</b> transformations in calculating the target Q-values. This analysis suggests <b>recommendations</b> on how to exploit <b>data</b> <b>augmentation</b> in a more principled way. In addition, we include a regularization term called tangent prop, previously proposed in computer vision, but whose adaptation to DRL is novel to the best of our knowledge. We evaluate our proposition and validate our analysis in several domains. Compared to different relevant baselines, we demonstrate that it achieves state-of-the-art performance in most environments and shows higher sample efficiency and better generalization ability in some complex environments.</p></p class="citation"></blockquote><h3 id=3471--143346-beyond-uniform-scaling-exploring-depth-heterogeneity-in-neural-architectures-akash-guna-r-t-et-al-2024>(34/71 | 143/346) Beyond Uniform Scaling: Exploring Depth Heterogeneity in Neural Architectures (Akash Guna R. T et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Akash Guna R. T, Arnav Chavan, Deepak Gupta. (2024)<br><strong>Beyond Uniform Scaling: Exploring Depth Heterogeneity in Neural Architectures</strong><br><button class=copy-to-clipboard title="Beyond Uniform Scaling: Exploring Depth Heterogeneity in Neural Architectures" index=143>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-143 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs-NE, cs.LG<br>Keyword Score: 30<br>Keywords: Vision Transformer, Transformer, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12418v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12418v1.pdf filename=2402.12418v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Conventional scaling of neural networks typically involves designing a base network and growing different dimensions like width, depth, etc. of the same by some predefined scaling factors. We introduce an automated scaling approach leveraging second-order loss landscape information. Our method is flexible towards skip connections a mainstay in modern <b>vision</b> <b>transformers.</b> Our training-aware method jointly scales and trains <b>transformers</b> without additional training iterations. Motivated by the hypothesis that not all neurons need uniform depth complexity, our approach embraces depth heterogeneity. Extensive evaluations on DeiT-S with ImageNet100 show a 2.5% accuracy gain and 10% parameter efficiency improvement over conventional scaling. Scaled networks demonstrate superior performance upon training small scale datasets from scratch. We introduce the first intact scaling mechanism for <b>vision</b> <b>transformers,</b> a step towards efficient model scaling.</p></p class="citation"></blockquote><h3 id=3571--144346-lora-training-in-the-ntk-regime-has-no-spurious-local-minima-uijeong-jang-et-al-2024>(35/71 | 144/346) LoRA Training in the NTK Regime has No Spurious Local Minima (Uijeong Jang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Uijeong Jang, Jason D. Lee, Ernest K. Ryu. (2024)<br><strong>LoRA Training in the NTK Regime has No Spurious Local Minima</strong><br><button class=copy-to-clipboard title="LoRA Training in the NTK Regime has No Spurious Local Minima" index=144>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-144 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, math-OC<br>Keyword Score: 30<br>Keywords: Fine-tuning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11867v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11867v1.pdf filename=2402.11867v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Low-rank adaptation (LoRA) has become the standard approach for parameter-efficient <b>fine-tuning</b> of <b>large</b> <b>language</b> <b>models</b> <b>(LLM),</b> but our theoretical understanding of LoRA has been limited. In this work, we theoretically analyze LoRA <b>fine-tuning</b> in the neural tangent kernel (NTK) regime with $N$ data points, showing: (i) full <b>fine-tuning</b> (without LoRA) admits a low-rank solution of rank $r\lesssim \sqrt{N}$; (ii) using LoRA with rank $r\gtrsim \sqrt{N}$ eliminates spurious local minima, allowing gradient descent to find the low-rank solutions; (iii) the low-rank solution found using LoRA generalizes well.</p></p class="citation"></blockquote><h3 id=3671--145346-parcv2-physics-aware-recurrent-convolutional-neural-networks-for-spatiotemporal-dynamics-modeling-phong-c-h-nguyen-et-al-2024>(36/71 | 145/346) PARCv2: Physics-aware Recurrent Convolutional Neural Networks for Spatiotemporal Dynamics Modeling (Phong C. H. Nguyen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Phong C. H. Nguyen, Xinlun Cheng, Shahab Azarfar, Pradeep Seshadri, Yen T. Nguyen, Munho Kim, Sanghun Choi, H. S. Udaykumar, Stephen Baek. (2024)<br><strong>PARCv2: Physics-aware Recurrent Convolutional Neural Networks for Spatiotemporal Dynamics Modeling</strong><br><button class=copy-to-clipboard title="PARCv2: Physics-aware Recurrent Convolutional Neural Networks for Spatiotemporal Dynamics Modeling" index=145>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-145 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 23<br>Keywords: Benchmarking, Convolution, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12503v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12503v2.pdf filename=2402.12503v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Modeling unsteady, fast transient, and advection-dominated physics problems is a pressing challenge for physics-aware deep learning (PADL). The physics of complex systems is governed by large systems of partial differential equations (PDEs) and ancillary constitutive models with nonlinear structures, as well as evolving state fields exhibiting sharp gradients and rapidly deforming material interfaces. Here, we investigate an inductive bias approach that is versatile and generalizable to model generic nonlinear field evolution problems. Our study focuses on the recent physics-aware recurrent <b>convolutions</b> (PARC), which incorporates a differentiator-integrator architecture that inductively models the spatiotemporal dynamics of generic physical systems. We extend the capabilities of PARC to simulate unsteady, transient, and advection-dominant systems. The extended model, referred to as PARCv2, is equipped with differential operators to model advection-reaction-diffusion equations, as well as a hybrid integral solver for stable, long-time predictions. PARCv2 is tested on both standard <b>benchmark</b> problems in fluid dynamics, namely Burgers and Navier-Stokes equations, and then applied to more complex shock-induced reaction problems in energetic materials. We evaluate the behavior of PARCv2 in comparison to other physics-informed and learning bias models and demonstrate its potential to model unsteady and advection-dominant dynamics regimes.</p></p class="citation"></blockquote><h3 id=3771--146346-end-to-end-supervised-prediction-of-arbitrary-size-graphs-with-partially-masked-fused-gromov-wasserstein-matching-paul-krzakala-et-al-2024>(37/71 | 146/346) End-to-end Supervised Prediction of Arbitrary-size Graphs with Partially-Masked Fused Gromov-Wasserstein Matching (Paul Krzakala et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Paul Krzakala, Junjie Yang, Rémi Flamary, Florence d&rsquo;Alché-Buc, Charlotte Laclau, Matthieu Labeau. (2024)<br><strong>End-to-end Supervised Prediction of Arbitrary-size Graphs with Partially-Masked Fused Gromov-Wasserstein Matching</strong><br><button class=copy-to-clipboard title="End-to-end Supervised Prediction of Arbitrary-size Graphs with Partially-Masked Fused Gromov-Wasserstein Matching" index=146>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-146 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 23<br>Keywords: Graph, Supervised Learning, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12269v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12269v2.pdf filename=2402.12269v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present a novel end-to-end deep learning-based approach for <b>Supervised</b> <b>Graph</b> Prediction (SGP). We introduce an original Optimal Transport (OT)-based loss, the Partially-Masked Fused Gromov-Wasserstein loss (PM-FGW), that allows to directly leverage <b>graph</b> representations such as adjacency and feature matrices. PM-FGW exhibits all the desirable properties for SGP: it is node permutation invariant, sub-differentiable and handles <b>graphs</b> of different sizes by comparing their padded representations as well as their masking vectors. Moreover, we present a flexible <b>transformer-based</b> architecture that easily adapts to different types of input data. In the experimental section, three different tasks, a novel and challenging synthetic dataset (image2graph) and two real-world tasks, image2map and fingerprint2molecule - showcase the efficiency and versatility of the approach compared to competitors.</p></p class="citation"></blockquote><h3 id=3871--147346-a-mechanistic-analysis-of-a-transformer-trained-on-a-symbolic-multi-step-reasoning-task-jannik-brinkmann-et-al-2024>(38/71 | 147/346) A Mechanistic Analysis of a Transformer Trained on a Symbolic Multi-Step Reasoning Task (Jannik Brinkmann et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jannik Brinkmann, Abhay Sheshadri, Victor Levoso, Paul Swoboda, Christian Bartelt. (2024)<br><strong>A Mechanistic Analysis of a Transformer Trained on a Symbolic Multi-Step Reasoning Task</strong><br><button class=copy-to-clipboard title="A Mechanistic Analysis of a Transformer Trained on a Symbolic Multi-Step Reasoning Task" index=147>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-147 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 23<br>Keywords: Benchmarking, Transformer, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11917v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11917v1.pdf filename=2402.11917v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Transformers</b> demonstrate impressive performance on a range of <b>reasoning</b> <b>benchmarks.</b> To evaluate the degree to which these abilities are a result of actual <b>reasoning,</b> existing work has focused on developing sophisticated <b>benchmarks</b> for behavioral studies. However, these studies do not provide insights into the internal mechanisms driving the observed capabilities. To improve our understanding of the internal mechanisms of <b>transformers,</b> we present a comprehensive mechanistic analysis of a <b>transformer</b> trained on a synthetic <b>reasoning</b> task. We identify a set of interpretable mechanisms the model uses to solve the task, and validate our findings using correlational and causal evidence. Our results suggest that it implements a depth-bounded recurrent mechanisms that operates in parallel and stores intermediate results in selected token positions. We anticipate that the motifs we identified in our synthetic setting can provide valuable insights into the broader operating principles of <b>transformers</b> and thus provide a basis for understanding more complex models.</p></p class="citation"></blockquote><h3 id=3971--148346-self-guided-robust-graph-structure-refinement-yeonjun-in-et-al-2024>(39/71 | 148/346) Self-Guided Robust Graph Structure Refinement (Yeonjun In et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yeonjun In, Kanghoon Yoon, Kibum Kim, Kijung Shin, Chanyoung Park. (2024)<br><strong>Self-Guided Robust Graph Structure Refinement</strong><br><button class=copy-to-clipboard title="Self-Guided Robust Graph Structure Refinement" index=148>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-148 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 23<br>Keywords: Graph, Graph Neural Network, Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11837v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11837v1.pdf filename=2402.11837v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent studies have revealed that <b>GNNs</b> are vulnerable to <b>adversarial</b> <b>attacks.</b> To defend against such attacks, robust <b>graph</b> structure refinement (GSR) methods aim at minimizing the effect of <b>adversarial</b> <b>edges</b> based on node features, <b>graph</b> structure, or external information. However, we have discovered that existing GSR methods are limited by narrowassumptions, such as assuming clean node features, moderate structural attacks, and the availability of external clean <b>graphs,</b> resulting in the restricted applicability in real-world scenarios. In this paper, we propose a self-guided GSR framework (SG-GSR), which utilizes a clean sub-graph found within the given attacked <b>graph</b> itself. Furthermore, we propose a novel <b>graph</b> augmentation and a group-training strategy to handle the two technical challenges in the clean sub-graph extraction: 1) loss of structural information, and 2) imbalanced node degree distribution. Extensive experiments demonstrate the effectiveness of SG-GSR under various scenarios including non-targeted attacks, targeted attacks, feature attacks, e-commerce fraud, and noisy node labels. Our code is available at <a href=https://github.com/yeonjun-in/torch-SG-GSR>https://github.com/yeonjun-in/torch-SG-GSR</a>.</p></p class="citation"></blockquote><h3 id=4071--149346-easy-as-abcs-unifying-boltzmann-q-learning-and-counterfactual-regret-minimization-luca-damico-wong-et-al-2024>(40/71 | 149/346) Easy as ABCs: Unifying Boltzmann Q-Learning and Counterfactual Regret Minimization (Luca D&rsquo;Amico-Wong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Luca D&rsquo;Amico-Wong, Hugh Zhang, Marc Lanctot, David C. Parkes. (2024)<br><strong>Easy as ABCs: Unifying Boltzmann Q-Learning and Counterfactual Regret Minimization</strong><br><button class=copy-to-clipboard title="Easy as ABCs: Unifying Boltzmann Q-Learning and Counterfactual Regret Minimization" index=149>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-149 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-GT, cs-LG, cs-MA, cs.LG<br>Keyword Score: 23<br>Keywords: Benchmarking, Counter-factual, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11835v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11835v1.pdf filename=2402.11835v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We propose ABCs (Adaptive Branching through Child stationarity), a best-of-both-worlds algorithm combining Boltzmann Q-learning (BQL), a classic <b>reinforcement</b> <b>learning</b> algorithm for single-agent domains, and <b>counterfactual</b> regret minimization (CFR), a central algorithm for learning in multi-agent domains. ABCs adaptively chooses what fraction of the environment to explore each iteration by measuring the stationarity of the environment&rsquo;s reward and transition dynamics. In Markov decision processes, ABCs converges to the optimal policy with at most an O(A) factor slowdown compared to BQL, where A is the number of actions in the environment. In two-player zero-sum games, ABCs is guaranteed to converge to a Nash equilibrium (assuming access to a perfect oracle for detecting stationarity), while BQL has no such guarantees. Empirically, ABCs demonstrates strong performance when <b>benchmarked</b> across environments drawn from the OpenSpiel game library and OpenAI Gym and exceeds all prior methods in environments which are neither fully stationary nor fully nonstationary.</p></p class="citation"></blockquote><h3 id=4171--150346-offline-multi-task-transfer-rl-with-representational-penalization-avinandan-bose-et-al-2024>(41/71 | 150/346) Offline Multi-task Transfer RL with Representational Penalization (Avinandan Bose et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Avinandan Bose, Simon Shaolei Du, Maryam Fazel. (2024)<br><strong>Offline Multi-task Transfer RL with Representational Penalization</strong><br><button class=copy-to-clipboard title="Offline Multi-task Transfer RL with Representational Penalization" index=150>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-150 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Offline Reinforcement Learning, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12570v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12570v1.pdf filename=2402.12570v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We study the problem of representation transfer in <b>offline</b> <b>Reinforcement</b> <b>Learning</b> (RL), where a learner has access to episodic data from a number of source tasks collected a priori, and aims to learn a shared representation to be used in finding a good policy for a target task. Unlike in online RL where the agent interacts with the environment while learning a policy, in the <b>offline</b> <b>setting</b> <b>there</b> cannot be such interactions in either the source tasks or the target task; thus multi-task <b>offline</b> <b>RL</b> <b>can</b> suffer from incomplete coverage. We propose an algorithm to compute pointwise uncertainty measures for the learnt representation, and establish a data-dependent upper bound for the suboptimality of the learnt policy for the target task. Our algorithm leverages the collective exploration done by source tasks to mitigate poor coverage at some points by a few tasks, thus overcoming the limitation of needing uniformly good coverage for a meaningful transfer by existing <b>offline</b> <b>algorithms.</b> <b>We</b> complement our theoretical results with empirical evaluation on a rich-observation MDP which requires many samples for complete coverage. Our findings illustrate the benefits of penalizing and quantifying the uncertainty in the learnt representation.</p></p class="citation"></blockquote><h3 id=4271--151346-dynamic-environment-responsive-online-meta-learning-with-fairness-awareness-chen-zhao-et-al-2024>(42/71 | 151/346) Dynamic Environment Responsive Online Meta-Learning with Fairness Awareness (Chen Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chen Zhao, Feng Mi, Xintao Wu, Kai Jiang, Latifur Khan, Feng Chen. (2024)<br><strong>Dynamic Environment Responsive Online Meta-Learning with Fairness Awareness</strong><br><button class=copy-to-clipboard title="Dynamic Environment Responsive Online Meta-Learning with Fairness Awareness" index=151>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-151 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CY, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Fairness, Meta Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12319v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12319v1.pdf filename=2402.12319v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The <b>fairness-aware</b> online learning framework has emerged as a potent tool within the context of continuous lifelong learning. In this scenario, the learner&rsquo;s objective is to progressively acquire new tasks as they arrive over time, while also guaranteeing statistical parity among various protected sub-populations, such as race and gender, when it comes to the newly introduced tasks. A significant limitation of current approaches lies in their heavy reliance on the i.i.d (independent and identically distributed) assumption concerning data, leading to a static regret analysis of the framework. Nevertheless, it&rsquo;s crucial to note that achieving low static regret does not necessarily translate to strong performance in dynamic environments characterized by tasks sampled from diverse distributions. In this paper, to tackle the <b>fairness-aware</b> online learning challenge in evolving settings, we introduce a unique regret measure, FairSAR, by incorporating long-term <b>fairness</b> constraints into a strongly adapted loss regret framework. Moreover, to determine an optimal model parameter at each time step, we introduce an innovative adaptive <b>fairness-aware</b> online <b>meta-learning</b> <b>algorithm,</b> referred to as FairSAOML. This algorithm possesses the ability to adjust to dynamic environments by effectively managing bias control and model accuracy. The problem is framed as a bi-level convex-concave optimization, considering both the model&rsquo;s primal and dual parameters, which pertain to its accuracy and <b>fairness</b> attributes, respectively. Theoretical analysis yields sub-linear upper bounds for both loss regret and the cumulative violation of <b>fairness</b> constraints. Our experimental evaluation on various real-world datasets in dynamic environments demonstrates that our proposed FairSAOML algorithm consistently outperforms alternative approaches rooted in the most advanced prior online learning methods.</p></p class="citation"></blockquote><h3 id=4371--152346-refining-minimax-regret-for-unsupervised-environment-design-michael-beukman-et-al-2024>(43/71 | 152/346) Refining Minimax Regret for Unsupervised Environment Design (Michael Beukman et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Michael Beukman, Samuel Coward, Michael Matthews, Mattie Fellows, Minqi Jiang, Michael Dennis, Jakob Foerster. (2024)<br><strong>Refining Minimax Regret for Unsupervised Environment Design</strong><br><button class=copy-to-clipboard title="Refining Minimax Regret for Unsupervised Environment Design" index=152>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-152 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Reinforcement Learning, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12284v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12284v1.pdf filename=2402.12284v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In <b>unsupervised</b> environment design, <b>reinforcement</b> <b>learning</b> agents are trained on environment configurations (levels) generated by an adversary that maximises some objective. Regret is a commonly used objective that theoretically results in a minimax regret (MMR) policy with desirable robustness guarantees; in particular, the agent&rsquo;s maximum regret is bounded. However, once the agent reaches this regret bound on all levels, the adversary will only sample levels where regret cannot be further reduced. Although there are possible performance improvements to be made outside of these regret-maximising levels, learning stagnates. In this work, we introduce Bayesian level-perfect MMR (BLP), a refinement of the minimax regret objective that overcomes this limitation. We formally show that solving for this objective results in a subset of MMR policies, and that BLP policies act consistently with a Perfect Bayesian policy over all levels. We further introduce an algorithm, ReMiDi, that results in a BLP policy at convergence. We empirically demonstrate that training on levels from a minimax regret adversary causes learning to prematurely stagnate, but that ReMiDi continues learning.</p></p class="citation"></blockquote><h3 id=4471--153346-dictionary-learning-improves-patch-free-circuit-discovery-in-mechanistic-interpretability-a-case-study-on-othello-gpt-zhengfu-he-et-al-2024>(44/71 | 153/346) Dictionary Learning Improves Patch-Free Circuit Discovery in Mechanistic Interpretability: A Case Study on Othello-GPT (Zhengfu He et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhengfu He, Xuyang Ge, Qiong Tang, Tianxiang Sun, Qinyuan Cheng, Xipeng Qiu. (2024)<br><strong>Dictionary Learning Improves Patch-Free Circuit Discovery in Mechanistic Interpretability: A Case Study on Othello-GPT</strong><br><button class=copy-to-clipboard title="Dictionary Learning Improves Patch-Free Circuit Discovery in Mechanistic Interpretability: A Case Study on Othello-GPT" index=153>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-153 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Out-of-distribution, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12201v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12201v1.pdf filename=2402.12201v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Sparse dictionary learning has been a rapidly growing technique in mechanistic interpretability to attack superposition and extract more human-understandable features from model activations. We ask a further question based on the extracted more monosemantic features: How do we recognize circuits connecting the enormous amount of dictionary features? We propose a circuit discovery framework alternative to activation patching. Our framework suffers less from <b>out-of-distribution</b> and proves to be more efficient in terms of asymptotic complexity. The basic unit in our framework is dictionary features decomposed from all modules writing to the residual stream, including embedding, attention output and MLP output. Starting from any logit, dictionary feature or attention score, we manage to trace down to lower-level dictionary features of all tokens and compute their contribution to these more interpretable and local model behaviors. We dig in a small <b>transformer</b> trained on a synthetic task named Othello and find a number of human-understandable fine-grained circuits inside of it.</p></p class="citation"></blockquote><h3 id=4571--154346-mlfef-machine-learning-fusion-model-with-empirical-formula-to-explore-the-momentum-in-competitive-sports-ruixin-peng-et-al-2024>(45/71 | 154/346) MLFEF: Machine Learning Fusion Model with Empirical Formula to Explore the Momentum in Competitive Sports (Ruixin Peng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ruixin Peng, Ziqing Li. (2024)<br><strong>MLFEF: Machine Learning Fusion Model with Empirical Formula to Explore the Momentum in Competitive Sports</strong><br><button class=copy-to-clipboard title="MLFEF: Machine Learning Fusion Model with Empirical Formula to Explore the Momentum in Competitive Sports" index=154>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-154 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12149v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12149v1.pdf filename=2402.12149v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Tennis is so popular that coaches and players are curious about factors other than skill, such as momentum. This article will try to define and quantify momentum, providing a basis for real-time analysis of tennis matches. Based on the tennis Grand Slam men&rsquo;s singles match data in recent years, we built two models, one is to build a model based on data-driven, and the other is to build a model based on empirical formulas. For the data-driven model, we first found a large amount of public data including public data on tennis matches in the past five years and personal information data of players. Then the data is preprocessed, and feature engineered, and a fusion model of SVM, Random Forrest algorithm and XGBoost was established. For the mechanism analysis model, important features were selected based on the suggestions of many tennis players and enthusiasts, the sliding window algorithm was used to calculate the weight, and different methods were used to visualize the momentum. For further analysis of the momentum fluctuation, it is based on the popular CUMSUM algorithm in the industry as well as the RUN Test, and the result shows the momentum is not random and the trend might be random. At last, the robustness of the fusion model is analyzed by Monte Carlo <b>simulation.</b></p></p class="citation"></blockquote><h3 id=4671--155346-linear-bandits-with-polylogarithmic-minimax-regret-josep-lumbreras-et-al-2024>(46/71 | 155/346) Linear bandits with polylogarithmic minimax regret (Josep Lumbreras et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Josep Lumbreras, Marco Tomamichel. (2024)<br><strong>Linear bandits with polylogarithmic minimax regret</strong><br><button class=copy-to-clipboard title="Linear bandits with polylogarithmic minimax regret" index=155>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-155 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG, stat-ML<br>Keyword Score: 20<br>Keywords: Bandit Algorithm, Bandit Algorithm<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12042v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12042v1.pdf filename=2402.12042v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We study a noise model for linear stochastic <b>bandits</b> <b>for</b> which the subgaussian noise parameter vanishes linearly as we select actions on the unit sphere closer and closer to the unknown vector. We introduce an algorithm for this problem that exhibits a minimax regret scaling as $\log^3(T)$ in the time horizon $T$, in stark contrast the square root scaling of this regret for typical <b>bandit</b> <b>algorithms.</b> Our strategy, based on weighted least-squares estimation, achieves the eigenvalue relation $\lambda_{\min} ( V_t ) = \Omega (\sqrt{\lambda_{\max}(V_t ) })$ for the design matrix $V_t$ at each time step $t$ through geometrical arguments that are independent of the noise model and might be of independent interest. This allows us to tightly control the expected regret in each time step to be of the order $O(\frac1{t})$, leading to the logarithmic scaling of the cumulative regret.</p></p class="citation"></blockquote><h3 id=4771--156346-bayesian-active-learning-for-censored-regression-frederik-boe-hüttel-et-al-2024>(47/71 | 156/346) Bayesian Active Learning for Censored Regression (Frederik Boe Hüttel et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Frederik Boe Hüttel, Christoffer Riis, Filipe Rodrigues, Francisco Câmara Pereira. (2024)<br><strong>Bayesian Active Learning for Censored Regression</strong><br><button class=copy-to-clipboard title="Bayesian Active Learning for Censored Regression" index=156>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-156 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-ML<br>Keyword Score: 20<br>Keywords: Active Learning, Mutual Information<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11973v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11973v1.pdf filename=2402.11973v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Bayesian <b>active</b> <b>learning</b> is based on information theoretical approaches that focus on maximising the information that new observations provide to the model parameters. This is commonly done by maximising the Bayesian <b>Active</b> <b>Learning</b> by Disagreement (BALD) acquisitions function. However, we highlight that it is challenging to estimate BALD when the new data points are subject to censorship, where only clipped values of the targets are observed. To address this, we derive the entropy and the <b>mutual</b> <b>information</b> for censored distributions and derive the BALD objective for <b>active</b> <b>learning</b> in censored regression ($\mathcal{C}$-BALD). We propose a novel modelling approach to estimate the $\mathcal{C}$-BALD objective and use it for <b>active</b> <b>learning</b> in the censored setting. Across a wide range of datasets and models, we demonstrate that $\mathcal{C}$-BALD outperforms other Bayesian <b>active</b> <b>learning</b> methods in censored regression.</p></p class="citation"></blockquote><h3 id=4871--157346-stochastic-approximation-with-delayed-updates-finite-time-rates-under-markovian-sampling-arman-adibi-et-al-2024>(48/71 | 157/346) Stochastic Approximation with Delayed Updates: Finite-Time Rates under Markovian Sampling (Arman Adibi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Arman Adibi, Nicolo Dal Fabbro, Luca Schenato, Sanjeev Kulkarni, H. Vincent Poor, George J. Pappas, Hamed Hassani, Aritra Mitra. (2024)<br><strong>Stochastic Approximation with Delayed Updates: Finite-Time Rates under Markovian Sampling</strong><br><button class=copy-to-clipboard title="Stochastic Approximation with Delayed Updates: Finite-Time Rates under Markovian Sampling" index=157>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-157 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs-MA, cs-SY, cs.LG, eess-SY, math-OC<br>Keyword Score: 20<br>Keywords: Reinforcement Learning, Stochastic Gradient Descent<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11800v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11800v1.pdf filename=2402.11800v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Motivated by applications in large-scale and multi-agent <b>reinforcement</b> <b>learning,</b> we study the non-asymptotic performance of <b>stochastic</b> <b>approximation</b> <b>(SA)</b> schemes with delayed updates under Markovian sampling. While the effect of delays has been extensively studied for optimization, the manner in which they interact with the underlying Markov process to shape the finite-time performance of SA remains poorly understood. In this context, our first main contribution is to show that under time-varying bounded delays, the delayed SA update rule guarantees exponentially fast convergence of the \emph{last iterate} to a ball around the SA operator&rsquo;s fixed point. Notably, our bound is \emph{tight} in its dependence on both the maximum delay $\tau_{max}$, and the mixing time $\tau_{mix}$. To achieve this tight bound, we develop a novel inductive proof technique that, unlike various existing delayed-optimization analyses, relies on establishing uniform boundedness of the iterates. As such, our proof may be of independent interest. Next, to mitigate the impact of the maximum delay on the convergence rate, we provide the first finite-time analysis of a delay-adaptive SA scheme under Markovian sampling. In particular, we show that the exponent of convergence of this scheme gets scaled down by $\tau_{avg}$, as opposed to $\tau_{max}$ for the vanilla delayed SA rule; here, $\tau_{avg}$ denotes the average delay across all iterations. Moreover, the adaptive scheme requires no prior knowledge of the delay sequence for step-size tuning. Our theoretical findings shed light on the finite-time effects of delays for a broad class of algorithms, including TD learning, Q-learning, and <b>stochastic</b> <b>gradient</b> <b>descent</b> under Markovian sampling.</p></p class="citation"></blockquote><h3 id=4971--158346-generative-kaleidoscopic-networks-harsh-shrivastava-2024>(49/71 | 158/346) Generative Kaleidoscopic Networks (Harsh Shrivastava, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Harsh Shrivastava. (2024)<br><strong>Generative Kaleidoscopic Networks</strong><br><button class=copy-to-clipboard title="Generative Kaleidoscopic Networks" index=158>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-158 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Convolutional Neural Network, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11793v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11793v1.pdf filename=2402.11793v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We discovered that the Deep ReLU networks (or Multilayer Perceptron architecture) demonstrate an &lsquo;over-generalization&rsquo; phenomenon. That is, the output values for the inputs that were not seen during training are mapped close to the output range that were observed during the learning process. In other words, the MLP learns a many-to-one mapping and this effect is more prominent as we increase the number of layers or depth of the MLP. We utilize this property of Deep ReLU networks to design a dataset kaleidoscope, termed as &lsquo;Generative Kaleidoscopic Networks&rsquo;. Briefly, if we learn a MLP to map from input $x\in\mathbb{R}^D$ to itself $f_\mathcal{N}(x)\rightarrow x$, the &lsquo;Kaleidoscopic sampling&rsquo; procedure starts with a random input noise $z\in\mathbb{R}^D$ and recursively applies $f_\mathcal{N}(\cdots f_\mathcal{N}(z)\cdots )$. After a burn-in period duration, we start observing samples from the input distribution and we found that deeper the MLP, higher is the quality of samples recovered. Scope: We observed this phenomenon to various degrees for the other deep learning architectures like <b>CNNs,</b> <b>Transformers</b> & U-Nets and we are currently investigating them further.</p></p class="citation"></blockquote><h3 id=5071--159346-diagonalisation-sgd-fast--convergent-sgd-for-non-differentiable-models-via-reparameterisation-and-smoothing-dominik-wagner-et-al-2024>(50/71 | 159/346) Diagonalisation SGD: Fast & Convergent SGD for Non-Differentiable Models via Reparameterisation and Smoothing (Dominik Wagner et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dominik Wagner, Basim Khajwal, C. -H. Luke Ong. (2024)<br><strong>Diagonalisation SGD: Fast & Convergent SGD for Non-Differentiable Models via Reparameterisation and Smoothing</strong><br><button class=copy-to-clipboard title="Diagonalisation SGD: Fast & Convergent SGD for Non-Differentiable Models via Reparameterisation and Smoothing" index=159>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-159 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG, math-OC<br>Keyword Score: 20<br>Keywords: Stochastic Gradient Descent, Stochastic Gradient Descent<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11752v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11752v2.pdf filename=2402.11752v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>It is well-known that the reparameterisation gradient estimator, which exhibits low variance in practice, is biased for non-differentiable models. This may compromise correctness of gradient-based optimisation methods such as <b>stochastic</b> <b>gradient</b> <b>descent</b> <b>(SGD).</b> We introduce a simple syntactic framework to define non-differentiable functions piecewisely and present a systematic approach to obtain smoothings for which the reparameterisation gradient estimator is unbiased. Our main contribution is a novel variant of <b>SGD,</b> Diagonalisation <b>Stochastic</b> <b>Gradient</b> <b>Descent,</b> which progressively enhances the accuracy of the smoothed approximation during optimisation, and we prove convergence to stationary points of the unsmoothed (original) objective. Our empirical evaluation reveals benefits over the state of the art: our approach is simple, fast, stable and attains orders of magnitude reduction in work-normalised variance.</p></p class="citation"></blockquote><h3 id=5171--160346-gaussian-process-neural-additive-models-wei-zhang-et-al-2024>(51/71 | 160/346) Gaussian Process Neural Additive Models (Wei Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wei Zhang, Brian Barr, John Paisley. (2024)<br><strong>Gaussian Process Neural Additive Models</strong><br><button class=copy-to-clipboard title="Gaussian Process Neural Additive Models" index=160>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-160 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 15<br>Keywords: Black Box, Gaussian Process<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12518v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12518v1.pdf filename=2402.12518v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep neural networks have revolutionized many fields, but their <b>black-box</b> <b>nature</b> also occasionally prevents their wider adoption in fields such as healthcare and finance, where interpretable and explainable models are required. The recent development of Neural Additive Models (NAMs) is a significant step in the direction of interpretable deep learning for tabular datasets. In this paper, we propose a new subclass of NAMs that use a single-layer neural network construction of the <b>Gaussian</b> <b>process</b> via random Fourier features, which we call <b>Gaussian</b> <b>Process</b> Neural Additive Models (GP-NAM). GP-NAMs have the advantage of a convex objective function and number of trainable parameters that grows linearly with feature dimensionality. It suffers no loss in performance compared to deeper NAM approaches because GPs are well-suited for learning complex non-parametric univariate functions. We demonstrate the performance of GP-NAM on several tabular datasets, showing that it achieves comparable or better performance in both classification and regression tasks with a large reduction in the number of parameters.</p></p class="citation"></blockquote><h3 id=5271--161346-energy-efficient-edge-learning-via-joint-data-deepening-and-prefetching-sujin-kook-et-al-2024>(52/71 | 161/346) Energy-Efficient Edge Learning via Joint Data Deepening-and-Prefetching (Sujin Kook et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sujin Kook, Won-Yong Shin, Seong-Lyun Kim, Seung-Woo Ko. (2024)<br><strong>Energy-Efficient Edge Learning via Joint Data Deepening-and-Prefetching</strong><br><button class=copy-to-clipboard title="Energy-Efficient Edge Learning via Joint Data Deepening-and-Prefetching" index=161>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-161 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-IT, cs-LG, cs.LG, math-IT<br>Keyword Score: 13<br>Keywords: MNIST, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11925v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11925v1.pdf filename=2402.11925v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The vision of pervasive artificial intelligence (AI) services can be realized by training an AI model on time using real-time data collected by internet of things (IoT) devices. To this end, IoT devices require offloading their data to an edge server in proximity. However, transmitting high-dimensional and voluminous data from energy-constrained IoT devices poses a significant challenge. To address this limitation, we propose a novel offloading architecture, called joint data deepening-and-prefetching (JD2P), which is feature-by-feature offloading comprising two key techniques. The first one is data deepening, where each data sample&rsquo;s features are sequentially offloaded in the order of importance determined by the data embedding technique such as principle component analysis (PCA). Offloading is terminated once the already transmitted features are sufficient for accurate data classification, resulting in a reduction in the amount of transmitted data. The criteria to offload data are derived for binary and multi-class classifiers, which are designed based on support vector machine (SVM) and deep neural network (DNN), respectively. The second one is data prefetching, where some features potentially required in the future are offloaded in advance, thus achieving high efficiency via precise prediction and parameter optimization. We evaluate the effectiveness of JD2P through experiments using the <b>MNIST</b> dataset, and the results demonstrate its significant reduction in expected energy consumption compared to several <b>benchmarks</b> without degrading learning accuracy.</p></p class="citation"></blockquote><h3 id=5371--162346-fairproof--confidential-and-certifiable-fairness-for-neural-networks-chhavi-yadav-et-al-2024>(53/71 | 162/346) FairProof : Confidential and Certifiable Fairness for Neural Networks (Chhavi Yadav et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chhavi Yadav, Amrita Roy Chowdhury, Dan Boneh, Kamalika Chaudhuri. (2024)<br><strong>FairProof : Confidential and Certifiable Fairness for Neural Networks</strong><br><button class=copy-to-clipboard title="FairProof : Confidential and Certifiable Fairness for Neural Networks" index=162>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-162 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CR, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Fairness<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12572v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12572v1.pdf filename=2402.12572v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Machine learning models are increasingly used in societal applications, yet legal and privacy concerns demand that they very often be kept confidential. Consequently, there is a growing distrust about the <b>fairness</b> properties of these models in the minds of consumers, who are often at the receiving end of model predictions. To this end, we propose FairProof - a system that uses Zero-Knowledge Proofs (a cryptographic primitive) to publicly verify the <b>fairness</b> of a model, while maintaining confidentiality. We also propose a <b>fairness</b> certification algorithm for fully-connected neural networks which is befitting to ZKPs and is used in this system. We implement FairProof in Gnark and demonstrate empirically that our system is practically feasible.</p></p class="citation"></blockquote><h3 id=5471--163346-dynamic-pricing-and-learning-with-long-term-reference-effects-shipra-agrawal-et-al-2024>(54/71 | 163/346) Dynamic Pricing and Learning with Long-term Reference Effects (Shipra Agrawal et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shipra Agrawal, Wei Tang. (2024)<br><strong>Dynamic Pricing and Learning with Long-term Reference Effects</strong><br><button class=copy-to-clipboard title="Dynamic Pricing and Learning with Long-term Reference Effects" index=163>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-163 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-GT, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Markov Decision Process<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12562v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12562v1.pdf filename=2402.12562v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We consider a dynamic pricing problem where customer response to the current price is impacted by the customer price expectation, aka reference price. We study a simple and novel reference price mechanism where reference price is the average of the past prices offered by the seller. As opposed to the more commonly studied exponential smoothing mechanism, in our reference price mechanism the prices offered by seller have a longer term effect on the future customer expectations. We show that under this mechanism, a markdown policy is near-optimal irrespective of the parameters of the model. This matches the common intuition that a seller may be better off by starting with a higher price and then decreasing it, as the customers feel like they are getting bargains on items that are ordinarily more expensive. For linear demand models, we also provide a detailed characterization of the near-optimal markdown policy along with an efficient way of computing it. We then consider a more challenging dynamic pricing and learning problem, where the demand model parameters are apriori unknown, and the seller needs to learn them online from the customers&rsquo; responses to the offered prices while simultaneously optimizing revenue. The objective is to minimize regret, i.e., the $T$-round revenue loss compared to a clairvoyant optimal policy. This task essentially amounts to learning a non-stationary optimal policy in a time-variant <b>Markov</b> <b>Decision</b> <b>Process</b> (MDP). For linear demand models, we provide an efficient learning algorithm with an optimal $\tilde{O}(\sqrt{T})$ regret upper bound.</p></p class="citation"></blockquote><h3 id=5571--164346-sdes-for-minimax-optimization-enea-monzio-compagnoni-et-al-2024>(55/71 | 164/346) SDEs for Minimax Optimization (Enea Monzio Compagnoni et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Enea Monzio Compagnoni, Antonio Orvieto, Hans Kersting, Frank Norbert Proske, Aurelien Lucchi. (2024)<br><strong>SDEs for Minimax Optimization</strong><br><button class=copy-to-clipboard title="SDEs for Minimax Optimization" index=164>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-164 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, math-OC<br>Keyword Score: 10<br>Keywords: Stochastic Gradient Descent<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12508v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12508v1.pdf filename=2402.12508v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Minimax optimization problems have attracted a lot of attention over the past few years, with applications ranging from economics to machine learning. While advanced optimization methods exist for such problems, characterizing their dynamics in <b>stochastic</b> <b>scenarios</b> <b>remains</b> notably challenging. In this paper, we pioneer the use of <b>stochastic</b> <b>differential</b> <b>equations</b> (SDEs) to analyze and compare Minimax optimizers. Our SDE models for <b>Stochastic</b> <b>Gradient</b> <b>Descent-Ascent,</b> <b>Stochastic</b> <b>Extragradient,</b> <b>and</b> <b>Stochastic</b> <b>Hamiltonian</b> <b>Gradient</b> Descent are provable approximations of their algorithmic counterparts, clearly showcasing the interplay between hyperparameters, implicit regularization, and implicit curvature-induced noise. This perspective also allows for a unified and simplified analysis strategy based on the principles of It^o calculus. Finally, our approach facilitates the derivation of convergence conditions and closed-form solutions for the dynamics in simplified settings, unveiling further insights into the behavior of different optimizers.</p></p class="citation"></blockquote><h3 id=5671--165346-lora-efficient-low-rank-adaptation-of-large-models-soufiane-hayou-et-al-2024>(56/71 | 165/346) LoRA+: Efficient Low Rank Adaptation of Large Models (Soufiane Hayou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Soufiane Hayou, Nikhil Ghosh, Bin Yu. (2024)<br><strong>LoRA+: Efficient Low Rank Adaptation of Large Models</strong><br><button class=copy-to-clipboard title="LoRA+: Efficient Low Rank Adaptation of Large Models" index=165>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-165 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CL, cs-LG, cs.LG, stat-ML<br>Keyword Score: 10<br>Keywords: Fine-tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12354v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12354v1.pdf filename=2402.12354v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we show that Low Rank Adaptation (LoRA) as originally introduced in Hu et al. (2021) leads to suboptimal <b>finetuning</b> of models with large width (embedding dimension). This is due to the fact that adapter matrices A and B in LoRA are updated with the same learning rate. Using scaling arguments for large width networks, we demonstrate that using the same learning rate for A and B does not allow efficient feature learning. We then show that this suboptimality of LoRA can be corrected simply by setting different learning rates for the LoRA adapter matrices A and B with a well-chosen ratio. We call this proposed algorithm LoRA$+$. In our extensive experiments, LoRA$+$ improves performance (1-2 $%$ improvements) and <b>finetuning</b> speed (up to $\sim$ 2X SpeedUp), at the same computational cost as LoRA.</p></p class="citation"></blockquote><h3 id=5771--166346-bears-make-neuro-symbolic-models-aware-of-their-reasoning-shortcuts-emanuele-marconato-et-al-2024>(57/71 | 166/346) BEARS Make Neuro-Symbolic Models Aware of their Reasoning Shortcuts (Emanuele Marconato et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Emanuele Marconato, Samuele Bortolotti, Emile van Krieken, Antonio Vergari, Andrea Passerini, Stefano Teso. (2024)<br><strong>BEARS Make Neuro-Symbolic Models Aware of their Reasoning Shortcuts</strong><br><button class=copy-to-clipboard title="BEARS Make Neuro-Symbolic Models Aware of their Reasoning Shortcuts" index=166>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-166 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12240v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12240v1.pdf filename=2402.12240v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Neuro-Symbolic (NeSy) predictors that conform to symbolic knowledge - encoding, e.g., safety constraints - can be affected by <b>Reasoning</b> Shortcuts (RSs): They learn concepts consistent with the symbolic knowledge by exploiting unintended semantics. RSs compromise reliability and generalization and, as we show in this paper, they are linked to NeSy models being overconfident about the predicted concepts. Unfortunately, the only trustworthy mitigation strategy requires collecting costly dense supervision over the concepts. Rather than attempting to avoid RSs altogether, we propose to ensure NeSy models are aware of the semantic ambiguity of the concepts they learn, thus enabling their users to identify and distrust low-quality concepts. Starting from three simple desiderata, we derive bears (BE Aware of <b>Reasoning</b> Shortcuts), an ensembling technique that calibrates the model&rsquo;s concept-level confidence without compromising prediction accuracy, thus encouraging NeSy architectures to be uncertain about concepts affected by RSs. We show empirically that bears improves RS-awareness of several state-of-the-art NeSy models, and also facilitates acquiring informative dense annotations for mitigation purposes.</p></p class="citation"></blockquote><h3 id=5871--167346-learning-discretized-bayesian-networks-with-gomea-damy-m-f-ha-et-al-2024>(58/71 | 167/346) Learning Discretized Bayesian Networks with GOMEA (Damy M. F. Ha et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Damy M. F. Ha, Tanja Alderliesten, Peter A. N. Bosman. (2024)<br><strong>Learning Discretized Bayesian Networks with GOMEA</strong><br><button class=copy-to-clipboard title="Learning Discretized Bayesian Networks with GOMEA" index=167>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-167 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs-NE, cs.LG<br>Keyword Score: 10<br>Keywords: Explainable AI<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12175v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12175v1.pdf filename=2402.12175v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Bayesian networks model relationships between random variables under uncertainty and can be used to predict the likelihood of events and outcomes while incorporating observed evidence. From an <b>eXplainable</b> <b>AI</b> (XAI) perspective, such models are interesting as they tend to be compact. Moreover, captured relations can be directly inspected by domain experts. In practice, data is often real-valued. Unless assumptions of normality can be made, discretization is often required. The optimal discretization, however, depends on the relations modelled between the variables. This complicates learning Bayesian networks from data. For this reason, most literature focuses on learning conditional dependencies between sets of variables, called structure learning. In this work, we extend an existing state-of-the-art structure learning approach based on the Gene-pool Optimal Mixing Evolutionary Algorithm (GOMEA) to jointly learn variable discretizations. The proposed Discretized Bayesian Network GOMEA (DBN-GOMEA) obtains similar or better results than the current state-of-the-art when tasked to retrieve randomly generated ground-truth networks. Moreover, leveraging a key strength of evolutionary algorithms, we can straightforwardly perform DBN learning multi-objectively. We show how this enables incorporating expert knowledge in a uniquely insightful fashion, finding multiple DBNs that trade-off complexity, accuracy, and the difference with a pre-determined expert network.</p></p class="citation"></blockquote><h3 id=5971--168346-federated-bayesian-network-ensembles-florian-van-daalen-et-al-2024>(59/71 | 168/346) Federated Bayesian Network Ensembles (Florian van Daalen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Florian van Daalen, Lianne Ippel, Andre Dekker, Inigo Bermejo. (2024)<br><strong>Federated Bayesian Network Ensembles</strong><br><button class=copy-to-clipboard title="Federated Bayesian Network Ensembles" index=168>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-168 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CR, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Federated Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12142v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12142v1.pdf filename=2402.12142v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Federated</b> <b>learning</b> allows us to run machine learning algorithms on decentralized data when data sharing is not permitted due to privacy concerns. Ensemble-based learning works by training multiple (weak) classifiers whose output is aggregated. <b>Federated</b> <b>ensembles</b> are ensembles applied to a <b>federated</b> <b>setting,</b> where each classifier in the ensemble is trained on one data location. In this article, we explore the use of <b>federated</b> <b>ensembles</b> of Bayesian networks (FBNE) in a range of experiments and compare their performance with locally trained models and models trained with VertiBayes, a <b>federated</b> <b>learning</b> algorithm to train Bayesian networks from decentralized data. Our results show that FBNE outperforms local models and provides a significant increase in training speed compared with VertiBayes while maintaining a similar performance in most settings, among other advantages. We show that FBNE is a potentially useful tool within the <b>federated</b> <b>learning</b> toolbox, especially when local populations are heavily biased, or there is a strong imbalance in population size across parties. We discuss the advantages and disadvantages of this approach in terms of time complexity, model accuracy, privacy protection, and model interpretability.</p></p class="citation"></blockquote><h3 id=6071--169346-interpretable-brain-inspired-representations-improve-rl-performance-on-visual-navigation-tasks-moritz-lange-et-al-2024>(60/71 | 169/346) Interpretable Brain-Inspired Representations Improve RL Performance on Visual Navigation Tasks (Moritz Lange et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Moritz Lange, Raphael C. Engelhardt, Wolfgang Konen, Laurenz Wiskott. (2024)<br><strong>Interpretable Brain-Inspired Representations Improve RL Performance on Visual Navigation Tasks</strong><br><button class=copy-to-clipboard title="Interpretable Brain-Inspired Representations Improve RL Performance on Visual Navigation Tasks" index=169>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-169 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs-NE, cs-RO, cs.LG<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12067v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12067v1.pdf filename=2402.12067v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Visual navigation requires a whole range of capabilities. A crucial one of these is the ability of an agent to determine its own location and heading in an environment. Prior works commonly assume this information as given, or use methods which lack a suitable inductive bias and accumulate error over time. In this work, we show how the method of slow feature analysis (SFA), inspired by neuroscience research, overcomes both limitations by generating interpretable representations of visual data that encode location and heading of an agent. We employ SFA in a modern <b>reinforcement</b> <b>learning</b> context, analyse and compare representations and illustrate where hierarchical SFA can outperform other feature extractors on navigation tasks.</p></p class="citation"></blockquote><h3 id=6171--170346-privacy-preserving-low-rank-adaptation-for-latent-diffusion-models-zihao-luo-et-al-2024>(61/71 | 170/346) Privacy-Preserving Low-Rank Adaptation for Latent Diffusion Models (Zihao Luo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zihao Luo, Xilie Xu, Feng Liu, Yun Sing Koh, Di Wang, Jingfeng Zhang. (2024)<br><strong>Privacy-Preserving Low-Rank Adaptation for Latent Diffusion Models</strong><br><button class=copy-to-clipboard title="Privacy-Preserving Low-Rank Adaptation for Latent Diffusion Models" index=170>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-170 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CR, cs-CV, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Diffusion Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11989v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11989v1.pdf filename=2402.11989v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Low-rank adaptation (LoRA) is an efficient strategy for adapting latent <b>diffusion</b> <b>models</b> (LDMs) on a training dataset to generate specific objects by minimizing the adaptation loss. However, adapted LDMs via LoRA are vulnerable to membership inference (MI) attacks that can judge whether a particular data point belongs to private training datasets, thus facing severe risks of privacy leakage. To defend against MI attacks, we make the first effort to propose a straightforward solution: privacy-preserving LoRA (PrivateLoRA). PrivateLoRA is formulated as a min-max optimization problem where a proxy attack model is trained by maximizing its MI gain while the LDM is adapted by minimizing the sum of the adaptation loss and the proxy attack model&rsquo;s MI gain. However, we empirically disclose that PrivateLoRA has the issue of unstable optimization due to the large fluctuation of the gradient scale which impedes adaptation. To mitigate this issue, we propose Stable PrivateLoRA that adapts the LDM by minimizing the ratio of the adaptation loss to the MI gain, which implicitly rescales the gradient and thus stabilizes the optimization. Our comprehensive empirical results corroborate that adapted LDMs via Stable PrivateLoRA can effectively defend against MI attacks while generating high-quality images. Our code is available at <a href=https://github.com/WilliamLUO0/StablePrivateLoRA>https://github.com/WilliamLUO0/StablePrivateLoRA</a>.</p></p class="citation"></blockquote><h3 id=6271--171346-mini-hes-a-parallelizable-second-order-latent-factor-analysis-model-jialiang-wang-et-al-2024>(62/71 | 171/346) Mini-Hes: A Parallelizable Second-order Latent Factor Analysis Model (Jialiang Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jialiang Wang, Weiling Li, Yurong Zhong, Xin Luo. (2024)<br><strong>Mini-Hes: A Parallelizable Second-order Latent Factor Analysis Model</strong><br><button class=copy-to-clipboard title="Mini-Hes: A Parallelizable Second-order Latent Factor Analysis Model" index=171>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-171 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG, stat-ML<br>Keyword Score: 10<br>Keywords: Recommender System<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11948v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11948v1.pdf filename=2402.11948v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Interactions among large number of entities is naturally high-dimensional and incomplete (HDI) in many big data related tasks. Behavioral characteristics of users are hidden in these interactions, hence, effective representation of the HDI data is a fundamental task for understanding user behaviors. Latent factor analysis (LFA) model has proven to be effective in representing HDI data. The performance of an LFA model relies heavily on its training process, which is a non-convex optimization. It has been proven that incorporating local curvature and preprocessing gradients during its training process can lead to superior performance compared to LFA models built with first-order family methods. However, with the escalation of data volume, the feasibility of second-order algorithms encounters challenges. To address this pivotal issue, this paper proposes a mini-block diagonal hessian-free (Mini-Hes) optimization for building an LFA model. It leverages the dominant diagonal blocks in the generalized Gauss-Newton matrix based on the analysis of the Hessian matrix of LFA model and serves as an intermediary strategy bridging the gap between first-order and second-order optimization methods. Experiment results indicate that, with Mini-Hes, the LFA model outperforms several state-of-the-art models in addressing missing data estimation task on multiple real HDI datasets from <b>recommender</b> <b>system.</b> (The source code of Mini-Hes is available at <a href=https://github.com/Goallow/Mini-Hes>https://github.com/Goallow/Mini-Hes</a>)</p></p class="citation"></blockquote><h3 id=6371--172346-predicting-trucking-accidents-with-truck-drivers-safety-climate-perception-across-companies-a-transfer-learning-approach-kailai-sun-et-al-2024>(63/71 | 172/346) Predicting trucking accidents with truck drivers &lsquo;safety climate perception across companies: A transfer learning approach (Kailai Sun et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kailai Sun, Tianxiang Lan, Say Hong Kam, Yang Miang Goh, Yueng-Hsiang Huang. (2024)<br><strong>Predicting trucking accidents with truck drivers &lsquo;safety climate perception across companies: A transfer learning approach</strong><br><button class=copy-to-clipboard title="Predicting trucking accidents with truck drivers 'safety climate perception across companies: A transfer learning approach" index=172>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-172 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Transfer Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12417v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12417v1.pdf filename=2402.12417v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>There is a rising interest in using artificial intelligence (AI)-powered safety analytics to predict accidents in the trucking industry. Companies may face the practical challenge, however, of not having enough data to develop good safety analytics models. Although pretrained models may offer a solution for such companies, existing safety research using <b>transfer</b> <b>learning</b> has mostly focused on computer vision and natural language processing, rather than accident analytics. To fill the above gap, we propose a pretrain-then-fine-tune <b>transfer</b> <b>learning</b> approach to help any company leverage other companies&rsquo; data to develop AI models for a more accurate prediction of accident risk. We also develop SafeNet, a deep neural network algorithm for classification tasks suitable for accident prediction. Using the safety climate survey data from seven trucking companies with different data sizes, we show that our proposed approach results in better model performance compared to training the model from scratch using only the target company&rsquo;s data. We also show that for the <b>transfer</b> <b>learning</b> model to be effective, the pretrained model should be developed with larger datasets from diverse sources. The trucking industry may, thus, consider pooling safety analytics data from a wide range of companies to develop pretrained models and share them within the industry for better knowledge and resource <b>transfer.</b> <b>The</b> above contributions point to the promise of advanced safety analytics to make the industry safer and more sustainable.</p></p class="citation"></blockquote><h3 id=6471--173346-vehicle-group-based-crash-risk-formation-and-propagation-analysis-for-expressways-tianheng-zhu-et-al-2024>(64/71 | 173/346) Vehicle-group-based Crash Risk Formation and Propagation Analysis for Expressways (Tianheng Zhu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tianheng Zhu, Ling Wang, Yiheng Feng, Wanjing Ma, Mohamed Abdel-Aty. (2024)<br><strong>Vehicle-group-based Crash Risk Formation and Propagation Analysis for Expressways</strong><br><button class=copy-to-clipboard title="Vehicle-group-based Crash Risk Formation and Propagation Analysis for Expressways" index=173>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-173 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CY, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Logistic Regression<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12415v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12415v1.pdf filename=2402.12415v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Previous studies in predicting crash risk primarily associated the number or likelihood of crashes on a road segment with traffic parameters or geometric characteristics of the segment, usually neglecting the impact of vehicles&rsquo; continuous movement and interactions with nearby vehicles. Advancements in communication technologies have empowered driving information collected from surrounding vehicles, enabling the study of group-based crash risks. Based on high-resolution vehicle trajectory data, this research focused on vehicle groups as the subject of analysis and explored risk formation and propagation mechanisms considering features of vehicle groups and road segments. Several key factors contributing to crash risks were identified, including past high-risk vehicle-group states, complex vehicle behaviors, high percentage of large vehicles, frequent lane changes within a vehicle group, and specific road geometries. A multinomial <b>logistic</b> <b>regression</b> model was developed to analyze the spatial risk propagation patterns, which were classified based on the trend of high-risk occurrences within vehicle groups. The results indicated that extended periods of high-risk states, increase in vehicle-group size, and frequent lane changes are associated with adverse risk propagation patterns. Conversely, smoother traffic flow and high initial crash risk values are linked to risk dissipation. Furthermore, the study conducted sensitivity analysis on different types of classifiers, prediction time intervalsss and adaptive TTC thresholds. The highest AUC value for vehicle-group risk prediction surpassed 0.93. The findings provide valuable insights to researchers and practitioners in understanding and prediction of vehicle-group safety, ultimately improving active traffic safety management and operations of Connected and Autonomous Vehicles.</p></p class="citation"></blockquote><h3 id=6571--174346-finite-time-error-analysis-of-online-model-based-q-learning-with-a-relaxed-sampling-model-han-dong-lim-et-al-2024>(65/71 | 174/346) Finite-Time Error Analysis of Online Model-Based Q-Learning with a Relaxed Sampling Model (Han-Dong Lim et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Han-Dong Lim, HyeAnn Lee, Donghwan Lee. (2024)<br><strong>Finite-Time Error Analysis of Online Model-Based Q-Learning with a Relaxed Sampling Model</strong><br><button class=copy-to-clipboard title="Finite-Time Error Analysis of Online Model-Based Q-Learning with a Relaxed Sampling Model" index=174>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-174 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11877v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11877v1.pdf filename=2402.11877v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Reinforcement</b> <b>learning</b> has witnessed significant advancements, particularly with the emergence of model-based approaches. Among these, $Q$-learning has proven to be a powerful algorithm in model-free settings. However, the extension of $Q$-learning to a model-based framework remains relatively unexplored. In this paper, we delve into the sample complexity of $Q$-learning when integrated with a model-based approach. Through theoretical analyses and empirical evaluations, we seek to elucidate the conditions under which model-based $Q$-learning excels in terms of sample efficiency compared to its model-free counterpart.</p></p class="citation"></blockquote><h3 id=6671--175346-communication-efficient-distributed-learning-with-local-immediate-error-compensation-yifei-cheng-et-al-2024>(66/71 | 175/346) Communication-Efficient Distributed Learning with Local Immediate Error Compensation (Yifei Cheng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yifei Cheng, Li Shen, Linli Xu, Xun Qian, Shiwei Wu, Yiming Zhou, Tie Zhang, Dacheng Tao, Enhong Chen. (2024)<br><strong>Communication-Efficient Distributed Learning with Local Immediate Error Compensation</strong><br><button class=copy-to-clipboard title="Communication-Efficient Distributed Learning with Local Immediate Error Compensation" index=175>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-175 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-DC, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Stochastic Gradient Descent<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11857v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11857v1.pdf filename=2402.11857v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Gradient compression with error compensation has attracted significant attention with the target of reducing the heavy communication overhead in distributed learning. However, existing compression methods either perform only unidirectional compression in one iteration with higher communication cost, or bidirectional compression with slower convergence rate. In this work, we propose the Local Immediate Error Compensated <b>SGD</b> (LIEC-SGD) optimization algorithm to break the above bottlenecks based on bidirectional compression and carefully designed compensation approaches. Specifically, the bidirectional compression technique is to reduce the communication cost, and the compensation technique compensates the local compression error to the model update immediately while only maintaining the global error variable on the server throughout the iterations to boost its efficacy. Theoretically, we prove that LIEC-SGD is superior to previous works in either the convergence rate or the communication cost, which indicates that LIEC-SGD could inherit the dual advantages from unidirectional compression and bidirectional compression. Finally, experiments of training deep neural networks validate the effectiveness of the proposed LIEC-SGD algorithm.</p></p class="citation"></blockquote><h3 id=6771--176346-towards-theoretical-understandings-of-self-consuming-generative-models-shi-fu-et-al-2024>(67/71 | 176/346) Towards Theoretical Understandings of Self-Consuming Generative Models (Shi Fu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shi Fu, Sen Zhang, Yingjie Wang, Xinmei Tian, Dacheng Tao. (2024)<br><strong>Towards Theoretical Understandings of Self-Consuming Generative Models</strong><br><button class=copy-to-clipboard title="Towards Theoretical Understandings of Self-Consuming Generative Models" index=176>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-176 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Diffusion Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11778v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11778v1.pdf filename=2402.11778v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper tackles the emerging challenge of training generative models within a self-consuming loop, wherein successive generations of models are recursively trained on mixtures of real and synthetic data from previous generations. We construct a theoretical framework to rigorously evaluate how this training regimen impacts the data distributions learned by future models. Specifically, we derive bounds on the total variation (TV) distance between the synthetic data distributions produced by future models and the original real data distribution under various mixed training scenarios. Our analysis demonstrates that this distance can be effectively controlled under the condition that mixed training dataset sizes or proportions of real data are large enough. Interestingly, we further unveil a phase transition induced by expanding synthetic data amounts, proving theoretically that while the TV distance exhibits an initial ascent, it declines beyond a threshold point. Finally, we specialize our general results to <b>diffusion</b> <b>models,</b> delivering nuanced insights such as the efficacy of optimal early stopping within the self-consuming loop.</p></p class="citation"></blockquote><h3 id=6871--177346-class-incremental-learning-for-time-series-benchmark-and-evaluation-zhongzheng-qiao-et-al-2024>(68/71 | 177/346) Class-incremental Learning for Time Series: Benchmark and Evaluation (Zhongzheng Qiao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhongzheng Qiao, Quang Pham, Zhen Cao, Hoang H Le, P. N. Suganthan, Xudong Jiang, Ramasamy Savitha. (2024)<br><strong>Class-incremental Learning for Time Series: Benchmark and Evaluation</strong><br><button class=copy-to-clipboard title="Class-incremental Learning for Time Series: Benchmark and Evaluation" index=177>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-177 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 6<br>Keywords: Benchmarking, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12035v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12035v1.pdf filename=2402.12035v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Real-world environments are inherently non-stationary, frequently introducing new classes over time. This is especially common in time series classification, such as the emergence of new disease classification in healthcare or the addition of new activities in human activity recognition. In such cases, a learning system is required to assimilate novel classes effectively while avoiding catastrophic forgetting of the old ones, which gives rise to the Class-incremental Learning (CIL) problem. However, despite the encouraging progress in the image and language domains, CIL for time series data remains relatively understudied. Existing studies suffer from inconsistent experimental designs, necessitating a comprehensive evaluation and <b>benchmarking</b> of methods across a wide range of datasets. To this end, we first present an overview of the Time Series Class-incremental Learning (TSCIL) problem, highlight its unique challenges, and cover the advanced methodologies. Further, based on standardized settings, we develop a unified experimental framework that supports the rapid development of new algorithms, easy integration of new datasets, and standardization of the evaluation process. Using this framework, we conduct a comprehensive evaluation of various generic and time-series-specific CIL methods in both standard and privacy-sensitive scenarios. Our extensive experiments not only provide a standard baseline to support future research but also shed light on the impact of various design factors such as normalization layers or memory budget thresholds. Codes are available at <a href=https://github.com/zqiao11/TSCIL>https://github.com/zqiao11/TSCIL</a>.</p></p class="citation"></blockquote><h3 id=6971--178346-network-inversion-of-binarised-neural-nets-pirzada-suhail-et-al-2024>(69/71 | 178/346) Network Inversion of Binarised Neural Nets (Pirzada Suhail et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Pirzada Suhail, Supratik Chakraborty, Amit Sethi. (2024)<br><strong>Network Inversion of Binarised Neural Nets</strong><br><button class=copy-to-clipboard title="Network Inversion of Binarised Neural Nets" index=178>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-178 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 5<br>Keywords: Black Box<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11995v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11995v1.pdf filename=2402.11995v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>While the deployment of neural networks, yielding impressive results, becomes more prevalent in various applications, their interpretability and understanding remain a critical challenge. Network inversion, a technique that aims to reconstruct the input space from the model&rsquo;s learned internal representations, plays a pivotal role in unraveling the <b>black-box</b> <b>nature</b> of input to output mappings in neural networks. In safety-critical scenarios, where model outputs may influence pivotal decisions, the integrity of the corresponding input space is paramount, necessitating the elimination of any extraneous &ldquo;garbage&rdquo; to ensure the trustworthiness of the network. Binarised Neural Networks (BNNs), characterized by binary weights and activations, offer computational efficiency and reduced memory requirements, making them suitable for resource-constrained environments. This paper introduces a novel approach to invert a trained BNN by encoding it into a CNF formula that captures the network&rsquo;s structure, allowing for both inference and inversion.</p></p class="citation"></blockquote><h3 id=7071--179346-graph-based-virtual-sensing-from-sparse-and-partial-multivariate-observations-giovanni-de-felice-et-al-2024>(70/71 | 179/346) Graph-based Virtual Sensing from Sparse and Partial Multivariate Observations (Giovanni De Felice et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Giovanni De Felice, Andrea Cini, Daniele Zambon, Vladimir V. Gusev, Cesare Alippi. (2024)<br><strong>Graph-based Virtual Sensing from Sparse and Partial Multivariate Observations</strong><br><button class=copy-to-clipboard title="Graph-based Virtual Sensing from Sparse and Partial Multivariate Observations" index=179>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-179 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12598v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12598v1.pdf filename=2402.12598v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Virtual sensing techniques allow for inferring signals at new unmonitored locations by exploiting spatio-temporal measurements coming from physical sensors at different locations. However, as the sensor coverage becomes sparse due to costs or other constraints, physical proximity cannot be used to support interpolation. In this paper, we overcome this challenge by leveraging dependencies between the target variable and a set of correlated variables (covariates) that can frequently be associated with each location of interest. From this viewpoint, covariates provide partial observability, and the problem consists of inferring values for unobserved channels by exploiting observations at other locations to learn how such variables can correlate. We introduce a novel <b>graph-based</b> methodology to exploit such relationships and design a <b>graph</b> deep learning architecture, named GgNet, implementing the framework. The proposed approach relies on propagating information over a nested <b>graph</b> structure that is used to learn dependencies between variables as well as locations. GgNet is extensively evaluated under different virtual sensing scenarios, demonstrating higher reconstruction accuracy compared to the state-of-the-art.</p></p class="citation"></blockquote><h3 id=7171--180346-dynamic-multi-network-mining-of-tensor-time-series-kohei-obata-et-al-2024>(71/71 | 180/346) Dynamic Multi-Network Mining of Tensor Time Series (Kohei Obata et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kohei Obata, Koki Kawabata, Yasuko Matsubara, Yasushi Sakurai. (2024)<br><strong>Dynamic Multi-Network Mining of Tensor Time Series</strong><br><button class=copy-to-clipboard title="Dynamic Multi-Network Mining of Tensor Time Series" index=180>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-180 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-IT, cs-LG, cs.LG, math-IT<br>Keyword Score: 3<br>Keywords: Clustering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11773v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11773v2.pdf filename=2402.11773v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Subsequence <b>clustering</b> of time series is an essential task in data mining, and interpreting the resulting clusters is also crucial since we generally do not have prior knowledge of the data. Thus, given a large collection of tensor time series consisting of multiple modes, including timestamps, how can we achieve subsequence <b>clustering</b> for tensor time series and provide interpretable insights? In this paper, we propose a new method, Dynamic Multi-network Mining (DMM), that converts a tensor time series into a set of segment groups of various lengths (i.e., clusters) characterized by a dependency network constrained with l1-norm. Our method has the following properties. (a) Interpretable: it characterizes the cluster with multiple networks, each of which is a sparse dependency network of a corresponding non-temporal mode, and thus provides visible and interpretable insights into the key relationships. (b) Accurate: it discovers the clusters with distinct networks from tensor time series according to the minimum description length (MDL). (c) Scalable: it scales linearly in terms of the input data size when solving a non-convex problem to optimize the number of segments and clusters, and thus it is applicable to long-range and high-dimensional tensors. Extensive experiments with synthetic datasets confirm that our method outperforms the state-of-the-art methods in terms of <b>clustering</b> accuracy. We then use real datasets to demonstrate that DMM is useful for providing interpretable insights from tensor time series.</p></p class="citation"></blockquote><h2 id=csai-9>cs.AI (9)</h2><h3 id=19--181346-llm-as-prompter-low-resource-inductive-reasoning-on-arbitrary-knowledge-graphs-kai-wang-et-al-2024>(1/9 | 181/346) LLM as Prompter: Low-resource Inductive Reasoning on Arbitrary Knowledge Graphs (Kai Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kai Wang, Yuwei Xu, Zhiyong Wu, Siqiang Luo. (2024)<br><strong>LLM as Prompter: Low-resource Inductive Reasoning on Arbitrary Knowledge Graphs</strong><br><button class=copy-to-clipboard title="LLM as Prompter: Low-resource Inductive Reasoning on Arbitrary Knowledge Graphs" index=181>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-181 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CL, cs-SI, cs.AI<br>Keyword Score: 93<br>Keywords: Graph, Graph Neural Network, Graph Neural Network, Knowledge Graph, Knowledge Graph, Low-Resource, Zero-shot, Reasoning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11804v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11804v1.pdf filename=2402.11804v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Knowledge</b> <b>Graph</b> <b>(KG)</b> <b>inductive</b> <b>reasoning,</b> which aims to infer missing facts from new <b>KGs</b> that are not seen during training, has been widely adopted in various applications. One critical challenge of <b>KG</b> inductive <b>reasoning</b> is handling <b>low-resource</b> scenarios with scarcity in both textual and structural aspects. In this paper, we attempt to address this challenge with <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs).</b> Particularly, we utilize the state-of-the-art <b>LLMs</b> to generate a <b>graph-structural</b> <b>prompt</b> <b>to</b> enhance the pre-trained <b>Graph</b> <b>Neural</b> <b>Networks</b> <b>(GNNs),</b> which brings us new methodological insights into the <b>KG</b> inductive <b>reasoning</b> methods, as well as high generalizability in practice. On the methodological side, we introduce a novel pretraining and <b>prompting</b> framework ProLINK, designed for <b>low-resource</b> inductive <b>reasoning</b> across arbitrary <b>KGs</b> without requiring additional training. On the practical side, we experimentally evaluate our approach on 36 <b>low-resource</b> <b>KG</b> datasets and find that ProLINK outperforms previous methods in three-shot, one-shot, and <b>zero-shot</b> <b>reasoning</b> tasks, exhibiting average performance improvements by 20%, 45%, and 147%, respectively. Furthermore, ProLINK demonstrates strong robustness for various <b>LLM</b> <b>promptings</b> as well as full-shot scenarios.</p></p class="citation"></blockquote><h3 id=29--182346-hip-network-historical-information-passing-network-for-extrapolation-reasoning-on-temporal-knowledge-graph-yongquan-he-et-al-2024>(2/9 | 182/346) HIP Network: Historical Information Passing Network for Extrapolation Reasoning on Temporal Knowledge Graph (Yongquan He et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yongquan He, Peng Zhang, Luchen Liu, Qi Liang, Wenyuan Zhang, Chuang Zhang. (2024)<br><strong>HIP Network: Historical Information Passing Network for Extrapolation Reasoning on Temporal Knowledge Graph</strong><br><button class=copy-to-clipboard title="HIP Network: Historical Information Passing Network for Extrapolation Reasoning on Temporal Knowledge Graph" index=182>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-182 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: I-2-4; I-2-6; I-2-7, cs-AI, cs.AI<br>Keyword Score: 41<br>Keywords: Graph, Benchmarking, Knowledge Graph, Reasoning, Temporal Knowledge Graph, Temporal Knowledge Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12074v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12074v1.pdf filename=2402.12074v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In recent years, <b>temporal</b> <b>knowledge</b> <b>graph</b> <b>(TKG)</b> <b>reasoning</b> has received significant attention. Most existing methods assume that all timestamps and corresponding <b>graphs</b> are available during training, which makes it difficult to predict future events. To address this issue, recent works learn to infer future events based on historical information. However, these methods do not comprehensively consider the latent patterns behind <b>temporal</b> <b>changes,</b> <b>to</b> pass historical information selectively, update representations appropriately and predict events accurately. In this paper, we propose the Historical Information Passing (HIP) network to predict future events. HIP network passes information from <b>temporal,</b> <b>structural</b> <b>and</b> repetitive perspectives, which are used to model the <b>temporal</b> <b>evolution</b> <b>of</b> events, the interactions of events at the same time step, and the known events respectively. In particular, our method considers the updating of relation representations and adopts three scoring functions corresponding to the above dimensions. Experimental results on five <b>benchmark</b> datasets show the superiority of HIP network, and the significant improvements on Hits@1 prove that our method can more accurately predict what is going to happen.</p></p class="citation"></blockquote><h3 id=39--183346-a-survey-on-extractive-knowledge-graph-summarization-applications-approaches-evaluation-and-future-directions-xiaxia-wang-et-al-2024>(3/9 | 183/346) A Survey on Extractive Knowledge Graph Summarization: Applications, Approaches, Evaluation, and Future Directions (Xiaxia Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiaxia Wang, Gong Cheng. (2024)<br><strong>A Survey on Extractive Knowledge Graph Summarization: Applications, Approaches, Evaluation, and Future Directions</strong><br><button class=copy-to-clipboard title="A Survey on Extractive Knowledge Graph Summarization: Applications, Approaches, Evaluation, and Future Directions" index=183>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-183 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-DB, cs-IR, cs-SI, cs.AI<br>Keyword Score: 33<br>Keywords: Graph, Knowledge Distillation, Knowledge Graph, Knowledge Graph, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12001v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12001v1.pdf filename=2402.12001v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the continuous growth of large <b>Knowledge</b> <b>Graphs</b> <b>(KGs),</b> extractive <b>KG</b> <b>summarization</b> becomes a trending task. Aiming at <b>distilling</b> a compact subgraph with condensed information, it facilitates various downstream <b>KG-based</b> tasks. In this survey paper, we are among the first to provide a systematic overview of its applications and define a taxonomy for existing methods from its interdisciplinary studies. Future directions are also laid out based on our extensive and comparative review.</p></p class="citation"></blockquote><h3 id=49--184346-sstkg-simple-spatio-temporal-knowledge-graph-for-intepretable-and-versatile-dynamic-information-embedding-ruiyi-yang-et-al-2024>(4/9 | 184/346) SSTKG: Simple Spatio-Temporal Knowledge Graph for Intepretable and Versatile Dynamic Information Embedding (Ruiyi Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ruiyi Yang, Flora D. Salim, Hao Xue. (2024)<br><strong>SSTKG: Simple Spatio-Temporal Knowledge Graph for Intepretable and Versatile Dynamic Information Embedding</strong><br><button class=copy-to-clipboard title="SSTKG: Simple Spatio-Temporal Knowledge Graph for Intepretable and Versatile Dynamic Information Embedding" index=184>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-184 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 23<br>Keywords: Graph, Knowledge Graph, Knowledge Graph, Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12132v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12132v1.pdf filename=2402.12132v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Knowledge</b> <b>graphs</b> <b>(KGs)</b> have been increasingly employed for link prediction and <b>recommendation</b> using real-world datasets. However, the majority of current methods rely on static data, neglecting the dynamic nature and the hidden spatio-temporal attributes of real-world scenarios. This often results in suboptimal predictions and <b>recommendations.</b> Although there are effective spatio-temporal inference methods, they face challenges such as scalability with large datasets and inadequate semantic understanding, which impede their performance. To address these limitations, this paper introduces a novel framework - Simple Spatio-Temporal <b>Knowledge</b> <b>Graph</b> (SSTKG), for constructing and exploring spatio-temporal <b>KGs.</b> To integrate spatial and temporal data into <b>KGs,</b> our framework exploited through a new 3-step embedding method. Output embeddings can be used for future temporal sequence prediction and spatial information <b>recommendation,</b> providing valuable insights for various applications such as retail sales forecasting and traffic volume prediction. Our framework offers a simple but comprehensive way to understand the underlying patterns and trends in dynamic <b>KG,</b> thereby enhancing the accuracy of predictions and the relevance of <b>recommendations.</b> This work paves the way for more effective utilization of spatio-temporal data in <b>KGs,</b> with potential impacts across a wide range of sectors.</p></p class="citation"></blockquote><h3 id=59--185346-shall-we-talk-exploring-spontaneous-collaborations-of-competing-llm-agents-zengqing-wu-et-al-2024>(5/9 | 185/346) Shall We Talk: Exploring Spontaneous Collaborations of Competing LLM Agents (Zengqing Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zengqing Wu, Shuyuan Zheng, Qianying Liu, Xu Han, Brian Inhyuk Kwon, Makoto Onizuka, Shaojie Tang, Run Peng, Chuan Xiao. (2024)<br><strong>Shall We Talk: Exploring Spontaneous Collaborations of Competing LLM Agents</strong><br><button class=copy-to-clipboard title="Shall We Talk: Exploring Spontaneous Collaborations of Competing LLM Agents" index=185>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-185 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CL, cs-CY, cs-MA, cs.AI, econ-GN, q-fin-EC<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12327v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12327v1.pdf filename=2402.12327v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advancements have shown that agents powered by <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> possess capabilities to simulate human behaviors and societal dynamics. However, the potential for <b>LLM</b> agents to spontaneously establish collaborative relationships in the absence of explicit instructions has not been studied. To address this gap, we conduct three case studies, revealing that <b>LLM</b> agents are capable of spontaneously forming collaborations even within competitive settings. This finding not only demonstrates the capacity of <b>LLM</b> agents to mimic competition and cooperation in human societies but also validates a promising vision of computational social science. Specifically, it suggests that <b>LLM</b> agents could be utilized to model human social interactions, including those with spontaneous collaborations, thus offering insights into social phenomena. The source codes for this study are available at <a href=https://github.com/wuzengqing001225/SABM_ShallWeTalk>https://github.com/wuzengqing001225/SABM_ShallWeTalk</a> .</p></p class="citation"></blockquote><h3 id=69--186346-grounding-from-an-ai-and-cognitive-science-lens-goonmeet-bajaj-et-al-2024>(6/9 | 186/346) Grounding from an AI and Cognitive Science Lens (Goonmeet Bajaj et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Goonmeet Bajaj, Srinivasan Parthasarathy, Valerie L. Shalin, Amit Sheth. (2024)<br><strong>Grounding from an AI and Cognitive Science Lens</strong><br><button class=copy-to-clipboard title="Grounding from an AI and Cognitive Science Lens" index=186>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-186 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 10<br>Keywords: Grounding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13290v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13290v1.pdf filename=2402.13290v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Grounding</b> is a challenging problem, requiring a formal definition and different levels of abstraction. This article explores <b>grounding</b> from both cognitive science and machine learning perspectives. It identifies the subtleties of <b>grounding,</b> its significance for collaborative agents, and similarities and differences in <b>grounding</b> approaches in both communities. The article examines the potential of neuro-symbolic approaches tailored for <b>grounding</b> tasks, showcasing how they can more comprehensively address <b>grounding.</b> Finally, we discuss areas for further exploration and development in <b>grounding.</b></p></p class="citation"></blockquote><h3 id=79--187346-worldcoder-a-model-based-llm-agent-building-world-models-by-writing-code-and-interacting-with-the-environment-hao-tang-et-al-2024>(7/9 | 187/346) WorldCoder, a Model-Based LLM Agent: Building World Models by Writing Code and Interacting with the Environment (Hao Tang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hao Tang, Darren Key, Kevin Ellis. (2024)<br><strong>WorldCoder, a Model-Based LLM Agent: Building World Models by Writing Code and Interacting with the Environment</strong><br><button class=copy-to-clipboard title="WorldCoder, a Model-Based LLM Agent: Building World Models by Writing Code and Interacting with the Environment" index=187>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-187 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CL, cs.AI<br>Keyword Score: 10<br>Keywords: Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12275v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12275v1.pdf filename=2402.12275v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We give a model-based agent that builds a Python program representing its knowledge of the world based on its interactions with the environment. The world model tries to explain its interactions, while also being optimistic about what reward it can achieve. We do this by extending work on program synthesis via <b>LLMs.</b> We study our agent on gridworlds, finding our approach is more sample-efficient compared to deep RL, and more compute-efficient compared to ReAct-style agents.</p></p class="citation"></blockquote><h3 id=89--188346-discerning-and-resolving-knowledge-conflicts-through-adaptive-decoding-with-contextual-information-entropy-constraint-xiaowei-yuan-et-al-2024>(8/9 | 188/346) Discerning and Resolving Knowledge Conflicts through Adaptive Decoding with Contextual Information-Entropy Constraint (Xiaowei Yuan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiaowei Yuan, Zhao Yang, Yequan Wang, Shengping Liu, Jun Zhao, Kang Liu. (2024)<br><strong>Discerning and Resolving Knowledge Conflicts through Adaptive Decoding with Contextual Information-Entropy Constraint</strong><br><button class=copy-to-clipboard title="Discerning and Resolving Knowledge Conflicts through Adaptive Decoding with Contextual Information-Entropy Constraint" index=188>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-188 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 10<br>Keywords: Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11893v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11893v1.pdf filename=2402.11893v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> internalize enormous parametric knowledge during pre-training. Concurrently, realistic applications necessitate external contextual knowledge to aid models on the underlying tasks. This raises a crucial dilemma known as knowledge conflicts, where the contextual knowledge clashes with the However, existing decoding works are specialized in resolving knowledge conflicts and could inadvertently deteriorate performance in absence of conflicts. In this paper, we propose an adaptive decoding method, termed as contextual information-entropy constraint decoding (COIECD), to discern whether the knowledge conflicts occur and resolve them. It can improve the model&rsquo;s faithfulness to conflicting context, and simultaneously maintain high performance among non- Our experiments show that COIECD exhibits strong performance and robustness over knowledge conflicts in realistic datasets. Code is available.</p></p class="citation"></blockquote><h3 id=99--189346-multifix-an-xai-friendly-feature-inducing-approach-to-building-models-from-multimodal-data-mafalda-malafaia-et-al-2024>(9/9 | 189/346) MultiFIX: An XAI-friendly feature inducing approach to building models from multimodal data (Mafalda Malafaia et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mafalda Malafaia, Thalea Schlender, Peter A. N. Bosman, Tanja Alderliesten. (2024)<br><strong>MultiFIX: An XAI-friendly feature inducing approach to building models from multimodal data</strong><br><button class=copy-to-clipboard title="MultiFIX: An XAI-friendly feature inducing approach to building models from multimodal data" index=189>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-189 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 6<br>Keywords: Multi-modal, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12183v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12183v1.pdf filename=2402.12183v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the health domain, decisions are often based on different data modalities. Thus, when creating prediction models, <b>multimodal</b> fusion approaches that can extract and combine relevant features from different data modalities, can be highly beneficial. Furthermore, it is important to understand how each modality impacts the final prediction, especially in high-stake domains, so that these models can be used in a trustworthy and responsible manner. We propose MultiFIX: a new interpretability-focused <b>multimodal</b> data fusion pipeline that explicitly induces separate features from different data types that can subsequently be combined to make a final prediction. An end-to-end deep learning architecture is used to train a predictive model and extract representative features of each modality. Each part of the model is then explained using explainable artificial intelligence techniques. Attention maps are used to highlight important regions in image inputs. Inherently interpretable symbolic expressions, learned with GP-GOMEA, are used to describe the contribution of tabular inputs. The fusion of the extracted features to predict the target label is also replaced by a symbolic expression, learned with GP-GOMEA. Results on synthetic problems demonstrate the strengths and limitations of MultiFIX. Lastly, we apply MultiFIX to a publicly available dataset for the detection of malignant skin lesions.</p></p class="citation"></blockquote><h2 id=cscr-9>cs.CR (9)</h2><h3 id=19--190346-deepcode-ai-fix-fixing-security-vulnerabilities-with-large-language-models-berkay-berabi-et-al-2024>(1/9 | 190/346) DeepCode AI Fix: Fixing Security Vulnerabilities with Large Language Models (Berkay Berabi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Berkay Berabi, Alexey Gronskiy, Veselin Raychev, Gishor Sivanrupan, Victor Chibotaru, Martin Vechev. (2024)<br><strong>DeepCode AI Fix: Fixing Security Vulnerabilities with Large Language Models</strong><br><button class=copy-to-clipboard title="DeepCode AI Fix: Fixing Security Vulnerabilities with Large Language Models" index=190>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-190 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs-LG, cs-PL, cs-SE, cs.CR<br>Keyword Score: 90<br>Keywords: Few-shot, Few-shot Learning, Fine-tuning, GPT, GPT-3, GPT-3.5, GPT-4, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13291v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13291v2.pdf filename=2402.13291v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The automated program repair field has attracted substantial interest over the years, but despite significant research efforts, creating a system that works well for complex semantic bugs such as security vulnerabilities has proven difficult. A promising direction to solve this challenge is by leveraging <b>large</b> <b>language</b> <b>models</b> <b>(LLMs),</b> which are increasingly used to solve various programming tasks. In this paper, we investigate the effectiveness of <b>LLMs</b> for solving code-repair task. We show that the task is difficult as it requires the model to learn long-range code relationships, a task that inherently relies on extensive amounts of training data. At the same time, creating a <b>large,</b> <b>clean</b> <b>dataset</b> for complex program bugs and their corresponding fixes is non-trivial. We propose a technique to address these challenges with a new approach for querying and <b>fine-tuning</b> <b>LLMs.</b> The idea is to use program analysis to limit the <b>LLM&rsquo;s</b> attention mechanism on the portions of code needed to perform the fix, drastically reducing the amount of required training data. Concretely, for training and inference, rather than feeding the entire program to the <b>LLM,</b> we reduce its code to a much shorter snippet that contains the reported defect together with the necessary context - and use that instead. Our evaluation shows that this code reduction approach substantially improves available models such as <b>GPT-4</b> using <b>few-shot</b> <b>learning,</b> as well as <b>fine-tuning</b> models. To train and evaluate our system, we created a comprehensive code fixing dataset by extensively labeling 156 bug patterns (including 40 security rules), requiring complex interprocedural dataflow to discover. Our best system with Mixtral-8x7B can remove more than 80% of the reported defects while exactly matching the human fix in between 10 and 50% of cases, outperforming baselines based on <b>GPT-3.5</b> and <b>GPT-4,</b> or based on window-based models like TFix.</p></p class="citation"></blockquote><h3 id=29--191346-stealing-the-invisible-unveiling-pre-trained-cnn-models-through-adversarial-examples-and-timing-side-channels-shubhi-shukla-et-al-2024>(2/9 | 191/346) Stealing the Invisible: Unveiling Pre-Trained CNN Models through Adversarial Examples and Timing Side-Channels (Shubhi Shukla et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shubhi Shukla, Manaar Alam, Pabitra Mitra, Debdeep Mukhopadhyay. (2024)<br><strong>Stealing the Invisible: Unveiling Pre-Trained CNN Models through Adversarial Examples and Timing Side-Channels</strong><br><button class=copy-to-clipboard title="Stealing the Invisible: Unveiling Pre-Trained CNN Models through Adversarial Examples and Timing Side-Channels" index=191>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-191 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs-LG, cs.CR<br>Keyword Score: 80<br>Keywords: Vision Transformer, Convolution, Convolutional Neural Network, Convolutional Neural Network, Fine-tuning, Transfer Learning, Transformer, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11953v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11953v1.pdf filename=2402.11953v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Machine learning, with its myriad applications, has become an integral component of numerous technological systems. A common practice in this domain is the use of <b>transfer</b> <b>learning,</b> where a pre-trained model&rsquo;s architecture, readily available to the public, is <b>fine-tuned</b> to suit specific tasks. As Machine Learning as a Service (MLaaS) platforms increasingly use pre-trained models in their backends, it&rsquo;s crucial to safeguard these architectures and understand their vulnerabilities. In this work, we present an approach based on the observation that the classification patterns of adversarial images can be used as a means to steal the models. Furthermore, the adversarial image classifications in conjunction with timing side channels can lead to a model stealing method. Our approach, designed for typical user-level access in remote MLaaS environments exploits varying misclassifications of adversarial images across different models to fingerprint several renowned <b>Convolutional</b> <b>Neural</b> <b>Network</b> <b>(CNN)</b> and <b>Vision</b> <b>Transformer</b> (ViT) architectures. We utilize the profiling of remote model inference times to reduce the necessary adversarial images, subsequently decreasing the number of queries required. We have presented our results over 27 pre-trained models of different <b>CNN</b> and ViT architectures using CIFAR-10 dataset and demonstrate a high accuracy of 88.8% while keeping the query budget under 20.</p></p class="citation"></blockquote><h3 id=39--192346-covrl-fuzzing-javascript-engines-with-coverage-guided-reinforcement-learning-for-llm-based-mutation-jueon-eom-et-al-2024>(3/9 | 192/346) CovRL: Fuzzing JavaScript Engines with Coverage-Guided Reinforcement Learning for LLM-based Mutation (Jueon Eom et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jueon Eom, Seyeon Jeong, Taekyoung Kwon. (2024)<br><strong>CovRL: Fuzzing JavaScript Engines with Coverage-Guided Reinforcement Learning for LLM-based Mutation</strong><br><button class=copy-to-clipboard title="CovRL: Fuzzing JavaScript Engines with Coverage-Guided Reinforcement Learning for LLM-based Mutation" index=192>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-192 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: D-4-6; I-2-5; D-2-4, cs-CL, cs-CR, cs-LG, cs-SE, cs.CR<br>Keyword Score: 45<br>Keywords: Black Box, Reinforcement Learning, Large Language Model, Large Language Model, TF-IDF<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12222v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12222v1.pdf filename=2402.12222v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Fuzzing is an effective bug-finding technique but it struggles with complex systems like JavaScript engines that demand precise grammatical input. Recently, researchers have adopted language models for context-aware mutation in fuzzing to address this problem. However, existing techniques are limited in utilizing coverage guidance for fuzzing, which is rather performed in a <b>black-box</b> <b>manner.</b> This paper presents a novel technique called CovRL (Coverage-guided <b>Reinforcement</b> <b>Learning)</b> that combines <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> with <b>reinforcement</b> <b>learning</b> from coverage feedback. Our fuzzer, CovRL-Fuzz, integrates coverage feedback directly into the <b>LLM</b> by leveraging the Term Frequency-Inverse Document Frequency <b>(TF-IDF)</b> method to construct a weighted coverage map. This map is key in calculating the fuzzing reward, which is then applied to the <b>LLM-based</b> mutator through <b>reinforcement</b> <b>learning.</b> CovRL-Fuzz, through this approach, enables the generation of test cases that are more likely to discover new coverage areas, thus improving vulnerability detection while minimizing syntax and semantic errors, all without needing extra post-processing. Our evaluation results indicate that CovRL-Fuzz outperforms the state-of-the-art fuzzers in terms of code coverage and bug-finding capabilities: CovRL-Fuzz identified 48 real-world security-related bugs in the latest JavaScript engines, including 39 previously unknown vulnerabilities and 11 CVEs.</p></p class="citation"></blockquote><h3 id=49--193346-evaluation-of-chatgpts-smart-contract-auditing-capabilities-based-on-chain-of-thought-yuying-du-et-al-2024>(4/9 | 193/346) Evaluation of ChatGPT&rsquo;s Smart Contract Auditing Capabilities Based on Chain of Thought (Yuying Du et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuying Du, Xueyan Tang. (2024)<br><strong>Evaluation of ChatGPT&rsquo;s Smart Contract Auditing Capabilities Based on Chain of Thought</strong><br><button class=copy-to-clipboard title="Evaluation of ChatGPT's Smart Contract Auditing Capabilities Based on Chain of Thought" index=193>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-193 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: 68, I-2; J-6, cs-AI, cs-CR, cs.CR<br>Keyword Score: 40<br>Keywords: ChatGPT, GPT, GPT-4, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12023v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12023v1.pdf filename=2402.12023v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Smart contracts, as a key component of blockchain technology, play a crucial role in ensuring the automation of transactions and adherence to protocol rules. However, smart contracts are susceptible to security vulnerabilities, which, if exploited, can lead to significant asset losses. This study explores the potential of enhancing smart contract security audits using the <b>GPT-4</b> model. We utilized a dataset of 35 smart contracts from the SolidiFI-benchmark vulnerability library, containing 732 vulnerabilities, and compared it with five other vulnerability detection tools to evaluate <b>GPT-4&rsquo;s</b> ability to identify seven common types of vulnerabilities. Moreover, we assessed <b>GPT-4&rsquo;s</b> performance in code parsing and vulnerability capture by simulating a professional auditor&rsquo;s auditing process using CoT(Chain of Thought) <b>prompts</b> based on the audit reports of eight groups of smart contracts. We also evaluated <b>GPT-4&rsquo;s</b> ability to write Solidity Proof of Concepts (PoCs). Through experimentation, we found that <b>GPT-4</b> performed poorly in detecting smart contract vulnerabilities, with a high Precision of 96.6%, but a low Recall of 37.8%, and an F1-score of 41.1%, indicating a tendency to miss vulnerabilities during detection. Meanwhile, it demonstrated good contract code parsing capabilities, with an average comprehensive score of 6.5, capable of identifying the background information and functional relationships of smart contracts; in 60% of the cases, it could write usable PoCs, suggesting <b>GPT-4</b> has significant potential application in PoC writing. These experimental results indicate that <b>GPT-4</b> lacks the ability to detect smart contract vulnerabilities effectively, but its performance in contract code parsing and PoC writing demonstrates its significant potential as an auxiliary tool in enhancing the efficiency and effectiveness of smart contract security audits.</p></p class="citation"></blockquote><h3 id=59--194346-an-empirical-evaluation-of-llms-for-solving-offensive-security-challenges-minghao-shao-et-al-2024>(5/9 | 194/346) An Empirical Evaluation of LLMs for Solving Offensive Security Challenges (Minghao Shao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Minghao Shao, Boyuan Chen, Sofija Jancheska, Brendan Dolan-Gavitt, Siddharth Garg, Ramesh Karri, Muhammad Shafique. (2024)<br><strong>An Empirical Evaluation of LLMs for Solving Offensive Security Challenges</strong><br><button class=copy-to-clipboard title="An Empirical Evaluation of LLMs for Solving Offensive Security Challenges" index=194>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-194 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs.CR<br>Keyword Score: 40<br>Keywords: human-in-the-loop, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11814v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11814v1.pdf filename=2402.11814v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Capture The Flag (CTF) challenges are puzzles related to computer security scenarios. With the advent of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs),</b> more and more CTF participants are using <b>LLMs</b> to understand and solve the challenges. However, so far no work has evaluated the effectiveness of <b>LLMs</b> in solving CTF challenges with a fully automated workflow. We develop two CTF-solving workflows, <b>human-in-the-loop</b> (HITL) and fully-automated, to examine the <b>LLMs&rsquo;</b> ability to solve a selected set of CTF challenges, <b>prompted</b> with information about the question. We collect human contestants&rsquo; results on the same set of questions, and find that <b>LLMs</b> achieve higher success rate than an average human participant. This work provides a comprehensive evaluation of the capability of <b>LLMs</b> in solving real world CTF challenges, from real competition to fully automated workflow. Our results provide references for applying <b>LLMs</b> in cybersecurity education and pave the way for systematic evaluation of offensive cybersecurity capabilities in <b>LLMs.</b></p></p class="citation"></blockquote><h3 id=69--195346-defending-against-weight-poisoning-backdoor-attacks-for-parameter-efficient-fine-tuning-shuai-zhao-et-al-2024>(6/9 | 195/346) Defending Against Weight-Poisoning Backdoor Attacks for Parameter-Efficient Fine-Tuning (Shuai Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shuai Zhao, Leilei Gan, Luu Anh Tuan, Jie Fu, Lingjuan Lyu, Meihuizi Jia, Jinming Wen. (2024)<br><strong>Defending Against Weight-Poisoning Backdoor Attacks for Parameter-Efficient Fine-Tuning</strong><br><button class=copy-to-clipboard title="Defending Against Weight-Poisoning Backdoor Attacks for Parameter-Efficient Fine-Tuning" index=195>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-195 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-AI, cs-CL, cs-CR, cs.CR<br>Keyword Score: 20<br>Keywords: Fine-tuning, Text Classification<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12168v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12168v1.pdf filename=2402.12168v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recently, various parameter-efficient <b>fine-tuning</b> (PEFT) strategies for application to language models have been proposed and successfully implemented. However, this raises the question of whether PEFT, which only updates a limited set of model parameters, constitutes security vulnerabilities when confronted with weight-poisoning backdoor attacks. In this study, we show that PEFT is more susceptible to weight-poisoning backdoor attacks compared to the full-parameter <b>fine-tuning</b> method, with pre-defined triggers remaining exploitable and pre-defined targets maintaining high confidence, even after <b>fine-tuning.</b> Motivated by this insight, we developed a Poisoned Sample Identification Module (PSIM) leveraging PEFT, which identifies poisoned samples through confidence, providing robust defense against weight-poisoning backdoor attacks. Specifically, we leverage PEFT to train the PSIM with randomly reset sample labels. During the inference process, extreme confidence serves as an indicator for poisoned samples, while others are clean. We conduct experiments on <b>text</b> <b>classification</b> tasks, five <b>fine-tuning</b> strategies, and three weight-poisoning backdoor attack methods. Experiments show near 100% success rates for weight-poisoning backdoor attacks when utilizing PEFT. Furthermore, our defensive approach exhibits overall competitive performance in mitigating weight-poisoning backdoor attacks.</p></p class="citation"></blockquote><h3 id=79--196346-deployment-of-advanced-and-intelligent-logistics-vehicles-with-enhanced-tracking-and-security-features-iqtiar-md-siddique-et-al-2024>(7/9 | 196/346) Deployment of Advanced and Intelligent Logistics Vehicles with Enhanced Tracking and Security Features (Iqtiar Md Siddique et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Iqtiar Md Siddique, Selim Molla, MD Rakib Hasan, Anamika Ahmed Siddique. (2024)<br><strong>Deployment of Advanced and Intelligent Logistics Vehicles with Enhanced Tracking and Security Features</strong><br><button class=copy-to-clipboard title="Deployment of Advanced and Intelligent Logistics Vehicles with Enhanced Tracking and Security Features" index=196>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-196 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs-SY, cs.CR, eess-SY<br>Keyword Score: 13<br>Keywords: Clustering, Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11829v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11829v1.pdf filename=2402.11829v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This study focuses on the implementation of modern and intelligent logistics vehicles equipped with advanced tracking and security features. In response to the evolving landscape of logistics management, the proposed system integrates cutting edge technologies to enhance efficiency and ensure the security of the entire logistics process. The core component of this implementation is the incorporation of state-of-the art tracking mechanisms, enabling real-time monitoring of vehicle locations and movements. Furthermore, the system addresses the paramount concern of security by introducing advanced security measures. Through the utilization of sophisticated tracking technologies and security protocols, the proposed logistics vehicles aim to safeguard both customer and provider data. The implementation includes the integration of QR code concepts, creating a binary image system that conceals sensitive information and ensures access only to authorized users. In addition to tracking and security, the study delves into the realm of information mining, employing techniques such as classification, <b>clustering,</b> and <b>recommendation</b> to extract meaningful patterns from vast datasets. Collaborative filtering techniques are incorporated to enhance customer experience by recommending services based on user preferences and historical data. This abstract encapsulates the comprehensive approach of deploying modern logistics vehicles, emphasizing their intelligence through advanced tracking, robust security measures, and data-driven insights. The proposed system aims to revolutionize logistics management, providing a seamless and secure experience for both customers and service providers in the dynamic logistics landscape.</p></p class="citation"></blockquote><h3 id=89--197346-an-interview-study-on-third-party-cyber-threat-hunting-processes-in-the-us-department-of-homeland-security-william-p-maxam-iii-et-al-2024>(8/9 | 197/346) An Interview Study on Third-Party Cyber Threat Hunting Processes in the U.S. Department of Homeland Security (William P. Maxam III et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>William P. Maxam III, James C. Davis. (2024)<br><strong>An Interview Study on Third-Party Cyber Threat Hunting Processes in the U.S. Department of Homeland Security</strong><br><button class=copy-to-clipboard title="An Interview Study on Third-Party Cyber Threat Hunting Processes in the U.S. Department of Homeland Security" index=197>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-197 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs-SE, cs.CR<br>Keyword Score: 10<br>Keywords: Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12252v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12252v1.pdf filename=2402.12252v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Cybersecurity is a major challenge for large organizations. Traditional cybersecurity defense is reactive. Cybersecurity operations centers keep out adversaries and incident response teams clean up after break-ins. Recently a proactive stage has been introduced: Cyber Threat Hunting (TH) looks for potential compromises missed by other cyber defenses. TH is mandated for federal executive agencies and government contractors. As threat hunting is a new cybersecurity discipline, most TH teams operate without a defined process. The practices and challenges of TH have not yet been documented. To address this gap, this paper describes the first interview study of threat hunt practitioners. We obtained access and interviewed 11 threat hunters associated with the U.S. government&rsquo;s Department of Homeland Security. Hour-long interviews were conducted. We analyzed the transcripts with process and thematic coding.We describe the diversity among their processes, show that their processes differ from the TH processes reported in the literature, and unify our subjects&rsquo; descriptions into a single TH process.We enumerate common TH challenges and solutions according to the subjects. The two most common challenges were difficulty in assessing a Threat Hunter&rsquo;s expertise, and developing and maintaining automation. We conclude with <b>recommendations</b> for TH teams (improve planning, focus on automation, and apprentice new members) and highlight directions for future work (finding a TH process that balances flexibility and formalism, and identifying assessments for TH team performance).</p></p class="citation"></blockquote><h3 id=99--198346-attack-tree-generation-via-process-mining-alyzia-maria-konsta-et-al-2024>(9/9 | 198/346) Attack Tree Generation via Process Mining (Alyzia-Maria Konsta et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Alyzia-Maria Konsta, Gemma Di Federico, Alberto Lluch Lafuente, Andrea Burattin. (2024)<br><strong>Attack Tree Generation via Process Mining</strong><br><button class=copy-to-clipboard title="Attack Tree Generation via Process Mining" index=198>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-198 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs-FL, cs.CR<br>Keyword Score: 10<br>Keywords: Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12040v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12040v1.pdf filename=2402.12040v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Attack Trees are a graphical model of security used to study threat scenarios. While visually appealing and supported by solid theories and effective tools, one of their main drawbacks remains the amount of effort required by security experts to design them from scratch. This work aims to remedy this by providing a method for the automatic generation of Attack Trees from attack logs. The main original feature of our approach w.r.t existing ones is the use of Process Mining algorithms to synthesize Attack Trees, which allow users to customize the way a set of logs are <b>summarized</b> as an Attack Tree, for example by discarding statistically irrelevant events. Our approach is supported by a prototype that, apart from the derivation and translation of the model, provides the user with an Attack Tree in the RisQFLan format, a tool used for quantitative risk modeling and analysis with Attack Trees. We illustrate our approach with the case study of attacks on a communication protocol, produced by a state-of-the-art protocol analyzer.</p></p class="citation"></blockquote><h2 id=csse-3>cs.SE (3)</h2><h3 id=13--199346-enhancing-large-language-models-for-text-to-testcase-generation-saranya-alagarsamy-et-al-2024>(1/3 | 199/346) Enhancing Large Language Models for Text-to-Testcase Generation (Saranya Alagarsamy et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Saranya Alagarsamy, Chakkrit Tantithamthavorn, Chetan Arora, Aldeida Aleti. (2024)<br><strong>Enhancing Large Language Models for Text-to-Testcase Generation</strong><br><button class=copy-to-clipboard title="Enhancing Large Language Models for Text-to-Testcase Generation" index=199>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-199 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-SE, cs.SE<br>Keyword Score: 90<br>Keywords: Fine-tuning, Fine-tuning, BLOOM, GPT, GPT-3, GPT-3.5, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11910v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11910v1.pdf filename=2402.11910v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Context: Test-driven development (TDD) is a widely employed software development practice that involves developing test cases based on requirements prior to writing the code. Although various methods for automated test case generation have been proposed, they are not specifically tailored for TDD, where requirements instead of code serve as input. Objective: In this paper, we introduce a text-to-testcase generation approach based on a <b>large</b> <b>language</b> <b>model</b> <b>(GPT-3.5)</b> that is <b>fine-tuned</b> on our curated dataset with an effective <b>prompt</b> design. Method: Our approach involves enhancing the capabilities of basic <b>GPT-3.5</b> for text-to-testcase generation task that is <b>fine-tuned</b> on our curated dataset with an effective <b>prompting</b> design. We evaluated the effectiveness of our approach using a span of five <b>large-scale</b> <b>open-source</b> <b>software</b> projects. Results: Our approach generated 7k test cases for open source projects, achieving 78.5% syntactic correctness, 67.09% requirement alignment, and 61.7% code coverage, which substantially outperforms all other <b>LLMs</b> (basic <b>GPT-3.5,</b> <b>Bloom,</b> and CodeT5). In addition, our ablation study demonstrates the substantial performance improvement of the <b>fine-tuning</b> and <b>prompting</b> components of the <b>GPT-3.5</b> model. Conclusions: These findings lead us to conclude that <b>fine-tuning</b> and <b>prompting</b> should be considered in the future when building a language model for the text-to-testcase generation task</p></p class="citation"></blockquote><h3 id=23--200346-codeart-better-code-models-by-attention-regularization-when-symbols-are-lacking-zian-su-et-al-2024>(2/3 | 200/346) CodeArt: Better Code Models by Attention Regularization When Symbols Are Lacking (Zian Su et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zian Su, Xiangzhe Xu, Ziyang Huang, Zhuo Zhang, Yapeng Ye, Jianjun Huang, Xiangyu Zhang. (2024)<br><strong>CodeArt: Better Code Models by Attention Regularization When Symbols Are Lacking</strong><br><button class=copy-to-clipboard title="CodeArt: Better Code Models by Attention Regularization When Symbols Are Lacking" index=200>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-200 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-AI, cs-SE, cs.SE<br>Keyword Score: 50<br>Keywords: BERT, Transformer, Tokenization, Masked Language Model, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11842v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11842v1.pdf filename=2402.11842v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Transformer</b> based code models have impressive performance in many software engineering tasks. However, their effectiveness degrades when symbols are missing or not informative. The reason is that the model may not learn to pay attention to the right correlations/contexts without the help of symbols. We propose a new method to pre-train general code models when symbols are lacking. We observe that in such cases, programs degenerate to something written in a very primitive language. We hence propose to use program analysis to extract contexts a priori (instead of relying on symbols and <b>masked</b> <b>language</b> <b>modeling</b> as in vanilla models). We then leverage a novel attention masking method to only allow the model attending to these contexts, e.g., bi-directional program dependence transitive closures and token co-occurrences. In the meantime, the inherent <b>self-attention</b> mechanism is utilized to learn which of the allowed attentions are more important compared to others. To realize the idea, we enhance the vanilla <b>tokenization</b> and model architecture of a <b>BERT</b> model, construct and utilize attention masks, and introduce a new pre-training algorithm. We pre-train this <b>BERT-like</b> model from scratch, using a dataset of 26 million stripped binary functions with explicit program dependence information extracted by our tool. We apply the model in three downstream tasks: binary similarity, type inference, and malware family classification. Our pre-trained model can improve the SOTAs in these tasks from 53% to 64%, 49% to 60%, and 74% to 94%, respectively. It also substantially outperforms other general pre-training techniques of code understanding models.</p></p class="citation"></blockquote><h3 id=33--201346-parallel-program-analysis-on-path-ranges-jan-haltermanna-et-al-2024>(3/3 | 201/346) Parallel Program Analysis on Path Ranges (Jan Haltermanna et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jan Haltermanna, Marie-Christine Jakobs, Cedric Richter, Heike Wehrheim. (2024)<br><strong>Parallel Program Analysis on Path Ranges</strong><br><button class=copy-to-clipboard title="Parallel Program Analysis on Path Ranges" index=201>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-201 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-SE, cs.SE<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11938v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11938v1.pdf filename=2402.11938v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Symbolic execution is a software verification technique symbolically running programs and thereby checking for bugs. Ranged symbolic execution performs symbolic execution on program parts, so called path ranges, in parallel. Due to the parallelism, verification is accelerated and hence scales to larger programs. In this paper, we discuss a generalization of ranged symbolic execution to arbitrary program analyses. More specifically, we present a verification approach that splits programs into path ranges and then runs arbitrary analyses on the ranges in parallel. Our approach in particular allows to run different analyses on different program parts. We have implemented this generalization on top of the tool CPAchecker and evaluated it on programs from the SV-COMP <b>benchmark.</b> Our evaluation shows that verification can benefit from the parallelisation of the verification task, but also needs a form of work stealing (between analyses) as to become efficient</p></p class="citation"></blockquote><h2 id=csir-7>cs.IR (7)</h2><h3 id=17--202346-feb4rag-evaluating-federated-search-in-the-context-of-retrieval-augmented-generation-shuai-wang-et-al-2024>(1/7 | 202/346) FeB4RAG: Evaluating Federated Search in the Context of Retrieval Augmented Generation (Shuai Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shuai Wang, Ekaterina Khramtsova, Shengyao Zhuang, Guido Zuccon. (2024)<br><strong>FeB4RAG: Evaluating Federated Search in the Context of Retrieval Augmented Generation</strong><br><button class=copy-to-clipboard title="FeB4RAG: Evaluating Federated Search in the Context of Retrieval Augmented Generation" index=202>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-202 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-CL, cs-IR, cs.IR<br>Keyword Score: 66<br>Keywords: Benchmarking, Benchmarking, Retrieval-Augmented Generation, Retrieval-Augmented Generation, Retrieval-Augmented Generation, Chatbot, Information Retrieval, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11891v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11891v1.pdf filename=2402.11891v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Federated search systems aggregate results from multiple search engines, selecting appropriate sources to enhance result quality and align with user intent. With the increasing uptake of <b>Retrieval-Augmented</b> <b>Generation</b> <b>(RAG)</b> pipelines, federated search can play a pivotal role in sourcing relevant <b>information</b> <b>across</b> heterogeneous data sources to generate informed responses. However, existing datasets, such as those developed in the past TREC FedWeb tracks, predate the <b>RAG</b> paradigm shift and lack representation of modern <b>information</b> <b>retrieval</b> <b>challenges.</b> <b>To</b> bridge this gap, we present FeB4RAG, a novel dataset specifically designed for federated search within <b>RAG</b> frameworks. This dataset, derived from 16 sub-collections of the widely used \beir <b>benchmarking</b> collection, includes 790 <b>information</b> <b>requests</b> (akin to conversational queries) tailored for <b>chatbot</b> applications, along with top results returned by each resource and associated <b>LLM-derived</b> relevance judgements. Additionally, to support the need for this collection, we demonstrate the impact on response generation of a high quality federated search system for <b>RAG</b> compared to a naive approach to federated search. We do so by comparing answers generated through the <b>RAG</b> pipeline through a qualitative side-by-side comparison. Our collection fosters and supports the development and evaluation of new federated search methods, especially in the context of <b>RAG</b> pipelines.</p></p class="citation"></blockquote><h3 id=27--203346-ask-optimal-questions-aligning-large-language-models-with-retrievers-preference-in-conversational-search-chanwoong-yoon-et-al-2024>(2/7 | 203/346) Ask Optimal Questions: Aligning Large Language Models with Retriever&rsquo;s Preference in Conversational Search (Chanwoong Yoon et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chanwoong Yoon, Gangwoo Kim, Byeongguk Jeon, Sungdong Kim, Yohan Jo, Jaewoo Kang. (2024)<br><strong>Ask Optimal Questions: Aligning Large Language Models with Retriever&rsquo;s Preference in Conversational Search</strong><br><button class=copy-to-clipboard title="Ask Optimal Questions: Aligning Large Language Models with Retriever's Preference in Conversational Search" index=203>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-203 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-CL, cs-IR, cs.IR<br>Keyword Score: 63<br>Keywords: Benchmarking, Fine-tuning, GPT, GPT-3, GPT-3.5, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11827v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11827v1.pdf filename=2402.11827v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Conversational search, unlike single-turn retrieval tasks, requires understanding the current question within a dialogue context. The common approach of rewrite-then-retrieve aims to decontextualize questions to be self-sufficient for off-the-shelf retrievers, but most existing methods produce sub-optimal query rewrites due to the limited ability to incorporate signals from the retrieval results. To overcome this limitation, we present a novel framework RetPO (Retriever&rsquo;s Preference Optimization), which is designed to optimize a language model (LM) for reformulating search queries in line with the preferences of the target retrieval systems. The process begins by <b>prompting</b> a <b>large</b> <b>LM</b> <b>to</b> produce various potential rewrites and then collects retrieval performance for these rewrites as the retrievers&rsquo; preferences. Through the process, we construct a <b>large-scale</b> <b>dataset</b> <b>called</b> RF collection, containing Retrievers&rsquo; Feedback on over 410K query rewrites across 12K conversations. Furthermore, we <b>fine-tune</b> a smaller LM using this dataset to align it with the retrievers&rsquo; preferences as feedback. The resulting model achieves state-of-the-art performance on two recent conversational search <b>benchmarks,</b> significantly outperforming existing baselines, including <b>GPT-3.5.</b></p></p class="citation"></blockquote><h3 id=37--204346-explain-then-rank-scale-calibration-of-neural-rankers-using-natural-language-explanations-from-large-language-models-puxuan-yu-et-al-2024>(3/7 | 204/346) Explain then Rank: Scale Calibration of Neural Rankers Using Natural Language Explanations from Large Language Models (Puxuan Yu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Puxuan Yu, Daniel Cohen, Hemank Lamba, Joel Tetreault, Alex Jaimes. (2024)<br><strong>Explain then Rank: Scale Calibration of Neural Rankers Using Natural Language Explanations from Large Language Models</strong><br><button class=copy-to-clipboard title="Explain then Rank: Scale Calibration of Neural Rankers Using Natural Language Explanations from Large Language Models" index=204>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-204 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 40<br>Keywords: Document Ranking, Natural Language Explanation, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12276v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12276v1.pdf filename=2402.12276v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The process of scale calibration in ranking systems involves adjusting the outputs of rankers to correspond with significant qualities like click-through rates or relevance, crucial for mirroring real-world value and thereby boosting the system&rsquo;s effectiveness and reliability. Although there has been research on calibrated ranking losses within learning-to-rank models, the particular issue of adjusting the scale for neural rankers, which excel in handling textual information, has not been thoroughly examined. Neural ranking models are adept at processing text data, yet the application of existing scale calibration techniques to these models poses significant challenges due to their complexity and the intensive training they require, often resulting in suboptimal outcomes. This study delves into the potential of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> to provide uncertainty measurements for a query and <b>document</b> <b>pair</b> that correlate with the scale-calibrated scores. By employing Monte Carlo sampling to gauge relevance probabilities from <b>LLMs</b> and incorporating <b>natural</b> <b>language</b> <b>explanations</b> (NLEs) to articulate this uncertainty, we carry out comprehensive tests on two major <b>document</b> <b>ranking</b> datasets. Our findings reveal that the approach leveraging NLEs outperforms existing calibration methods under various training scenarios, leading to better calibrated neural rankers.</p></p class="citation"></blockquote><h3 id=47--205346-leveraging-opposite-gender-interaction-ratio-as-a-path-towards-fairness-in-online-dating-recommendations-based-on-user-sexual-orientation-yuying-zhao-et-al-2024>(4/7 | 205/346) Leveraging Opposite Gender Interaction Ratio as a Path towards Fairness in Online Dating Recommendations Based on User Sexual Orientation (Yuying Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuying Zhao, Yu Wang, Yi Zhang, Pamela Wisniewski, Charu Aggarwal, Tyler Derr. (2024)<br><strong>Leveraging Opposite Gender Interaction Ratio as a Path towards Fairness in Online Dating Recommendations Based on User Sexual Orientation</strong><br><button class=copy-to-clipboard title="Leveraging Opposite Gender Interaction Ratio as a Path towards Fairness in Online Dating Recommendations Based on User Sexual Orientation" index=205>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-205 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 30<br>Keywords: Fairness, Recommendation, Recommender System<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12541v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12541v1.pdf filename=2402.12541v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Online dating platforms have gained widespread popularity as a means for individuals to seek potential romantic relationships. While <b>recommender</b> <b>systems</b> have been designed to improve the user experience in dating platforms by providing personalized <b>recommendations,</b> increasing concerns about <b>fairness</b> have encouraged the development of <b>fairness-aware</b> <b>recommender</b> <b>systems</b> from various perspectives (e.g., gender and race). However, sexual orientation, which plays a significant role in finding a satisfying relationship, is under-investigated. To fill this crucial gap, we propose a novel metric, Opposite Gender Interaction Ratio (OGIR), as a way to investigate potential unfairness for users with varying preferences towards the opposite gender. We empirically analyze a real online dating dataset and observe existing <b>recommender</b> <b>algorithms</b> could suffer from group unfairness according to OGIR. We further investigate the potential causes for such gaps in <b>recommendation</b> quality, which lead to the challenges of group quantity imbalance and group calibration imbalance. Ultimately, we propose a fair <b>recommender</b> <b>system</b> based on re-weighting and re-ranking strategies to respectively mitigate these associated imbalance challenges. Experimental results demonstrate both strategies improve <b>fairness</b> while their combination achieves the best performance towards maintaining model utility while improving <b>fairness.</b></p></p class="citation"></blockquote><h3 id=57--206346-large-language-models-for-stemming-promises-pitfalls-and-failures-shuai-wang-et-al-2024>(5/7 | 206/346) Large Language Models for Stemming: Promises, Pitfalls and Failures (Shuai Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shuai Wang, Shengyao Zhuang, Guido Zuccon. (2024)<br><strong>Large Language Models for Stemming: Promises, Pitfalls and Failures</strong><br><button class=copy-to-clipboard title="Large Language Models for Stemming: Promises, Pitfalls and Failures" index=206>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-206 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-CL, cs-IR, cs.IR<br>Keyword Score: 30<br>Keywords: Stemming, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11757v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11757v1.pdf filename=2402.11757v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Text <b>stemming</b> is a natural language processing technique that is used to reduce words to their base form, also known as the root form. The use of <b>stemming</b> in IR has been shown to often improve the effectiveness of keyword-matching models such as BM25. However, traditional <b>stemming</b> methods, focusing solely on individual terms, overlook the richness of contextual information. Recognizing this gap, in this paper, we investigate the promising idea of using <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> to stem words by leveraging its capability of context understanding. With this respect, we identify three avenues, each characterised by different trade-offs in terms of computational cost, effectiveness and robustness : (1) use <b>LLMs</b> to stem the vocabulary for a collection, i.e., the set of unique words that appear in the collection (vocabulary <b>stemming),</b> (2) use <b>LLMs</b> to stem each document separately (contextual <b>stemming),</b> and (3) use <b>LLMs</b> to extract from each document entities that should not be stemmed, then use vocabulary <b>stemming</b> to stem the rest of the terms (entity-based contextual <b>stemming).</b> Through a series of empirical experiments, we compare the use of <b>LLMs</b> for <b>stemming</b> with that of traditional lexical stemmers such as Porter and Krovetz for English text. We find that while vocabulary <b>stemming</b> and contextual <b>stemming</b> fail to achieve higher effectiveness than traditional stemmers, entity-based contextual <b>stemming</b> can achieve a higher effectiveness than using Porter stemmer alone, under specific conditions.</p></p class="citation"></blockquote><h3 id=67--207346-heterogeneity-aware-cross-school-electives-recommendation-a-hybrid-federated-approach-chengyi-ju-et-al-2024>(6/7 | 207/346) Heterogeneity-aware Cross-school Electives Recommendation: a Hybrid Federated Approach (Chengyi Ju et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chengyi Ju, Jiannong Cao, Yu Yang, Zhen-Qun Yang, Ho Man Lee. (2024)<br><strong>Heterogeneity-aware Cross-school Electives Recommendation: a Hybrid Federated Approach</strong><br><button class=copy-to-clipboard title="Heterogeneity-aware Cross-school Electives Recommendation: a Hybrid Federated Approach" index=207>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-207 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-AI, cs-IR, cs.IR<br>Keyword Score: 23<br>Keywords: Graph, Recommendation, Recommender System<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12202v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12202v1.pdf filename=2402.12202v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the era of modern education, addressing cross-school learner diversity is crucial, especially in personalized <b>recommender</b> <b>systems</b> for elective course selection. However, privacy concerns often limit cross-school data sharing, which hinders existing methods&rsquo; ability to model sparse data and address heterogeneity effectively, ultimately leading to suboptimal <b>recommendations.</b> In response, we propose HFRec, a heterogeneity-aware hybrid federated <b>recommender</b> <b>system</b> designed for cross-school elective course <b>recommendations.</b> The proposed model constructs heterogeneous <b>graphs</b> for each school, incorporating various interactions and historical behaviors between students to integrate context and content information. We design an attention mechanism to capture heterogeneity-aware representations. Moreover, under a federated scheme, we train individual school-based models with adaptive learning settings to recommend tailored electives. Our HFRec model demonstrates its effectiveness in providing personalized elective <b>recommendations</b> while maintaining privacy, as it outperforms state-of-the-art models on both open-source and real-world datasets.</p></p class="citation"></blockquote><h3 id=77--208346-trisampler-a-better-negative-sampling-principle-for-dense-retrieval-zhen-yang-et-al-2024>(7/7 | 208/346) TriSampler: A Better Negative Sampling Principle for Dense Retrieval (Zhen Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhen Yang, Zhou Shao, Yuxiao Dong, Jie Tang. (2024)<br><strong>TriSampler: A Better Negative Sampling Principle for Dense Retrieval</strong><br><button class=copy-to-clipboard title="TriSampler: A Better Negative Sampling Principle for Dense Retrieval" index=208>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-208 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 10<br>Keywords: Dense Retrieval<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11855v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11855v1.pdf filename=2402.11855v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Negative sampling stands as a pivotal technique in <b>dense</b> <b>retrieval,</b> essential for training effective retrieval models and significantly impacting retrieval performance. While existing negative sampling methods have made commendable progress by leveraging hard negatives, a comprehensive guiding principle for constructing negative candidates and designing negative sampling distributions is still lacking. To bridge this gap, we embark on a theoretical analysis of negative sampling in <b>dense</b> <b>retrieval.</b> This exploration culminates in the unveiling of the quasi-triangular principle, a novel framework that elucidates the triangular-like interplay between query, positive document, and negative document. Fueled by this guiding principle, we introduce TriSampler, a straightforward yet highly effective negative sampling method. The keypoint of TriSampler lies in its ability to selectively sample more informative negatives within a prescribed constrained region. Experimental evaluation show that TriSampler consistently attains superior retrieval performance across a diverse of representative retrieval models.</p></p class="citation"></blockquote><h2 id=cscv-45>cs.CV (45)</h2><h3 id=145--209346-physu-net-long-temporal-context-transformer-for-rppg-with-self-supervised-pre-training-marko-savic-et-al-2024>(1/45 | 209/346) PhySU-Net: Long Temporal Context Transformer for rPPG with Self-Supervised Pre-training (Marko Savic et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Marko Savic, Guoying Zhao. (2024)<br><strong>PhySU-Net: Long Temporal Context Transformer for rPPG with Self-Supervised Pre-training</strong><br><button class=copy-to-clipboard title="PhySU-Net: Long Temporal Context Transformer for rPPG with Self-Supervised Pre-training" index=209>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-209 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 60<br>Keywords: Convolution, Convolutional Neural Network, Self-supervised Learning, Self-supervised Pre-training, Supervised Learning, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11913v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11913v1.pdf filename=2402.11913v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Remote photoplethysmography (rPPG) is a promising technology that consists of contactless measuring of cardiac activity from facial videos. Most recent approaches utilize <b>convolutional</b> <b>networks</b> with limited temporal modeling capability or ignore long temporal context. <b>Supervised</b> rPPG methods are also severely limited by scarce data availability. In this work, we propose PhySU-Net, the first long spatial-temporal map rPPG <b>transformer</b> network and a <b>self-supervised</b> <b>pre-training</b> strategy that exploits unlabeled data to improve our model. Our strategy leverages traditional methods and image masking to provide pseudo-labels for <b>self-supervised</b> <b>pre-training.</b> Our model is tested on two public datasets (OBF and VIPL-HR) and shows superior performance in <b>supervised</b> training. Furthermore, we demonstrate that our <b>self-supervised</b> <b>pre-training</b> strategy further improves our model&rsquo;s performance by leveraging representations learned from unlabeled data.</p></p class="citation"></blockquote><h3 id=245--210346-a-lightweight-parallel-framework-for-blind-image-quality-assessment-qunyue-huang-et-al-2024>(2/45 | 210/346) A Lightweight Parallel Framework for Blind Image Quality Assessment (Qunyue Huang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qunyue Huang, Bin Fang. (2024)<br><strong>A Lightweight Parallel Framework for Blind Image Quality Assessment</strong><br><button class=copy-to-clipboard title="A Lightweight Parallel Framework for Blind Image Quality Assessment" index=210>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-210 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 53<br>Keywords: Benchmarking, Convolution, Convolutional Neural Network, Convolutional Neural Network, Self-supervised Learning, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12043v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12043v1.pdf filename=2402.12043v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Existing blind image quality assessment (BIQA) methods focus on designing complicated networks based on <b>convolutional</b> <b>neural</b> <b>networks</b> <b>(CNNs)</b> or <b>transformer.</b> In addition, some BIQA methods enhance the performance of the model in a two-stage training manner. Despite the significant advancements, these methods remarkably raise the parameter count of the model, thus requiring more training time and computational resources. To tackle the above issues, we propose a lightweight parallel framework (LPF) for BIQA. First, we extract the visual features using a pre-trained feature extraction network. Furthermore, we construct a simple yet effective feature embedding network (FEN) to transform the visual features, aiming to generate the latent representations that contain salient distortion information. To improve the robustness of the latent representations, we present two novel <b>self-supervised</b> subtasks, including a sample-level category prediction task and a batch-level quality comparison task. The sample-level category prediction task is presented to help the model with coarse-grained distortion perception. The batch-level quality comparison task is formulated to enhance the training data and thus improve the robustness of the latent representations. Finally, the latent representations are fed into a distortion-aware quality regression network (DaQRN), which simulates the human vision system (HVS) and thus generates accurate quality scores. Experimental results on multiple <b>benchmark</b> datasets demonstrate that the proposed method achieves superior performance over state-of-the-art approaches. Moreover, extensive analyses prove that the proposed method has lower computational complexity and faster convergence speed.</p></p class="citation"></blockquote><h3 id=345--211346-adversarial-feature-alignment-balancing-robustness-and-accuracy-in-deep-learning-via-adversarial-training-leo-hyun-park-et-al-2024>(3/45 | 211/346) Adversarial Feature Alignment: Balancing Robustness and Accuracy in Deep Learning via Adversarial Training (Leo Hyun Park et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Leo Hyun Park, Jaeuk Kim, Myung Gyo Oh, Jaewoo Park, Taekyoung Kwon. (2024)<br><strong>Adversarial Feature Alignment: Balancing Robustness and Accuracy in Deep Learning via Adversarial Training</strong><br><button class=copy-to-clipboard title="Adversarial Feature Alignment: Balancing Robustness and Accuracy in Deep Learning via Adversarial Training" index=211>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-211 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: I-4-0; K-6-5; D-2-7, cs-CR, cs-CV, cs-LG, cs.CV<br>Keyword Score: 50<br>Keywords: Diffusion Model, Adversarial Learning, Contrastive Learning, Data Augmentation, Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12187v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12187v1.pdf filename=2402.12187v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep learning models continue to advance in accuracy, yet they remain vulnerable to <b>adversarial</b> <b>attacks,</b> which often lead to the misclassification of <b>adversarial</b> <b>examples.</b> <b>Adversarial</b> <b>training</b> is used to mitigate this problem by increasing robustness against these attacks. However, this approach typically reduces a model&rsquo;s standard accuracy on clean, non-adversarial samples. The necessity for deep learning models to balance both robustness and accuracy for security is obvious, but achieving this balance remains challenging, and the underlying reasons are yet to be clarified. This paper proposes a novel <b>adversarial</b> <b>training</b> method called <b>Adversarial</b> <b>Feature</b> Alignment (AFA), to address these problems. Our research unveils an intriguing insight: misalignment within the feature space often leads to misclassification, regardless of whether the samples are benign or <b>adversarial.</b> <b>AFA</b> mitigates this risk by employing a novel optimization algorithm based on <b>contrastive</b> <b>learning</b> to alleviate potential feature misalignment. Through our evaluations, we demonstrate the superior performance of AFA. The baseline AFA delivers higher robust accuracy than previous <b>adversarial</b> <b>contrastive</b> <b>learning</b> methods while minimizing the drop in clean accuracy to 1.86% and 8.91% on CIFAR10 and CIFAR100, respectively, in comparison to cross-entropy. We also show that joint optimization of AFA and TRADES, accompanied by <b>data</b> <b>augmentation</b> using a recent <b>diffusion</b> <b>model,</b> achieves state-of-the-art accuracy and robustness.</p></p class="citation"></blockquote><h3 id=445--212346-direct-consistency-optimization-for-compositional-text-to-image-personalization-kyungmin-lee-et-al-2024>(4/45 | 212/346) Direct Consistency Optimization for Compositional Text-to-Image Personalization (Kyungmin Lee et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kyungmin Lee, Sangkyung Kwak, Kihyuk Sohn, Jinwoo Shin. (2024)<br><strong>Direct Consistency Optimization for Compositional Text-to-Image Personalization</strong><br><button class=copy-to-clipboard title="Direct Consistency Optimization for Compositional Text-to-Image Personalization" index=212>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-212 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 50<br>Keywords: Diffusion Model, Fine-tuning, Image2text, Text2image, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12004v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12004v1.pdf filename=2402.12004v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Text-to-image</b> (T2I) <b>diffusion</b> <b>models,</b> when <b>fine-tuned</b> on a few personal images, are able to generate visuals with a high degree of consistency. However, they still lack in synthesizing images of different scenarios or styles that are possible in the original pretrained models. To address this, we propose to <b>fine-tune</b> the T2I model by maximizing consistency to reference images, while penalizing the deviation from the pretrained model. We devise a novel training objective for T2I <b>diffusion</b> <b>models</b> that minimally <b>fine-tunes</b> the pretrained model to achieve consistency. Our method, dubbed \emph{Direct Consistency Optimization}, is as simple as regular <b>diffusion</b> <b>loss,</b> while significantly enhancing the compositionality of personalized T2I models. Also, our approach induces a new sampling method that controls the tradeoff between image fidelity and <b>prompt</b> fidelity. Lastly, we emphasize the necessity of using a comprehensive caption for reference images to further enhance the <b>image-text</b> alignment. We show the efficacy of the proposed method on the T2I personalization for subject, style, or both. In particular, our method results in a superior Pareto frontier to the baselines. Generated examples and codes are in our project page( <a href=https://dco-t2i.github.io/)>https://dco-t2i.github.io/)</a>.</p></p class="citation"></blockquote><h3 id=545--213346-note-notable-generation-of-patient-text-summaries-through-efficient-approach-based-on-direct-preference-optimization-imjin-ahn-et-al-2024>(5/45 | 213/346) NOTE: Notable generation Of patient Text summaries through Efficient approach based on direct preference optimization (Imjin Ahn et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Imjin Ahn, Hansle Gwon, Young-Hak Kim, Tae Joon Jun, Sanghyun Park. (2024)<br><strong>NOTE: Notable generation Of patient Text summaries through Efficient approach based on direct preference optimization</strong><br><button class=copy-to-clipboard title="NOTE: Notable generation Of patient Text summaries through Efficient approach based on direct preference optimization" index=213>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-213 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: J-3, cs-CV, cs.CV<br>Keyword Score: 50<br>Keywords: Direct Preference Optimization, Fine-tuning, Large Language Model, Large Language Model, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11882v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11882v1.pdf filename=2402.11882v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The discharge summary is a one of critical documents in the patient journey, encompassing all events experienced during hospitalization, including multiple visits, medications, tests, surgery/procedures, and admissions/discharge. Providing a summary of the patient&rsquo;s progress is crucial, as it significantly influences future care and planning. Consequently, clinicians face the laborious and resource-intensive task of manually collecting, organizing, and combining all the necessary data for a discharge summary. Therefore, we propose &ldquo;NOTE&rdquo;, which stands for &ldquo;Notable generation Of patient Text summaries through an Efficient approach based on <b>direct</b> <b>preference</b> <b>optimization&rdquo;.</b> NOTE is based on Medical Information Mart for Intensive Care- III dataset and <b>summarizes</b> a single hospitalization of a patient. Patient events are sequentially combined and used to generate a discharge summary for each hospitalization. In the present circumstances, <b>large</b> <b>language</b> <b>models&rsquo;</b> application programming interfaces <b>(LLMs&rsquo;</b> APIs) are widely available, but importing and exporting medical data presents significant challenges due to privacy protection policies in healthcare institutions. Moreover, to ensure optimal performance, it is essential to implement a lightweight model for internal server or program within the hospital. Therefore, we utilized DPO and parameter efficient fine tuning (PEFT) techniques to apply a <b>fine-tuning</b> method that guarantees superior performance. To demonstrate the practical application of the developed NOTE, we provide a webpage-based demonstration software. In the future, we will aim to deploy the software available for actual use by clinicians in hospital. NOTE can be utilized to generate various summaries not only discharge summaries but also throughout a patient&rsquo;s journey, thereby alleviating the labor-intensive workload of clinicians and aiming for increased efficiency.</p></p class="citation"></blockquote><h3 id=645--214346-rock-classification-based-on-residual-networks-sining-zhoubian-et-al-2024>(6/45 | 214/346) Rock Classification Based on Residual Networks (Sining Zhoubian et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sining Zhoubian, Yuyang Wang, Zhihuan Jiang. (2024)<br><strong>Rock Classification Based on Residual Networks</strong><br><button class=copy-to-clipboard title="Rock Classification Based on Residual Networks" index=214>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-214 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 50<br>Keywords: Convolution, Convolutional Neural Network, Data Augmentation, Transformer, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11831v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11831v1.pdf filename=2402.11831v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Rock Classification is an essential geological problem since it provides important formation information. However, exploration on this problem using <b>convolutional</b> <b>neural</b> <b>networks</b> is not sufficient. To tackle this problem, we propose two approaches using residual neural networks. We first adopt <b>data</b> <b>augmentation</b> methods to enlarge our dataset. By modifying kernel sizes, normalization methods and composition based on ResNet34, we achieve an accuracy of 70.1% on the test dataset, with an increase of 3.5% compared to regular Resnet34. Furthermore, using a similar backbone like BoTNet that incorporates multihead self attention, we additionally use internal residual connections in our model. This boosts the model&rsquo;s performance, achieving an accuracy of 73.7% on the test dataset. We also explore how the number of bottleneck <b>transformer</b> blocks may influence model performance. We discover that models with more than one bottleneck <b>transformer</b> block may not further improve performance. Finally, we believe that our approach can inspire future work related to this problem and our model design can facilitate the development of new residual model architectures.</p></p class="citation"></blockquote><h3 id=745--215346-chartx--chartvlm-a-versatile-benchmark-and-foundation-model-for-complicated-chart-reasoning-renqiu-xia-et-al-2024>(7/45 | 215/346) ChartX & ChartVLM: A Versatile Benchmark and Foundation Model for Complicated Chart Reasoning (Renqiu Xia et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Renqiu Xia, Bo Zhang, Hancheng Ye, Xiangchao Yan, Qi Liu, Hongbin Zhou, Zijun Chen, Min Dou, Botian Shi, Junchi Yan, Yu Qiao. (2024)<br><strong>ChartX & ChartVLM: A Versatile Benchmark and Foundation Model for Complicated Chart Reasoning</strong><br><button class=copy-to-clipboard title="ChartX & ChartVLM: A Versatile Benchmark and Foundation Model for Complicated Chart Reasoning" index=215>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-215 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 46<br>Keywords: Benchmarking, Foundation Model, Multi-modal, GPT, Reasoning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12185v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12185v1.pdf filename=2402.12185v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recently, many versatile <b>Multi-modal</b> <b>Large</b> <b>Language</b> <b>Models</b> (MLLMs) have emerged continuously. However, their capacity to query information depicted in visual charts and engage in <b>reasoning</b> based on the queried contents remains under-explored. In this paper, to comprehensively and rigorously <b>benchmark</b> the ability of the off-the-shelf MLLMs in the chart domain, we construct ChartX, a <b>multi-modal</b> evaluation set covering 18 chart types, 7 chart tasks, 22 disciplinary topics, and high-quality chart data. Besides, we develop ChartVLM to offer a new perspective on handling <b>multi-modal</b> tasks that strongly depend on interpretable patterns, such as <b>reasoning</b> tasks in the field of charts or geometric images. We evaluate the chart-related ability of mainstream MLLMs and our ChartVLM on the proposed ChartX evaluation set. Extensive experiments demonstrate that ChartVLM surpasses both versatile and chart-related <b>large</b> <b>models,</b> <b>achieving</b> results comparable to <b>GPT-4V.</b> We believe that our study can pave the way for further exploration in creating a more comprehensive chart evaluation set and developing more interpretable <b>multi-modal</b> models. Both ChartX and ChartVLM are available at: <a href=https://github.com/UniModal4Reasoning/ChartVLM>https://github.com/UniModal4Reasoning/ChartVLM</a></p></p class="citation"></blockquote><h3 id=845--216346-mm-survnet-deep-learning-based-survival-risk-stratification-in-breast-cancer-through-multimodal-data-fusion-raktim-kumar-mondol-et-al-2024>(8/45 | 216/346) MM-SurvNet: Deep Learning-Based Survival Risk Stratification in Breast Cancer Through Multimodal Data Fusion (Raktim Kumar Mondol et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Raktim Kumar Mondol, Ewan K. A. Millar, Arcot Sowmya, Erik Meijering. (2024)<br><strong>MM-SurvNet: Deep Learning-Based Survival Risk Stratification in Breast Cancer Through Multimodal Data Fusion</strong><br><button class=copy-to-clipboard title="MM-SurvNet: Deep Learning-Based Survival Risk Stratification in Breast Cancer Through Multimodal Data Fusion" index=216>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-216 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 46<br>Keywords: Vision Transformer, Multi-modal, Multi-modal, Transformer, Self-Attention, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11788v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11788v1.pdf filename=2402.11788v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Survival risk stratification is an important step in clinical decision making for breast cancer management. We propose a novel deep learning approach for this purpose by integrating histopathological imaging, genetic and clinical data. It employs <b>vision</b> <b>transformers,</b> specifically the MaxViT model, for image feature extraction, and <b>self-attention</b> to capture intricate image relationships at the patient level. A dual cross-attention mechanism fuses these features with genetic data, while clinical data is incorporated at the final layer to enhance predictive accuracy. Experiments on the public TCGA-BRCA dataset show that our model, trained using the negative log likelihood loss function, can achieve superior performance with a mean C-index of 0.64, surpassing existing methods. This advancement facilitates tailored treatment strategies, potentially leading to improved patient outcomes.</p></p class="citation"></blockquote><h3 id=945--217346-improving-deep-generative-models-on-many-to-one-image-to-image-translation-sagar-saxena-et-al-2024>(9/45 | 217/346) Improving Deep Generative Models on Many-To-One Image-to-Image Translation (Sagar Saxena et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sagar Saxena, Mohammad Nayeem Teli. (2024)<br><strong>Improving Deep Generative Models on Many-To-One Image-to-Image Translation</strong><br><button class=copy-to-clipboard title="Improving Deep Generative Models on Many-To-One Image-to-Image Translation" index=217>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-217 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 43<br>Keywords: Diffusion Model, MNIST, Benchmarking, Generative Adversarial Network, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12531v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12531v2.pdf filename=2402.12531v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep <b>generative</b> <b>models</b> <b>have</b> been applied to multiple applications in image-to-image translation. <b>Generative</b> <b>Adversarial</b> <b>Networks</b> and <b>Diffusion</b> <b>Models</b> have presented impressive results, setting new state-of-the-art results on these tasks. Most methods have symmetric setups across the different domains in a dataset. These methods assume that all domains have either multiple modalities or only one modality. However, there are many datasets that have a many-to-one relationship between two domains. In this work, we first introduce a Colorized <b>MNIST</b> dataset and a Color-Recall score that can provide a simple <b>benchmark</b> for evaluating models on many-to-one translation. We then introduce a new asymmetric framework to improve existing deep <b>generative</b> <b>models</b> <b>on</b> many-to-one image-to-image translation. We apply this framework to StarGAN V2 and show that in both <b>unsupervised</b> and semi-supervised settings, the performance of this new model improves on many-to-one image-to-image translation.</p></p class="citation"></blockquote><h3 id=1045--218346-open3dsg-open-vocabulary-3d-scene-graphs-from-point-clouds-with-queryable-objects-and-open-set-relationships-sebastian-koch-et-al-2024>(10/45 | 218/346) Open3DSG: Open-Vocabulary 3D Scene Graphs from Point Clouds with Queryable Objects and Open-Set Relationships (Sebastian Koch et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sebastian Koch, Narunas Vaskevicius, Mirco Colosi, Pedro Hermosilla, Timo Ropinski. (2024)<br><strong>Open3DSG: Open-Vocabulary 3D Scene Graphs from Point Clouds with Queryable Objects and Open-Set Relationships</strong><br><button class=copy-to-clipboard title="Open3DSG: Open-Vocabulary 3D Scene Graphs from Point Clouds with Queryable Objects and Open-Set Relationships" index=218>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-218 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 43<br>Keywords: Graph, Foundation Model, Zero-shot, Large Language Model, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12259v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12259v1.pdf filename=2402.12259v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Current approaches for 3D scene <b>graph</b> prediction rely on labeled datasets to train models for a fixed set of known object classes and relationship categories. We present Open3DSG, an alternative approach to learn 3D scene <b>graph</b> prediction in an open world without requiring labeled scene <b>graph</b> data. We co-embed the features from a 3D scene <b>graph</b> prediction backbone with the feature space of powerful open world 2D vision language <b>foundation</b> <b>models.</b> This enables us to predict 3D scene <b>graphs</b> from 3D point clouds in a <b>zero-shot</b> manner by querying object classes from an open vocabulary and predicting the inter-object relationships from a grounded <b>LLM</b> with scene <b>graph</b> features and queried object classes as context. Open3DSG is the first 3D point cloud method to predict not only explicit open-vocabulary object classes, but also open-set relationships that are not limited to a predefined label set, making it possible to express rare as well as specific objects and relationships in the predicted 3D scene <b>graph.</b> Our experiments show that Open3DSG is effective at predicting arbitrary object classes as well as their complex inter-object relationships describing spatial, supportive, semantic and comparative relationships.</p></p class="citation"></blockquote><h3 id=1145--219346-scaffolding-coordinates-to-promote-vision-language-coordination-in-large-multi-modal-models-xuanyu-lei-et-al-2024>(11/45 | 219/346) Scaffolding Coordinates to Promote Vision-Language Coordination in Large Multi-Modal Models (Xuanyu Lei et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xuanyu Lei, Zonghan Yang, Xinrui Chen, Peng Li, Yang Liu. (2024)<br><strong>Scaffolding Coordinates to Promote Vision-Language Coordination in Large Multi-Modal Models</strong><br><button class=copy-to-clipboard title="Scaffolding Coordinates to Promote Vision-Language Coordination in Large Multi-Modal Models" index=219>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-219 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CL, cs-CV, cs.CV<br>Keyword Score: 43<br>Keywords: Multi-modal, GPT, Reasoning, Prompt, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12058v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12058v1.pdf filename=2402.12058v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>State-of-the-art Large <b>Multi-Modal</b> Models (LMMs) have demonstrated exceptional capabilities in <b>vision-language</b> tasks. Despite their advanced functionalities, the performances of LMMs are still limited in challenging scenarios that require complex <b>reasoning</b> with multiple levels of visual information. Existing <b>prompting</b> techniques for LMMs focus on either improving textual <b>reasoning</b> or leveraging tools for image preprocessing, lacking a simple and general visual <b>prompting</b> scheme to promote <b>vision-language</b> coordination in LMMs. In this work, we propose Scaffold <b>prompting</b> that scaffolds coordinates to promote <b>vision-language</b> coordination. Specifically, Scaffold overlays a dot matrix within the image as visual information anchors and leverages multi-dimensional coordinates as textual positional references. Extensive experiments on a wide range of challenging <b>vision-language</b> tasks demonstrate the superiority of Scaffold over <b>GPT-4V</b> with the textual CoT <b>prompting.</b> Our code is released in <a href=https://github.com/leixy20/Scaffold>https://github.com/leixy20/Scaffold</a>.</p></p class="citation"></blockquote><h3 id=1245--220346-fit-flexible-vision-transformer-for-diffusion-model-zeyu-lu-et-al-2024>(12/45 | 220/346) FiT: Flexible Vision Transformer for Diffusion Model (Zeyu Lu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zeyu Lu, Zidong Wang, Di Huang, Chengyue Wu, Xihui Liu, Wanli Ouyang, Lei Bai. (2024)<br><strong>FiT: Flexible Vision Transformer for Diffusion Model</strong><br><button class=copy-to-clipboard title="FiT: Flexible Vision Transformer for Diffusion Model" index=220>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-220 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Diffusion Model, Vision Transformer, Transformer, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12376v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12376v1.pdf filename=2402.12376v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Nature is infinitely resolution-free. In the context of this reality, existing <b>diffusion</b> <b>models,</b> such as <b>Diffusion</b> <b>Transformers,</b> often face challenges when processing image resolutions outside of their trained domain. To overcome this limitation, we present the Flexible <b>Vision</b> <b>Transformer</b> (FiT), a <b>transformer</b> architecture specifically designed for generating images with unrestricted resolutions and aspect ratios. Unlike traditional methods that perceive images as static-resolution grids, FiT conceptualizes images as sequences of dynamically-sized tokens. This perspective enables a flexible training strategy that effortlessly adapts to diverse aspect ratios during both training and inference phases, thus promoting resolution generalization and eliminating biases induced by image cropping. Enhanced by a meticulously adjusted network structure and the integration of training-free extrapolation techniques, FiT exhibits remarkable flexibility in resolution extrapolation generation. Comprehensive experiments demonstrate the exceptional performance of FiT across a broad range of resolutions, showcasing its effectiveness both within and beyond its training resolution distribution. Repository available at <a href=https://github.com/whlzy/FiT>https://github.com/whlzy/FiT</a>.</p></p class="citation"></blockquote><h3 id=1345--221346-weakly-supervised-object-detection-in-chest-x-rays-with-differentiable-roi-proposal-networks-and-soft-roi-pooling-philip-müller-et-al-2024>(13/45 | 221/346) Weakly Supervised Object Detection in Chest X-Rays with Differentiable ROI Proposal Networks and Soft ROI Pooling (Philip Müller et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Philip Müller, Felix Meissen, Georgios Kaissis, Daniel Rueckert. (2024)<br><strong>Weakly Supervised Object Detection in Chest X-Rays with Differentiable ROI Proposal Networks and Soft ROI Pooling</strong><br><button class=copy-to-clipboard title="Weakly Supervised Object Detection in Chest X-Rays with Differentiable ROI Proposal Networks and Soft ROI Pooling" index=221>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-221 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 40<br>Keywords: Object Detection, Multiple Instance Learning, Supervised Learning, Weakly-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11985v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11985v1.pdf filename=2402.11985v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Weakly <b>supervised</b> <b>object</b> <b>detection</b> (WSup-OD) increases the usefulness and interpretability of image classification algorithms without requiring additional supervision. The successes of <b>multiple</b> <b>instance</b> <b>learning</b> in this task for natural images, however, do not translate well to medical images due to the very different characteristics of their <b>objects</b> <b>(i.e.</b> pathologies). In this work, we propose Weakly <b>Supervised</b> ROI Proposal Networks (WSRPN), a new method for generating bounding box proposals on the fly using a specialized region of interest-attention (ROI-attention) module. WSRPN integrates well with classic backbone-head classification algorithms and is end-to-end trainable with only image-label supervision. We experimentally demonstrate that our new method outperforms existing methods in the challenging task of disease localization in chest X-ray images. Code: <a href=https://github.com/philip-mueller/wsrpn>https://github.com/philip-mueller/wsrpn</a></p></p class="citation"></blockquote><h3 id=1445--222346-the-revolution-of-multimodal-large-language-models-a-survey-davide-caffagni-et-al-2024>(14/45 | 222/346) The (R)Evolution of Multimodal Large Language Models: A Survey (Davide Caffagni et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Davide Caffagni, Federico Cocchi, Luca Barsellotti, Nicholas Moratelli, Sara Sarto, Lorenzo Baraldi, Lorenzo Baraldi, Marcella Cornia, Rita Cucchiara. (2024)<br><strong>The (R)Evolution of Multimodal Large Language Models: A Survey</strong><br><button class=copy-to-clipboard title="The (R)Evolution of Multimodal Large Language Models: A Survey" index=222>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-222 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CL, cs-CV, cs-MM, cs.CV<br>Keyword Score: 39<br>Keywords: Benchmarking, Multi-modal, Multi-modal, Grounding, Instruction Following, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12451v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12451v1.pdf filename=2402.12451v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Connecting text and visual modalities plays an essential role in generative intelligence. For this reason, inspired by the success of <b>large</b> <b>language</b> <b>models,</b> significant research efforts are being devoted to the development of <b>Multimodal</b> <b>Large</b> <b>Language</b> <b>Models</b> (MLLMs). These models can seamlessly integrate visual and textual modalities, both as input and output, while providing a dialogue-based interface and <b>instruction-following</b> <b>capabilities.</b> In this paper, we provide a comprehensive review of recent visual-based MLLMs, analyzing their architectural choices, <b>multimodal</b> alignment strategies, and training techniques. We also conduct a detailed analysis of these models across a wide range of tasks, including visual <b>grounding,</b> image generation and editing, visual understanding, and domain-specific applications. Additionally, we compile and describe training datasets and evaluation <b>benchmarks,</b> conducting comparisons among existing models in terms of performance and computational requirements. Overall, this survey offers a comprehensive overview of the current state of the art, laying the groundwork for future MLLMs.</p></p class="citation"></blockquote><h3 id=1545--223346-lvchat-facilitating-long-video-comprehension-yu-wang-et-al-2024>(15/45 | 223/346) LVCHAT: Facilitating Long Video Comprehension (Yu Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yu Wang, Zeyuan Zhang, Julian McAuley, Zexue He. (2024)<br><strong>LVCHAT: Facilitating Long Video Comprehension</strong><br><button class=copy-to-clipboard title="LVCHAT: Facilitating Long Video Comprehension" index=223>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-223 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CL, cs-CV, cs.CV<br>Keyword Score: 39<br>Keywords: Benchmarking, Multi-modal, Multi-modal, Question Answering, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12079v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12079v1.pdf filename=2402.12079v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Enabling <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> to read videos is vital for <b>multimodal</b> <b>LLMs.</b> Existing works show promise on short videos whereas long video (longer than e.g.~1 minute) comprehension remains challenging. The major problem lies in the over-compression of videos, i.e., the encoded video representations are not enough to represent the whole video. To address this issue, we propose Long Video Chat (LVChat), where Frame-Scalable Encoding (FSE) is introduced to dynamically adjust the number of embeddings in alignment with the duration of the video to ensure long videos are not overly compressed into a few embeddings. To deal with long videos whose length is beyond videos seen during training, we propose Interleaved Frame Encoding (IFE), repeating positional embedding and interleaving multiple groups of videos to enable long video input, avoiding performance degradation due to overly long videos. Experimental results show that LVChat significantly outperforms existing methods by up to 27% in accuracy on long-video <b>QA</b> datasets and long-video captioning <b>benchmarks.</b> Our code is published at <a href=https://github.com/wangyu-ustc/LVChat>https://github.com/wangyu-ustc/LVChat</a>.</p></p class="citation"></blockquote><h3 id=1645--224346-integrating-knn-with-foundation-models-for-adaptable-and-privacy-aware-image-classification-sebastian-doerrich-et-al-2024>(16/45 | 224/346) Integrating kNN with Foundation Models for Adaptable and Privacy-Aware Image Classification (Sebastian Doerrich et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sebastian Doerrich, Tobias Archut, Francesco Di Salvo, Christian Ledig. (2024)<br><strong>Integrating kNN with Foundation Models for Adaptable and Privacy-Aware Image Classification</strong><br><button class=copy-to-clipboard title="Integrating kNN with Foundation Models for Adaptable and Privacy-Aware Image Classification" index=224>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-224 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV, eess-IV<br>Keyword Score: 33<br>Keywords: Benchmarking, Continual Learning, Foundation Model, Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12500v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12500v1.pdf filename=2402.12500v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Traditional deep learning models implicity encode knowledge limiting their transparency and ability to adapt to data changes. Yet, this adaptability is vital for addressing user data privacy concerns. We address this limitation by storing embeddings of the underlying training data independently of the model weights, enabling dynamic data modifications without retraining. Specifically, our approach integrates the $k$-Nearest Neighbor ($k$-NN) classifier with a vision-based <b>foundation</b> <b>model,</b> pre-trained <b>self-supervised</b> on natural images, enhancing interpretability and adaptability. We share open-source implementations of a previously unpublished baseline method as well as our performance-improving contributions. Quantitative experiments confirm improved classification across established <b>benchmark</b> datasets and the method&rsquo;s applicability to distinct medical image classification tasks. Additionally, we assess the method&rsquo;s robustness in <b>continual</b> <b>learning</b> and data removal scenarios. The approach exhibits great promise for bridging the gap between <b>foundation</b> <b>models&rsquo;</b> performance and challenges tied to data privacy. The source code is available at <a href=https://github.com/TobArc/privacy-aware-image-classification-with-kNN>https://github.com/TobArc/privacy-aware-image-classification-with-kNN</a>.</p></p class="citation"></blockquote><h3 id=1745--225346-unlearncanvas-a-stylized-image-dataset-to-benchmark-machine-unlearning-for-diffusion-models-yihua-zhang-et-al-2024>(17/45 | 225/346) UnlearnCanvas: A Stylized Image Dataset to Benchmark Machine Unlearning for Diffusion Models (Yihua Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yihua Zhang, Yimeng Zhang, Yuguang Yao, Jinghan Jia, Jiancheng Liu, Xiaoming Liu, Sijia Liu. (2024)<br><strong>UnlearnCanvas: A Stylized Image Dataset to Benchmark Machine Unlearning for Diffusion Models</strong><br><button class=copy-to-clipboard title="UnlearnCanvas: A Stylized Image Dataset to Benchmark Machine Unlearning for Diffusion Models" index=225>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-225 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 33<br>Keywords: Diffusion Model, Benchmarking, Machine Unlearning, Style Transfer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11846v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11846v1.pdf filename=2402.11846v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The rapid advancement of <b>diffusion</b> <b>models</b> (DMs) has not only transformed various real-world industries but has also introduced negative societal concerns, including the generation of harmful content, copyright disputes, and the rise of stereotypes and biases. To mitigate these issues, <b>machine</b> <b>unlearning</b> (MU) has emerged as a potential solution, demonstrating its ability to remove undesired generative capabilities of DMs in various applications. However, by examining existing MU evaluation methods, we uncover several key challenges that can result in incomplete, inaccurate, or biased evaluations for MU in DMs. To address them, we enhance the evaluation metrics for MU, including the introduction of an often-overlooked retainability measurement for DMs post-unlearning. Additionally, we introduce UnlearnCanvas, a comprehensive high-resolution stylized image dataset that facilitates us to evaluate the unlearning of artistic painting <b>styles</b> <b>in</b> conjunction with associated image objects. We show that this dataset plays a pivotal role in establishing a standardized and automated evaluation framework for MU techniques on DMs, featuring 7 quantitative metrics to address various aspects of unlearning effectiveness. Through extensive experiments, we <b>benchmark</b> 5 state-of-the-art MU methods, revealing novel insights into their pros and cons, and the underlying unlearning mechanisms. Furthermore, we demonstrate the potential of UnlearnCanvas to <b>benchmark</b> other generative modeling tasks, such as <b>style</b> <b>transfer.</b> The UnlearnCanvas dataset, <b>benchmark,</b> and the codes to reproduce all the results in this work can be found at <a href=https://github.com/OPTML-Group/UnlearnCanvas>https://github.com/OPTML-Group/UnlearnCanvas</a>.</p></p class="citation"></blockquote><h3 id=1845--226346-multilinear-mixture-of-experts-scalable-expert-specialization-through-factorization-james-oldfield-et-al-2024>(18/45 | 226/346) Multilinear Mixture of Experts: Scalable Expert Specialization through Factorization (James Oldfield et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>James Oldfield, Markos Georgopoulos, Grigorios G. Chrysos, Christos Tzelepis, Yannis Panagakis, Mihalis A. Nicolaou, Jiankang Deng, Ioannis Patras. (2024)<br><strong>Multilinear Mixture of Experts: Scalable Expert Specialization through Factorization</strong><br><button class=copy-to-clipboard title="Multilinear Mixture of Experts: Scalable Expert Specialization through Factorization" index=226>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-226 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 30<br>Keywords: Counter-factual, Fine-tuning, Foundation Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12550v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12550v1.pdf filename=2402.12550v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The Mixture of Experts (MoE) paradigm provides a powerful way to decompose inscrutable dense layers into smaller, modular computations often more amenable to human interpretation, debugging, and editability. A major problem however lies in the computational cost of scaling the number of experts to achieve sufficiently fine-grained specialization. In this paper, we propose the Multilinear Mixutre of Experts (MMoE) layer to address this, focusing on vision models. MMoE layers perform an implicit computation on prohibitively large weight tensors entirely in factorized form. Consequently, MMoEs both (1) avoid the issues incurred through the discrete expert routing in the popular &lsquo;sparse&rsquo; MoE models, yet (2) do not incur the restrictively high inference-time costs of &lsquo;soft&rsquo; MoE alternatives. We present both qualitative and quantitative evidence (through visualization and <b>counterfactual</b> interventions respectively) that scaling MMoE layers when <b>fine-tuning</b> <b>foundation</b> <b>models</b> for vision tasks leads to more specialized experts at the class-level whilst remaining competitive with the performance of parameter-matched linear layer counterparts. Finally, we show that learned expert specialism further facilitates manual correction of demographic bias in CelebA attribute classification. Our MMoE model code is available at <a href=https://github.com/james-oldfield/MMoE>https://github.com/james-oldfield/MMoE</a>.</p></p class="citation"></blockquote><h3 id=1945--227346-system-identification-of-neural-systems-going-beyond-images-to-modelling-dynamics-mai-gamal-et-al-2024>(19/45 | 227/346) System Identification of Neural Systems: Going Beyond Images to Modelling Dynamics (Mai Gamal et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mai Gamal, Mohamed Rashad, Eman Ehab, Seif Eldawlatly, Mennatullah Siam. (2024)<br><strong>System Identification of Neural Systems: Going Beyond Images to Modelling Dynamics</strong><br><button class=copy-to-clipboard title="System Identification of Neural Systems: Going Beyond Images to Modelling Dynamics" index=227>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-227 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Convolution, Self-supervised Learning, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12519v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12519v1.pdf filename=2402.12519v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Vast literature has compared the recordings of biological neurons in the brain to deep neural networks. The ultimate goal is to interpret deep networks or to better understand and encode biological neural systems. Recently, there has been a debate on whether system identification is possible and how much it can tell us about the brain computation. System identification recognizes whether one model is more valid to represent the brain computation over another. Nonetheless, previous work did not consider the time aspect and how video and dynamics (e.g., motion) modelling in deep networks relate to these biological neural systems within a large-scale comparison. Towards this end, we propose a system identification study focused on comparing single image vs. video understanding models with respect to the visual cortex recordings. Our study encompasses two sets of experiments; a real environment setup and a simulated environment setup. The study also encompasses more than 30 models and, unlike prior works, we focus on <b>convolutional</b> vs. <b>transformer-based,</b> single vs. two-stream, and fully vs. <b>self-supervised</b> video understanding models. The goal is to capture a greater variety of architectures that model dynamics. As such, this signifies the first large-scale study of video understanding models from a neuroscience perspective. Our results in the simulated experiments, show that system identification can be attained to a certain level in differentiating image vs. video understanding models. Moreover, we provide key insights on how video understanding models predict visual cortex responses; showing video understanding better than image understanding models, <b>convolutional</b> models are better in the early-mid regions than <b>transformer</b> based except for multiscale <b>transformers</b> that are still good in predicting these regions, and that two-stream models are better than single stream.</p></p class="citation"></blockquote><h3 id=2045--228346-3d-vascular-segmentation-supervised-by-2d-annotation-of-maximum-intensity-projection-zhanqiang-guo-et-al-2024>(20/45 | 228/346) 3D Vascular Segmentation Supervised by 2D Annotation of Maximum Intensity Projection (Zhanqiang Guo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhanqiang Guo, Zimeng Tan, Jianjiang Feng, Jie Zhou. (2024)<br><strong>3D Vascular Segmentation Supervised by 2D Annotation of Maximum Intensity Projection</strong><br><button class=copy-to-clipboard title="3D Vascular Segmentation Supervised by 2D Annotation of Maximum Intensity Projection" index=228>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-228 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Fine-tuning, Supervised Learning, Weakly-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12128v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12128v1.pdf filename=2402.12128v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Vascular structure segmentation plays a crucial role in medical analysis and clinical applications. The practical adoption of fully <b>supervised</b> segmentation models is impeded by the intricacy and time-consuming nature of annotating vessels in the 3D space. This has spurred the exploration of <b>weakly-supervised</b> approaches that reduce reliance on expensive segmentation annotations. Despite this, existing weakly <b>supervised</b> methods employed in organ segmentation, which encompass points, bounding boxes, or graffiti, have exhibited suboptimal performance when handling sparse vascular structure. To alleviate this issue, we employ maximum intensity projection (MIP) to decrease the dimensionality of 3D volume to 2D image for efficient annotation, and the 2D labels are utilized to provide guidance and oversight for training 3D vessel segmentation model. Initially, we generate pseudo-labels for 3D blood vessels using the annotations of 2D projections. Subsequently, taking into account the acquisition method of the 2D labels, we introduce a <b>weakly-supervised</b> network that fuses 2D-3D deep features via MIP to further improve segmentation performance. Furthermore, we integrate confidence learning and uncertainty estimation to refine the generated pseudo-labels, followed by <b>fine-tuning</b> the segmentation network. Our method is validated on five datasets (including cerebral vessel, aorta and coronary artery), demonstrating highly competitive performance in segmenting vessels and the potential to significantly reduce the time and effort required for vessel annotation. Our code is available at: <a href=https://github.com/gzq17/Weakly-Supervised-by-MIP>https://github.com/gzq17/Weakly-Supervised-by-MIP</a>.</p></p class="citation"></blockquote><h3 id=2145--229346-iscute-instance-segmentation-of-cables-using-text-embedding-shir-kozlovsky-et-al-2024>(21/45 | 229/346) ISCUTE: Instance Segmentation of Cables Using Text Embedding (Shir Kozlovsky et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shir Kozlovsky, Omkar Joglekar, Dotan Di Castro. (2024)<br><strong>ISCUTE: Instance Segmentation of Cables Using Text Embedding</strong><br><button class=copy-to-clipboard title="ISCUTE: Instance Segmentation of Cables Using Text Embedding" index=229>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-229 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 30<br>Keywords: Foundation Model, Zero-shot, Text Embedding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11996v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11996v1.pdf filename=2402.11996v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the field of robotics and automation, conventional object recognition and instance segmentation methods face a formidable challenge when it comes to perceiving Deformable Linear Objects (DLOs) like wires, cables, and flexible tubes. This challenge arises primarily from the lack of distinct attributes such as shape, color, and texture, which calls for tailored solutions to achieve precise identification. In this work, we propose a <b>foundation</b> <b>model-based</b> DLO instance segmentation technique that is <b>text-promptable</b> <b>and</b> user-friendly. Specifically, our approach combines the <b>text-conditioned</b> <b>semantic</b> segmentation capabilities of CLIPSeg model with the <b>zero-shot</b> generalization capabilities of Segment Anything Model (SAM). We show that our method exceeds SOTA performance on DLO instance segmentation, achieving a mIoU of $91.21%$. We also introduce a rich and diverse DLO-specific dataset for instance segmentation.</p></p class="citation"></blockquote><h3 id=2245--230346-inmd-x-large-language-models-for-internal-medicine-doctors-hansle-gwon-et-al-2024>(22/45 | 230/346) InMD-X: Large Language Models for Internal Medicine Doctors (Hansle Gwon et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hansle Gwon, Imjin Ahn, Hyoje Jung, Byeolhee Kim, Young-Hak Kim, Tae Joon Jun. (2024)<br><strong>InMD-X: Large Language Models for Internal Medicine Doctors</strong><br><button class=copy-to-clipboard title="InMD-X: Large Language Models for Internal Medicine Doctors" index=230>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-230 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Fine-tuning, Text Analysis, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11883v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11883v2.pdf filename=2402.11883v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we introduce InMD-X, a collection of multiple <b>large</b> <b>language</b> <b>models</b> specifically designed to cater to the unique characteristics and demands of Internal Medicine Doctors (IMD). InMD-X represents a groundbreaking development in natural language processing, offering a suite of language models <b>fine-tuned</b> for various aspects of the internal medicine field. These models encompass a wide range of medical sub-specialties, enabling IMDs to perform more efficient and accurate research, diagnosis, and documentation. InMD-X&rsquo;s versatility and adaptability make it a valuable tool for improving the healthcare industry, enhancing communication between healthcare professionals, and advancing medical research. Each model within InMD-X is meticulously tailored to address specific challenges faced by IMDs, ensuring the highest level of precision and comprehensiveness in clinical <b>text</b> <b>analysis</b> and decision support. This paper provides an overview of the design, development, and evaluation of InMD-X, showcasing its potential to revolutionize the way internal medicine practitioners interact with medical data and information. We present results from extensive testing, demonstrating the effectiveness and practical utility of InMD-X in real-world medical scenarios.</p></p class="citation"></blockquote><h3 id=2345--231346-sdge-stereo-guided-depth-estimation-for-360-camera-sets-jialei-xu-et-al-2024>(23/45 | 231/346) SDGE: Stereo Guided Depth Estimation for 360° Camera Sets (Jialei Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jialei Xu, Xianming Liu, Junjun Jiang, Xiangyang Ji. (2024)<br><strong>SDGE: Stereo Guided Depth Estimation for 360° Camera Sets</strong><br><button class=copy-to-clipboard title="SDGE: Stereo Guided Depth Estimation for 360° Camera Sets" index=231>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-231 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Object Detection, Self-supervised Learning, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11791v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11791v1.pdf filename=2402.11791v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Depth estimation is a critical technology in autonomous driving, and multi-camera systems are often used to achieve a 360{\deg} perception. These 360{\deg} camera sets often have limited or low-quality overlap regions, making multi-view stereo methods infeasible for the entire image. Alternatively, monocular methods may not produce consistent cross-view predictions. To address these issues, we propose the Stereo Guided Depth Estimation (SGDE) method, which enhances depth estimation of the full image by explicitly utilizing multi-view stereo results on the overlap. We suggest building virtual pinhole cameras to resolve the distortion problem of fisheye cameras and unify the processing for the two types of 360{\deg} cameras. For handling the varying noise on camera poses caused by unstable movement, the approach employs a self-calibration method to obtain highly accurate relative poses of the adjacent cameras with minor overlap. These enable the use of robust stereo methods to obtain high-quality depth prior in the overlap region. This prior serves not only as an additional input but also as pseudo-labels that enhance the accuracy of depth estimation methods and improve cross-view prediction consistency. The effectiveness of SGDE is evaluated on one fisheye camera dataset, Synthetic Urban, and two pinhole camera datasets, DDAD and nuScenes. Our experiments demonstrate that SGDE is effective for both <b>supervised</b> and <b>self-supervised</b> depth estimation, and highlight the potential of our method for advancing downstream autonomous driving technologies, such as 3D <b>object</b> <b>detection</b> and occupancy prediction.</p></p class="citation"></blockquote><h3 id=2445--232346-separating-common-from-salient-patterns-with-contrastive-representation-learning-robin-louiset-et-al-2024>(24/45 | 232/346) Separating common from salient patterns with Contrastive Representation Learning (Robin Louiset et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Robin Louiset, Edouard Duchesnay, Antoine Grigis, Pietro Gori. (2024)<br><strong>Separating common from salient patterns with Contrastive Representation Learning</strong><br><button class=copy-to-clipboard title="Separating common from salient patterns with Contrastive Representation Learning" index=232>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-232 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 28<br>Keywords: Clustering, Contrastive Learning, Mutual Information, Representation Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11928v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11928v1.pdf filename=2402.11928v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Contrastive</b> <b>Analysis</b> is a sub-field of <b>Representation</b> <b>Learning</b> that aims at separating common factors of variation between two datasets, a background (i.e., healthy subjects) and a target (i.e., diseased subjects), from the salient factors of variation, only present in the target dataset. Despite their relevance, current models based on Variational Auto-Encoders have shown poor performance in learning semantically-expressive <b>representations.</b> <b>On</b> the other hand, <b>Contrastive</b> <b>Representation</b> <b>Learning</b> has shown tremendous performance leaps in various applications (classification, <b>clustering,</b> etc.). In this work, we propose to leverage the ability of <b>Contrastive</b> <b>Learning</b> to learn semantically expressive <b>representations</b> <b>well</b> adapted for <b>Contrastive</b> <b>Analysis.</b> We reformulate it under the lens of the InfoMax Principle and identify two <b>Mutual</b> <b>Information</b> terms to maximize and one to minimize. We decompose the first two terms into an Alignment and a Uniformity term, as commonly done in <b>Contrastive</b> <b>Learning.</b> Then, we motivate a novel <b>Mutual</b> <b>Information</b> minimization strategy to prevent information leakage between common and salient distributions. We validate our method, called SepCLR, on three visual datasets and three medical datasets, specifically conceived to assess the pattern separation capability in <b>Contrastive</b> <b>Analysis.</b> Code available at <a href=https://github.com/neurospin-projects/2024_rlouiset_sep_clr>https://github.com/neurospin-projects/2024_rlouiset_sep_clr</a>.</p></p class="citation"></blockquote><h3 id=2545--233346-dilightnet-fine-grained-lighting-control-for-diffusion-based-image-generation-chong-zeng-et-al-2024>(25/45 | 233/346) DiLightNet: Fine-grained Lighting Control for Diffusion-based Image Generation (Chong Zeng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chong Zeng, Yue Dong, Pieter Peers, Youkang Kong, Hongzhi Wu, Xin Tong. (2024)<br><strong>DiLightNet: Fine-grained Lighting Control for Diffusion-based Image Generation</strong><br><button class=copy-to-clipboard title="DiLightNet: Fine-grained Lighting Control for Diffusion-based Image Generation" index=233>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-233 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-GR, cs.CV<br>Keyword Score: 25<br>Keywords: Diffusion Model, Geometry, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11929v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11929v1.pdf filename=2402.11929v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents a novel method for exerting fine-grained lighting control during text-driven <b>diffusion-based</b> <b>image</b> generation. While existing <b>diffusion</b> <b>models</b> already have the ability to generate images under any lighting condition, without additional guidance these models tend to correlate image content and lighting. Moreover, text <b>prompts</b> lack the necessary expressional power to describe detailed lighting setups. To provide the content creator with fine-grained control over the lighting during image generation, we augment the text-prompt with detailed lighting information in the form of radiance hints, i.e., visualizations of the scene <b>geometry</b> with a homogeneous canonical material under the target lighting. However, the scene <b>geometry</b> needed to produce the radiance hints is unknown. Our key observation is that we only need to guide the <b>diffusion</b> <b>process,</b> hence exact radiance hints are not necessary; we only need to point the <b>diffusion</b> <b>model</b> in the right direction. Based on this observation, we introduce a three stage method for controlling the lighting during image generation. In the first stage, we leverage a standard pretrained <b>diffusion</b> <b>model</b> to generate a provisional image under uncontrolled lighting. Next, in the second stage, we resynthesize and refine the foreground object in the generated image by passing the target lighting to a refined <b>diffusion</b> <b>model,</b> named DiLightNet, using radiance hints computed on a coarse shape of the foreground object inferred from the provisional image. To retain the texture details, we multiply the radiance hints with a neural encoding of the provisional synthesized image before passing it to DiLightNet. Finally, in the third stage, we resynthesize the background to be consistent with the lighting on the foreground object. We demonstrate and validate our lighting controlled <b>diffusion</b> <b>model</b> on a variety of text <b>prompts</b> and lighting conditions.</p></p class="citation"></blockquote><h3 id=2645--234346-feudal-networks-for-visual-navigation-faith-johnson-et-al-2024>(26/45 | 234/346) Feudal Networks for Visual Navigation (Faith Johnson et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Faith Johnson, Bryan Bo Cao, Kristin Dana, Shubham Jain, Ashwin Ashok. (2024)<br><strong>Feudal Networks for Visual Navigation</strong><br><button class=copy-to-clipboard title="Feudal Networks for Visual Navigation" index=234>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-234 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs-RO, cs.CV<br>Keyword Score: 23<br>Keywords: Graph, Self-supervised Learning, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12498v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12498v1.pdf filename=2402.12498v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Visual navigation follows the intuition that humans can navigate without detailed maps. A common approach is interactive exploration while building a topological <b>graph</b> with images at nodes that can be used for planning. Recent variations learn from passive videos and can navigate using complex social and semantic cues. However, a significant number of training videos are needed, large <b>graphs</b> are utilized, and scenes are not unseen since odometry is utilized. We introduce a new approach to visual navigation using feudal learning, which employs a hierarchical structure consisting of a worker agent, a mid-level manager, and a high-level manager. Key to the feudal learning paradigm, agents at each level see a different aspect of the task and operate at different spatial and temporal scales. Two unique modules are developed in this framework. For the high-level manager, we learn a memory proxy map in a self <b>supervised</b> manner to record prior observations in a learned latent space and avoid the use of <b>graphs</b> and odometry. For the mid-level manager, we develop a waypoint network that outputs intermediate subgoals imitating human waypoint selection during local navigation. This waypoint network is pre-trained using a new, small set of teleoperation videos that we make publicly available, with training environments different from testing environments. The resulting feudal navigation network achieves near SOTA performance, while providing a novel no-RL, no-graph, no-odometry, no-metric map approach to the image goal navigation task.</p></p class="citation"></blockquote><h3 id=2745--235346-avoiding-feature-suppression-in-contrastive-learning-learning-what-has-not-been-learned-before-jihai-zhang-et-al-2024>(27/45 | 235/346) Avoiding Feature Suppression in Contrastive Learning: Learning What Has Not Been Learned Before (Jihai Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jihai Zhang, Xiang Lan, Xiaoye Qu, Yu Cheng, Mengling Feng, Bryan Hooi. (2024)<br><strong>Avoiding Feature Suppression in Contrastive Learning: Learning What Has Not Been Learned Before</strong><br><button class=copy-to-clipboard title="Avoiding Feature Suppression in Contrastive Learning: Learning What Has Not Been Learned Before" index=235>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-235 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 23<br>Keywords: Benchmarking, Contrastive Learning, Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11816v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11816v1.pdf filename=2402.11816v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Self-Supervised</b> <b>contrastive</b> <b>learning</b> has emerged as a powerful method for obtaining high-quality representations from unlabeled data. However, feature suppression has recently been identified in standard <b>contrastive</b> <b>learning</b> ($e.g.$, SimCLR, CLIP): in a single end-to-end training stage, the <b>contrastive</b> <b>model</b> captures only parts of the shared information across contrasting views, while ignore the other potentially useful information. With feature suppression, <b>contrastive</b> <b>models</b> often fail to learn sufficient representations capable for various downstream tasks. To mitigate the feature suppression problem and ensure the <b>contrastive</b> <b>model</b> to learn comprehensive representations, we develop a novel Multistage <b>Contrastive</b> <b>Learning</b> (MCL) framework. Unlike standard <b>contrastive</b> <b>learning</b> that often result in feature suppression, MCL progressively learn new features that have not been explored in the previous stage, while maintaining the well-learned features. Extensive experiments conducted on various publicly available <b>benchmarks</b> validate the effectiveness of our proposed framework. In addition, we demonstrate that the proposed MCL can be adapted to a variety of popular <b>contrastive</b> <b>learning</b> backbones and boost their performance by learning features that could not be gained from standard <b>contrastive</b> <b>learning</b> procedures.</p></p class="citation"></blockquote><h3 id=2845--236346-langxai-integrating-large-vision-models-for-generating-textual-explanations-to-enhance-explainability-in-visual-perception-tasks-truong-thanh-hung-nguyen-et-al-2024>(28/45 | 236/346) LangXAI: Integrating Large Vision Models for Generating Textual Explanations to Enhance Explainability in Visual Perception Tasks (Truong Thanh Hung Nguyen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Truong Thanh Hung Nguyen, Tobias Clement, Phuc Truong Loc Nguyen, Nils Kemmerzell, Van Binh Truong, Vo Thanh Khang Nguyen, Mohamed Abdelaal, Hung Cao. (2024)<br><strong>LangXAI: Integrating Large Vision Models for Generating Textual Explanations to Enhance Explainability in Visual Perception Tasks</strong><br><button class=copy-to-clipboard title="LangXAI: Integrating Large Vision Models for Generating Textual Explanations to Enhance Explainability in Visual Perception Tasks" index=236>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-236 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Object Detection, BERTScore<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12525v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12525v1.pdf filename=2402.12525v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>LangXAI is a framework that integrates Explainable Artificial Intelligence (XAI) with advanced vision models to generate textual explanations for visual recognition tasks. Despite XAI advancements, an understanding gap persists for end-users with limited domain knowledge in artificial intelligence and computer vision. LangXAI addresses this by furnishing text-based explanations for classification, <b>object</b> <b>detection,</b> and semantic segmentation model outputs to end-users. Preliminary results demonstrate LangXAI&rsquo;s enhanced plausibility, with high <b>BERTScore</b> across tasks, fostering a more transparent and reliable AI framework on vision tasks for end-users.</p></p class="citation"></blockquote><h3 id=2945--237346-drivevlm-the-convergence-of-autonomous-driving-and-large-vision-language-models-xiaoyu-tian-et-al-2024>(29/45 | 237/346) DriveVLM: The Convergence of Autonomous Driving and Large Vision-Language Models (Xiaoyu Tian et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiaoyu Tian, Junru Gu, Bailin Li, Yicheng Liu, Chenxu Hu, Yang Wang, Kun Zhan, Peng Jia, Xianpeng Lang, Hang Zhao. (2024)<br><strong>DriveVLM: The Convergence of Autonomous Driving and Large Vision-Language Models</strong><br><button class=copy-to-clipboard title="DriveVLM: The Convergence of Autonomous Driving and Large Vision-Language Models" index=237>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-237 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Reasoning, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12289v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12289v1.pdf filename=2402.12289v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A primary hurdle of autonomous driving in urban environments is understanding complex and long-tail scenarios, such as challenging road conditions and delicate human behaviors. We introduce DriveVLM, an autonomous driving system leveraging <b>Vision-Language</b> Models (VLMs) for enhanced scene understanding and planning capabilities. DriveVLM integrates a unique combination of chain-of-thought (CoT) modules for scene description, scene analysis, and hierarchical planning. Furthermore, recognizing the limitations of VLMs in spatial <b>reasoning</b> and heavy computational requirements, we propose DriveVLM-Dual, a hybrid system that synergizes the strengths of DriveVLM with the traditional autonomous driving pipeline. DriveVLM-Dual achieves robust spatial understanding and real-time inference speed. Extensive experiments on both the nuScenes dataset and our SUP-AD dataset demonstrate the effectiveness of DriveVLM and the enhanced performance of DriveVLM-Dual, surpassing existing methods in complex and unpredictable driving conditions.</p></p class="citation"></blockquote><h3 id=3045--238346-human-video-translation-via-query-warping-haiming-zhu-et-al-2024>(30/45 | 238/346) Human Video Translation via Query Warping (Haiming Zhu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haiming Zhu, Yangyang Xu, Shengfeng He. (2024)<br><strong>Human Video Translation via Query Warping</strong><br><button class=copy-to-clipboard title="Human Video Translation via Query Warping" index=238>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-238 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Diffusion Model, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12099v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12099v1.pdf filename=2402.12099v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we present QueryWarp, a novel framework for temporally coherent human motion video translation. Existing <b>diffusion-based</b> <b>video</b> editing approaches that rely solely on key and value tokens to ensure temporal consistency, which scarifies the preservation of local and structural regions. In contrast, we aim to consider complementary query priors by constructing the temporal correlations among query tokens from different frames. Initially, we extract appearance flows from source poses to capture continuous human foreground motion. Subsequently, during the denoising process of the <b>diffusion</b> <b>model,</b> we employ appearance flows to warp the previous frame&rsquo;s query token, aligning it with the current frame&rsquo;s query. This query warping imposes explicit constraints on the outputs of <b>self-attention</b> layers, effectively guaranteeing temporally coherent translation. We perform experiments on various human motion video translation tasks, and the results demonstrate that our QueryWarp framework surpasses state-of-the-art methods both qualitatively and quantitatively.</p></p class="citation"></blockquote><h3 id=3145--239346-surround-view-fisheye-optics-in-computer-vision-and-simulation-survey-and-challenges-daniel-jakab-et-al-2024>(31/45 | 239/346) Surround-View Fisheye Optics in Computer Vision and Simulation: Survey and Challenges (Daniel Jakab et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Daniel Jakab, Brian Michael Deegan, Sushil Sharma, Eoin Martino Grua, Jonathan Horgan, Enda Ward, Pepijn Van De Ven, Anthony Scanlan, Ciarán Eising. (2024)<br><strong>Surround-View Fisheye Optics in Computer Vision and Simulation: Survey and Challenges</strong><br><button class=copy-to-clipboard title="Surround-View Fisheye Optics in Computer Vision and Simulation: Survey and Challenges" index=239>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-239 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV, eess-IV<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12041v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12041v2.pdf filename=2402.12041v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we provide a survey on automotive surround-view fisheye optics, with an emphasis on the impact of optical artifacts on computer vision tasks in autonomous driving and ADAS. The automotive industry has advanced in applying state-of-the-art computer vision to enhance road safety and provide automated driving functionality. When using camera systems on vehicles, there is a particular need for a wide field of view to capture the entire vehicle&rsquo;s surroundings, in areas such as low-speed maneuvering, automated parking, and cocoon sensing. However, one crucial challenge in surround-view cameras is the strong optical aberrations of the fisheye camera, which is an area that has received little attention in the literature. Additionally, a comprehensive dataset is needed for testing safety-critical scenarios in vehicle automation. The industry has turned to <b>simulation</b> as a cost-effective strategy for creating synthetic datasets with surround-view camera imagery. We examine different <b>simulation</b> methods (such as model-driven and data-driven <b>simulations)</b> and discuss the simulators&rsquo; ability (or lack thereof) to model real-world optical performance. Overall, this paper highlights the optical aberrations in automotive fisheye datasets, and the limitations of optical reality in simulated fisheye datasets, with a focus on computer vision in surround-view optical systems.</p></p class="citation"></blockquote><h3 id=3245--240346-language-guided-image-reflection-separation-haofeng-zhong-et-al-2024>(32/45 | 240/346) Language-guided Image Reflection Separation (Haofeng Zhong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haofeng Zhong, Yuchen Hong, Shuchen Weng, Jinxiu Liang, Boxin Shi. (2024)<br><strong>Language-guided Image Reflection Separation</strong><br><button class=copy-to-clipboard title="Language-guided Image Reflection Separation" index=240>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-240 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Graph Attention Networks, Contrastive Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11874v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11874v1.pdf filename=2402.11874v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper studies the problem of language-guided reflection separation, which aims at addressing the ill-posed reflection separation problem by introducing language descriptions to provide layer content. We propose a unified framework to solve this problem, which leverages the cross-attention mechanism with <b>contrastive</b> <b>learning</b> strategies to construct the correspondence between language descriptions and image layers. A <b>gated</b> network design and a randomized training strategy are employed to tackle the recognizable layer ambiguity. The effectiveness of the proposed method is validated by the significant performance advantage over existing reflection separation methods on both quantitative and qualitative comparisons.</p></p class="citation"></blockquote><h3 id=3345--241346-comfusion-personalized-subject-generation-in-multiple-specific-scenes-from-single-image-yan-hong-et-al-2024>(33/45 | 241/346) ComFusion: Personalized Subject Generation in Multiple Specific Scenes From Single Image (Yan Hong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yan Hong, Jianfu Zhang. (2024)<br><strong>ComFusion: Personalized Subject Generation in Multiple Specific Scenes From Single Image</strong><br><button class=copy-to-clipboard title="ComFusion: Personalized Subject Generation in Multiple Specific Scenes From Single Image" index=241>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-241 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Diffusion Model, Text2image<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11849v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11849v1.pdf filename=2402.11849v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advancements in personalizing <b>text-to-image</b> (T2I) <b>diffusion</b> <b>models</b> have shown the capability to generate images based on personalized visual concepts using a limited number of user-provided examples. However, these models often struggle with maintaining high visual fidelity, particularly in manipulating scenes as defined by textual inputs. Addressing this, we introduce ComFusion, a novel approach that leverages pretrained models generating composition of a few user-provided subject images and predefined-text scenes, effectively fusing visual-subject instances with textual-specific scenes, resulting in the generation of high-fidelity instances within diverse scenes. ComFusion integrates a class-scene prior preservation regularization, which leverages composites the subject class and scene-specific knowledge from pretrained models to enhance generation fidelity. Additionally, ComFusion uses coarse generated images, ensuring they align effectively with both the instance image and scene texts. Consequently, ComFusion maintains a delicate balance between capturing the essence of the subject and maintaining scene fidelity.Extensive evaluations of ComFusion against various baselines in T2I personalization have demonstrated its qualitative and quantitative superiority.</p></p class="citation"></blockquote><h3 id=3445--242346-wildfake-a-large-scale-challenging-dataset-for-ai-generated-images-detection-yan-hong-et-al-2024>(34/45 | 242/346) WildFake: A Large-scale Challenging Dataset for AI-Generated Images Detection (Yan Hong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yan Hong, Jianfu Zhang. (2024)<br><strong>WildFake: A Large-scale Challenging Dataset for AI-Generated Images Detection</strong><br><button class=copy-to-clipboard title="WildFake: A Large-scale Challenging Dataset for AI-Generated Images Detection" index=242>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-242 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Diffusion Model, Generative Adversarial Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11843v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11843v1.pdf filename=2402.11843v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The extraordinary ability of generative models enabled the generation of images with such high quality that human beings cannot distinguish Artificial Intelligence (AI) generated images from real-life photographs. The development of generation techniques opened up new opportunities but concurrently introduced potential risks to privacy, authenticity, and security. Therefore, the task of detecting AI-generated imagery is of paramount importance to prevent illegal activities. To assess the generalizability and robustness of AI-generated image detection, we present a large-scale dataset, referred to as WildFake, comprising state-of-the-art generators, diverse object categories, and real-world applications. WildFake dataset has the following advantages: 1) Rich Content with Wild collection: WildFake collects fake images from the open-source community, enriching its diversity with a broad range of image classes and image styles. 2) Hierarchical structure: WildFake contains fake images synthesized by different types of generators from <b>GANs,</b> <b>diffusion</b> <b>models,</b> to other generative models. These key strengths enhance the generalization and robustness of detectors trained on WildFake, thereby demonstrating WildFake&rsquo;s considerable relevance and effectiveness for AI-generated detectors in real-world scenarios. Moreover, our extensive evaluation experiments are tailored to yield profound insights into the capabilities of different levels of generative models, a distinctive advantage afforded by WildFake&rsquo;s unique hierarchical structure.</p></p class="citation"></blockquote><h3 id=3545--243346-aicattack-adversarial-image-captioning-attack-with-attention-based-optimization-jiyao-li-et-al-2024>(35/45 | 243/346) AICAttack: Adversarial Image Captioning Attack with Attention-Based Optimization (Jiyao Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiyao Li, Mingze Ni, Yifei Dong, Tianqing Zhu, Wei Liu. (2024)<br><strong>AICAttack: Adversarial Image Captioning Attack with Attention-Based Optimization</strong><br><button class=copy-to-clipboard title="AICAttack: Adversarial Image Captioning Attack with Attention-Based Optimization" index=243>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-243 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CR, cs-CV, cs-LG, cs.CV<br>Keyword Score: 18<br>Keywords: Benchmarking, Black Box, Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11940v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11940v2.pdf filename=2402.11940v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advances in deep learning research have shown remarkable achievements across many tasks in computer vision (CV) and natural language processing (NLP). At the intersection of CV and NLP is the problem of image captioning, where the related models&rsquo; robustness against <b>adversarial</b> <b>attacks</b> has not been well studied. In this paper, we present a novel <b>adversarial</b> <b>attack</b> strategy, which we call AICAttack (Attention-based Image Captioning Attack), designed to attack image captioning models through subtle perturbations on images. Operating within a <b>black-box</b> <b>attack</b> scenario, our algorithm requires no access to the target model&rsquo;s architecture, parameters, or gradient information. We introduce an attention-based candidate selection mechanism that identifies the optimal pixels to attack, followed by Differential Evolution (DE) for perturbing pixels&rsquo; RGB values. We demonstrate AICAttack&rsquo;s effectiveness through extensive experiments on <b>benchmark</b> datasets with multiple victim models. The experimental results demonstrate that our method surpasses current leading-edge techniques by effectively distributing the alignment and semantics of words in the output.</p></p class="citation"></blockquote><h3 id=3645--244346-interpretable-embedding-for-ad-hoc-video-search-jiaxin-wu-et-al-2024>(36/45 | 244/346) Interpretable Embedding for Ad-hoc Video Search (Jiaxin Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiaxin Wu, Chong-Wah Ngo. (2024)<br><strong>Interpretable Embedding for Ad-hoc Video Search</strong><br><button class=copy-to-clipboard title="Interpretable Embedding for Ad-hoc Video Search" index=244>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-244 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-MM, cs.CV<br>Keyword Score: 13<br>Keywords: Benchmarking, Pruning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11812v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11812v1.pdf filename=2402.11812v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Answering query with semantic concepts has long been the mainstream approach for video search. Until recently, its performance is surpassed by concept-free approach, which embeds queries in a joint space as videos. Nevertheless, the embedded features as well as search results are not interpretable, hindering subsequent steps in video browsing and query reformulation. This paper integrates feature embedding and concept interpretation into a neural network for unified dual-task learning. In this way, an embedding is associated with a list of semantic concepts as an interpretation of video content. This paper empirically demonstrates that, by using either the embedding features or concepts, considerable search improvement is attainable on TRECVid <b>benchmarked</b> datasets. Concepts are not only effective in <b>pruning</b> false positive videos, but also highly complementary to concept-free search, leading to large margin of improvement compared to state-of-the-art approaches.</p></p class="citation"></blockquote><h3 id=3745--245346-landmark-based-localization-using-stereo-vision-and-deep-learning-in-gps-denied-battlefield-environment-ganesh-sapkota-et-al-2024>(37/45 | 245/346) Landmark-based Localization using Stereo Vision and Deep Learning in GPS-Denied Battlefield Environment (Ganesh Sapkota et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ganesh Sapkota, Sanjay Madria. (2024)<br><strong>Landmark-based Localization using Stereo Vision and Deep Learning in GPS-Denied Battlefield Environment</strong><br><button class=copy-to-clipboard title="Landmark-based Localization using Stereo Vision and Deep Learning in GPS-Denied Battlefield Environment" index=245>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-245 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Fine-tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12551v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12551v1.pdf filename=2402.12551v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Localization in a battlefield environment is increasingly challenging as GPS connectivity is often denied or unreliable, and physical deployment of anchor nodes across wireless networks for localization can be difficult in hostile battlefield terrain. Existing range-free localization methods rely on radio-based anchors and their average hop distance which suffers from accuracy and stability in dynamic and sparse wireless network topology. Vision-based methods like SLAM and Visual Odometry use expensive sensor fusion techniques for map generation and pose estimation. This paper proposes a novel framework for localization in non-GPS battlefield environments using only the passive camera sensors and considering naturally existing or artificial landmarks as anchors. The proposed method utilizes a customcalibrated stereo vision camera for distance estimation and the YOLOv8s model, which is trained and <b>fine-tuned</b> with our real-world dataset for landmark recognition. The depth images are generated using an efficient stereomatching algorithm, and distances to landmarks are determined by extracting the landmark depth feature utilizing a bounding box predicted by the landmark recognition model. The position of the unknown node is then obtained using the efficient least square algorithm and then optimized using the L-BFGS-B (limited-memory quasi-Newton code for bound-constrained optimization) method. Experimental results demonstrate that our proposed framework performs better than existing anchorbased DV-Hop algorithms and competes with the most efficient vision-based algorithms in terms of localization error (RMSE).</p></p class="citation"></blockquote><h3 id=3845--246346-uncertaintytrack-exploiting-detection-and-localization-uncertainty-in-multi-object-tracking-chang-won-lee-et-al-2024>(38/45 | 246/346) UncertaintyTrack: Exploiting Detection and Localization Uncertainty in Multi-Object Tracking (Chang Won Lee et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chang Won Lee, Steven L. Waslander. (2024)<br><strong>UncertaintyTrack: Exploiting Detection and Localization Uncertainty in Multi-Object Tracking</strong><br><button class=copy-to-clipboard title="UncertaintyTrack: Exploiting Detection and Localization Uncertainty in Multi-Object Tracking" index=246>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-246 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-RO, cs.CV<br>Keyword Score: 10<br>Keywords: Object Detection<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12303v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12303v1.pdf filename=2402.12303v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Multi-object tracking (MOT) methods have seen a significant boost in performance recently, due to strong interest from the research community and steadily improving <b>object</b> <b>detection</b> methods. The majority of tracking methods follow the tracking-by-detection (TBD) paradigm, blindly trust the incoming detections with no sense of their associated localization uncertainty. This lack of uncertainty awareness poses a problem in safety-critical tasks such as autonomous driving where passengers could be put at risk due to erroneous detections that have propagated to downstream tasks, including MOT. While there are existing works in probabilistic <b>object</b> <b>detection</b> that predict the localization uncertainty around the boxes, no work in 2D MOT for autonomous driving has studied whether these estimates are meaningful enough to be leveraged effectively in <b>object</b> <b>tracking.</b> We introduce UncertaintyTrack, a collection of extensions that can be applied to multiple TBD trackers to account for localization uncertainty estimates from probabilistic <b>object</b> <b>detectors.</b> Experiments on the Berkeley Deep Drive MOT dataset show that the combination of our method and informative uncertainty estimates reduces the number of ID switches by around 19% and improves mMOTA by 2-3%. The source code is available at <a href=https://github.com/TRAILab/UncertaintyTrack>https://github.com/TRAILab/UncertaintyTrack</a></p></p class="citation"></blockquote><h3 id=3945--247346-mixed-gaussian-flow-for-diverse-trajectory-prediction-jiahe-chen-et-al-2024>(39/45 | 247/346) Mixed Gaussian Flow for Diverse Trajectory Prediction (Jiahe Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiahe Chen, Jinkun Cao, Dahua Lin, Kris Kitani, Jiangmiao Pang. (2024)<br><strong>Mixed Gaussian Flow for Diverse Trajectory Prediction</strong><br><button class=copy-to-clipboard title="Mixed Gaussian Flow for Diverse Trajectory Prediction" index=247>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-247 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Out-of-distribution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12238v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12238v1.pdf filename=2402.12238v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Existing trajectory prediction studies intensively leverage generative models. Normalizing flow is one of the genres with the advantage of being invertible to derive the probability density of predicted trajectories. However, mapping from a standard Gaussian by a flow-based model hurts the capacity to capture complicated patterns of trajectories, ignoring the under-represented motion intentions in the training data. To solve the problem, we propose a flow-based model to transform a mixed Gaussian prior into the future trajectory manifold. The model shows a better capacity for generating diverse trajectory patterns. Also, by associating each sub-Gaussian with a certain subspace of trajectories, we can generate future trajectories with controllable motion intentions. In such a fashion, the flow-based model is not encouraged to simply seek the most likelihood of the intended manifold anymore but a family of controlled manifolds with explicit interpretability. Our proposed method is demonstrated to show state-of-the-art performance in the quantitative evaluation of sampling well-aligned trajectories in top-M generated candidates. We also demonstrate that it can generate diverse, controllable, and <b>out-of-distribution</b> trajectories. Code is available at <a href=https://github.com/mulplue/MGF>https://github.com/mulplue/MGF</a>.</p></p class="citation"></blockquote><h3 id=4045--248346-perceiving-longer-sequences-with-bi-directional-cross-attention-transformers-markus-hiller-et-al-2024>(40/45 | 248/346) Perceiving Longer Sequences With Bi-Directional Cross-Attention Transformers (Markus Hiller et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Markus Hiller, Krista A. Ehinger, Tom Drummond. (2024)<br><strong>Perceiving Longer Sequences With Bi-Directional Cross-Attention Transformers</strong><br><button class=copy-to-clipboard title="Perceiving Longer Sequences With Bi-Directional Cross-Attention Transformers" index=248>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-248 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12138v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12138v1.pdf filename=2402.12138v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present a novel bi-directional <b>Transformer</b> architecture (BiXT) which scales linearly with input size in terms of computational cost and memory consumption, but does not suffer the drop in performance or limitation to only one input modality seen with other efficient <b>Transformer-based</b> approaches. BiXT is inspired by the Perceiver architectures but replaces iterative attention with an efficient bi-directional cross-attention module in which input tokens and latent variables attend to each other simultaneously, leveraging a naturally emerging attention-symmetry between the two. This approach unlocks a key bottleneck experienced by Perceiver-like architectures and enables the processing and interpretation of both semantics (<code>what') and location (</code>where&rsquo;) to develop alongside each other over multiple layers &ndash; allowing its direct application to dense and instance-based tasks alike. By combining efficiency with the generality and performance of a full <b>Transformer</b> architecture, BiXT can process longer sequences like point clouds or images at higher feature resolutions and achieves competitive performance across a range of tasks like point cloud part segmentation, semantic image segmentation and image classification.</p></p class="citation"></blockquote><h3 id=4145--249346-one2avatar-generative-implicit-head-avatar-for-few-shot-user-adaptation-zhixuan-yu-et-al-2024>(41/45 | 249/346) One2Avatar: Generative Implicit Head Avatar For Few-shot User Adaptation (Zhixuan Yu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhixuan Yu, Ziqian Bai, Abhimitra Meka, Feitong Tan, Qiangeng Xu, Rohit Pandey, Sean Fanello, Hyun Soo Park, Yinda Zhang. (2024)<br><strong>One2Avatar: Generative Implicit Head Avatar For Few-shot User Adaptation</strong><br><button class=copy-to-clipboard title="One2Avatar: Generative Implicit Head Avatar For Few-shot User Adaptation" index=249>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-249 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Few-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11909v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11909v1.pdf filename=2402.11909v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Traditional methods for constructing high-quality, personalized head avatars from monocular videos demand extensive face captures and training time, posing a significant challenge for scalability. This paper introduces a novel approach to create high quality head avatar utilizing only a single or a few images per user. We learn a generative model for 3D animatable photo-realistic head avatar from a multi-view dataset of expressions from 2407 subjects, and leverage it as a prior for creating personalized avatar from <b>few-shot</b> images. Different from previous 3D-aware face generative models, our prior is built with a 3DMM-anchored neural radiance field backbone, which we show to be more effective for avatar creation through auto-decoding based on <b>few-shot</b> inputs. We also handle unstable 3DMM fitting by jointly optimizing the 3DMM fitting and camera calibration that leads to better <b>few-shot</b> adaptation. Our method demonstrates compelling results and outperforms existing state-of-the-art methods for <b>few-shot</b> avatar adaptation, paving the way for more efficient and personalized avatar creation.</p></p class="citation"></blockquote><h3 id=4245--250346-an-evaluation-of-deep-learning-based-stereo-dense-matching-dataset-shift-from-aerial-images-and-a-large-scale-stereo-dataset-teng-wu-et-al-2024>(42/45 | 250/346) An evaluation of Deep Learning based stereo dense matching dataset shift from aerial images and a large scale stereo dataset (Teng Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Teng Wu, Bruno Vallet, Marc Pierrot-Deseilligny, Ewelina Rupnik. (2024)<br><strong>An evaluation of Deep Learning based stereo dense matching dataset shift from aerial images and a large scale stereo dataset</strong><br><button class=copy-to-clipboard title="An evaluation of Deep Learning based stereo dense matching dataset shift from aerial images and a large scale stereo dataset" index=250>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-250 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12522v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12522v1.pdf filename=2402.12522v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Dense matching is crucial for 3D scene reconstruction since it enables the recovery of scene 3D <b>geometry</b> from image acquisition. Deep Learning (DL)-based methods have shown effectiveness in the special case of epipolar stereo disparity estimation in the computer vision community. DL-based methods depend heavily on the quality and quantity of training datasets. However, generating ground-truth disparity maps for real scenes remains a challenging task in the photogrammetry community. To address this challenge, we propose a method for generating ground-truth disparity maps directly from Light Detection and Ranging (LiDAR) and images to produce a large and diverse dataset for six aerial datasets across four different areas and two areas with different resolution images. We also introduce a LiDAR-to-image co-registration refinement to the framework that takes special precautions regarding occlusions and refrains from disparity interpolation to avoid precision loss. Evaluating 11 dense matching methods across datasets with diverse scene types, image resolutions, and geometric configurations, which are deeply investigated in dataset shift, GANet performs best with identical training and testing data, and PSMNet shows robustness across different datasets, and we proposed the best strategy for training with a limit dataset. We will also provide the dataset and training models; more information can be found at <a href=https://github.com/whuwuteng/Aerial_Stereo_Dataset>https://github.com/whuwuteng/Aerial_Stereo_Dataset</a>.</p></p class="citation"></blockquote><h3 id=4345--251346-binary-opacity-grids-capturing-fine-geometric-detail-for-mesh-based-view-synthesis-christian-reiser-et-al-2024>(43/45 | 251/346) Binary Opacity Grids: Capturing Fine Geometric Detail for Mesh-Based View Synthesis (Christian Reiser et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Christian Reiser, Stephan Garbin, Pratul P. Srinivasan, Dor Verbin, Richard Szeliski, Ben Mildenhall, Jonathan T. Barron, Peter Hedman, Andreas Geiger. (2024)<br><strong>Binary Opacity Grids: Capturing Fine Geometric Detail for Mesh-Based View Synthesis</strong><br><button class=copy-to-clipboard title="Binary Opacity Grids: Capturing Fine Geometric Detail for Mesh-Based View Synthesis" index=251>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-251 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12377v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12377v1.pdf filename=2402.12377v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>While surface-based view synthesis algorithms are appealing due to their low computational requirements, they often struggle to reproduce thin structures. In contrast, more expensive methods that model the scene&rsquo;s <b>geometry</b> as a volumetric density field (e.g. NeRF) excel at reconstructing fine geometric detail. However, density fields often represent <b>geometry</b> in a &ldquo;fuzzy&rdquo; manner, which hinders exact localization of the surface. In this work, we modify density fields to encourage them to converge towards surfaces, without compromising their ability to reconstruct thin structures. First, we employ a discrete opacity grid representation instead of a continuous density field, which allows opacity values to discontinuously transition from zero to one at the surface. Second, we anti-alias by casting multiple rays per pixel, which allows occlusion boundaries and subpixel structures to be modelled without using semi-transparent voxels. Third, we minimize the binary entropy of the opacity values, which facilitates the extraction of surface <b>geometry</b> by encouraging opacity values to binarize towards the end of training. Lastly, we develop a fusion-based meshing strategy followed by mesh simplification and appearance model fitting. The compact meshes produced by our model can be rendered in real-time on mobile devices and achieve significantly higher view synthesis quality compared to existing mesh-based approaches.</p></p class="citation"></blockquote><h3 id=4445--252346-pushing-auto-regressive-models-for-3d-shape-generation-at-capacity-and-scalability-xuelin-qian-et-al-2024>(44/45 | 252/346) Pushing Auto-regressive Models for 3D Shape Generation at Capacity and Scalability (Xuelin Qian et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xuelin Qian, Yu Wang, Simian Luo, Yinda Zhang, Ying Tai, Zhenyu Zhang, Chengjie Wang, Xiangyang Xue, Bo Zhao, Tiejun Huang, Yunsheng Wu, Yanwei Fu. (2024)<br><strong>Pushing Auto-regressive Models for 3D Shape Generation at Capacity and Scalability</strong><br><button class=copy-to-clipboard title="Pushing Auto-regressive Models for 3D Shape Generation at Capacity and Scalability" index=252>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-252 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 5<br>Keywords: Representation Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12225v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12225v1.pdf filename=2402.12225v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Auto-regressive models have achieved impressive results in 2D image generation by modeling joint distributions in grid space. In this paper, we extend auto-regressive models to 3D domains, and seek a stronger ability of 3D shape generation by improving auto-regressive models at capacity and scalability simultaneously. Firstly, we leverage an ensemble of publicly available 3D datasets to facilitate the training of large-scale models. It consists of a comprehensive collection of approximately 900,000 objects, with multiple properties of meshes, points, voxels, rendered images, and text captions. This diverse labeled dataset, termed Objaverse-Mix, empowers our model to learn from a wide range of object variations. However, directly applying 3D auto-regression encounters critical challenges of high computational demands on volumetric grids and ambiguous auto-regressive order along grid dimensions, resulting in inferior quality of 3D shapes. To this end, we then present a novel framework Argus3D in terms of capacity. Concretely, our approach introduces discrete <b>representation</b> <b>learning</b> based on a latent vector instead of volumetric grids, which not only reduces computational costs but also preserves essential geometric details by learning the joint distributions in a more tractable order. The capacity of conditional generation can thus be realized by simply concatenating various conditioning inputs to the latent vector, such as point clouds, categories, images, and texts. In addition, thanks to the simplicity of our model architecture, we naturally scale up our approach to a larger model with an impressive 3.6 billion parameters, further enhancing the quality of versatile 3D generation. Extensive experiments on four generation tasks demonstrate that Argus3D can synthesize diverse and faithful shapes across multiple categories, achieving remarkable performance.</p></p class="citation"></blockquote><h3 id=4545--253346-unveiling-the-depths-a-multi-modal-fusion-framework-for-challenging-scenarios-jialei-xu-et-al-2024>(45/45 | 253/346) Unveiling the Depths: A Multi-Modal Fusion Framework for Challenging Scenarios (Jialei Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jialei Xu, Xianming Liu, Junjun Jiang, Kui Jiang, Rui Li, Kai Cheng, Xiangyang Ji. (2024)<br><strong>Unveiling the Depths: A Multi-Modal Fusion Framework for Challenging Scenarios</strong><br><button class=copy-to-clipboard title="Unveiling the Depths: A Multi-Modal Fusion Framework for Challenging Scenarios" index=253>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-253 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 3<br>Keywords: Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11826v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11826v1.pdf filename=2402.11826v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Monocular depth estimation from RGB images plays a pivotal role in 3D vision. However, its accuracy can deteriorate in challenging environments such as nighttime or adverse weather conditions. While long-wave infrared cameras offer stable imaging in such challenging conditions, they are inherently low-resolution, lacking rich texture and semantics as delivered by the RGB image. Current methods focus solely on a single modality due to the difficulties to identify and integrate faithful depth cues from both sources. To address these issues, this paper presents a novel approach that identifies and integrates dominant cross-modality depth features with a learning-based framework. Concretely, we independently compute the coarse depth maps with separate networks by fully utilizing the individual depth cues from each modality. As the advantageous depth spreads across both modalities, we propose a novel confidence loss steering a confidence predictor network to yield a confidence map specifying latent potential depth areas. With the resulting confidence map, we propose a <b>multi-modal</b> fusion network that fuses the final depth in an end-to-end manner. Harnessing the proposed pipeline, our method demonstrates the ability of robust depth estimation in a variety of difficult scenarios. Experimental results on the challenging MS$^2$ and ViViD++ datasets demonstrate the effectiveness and robustness of our method.</p></p class="citation"></blockquote><h2 id=cssd-5>cs.SD (5)</h2><h3 id=15--254346-multimodal-emotion-recognition-from-raw-audio-with-sinc-convolution-xiaohui-zhang-et-al-2024>(1/5 | 254/346) Multimodal Emotion Recognition from Raw Audio with Sinc-convolution (Xiaohui Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiaohui Zhang, Wenjie Fu, Mangui Liang. (2024)<br><strong>Multimodal Emotion Recognition from Raw Audio with Sinc-convolution</strong><br><button class=copy-to-clipboard title="Multimodal Emotion Recognition from Raw Audio with Sinc-convolution" index=254>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-254 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SD<br>Categories: cs-MM, cs-SD, cs.SD, eess-AS<br>Keyword Score: 56<br>Keywords: Multi-modal, Multi-modal, LSTM, LSTM, LSTM, Automatic Speech Recognition, Emotion Recognition<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11954v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11954v1.pdf filename=2402.11954v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Speech</b> <b>Emotion</b> <b>Recognition</b> (SER) is still a complex task for computers with average recall rates usually about 70% on the most realistic datasets. Most SER systems use hand-crafted features extracted from audio signal such as energy, zero crossing rate, spectral information, prosodic, mel frequency cepstral coefficient (MFCC), and so on. More recently, using raw waveform for training neural network is becoming an emerging trend. This approach is advantageous as it eliminates the feature extraction pipeline. Learning from time-domain signal has shown good results for tasks such as <b>speech</b> <b>recognition,</b> speaker verification etc. In this paper, we utilize Sinc-convolution layer, which is an efficient architecture for preprocessing raw <b>speech</b> <b>waveform</b> for <b>emotion</b> <b>recognition,</b> to extract acoustic features from raw audio signals followed by a <b>long</b> <b>short-term</b> <b>memory</b> <b>(LSTM).</b> We also incorporate linguistic features and append a dialogical <b>emotion</b> <b>decoding</b> (DED) strategy. Our approach achieves a weighted accuracy of 85.1% in four class <b>emotion</b> <b>on</b> the Interactive <b>Emotional</b> <b>Dyadic</b> Motion Capture (IEMOCAP) dataset.</p></p class="citation"></blockquote><h3 id=25--255346-on-the-semantic-latent-space-of-diffusion-based-text-to-speech-models-miri-varshavsky-hassid-et-al-2024>(2/5 | 255/346) On the Semantic Latent Space of Diffusion-Based Text-to-Speech Models (Miri Varshavsky Hassid et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Miri Varshavsky Hassid, Roy Hirsch, Regev Cohen, Tomer Golany, Daniel Freedman, Ehud Rivlin. (2024)<br><strong>On the Semantic Latent Space of Diffusion-Based Text-to-Speech Models</strong><br><button class=copy-to-clipboard title="On the Semantic Latent Space of Diffusion-Based Text-to-Speech Models" index=255>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-255 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SD<br>Categories: cs-CL, cs-LG, cs-SD, cs.SD, eess-AS<br>Keyword Score: 50<br>Keywords: Diffusion Model, Supervised Learning, Unsupervised Learning, Text-to-speech, Text-to-speech<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12423v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12423v1.pdf filename=2402.12423v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The incorporation of Denoising <b>Diffusion</b> <b>Models</b> (DDMs) in the <b>Text-to-Speech</b> <b>(TTS)</b> domain is rising, providing great value in synthesizing high quality speech. Although they exhibit impressive audio quality, the extent of their semantic capabilities is unknown, and controlling their synthesized speech&rsquo;s vocal properties remains a challenge. Inspired by recent advances in image synthesis, we explore the latent space of frozen <b>TTS</b> models, which is composed of the latent bottleneck activations of the DDM&rsquo;s denoiser. We identify that this space contains rich semantic information, and outline several novel methods for finding semantic directions within it, both <b>supervised</b> and <b>unsupervised.</b> We then demonstrate how these enable off-the-shelf audio editing, without any further training, architectural changes or data requirements. We present evidence of the semantic and acoustic qualities of the edited audio, and provide supplemental samples: <a href=https://latent-analysis-grad-tts.github.io/speech-samples/>https://latent-analysis-grad-tts.github.io/speech-samples/</a>.</p></p class="citation"></blockquote><h3 id=35--256346-secp-a-speech-enhancement-based-curation-pipeline-for-scalable-acquisition-of-clean-speech-adam-sabra-et-al-2024>(3/5 | 256/346) SECP: A Speech Enhancement-Based Curation Pipeline For Scalable Acquisition Of Clean Speech (Adam Sabra et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Adam Sabra, Cyprian Wronka, Michelle Mao, Samer Hijazi. (2024)<br><strong>SECP: A Speech Enhancement-Based Curation Pipeline For Scalable Acquisition Of Clean Speech</strong><br><button class=copy-to-clipboard title="SECP: A Speech Enhancement-Based Curation Pipeline For Scalable Acquisition Of Clean Speech" index=256>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-256 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SD<br>Categories: cs-IR, cs-LG, cs-SD, cs.SD, eess-AS<br>Keyword Score: 20<br>Keywords: Supervised Learning, human-in-the-loop<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12482v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12482v1.pdf filename=2402.12482v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>As more speech technologies rely on a <b>supervised</b> deep learning approach with clean speech as the ground truth, a methodology to onboard said speech at scale is needed. However, this approach needs to minimize the dependency on human listening and annotation, only requiring a <b>human-in-the-loop</b> when needed. In this paper, we address this issue by outlining Speech Enhancement-based Curation Pipeline (SECP) which serves as a framework to onboard clean speech. This clean speech can then train a speech enhancement model, which can further refine the original dataset and thus close the iterative loop. By running two iterative rounds, we observe that enhanced output used as ground truth does not degrade model performance according to $\Delta_{PESQ}$, a metric used in this paper. We also show through comparative mean opinion score (CMOS) based subjective tests that the highest and lowest bound of refined data is perceptually better than the original data.</p></p class="citation"></blockquote><h3 id=45--257346-unraveling-complex-data-diversity-in-underwater-acoustic-target-recognition-through-convolution-based-mixture-of-experts-yuan-xie-et-al-2024>(4/5 | 257/346) Unraveling Complex Data Diversity in Underwater Acoustic Target Recognition through Convolution-based Mixture of Experts (Yuan Xie et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuan Xie, Jiawei Ren, Ji Xu. (2024)<br><strong>Unraveling Complex Data Diversity in Underwater Acoustic Target Recognition through Convolution-based Mixture of Experts</strong><br><button class=copy-to-clipboard title="Unraveling Complex Data Diversity in Underwater Acoustic Target Recognition through Convolution-based Mixture of Experts" index=257>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-257 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SD<br>Categories: cs-SD, cs.SD, eess-AS<br>Keyword Score: 10<br>Keywords: Convolution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11919v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11919v1.pdf filename=2402.11919v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Underwater acoustic target recognition is a difficult task owing to the intricate nature of underwater acoustic signals. The complex underwater environments, unpredictable transmission channels, and dynamic motion states greatly impact the real-world underwater acoustic signals, and may even obscure the intrinsic characteristics related to targets. Consequently, the data distribution of underwater acoustic signals exhibits high intra-class diversity, thereby compromising the accuracy and robustness of recognition systems.To address these issues, this work proposes a <b>convolution-based</b> mixture of experts (CMoE) that recognizes underwater targets in a fine-grained manner. The proposed technique introduces multiple expert layers as independent learners, along with a routing layer that determines the assignment of experts according to the characteristics of inputs. This design allows the model to utilize independent parameter spaces, facilitating the learning of complex underwater signals with high intra-class diversity. Furthermore, this work optimizes the CMoE structure by balancing regularization and an optional residual module. To validate the efficacy of our proposed techniques, we conducted detailed experiments and visualization analyses on three underwater acoustic databases across several acoustic features. The experimental results demonstrate that our CMoE consistently achieves significant performance improvements, delivering superior recognition accuracy when compared to existing advanced methods.</p></p class="citation"></blockquote><h3 id=55--258346-low-power-snn-based-audio-source-localisation-using-a-hilbert-transform-spike-encoding-scheme-saeid-haghighatshoar-et-al-2024>(5/5 | 258/346) Low-power SNN-based audio source localisation using a Hilbert Transform spike encoding scheme (Saeid Haghighatshoar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Saeid Haghighatshoar, Dylan R Muir. (2024)<br><strong>Low-power SNN-based audio source localisation using a Hilbert Transform spike encoding scheme</strong><br><button class=copy-to-clipboard title="Low-power SNN-based audio source localisation using a Hilbert Transform spike encoding scheme" index=258>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-258 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SD<br>Categories: cs-NE, cs-SD, cs.SD, eess-AS<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11748v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11748v1.pdf filename=2402.11748v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Sound source localisation is used in many consumer electronics devices, to help isolate audio from individual speakers and to reject noise. Localization is frequently accomplished by &ldquo;beamforming&rdquo; algorithms, which combine microphone audio streams to improve received signal power from particular incident source directions. Beamforming algorithms generally use knowledge of the frequency components of the audio source, along with the known microphone array <b>geometry,</b> to analytically phase-shift microphone streams before combining them. A dense set of band-pass filters is often used to obtain known-frequency &ldquo;narrowband&rdquo; components from wide-band audio streams. These approaches achieve high accuracy, but state of the art narrowband beamforming algorithms are computationally demanding, and are therefore difficult to integrate into low-power IoT devices. We demonstrate a novel method for sound source localisation in arbitrary microphone arrays, designed for efficient implementation in ultra-low-power spiking neural networks (SNNs). We use a novel short-time Hilbert transform (STHT) to remove the need for demanding band-pass filtering of audio, and introduce a new accompanying method for audio encoding with spiking events. Our beamforming and localisation approach achieves state-of-the-art accuracy for SNN methods, and comparable with traditional non-SNN super-resolution approaches. We deploy our method to low-power SNN audio inference hardware, and achieve much lower power consumption compared with super-resolution methods. We demonstrate that signal processing approaches can be co-designed with spiking neural network implementations to achieve high levels of power efficiency. Our new Hilbert-transform-based method for beamforming promises to also improve the efficiency of traditional DSP-based signal processing.</p></p class="citation"></blockquote><h2 id=cssi-5>cs.SI (5)</h2><h3 id=15--259346-attacks-on-node-attributes-in-graph-neural-networks-ying-xu-et-al-2024>(1/5 | 259/346) Attacks on Node Attributes in Graph Neural Networks (Ying Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ying Xu, Michael Lanier, Anindya Sarkar, Yevgeniy Vorobeychik. (2024)<br><strong>Attacks on Node Attributes in Graph Neural Networks</strong><br><button class=copy-to-clipboard title="Attacks on Node Attributes in Graph Neural Networks" index=259>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-259 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SI<br>Categories: cs-CR, cs-LG, cs-SI, cs.SI<br>Keyword Score: 53<br>Keywords: Graph, Graph Contrastive Learning, Graph Neural Network, Node Embedding, Contrastive Learning, Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12426v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12426v1.pdf filename=2402.12426v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Graphs</b> <b>are</b> <b>commonly</b> used to model complex networks prevalent in modern social media and literacy applications. Our research investigates the vulnerability of these <b>graphs</b> <b>through</b> <b>the</b> application of feature based <b>adversarial</b> <b>attacks,</b> focusing on both decision-time attacks and poisoning attacks. In contrast to state-of-the-art models like Net Attack and Meta Attack, which target <b>node</b> <b>attributes</b> and <b>graph</b> <b>structure,</b> <b>our</b> study specifically targets <b>node</b> <b>attributes.</b> For our analysis, we utilized the text dataset Hellaswag and <b>graph</b> <b>datasets</b> <b>Cora</b> and CiteSeer, providing a diverse basis for evaluation. Our findings indicate that decision-time attacks using Projected Gradient Descent (PGD) are more potent compared to poisoning attacks that employ Mean <b>Node</b> <b>Embeddings</b> and <b>Graph</b> <b>Contrastive</b> <b>Learning</b> strategies. This provides insights for <b>graph</b> <b>data</b> <b>security,</b> pinpointing where <b>graph-based</b> <b>models</b> <b>are</b> most vulnerable and thereby informing the development of stronger defense mechanisms against such attacks.</p></p class="citation"></blockquote><h3 id=25--260346-analysis-of-persian-news-agencies-on-instagram-a-words-co-occurrence-graph-based-approach-mohammad-heydari-et-al-2024>(2/5 | 260/346) Analysis of Persian News Agencies on Instagram, A Words Co-occurrence Graph-based Approach (Mohammad Heydari et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mohammad Heydari, Babak Teimourpour. (2024)<br><strong>Analysis of Persian News Agencies on Instagram, A Words Co-occurrence Graph-based Approach</strong><br><button class=copy-to-clipboard title="Analysis of Persian News Agencies on Instagram, A Words Co-occurrence Graph-based Approach" index=260>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-260 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SI<br>Categories: cs-IR, cs-SI, cs.SI<br>Keyword Score: 36<br>Keywords: Graph, Clustering, Unsupervised Learning, Keyword Extraction, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12272v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12272v1.pdf filename=2402.12272v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The rise of the Internet and the exponential increase in data have made manual data <b>summarization</b> and analysis a challenging task. Instagram social network is a prominent social network widely utilized in Iran for information sharing and communication across various age groups. The inherent structure of Instagram, characterized by its text-rich content and <b>graph-like</b> data representation, enables the utilization of text and <b>graph</b> processing techniques for data analysis purposes. The degree distributions of these networks exhibit scale-free characteristics, indicating non-random growth patterns. Recently, word co-occurrence has gained attention from researchers across multiple disciplines due to its simplicity and practicality. <b>Keyword</b> <b>extraction</b> is a crucial task in natural language processing. In this study, we demonstrated that high-precision extraction of <b>keywords</b> <b>from</b> Instagram posts in the Persian language can be achieved using <b>unsupervised</b> word co-occurrence methods without resorting to conventional techniques such as <b>clustering</b> or pre-trained models. After <b>graph</b> visualization and community detection, it was observed that the top topics covered by news agencies are represented by these <b>graphs.</b> This approach is generalizable to new and diverse datasets and can provide acceptable outputs for new data. To the author&rsquo;s knowledge, this method has not been employed in the Persian language before on Instagram social network. The new crawled data has been publicly released on GitHub for exploration by other researchers. By employing this method, it is possible to use other <b>graph-based</b> algorithms, such as community detections. The results help us to identify the key role of different news agencies in information diffusion among the public, identify hidden communities, and discover latent patterns among a massive amount of data.</p></p class="citation"></blockquote><h3 id=35--261346-a-machine-learning-ensemble-model-for-the-detection-of-cyberbullying-abulkarim-faraj-alqahtani-et-al-2024>(3/5 | 261/346) A Machine Learning Ensemble Model for the Detection of Cyberbullying (Abulkarim Faraj Alqahtani et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Abulkarim Faraj Alqahtani, Mohammad Ilyas. (2024)<br><strong>A Machine Learning Ensemble Model for the Detection of Cyberbullying</strong><br><button class=copy-to-clipboard title="A Machine Learning Ensemble Model for the Detection of Cyberbullying" index=261>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-261 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SI<br>Categories: cs-LG, cs-SI, cs.SI<br>Keyword Score: 10<br>Keywords: Logistic Regression<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12538v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12538v1.pdf filename=2402.12538v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The pervasive use of social media platforms, such as Facebook, Instagram, and X, has significantly amplified our electronic interconnectedness. Moreover, these platforms are now easily accessible from any location at any given time. However, the increased popularity of social media has also led to cyberbullying.It is imperative to address the need for finding, monitoring, and mitigating cyberbullying posts on social media platforms. Motivated by this necessity, we present this paper to contribute to developing an automated system for detecting binary labels of aggressive tweets.Our study has demonstrated remarkable performance compared to previous experiments on the same dataset. We employed the stacking ensemble machine learning method, utilizing four various feature extraction techniques to optimize performance within the stacking ensemble learning framework. Combining five machine learning algorithms,Decision Trees, Random Forest, Linear Support Vector Classification, <b>Logistic</b> <b>Regression,</b> and K-Nearest Neighbors into an ensemble method, we achieved superior results compared to traditional machine learning classifier models. The stacking classifier achieved a high accuracy rate of 94.00%, outperforming traditional machine learning models and surpassing the results of prior experiments that utilized the same dataset. The outcomes of our experiments showcased an accuracy rate of 0.94% in detection tweets as aggressive or non-aggressive.</p></p class="citation"></blockquote><h3 id=45--262346-bridging-or-breaking-impact-of-intergroup-interactions-on-religious-polarization-rochana-chaturvedi-et-al-2024>(4/5 | 262/346) Bridging or Breaking: Impact of Intergroup Interactions on Religious Polarization (Rochana Chaturvedi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rochana Chaturvedi, Sugat Chaturvedi, Elena Zheleva. (2024)<br><strong>Bridging or Breaking: Impact of Intergroup Interactions on Religious Polarization</strong><br><button class=copy-to-clipboard title="Bridging or Breaking: Impact of Intergroup Interactions on Religious Polarization" index=262>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-262 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SI<br>Categories: cs-CL, cs-SI, cs.SI, physics-soc-ph<br>Keyword Score: 10<br>Keywords: Meta Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11895v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11895v2.pdf filename=2402.11895v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>While exposure to diverse viewpoints may reduce polarization, it can also have a backfire effect and exacerbate polarization when the discussion is adversarial. Here, we examine the question whether intergroup interactions around important events affect polarization between majority and minority groups in social networks. We compile data on the religious identity of nearly 700,000 Indian Twitter users engaging in COVID-19-related discourse during 2020. We introduce a new measure for an individual&rsquo;s group conformity based on contextualized embeddings of tweet text, which helps us assess polarization between religious groups. We then use a <b>meta-learning</b> <b>framework</b> to examine heterogeneous treatment effects of intergroup interactions on an individual&rsquo;s group conformity in the light of communal, political, and socio-economic events. We find that for political and social events, intergroup interactions reduce polarization. This decline is weaker for individuals at the extreme who already exhibit high conformity to their group. In contrast, during communal events, intergroup interactions can increase group conformity. Finally, we decompose the differential effects across religious groups in terms of emotions and topics of discussion. The results show that the dynamics of religious polarization are sensitive to the context and have important implications for understanding the role of intergroup interactions.</p></p class="citation"></blockquote><h3 id=55--263346-deep-structural-knowledge-exploitation-and-synergy-for-estimating-node-importance-value-on-heterogeneous-information-networks-yankai-chen-et-al-2024>(5/5 | 263/346) Deep Structural Knowledge Exploitation and Synergy for Estimating Node Importance Value on Heterogeneous Information Networks (Yankai Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yankai Chen, Yixiang Fang, Qiongyan Wang, Xin Cao, Irwin King. (2024)<br><strong>Deep Structural Knowledge Exploitation and Synergy for Estimating Node Importance Value on Heterogeneous Information Networks</strong><br><button class=copy-to-clipboard title="Deep Structural Knowledge Exploitation and Synergy for Estimating Node Importance Value on Heterogeneous Information Networks" index=263>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-263 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SI<br>Categories: cs-AI, cs-LG, cs-SI, cs.SI<br>Keyword Score: 6<br>Keywords: Graph, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12411v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12411v1.pdf filename=2402.12411v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Node importance estimation problem has been studied conventionally with homogeneous network topology analysis. To deal with network heterogeneity, a few recent methods employ <b>graph</b> neural models to automatically learn diverse sources of information. However, the major concern revolves around that their full adaptive learning process may lead to insufficient information exploration, thereby formulating the problem as the isolated node value prediction with underperformance and less interpretability. In this work, we propose a novel learning framework: SKES. Different from previous automatic learning designs, SKES exploits heterogeneous structural knowledge to enrich the informativeness of node representations. Based on a sufficiently uninformative reference, SKES estimates the importance value for any input node, by quantifying its disparity against the reference. This establishes an interpretable node importance computation paradigm. Furthermore, SKES dives deep into the understanding that &ldquo;nodes with similar characteristics are prone to have similar importance values&rdquo; whilst guaranteeing that such informativeness disparity between any different nodes is orderly reflected by the embedding distance of their associated latent features. Extensive experiments on three widely-evaluated <b>benchmarks</b> demonstrate the performance superiority of SKES over several recent competing methods.</p></p class="citation"></blockquote><h2 id=csro-11>cs.RO (11)</h2><h3 id=111--264346-sinvig-a-self-evolving-interactive-visual-agent-for-human-robot-interaction-jie-xu-et-al-2024>(1/11 | 264/346) SInViG: A Self-Evolving Interactive Visual Agent for Human-Robot Interaction (Jie Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jie Xu, Hanbo Zhang, Xinghang Li, Huaping Liu, Xuguang Lan, Tao Kong. (2024)<br><strong>SInViG: A Self-Evolving Interactive Visual Agent for Human-Robot Interaction</strong><br><button class=copy-to-clipboard title="SInViG: A Self-Evolving Interactive Visual Agent for Human-Robot Interaction" index=264>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-264 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 53<br>Keywords: Benchmarking, Human Intervention, Disambiguation, Grounding, Stemming, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11792v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11792v2.pdf filename=2402.11792v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Linguistic ambiguity is ubiquitous in our daily lives. Previous works adopted interaction between robots and <b>humans</b> <b>for</b> language <b>disambiguation.</b> Nevertheless, when interactive robots are deployed in daily environments, there are significant challenges for natural <b>human-robot</b> <b>interaction,</b> <b>stemming</b> from complex and unpredictable visual inputs, open-ended interaction, and diverse user demands. In this paper, we present SInViG, which is a self-evolving interactive visual agent for <b>human-robot</b> <b>interaction</b> based on natural languages, aiming to resolve language ambiguity, if any, through multi-turn visual-language dialogues. It continuously and automatically learns from unlabeled images and <b>large</b> <b>language</b> <b>models,</b> without <b>human</b> <b>intervention,</b> to be more robust against visual and linguistic complexity. Benefiting from self-evolving, it sets new state-of-the-art on several interactive visual <b>grounding</b> <b>benchmarks.</b> Moreover, our <b>human-robot</b> <b>interaction</b> experiments show that the evolved models consistently acquire more and more preferences from <b>human</b> <b>users.</b> Besides, we also deployed our model on a Franka robot for interactive manipulation tasks. Results demonstrate that our model can follow diverse user instructions and interact naturally with <b>humans</b> <b>in</b> natural language, despite the complexity and disturbance of the environment.</p></p class="citation"></blockquote><h3 id=211--265346-decentralized-multi-robot-navigation-for-autonomous-surface-vehicles-with-distributional-reinforcement-learning-xi-lin-et-al-2024>(2/11 | 265/346) Decentralized Multi-Robot Navigation for Autonomous Surface Vehicles with Distributional Reinforcement Learning (Xi Lin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xi Lin, Yewei Huang, Fanfei Chen, Brendan Englot. (2024)<br><strong>Decentralized Multi-Robot Navigation for Autonomous Surface Vehicles with Distributional Reinforcement Learning</strong><br><button class=copy-to-clipboard title="Decentralized Multi-Robot Navigation for Autonomous Surface Vehicles with Distributional Reinforcement Learning" index=265>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-265 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 40<br>Keywords: Distributional Reinforcement Learning, Reinforcement Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11799v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11799v1.pdf filename=2402.11799v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Collision avoidance algorithms for Autonomous Surface Vehicles (ASV) that follow the Convention on the International Regulations for Preventing Collisions at Sea (COLREGs) have been proposed in recent years. However, it may be difficult and unsafe to follow COLREGs in congested waters, where multiple ASVs are navigating in the presence of static obstacles and strong currents, due to the complex interactions. To address this problem, we propose a decentralized multi-ASV collision avoidance policy based on <b>Distributional</b> <b>Reinforcement</b> <b>Learning,</b> which considers the interactions among ASVs as well as with static obstacles and current flows. We evaluate the performance of the proposed <b>Distributional</b> <b>RL</b> <b>based</b> policy against a traditional RL-based policy and two classical methods, Artificial Potential Fields (APF) and Reciprocal Velocity Obstacles (RVO), in <b>simulation</b> experiments, which show that the proposed policy achieves superior performance in navigation safety, while requiring minimal travel time and energy. A variant of our framework that automatically adapts its risk sensitivity is also demonstrated to improve ASV safety in highly congested environments.</p></p class="citation"></blockquote><h3 id=311--266346-dio-dataset-of-3d-mesh-models-of-indoor-objects-for-robotics-and-computer-vision-applications-nillan-nimal-et-al-2024>(3/11 | 266/346) DIO: Dataset of 3D Mesh Models of Indoor Objects for Robotics and Computer Vision Applications (Nillan Nimal et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nillan Nimal, Wenbin Li, Ronald Clark, Sajad Saeedi. (2024)<br><strong>DIO: Dataset of 3D Mesh Models of Indoor Objects for Robotics and Computer Vision Applications</strong><br><button class=copy-to-clipboard title="DIO: Dataset of 3D Mesh Models of Indoor Objects for Robotics and Computer Vision Applications" index=266>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-266 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-CV, cs-RO, cs.RO<br>Keyword Score: 25<br>Keywords: Geometry, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11836v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11836v1.pdf filename=2402.11836v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The creation of accurate virtual models of real-world objects is imperative to robotic <b>simulations</b> and applications such as computer vision, artificial intelligence, and machine learning. This paper documents the different methods employed for generating a database of mesh models of real-world objects. These methods address the tedious and time-intensive process of manually generating the models using CAD software. Essentially, DSLR/phone cameras were employed to acquire images of target objects. These images were processed using a photogrammetry software known as Meshroom to generate a dense surface reconstruction of the scene. The result produced by Meshroom was edited and simplified using MeshLab, a mesh-editing software to produce the final model. Based on the obtained models, this process was effective in modelling the <b>geometry</b> and texture of real-world objects with high fidelity. An active 3D scanner was also utilized to accelerate the process for large objects. All generated models and captured images are made available on the website of the project.</p></p class="citation"></blockquote><h3 id=411--267346-learning-input-constrained-control-barrier-functions-for-guaranteed-safety-of-car-like-robots-sven-brüggemann-et-al-2024>(4/11 | 267/346) Learning Input Constrained Control Barrier Functions for Guaranteed Safety of Car-Like Robots (Sven Brüggemann et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sven Brüggemann, Dominic Nightingale, Jack Silberman, Maurício de Oliveira. (2024)<br><strong>Learning Input Constrained Control Barrier Functions for Guaranteed Safety of Car-Like Robots</strong><br><button class=copy-to-clipboard title="Learning Input Constrained Control Barrier Functions for Guaranteed Safety of Car-Like Robots" index=267>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-267 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs-SY, cs.RO, eess-SY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12512v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12512v1.pdf filename=2402.12512v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We propose a design method for a robust safety filter based on Input Constrained Control Barrier Functions (ICCBF) for car-like robots moving in complex environments. A robust ICCBF that can be efficiently implemented is obtained by learning a smooth function of the environment using Support Vector Machine regression. The method takes into account steering constraints and is validated in <b>simulation</b> and a real experiment.</p></p class="citation"></blockquote><h3 id=511--268346-talk-through-it-end-user-directed-manipulation-learning-carl-winge-et-al-2024>(5/11 | 268/346) Talk Through It: End User Directed Manipulation Learning (Carl Winge et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Carl Winge, Adam Imdieke, Bahaa Aldeeb, Dongyeop Kang, Karthik Desingh. (2024)<br><strong>Talk Through It: End User Directed Manipulation Learning</strong><br><button class=copy-to-clipboard title="Talk Through It: End User Directed Manipulation Learning" index=268>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-268 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Bard, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12509v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12509v1.pdf filename=2402.12509v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Training generalist robot agents is an immensely difficult feat due to the requirement to perform a huge range of tasks in many different environments. We propose selectively training robots based on end-user preferences instead. Given a factory model that lets an end user instruct a robot to perform lower-level actions (e.g. &lsquo;Move left&rsquo;), we show that end users can collect demonstrations using language to train their home model for higher-level tasks specific to their needs (e.g. &lsquo;Open the top drawer and put the block inside&rsquo;). We demonstrate this hierarchical robot learning framework on robot manipulation tasks using RLBench environments. Our method results in a 16% improvement in skill success rates compared to a baseline method. In further experiments, we explore the use of the large <b>vision-language</b> model (VLM), <b>Bard,</b> to automatically break down tasks into sequences of lower-level instructions, aiming to bypass end-user involvement. The VLM is unable to break tasks down to our lowest level, but does achieve good results breaking high-level tasks into mid-level skills. We have a supplemental video and additional results at talk-through-it.github.io.</p></p class="citation"></blockquote><h3 id=611--269346-a-novel-framework-for-adaptive-stress-testing-of-autonomous-vehicles-in-highways-linh-trinh-et-al-2024>(6/11 | 269/346) A novel framework for adaptive stress testing of autonomous vehicles in highways (Linh Trinh et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Linh Trinh, Quang-Hung Luu, Thai M. Nguyen, Hai L. Vu. (2024)<br><strong>A novel framework for adaptive stress testing of autonomous vehicles in highways</strong><br><button class=copy-to-clipboard title="A novel framework for adaptive stress testing of autonomous vehicles in highways" index=269>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-269 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Markov Decision Process, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11813v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11813v1.pdf filename=2402.11813v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Guaranteeing the safe operations of autonomous vehicles (AVs) is crucial for their widespread adoption and public acceptance. It is thus of a great significance to not only assess the AV against the standard safety tests, but also discover potential corner cases of the AV under test that could lead to unsafe behaviour or scenario. In this paper, we propose a novel framework to systematically explore corner cases that can result in safety concerns in a highway traffic scenario. The framework is based on an adaptive stress testing (AST) approach, an emerging validation method that leverages a <b>Markov</b> <b>decision</b> <b>process</b> to formulate the scenarios and deep <b>reinforcement</b> <b>learning</b> (DRL) to discover the desirable patterns representing corner cases. To this end, we develop a new reward function for DRL to guide the AST in identifying crash scenarios based on the collision probability estimate between the AV under test (i.e., the ego vehicle) and the trajectory of other vehicles on the highway. The proposed framework is further integrated with a new driving model enabling us to create more realistic traffic scenarios capturing both the longitudinal and lateral movements of vehicles on the highway. In our experiment, we calibrate our model using real-world crash statistics involving automated vehicles in California, and then we analyze the characteristics of the AV and the framework. Quantitative and qualitative analyses of our experimental results demonstrate that our framework outperforms other existing AST schemes. The study can help discover crash scenarios of AV that are unknown or absent in human driving, thereby enhancing the safety and trustworthiness of AV technology.</p></p class="citation"></blockquote><h3 id=711--270346-decentralized-lifelong-path-planning-for-multiple-ackerman-car-like-robots-teng-guo-et-al-2024>(7/11 | 270/346) Decentralized Lifelong Path Planning for Multiple Ackerman Car-Like Robots (Teng Guo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Teng Guo, Jingjin Yu. (2024)<br><strong>Decentralized Lifelong Path Planning for Multiple Ackerman Car-Like Robots</strong><br><button class=copy-to-clipboard title="Decentralized Lifelong Path Planning for Multiple Ackerman Car-Like Robots" index=270>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-270 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11767v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11767v1.pdf filename=2402.11767v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Path planning for multiple non-holonomic robots in continuous domains constitutes a difficult robotics challenge with many applications. Despite significant recent progress on the topic, computationally efficient and high-quality solutions are lacking, especially in lifelong settings where robots must continuously take on new tasks. In this work, we make it possible to extend key ideas enabling state-of-the-art (SOTA) methods for multi-robot planning in discrete domains to the motion planning of multiple Ackerman (car-like) robots in lifelong settings, yielding high-performance centralized and decentralized planners. Our planners compute trajectories that allow the robots to reach precise $SE(2)$ goal poses. The effectiveness of our methods is thoroughly evaluated and confirmed using both <b>simulation</b> and real-world experiments.</p></p class="citation"></blockquote><h3 id=811--271346-from-reals-to-logic-and-back-inventing-symbolic-vocabularies-actions-and-models-for-planning-from-raw-data-naman-shah-et-al-2024>(8/11 | 271/346) From Reals to Logic and Back: Inventing Symbolic Vocabularies, Actions and Models for Planning from Raw Data (Naman Shah et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Naman Shah, Jayesh Nagpal, Pulkit Verma, Siddharth Srivastava. (2024)<br><strong>From Reals to Logic and Back: Inventing Symbolic Vocabularies, Actions and Models for Planning from Raw Data</strong><br><button class=copy-to-clipboard title="From Reals to Logic and Back: Inventing Symbolic Vocabularies, Actions and Models for Planning from Raw Data" index=271>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-271 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-RO, cs.RO<br>Keyword Score: 10<br>Keywords: Planning Domain Descrition Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11871v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11871v2.pdf filename=2402.11871v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Hand-crafted, logic-based state and action representations have been widely used to overcome the intractable computational complexity of long-horizon robot planning problems, including task and motion planning problems. However, creating such representations requires experts with strong intuitions and detailed knowledge about the robot and the tasks it may need to accomplish in a given setting. Removing this dependency on human intuition is a highly active research area. This paper presents the first approach for autonomously learning generalizable, logic-based relational representations for abstract states and actions starting from unannotated high-dimensional, real-valued robot trajectories. The learned representations constitute auto-invented <b>PDDL-like</b> domain models. Empirical results in deterministic settings show that powerful abstract representations can be learned from just a handful of robot trajectories; the learned relational representations include but go beyond classical, intuitive notions of high-level actions; and that the learned models allow planning algorithms to scale to tasks that were previously beyond the scope of planning without hand-crafted abstractions.</p></p class="citation"></blockquote><h3 id=911--272346-colrio-lidar-ranging-inertial-centralized-state-estimation-for-robotic-swarms-shipeng-zhong-et-al-2024>(9/11 | 272/346) CoLRIO: LiDAR-Ranging-Inertial Centralized State Estimation for Robotic Swarms (Shipeng Zhong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shipeng Zhong, Hongbo Chen, Yuhua Qi, Dapeng Feng, Zhiqiang Chen, Jin Wu, Weisong Wen, Ming Liu. (2024)<br><strong>CoLRIO: LiDAR-Ranging-Inertial Centralized State Estimation for Robotic Swarms</strong><br><button class=copy-to-clipboard title="CoLRIO: LiDAR-Ranging-Inertial Centralized State Estimation for Robotic Swarms" index=272>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-272 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11790v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11790v2.pdf filename=2402.11790v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Collaborative state estimation using different heterogeneous sensors is a fundamental prerequisite for robotic swarms operating in GPS-denied environments, posing a significant research challenge. In this paper, we introduce a centralized system to facilitate collaborative LiDAR-ranging-inertial state estimation, enabling robotic swarms to operate without the need for anchor deployment. The system efficiently distributes computationally intensive tasks to a central server, thereby reducing the computational burden on individual robots for local odometry calculations. The server back-end establishes a global reference by leveraging shared data and refining joint pose <b>graph</b> optimization through place recognition, global optimization techniques, and removal of outlier data to ensure precise and robust collaborative state estimation. Extensive evaluations of our system, utilizing both publicly available datasets and our custom datasets, demonstrate significant enhancements in the accuracy of collaborative SLAM estimates. Moreover, our system exhibits remarkable proficiency in large-scale missions, seamlessly enabling ten robots to collaborate effectively in performing SLAM tasks. In order to contribute to the research community, we will make our code open-source and accessible at \url{https://github.com/PengYu-team/Co-LRIO}.</p></p class="citation"></blockquote><h3 id=1011--273346-targeted-parallelization-of-conflict-based-search-for-multi-robot-path-planning-teng-guo-et-al-2024>(10/11 | 273/346) Targeted Parallelization of Conflict-Based Search for Multi-Robot Path Planning (Teng Guo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Teng Guo, Jingjin Yu. (2024)<br><strong>Targeted Parallelization of Conflict-Based Search for Multi-Robot Path Planning</strong><br><button class=copy-to-clipboard title="Targeted Parallelization of Conflict-Based Search for Multi-Robot Path Planning" index=273>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-273 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11768v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11768v1.pdf filename=2402.11768v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Multi-Robot Path Planning (MRPP) on <b>graphs,</b> equivalently known as Multi-Agent Path Finding (MAPF), is a well-established NP-hard problem with critically important applications. As serial computation in (near)-optimally solving MRPP approaches the computation efficiency limit, parallelization offers a promising route to push the limit further, especially in handling hard or large MRPP instances. In this study, we initiated a \emph{targeted} parallelization effort to boost the performance of conflict-based search for MRPP. Specifically, when instances are relatively small but robots are densely packed with strong interactions, we apply a decentralized parallel algorithm that concurrently explores multiple branches that leads to markedly enhanced solution discovery. On the other hand, when instances are large with sparse robot-robot interactions, we prioritize node expansion and conflict resolution. Our innovative multi-threaded approach to parallelizing bounded-suboptimal conflict search-based algorithms demonstrates significant improvements over baseline serial methods in success rate or runtime. Our contribution further pushes the understanding of MRPP and charts a promising path for elevating solution quality and computational efficiency through parallel algorithmic strategies.</p></p class="citation"></blockquote><h3 id=1111--274346-well-connected-set-and-its-application-to-multi-robot-path-planning-teng-guo-et-al-2024>(11/11 | 274/346) Well-Connected Set and Its Application to Multi-Robot Path Planning (Teng Guo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Teng Guo, Jingjin Yu. (2024)<br><strong>Well-Connected Set and Its Application to Multi-Robot Path Planning</strong><br><button class=copy-to-clipboard title="Well-Connected Set and Its Application to Multi-Robot Path Planning" index=274>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-274 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11766v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11766v1.pdf filename=2402.11766v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Parking lots and autonomous warehouses for accommodating many vehicles/robots adopt designs in which the underlying <b>graphs</b> are \emph{well-connected} to simplify planning and reduce congestion. In this study, we formulate and delve into the \emph{largest well-connected set} (LWCS) problem and explore its applications in layout design for multi-robot path planning. Roughly speaking, a well-connected set over a connected <b>graph</b> is a set of vertices such that there is a path on the <b>graph</b> connecting any pair of vertices in the set without passing through any additional vertices of the set. Identifying an LWCS has many potential high-utility applications, e.g., for determining parking garage layout and capacity, as prioritized planning can be shown to be complete when start/goal configurations belong to an LWCS. In this work, we establish that computing an LWCS is NP-complete. We further develop optimal and near-optimal LWCS algorithms, with the near-optimal algorithm targeting large maps. A complete prioritized planning method is given for planning paths for multiple robots residing on an LWCS.</p></p class="citation"></blockquote><h2 id=cshc-4>cs.HC (4)</h2><h3 id=14--275346-imbue-improving-interpersonal-effectiveness-through-simulation-and-just-in-time-feedback-with-human-language-model-interaction-inna-wanyin-lin-et-al-2024>(1/4 | 275/346) IMBUE: Improving Interpersonal Effectiveness through Simulation and Just-in-time Feedback with Human-Language Model Interaction (Inna Wanyin Lin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Inna Wanyin Lin, Ashish Sharma, Christopher Michael Rytting, Adam S. Miner, Jina Suh, Tim Althoff. (2024)<br><strong>IMBUE: Improving Interpersonal Effectiveness through Simulation and Just-in-time Feedback with Human-Language Model Interaction</strong><br><button class=copy-to-clipboard title="IMBUE: Improving Interpersonal Effectiveness through Simulation and Just-in-time Feedback with Human-Language Model Interaction" index=275>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-275 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-CL, cs-HC, cs.HC<br>Keyword Score: 40<br>Keywords: Simulation, Simulator, GPT, GPT-4<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12556v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12556v1.pdf filename=2402.12556v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Navigating certain communication situations can be challenging due to individuals&rsquo; lack of skills and the interference of strong emotions. However, effective learning opportunities are rarely accessible. In this work, we conduct a human-centered study that uses language models to simulate bespoke communication training and provide just-in-time feedback to support the practice and learning of interpersonal effectiveness skills. We apply the interpersonal effectiveness framework from Dialectical Behavioral Therapy (DBT), DEAR MAN, which focuses on both conversational and emotional skills. We present IMBUE, an interactive training system that provides feedback 25% more similar to experts&rsquo; feedback, compared to that generated by <b>GPT-4.</b> IMBUE is the first to focus on communication skills and emotion management simultaneously, incorporate experts&rsquo; domain knowledge in providing feedback, and be grounded in psychology theory. Through a randomized trial of 86 participants, we find that IMBUE&rsquo;s <b>simulation-only</b> variant significantly improves participants&rsquo; self-efficacy (up to 17%) and reduces negative emotions (up to 25%). With IMBUE&rsquo;s additional just-in-time feedback, participants demonstrate 17% improvement in skill mastery, along with greater enhancements in self-efficacy (27% more) and reduction of negative emotions (16% more) compared to <b>simulation-only.</b> The improvement in skill mastery is the only measure that is transferred to new and more difficult situations; situation specific training is necessary for improving self-efficacy and emotion reduction.</p></p class="citation"></blockquote><h3 id=24--276346-dynamic-and-super-personalized-media-ecosystem-driven-by-generative-ai-unpredictable-plays-never-repeating-the-same-sungjun-ahn-et-al-2024>(2/4 | 276/346) Dynamic and Super-Personalized Media Ecosystem Driven by Generative AI: Unpredictable Plays Never Repeating The Same (Sungjun Ahn et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sungjun Ahn, Hyun-Jeong Yim, Youngwan Lee, Sung-Ik Park. (2024)<br><strong>Dynamic and Super-Personalized Media Ecosystem Driven by Generative AI: Unpredictable Plays Never Repeating The Same</strong><br><button class=copy-to-clipboard title="Dynamic and Super-Personalized Media Ecosystem Driven by Generative AI: Unpredictable Plays Never Repeating The Same" index=276>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-276 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-AI, cs-HC, cs-MM, cs.HC, eess-SP<br>Keyword Score: 30<br>Keywords: Generative AI, In-context Learning, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12412v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12412v1.pdf filename=2402.12412v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper introduces a media service model that exploits artificial intelligence (AI) video generators at the receive end. This proposal deviates from the traditional multimedia ecosystem, completely relying on in-house production, by shifting part of the content creation onto the receiver. We bring a semantic process into the framework, allowing the distribution network to provide service elements that <b>prompt</b> the content generator, rather than distributing encoded data of fully finished programs. The service elements include fine-tailored text descriptions, lightweight image data of some objects, or application programming interfaces, comprehensively referred to as semantic sources, and the user terminal translates the received semantic data into video frames. Empowered by the random nature of <b>generative</b> <b>AI,</b> the users could then experience super-personalized services accordingly. The proposed idea incorporates the situations in which the user receives different service providers&rsquo; element packages; a sequence of packages over time, or multiple packages at the same time. Given promised <b>in-context</b> coherence and content integrity, the combinatory dynamics will amplify the service diversity, allowing the users to always chance upon new experiences. This work particularly aims at short-form videos and advertisements, which the users would easily feel fatigued by seeing the same frame sequence every time. In those use cases, the content provider&rsquo;s role will be recast as scripting semantic sources, transformed from a thorough producer. Overall, this work explores a new form of media ecosystem facilitated by receiver-embedded <b>generative</b> <b>models,</b> featuring both random content dynamics and enhanced delivery efficiency simultaneously.</p></p class="citation"></blockquote><h3 id=34--277346-enhancing-empathetic-response-generation-by-augmenting-llms-with-small-scale-empathetic-models-zhou-yang-et-al-2024>(3/4 | 277/346) Enhancing Empathetic Response Generation by Augmenting LLMs with Small-scale Empathetic Models (Zhou Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhou Yang, Zhaochun Ren, Wang Yufeng, Shizhong Peng, Haizhou Sun, Xiaofei Zhu, Xiangwen Liao. (2024)<br><strong>Enhancing Empathetic Response Generation by Augmenting LLMs with Small-scale Empathetic Models</strong><br><button class=copy-to-clipboard title="Enhancing Empathetic Response Generation by Augmenting LLMs with Small-scale Empathetic Models" index=277>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-277 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-HC, cs.HC<br>Keyword Score: 30<br>Keywords: Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11801v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11801v1.pdf filename=2402.11801v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Empathetic response generation is increasingly significant in AI, necessitating nuanced emotional and cognitive understanding coupled with articulate response expression. Current <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> excel in response expression; however, they lack the ability to deeply understand emotional and cognitive nuances, particularly in pinpointing fine-grained emotions and their triggers. Conversely, small-scale empathetic models (SEMs) offer strength in fine-grained emotion detection and detailed emotion cause identification. To harness the complementary strengths of both <b>LLMs</b> and SEMs, we introduce a Hybrid Empathetic Framework (HEF). HEF regards SEMs as flexible plugins to improve <b>LLM&rsquo;s</b> nuanced emotional and cognitive understanding. Regarding emotional understanding, HEF implements a two-stage emotion prediction strategy, encouraging <b>LLMs</b> to prioritize primary emotions emphasized by SEMs, followed by other categories, substantially alleviates the difficulties for <b>LLMs</b> in fine-grained emotion detection. Regarding cognitive understanding, HEF employs an emotion cause perception strategy, <b>prompting</b> <b>LLMs</b> to focus on crucial emotion-eliciting words identified by SEMs, thus boosting <b>LLMs&rsquo;</b> capabilities in identifying emotion causes. This collaborative approach enables <b>LLMs</b> to discern emotions more precisely and formulate empathetic responses. We validate HEF on the Empathetic-Dialogue dataset, and the findings indicate that our framework enhances the refined understanding of <b>LLMs</b> and their ability to convey empathetic responses.</p></p class="citation"></blockquote><h3 id=44--278346-beyond-voice-assistants-exploring-advantages-and-risks-of-an-in-car-social-robot-in-real-driving-scenarios-yuanchao-li-et-al-2024>(4/4 | 278/346) Beyond Voice Assistants: Exploring Advantages and Risks of an In-Car Social Robot in Real Driving Scenarios (Yuanchao Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuanchao Li, Lachlan Urquhart, Nihan Karatas, Shun Shao, Hiroshi Ishiguro, Xun Shen. (2024)<br><strong>Beyond Voice Assistants: Exploring Advantages and Risks of an In-Car Social Robot in Real Driving Scenarios</strong><br><button class=copy-to-clipboard title="Beyond Voice Assistants: Exploring Advantages and Risks of an In-Car Social Robot in Real Driving Scenarios" index=278>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-278 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-CY, cs-HC, cs-RO, cs-SD, cs.HC<br>Keyword Score: 10<br>Keywords: Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11853v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11853v2.pdf filename=2402.11853v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In-car Voice Assistants (VAs) play an increasingly critical role in automotive user interface design. However, existing VAs primarily perform simple &lsquo;query-answer&rsquo; tasks, limiting their ability to sustain drivers&rsquo; long-term attention. In this study, we investigate the effectiveness of an in-car Robot Assistant (RA) that offers functionalities beyond voice interaction. We aim to answer the question: How does the presence of a social robot impact user experience in real driving scenarios? Our study begins with a user survey to understand perspectives on in-car VAs and their influence on driving experiences. We then conduct non-driving and on-road experiments with selected participants to assess user experiences with an RA. Additionally, we conduct subjective ratings to evaluate user perceptions of the RA&rsquo;s personality, which is crucial for robot design. We also explore potential concerns regarding ethical risks. Finally, we provide a comprehensive discussion and <b>recommendations</b> for the future development of in-car RAs.</p></p class="citation"></blockquote><h2 id=eessas-3>eess.AS (3)</h2><h3 id=13--279346-language-codec-reducing-the-gaps-between-discrete-codec-representation-and-speech-language-models-shengpeng-ji-et-al-2024>(1/3 | 279/346) Language-Codec: Reducing the Gaps Between Discrete Codec Representation and Speech Language Models (Shengpeng Ji et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shengpeng Ji, Minghui Fang, Ziyue Jiang, Rongjie Huang, Jialung Zuo, Shulei Wang, Zhou Zhao. (2024)<br><strong>Language-Codec: Reducing the Gaps Between Discrete Codec Representation and Speech Language Models</strong><br><button class=copy-to-clipboard title="Language-Codec: Reducing the Gaps Between Discrete Codec Representation and Speech Language Models" index=279>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-279 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.AS<br>Categories: cs-SD, eess-AS, eess.AS<br>Keyword Score: 40<br>Keywords: Quantization, Supervised Learning, Weakly-supervised Learning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12208v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12208v2.pdf filename=2402.12208v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In recent years, <b>large</b> <b>language</b> <b>models</b> have achieved significant success in generative tasks (e.g., speech cloning and audio generation) related to speech, audio, music, and other signal domains. A crucial element of these models is the discrete acoustic codecs, which serves as an intermediate representation replacing the mel-spectrogram. However, there exist several gaps between discrete codecs and downstream speech language models. Specifically, 1) most codec models are trained on only 1,000 hours of data, whereas most speech language models are trained on 60,000 hours; 2) Achieving good reconstruction performance requires the utilization of numerous codebooks, which increases the burden on downstream speech language models; 3) The initial channel of the codebooks contains excessive information, making it challenging to directly generate acoustic tokens from weakly <b>supervised</b> signals such as text in downstream tasks. Consequently, leveraging the characteristics of speech language models, we propose Language-Codec. In the Language-Codec, we introduce a Mask Channel Residual Vector <b>Quantization</b> (MCRVQ) mechanism along with improved Fourier transform structures and larger training datasets to address the aforementioned gaps. We compare our method with competing audio compression algorithms and observe significant outperformance across extensive evaluations. Furthermore, we also validate the efficiency of the Language-Codec on downstream speech language models. The source code and pre-trained models can be accessed at <a href=https://github.com/jishengpeng/languagecodec>https://github.com/jishengpeng/languagecodec</a> .</p></p class="citation"></blockquote><h3 id=23--280346-parameter-efficient-finetuning-for-speech-emotion-recognition-and-domain-adaptation-nineli-lashkarashvili-et-al-2024>(2/3 | 280/346) Parameter Efficient Finetuning for Speech Emotion Recognition and Domain Adaptation (Nineli Lashkarashvili et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nineli Lashkarashvili, Wen Wu, Guangzhi Sun, Philip C. Woodland. (2024)<br><strong>Parameter Efficient Finetuning for Speech Emotion Recognition and Domain Adaptation</strong><br><button class=copy-to-clipboard title="Parameter Efficient Finetuning for Speech Emotion Recognition and Domain Adaptation" index=280>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-280 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.AS<br>Categories: cs-LG, cs-SD, eess-AS, eess.AS<br>Keyword Score: 40<br>Keywords: Fine-tuning, Foundation Model, Emotion Recognition, Domain Adaptation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11747v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11747v1.pdf filename=2402.11747v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Foundation</b> <b>models</b> have shown superior performance for speech <b>emotion</b> <b>recognition</b> (SER). However, given the limited data in <b>emotion</b> <b>corpora,</b> <b>finetuning</b> all parameters of large pre-trained models for SER can be both resource-intensive and susceptible to overfitting. This paper investigates parameter-efficient <b>finetuning</b> (PEFT) for SER. Various PEFT adaptors are systematically studied for both classification of discrete <b>emotion</b> <b>categories</b> and prediction of dimensional <b>emotional</b> <b>attributes.</b> The results demonstrate that the combination of PEFT methods surpasses full <b>finetuning</b> with a significant reduction in the number of trainable parameters. Furthermore, a two-stage adaptation strategy is proposed to adapt models trained on acted <b>emotion</b> <b>data,</b> which is more readily available, to make the model more adept at capturing natural <b>emotional</b> <b>expressions.</b> Both intra- and cross-corpus experiments validate the efficacy of the proposed approach in enhancing the performance on both the source and target domains.</p></p class="citation"></blockquote><h3 id=33--281346-bayesian-parameter-efficient-fine-tuning-for-overcoming-catastrophic-forgetting-haolin-chen-et-al-2024>(3/3 | 281/346) Bayesian Parameter-Efficient Fine-Tuning for Overcoming Catastrophic Forgetting (Haolin Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haolin Chen, Philip N. Garner. (2024)<br><strong>Bayesian Parameter-Efficient Fine-Tuning for Overcoming Catastrophic Forgetting</strong><br><button class=copy-to-clipboard title="Bayesian Parameter-Efficient Fine-Tuning for Overcoming Catastrophic Forgetting" index=281>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-281 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.AS<br>Categories: cs-LG, eess-AS, eess.AS<br>Keyword Score: 30<br>Keywords: Fine-tuning, Fine-tuning, Text-to-speech<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12220v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12220v1.pdf filename=2402.12220v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Although motivated by the adaptation of <b>text-to-speech</b> synthesis models, we argue that more generic parameter-efficient <b>fine-tuning</b> (PEFT) is an appropriate framework to do such adaptation. However, catastrophic forgetting remains an issue with PEFT, damaging the pre-trained model&rsquo;s inherent capabilities. We demonstrate that existing Bayesian learning techniques can be applied to PEFT to prevent catastrophic forgetting as long as the parameter shift of the <b>fine-tuned</b> layers can be calculated differentiably. In a principled series of experiments on language modeling and speech synthesis tasks, we utilize established Laplace approximations, including diagonal and Kronecker factored approaches, to regularize PEFT with the low-rank adaptation (LoRA) and compare their performance in pre-training knowledge preservation. Our results demonstrate that catastrophic forgetting can be overcome by our methods without degrading the <b>fine-tuning</b> performance, and using the Kronecker factored approximations produces a better preservation of the pre-training knowledge than the diagonal ones.</p></p class="citation"></blockquote><h2 id=csdb-2>cs.DB (2)</h2><h3 id=12--282346-training-table-question-answering-via-sql-query-decomposition-raphaël-mouravieff-et-al-2024>(1/2 | 282/346) Training Table Question Answering via SQL Query Decomposition (Raphaël Mouravieff et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Raphaël Mouravieff, Benjamin Piwowarski, Sylvain Lamprier. (2024)<br><strong>Training Table Question Answering via SQL Query Decomposition</strong><br><button class=copy-to-clipboard title="Training Table Question Answering via SQL Query Decomposition" index=282>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-282 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DB<br>Categories: cs-AI, cs-DB, cs.DB<br>Keyword Score: 40<br>Keywords: Grounding, Question Answering, Reasoning, Semantic Parsing<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13288v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13288v1.pdf filename=2402.13288v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Table <b>Question-Answering</b> <b>involves</b> both understanding the natural language query and <b>grounding</b> it in the context of the input table to extract the relevant information. In this context, many methods have highlighted the benefits of intermediate pre-training from SQL queries. However, while most approaches aim at generating final answers from inputs directly, we claim that there is better to do with SQL queries during training. By learning to imitate a restricted portion of SQL-like algebraic operations, we show that their execution flow provides intermediate supervision steps that allow increased generalization and structural <b>reasoning</b> compared with classical approaches of the field. Our study bridges the gap between <b>semantic</b> <b>parsing</b> and direct answering methods and provides useful insights regarding what types of operations should be predicted by a generative architecture or be preferably executed by an external algorithm.</p></p class="citation"></blockquote><h3 id=22--283346-structure-guided-large-language-model-for-sql-generation-qinggang-zhang-et-al-2024>(2/2 | 283/346) Structure Guided Large Language Model for SQL Generation (Qinggang Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qinggang Zhang, Junnan Dong, Hao Chen, Wentao Li, Feiran Huang, Xiao Huang. (2024)<br><strong>Structure Guided Large Language Model for SQL Generation</strong><br><button class=copy-to-clipboard title="Structure Guided Large Language Model for SQL Generation" index=283>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-283 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DB<br>Categories: cs-AI, cs-CL, cs-DB, cs.DB<br>Keyword Score: 23<br>Keywords: Benchmarking, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13284v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13284v1.pdf filename=2402.13284v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Generating accurate Structured Querying Language (SQL) is a long-standing problem, especially in matching users&rsquo; semantic queries with structured databases and then generating structured SQL. Existing models typically input queries and database schemas into the <b>LLM</b> and rely on the <b>LLM</b> to perform semantic-structure matching and generate structured SQL. However, such solutions overlook the structural information within user queries and databases, which can be utilized to enhance the generation of structured SQL. This oversight can lead to inaccurate or unexecutable SQL generation. To fully exploit the structure, we propose a structure-to-SQL framework, which leverages the inherent structure information to improve the SQL generation of <b>LLMs.</b> Specifically, we introduce our Structure Guided SQL~(SGU-SQL) generation model. SGU-SQL first links user queries and databases in a structure-enhanced manner. It then decomposes complicated linked structures with grammar trees to guide the <b>LLM</b> to generate the SQL step by step. Extensive experiments on two <b>benchmark</b> datasets illustrate that SGU-SQL can outperform sixteen SQL generation baselines.</p></p class="citation"></blockquote><h2 id=statml-6>stat.ML (6)</h2><h3 id=16--284346-kernel-kmeans-clustering-splits-for-end-to-end-unsupervised-decision-trees-louis-ohl-et-al-2024>(1/6 | 284/346) Kernel KMeans clustering splits for end-to-end unsupervised decision trees (Louis Ohl et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Louis Ohl, Pierre-Alexandre Mattei, Mickaël Leclercq, Arnaud Droit, Frédéric Precioso. (2024)<br><strong>Kernel KMeans clustering splits for end-to-end unsupervised decision trees</strong><br><button class=copy-to-clipboard title="Kernel KMeans clustering splits for end-to-end unsupervised decision trees" index=284>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-284 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: 62h30, G-3, cs-AI, cs-LG, stat-ML, stat.ML<br>Keyword Score: 33<br>Keywords: Clustering, Supervised Learning, Supervised Learning, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12232v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12232v1.pdf filename=2402.12232v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Trees are convenient models for obtaining explainable predictions on relatively small datasets. Although there are many proposals for the end-to-end construction of such trees in <b>supervised</b> <b>learning,</b> learning a tree end-to-end for <b>clustering</b> without labels remains an open challenge. As most works focus on interpreting with trees the result of another <b>clustering</b> algorithm, we present here a novel end-to-end trained <b>unsupervised</b> binary tree for <b>clustering:</b> Kauri. This method performs a greedy maximisation of the kernel KMeans objective without requiring the definition of centroids. We compare this model on multiple datasets with recent <b>unsupervised</b> trees and show that Kauri performs identically when using a linear kernel. For other kernels, Kauri often outperforms the concatenation of kernel KMeans and a CART decision tree.</p></p class="citation"></blockquote><h3 id=26--285346-when-do-off-policy-and-on-policy-policy-gradient-methods-align-davide-mambelli-et-al-2024>(2/6 | 285/346) When Do Off-Policy and On-Policy Policy Gradient Methods Align? (Davide Mambelli et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Davide Mambelli, Stephan Bongers, Onno Zoeter, Matthijs T. J. Spaan, Frans A. Oliehoek. (2024)<br><strong>When Do Off-Policy and On-Policy Policy Gradient Methods Align?</strong><br><button class=copy-to-clipboard title="When Do Off-Policy and On-Policy Policy Gradient Methods Align?" index=285>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-285 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-LG, stat-ML, stat.ML<br>Keyword Score: 30<br>Keywords: Reinforcement Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12034v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12034v1.pdf filename=2402.12034v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Policy gradient methods are widely adopted <b>reinforcement</b> <b>learning</b> algorithms for tasks with continuous action spaces. These methods succeeded in many application domains, however, because of their notorious sample inefficiency their use remains limited to problems where fast and accurate <b>simulations</b> are available. A common way to improve sample efficiency is to modify their objective function to be computable from off-policy samples without importance sampling. A well-established off-policy objective is the excursion objective. This work studies the difference between the excursion objective and the traditional on-policy objective, which we refer to as the on-off gap. We provide the first theoretical analysis showing conditions to reduce the on-off gap while establishing empirical evidence of shortfalls arising when these conditions are not met.</p></p class="citation"></blockquote><h3 id=36--286346-statistical-test-for-generated-hypotheses-by-diffusion-models-teruyuki-katsuoka-et-al-2024>(3/6 | 286/346) Statistical Test for Generated Hypotheses by Diffusion Models (Teruyuki Katsuoka et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Teruyuki Katsuoka, Tomohiro Shiraishi, Daiki Miwa, Vo Nguyen Le Duy, Ichiro Takeuchi. (2024)<br><strong>Statistical Test for Generated Hypotheses by Diffusion Models</strong><br><button class=copy-to-clipboard title="Statistical Test for Generated Hypotheses by Diffusion Models" index=286>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-286 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-CV, cs-LG, stat-ML, stat.ML<br>Keyword Score: 20<br>Keywords: Diffusion Model, Generative AI<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11789v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11789v1.pdf filename=2402.11789v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The enhanced performance of AI has accelerated its integration into scientific research. In particular, the use of <b>generative</b> <b>AI</b> to create scientific hypotheses is promising and is increasingly being applied across various fields. However, when employing AI-generated hypotheses for critical decisions, such as medical diagnoses, verifying their reliability is crucial. In this study, we consider a medical diagnostic task using generated images by <b>diffusion</b> <b>models,</b> and propose a statistical test to quantify its reliability. The basic idea behind the proposed statistical test is to employ a selective inference framework, where we consider a statistical test conditional on the fact that the generated images are produced by a trained <b>diffusion</b> <b>model.</b> Using the proposed method, the statistical reliability of medical image diagnostic results can be quantified in the form of a p-value, allowing for decision-making with a controlled error rate. We show the theoretical validity of the proposed statistical test and its effectiveness through numerical experiments on synthetic and brain image datasets.</p></p class="citation"></blockquote><h3 id=46--287346-towards-ai-based-precision-oncology-a-machine-learning-framework-for-personalized-counterfactual-treatment-suggestions-based-on-multi-omics-data-manuel-schürch-et-al-2024>(4/6 | 287/346) Towards AI-Based Precision Oncology: A Machine Learning Framework for Personalized Counterfactual Treatment Suggestions based on Multi-Omics Data (Manuel Schürch et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Manuel Schürch, Laura Boos, Viola Heinzelmann-Schwarz, Gabriele Gut, Michael Krauthammer, Andreas Wicki, Tumor Profiler Consortium. (2024)<br><strong>Towards AI-Based Precision Oncology: A Machine Learning Framework for Personalized Counterfactual Treatment Suggestions based on Multi-Omics Data</strong><br><button class=copy-to-clipboard title="Towards AI-Based Precision Oncology: A Machine Learning Framework for Personalized Counterfactual Treatment Suggestions based on Multi-Omics Data" index=287>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-287 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-LG, q-bio-QM, stat-ML, stat.ML<br>Keyword Score: 16<br>Keywords: Counter-factual, Multi-modal, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12190v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12190v1.pdf filename=2402.12190v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>AI-driven precision oncology has the transformative potential to reshape cancer treatment by leveraging the power of AI models to analyze the interaction between complex patient characteristics and their corresponding treatment outcomes. New technological platforms have facilitated the timely acquisition of <b>multimodal</b> data on tumor biology at an unprecedented resolution, such as single-cell multi-omics data, making this quality and quantity of data available for data-driven improved clinical decision-making. In this work, we propose a modular machine learning framework designed for personalized <b>counterfactual</b> cancer treatment suggestions based on an ensemble of machine learning experts trained on diverse multi-omics technologies. These specialized <b>counterfactual</b> experts per technology are consistently aggregated into a more powerful expert with superior performance and can provide both confidence and an explanation of its decision. The framework is tailored to address critical challenges inherent in data-driven cancer research, including the high-dimensional nature of the data, and the presence of treatment assignment bias in the retrospective observational data. The framework is showcased through comprehensive demonstrations using data from in-vitro and in-vivo treatment responses from a cohort of patients with ovarian cancer. Our method aims to empower clinicians with a reality-centric decision-support tool including probabilistic treatment suggestions with calibrated confidence and personalized explanations for tailoring treatment strategies to multi-omics characteristics of individual cancer patients.</p></p class="citation"></blockquote><h3 id=56--288346-regularization-by-denoising-bayesian-model-and-langevin-within-split-gibbs-sampling-elhadji-c-faye-et-al-2024>(5/6 | 288/346) Regularization by denoising: Bayesian model and Langevin-within-split Gibbs sampling (Elhadji C. Faye et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Elhadji C. Faye, Mame Diarra Fall, Nicolas Dobigeon. (2024)<br><strong>Regularization by denoising: Bayesian model and Langevin-within-split Gibbs sampling</strong><br><button class=copy-to-clipboard title="Regularization by denoising: Bayesian model and Langevin-within-split Gibbs sampling" index=288>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-288 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-CV, cs-LG, stat-ML, stat.ML<br>Keyword Score: 10<br>Keywords: Data Augmentation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12292v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12292v1.pdf filename=2402.12292v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper introduces a Bayesian framework for image inversion by deriving a probabilistic counterpart to the regularization-by-denoising (RED) paradigm. It additionally implements a Monte Carlo algorithm specifically tailored for sampling from the resulting posterior distribution, based on an asymptotically exact <b>data</b> <b>augmentation</b> (AXDA). The proposed algorithm is an approximate instance of split Gibbs sampling (SGS) which embeds one Langevin Monte Carlo step. The proposed method is applied to common imaging tasks such as deblurring, inpainting and super-resolution, demonstrating its efficacy through extensive numerical experiments. These contributions advance Bayesian inference in imaging by leveraging <b>data-driven</b> <b>regularization</b> strategies within a probabilistic framework.</p></p class="citation"></blockquote><h3 id=66--289346-asymptotic-gaussian-fluctuations-of-eigenvectors-in-spectral-clustering-hugo-lebeau-et-al-2024>(6/6 | 289/346) Asymptotic Gaussian Fluctuations of Eigenvectors in Spectral Clustering (Hugo Lebeau et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hugo Lebeau, Florent Chatelain, Romain Couillet. (2024)<br><strong>Asymptotic Gaussian Fluctuations of Eigenvectors in Spectral Clustering</strong><br><button class=copy-to-clipboard title="Asymptotic Gaussian Fluctuations of Eigenvectors in Spectral Clustering" index=289>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-289 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-LG, math-PR, stat-ML, stat.ML<br>Keyword Score: 3<br>Keywords: Clustering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12302v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12302v1.pdf filename=2402.12302v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The performance of spectral <b>clustering</b> relies on the fluctuations of the entries of the eigenvectors of a similarity matrix, which has been left uncharacterized until now. In this letter, it is shown that the signal $+$ noise structure of a general spike random matrix model is transferred to the eigenvectors of the corresponding Gram kernel matrix and the fluctuations of their entries are Gaussian in the large-dimensional regime. This CLT-like result was the last missing piece to precisely predict the classification performance of spectral <b>clustering.</b> The proposed proof is very general and relies solely on the rotational invariance of the noise. Numerical experiments on synthetic and real data illustrate the universality of this phenomenon.</p></p class="citation"></blockquote><h2 id=csgt-3>cs.GT (3)</h2><h3 id=13--290346-automated-security-response-through-online-learning-with-adaptive-conjectures-kim-hammar-et-al-2024>(1/3 | 290/346) Automated Security Response through Online Learning with Adaptive Conjectures (Kim Hammar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kim Hammar, Tao Li, Rolf Stadler, Quanyan Zhu. (2024)<br><strong>Automated Security Response through Online Learning with Adaptive Conjectures</strong><br><button class=copy-to-clipboard title="Automated Security Response through Online Learning with Adaptive Conjectures" index=290>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-290 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.GT<br>Categories: cs-AI, cs-CR, cs-GT, cs-LG, cs-SY, cs.GT, eess-SY<br>Keyword Score: 30<br>Keywords: Reinforcement Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12499v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12499v1.pdf filename=2402.12499v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We study automated security response for an IT infrastructure and formulate the interaction between an attacker and a defender as a partially observed, non-stationary game. We relax the standard assumption that the game model is correctly specified and consider that each player has a probabilistic conjecture about the model, which may be misspecified in the sense that the true model has probability 0. This formulation allows us to capture uncertainty about the infrastructure and the intents of the players. To learn effective game strategies online, we design a novel method where a player iteratively adapts its conjecture using Bayesian learning and updates its strategy through rollout. We prove that the conjectures converge to best fits, and we provide a bound on the performance improvement that rollout enables with a conjectured model. To characterize the steady state of the game, we propose a variant of the Berk-Nash equilibrium. We present our method through an advanced persistent threat use case. <b>Simulation</b> studies based on testbed measurements show that our method produces effective security strategies that adapt to a changing environment. We also find that our method enables faster convergence than current <b>reinforcement</b> <b>learning</b> techniques.</p></p class="citation"></blockquote><h3 id=23--291346-integrating-dynamic-weighted-approach-with-fictitious-play-and-pure-counterfactual-regret-minimization-for-equilibrium-finding-qi-ju-et-al-2024>(2/3 | 291/346) Integrating Dynamic Weighted Approach with Fictitious Play and Pure Counterfactual Regret Minimization for Equilibrium Finding (Qi Ju et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qi Ju, Falin Hei, Zhemei Fang, Yunfeng Luo. (2024)<br><strong>Integrating Dynamic Weighted Approach with Fictitious Play and Pure Counterfactual Regret Minimization for Equilibrium Finding</strong><br><button class=copy-to-clipboard title="Integrating Dynamic Weighted Approach with Fictitious Play and Pure Counterfactual Regret Minimization for Equilibrium Finding" index=291>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-291 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.GT<br>Categories: cs-GT, cs.GT<br>Keyword Score: 10<br>Keywords: Counter-factual<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12164v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12164v1.pdf filename=2402.12164v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Developing efficient algorithms to converge to Nash Equilibrium is a key focus in game theory. The use of dynamic weighting has been especially advantageous in normal-form games, enhancing the rate of convergence. For instance, the Greedy Regret Minimization (RM) algorithm has markedly outperformed earlier techniques. Nonetheless, its dependency on mixed strategies throughout the iterative process introduces complexity to dynamic weighting, which in turn restricts its use in extensive-form games. In this study, we introduce two novel dynamic weighting algorithms: Dynamic Weighted Fictitious Play (DW-FP) and Dynamic Weighted Pure <b>Counterfactual</b> Regret Minimization (DW-PCFR). These algorithms, utilizing pure strategies in each iteration, offer key benefits: (i) Addressing the complexity of dynamic weight computation in Greedy RM, thereby facilitating application in extensive-form games; (ii) Incorporating the low-memory usage and ease-of-use features of FP and CFR; (iii) They guarantee a convergence lower bound of $\mathcal{O}\left(T^{-\frac{1}{2}}\right)$, with a tendency to achieve a convergence rate of $\mathcal{O}(T^{-1})$ as runtime increases. This research not only theoretically affirms the convergence capabilities of these algorithms but also empirically demonstrates their superiority over existing leading algorithms across all our tests.</p></p class="citation"></blockquote><h3 id=33--292346-simple-mechanisms-for-utility-maximization-approximating-welfare-in-the-iid-unit-demand-setting-kira-goldner-et-al-2024>(3/3 | 292/346) Simple Mechanisms for Utility Maximization: Approximating Welfare in the I.I.D. Unit-Demand Setting (Kira Goldner et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kira Goldner, Taylor Lundy. (2024)<br><strong>Simple Mechanisms for Utility Maximization: Approximating Welfare in the I.I.D. Unit-Demand Setting</strong><br><button class=copy-to-clipboard title="Simple Mechanisms for Utility Maximization: Approximating Welfare in the I.I.D. Unit-Demand Setting" index=292>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-292 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.GT<br>Categories: cs-GT, cs.GT<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12340v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12340v1.pdf filename=2402.12340v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We investigate the objective of utility maximization from the perspective of Bayesian mechanism design, initiating this direction, and focus on the unit-demand setting where values are i.i.d. across both items and buyers. We take the approach of developing simple, approximately optimal mechanisms, targeting the simplest <b>benchmark</b> of optimal welfare. We give a $(1-1/e)$-approximation when there are more items than buyers, and an $O(\log(n/m))$-approximation when there are more buyers than items, which is tight up to constant factors. We also characterize complexities in this setting that defy our intuition from the welfare and revenue literature, and motivate why coming up with a better <b>benchmark</b> than welfare is a hard problem itself.</p></p class="citation"></blockquote><h2 id=astro-phep-1>astro-ph.EP (1)</h2><h3 id=11--293346-dbnets-a-publicly-available-deep-learning-tool-to-measure-the-masses-of-young-planets-in-dusty-protoplanetary-discs-alessandro-ruzza-et-al-2024>(1/1 | 293/346) DBNets: A publicly available deep learning tool to measure the masses of young planets in dusty protoplanetary discs (Alessandro Ruzza et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Alessandro Ruzza, Giuseppe Lodato, Giovanni Pietro Rosotti. (2024)<br><strong>DBNets: A publicly available deep learning tool to measure the masses of young planets in dusty protoplanetary discs</strong><br><button class=copy-to-clipboard title="DBNets: A publicly available deep learning tool to measure the masses of young planets in dusty protoplanetary discs" index=293>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-293 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: astro-ph.EP<br>Categories: astro-ph-EP, astro-ph-IM, astro-ph.EP, cs-LG<br>Keyword Score: 30<br>Keywords: Convolution, Convolutional Neural Network, Out-of-distribution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12448v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12448v1.pdf filename=2402.12448v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Current methods to characterize embedded planets in protoplanetary disc observations are severely limited either in their ability to fully account for the observed complex physics or in their computational and time costs. To address this shortcoming, we developed DBNets: a deep learning tool, based on <b>convolutional</b> <b>neural</b> <b>networks,</b> that analyses substructures observed in the dust continuum emission of protoplanetary discs to quickly infer the mass of allegedly embedded planets. We focussed on developing a method to reliably quantify not only the planet mass, but also the associated uncertainty introduced by our modelling and adopted techniques. Our tests gave promising results achieving an 87% reduction of the log Mp mean squared error with respect to an analytical formula fitted on the same data (DBNets metrics: lmse 0.016, r2-score 97%). With the goal of providing the final user of DBNets with all the tools needed to interpret their measurements and decide on their significance, we extensively tested our tool on <b>out-of-distribution</b> data. We found that DBNets can identify inputs strongly outside its training scope returning an uncertainty above a specific threshold and we thus provided a rejection criterion that helps determine the significance of the results obtained. Additionally, we outlined some limitations of our tool: it can be reliably applied only on discs observed with inclinations below approximately 60{\deg}, in the optically thin regime, with a resolution 8 times better than the gap radial location and with a signal-to-noise ratio higher than approximately ten. Finally, we applied DBNets to 33 actual observations of protoplanetary discs measuring the mass of 48 proposed planets and comparing our results with the available literature. We confirmed that most of the observed gaps imply planets in the sub-Jupiter regime. DBNets is publicly available at dbnets.fisica.unimi.it.</p></p class="citation"></blockquote><h2 id=eesssy-5>eess.SY (5)</h2><h3 id=15--294346-an-adversarial-approach-to-evaluating-the-robustness-of-event-identification-models-obai-bahwal-et-al-2024>(1/5 | 294/346) An Adversarial Approach to Evaluating the Robustness of Event Identification Models (Obai Bahwal et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Obai Bahwal, Oliver Kosut, Lalitha Sankar. (2024)<br><strong>An Adversarial Approach to Evaluating the Robustness of Event Identification Models</strong><br><button class=copy-to-clipboard title="An Adversarial Approach to Evaluating the Robustness of Event Identification Models" index=294>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-294 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-CR, cs-LG, cs-SY, eess-SY, eess.SY<br>Keyword Score: 30<br>Keywords: Logistic Regression, Event Detection, Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12338v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12338v1.pdf filename=2402.12338v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Intelligent machine learning approaches are finding active use for <b>event</b> <b>detection</b> and identification that allow real-time situational awareness. Yet, such machine learning algorithms have been shown to be susceptible to <b>adversarial</b> <b>attacks</b> on the incoming telemetry data. This paper considers a physics-based modal decomposition method to extract features for <b>event</b> <b>classification</b> and focuses on interpretable classifiers including <b>logistic</b> <b>regression</b> and gradient boosting to distinguish two types of <b>events:</b> <b>load</b> loss and generation loss. The resulting classifiers are then tested against an <b>adversarial</b> <b>algorithm</b> to evaluate their robustness. The <b>adversarial</b> <b>attack</b> is tested in two settings: the white box setting, wherein the attacker knows exactly the classification model; and the gray box setting, wherein the attacker has access to historical data from the same network as was used to train the classifier, but does not know the classification model. Thorough experiments on the synthetic South Carolina 500-bus system highlight that a relatively simpler model such as <b>logistic</b> <b>regression</b> is more susceptible to <b>adversarial</b> <b>attacks</b> than gradient boosting.</p></p class="citation"></blockquote><h3 id=25--295346-an-index-policy-based-on-sarsa-and-q-learning-for-heterogeneous-smart-target-tracking-yuhang-hao-et-al-2024>(2/5 | 295/346) An Index Policy Based on Sarsa and Q-learning for Heterogeneous Smart Target Tracking (Yuhang Hao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuhang Hao, Zengfu Wang, Jing Fu, Quan Pan. (2024)<br><strong>An Index Policy Based on Sarsa and Q-learning for Heterogeneous Smart Target Tracking</strong><br><button class=copy-to-clipboard title="An Index Policy Based on Sarsa and Q-learning for Heterogeneous Smart Target Tracking" index=295>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-295 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-LG, cs-SY, eess-SY, eess.SY<br>Keyword Score: 23<br>Keywords: Bandit Algorithm, Benchmarking, Markov Decision Process<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12015v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12015v1.pdf filename=2402.12015v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In solving the non-myopic radar scheduling for multiple smart target tracking within an active and passive radar network, we need to consider both short-term enhanced tracking performance and a higher probability of target maneuvering in the future with active tracking. Acquiring the long-term tracking performance while scheduling the beam resources of active and passive radars poses a challenge. To address this challenge, we model this problem as a <b>Markov</b> <b>decision</b> <b>process</b> consisting of parallel restless <b>bandit</b> processes. Each <b>bandit</b> process is associated with a smart target, of which the estimation state evolves according to different discrete dynamic models for different actions - whether or not the target is being tracked. The discrete state is defined by the dynamic mode. The problem exhibits the curse of dimensionality, where optimal solutions are in general intractable. We resort to heuristics through the famous restless multi-armed <b>bandit</b> techniques. It follows with efficient scheduling policies based on the indices that are real numbers representing the marginal rewards of taking different actions. For the inevitable practical case with unknown transition matrices, we propose a new method that utilizes the forward Sarsa and backward Q-learning to approximate the indices through adapting the state-action value functions, or equivalently the Q-functions, and propose a new policy, namely ISQ, aiming to maximize the long-term tracking rewards. Numerical results demonstrate that the proposed ISQ policy outperforms conventional Q-learning-based methods and rapidly converges to the well-known Whittle index policy with revealed state transition models, which is considered the <b>benchmark.</b></p></p class="citation"></blockquote><h3 id=35--296346-impact-of-data-usage-for-forecasting-on-performance-of-model-predictive-control-in-buildings-with-smart-energy-storage-max-langtry-et-al-2024>(3/5 | 296/346) Impact of data usage for forecasting on performance of model predictive control in buildings with smart energy storage (Max Langtry et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Max Langtry, Vijja Wichitwechkarn, Rebecca Ward, Chaoqun Zhuang, Monika J. Kreitmair, Nikolas Makasis, Zack Xuereb Conti, Ruchi Choudhary. (2024)<br><strong>Impact of data usage for forecasting on performance of model predictive control in buildings with smart energy storage</strong><br><button class=copy-to-clipboard title="Impact of data usage for forecasting on performance of model predictive control in buildings with smart energy storage" index=296>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-296 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-LG, cs-SY, eess-SY, eess.SY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12539v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12539v1.pdf filename=2402.12539v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Data is required to develop forecasting models for use in Model Predictive Control (MPC) schemes in building energy systems. However, data usage incurs costs from both its collection and exploitation. Determining cost optimal data usage requires understanding of the forecast accuracy and resulting MPC operational performance it enables. This study investigates the performance of both simple and state-of-the-art machine learning prediction models for MPC in a multi-building energy system <b>simulation</b> using historic building energy data. The impact of data usage on forecast accuracy is quantified for the following data efficiency measures: reuse of prediction models, reduction of training data volumes, reduction of model data features, and online model training. A simple linear multi-layer perceptron model is shown to provide equivalent forecast accuracy to state-of-the-art models, with greater data efficiency and generalisability. The use of more than 2 years of training data for load prediction models provided no significant improvement in forecast accuracy. Forecast accuracy and data efficiency were improved simultaneously by using change-point analysis to screen training data. Reused models and those trained with 3 months of data had on average 10% higher error than baseline, indicating that deploying MPC systems without prior data collection may be economic.</p></p class="citation"></blockquote><h3 id=45--297346-flexible-robust-optimal-bidding-of-renewable-virtual-power-plants-in-sequential-markets-hadi-nemati-et-al-2024>(4/5 | 297/346) Flexible Robust Optimal Bidding of Renewable Virtual Power Plants in Sequential Markets (Hadi Nemati et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hadi Nemati, Pedro Sánchez-Martín, Álvaro Ortega, Lukas Sigrist, Enrique Lobato, Luis Rouco. (2024)<br><strong>Flexible Robust Optimal Bidding of Renewable Virtual Power Plants in Sequential Markets</strong><br><button class=copy-to-clipboard title="Flexible Robust Optimal Bidding of Renewable Virtual Power Plants in Sequential Markets" index=297>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-297 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12032v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12032v1.pdf filename=2402.12032v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, a novel approach to define the optimal bidding of renewable-only virtual power plants (RVPPs) in the day-ahead, secondary reserve, and intra-day markets is proposed. To this aim, a robust optimization algorithm is developed to account for the asymmetric nature of the uncertainties that characterize the market prices, as well as the energy production of the RVPP stochastic sources and flexible demand consumption. <b>Simulation</b> results show increased RVPP benefits compared to other existing solutions and demonstrate the potential of renewable sources to further increase their economic competitiveness. The simplicity of the implementation, the computational efficiency, and the flexible robustness are also verified.</p></p class="citation"></blockquote><h3 id=55--298346-terahertz-user-centric-clustering-in-the-presence-of-beam-misalignment-khaled-humadi-et-al-2024>(5/5 | 298/346) Terahertz User-Centric Clustering in the Presence of Beam Misalignment (Khaled Humadi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Khaled Humadi, Imene Trigui, Wei-Ping Zhu, Wessam Ajib. (2024)<br><strong>Terahertz User-Centric Clustering in the Presence of Beam Misalignment</strong><br><button class=copy-to-clipboard title="Terahertz User-Centric Clustering in the Presence of Beam Misalignment" index=298>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-298 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SP, eess-SY, eess.SY<br>Keyword Score: 3<br>Keywords: Clustering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11834v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11834v1.pdf filename=2402.11834v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Beam misalignment is one of the main challenges for the design of reliable wireless systems in terahertz (THz) bands. This paper investigates how to apply user-centric base station (BS) <b>clustering</b> as a valuable add-on in THz networks. In particular, to reduce the impact of beam misalignment, a user-centric BS <b>clustering</b> design that provides multi-connectivity via BS cooperation is investigated. The coverage probability is derived by leveraging an accurate approximation of the aggregate interference distribution that captures the effect of beam misalignment and THz fading. The numerical results reveal the impact of beam misalignment with respect to crucial link parameters, such as the transmitter&rsquo;s beam width and the serving cluster size, demonstrating that user-centric BS <b>clustering</b> is a promising enabler of THz networks.</p></p class="citation"></blockquote><h2 id=csdc-5>cs.DC (5)</h2><h3 id=15--299346-secure-federated-learning-across-heterogeneous-cloud-and-high-performance-computing-resources----a-case-study-on-federated-fine-tuning-of-llama-2-zilinghan-li-et-al-2024>(1/5 | 299/346) Secure Federated Learning Across Heterogeneous Cloud and High-Performance Computing Resources &ndash; A Case Study on Federated Fine-tuning of LLaMA 2 (Zilinghan Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zilinghan Li, Shilan He, Pranshu Chaturvedi, Volodymyr Kindratenko, Eliu A Huerta, Kibaek Kim, Ravi Madduri. (2024)<br><strong>Secure Federated Learning Across Heterogeneous Cloud and High-Performance Computing Resources &ndash; A Case Study on Federated Fine-tuning of LLaMA 2</strong><br><button class=copy-to-clipboard title="Secure Federated Learning Across Heterogeneous Cloud and High-Performance Computing Resources -- A Case Study on Federated Fine-tuning of LLaMA 2" index=299>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-299 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DC<br>Categories: cs-DC, cs-LG, cs.DC<br>Keyword Score: 30<br>Keywords: Federated Learning, Fine-tuning, LLaMA<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12271v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12271v1.pdf filename=2402.12271v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Federated</b> <b>learning</b> enables multiple data owners to collaboratively train robust machine learning models without transferring large or sensitive local datasets by only sharing the parameters of the locally trained models. In this paper, we elaborate on the design of our Advanced Privacy-Preserving <b>Federated</b> <b>Learning</b> (APPFL) framework, which streamlines end-to-end secure and reliable <b>federated</b> <b>learning</b> experiments across cloud computing facilities and high-performance computing resources by leveraging Globus Compute, a distributed function as a service platform, and Amazon Web Services. We further demonstrate the use case of APPFL in <b>fine-tuning</b> a <b>LLaMA</b> 2 7B model using several cloud resources and supercomputers.</p></p class="citation"></blockquote><h3 id=25--300346-proximal-byzantine-consensus-roy-shadmon-et-al-2024>(2/5 | 300/346) Proximal Byzantine Consensus (Roy Shadmon et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Roy Shadmon, Daniel Spencer, Owen Arden. (2024)<br><strong>Proximal Byzantine Consensus</strong><br><button class=copy-to-clipboard title="Proximal Byzantine Consensus" index=300>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-300 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DC<br>Categories: cs-DC, cs.DC<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12577v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12577v1.pdf filename=2402.12577v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Distributed control systems require high reliability and availability guarantees despite often being deployed at the edge of network infrastructure. Edge computing resources are less secure and less reliable than centralized resources in data centers. Replication and consensus protocols improve robustness to network faults and crashed or corrupted nodes, but these volatile environments can cause non-faulty nodes to temporarily diverge, increasing the time needed for replicas to converge on a consensus value, and give Byzantine attackers too much influence over the convergence process. This paper proposes proximal Byzantine consensus, a new approximate consensus protocol where clients use statistical models of streaming computations to decide a consensus value. In addition, it provides an interval around the decision value and the probability that the true (non-faulty, noise-free) value falls within this interval. Proximal consensus (PC) tolerates unreliable network conditions, Byzantine behavior, and other sources of noise that cause honest replica states to diverge. We evaluate our approach for scalar values, and compare PC <b>simulations</b> against a vector consensus (VC) protocol <b>simulation.</b> Our <b>simulations</b> demonstrate that consensus values selected by PC have lower error and are more robust against Byzantine attacks. We formally characterize the security guarantees against Byzantine attacks and demonstrate attacker influence is bound with high probability. Additionally, an informal complexity analysis suggests PC scales better to higher dimensions than convex hull-based protocols such as VC.</p></p class="citation"></blockquote><h3 id=35--301346-even-cycle-detection-in-the-randomized-and-quantum-congest-model-pierre-fraigniaud-et-al-2024>(3/5 | 301/346) Even-Cycle Detection in the Randomized and Quantum CONGEST Model (Pierre Fraigniaud et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Pierre Fraigniaud, Mael Luce, Frederic Magniez, Ioan Todinca. (2024)<br><strong>Even-Cycle Detection in the Randomized and Quantum CONGEST Model</strong><br><button class=copy-to-clipboard title="Even-Cycle Detection in the Randomized and Quantum CONGEST Model" index=301>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-301 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DC<br>Categories: cs-DC, cs.DC, quant-ph<br>Keyword Score: 20<br>Keywords: Quantization, Quantization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12018v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12018v1.pdf filename=2402.12018v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We show that, for every $k\geq 2$, $C_{2k}$-freeness can be decided in $O(n^{1-1/k})$ rounds in the \CONGEST{} model by a randomized Monte-Carlo distributed algorithm with one-sided error probability $1/3$. This matches the best round-complexities of previously known algorithms for $k\in{2,3,4,5}$ by Drucker et al. [PODC'14] and Censor-Hillel et al. [DISC'20], but improves the complexities of the known algorithms for $k>5$ by Eden et al. [DISC'19], which were essentially of the form $\tilde O(n^{1-2/k^2})$. Our algorithm uses colored BFS-explorations with threshold, but with an original \emph{global} approach that enables to overcome a recent impossibility result by Fraigniaud et al. [SIROCCO'23] about using colored BFS-exploration with \emph{local} threshold for detecting cycles. We also show how to <b>quantize</b> our algorithm for achieving a round-complexity $\tilde O(n^{\frac{1}{2}-\frac{1}{2k}})$ in the quantum setting for deciding $C_{2k}$ freeness. Furthermore, this allows us to improve the known quantum complexities of the simpler problem of detecting cycles of length \emph{at most}~$2k$ by van Apeldoorn and de Vos [PODC'22]. Our <b>quantization</b> is in two steps. First, the congestion of our randomized algorithm is reduced, to the cost of reducing its success probability too. Second, the success probability is boosted using a new quantum framework derived from sequential algorithms, namely Monte-Carlo quantum amplification.</p></p class="citation"></blockquote><h3 id=45--302346-local-certification-of-forbidden-subgraphs-nicolas-bousquet-et-al-2024>(4/5 | 302/346) Local certification of forbidden subgraphs (Nicolas Bousquet et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nicolas Bousquet, Linda Cook, Laurent Feuilloley, Théo Pierron, Sébastien Zeitoun. (2024)<br><strong>Local certification of forbidden subgraphs</strong><br><button class=copy-to-clipboard title="Local certification of forbidden subgraphs" index=302>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-302 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DC<br>Categories: cs-DC, cs-DM, cs-DS, cs.DC<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12148v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12148v1.pdf filename=2402.12148v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Detecting specific structures in a network has been a very active theme of research in distributed computing for at least a decade. In this paper, we start the study of subgraph detection from the perspective of local certification. Remember that a local certification is a distributed mechanism enabling the nodes of a network to check the correctness of the current configuration, thanks to small pieces of information called certificates. Our main question is: For a given <b>graph</b> $H$, what is the minimum certificate size that allows checking that the network does not contain $H$ as a (possibly induced) subgraph? We show a variety of lower and upper bounds, uncovering an interesting interplay between the optimal certificate size, the size of the forbidden subgraph, and the locality of the verification. Along the way we introduce several new technical tools, in particular what we call the \emph{layered map}, which is not specific to forbidden subgraphs and that we expect to be useful for certifying many other properties.</p></p class="citation"></blockquote><h3 id=55--303346-evaluating-versal-ai-engines-for-option-price-discovery-in-market-risk-analysis-mark-klaisoongnoen-et-al-2024>(5/5 | 303/346) Evaluating Versal AI Engines for option price discovery in market risk analysis (Mark Klaisoongnoen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mark Klaisoongnoen, Nick Brown, Tim Dykes, Jessica R. Jones, Utz-Uwe Haus. (2024)<br><strong>Evaluating Versal AI Engines for option price discovery in market risk analysis</strong><br><button class=copy-to-clipboard title="Evaluating Versal AI Engines for option price discovery in market risk analysis" index=303>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-303 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DC<br>Categories: cs-DC, cs.DC<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12111v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12111v1.pdf filename=2402.12111v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Whilst Field-Programmable Gate Arrays (FPGAs) have been popular in accelerating high-frequency financial workload for many years, their application in quantitative finance, the utilisation of mathematical models to analyse financial markets and securities, is less mature. Nevertheless, recent work has demonstrated the benefits that FPGAs can deliver to quantitative workloads, and in this paper, we study whether the Versal ACAP and its AI Engines (AIEs) can also deliver improved performance. We focus specifically on the industry standard Strategic Technology Analysis Center&rsquo;s (STAC) derivatives risk analysis <b>benchmark</b> STAC-A2. Porting a purely FPGA-based accelerator STAC-A2 inspired market risk (SIMR) <b>benchmark</b> to the Versal ACAP device by combining Programmable Logic (PL) and AIEs, we explore the development approach and techniques, before comparing performance across PL and AIEs. Ultimately, we found that our AIE approach is slower than a highly optimised existing PL-only version due to limits on both the AIE and PL that we explore and describe.</p></p class="citation"></blockquote><h2 id=csne-3>cs.NE (3)</h2><h3 id=13--304346-hebbian-learning-based-orthogonal-projection-for-continual-learning-of-spiking-neural-networks-mingqing-xiao-et-al-2024>(1/3 | 304/346) Hebbian Learning based Orthogonal Projection for Continual Learning of Spiking Neural Networks (Mingqing Xiao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mingqing Xiao, Qingyan Meng, Zongpeng Zhang, Di He, Zhouchen Lin. (2024)<br><strong>Hebbian Learning based Orthogonal Projection for Continual Learning of Spiking Neural Networks</strong><br><button class=copy-to-clipboard title="Hebbian Learning based Orthogonal Projection for Continual Learning of Spiking Neural Networks" index=304>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-304 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NE<br>Categories: cs-AI, cs-LG, cs-NE, cs.NE<br>Keyword Score: 30<br>Keywords: Continual Learning, Supervised Learning, Grounding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11984v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11984v1.pdf filename=2402.11984v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Neuromorphic computing with spiking neural networks is promising for energy-efficient artificial intelligence (AI) applications. However, different from humans who continually learn different tasks in a lifetime, neural network models suffer from catastrophic forgetting. How could neuronal operations solve this problem is an important question for AI and neuroscience. Many previous studies draw inspiration from observed neuroscience phenomena and propose episodic replay or synaptic metaplasticity, but they are not guaranteed to explicitly preserve knowledge for neuron populations. Other works focus on machine learning methods with more mathematical <b>grounding,</b> e.g., orthogonal projection on high dimensional spaces, but there is no neural correspondence for neuromorphic computing. In this work, we develop a new method with neuronal operations based on lateral connections and Hebbian learning, which can protect knowledge by projecting activity traces of neurons into an orthogonal subspace so that synaptic weight update will not interfere with old tasks. We show that Hebbian and anti-Hebbian learning on recurrent lateral connections can effectively extract the principal subspace of neural activities and enable orthogonal projection. This provides new insights into how neural circuits and Hebbian learning can help <b>continual</b> <b>learning,</b> and also how the concept of orthogonal projection can be realized in neuronal systems. Our method is also flexible to utilize arbitrary training methods based on presynaptic activities/traces. Experiments show that our method consistently solves forgetting for spiking neural networks with nearly zero forgetting under various <b>supervised</b> training methods with different error propagation approaches, and outperforms previous approaches under various settings. Our method can pave a solid path for building <b>continual</b> <b>neuromorphic</b> computing systems.</p></p class="citation"></blockquote><h3 id=23--305346-an-enhanced-teaching-learning-based-optimization-tlbo-with-grey-wolf-optimizer-gwo-for-text-feature-selection-and-clustering-mahsa-azarshab-et-al-2024>(2/3 | 305/346) An enhanced Teaching-Learning-Based Optimization (TLBO) with Grey Wolf Optimizer (GWO) for text feature selection and clustering (Mahsa Azarshab et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mahsa Azarshab, Mohammad Fathian, Babak Amiri. (2024)<br><strong>An enhanced Teaching-Learning-Based Optimization (TLBO) with Grey Wolf Optimizer (GWO) for text feature selection and clustering</strong><br><button class=copy-to-clipboard title="An enhanced Teaching-Learning-Based Optimization (TLBO) with Grey Wolf Optimizer (GWO) for text feature selection and clustering" index=305>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-305 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NE<br>Categories: cs-CL, cs-LG, cs-NE, cs.NE<br>Keyword Score: 16<br>Keywords: Benchmarking, Clustering, Text Clustering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11839v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11839v1.pdf filename=2402.11839v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Text</b> <b>document</b> <b>clustering</b> can play a vital role in organizing and handling the everincreasing number of <b>text</b> <b>documents.</b> Uninformative and redundant features included in large <b>text</b> <b>documents</b> reduce the effectiveness of the <b>clustering</b> algorithm. Feature selection (FS) is a well-known technique for removing these features. Since FS can be formulated as an optimization problem, various meta-heuristic algorithms have been employed to solve it. Teaching-Learning-Based Optimization (TLBO) is a novel meta-heuristic algorithm that benefits from the low number of parameters and fast convergence. A hybrid method can simultaneously benefit from the advantages of TLBO and tackle the possible entrapment in the local optimum. By proposing a hybrid of TLBO, Grey Wolf Optimizer (GWO), and Genetic Algorithm (GA) operators, this paper suggests a filter-based FS algorithm (TLBO-GWO). Six <b>benchmark</b> datasets are selected, and TLBO-GWO is compared with three recently proposed FS algorithms with similar approaches, the main TLBO and GWO. The comparison is conducted based on <b>clustering</b> evaluation measures, convergence behavior, and dimension reduction, and is validated using statistical tests. The results reveal that TLBO-GWO can significantly enhance the effectiveness of the <b>text</b> <b>clustering</b> technique (K-means).</p></p class="citation"></blockquote><h3 id=33--306346-function-class-learning-with-genetic-programming-towards-explainable-meta-learning-for-tumor-growth-functionals-e-m-c-sijben-et-al-2024>(3/3 | 306/346) Function Class Learning with Genetic Programming: Towards Explainable Meta Learning for Tumor Growth Functionals (E. M. C. Sijben et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>E. M. C. Sijben, J. C. Jansen, P. A. N. Bosman, T. Alderliesten. (2024)<br><strong>Function Class Learning with Genetic Programming: Towards Explainable Meta Learning for Tumor Growth Functionals</strong><br><button class=copy-to-clipboard title="Function Class Learning with Genetic Programming: Towards Explainable Meta Learning for Tumor Growth Functionals" index=306>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-306 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NE<br>Categories: cs-NE, cs.NE<br>Keyword Score: 13<br>Keywords: Meta Learning, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12510v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12510v1.pdf filename=2402.12510v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Paragangliomas are rare, primarily slow-growing tumors for which the underlying growth pattern is unknown. Therefore, determining the best care for a patient is hard. Currently, if no significant tumor growth is observed, treatment is often delayed, as treatment itself is not without risk. However, by doing so, the risk of (irreversible) adverse effects due to tumor growth may increase. Being able to predict the growth accurately could assist in determining whether a patient will need treatment during their lifetime and, if so, the timing of this treatment. The aim of this work is to learn the general underlying growth pattern of paragangliomas from multiple tumor growth data sets, in which each data set contains a tumor&rsquo;s volume over time. To do so, we propose a novel approach based on genetic programming to learn a function class, i.e., a parameterized function that can be fit anew for each tumor. We do so in a unique, <b>multi-modal,</b> multi-objective fashion to find multiple potentially interesting function classes in a single run. We evaluate our approach on a synthetic and a real-world data set. By analyzing the resulting function classes, we can effectively explain the general patterns in the data.</p></p class="citation"></blockquote><h2 id=csit-3>cs.IT (3)</h2><h3 id=13--307346-cooperative-backscatter-communications-with-reconfigurable-intelligent-surfaces-an-apsk-approach-qiang-li-et-al-2024>(1/3 | 307/346) Cooperative Backscatter Communications with Reconfigurable Intelligent Surfaces: An APSK Approach (Qiang Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qiang Li, Yehuai Feng, Miaowen Wen, Jinming Wen, George C. Alexandropoulos, Ertugrul Basar, H. Vincent Poor. (2024)<br><strong>Cooperative Backscatter Communications with Reconfigurable Intelligent Surfaces: An APSK Approach</strong><br><button class=copy-to-clipboard title="Cooperative Backscatter Communications with Reconfigurable Intelligent Surfaces: An APSK Approach" index=307>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-307 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-ET, cs-IT, cs.IT, math-IT<br>Keyword Score: 23<br>Keywords: Benchmarking, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11870v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11870v1.pdf filename=2402.11870v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, a novel amplitude phase shift keying (APSK) modulation scheme for cooperative backscatter communications aided by a reconfigurable intelligent surface (RIS-CBC) is presented, according to which the RIS is configured to modulate backscatter information onto unmodulated or PSK-modulated signals impinging on its surface via APSK. We consider both passive and active RISs, with the latter including an amplification unit at each reflecting element. In the passive (resp. active) RIS-CBC-APSK, backscatter information is conveyed through the number of RIS reflecting elements being on the ON state (resp. active mode) and their phase shift values. By using the optimal APSK constellation to ensure that reflected signals from the RIS undergo APSK modulation, a bit-mapping mechanism is presented. Assuming maximum-likelihood detection, we also present closed-form upper bounds for the symbol error rate (SER) performance for both passive and active RIS-CBC-APSK schemes over Rician fading channels. In addition, we devise a low-complexity detector that can achieve flexible trade-offs between performance and complexity. Finally, we extend RIS-CBC-APSK to multiple-input single-output scenarios and present an alternating optimization approach for the joint design of transmit beamforming and RIS reflection. Our extensive <b>simulation</b> results on the SER performance corroborate our conducted performance analysis and showcase the superiority of the proposed RIS-CBC-APSK schemes over the state-of-the-art RIS-CBC <b>benchmarks.</b></p></p class="citation"></blockquote><h3 id=23--308346-rate-splitting-multiple-access-for-transmissive-reconfigurable-intelligent-surface-transceiver-empowered-isac-system-ziwei-liu-et-al-2024>(2/3 | 308/346) Rate-Splitting Multiple Access for Transmissive Reconfigurable Intelligent Surface Transceiver Empowered ISAC System (Ziwei Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ziwei Liu, Wen Chen, Qingqing Wu, Jinhong Yuan, Shanshan Zhang, Zhendong Li, Jun Li. (2024)<br><strong>Rate-Splitting Multiple Access for Transmissive Reconfigurable Intelligent Surface Transceiver Empowered ISAC System</strong><br><button class=copy-to-clipboard title="Rate-Splitting Multiple Access for Transmissive Reconfigurable Intelligent Surface Transceiver Empowered ISAC System" index=308>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-308 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, eess-SP, math-IT<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12127v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12127v1.pdf filename=2402.12127v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, a novel transmissive reconfigurable intelligent surface (TRIS) transceiver empowered integrated sensing and communications (ISAC) system is proposed for future multi-demand terminals. To address interference management, we implement rate-splitting multiple access (RSMA), where the common stream is independently designed for the sensing service. We introduce the sensing quality of service (QoS) criteria based on this structure and construct an optimization problem with the sensing QoS criteria as the objective function to optimize the sensing stream precoding matrix and the communication stream precoding matrix. Due to the coupling of optimization variables, the formulated problem is a non-convex optimization problem that cannot be solved directly. To tackle the above-mentioned challenging problem, alternating optimization (AO) is utilized to decouple the optimization variables. Specifically, the problem is decoupled into three subproblems about the sensing stream precoding matrix, the communication stream precoding matrix, and the auxiliary variables, which is solved alternatively through AO until the convergence is reached. For solving the problem, successive convex approximation (SCA) is applied to deal with the sum-rate threshold constraints on communications, and difference-of-convex (DC) programming is utilized to solve rank-one non-convex constraints. Numerical <b>simulation</b> results verify the superiority of the proposed scheme in terms of improving the communication and sensing QoS.</p></p class="citation"></blockquote><h3 id=33--309346-max-min-fairness-for-uplink-rate-splitting-multiple-access-with-finite-blocklength-jiawei-xu-et-al-2024>(3/3 | 309/346) Max-Min Fairness for Uplink Rate-Splitting Multiple Access with Finite Blocklength (Jiawei Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiawei Xu, Yijie Mao. (2024)<br><strong>Max-Min Fairness for Uplink Rate-Splitting Multiple Access with Finite Blocklength</strong><br><button class=copy-to-clipboard title="Max-Min Fairness for Uplink Rate-Splitting Multiple Access with Finite Blocklength" index=309>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-309 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, math-IT<br>Keyword Score: 10<br>Keywords: Fairness<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12066v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12066v1.pdf filename=2402.12066v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this letter, we investigate the performance of Max Minimum <b>Fairness</b> (MMF) for uplink Rate-Splitting Multiple Access (RSMA) in short-packet communications. Specifically, considering a Single-Input Single-Output (SISO) Multiple Access Channel (MAC), we optimize the transmit power allocation between the splitting user messages to maximize the minimum rate among users with Finite Blocklength (FBL) constraints. To tackle this problem, we propose a Successive Convex Approximation (SCA)-based approach. Additionally, we introduce a low-complexity scheme to design the decoding order at the receiver. Numerical results show that RSMA outperforms conventional transmission schemes such as Non-orthogonal Multiple Access (NOMA) in terms of MMF.</p></p class="citation"></blockquote><h2 id=eesssp-2>eess.SP (2)</h2><h3 id=12--310346-truncated-polynomial-expansion-based-detection-in-massive-mimo-a-model-driven-deep-learning-approach-kazem-izadinasab-et-al-2024>(1/2 | 310/346) Truncated Polynomial Expansion-Based Detection in Massive MIMO: A Model-Driven Deep Learning Approach (Kazem Izadinasab et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kazem Izadinasab, Ahmed Wagdy Shaban, Oussama Damen. (2024)<br><strong>Truncated Polynomial Expansion-Based Detection in Massive MIMO: A Model-Driven Deep Learning Approach</strong><br><button class=copy-to-clipboard title="Truncated Polynomial Expansion-Based Detection in Massive MIMO: A Model-Driven Deep Learning Approach" index=310>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-310 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SP<br>Categories: cs-LG, eess-SP, eess.SP<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12595v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12595v1.pdf filename=2402.12595v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we propose a deep learning (DL)-based approach for efficiently computing the inverse of Hermitian matrices using truncated polynomial expansion (TPE). Our model-driven approach involves optimizing the coefficients of the TPE during an offline training procedure for a given number of TPE terms. We apply this method to signal detection in uplink massive multiple-input multiple-output (MIMO) systems, where the matrix inverse operation required by linear detectors, such as zero-forcing (ZF) and minimum mean square error (MMSE), is approximated using TPE. Our <b>simulation</b> results demonstrate that the proposed learned TPE-based method outperforms the conventional TPE method with optimal coefficients in terms of asymptotic convergence speed and reduces the computational complexity of the online detection stage, albeit at the expense of the offline training stage. However, the limited number of trainable parameters leads to a swift offline training process.</p></p class="citation"></blockquote><h3 id=22--311346-a-simple-detection-and-identification-scheme-for-reconfigurable-intelligent-surfaces-aymen-khaleel-et-al-2024>(2/2 | 311/346) A Simple Detection and Identification Scheme For Reconfigurable Intelligent Surfaces (Aymen Khaleel et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Aymen Khaleel, Recep Vural, Mehmet Cagri Ilter, Majid Gerami, Ertugrul Basar. (2024)<br><strong>A Simple Detection and Identification Scheme For Reconfigurable Intelligent Surfaces</strong><br><button class=copy-to-clipboard title="A Simple Detection and Identification Scheme For Reconfigurable Intelligent Surfaces" index=311>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-311 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SP<br>Categories: cs-IT, eess-SP, eess.SP, math-IT<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12565v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12565v1.pdf filename=2402.12565v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Reconfigurable intelligent surface (RIS)-empowered communication is one of the promising physical layer enabling technologies for the sixth generation (6G) wireless networks due to their unprecedented capabilities in shaping the wireless communication environment. RISs are modeled as passive objects that can not transmit or receive wireless signals. While the passiveness of these surfaces is a key advantage in terms of power consumption and implementation complexity, it limits their capability to interact with the other active components in the network. Specifically, unlike conventional base stations (BSs), which actively identify themselves to user equipment (UEs) by periodically sending pilot signals, RISs need to be detected from the UE side. This paper proposes a novel RIS identification (RIS- ID) scheme, enabling UEs to detect and uniquely identify RISs in their surrounding environment. Furthermore, to assess the proposed RIS-ID scheme, we propose two performance metrics: the false and miss detection probabilities. These probabilities are analytically derived and verified through computer <b>simulations,</b> revealing the effectiveness of the proposed RIS-ID scheme under different operating scenarios.</p></p class="citation"></blockquote><h2 id=astro-phga-1>astro-ph.GA (1)</h2><h3 id=11--312346-emulating-the-interstellar-medium-chemistry-with-neural-operators-lorenzo-branca-et-al-2024>(1/1 | 312/346) Emulating the interstellar medium chemistry with neural operators (Lorenzo Branca et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lorenzo Branca, Andrea Pallottini. (2024)<br><strong>Emulating the interstellar medium chemistry with neural operators</strong><br><button class=copy-to-clipboard title="Emulating the interstellar medium chemistry with neural operators" index=312>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-312 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: astro-ph.GA<br>Categories: astro-ph-GA, astro-ph.GA, cs-LG<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12435v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12435v1.pdf filename=2402.12435v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Galaxy formation and evolution critically depend on understanding the complex photo-chemical processes that govern the evolution and thermodynamics of the InterStellar Medium (ISM). Computationally, solving chemistry is among the most heavy tasks in cosmological and astrophysical <b>simulations.</b> The evolution of such non-equilibrium photo-chemical network relies on implicit, precise, computationally costly, ordinary differential equations (ODE) solvers. Here, we aim at substituting such procedural solvers with fast, pre-trained, emulators based on neural operators. We emulate a non-equilibrium chemical network up to H$_2$ formation (9 species, 52 reactions) by adopting the DeepONet formalism, i.e. by splitting the ODE solver operator that maps the initial conditions and time evolution into a tensor product of two neural networks. We use $\texttt{KROME}$ to generate a training set spanning $-2\leq \log(n/\mathrm{cm}^{-3}) \leq 3.5$, $\log(20) \leq\log(T/\mathrm{K}) \leq 5.5$, $-6 \leq \log(n_i/n) &lt; 0$, and by adopting an incident radiation field $\textbf{F}$ sampled in 10 energy bins with a continuity prior. We separately train the solver for $T$ and each $n_i$ for $\simeq 4.34,\rm GPUhrs$. Compared with the reference solutions obtained by $\texttt{KROME}$ for single zone models, the typical precision obtained is of order $10^{-2}$, i.e. the $10 \times$ better with a training that is $40 \times$ less costly with respect to previous emulators which however considered only a fixed $\mathbf{F}$. The present model achieves a speed-up of a factor of $128 \times$ with respect to stiff ODE solvers. Our neural emulator represents a significant leap forward in the modeling of ISM chemistry, offering a good balance of precision, versatility, and computational efficiency.</p></p class="citation"></blockquote><h2 id=astro-phsr-1>astro-ph.SR (1)</h2><h3 id=11--313346-short-period-variables-in-tess-full-frame-image-light-curves-identified-via-convolutional-neural-networks-greg-olmschenk-et-al-2024>(1/1 | 313/346) Short-Period Variables in TESS Full-Frame Image Light Curves Identified via Convolutional Neural Networks (Greg Olmschenk et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Greg Olmschenk, Richard K. Barry, Stela Ishitani Silva, Brian P. Powell, Ethan Kruse, Jeremy D. Schnittman, Agnieszka M. Cieplak, Thomas Barclay, Siddhant Solanki, Bianca Ortega, John Baker, Yesenia Helem Salinas Mamani. (2024)<br><strong>Short-Period Variables in TESS Full-Frame Image Light Curves Identified via Convolutional Neural Networks</strong><br><button class=copy-to-clipboard title="Short-Period Variables in TESS Full-Frame Image Light Curves Identified via Convolutional Neural Networks" index=313>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-313 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: astro-ph.SR<br>Categories: astro-ph-EP, astro-ph-IM, astro-ph-SR, astro-ph.SR, cs-LG, eess-IV<br>Keyword Score: 20<br>Keywords: Convolution, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12369v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12369v1.pdf filename=2402.12369v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The Transiting Exoplanet Survey Satellite (TESS) mission measured light from stars in ~85% of the sky throughout its two-year primary mission, resulting in millions of TESS 30-minute cadence light curves to analyze in the search for transiting exoplanets. To search this vast dataset, we aim to provide an approach that is both computationally efficient, produces highly performant predictions, and minimizes the required human search effort. We present a <b>convolutional</b> <b>neural</b> <b>network</b> that we train to identify short period variables. To make a prediction for a given light curve, our network requires no prior target parameters identified using other methods. Our network performs inference on a TESS 30-minute cadence light curve in ~5ms on a single GPU, enabling large scale archival searches. We present a collection of 14156 short-period variables identified by our network. The majority of our identified variables fall into two prominent populations, one of short-period main sequence binaries and another of Delta Scuti stars. Our neural network model and related code is additionally provided as open-source code for public use and extension.</p></p class="citation"></blockquote><h2 id=mathna-6>math.NA (6)</h2><h3 id=16--314346-a-high-order-fully-well-balanced-unconditionally-positivity-preserving-finite-volume-framework-for-flood-simulations-mirco-ciallella-et-al-2024>(1/6 | 314/346) A high-order, fully well-balanced, unconditionally positivity-preserving finite volume framework for flood simulations (Mirco Ciallella et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mirco Ciallella, Lorenzo Micalizzi, Victor Michel-Dansac, Philipp Öffner, Davide Torlo. (2024)<br><strong>A high-order, fully well-balanced, unconditionally positivity-preserving finite volume framework for flood simulations</strong><br><button class=copy-to-clipboard title="A high-order, fully well-balanced, unconditionally positivity-preserving finite volume framework for flood simulations" index=314>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-314 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: cs-NA, math-NA, math.NA, physics-flu-dyn<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12248v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12248v1.pdf filename=2402.12248v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this work, we present a high-order finite volume framework for the numerical <b>simulation</b> of shallow water flows. The method is designed to accurately capture complex dynamics inherent in shallow water systems, particularly suited for applications such as tsunami <b>simulations.</b> The arbitrarily high-order framework ensures precise representation of flow behaviors, crucial for simulating phenomena characterized by rapid changes and fine-scale features. Thanks to an {\it ad-hoc} reformulation in terms of production-destruction terms, the time integration ensures positivity preservation without any time-step restrictions, a vital attribute for physical consistency, especially in scenarios where negative water depth reconstructions could lead to unrealistic results. In order to introduce the preservation of general steady equilibria dictated by the underlying balance law, the high-order reconstruction and numerical flux are blended in a convex fashion with a well-balanced approximation, which is able to provide exact preservation of both static and moving equilibria. Through numerical experiments, we demonstrate the effectiveness and robustness of the proposed approach in capturing the intricate dynamics of shallow water flows, while preserving key physical properties essential for flood <b>simulations.</b></p></p class="citation"></blockquote><h3 id=26--315346-nonlinear-discrete-time-observers-with-physics-informed-neural-networks-hector-vargas-alvarez-et-al-2024>(2/6 | 315/346) Nonlinear Discrete-Time Observers with Physics-Informed Neural Networks (Hector Vargas Alvarez et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hector Vargas Alvarez, Gianluca Fabiani, Ioannis G. Kevrekidis, Nikolaos Kazantzis, Constantinos Siettos. (2024)<br><strong>Nonlinear Discrete-Time Observers with Physics-Informed Neural Networks</strong><br><button class=copy-to-clipboard title="Nonlinear Discrete-Time Observers with Physics-Informed Neural Networks" index=315>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-315 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: 37N30, 68T05, 93C55, 65D15, cs-AI, cs-NA, math-DS, math-NA, math.NA<br>Keyword Score: 10<br>Keywords: Discrete Time, Discrete Time<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12360v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12360v1.pdf filename=2402.12360v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We use Physics-Informed Neural Networks (PINNs) to solve the <b>discrete-time</b> <b>nonlinear</b> observer state estimation problem. Integrated within a single-step exact observer linearization framework, the proposed PINN approach aims at learning a nonlinear state transformation map by solving a system of inhomogeneous functional equations. The performance of the proposed PINN approach is assessed via two illustrative case studies for which the observer linearizing transformation map can be derived analytically. We also perform an uncertainty quantification analysis for the proposed PINN scheme and we compare it with conventional power-series numerical implementations, which rely on the computation of a power series solution.</p></p class="citation"></blockquote><h3 id=36--316346-lax-wendroff-flux-reconstruction-on-adaptive-curvilinear-meshes-with-error-based-time-stepping-for-hyperbolic-conservation-laws-arpit-babbar-et-al-2024>(3/6 | 316/346) Lax-Wendroff Flux Reconstruction on adaptive curvilinear meshes with error based time stepping for hyperbolic conservation laws (Arpit Babbar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Arpit Babbar, Praveen Chandrashekar. (2024)<br><strong>Lax-Wendroff Flux Reconstruction on adaptive curvilinear meshes with error based time stepping for hyperbolic conservation laws</strong><br><button class=copy-to-clipboard title="Lax-Wendroff Flux Reconstruction on adaptive curvilinear meshes with error based time stepping for hyperbolic conservation laws" index=316>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-316 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: 65M60, G-1-8, cs-NA, math-NA, math.NA<br>Keyword Score: 10<br>Keywords: Fine-tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11926v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11926v2.pdf filename=2402.11926v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Lax-Wendroff Flux Reconstruction (LWFR) is a single-stage, high order, quadrature free method for solving hyperbolic conservation laws. This work extends the LWFR scheme to solve conservation laws on curvilinear meshes with adaptive mesh refinement (AMR). The scheme uses a subcell based blending limiter to perform shock capturing and exploits the same subcell structure to obtain admissibility preservation on curvilinear meshes. It is proven that the proposed extension of LWFR scheme to curvilinear grids preserves constant solution (free stream preservation) under the standard metric identities. For curvilinear meshes, linear Fourier stability analysis cannot be used to obtain an optimal CFL number. Thus, an embedded-error based time step computation method is proposed for LWFR method which reduces <b>fine-tuning</b> process required to select a stable CFL number using the wave speed based time step computation. The developments are tested on compressible Euler&rsquo;s equations, validating the blending limiter, admissibility preservation, AMR algorithm, curvilinear meshes and error based time stepping.</p></p class="citation"></blockquote><h3 id=46--317346-solving-fluid-flow-problems-in-space-time-with-multiscale-stabilization-formulation-and-examples-biswajit-khara-et-al-2024>(4/6 | 317/346) Solving fluid flow problems in space-time with multiscale stabilization: formulation and examples (Biswajit Khara et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Biswajit Khara, Robert Dyja, Kumar Saurabh, Anupam Sharma, Baskar Ganapathysubramanian. (2024)<br><strong>Solving fluid flow problems in space-time with multiscale stabilization: formulation and examples</strong><br><button class=copy-to-clipboard title="Solving fluid flow problems in space-time with multiscale stabilization: formulation and examples" index=317>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-317 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: cs-NA, math-AP, math-NA, math.NA<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12571v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12571v1.pdf filename=2402.12571v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We solve fluid flow problems through a space-time finite element method. The weak form of the Navier-Stokes equations is stabilized using the variational multi-scale formulation. The finite element problem is posed on the &ldquo;full&rdquo; space-time domain, considering time as another dimension. We apply this method on two <b>benchmark</b> problems in computational fluid dynamics, namely, lid-driven cavity flow and flow past a circular cylinder. We validate the current method with existing results from literature and show that very large space-time blocks can be solved using our approach.</p></p class="citation"></blockquote><h3 id=56--318346-a-unified-field-monolithic-fictitious-domain-finite-element-method-for-fluid-structure-contact-interactions-and-applications-to-deterministic-lateral-displacement-problems-cheng-wang-et-al-2024>(5/6 | 318/346) A Unified-Field Monolithic Fictitious Domain-Finite Element Method for Fluid-Structure-Contact Interactions and Applications to Deterministic Lateral Displacement Problems (Cheng Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Cheng Wang, Pengtao Sun, Yumiao Zhang, Jinchao Xu, Yan Chen, Jiarui Han. (2024)<br><strong>A Unified-Field Monolithic Fictitious Domain-Finite Element Method for Fluid-Structure-Contact Interactions and Applications to Deterministic Lateral Displacement Problems</strong><br><button class=copy-to-clipboard title="A Unified-Field Monolithic Fictitious Domain-Finite Element Method for Fluid-Structure-Contact Interactions and Applications to Deterministic Lateral Displacement Problems" index=318>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-318 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: 65M22, 65M60, 65M85, 65Z05, 65D17, 70F35, 70F40, 74S05, 74F10,
76M10, 76M30, 76D05, 76D09, cs-NA, math-NA, math.NA, physics-flu-dyn<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12517v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12517v1.pdf filename=2402.12517v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Based upon two overlapped, body-unfitted meshes, a type of unified-field monolithic fictitious domain-finite element method (UFMFD-FEM) is developed in this paper for moving interface problems of dynamic fluid-structure interactions (FSI) accompanying with high-contrast physical coefficients across the interface and contacting collisions between the structure and fluidic channel wall when the structure is immersed in the fluid. In particular, the proposed novel numerical method consists of a monolithic, stabilized mixed finite element method within the frame of fictitious domain/immersed boundary method (IBM) for generic fluid-structure-contact interaction (FSCI) problems in the Eulerian-updated Lagrangian description, while involving the no-slip type of interface conditions on the fluid-structure interface, and the repulsive contact force on the structural surface when the immersed structure contacts the fluidic channel wall. The developed UFMFD-FEM for FSI or FSCI problems can deal with the structural motion with large rotational and translational displacements and/or large deformation in an accurate and efficient fashion, which are first validated by two <b>benchmark</b> FSI problems and one FSCI model problem, then by experimental results of a realistic FSCI scenario &ndash; the microfluidic deterministic lateral displacement (DLD) problem that is applied to isolate circulating tumor cells (CTCs) from blood cells in the blood fluid through a cascaded filter DLD microchip in practice, where a particulate fluid with the pillar obstacles effect in the fluidic channel, i.e., the effects of fluid-structure interaction and structure collision, play significant roles to sort particles (cells) of different sizes with tilted pillar arrays.</p></p class="citation"></blockquote><h3 id=66--319346-analysis-of-the-picard-newton-iteration-for-the-navier-stokes-equations-global-stability-and-quadratic-convergence-sara-pollock-et-al-2024>(6/6 | 319/346) Analysis of the Picard-Newton iteration for the Navier-Stokes equations: global stability and quadratic convergence (Sara Pollock et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sara Pollock, Leo Rebholz, Xuemin Tu, Menyging Xiao. (2024)<br><strong>Analysis of the Picard-Newton iteration for the Navier-Stokes equations: global stability and quadratic convergence</strong><br><button class=copy-to-clipboard title="Analysis of the Picard-Newton iteration for the Navier-Stokes equations: global stability and quadratic convergence" index=319>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-319 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: cs-NA, math-NA, math.NA<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12304v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12304v1.pdf filename=2402.12304v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We analyze and test a simple-to-implement two-step iteration for the incompressible Navier-Stokes equations that consists of first applying the Picard iteration and then applying the Newton iteration to the Picard output. We prove that this composition of Picard and Newton converges quadratically, and our analysis (which covers both the unique solution and non-unique solution cases) also suggests that this solver has a larger convergence basin than usual Newton because of the improved stability properties of Picard-Newton over Newton. Numerical tests show that Picard-Newton dramatically outperforms both the Picard and Newton iterations, especially as the Reynolds number increases. We also consider enhancing the Picard step with Anderson acceleration (AA), and find that the AAPicard-Newton iteration has even better convergence properties on several <b>benchmark</b> test problems.</p></p class="citation"></blockquote><h2 id=physicscomp-ph-2>physics.comp-ph (2)</h2><h3 id=12--320346-second-order-meanfield-approximation-for-calculating-dynamics-in-au-nanoparticle-networks-evan-wonisch-et-al-2024>(1/2 | 320/346) Second Order Meanfield Approximation for calculating Dynamics in Au-Nanoparticle Networks (Evan Wonisch et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Evan Wonisch, Jonas Mensing, Andreas Heuer. (2024)<br><strong>Second Order Meanfield Approximation for calculating Dynamics in Au-Nanoparticle Networks</strong><br><button class=copy-to-clipboard title="Second Order Meanfield Approximation for calculating Dynamics in Au-Nanoparticle Networks" index=320>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-320 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: physics.comp-ph<br>Categories: cs-ET, cs-NA, math-NA, physics-comp-ph, physics-data-an, physics.comp-ph<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12223v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12223v1.pdf filename=2402.12223v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Exploiting physical processes for fast and energy-efficient computation bears great potential in the advancement of modern hardware components. This paper explores non-linear charge tunneling in nanoparticle networks, controlled by external voltages. The dynamics are described by a master equation, which describes the development of a distribution function over the set of charge occupation numbers. The driving force behind this evolution are charge tunneling events among nanoparticles and their associated rates. In this paper, we introduce two meanfield approximations to this master equation. By parametrization of the distribution function using its first- and second-order statistical moments, and a subsequent projection of the dynamics onto the resulting moment manifold, one can deterministically calculate expected charges and currents. Unlike a kinetic Monte Carlo approach, which extracts samples from the distribution function, this meanfield approach avoids any random elements. A comparison of results between the meanfield approximation and an already available kinetic Monte Carlo <b>simulation</b> demonstrates great accuracy. Our analysis also reveals that transitioning from a first-order to a second-order approximation significantly enhances the accuracy. Furthermore, we demonstrate the applicability of our approach to time-dependent <b>simulations,</b> using eulerian time-integration schemes.</p></p class="citation"></blockquote><h3 id=22--321346-recent-extensions-of-the-zkcm-library-for-parallel-and-accurate-mps-simulation-of-quantum-circuits-akira-saitoh-2024>(2/2 | 321/346) Recent Extensions of the ZKCM Library for Parallel and Accurate MPS Simulation of Quantum Circuits (Akira SaiToh, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Akira SaiToh. (2024)<br><strong>Recent Extensions of the ZKCM Library for Parallel and Accurate MPS Simulation of Quantum Circuits</strong><br><button class=copy-to-clipboard title="Recent Extensions of the ZKCM Library for Parallel and Accurate MPS Simulation of Quantum Circuits" index=321>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-321 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: physics.comp-ph<br>Categories: 97N80, 81-04, G-4, cs-MS, physics-comp-ph, physics.comp-ph, quant-ph<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11868v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11868v1.pdf filename=2402.11868v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A C++ library ZKCM and its extension library ZKCM_QC have been developed since 2011 for multiple-precision matrix computation and accurate matrix-product-state (MPS) quantum circuit <b>simulation,</b> respectively. In this report, a recent progress in the extensions of these libraries is described, which are mainly for parallel processing with the OpenMP and CUDA frameworks.</p></p class="citation"></blockquote><h2 id=mathoc-1>math.OC (1)</h2><h3 id=11--322346-the-new-era-of-dynamic-pricing-synergizing-supervised-learning-and-quadratic-programming-gustavo-bramao-et-al-2024>(1/1 | 322/346) The New Era of Dynamic Pricing: Synergizing Supervised Learning and Quadratic Programming (Gustavo Bramao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Gustavo Bramao, Ilia Tarygin. (2024)<br><strong>The New Era of Dynamic Pricing: Synergizing Supervised Learning and Quadratic Programming</strong><br><button class=copy-to-clipboard title="The New Era of Dynamic Pricing: Synergizing Supervised Learning and Quadratic Programming" index=322>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-322 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.OC<br>Categories: cs-LG, math-OC, math.OC<br>Keyword Score: 20<br>Keywords: Supervised Learning, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14844v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14844v1.pdf filename=2402.14844v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we explore a novel combination of <b>supervised</b> <b>learning</b> and quadratic programming to refine dynamic pricing models in the car rental industry. We utilize dynamic modeling of price elasticity, informed by ordinary least squares (OLS) metrics such as p-values, homoscedasticity, error normality. These metrics, when their underlying assumptions hold, are integral in guiding a quadratic programming agent. The program is tasked with optimizing margin for a given finite set target.</p></p class="citation"></blockquote><h2 id=csar-2>cs.AR (2)</h2><h3 id=12--323346-towards-joint-optimization-for-dnn-architecture-and-configuration-for-compute-in-memory-hardware-souvik-kundu-et-al-2024>(1/2 | 323/346) Towards Joint Optimization for DNN Architecture and Configuration for Compute-In-Memory Hardware (Souvik Kundu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Souvik Kundu, Anthony Sarah, Vinay Joshi, Om J Omer, Sreenivas Subramoney. (2024)<br><strong>Towards Joint Optimization for DNN Architecture and Configuration for Compute-In-Memory Hardware</strong><br><button class=copy-to-clipboard title="Towards Joint Optimization for DNN Architecture and Configuration for Compute-In-Memory Hardware" index=323>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-323 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AR<br>Categories: cs-AI, cs-AR, cs.AR<br>Keyword Score: 20<br>Keywords: Convolutional Neural Network, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11780v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11780v1.pdf filename=2402.11780v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the recent growth in demand for large-scale deep neural networks, compute in-memory (CiM) has come up as a prominent solution to alleviate bandwidth and on-chip interconnect bottlenecks that constrain Von-Neuman architectures. However, the construction of CiM hardware poses a challenge as any specific memory hierarchy in terms of cache sizes and memory bandwidth at different interfaces may not be ideally matched to any neural network&rsquo;s attributes such as tensor dimension and arithmetic intensity, thus leading to suboptimal and under-performing systems. Despite the success of neural architecture search (NAS) techniques in yielding efficient sub-networks for a given hardware metric budget (e.g., DNN execution time or latency), it assumes the hardware configuration to be frozen, often yielding sub-optimal sub-networks for a given budget. In this paper, we present CiMNet, a framework that jointly searches for optimal sub-networks and hardware configurations for CiM architectures creating a Pareto optimal frontier of downstream task accuracy and execution metrics (e.g., latency). The proposed framework can comprehend the complex interplay between a sub-network&rsquo;s performance and the CiM hardware configuration choices including bandwidth, processing element size, and memory size. Exhaustive experiments on different model architectures from both <b>CNN</b> and <b>Transformer</b> families demonstrate the efficacy of the CiMNet in finding co-optimized sub-networks and CiM hardware configurations. Specifically, for similar ImageNet classification accuracy as baseline ViT-B, optimizing only the model architecture increases performance (or reduces workload execution time) by 1.7x while optimizing for both the model architecture and hardware configuration increases it by 3.1x.</p></p class="citation"></blockquote><h3 id=22--324346-factor-machine-mixed-signal-architecture-for-fine-grained-graph-based-computing-piotr-dudek-2024>(2/2 | 324/346) Factor Machine: Mixed-signal Architecture for Fine-Grained Graph-Based Computing (Piotr Dudek, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Piotr Dudek. (2024)<br><strong>Factor Machine: Mixed-signal Architecture for Fine-Grained Graph-Based Computing</strong><br><button class=copy-to-clipboard title="Factor Machine: Mixed-signal Architecture for Fine-Grained Graph-Based Computing" index=324>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-324 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AR<br>Categories: C-1-4, cs-AR, cs.AR<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12130v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12130v2.pdf filename=2402.12130v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper proposes the design and implementation strategy of a novel computing architecture, the Factor Machine. The work is a step towards a general-purpose parallel system operating in a non-sequential manner, exploiting processing/memory co-integration and replacing the traditional Turing/von Neumann model of a computer system with a framework based on &ldquo;factorised computation&rdquo;. This architecture is inspired by neural information processing principles and aims to progress the development of brain-like machine intelligence systems, through providing a computing substrate designed from the ground up to enable efficient implementations of algorithms based on relational networks. The paper provides a rationale for such machine, in the context of the history of computing, and more recent developments in neuromorphic hardware, reviews its general features, and proposes a mixed-signal hardware implementation, based on using analogue circuits to carry out computation and localised and sparse communication between the compute units.</p></p class="citation"></blockquote><h2 id=cslo-1>cs.LO (1)</h2><h3 id=11--325346-automating-boundary-filling-in-cubical-agda-maximilian-doré-et-al-2024>(1/1 | 325/346) Automating Boundary Filling in Cubical Agda (Maximilian Doré et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Maximilian Doré, Evan Cavallo, Anders Mörtberg. (2024)<br><strong>Automating Boundary Filling in Cubical Agda</strong><br><button class=copy-to-clipboard title="Automating Boundary Filling in Cubical Agda" index=325>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-325 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LO<br>Categories: cs-LO, cs.LO<br>Keyword Score: 13<br>Keywords: Benchmarking, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12169v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12169v1.pdf filename=2402.12169v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>When working in a proof assistant, automation is key to discharging routine proof goals such as equations between algebraic expressions. Homotopy Type Theory allows the user to reason about higher structures, such as topological spaces, using higher inductive types (HITs) and univalence. Cubical Agda is an extension of Agda with computational support for HITs and univalence. A difficulty when working in Cubical Agda is dealing with the complex combinatorics of higher structures, an infinite-dimensional generalisation of equational <b>reasoning.</b> To solve these higher-dimensional equations consists in constructing cubes with specified boundaries. We develop a simplified cubical language in which we isolate and study two automation problems: contortion solving, where we attempt to &ldquo;contort&rdquo; a cube to fit a given boundary, and the more general Kan solving, where we search for solutions that involve pasting multiple cubes together. Both problems are difficult in the general case - Kan solving is even undecidable - so we focus on heuristics that perform well on practical examples. We provide a solver for the contortion problem using a reformulation of contortions in terms of poset maps, while we solve Kan problems using constraint satisfaction programming. We have implemented our algorithms in an experimental Haskell solver that can be used to automatically solve goals presented by Cubical Agda. We illustrate this with a case study establishing the Eckmann-Hilton theorem using our solver, as well as various <b>benchmarks</b> - providing the ground for further study of proof automation in cubical type theories.</p></p class="citation"></blockquote><h2 id=physicschem-ph-1>physics.chem-ph (1)</h2><h3 id=11--326346-molecule-generation-and-optimization-for-efficient-fragrance-creation-bruno-c-l-rodrigues-et-al-2024>(1/1 | 326/346) Molecule Generation and Optimization for Efficient Fragrance Creation (Bruno C. L. Rodrigues et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bruno C. L. Rodrigues, Vinicius V. Santana, Sandris Murins, Idelfonso B. R. Nogueira. (2024)<br><strong>Molecule Generation and Optimization for Efficient Fragrance Creation</strong><br><button class=copy-to-clipboard title="Molecule Generation and Optimization for Efficient Fragrance Creation" index=326>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-326 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: physics.chem-ph<br>Categories: cs-LG, physics-chem-ph, physics.chem-ph<br>Keyword Score: 13<br>Keywords: Graph, Transfer Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12134v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12134v1.pdf filename=2402.12134v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This research introduces a Machine Learning-centric approach to replicate olfactory experiences, validated through experimental quantification of perfume perception. Key contributions encompass a hybrid model connecting perfume molecular structure to human olfactory perception. This model includes an AI-driven molecule generator (utilizing <b>Graph</b> and Generative Neural Networks), quantification and prediction of odor intensity, and refinery of optimal solvent and molecule combinations for desired fragrances. Additionally, a thermodynamic-based model establishes a link between olfactory perception and liquid-phase concentrations. The methodology employs <b>Transfer</b> <b>Learning</b> and selects the most suitable molecules based on vapor pressure and fragrance notes. Ultimately, a mathematical optimization problem is formulated to minimize discrepancies between new and target olfactory experiences. The methodology is validated by reproducing two distinct olfactory experiences using available experimental data.</p></p class="citation"></blockquote><h2 id=csma-2>cs.MA (2)</h2><h3 id=12--327346-aligning-individual-and-collective-objectives-in-multi-agent-cooperation-yang-li-et-al-2024>(1/2 | 327/346) Aligning Individual and Collective Objectives in Multi-Agent Cooperation (Yang Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yang Li, Wenhao Zhang, Jianhong Wang, Shao Zhang, Yali Du, Ying Wen, Wei Pan. (2024)<br><strong>Aligning Individual and Collective Objectives in Multi-Agent Cooperation</strong><br><button class=copy-to-clipboard title="Aligning Individual and Collective Objectives in Multi-Agent Cooperation" index=327>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-327 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.MA<br>Categories: cs-AI, cs-MA, cs.MA<br>Keyword Score: 13<br>Keywords: Benchmarking, Grounding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12416v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12416v1.pdf filename=2402.12416v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the field of multi-agent learning, the challenge of mixed-motive cooperation is pronounced, given the inherent contradictions between individual and collective goals. Current research in this domain primarily focuses on incorporating domain knowledge into rewards or introducing additional mechanisms to foster cooperation. However, many of these methods suffer from the drawbacks of manual design costs and the lack of a theoretical <b>grounding</b> convergence procedure to the solution. To address this gap, we approach the mixed-motive game by modeling it as a differentiable game to study learning dynamics. We introduce a novel optimization method named Altruistic Gradient Adjustment (AgA) that employs gradient adjustments to novelly align individual and collective objectives. Furthermore, we provide theoretical proof that the selection of an appropriate alignment weight in AgA can accelerate convergence towards the desired solutions while effectively avoiding the undesired ones. The visualization of learning dynamics effectively demonstrates that AgA successfully achieves alignment between individual and collective objectives. Additionally, through evaluations conducted on established mixed-motive <b>benchmarks</b> such as the public good game, Cleanup, Harvest, and our modified mixed-motive SMAC environment, we validate AgA&rsquo;s capability to facilitate altruistic and fair collaboration.</p></p class="citation"></blockquote><h3 id=22--328346-a-conflict-aware-optimal-goal-assignment-algorithm-for-multi-robot-systems-aakash-et-al-2024>(2/2 | 328/346) A Conflict-Aware Optimal Goal Assignment Algorithm for Multi-Robot Systems (Aakash et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Aakash, Indranil Saha. (2024)<br><strong>A Conflict-Aware Optimal Goal Assignment Algorithm for Multi-Robot Systems</strong><br><button class=copy-to-clipboard title="A Conflict-Aware Optimal Goal Assignment Algorithm for Multi-Robot Systems" index=328>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-328 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.MA<br>Categories: cs-AI, cs-MA, cs-RO, cs.MA<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13292v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13292v1.pdf filename=2402.13292v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The fundamental goal assignment problem for a multi-robot application aims to assign a unique goal to each robot while ensuring collision-free paths, minimizing the total movement cost. A plausible algorithmic solution to this NP-hard problem involves an iterative process that integrates a task planner to compute the goal assignment while ignoring the collision possibilities among the robots and a multi-agent path-finding algorithm to find the collision-free trajectories for a given assignment. This procedure involves a method for computing the next best assignment given the current best assignment. A naive way of computing the next best assignment, as done in the state-of-the-art solutions, becomes a roadblock to achieving scalability in solving the overall problem. To obviate this bottleneck, we propose an efficient conflict-guided method to compute the next best assignment. Additionally, we introduce two more optimizations to the algorithm &ndash; first for avoiding the unconstrained path computations between robot-goal pairs wherever possible, and the second to prevent duplicate constrained path computations for multiple robot-goal pairs. We extensively evaluate our algorithm for up to a hundred robots on several <b>benchmark</b> workspaces. The results demonstrate that the proposed algorithm achieves nearly an order of magnitude speedup over the state-of-the-art algorithm, showcasing its efficacy in real-world scenarios.</p></p class="citation"></blockquote><h2 id=mathco-2>math.CO (2)</h2><h3 id=12--329346-conformally-rigid-graphs-stefan-steinerberger-et-al-2024>(1/2 | 329/346) Conformally rigid graphs (Stefan Steinerberger et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Stefan Steinerberger, Rekha R. Thomas. (2024)<br><strong>Conformally rigid graphs</strong><br><button class=copy-to-clipboard title="Conformally rigid graphs" index=329>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-329 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.CO<br>Categories: cs-DM, math-CO, math-OC, math-SP, math.CO<br>Keyword Score: 13<br>Keywords: Graph, Graph Embedding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11758v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11758v1.pdf filename=2402.11758v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Given a finite, simple, connected <b>graph</b> <b>$G=(V,E)$</b> with $|V|=n$, we consider the associated <b>graph</b> <b>Laplacian</b> matrix $L = D - A$ with eigenvalues $0 = \lambda_1 &lt; \lambda_2 \leq \dots \leq \lambda_n$. One can also consider the same <b>graph</b> <b>equipped</b> with positive edge weights $w:E \rightarrow \mathbb{R}<em>{> 0}$ normalized to $\sum</em>{e \in E} w_e = |E|$ and the associated weighted Laplacian matrix $L_w$. We say that $G$ is conformally rigid if constant edge-weights maximize the second eigenvalue $\lambda_2(w)$ of $L_w$ over all $w$, and minimize $\lambda_n(w&rsquo;)$ of $L_{w&rsquo;}$ over all $w&rsquo;$, i.e., for all $w,w&rsquo;$, $$ \lambda_2(w) \leq \lambda_2(1) \leq \lambda_n(1) \leq \lambda_n(w&rsquo;).$$ Conformal rigidity requires an extraordinary amount of symmetry in $G$. Every edge-transitive <b>graph</b> <b>is</b> conformally rigid. We prove that every distance-regular <b>graph,</b> <b>and</b> hence every strongly-regular <b>graph,</b> <b>is</b> conformally rigid. Certain special <b>graph</b> <b>embeddings</b> can be used to characterize conformal rigidity. Cayley <b>graphs</b> <b>can</b> be conformally rigid but need not be, we prove a sufficient criterion. We also find a small set of conformally rigid <b>graphs</b> <b>that</b> do not belong into any of the above categories; these include the Hoffman <b>graph,</b> <b>the</b> crossing number <b>graph</b> <b>6B</b> and others. Conformal rigidity can be certified via semidefinite programming, we provide explicit examples.</p></p class="citation"></blockquote><h3 id=22--330346-lettericity-of-graphs-an-fpt-algorithm-and-a-bound-on-the-size-of-obstructions-bogdan-alecu-et-al-2024>(2/2 | 330/346) Lettericity of graphs: an FPT algorithm and a bound on the size of obstructions (Bogdan Alecu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bogdan Alecu, Mamadou Moustapha Kanté, Vadim Lozin, Viktor Zamaraev. (2024)<br><strong>Lettericity of graphs: an FPT algorithm and a bound on the size of obstructions</strong><br><button class=copy-to-clipboard title="Lettericity of graphs: an FPT algorithm and a bound on the size of obstructions" index=330>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-330 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.CO<br>Categories: cs-DM, cs-DS, math-CO, math.CO<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12559v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12559v1.pdf filename=2402.12559v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Lettericity is a <b>graph</b> parameter responsible for many attractive structural properties. In particular, <b>graphs</b> of bounded lettericity have bounded linear clique-width and they are well-quasi-ordered by induced subgraphs. The latter property implies that any hereditary class of <b>graphs</b> of bounded lettericity can be described by finitely many forbidden induced subgraphs. This, in turn, implies, in a non-constructive way, polynomial-time recognition of such classes. However, no constructive algorithms and no specific bounds on the size of forbidden <b>graphs</b> are available up to date. In the present paper, we develop an algorithm that recognizes $n$-vertex <b>graphs</b> of lettericity at most $k$ in time $f(k)n^3$ and show that any minimal <b>graph</b> of lettericity more than $k$ has at most $2^{O(k^2\log k)}$ vertices.</p></p class="citation"></blockquote><h2 id=csni-2>cs.NI (2)</h2><h3 id=12--331346-optimize-energy-consumption-of-wireless-sensor-networks-by-using-modified-ant-colony-optimization-aco-yasameen-sajid-razooqi-et-al-2024>(1/2 | 331/346) Optimize Energy Consumption of Wireless Sensor Networks by using modified Ant Colony Optimization ACO (Yasameen Sajid Razooqi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yasameen Sajid Razooqi, Muntasir Al-Asfoor, Mohammed Hamzah Abed. (2024)<br><strong>Optimize Energy Consumption of Wireless Sensor Networks by using modified Ant Colony Optimization ACO</strong><br><button class=copy-to-clipboard title="Optimize Energy Consumption of Wireless Sensor Networks by using modified Ant Colony Optimization ACO" index=331>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-331 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NI<br>Categories: cs-NI, cs.NI<br>Keyword Score: 10<br>Keywords: Fine-tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12526v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12526v1.pdf filename=2402.12526v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Routing represents a pivotal concern in the context of Wireless Sensor Networks (WSN) owing to its divergence from traditional network routing paradigms. The inherent dynamism of the WSN environment, coupled with the scarcity of available resources, engenders considerable challenges for industry and academia alike in devising efficient routing strategies. Addressing these challenges, a viable recourse lies in applying heuristic search methodologies to ascertain the most optimal path in WSNs. Ant Colony Optimization (ACO) is a well-established heuristic algorithm that has demonstrated notable advancements in routing contexts. This paper introduces a modify routing protocols based on Ant colony optimization. In these protocols, we incorporate the inverse of the distance between nodes and their neighbours in the probability equations of ACO along with considering pheromone levels and residual energy. These formulation modifications facilitate the selection of the most suitable candidate for the subsequent hop, effectively minimizing the average energy consumption across all nodes in each iteration. Furthermore, in this protocol, we iteratively <b>fine-tune</b> ACO&rsquo;s parameter values based on the outcomes of several experimental trials. The experimental analysis is conducted through a diverse set of network topologies, and the results are subjected to comparison against well-established ACO algorithm and routing protocols. The efficacy of the proposed protocol is assessed based on various performance metrics, encompassing throughput, energy consumption, network lifetime, energy consumption, the extent of data transferred over the network, and the length of paths traversed by packets. These metrics collectively provide a comprehensive evaluation of the performance attainments of the routing protocols.</p></p class="citation"></blockquote><h3 id=22--332346-strengths-and-weaknesses-of-the-etsi-adaptive-dcc-algorithm-a-proposal-for-improvement-ignacio-soto-et-al-2024>(2/2 | 332/346) Strengths and Weaknesses of the ETSI Adaptive DCC Algorithm: A Proposal for Improvement (Ignacio Soto et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ignacio Soto, Oscar Amador, Manuel Urueña, Maria Calderon. (2024)<br><strong>Strengths and Weaknesses of the ETSI Adaptive DCC Algorithm: A Proposal for Improvement</strong><br><button class=copy-to-clipboard title="Strengths and Weaknesses of the ETSI Adaptive DCC Algorithm: A Proposal for Improvement" index=332>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-332 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NI<br>Categories: cs-NI, cs.NI<br>Keyword Score: 10<br>Keywords: Fairness<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12089v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12089v1.pdf filename=2402.12089v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This letter studies the adaptive Decentralized Congestion Control (DCC) algorithm defined in the ETSI TS 102 687 V1.2.1 specification. We provide insights on the parameters used in the algorithm and explore the impact of those parameters on its performance. We show how the algorithm achieves good average medium utilization while protecting against congestion, but we also show how the chosen parameters can result in slow speed of convergence and long periods of unfairness in transitory situations. Finally, we propose a modification to the algorithm which results in significant improvements in speed of convergence and <b>fairness.</b></p></p class="citation"></blockquote><h2 id=q-bionc-1>q-bio.NC (1)</h2><h3 id=11--333346-structure-of-activity-in-multiregion-recurrent-neural-networks-david-g-clark-et-al-2024>(1/1 | 333/346) Structure of activity in multiregion recurrent neural networks (David G. Clark et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>David G. Clark, Manuel Beiran. (2024)<br><strong>Structure of activity in multiregion recurrent neural networks</strong><br><button class=copy-to-clipboard title="Structure of activity in multiregion recurrent neural networks" index=333>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-333 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: q-bio.NC<br>Categories: cond-mat-dis-nn, cs-NE, q-bio-NC, q-bio.NC<br>Keyword Score: 10<br>Keywords: Recurrent Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12188v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12188v2.pdf filename=2402.12188v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Neural circuits are composed of multiple regions, each with rich dynamics and engaging in communication with other regions. The combination of local, within-region dynamics and global, network-level dynamics is thought to provide computational flexibility. However, the nature of such multiregion dynamics and the underlying synaptic connectivity patterns remain poorly understood. Here, we study the dynamics of <b>recurrent</b> <b>neural</b> <b>networks</b> with multiple interconnected regions. Within each region, neurons have a combination of random and structured <b>recurrent</b> <b>connections.</b> <b>Motivated</b> by experimental evidence of communication subspaces between cortical areas, these networks have low-rank connectivity between regions, enabling selective routing of activity. These networks exhibit two interacting forms of dynamics: high-dimensional fluctuations within regions and low-dimensional signal transmission between regions. To characterize this interaction, we develop a dynamical mean-field theory to analyze such networks in the limit where each region contains infinitely many neurons, with cross-region currents as key order parameters. Regions can act as both generators and transmitters of activity, roles that we show are in conflict. Specifically, taming the complexity of activity within a region is necessary for it to route signals to and from other regions. Unlike previous models of routing in neural circuits, which suppressed the activities of neuronal groups to control signal flow, routing in our model is achieved by exciting different high-dimensional activity patterns through a combination of connectivity structure and nonlinear <b>recurrent</b> <b>dynamics.</b> <b>This</b> theory provides insight into the interpretation of both multiregion neural data and trained neural networks.</p></p class="citation"></blockquote><h2 id=cscy-1>cs.CY (1)</h2><h3 id=11--334346-causal-equal-protection-as-algorithmic-fairness-marcello-di-bello-et-al-2024>(1/1 | 334/346) Causal Equal Protection as Algorithmic Fairness (Marcello Di Bello et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Marcello Di Bello, Nicolò Cangiotti, Michele Loi. (2024)<br><strong>Causal Equal Protection as Algorithmic Fairness</strong><br><button class=copy-to-clipboard title="Causal Equal Protection as Algorithmic Fairness" index=334>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-334 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CY<br>Categories: cs-AI, cs-CY, cs-DS, cs-LG, cs.CY<br>Keyword Score: 10<br>Keywords: Fairness<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12062v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12062v2.pdf filename=2402.12062v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Over the last ten years the literature in computer science and philosophy has formulated different criteria of algorithmic <b>fairness.</b> One of the most discussed, classification parity, requires that the erroneous classifications of a predictive algorithm occur with equal frequency for groups picked out by protected characteristics. Despite its intuitive appeal, classification parity has come under attack. Multiple scenarios can be imagined in which - intuitively - a predictive algorithm does not treat any individual unfairly, and yet classification parity is violated. To make progress, we turn to a related principle, equal protection, originally developed in the context of criminal justice. Key to equal protection is equalizing the risks of erroneous classifications (in a sense to be specified) as opposed to equalizing the rates of erroneous classifications. We show that equal protection avoids many of the counterexamples to classification parity, but also fails to model our moral intuitions in a number of common scenarios, for example, when the predictor is causally downstream relative to the protected characteristic. To address these difficulties, we defend a novel principle, causal equal protection, that models the fair allocation of the risks of erroneous classification through the lenses of causality.</p></p class="citation"></blockquote><h2 id=q-biobm-1>q-bio.BM (1)</h2><h3 id=11--335346-a-novel-molecule-generative-model-of-vae-combined-with-transformer-yasuhiro-yoshikai-et-al-2024>(1/1 | 335/346) A novel molecule generative model of VAE combined with Transformer (Yasuhiro Yoshikai et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yasuhiro Yoshikai, Tadahaya Mizuno, Shumpei Nemoto, Hiroyuki Kusuhara. (2024)<br><strong>A novel molecule generative model of VAE combined with Transformer</strong><br><button class=copy-to-clipboard title="A novel molecule generative model of VAE combined with Transformer" index=335>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-335 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: q-bio.BM<br>Categories: J-2; I-2-7, cs-LG, physics-chem-ph, q-bio-BM, q-bio.BM<br>Keyword Score: 10<br>Keywords: Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11950v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11950v1.pdf filename=2402.11950v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recently, molecule generation using deep learning has been actively investigated in drug discovery. In this field, <b>Transformer</b> and VAE are widely used as powerful models, but they are rarely used in combination due to structural and performance mismatch of them. This study proposes a model that combines these two models through structural and parameter optimization in handling diverse molecules. The proposed model shows comparable performance to existing models in generating molecules, and showed by far superior performance in generating molecules with unseen structures. In addition, the proposed model successfully predicted molecular properties using the latent representation of VAE. Ablation studies suggested the advantage of VAE over other generative models like language model in generating novel molecules, and that the molecules can be described by ~32 dimensional variables, much smaller than existing descriptors and models. This study is expected to provide a virtual chemical library containing a wide variety of compounds for virtual screening and to enable efficient screening.</p></p class="citation"></blockquote><h2 id=cscg-3>cs.CG (3)</h2><h3 id=13--336346-two-online-map-matching-algorithms-based-on-analytic-hierarchy-process-and-fuzzy-logic-jeremy-j-lin-et-al-2024>(1/3 | 336/346) Two Online Map Matching Algorithms Based on Analytic Hierarchy Process and Fuzzy Logic (Jeremy J. Lin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jeremy J. Lin, Tomoro Mochida, Riley C. W. O&rsquo;Neill, Atsuro Yoshida, Masashi Yamazaki, Akinobu Sasada. (2024)<br><strong>Two Online Map Matching Algorithms Based on Analytic Hierarchy Process and Fuzzy Logic</strong><br><button class=copy-to-clipboard title="Two Online Map Matching Algorithms Based on Analytic Hierarchy Process and Fuzzy Logic" index=336>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-336 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CG<br>Categories: cs-AI, cs-CG, cs-CV, cs.CG<br>Keyword Score: 10<br>Keywords: Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11866v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11866v1.pdf filename=2402.11866v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Our aim of this paper is to develop new map matching algorithms and to improve upon previous work. We address two key approaches: Analytic Hierarchy Process (AHP) map matching and fuzzy logic map matching. AHP is a decision-making method that combines mathematical analysis with human judgment, and fuzzy logic is an approach to computing based on the degree of truth and aims at modeling the imprecise modes of <b>reasoning</b> from 0 to 1 rather than the usual boolean logic. Of these algorithms, the way of our applying AHP to map matching is newly developed in this paper, meanwhile, our application of fuzzy logic to map matching is mostly the same as existing research except for some small changes. Because of the common characteristic that both methods are designed to handle imprecise information and simplicity for implementation, we decided to use these methods.</p></p class="citation"></blockquote><h3 id=23--337346-flip-graphs-of-pseudo-triangulations-with-face-degree-at-most-4-maarten-löffler-et-al-2024>(2/3 | 337/346) Flip Graphs of Pseudo-Triangulations With Face Degree at Most 4 (Maarten Löffler et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Maarten Löffler, Tamara Mchedlidze, David Orden, Josef Tkadlec, Jules Wulms. (2024)<br><strong>Flip Graphs of Pseudo-Triangulations With Face Degree at Most 4</strong><br><button class=copy-to-clipboard title="Flip Graphs of Pseudo-Triangulations With Face Degree at Most 4" index=337>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-337 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CG<br>Categories: cs-CG, cs.CG<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12357v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12357v1.pdf filename=2402.12357v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A pseudo-triangle is a simple polygon with exactly three convex vertices, and all other vertices (if any) are distributed on three concave chains. A pseudo-triangulation~$\mathcal{T}$ of a point set~$P$ in~$\mathbb{R}^2$ is a partitioning of the convex hull of~$P$ into pseudo-triangles, such that the union of the vertices of the pseudo-triangles is exactly~$P$. We call a size-4 pseudo-triangle a dart. For a fixed $k\geq 1$, we study $k$-dart pseudo-triangulations ($k$-DPTs), that is, pseudo-triangulations in which exactly $k$ faces are darts and all other faces are triangles. We study the flip <b>graph</b> for such pseudo-triangulations, in which a flip exchanges the diagonals of a pseudo-quadrilatral. Our results are as follows. We prove that the flip <b>graph</b> of $1$-DPTs is generally not connected, and show how to compute its connected components. Furthermore, for $k$-DPTs on a point configuration called the double chain we analyze the structure of the flip <b>graph</b> on a more fine-grained level.</p></p class="citation"></blockquote><h3 id=33--338346-the-complexity-of-geodesic-spanners-using-steiner-points-sarita-de-berg-et-al-2024>(3/3 | 338/346) The Complexity of Geodesic Spanners using Steiner Points (Sarita de Berg et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sarita de Berg, Tim Ophelders, Irene Parada, Frank Staals, Jules Wulms. (2024)<br><strong>The Complexity of Geodesic Spanners using Steiner Points</strong><br><button class=copy-to-clipboard title="The Complexity of Geodesic Spanners using Steiner Points" index=338>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-338 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CG<br>Categories: cs-CG, cs-DS, cs.CG<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12110v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12110v1.pdf filename=2402.12110v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A geometric $t$-spanner $\mathcal{G}$ on a set $S$ of $n$ point sites in a metric space $P$ is a subgraph of the complete <b>graph</b> on $S$ such that for every pair of sites $p,q$ the distance in $\mathcal{G}$ is a most $t$ times the distance $d(p,q)$ in $P$. We call a connection between two sites in the spanner a link. In some settings, such as when $P$ is a simple polygon with $m$ vertices and a link is a shortest path in $P$, links can consist of $\Theta (m)$ segments and thus have non-constant complexity. The total spanner complexity is a recently-introduced measure of how compact a spanner is. In this paper, we study what happens if we are allowed to introduce $k$ Steiner points to reduce the spanner complexity. We study such Steiner spanners in simple polygons, polygonal domains, and edge-weighted trees. Surprisingly, we show that Steiner points have only limited utility. For a spanner that uses $k$ Steiner points, we provide an $\Omega(nm/k)$ lower bound on the worst-case complexity of any $(3-\varepsilon)$-spanner, and an $\Omega(mn^{1/(t+1)}/k^{1/(t+1)})$ lower bound on the worst-case complexity of any $(t-\varepsilon)$-spanner, for any constant $\varepsilon\in (0,1)$ and integer constant $t \geq 2$. These lower bounds hold in all settings. Additionally, we show NP-hardness for the problem of deciding whether a set of sites in a polygonal domain admits a $3$-spanner with a given maximum complexity using $k$ Steiner points. On the positive side, for trees we show how to build a $2t$-spanner that uses $k$ Steiner points and of complexity $O(mn^{1/t}/k^{1/t} + n \log (n/k))$, for any integer $t \geq 1$. We generalize this result to forests, and apply it to obtain a $2\sqrt{2}t$-spanner in a simple polygon or a $6t$-spanner in a polygonal domain, with total complexity $O(mn^{1/t}(\log k)^{1+1/t}/k^{1/t} + n\log^2 n)$.</p></p class="citation"></blockquote><h2 id=eessiv-1>eess.IV (1)</h2><h3 id=11--339346-fod-swin-net-angular-super-resolution-of-fiber-orientation-distribution-using-a-transformer-based-deep-model-mateus-oliveira-da-silva-et-al-2024>(1/1 | 339/346) FOD-Swin-Net: angular super resolution of fiber orientation distribution using a transformer-based deep model (Mateus Oliveira da Silva et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mateus Oliveira da Silva, Caio Pinheiro Santana, Diedre Santos do Carmo, Letícia Rittner. (2024)<br><strong>FOD-Swin-Net: angular super resolution of fiber orientation distribution using a transformer-based deep model</strong><br><button class=copy-to-clipboard title="FOD-Swin-Net: angular super resolution of fiber orientation distribution using a transformer-based deep model" index=339>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-339 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, cs-LG, eess-IV, eess.IV, q-bio-NC<br>Keyword Score: 10<br>Keywords: Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11775v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11775v1.pdf filename=2402.11775v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Identifying and characterizing brain fiber bundles can help to understand many diseases and conditions. An important step in this process is the estimation of fiber orientations using Diffusion-Weighted Magnetic Resonance Imaging (DW-MRI). However, obtaining robust orientation estimates demands high-resolution data, leading to lengthy acquisitions that are not always clinically available. In this work, we explore the use of automated angular super resolution from faster acquisitions to overcome this challenge. Using the publicly available Human Connectome Project (HCP) DW-MRI data, we trained a <b>transformer-based</b> deep learning architecture to achieve angular super resolution in fiber orientation distribution (FOD). Our patch-based methodology, FOD-Swin-Net, is able to bring a single-shell reconstruction driven from 32 directions to be comparable to a multi-shell 288 direction FOD reconstruction, greatly reducing the number of required directions on initial acquisition. Evaluations of the reconstructed FOD with Angular Correlation Coefficient and qualitative visualizations reveal superior performance than the state-of-the-art in HCP testing data. Open source code for reproducibility is available at <a href=https://github.com/MICLab-Unicamp/FOD-Swin-Net>https://github.com/MICLab-Unicamp/FOD-Swin-Net</a>.</p></p class="citation"></blockquote><h2 id=csdl-1>cs.DL (1)</h2><h3 id=11--340346-thinking-outside-the-black-box-insights-from-a-digital-exhibition-in-the-humanities-sebastian-barzaghi-et-al-2024>(1/1 | 340/346) Thinking Outside the Black Box: Insights from a Digital Exhibition in the Humanities (Sebastian Barzaghi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sebastian Barzaghi, Alice Bordignon, Bianca Gualandi, Silvio Peroni. (2024)<br><strong>Thinking Outside the Black Box: Insights from a Digital Exhibition in the Humanities</strong><br><button class=copy-to-clipboard title="Thinking Outside the Black Box: Insights from a Digital Exhibition in the Humanities" index=340>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-340 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DL<br>Categories: cs-DL, cs.DL<br>Keyword Score: 5<br>Keywords: Black Box<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12000v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12000v1.pdf filename=2402.12000v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>One of the main goals of Open Science is to make research more reproducible. There is no consensus, however, on what exactly &ldquo;reproducibility&rdquo; is, as opposed for example to &ldquo;replicability&rdquo;, and how it applies to different research fields. After a short review of the literature on reproducibility/replicability with a focus on the humanities, we describe how the creation of the digital twin of the temporary exhibition &ldquo;The Other Renaissance&rdquo; has been documented throughout, with different methods, but with constant attention to research transparency, openness and accountability. A careful documentation of the study design, data collection and analysis techniques helps reflect and make all possible influencing factors explicit, and is a fundamental tool for reliability and rigour and for opening the <b>&ldquo;black</b> <b>box&rdquo;</b> of research.</p></p class="citation"></blockquote><h2 id=mathst-1>math.ST (1)</h2><h3 id=11--341346-on-the-fourier-coefficients-of-high-dimensional-random-geometric-graphs-kiril-bangachev-et-al-2024>(1/1 | 341/346) On The Fourier Coefficients of High-Dimensional Random Geometric Graphs (Kiril Bangachev et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kiril Bangachev, Guy Bresler. (2024)<br><strong>On The Fourier Coefficients of High-Dimensional Random Geometric Graphs</strong><br><button class=copy-to-clipboard title="On The Fourier Coefficients of High-Dimensional Random Geometric Graphs" index=341>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-341 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.ST<br>Categories: cs-DM, math-PR, math-ST, math.ST, stat-TH<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12589v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12589v1.pdf filename=2402.12589v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The random geometric <b>graph</b> $\mathsf{RGG}(n,\mathbb{S}^{d-1}, p)$ is formed by sampling $n$ i.i.d. vectors ${V_i}_{i = 1}^n$ uniformly on $\mathbb{S}^{d-1}$ and placing an edge between pairs of vertices $i$ and $j$ for which $\langle V_i,V_j\rangle \ge \tau^p_d,$ where $\tau^p_d$ is such that the expected density is $p.$ We study the low-degree Fourier coefficients of the distribution $\mathsf{RGG}(n,\mathbb{S}^{d-1}, p)$ and its Gaussian analogue. Our main conceptual contribution is a novel two-step strategy for bounding Fourier coefficients which we believe is more widely applicable to studying latent space distributions. First, we localize the dependence among edges to few fragile edges. Second, we partition the space of latent vector configurations $(\mathsf{RGG}(n,\mathbb{S}^{d-1}, p))^{\otimes n}$ based on the set of fragile edges and on each subset of configurations, we define a noise operator acting independently on edges not incident (in an appropriate sense) to fragile edges. We apply the resulting bounds to: 1) Settle the low-degree polynomial complexity of distinguishing spherical and Gaussian random geometric <b>graphs</b> from Erdos-Renyi both in the case of observing a complete set of edges and in the non-adaptively chosen mask $\mathcal{M}$ model recently introduced by [MVW24]; 2) Exhibit a statistical-computational gap for distinguishing $\mathsf{RGG}$ and the planted coloring model [KVWX23] in a regime when $\mathsf{RGG}$ is distinguishable from Erdos-Renyi; 3) Reprove known bounds on the second eigenvalue of random geometric <b>graphs.</b></p></p class="citation"></blockquote><h2 id=cspl-1>cs.PL (1)</h2><h3 id=11--342346-ltl-learning-on-gpus-mojtaba-valizadeh-et-al-2024>(1/1 | 342/346) LTL learning on GPUs (Mojtaba Valizadeh et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mojtaba Valizadeh, Nathanaël Fijalkow, Martin Berger. (2024)<br><strong>LTL learning on GPUs</strong><br><button class=copy-to-clipboard title="LTL learning on GPUs" index=342>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-342 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.PL<br>Categories: 68, D-3, cs-AI, cs-PL, cs.PL<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12373v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12373v1.pdf filename=2402.12373v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Linear temporal logic (LTL) is widely used in industrial verification. LTL formulae can be learned from traces. Scaling LTL formula learning is an open problem. We implement the first GPU-based LTL learner using a novel form of enumerative program synthesis. The learner is sound and complete. Our <b>benchmarks</b> indicate that it handles traces at least 2048 times more numerous, and on average at least 46 times faster than existing state-of-the-art learners. This is achieved with, among others, novel branch-free LTL semantics that has $O(\log n)$ time complexity, where $n$ is trace length, while previous implementations are $O(n^2)$ or worse (assuming bitwise boolean operations and shifts by powers of 2 have unit costs &ndash; a realistic assumption on modern processors).</p></p class="citation"></blockquote><h2 id=csds-4>cs.DS (4)</h2><h3 id=14--343346-almost-linear-time-parameterized-algorithm-for-rankwidth-via-dynamic-rankwidth-tuukka-korhonen-et-al-2024>(1/4 | 343/346) Almost-linear time parameterized algorithm for rankwidth via dynamic rankwidth (Tuukka Korhonen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tuukka Korhonen, Marek Sokołowski. (2024)<br><strong>Almost-linear time parameterized algorithm for rankwidth via dynamic rankwidth</strong><br><button class=copy-to-clipboard title="Almost-linear time parameterized algorithm for rankwidth via dynamic rankwidth" index=343>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-343 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DS<br>Categories: cs-DM, cs-DS, cs.DS, math-CO<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12364v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12364v1.pdf filename=2402.12364v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We give an algorithm that given a <b>graph</b> $G$ with $n$ vertices and $m$ edges and an integer $k$, in time $O_k(n^{1+o(1)}) + O(m)$ either outputs a rank decomposition of $G$ of width at most $k$ or determines that the rankwidth of $G$ is larger than $k$; the $O_k(\cdot)$-notation hides factors depending on $k$. Our algorithm returns also a $(2^{k+1}-1)$-expression for cliquewidth, yielding a $(2^{k+1}-1)$-approximation algorithm for cliquewidth with the same running time. This improves upon the $O_k(n^2)$ time algorithm of Fomin and Korhonen [STOC 2022]. The main ingredient of our algorithm is a fully dynamic algorithm for maintaining rank decompositions of bounded width: We give a data structure that for a dynamic $n$-vertex <b>graph</b> $G$ that is updated by edge insertions and deletions maintains a rank decomposition of $G$ of width at most $4k$ under the promise that the rankwidth of $G$ never grows above $k$. The amortized running time of each update is $O_k(2^{\sqrt{\log n} \log \log n})$. The data structure furthermore can maintain whether $G$ satisfies some fixed ${\sf CMSO}_1$ property within the same running time. We also give a framework for performing ``dense&rsquo;&rsquo; edge updates inside a given set of vertices $X$, where the new edges inside $X$ are described by a given ${\sf CMSO}_1$ sentence and vertex labels, in amortized $O_k(|X| \cdot 2^{\sqrt{\log n} \log \log n})$ time. Our dynamic algorithm generalizes the dynamic treewidth algorithm of Korhonen, Majewski, Nadara, Pilipczuk, and Soko{\l}owski [FOCS 2023].</p></p class="citation"></blockquote><h3 id=24--344346-connectivity-labeling-in-faulty-colored-graphs-asaf-petruschka-et-al-2024>(2/4 | 344/346) Connectivity Labeling in Faulty Colored Graphs (Asaf Petruschka et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Asaf Petruschka, Shay Sapir, Elad Tzalik. (2024)<br><strong>Connectivity Labeling in Faulty Colored Graphs</strong><br><button class=copy-to-clipboard title="Connectivity Labeling in Faulty Colored Graphs" index=344>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-344 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DS<br>Categories: cs-DS, cs.DS<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12144v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12144v1.pdf filename=2402.12144v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Fault-tolerant connectivity labelings are schemes that, given an $n$-vertex <b>graph</b> $G=(V,E)$ and $f\geq 1$, produce succinct yet informative labels for the elements of the <b>graph.</b> Given only the labels of two vertices $u,v$ and of the elements in a faulty-set $F$ with $|F|\leq f$, one can determine if $u,v$ are connected in $G-F$, the surviving <b>graph</b> after removing $F$. For the edge or vertex faults models, i.e., $F\subseteq E$ or $F\subseteq V$, a sequence of recent work established schemes with $poly(f,\log n)$-bit labels. This paper considers the color faults model, recently introduced in the context of spanners [Petruschka, Sapir and Tzalik, ITCS'24], which accounts for known correlations between failures. Here, the edges (or vertices) of the input $G$ are arbitrarily colored, and the faulty elements in $F$ are colors; a failing color causes all edges (vertices) of that color to crash. Our main contribution is settling the label length complexity for connectivity under one color fault ($f=1$). The existing implicit solution, by applying the state-of-the-art scheme for edge faults of [Dory and Parter, PODC'21], might yield labels of $\Omega(n)$ bits. We provide a deterministic scheme with labels of $\tilde{O}(\sqrt{n})$ bits in the worst case, and a matching lower bound. Moreover, our scheme is universally optimal: even schemes tailored to handle only colorings of one specific <b>graph</b> topology cannot produce asymptotically smaller labels. We extend our labeling approach to yield a routing scheme avoiding a single forbidden color. We also consider the centralized setting, and show an $\tilde{O}(n)$-space oracle, answering connectivity queries under one color fault in $\tilde{O}(1)$ time. Turning to $f\geq 2$ color faults, we give a randomized labeling scheme with $\tilde{O}(n^{1-1/2^f})$-bit labels, along with a lower bound of $\Omega(n^{1-1/(f+1)})$ bits.</p></p class="citation"></blockquote><h3 id=34--345346-collision-free-robot-scheduling-duncan-adamson-et-al-2024>(3/4 | 345/346) Collision-Free Robot Scheduling (Duncan Adamson et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Duncan Adamson, Nathan Flaherty, Igor Potapov, Paul Spirakis. (2024)<br><strong>Collision-Free Robot Scheduling</strong><br><button class=copy-to-clipboard title="Collision-Free Robot Scheduling" index=345>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-345 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DS<br>Categories: cs-DS, cs.DS<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12019v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12019v1.pdf filename=2402.12019v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Robots are becoming an increasingly common part of scientific work within laboratory environments. In this paper, we investigate the problem of designing \emph{schedules} for completing a set of tasks at fixed locations with multiple robots in a laboratory. We represent the laboratory as a <b>graph</b> with tasks placed on fixed vertices and robots represented as agents, with the constraint that no two robots may occupy the same vertex at any given timestep. Each schedule is partitioned into a set of timesteps, corresponding to a walk through the <b>graph</b> (allowing for a robot to wait at a vertex to complete a task), with each timestep taking time equal to the time for a robot to move from one vertex to another and each task taking some given number of timesteps during the completion of which a robot must stay at the vertex containing the task. The goal is to determine a set of schedules, with one schedule for each robot, minimising the number of timesteps taken by the schedule taking the greatest number of timesteps within the set of schedules. We show that this problem is NP-complete for many simple classes of <b>graphs,</b> the problem of determining the fastest schedule, defined by the number of time steps required for a robot to visit every vertex in the schedule and complete every task assigned in its assigned schedule. Explicitly, we provide this result for complete <b>graphs,</b> bipartite <b>graphs,</b> star <b>graphs,</b> and planar <b>graphs.</b> Finally, we provide positive results for line <b>graphs,</b> showing that we can find an optimal set of schedules for $k$ robots completing $m$ tasks of equal length of a path of length $n$ in $O(kmn)$ time, and a $k$-approximation when the length of the tasks is unbounded.</p></p class="citation"></blockquote><h3 id=44--346346-buffered-streaming-edge-partitioning-adil-chhabra-et-al-2024>(4/4 | 346/346) Buffered Streaming Edge Partitioning (Adil Chhabra et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Adil Chhabra, Marcelo Fonseca Faraj, Christian Schulz, Daniel Seemaier. (2024)<br><strong>Buffered Streaming Edge Partitioning</strong><br><button class=copy-to-clipboard title="Buffered Streaming Edge Partitioning" index=346>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-346 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DS<br>Categories: cs-DS, cs.DS<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.11980v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.11980v1.pdf filename=2402.11980v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Addressing the challenges of processing massive <b>graphs,</b> which are prevalent in diverse fields such as social, biological, and technical networks, we introduce HeiStreamE and FreightE, two innovative (buffered) streaming algorithms designed for efficient edge partitioning of large-scale <b>graphs.</b> HeiStreamE utilizes an adapted Split-and-Connect <b>graph</b> model and a Fennel-based multilevel partitioning scheme, while FreightE partitions a hypergraph representation of the input <b>graph.</b> Besides ensuring superior solution quality, these approaches also overcome the limitations of existing algorithms by maintaining linear dependency on the <b>graph</b> size in both time and memory complexity with no dependence on the number of blocks of partition. Our comprehensive experimental analysis demonstrates that HeiStreamE outperforms current streaming algorithms and the re-streaming algorithm 2PS in partitioning quality (replication factor), and is more memory-efficient for real-world networks where the number of edges is far greater than the number of vertices. Further, FreightE is shown to produce fast and efficient partitions, particularly for higher numbers of partition blocks.</p></p class="citation"></blockquote></div><div class="row pl-3 pr-3"><div class="col-md-6 share-buttons"></div></div><hr><div class="row next-prev-navigator"><div class="col-md-6 previous-article"><a href=/akitenkrad-blog/posts/arxiv/202402/20240220000000/ title="arXiv @ 2024.02.20" class="btn btn-outline-info"><div><i class="fas fa-chevron-circle-left"></i> Prev</div><div class=next-prev-text>arXiv @ 2024.02.20</div></a></div><div class="col-md-6 next-article"><a href=/akitenkrad-blog/posts/arxiv/202402/20240222000000/ title="arXiv @ 2024.02.22" class="btn btn-outline-info"><div>Next <i class="fas fa-chevron-circle-right"></i></div><div class=next-prev-text>arXiv @ 2024.02.22</div></a></div></div><hr></div></div></div><a id=scroll-to-top class=btn><i class="fas fa-chevron-circle-up"></i></a></section><section class=toc-section id=toc-section><div class=toc-holder><h5 class="text-center pl-3">Table of Contents</h5><hr><div class=toc><nav id=TableOfContents><ul><li><a href=#primary-categories>Primary Categories</a></li><li><a href=#keywords>Keywords</a></li><li><a href=#cscl-109>cs.CL (109)</a><ul><li><a href=#1109--1346-artprompt-ascii-art-based-jailbreak-attacks-against-aligned-llms-fengqing-jiang-et-al-2024>(1/109 | 1/346) ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs (Fengqing Jiang et al., 2024)</a></li><li><a href=#2109--2346-graph-based-retriever-captures-the-long-tail-of-biomedical-knowledge-julien-delile-et-al-2024>(2/109 | 2/346) Graph-Based Retriever Captures the Long Tail of Biomedical Knowledge (Julien Delile et al., 2024)</a></li><li><a href=#3109--3346-is-open-source-there-yet-a-comparative-study-on-commercial-and-open-source-llms-in-their-ability-to-label-chest-x-ray-reports-felix-j-dorfner-et-al-2024>(3/109 | 3/346) Is Open-Source There Yet? A Comparative Study on Commercial and Open-Source LLMs in Their Ability to Label Chest X-Ray Reports (Felix J. Dorfner et al., 2024)</a></li><li><a href=#4109--4346-bider-bridging-knowledge-inconsistency-for-efficient-retrieval-augmented-llms-via-key-supporting-evidence-jiajie-jin-et-al-2024>(4/109 | 4/346) BIDER: Bridging Knowledge Inconsistency for Efficient Retrieval-Augmented LLMs via Key Supporting Evidence (Jiajie Jin et al., 2024)</a></li><li><a href=#5109--5346-your-large-language-model-is-secretly-a-fairness-proponent-and-you-should-prompt-it-like-one-tianlin-li-et-al-2024>(5/109 | 5/346) Your Large Language Model is Secretly a Fairness Proponent and You Should Prompt it Like One (Tianlin Li et al., 2024)</a></li><li><a href=#6109--6346-meta-ranking-less-capable-language-models-are-capable-for-single-response-judgement-zijun-liu-et-al-2024>(6/109 | 6/346) Meta Ranking: Less Capable Language Models are Capable for Single Response Judgement (Zijun Liu et al., 2024)</a></li><li><a href=#7109--7346-modularized-networks-for-few-shot-hateful-meme-detection-rui-cao-et-al-2024>(7/109 | 7/346) Modularized Networks for Few-shot Hateful Meme Detection (Rui Cao et al., 2024)</a></li><li><a href=#8109--8346-end-to-end-multilingual-fact-checking-at-scale-vinay-setty-2024>(8/109 | 8/346) End-to-end multilingual fact-checking at scale (Vinay Setty, 2024)</a></li><li><a href=#9109--9346-creating-a-fine-grained-entity-type-taxonomy-using-llms-michael-gunn-et-al-2024>(9/109 | 9/346) Creating a Fine Grained Entity Type Taxonomy Using LLMs (Michael Gunn et al., 2024)</a></li><li><a href=#10109--10346-do-large-language-models-understand-logic-or-just-mimick-context-junbing-yan-et-al-2024>(10/109 | 10/346) Do Large Language Models Understand Logic or Just Mimick Context? (Junbing Yan et al., 2024)</a></li><li><a href=#11109--11346-where-it-really-matters-few-shot-environmental-conservation-media-monitoring-for-low-resource-languages-sameer-jain-et-al-2024>(11/109 | 11/346) Where It Really Matters: Few-Shot Environmental Conservation Media Monitoring for Low-Resource Languages (Sameer Jain et al., 2024)</a></li><li><a href=#12109--12346-structured-chain-of-thought-prompting-for-few-shot-generation-of-content-grounded-qa-conversations-md-arafat-sultan-et-al-2024>(12/109 | 12/346) Structured Chain-of-Thought Prompting for Few-Shot Generation of Content-Grounded QA Conversations (Md Arafat Sultan et al., 2024)</a></li><li><a href=#13109--13346-in-context-learning-demonstration-selection-via-influence-analysis-vinay-m-s-et-al-2024>(13/109 | 13/346) In-Context Learning Demonstration Selection via Influence Analysis (Vinay M. S. et al., 2024)</a></li><li><a href=#14109--14346-rjua-meddqa-a-multimodal-benchmark-for-medical-document-question-answering-and-clinical-reasoning-congyun-jin-et-al-2024>(14/109 | 14/346) RJUA-MedDQA: A Multimodal Benchmark for Medical Document Question Answering and Clinical Reasoning (Congyun Jin et al., 2024)</a></li><li><a href=#15109--15346-arks-active-retrieval-in-knowledge-soup-for-code-generation-hongjin-su-et-al-2024>(15/109 | 15/346) ARKS: Active Retrieval in Knowledge Soup for Code Generation (Hongjin Su et al., 2024)</a></li><li><a href=#16109--16346-mrke-the-multi-hop-reasoning-evaluation-of-llms-by-knowledge-edition-jian-wu-et-al-2024>(16/109 | 16/346) MRKE: The Multi-hop Reasoning Evaluation of LLMs by Knowledge Edition (Jian Wu et al., 2024)</a></li><li><a href=#17109--17346-parallel-structures-in-pre-training-data-yield-in-context-learning-yanda-chen-et-al-2024>(17/109 | 17/346) Parallel Structures in Pre-training Data Yield In-Context Learning (Yanda Chen et al., 2024)</a></li><li><a href=#18109--18346-reformatted-alignment-run-ze-fan-et-al-2024>(18/109 | 18/346) Reformatted Alignment (Run-Ze Fan et al., 2024)</a></li><li><a href=#19109--19346-distilling-large-language-models-for-text-attributed-graph-learning-bo-pan-et-al-2024>(19/109 | 19/346) Distilling Large Language Models for Text-Attributed Graph Learning (Bo Pan et al., 2024)</a></li><li><a href=#20109--20346-sibo-a-simple-booster-for-parameter-efficient-fine-tuning-zhihao-wen-et-al-2024>(20/109 | 20/346) SIBO: A Simple Booster for Parameter-Efficient Fine-Tuning (Zhihao Wen et al., 2024)</a></li><li><a href=#21109--21346-standardize-aligning-language-models-with-expert-defined-standards-for-content-generation-joseph-marvin-imperial-et-al-2024>(21/109 | 21/346) Standardize: Aligning Language Models with Expert-Defined Standards for Content Generation (Joseph Marvin Imperial et al., 2024)</a></li><li><a href=#22109--22346-task-oriented-dialogue-with-in-context-learning-tom-bocklisch-et-al-2024>(22/109 | 22/346) Task-Oriented Dialogue with In-Context Learning (Tom Bocklisch et al., 2024)</a></li><li><a href=#23109--23346-analysis-of-multidomain-abstractive-summarization-using-salience-allocation-tohida-rehman-et-al-2024>(23/109 | 23/346) Analysis of Multidomain Abstractive Summarization Using Salience Allocation (Tohida Rehman et al., 2024)</a></li><li><a href=#24109--24346-investigating-multi-hop-factual-shortcuts-in-knowledge-editing-of-large-language-models-tianjie-ju-et-al-2024>(24/109 | 24/346) Investigating Multi-Hop Factual Shortcuts in Knowledge Editing of Large Language Models (Tianjie Ju et al., 2024)</a></li><li><a href=#25109--25346-chatgpt-based-data-augmentation-for-improved-parameter-efficient-debiasing-of-llms-pengrui-han-et-al-2024>(25/109 | 25/346) ChatGPT Based Data Augmentation for Improved Parameter-Efficient Debiasing of LLMs (Pengrui Han et al., 2024)</a></li><li><a href=#26109--26346-speech-translation-with-speech-foundation-models-and-large-language-models-what-is-there-and-what-is-missing-marco-gaido-et-al-2024>(26/109 | 26/346) Speech Translation with Speech Foundation Models and Large Language Models: What is There and What is Missing? (Marco Gaido et al., 2024)</a></li><li><a href=#27109--27346-hu-at-semeval-2024-task-8a-can-contrastive-learning-learn-embeddings-to-detect-machine-generated-text-shubhashis-roy-dipta-et-al-2024>(27/109 | 27/346) HU at SemEval-2024 Task 8A: Can Contrastive Learning Learn Embeddings to Detect Machine-Generated Text? (Shubhashis Roy Dipta et al., 2024)</a></li><li><a href=#28109--28346-a-synthetic-data-approach-for-domain-generalization-of-nli-models-mohammad-javad-hosseini-et-al-2024>(28/109 | 28/346) A synthetic data approach for domain generalization of NLI models (Mohammad Javad Hosseini et al., 2024)</a></li><li><a href=#29109--29346-neo-bench-evaluating-robustness-of-large-language-models-with-neologisms-jonathan-zheng-et-al-2024>(29/109 | 29/346) NEO-BENCH: Evaluating Robustness of Large Language Models with Neologisms (Jonathan Zheng et al., 2024)</a></li><li><a href=#30109--30346-model-tailor-mitigating-catastrophic-forgetting-in-multi-modal-large-language-models-didi-zhu-et-al-2024>(30/109 | 30/346) Model Tailor: Mitigating Catastrophic Forgetting in Multi-modal Large Language Models (Didi Zhu et al., 2024)</a></li><li><a href=#31109--31346-trustscore-reference-free-evaluation-of-llm-response-trustworthiness-danna-zheng-et-al-2024>(31/109 | 31/346) TrustScore: Reference-Free Evaluation of LLM Response Trustworthiness (Danna Zheng et al., 2024)</a></li><li><a href=#32109--32346-your-vision-language-model-itself-is-a-strong-filter-towards-high-quality-instruction-tuning-with-data-selection-ruibo-chen-et-al-2024>(32/109 | 32/346) Your Vision-Language Model Itself Is a Strong Filter: Towards High-Quality Instruction Tuning with Data Selection (Ruibo Chen et al., 2024)</a></li><li><a href=#33109--33346-gtbench-uncovering-the-strategic-reasoning-limitations-of-llms-via-game-theoretic-evaluations-jinhao-duan-et-al-2024>(33/109 | 33/346) GTBench: Uncovering the Strategic Reasoning Limitations of LLMs via Game-Theoretic Evaluations (Jinhao Duan et al., 2024)</a></li><li><a href=#34109--34346-emulated-disalignment-safety-alignment-for-large-language-models-may-backfire-zhanhui-zhou-et-al-2024>(34/109 | 34/346) Emulated Disalignment: Safety Alignment for Large Language Models May Backfire! (Zhanhui Zhou et al., 2024)</a></li><li><a href=#35109--35346-same-task-more-tokens-the-impact-of-input-length-on-the-reasoning-performance-of-large-language-models-mosh-levy-et-al-2024>(35/109 | 35/346) Same Task, More Tokens: the Impact of Input Length on the Reasoning Performance of Large Language Models (Mosh Levy et al., 2024)</a></li><li><a href=#36109--36346-enhancing-multilingual-capabilities-of-large-language-models-through-self-distillation-from-resource-rich-languages-yuanchi-zhang-et-al-2024>(36/109 | 36/346) Enhancing Multilingual Capabilities of Large Language Models through Self-Distillation from Resource-Rich Languages (Yuanchi Zhang et al., 2024)</a></li><li><a href=#37109--37346-towards-cross-tokenizer-distillation-the-universal-logit-distillation-loss-for-llms-nicolas-boizard-et-al-2024>(37/109 | 37/346) Towards Cross-Tokenizer Distillation: the Universal Logit Distillation Loss for LLMs (Nicolas Boizard et al., 2024)</a></li><li><a href=#38109--38346-uncovering-latent-human-wellbeing-in-language-model-embeddings-pedro-freire-et-al-2024>(38/109 | 38/346) Uncovering Latent Human Wellbeing in Language Model Embeddings (Pedro Freire et al., 2024)</a></li><li><a href=#39109--39346-artifacts-or-abduction-how-do-llms-answer-multiple-choice-questions-without-the-question-nishant-balepur-et-al-2024>(39/109 | 39/346) Artifacts or Abduction: How Do LLMs Answer Multiple-Choice Questions Without the Question? (Nishant Balepur et al., 2024)</a></li><li><a href=#40109--40346-browse-and-concentrate-comprehending-multimodal-content-via-prior-llm-context-fusion-ziyue-wang-et-al-2024>(40/109 | 40/346) Browse and Concentrate: Comprehending Multimodal Content via prior-LLM Context Fusion (Ziyue Wang et al., 2024)</a></li><li><a href=#41109--41346-adaptive-skeleton-graph-decoding-shuowei-jin-et-al-2024>(41/109 | 41/346) Adaptive Skeleton Graph Decoding (Shuowei Jin et al., 2024)</a></li><li><a href=#42109--42346-stick-to-your-role-stability-of-personal-values-expressed-in-large-language-models-grgur-kovač-et-al-2024>(42/109 | 42/346) Stick to your Role! Stability of Personal Values Expressed in Large Language Models (Grgur Kovač et al., 2024)</a></li><li><a href=#43109--43346-transformer-based-causal-language-models-perform-clustering-xinbo-wu-et-al-2024>(43/109 | 43/346) Transformer-based Causal Language Models Perform Clustering (Xinbo Wu et al., 2024)</a></li><li><a href=#44109--44346-emobench-evaluating-the-emotional-intelligence-of-large-language-models-sahand-sabour-et-al-2024>(44/109 | 44/346) EmoBench: Evaluating the Emotional Intelligence of Large Language Models (Sahand Sabour et al., 2024)</a></li><li><a href=#45109--45346-how-interpretable-are-reasoning-explanations-from-prompting-large-language-models-yeo-wei-jie-et-al-2024>(45/109 | 45/346) How Interpretable are Reasoning Explanations from Prompting Large Language Models? (Yeo Wei Jie et al., 2024)</a></li><li><a href=#46109--46346-fipo-free-form-instruction-oriented-prompt-optimization-with-preference-dataset-and-modular-fine-tuning-schema-junru-lu-et-al-2024>(46/109 | 46/346) FIPO: Free-form Instruction-oriented Prompt Optimization with Preference Dataset and Modular Fine-tuning Schema (Junru Lu et al., 2024)</a></li><li><a href=#47109--47346-language-models-are-homer-simpson-safety-re-alignment-of-fine-tuned-language-models-through-task-arithmetic-rishabh-bhardwaj-et-al-2024>(47/109 | 47/346) Language Models are Homer Simpson! Safety Re-Alignment of Fine-tuned Language Models through Task Arithmetic (Rishabh Bhardwaj et al., 2024)</a></li><li><a href=#48109--48346-tilp-differentiable-learning-of-temporal-logical-rules-on-knowledge-graphs-siheng-xiong-et-al-2024>(48/109 | 48/346) TILP: Differentiable Learning of Temporal Logical Rules on Knowledge Graphs (Siheng Xiong et al., 2024)</a></li><li><a href=#49109--49346-asynchronous-and-segmented-bidirectional-encoding-for-nmt-jingpu-yang-et-al-2024>(49/109 | 49/346) Asynchronous and Segmented Bidirectional Encoding for NMT (Jingpu Yang et al., 2024)</a></li><li><a href=#50109--50346-understanding-fine-grained-distortions-in-reports-of-scientific-findings-amelie-wührl-et-al-2024>(50/109 | 50/346) Understanding Fine-grained Distortions in Reports of Scientific Findings (Amelie Wührl et al., 2024)</a></li><li><a href=#51109--51346-query-based-adversarial-prompt-generation-jonathan-hayase-et-al-2024>(51/109 | 51/346) Query-Based Adversarial Prompt Generation (Jonathan Hayase et al., 2024)</a></li><li><a href=#52109--52346-key-ingredients-for-effective-zero-shot-cross-lingual-knowledge-transfer-in-generative-tasks-nadezhda-chirkova-et-al-2024>(52/109 | 52/346) Key ingredients for effective zero-shot cross-lingual knowledge transfer in generative tasks (Nadezhda Chirkova et al., 2024)</a></li><li><a href=#53109--53346-analysis-of-levenshtein-transformers-decoder-and-its-variants-ruiyang-zhou-2024>(53/109 | 53/346) Analysis of Levenshtein Transformer&rsquo;s Decoder and Its Variants (Ruiyang Zhou, 2024)</a></li><li><a href=#54109--54346-polarization-of-autonomous-generative-ai-agents-under-echo-chambers-masaya-ohagi-2024>(54/109 | 54/346) Polarization of Autonomous Generative AI Agents Under Echo Chambers (Masaya Ohagi, 2024)</a></li><li><a href=#55109--55346-a-chinese-dataset-for-evaluating-the-safeguards-in-large-language-models-yuxia-wang-et-al-2024>(55/109 | 55/346) A Chinese Dataset for Evaluating the Safeguards in Large Language Models (Yuxia Wang et al., 2024)</a></li><li><a href=#56109--56346-is-it-a-free-lunch-for-removing-outliers-during-pretraining-baohao-liao-et-al-2024>(56/109 | 56/346) Is It a Free Lunch for Removing Outliers during Pretraining? (Baohao Liao et al., 2024)</a></li><li><a href=#57109--57346-small-models-big-insights-leveraging-slim-proxy-models-to-decide-when-and-what-to-retrieve-for-llms-jiejun-tan-et-al-2024>(57/109 | 57/346) Small Models, Big Insights: Leveraging Slim Proxy Models To Decide When and What to Retrieve for LLMs (Jiejun Tan et al., 2024)</a></li><li><a href=#58109--58346-remember-this-event-that-year-assessing-temporal-information-and-reasoning-in-large-language-models-himanshu-beniwal-et-al-2024>(58/109 | 58/346) Remember This Event That Year? Assessing Temporal Information and Reasoning in Large Language Models (Himanshu Beniwal et al., 2024)</a></li><li><a href=#59109--59346-direct-large-language-model-alignment-through-self-rewarding-contrastive-prompt-distillation-aiwei-liu-et-al-2024>(59/109 | 59/346) Direct Large Language Model Alignment Through Self-Rewarding Contrastive Prompt Distillation (Aiwei Liu et al., 2024)</a></li><li><a href=#60109--60346-the-colorful-future-of-llms-evaluating-and-improving-llms-as-emotional-supporters-for-queer-youth-shir-lissak-et-al-2024>(60/109 | 60/346) The Colorful Future of LLMs: Evaluating and Improving LLMs as Emotional Supporters for Queer Youth (Shir Lissak et al., 2024)</a></li><li><a href=#61109--61346-generation-meets-verification-accelerating-large-language-model-inference-with-smart-parallel-auto-correct-decoding-hanling-yi-et-al-2024>(61/109 | 61/346) Generation Meets Verification: Accelerating Large Language Model Inference with Smart Parallel Auto-Correct Decoding (Hanling Yi et al., 2024)</a></li><li><a href=#62109--62346-unveiling-the-magic-investigating-attention-distillation-in-retrieval-augmented-generation-zizhong-li-et-al-2024>(62/109 | 62/346) Unveiling the Magic: Investigating Attention Distillation in Retrieval-augmented Generation (Zizhong Li et al., 2024)</a></li><li><a href=#63109--63346-mars-meaning-aware-response-scoring-for-uncertainty-estimation-in-generative-llms-yavuz-faruk-bakman-et-al-2024>(63/109 | 63/346) MARS: Meaning-Aware Response Scoring for Uncertainty Estimation in Generative LLMs (Yavuz Faruk Bakman et al., 2024)</a></li><li><a href=#64109--64346-ontology-enhanced-claim-detection-zehra-melce-hüsünbeyi-et-al-2024>(64/109 | 64/346) Ontology Enhanced Claim Detection (Zehra Melce Hüsünbeyi et al., 2024)</a></li><li><a href=#65109--65346-analobench-benchmarking-the-identification-of-abstract-and-long-context-analogies-xiao-ye-et-al-2024>(65/109 | 65/346) AnaloBench: Benchmarking the Identification of Abstract and Long-context Analogies (Xiao Ye et al., 2024)</a></li><li><a href=#66109--66346-lemma-towards-lvlm-enhanced-multimodal-misinformation-detection-with-external-knowledge-augmentation-keyang-xuan-et-al-2024>(66/109 | 66/346) LEMMA: Towards LVLM-Enhanced Multimodal Misinformation Detection with External Knowledge Augmentation (Keyang Xuan et al., 2024)</a></li><li><a href=#67109--67346-shallow-synthesis-of-knowledge-in-gpt-generated-texts-a-case-study-in-automatic-related-work-composition-anna-martin-boyle-et-al-2024>(67/109 | 67/346) Shallow Synthesis of Knowledge in GPT-Generated Texts: A Case Study in Automatic Related Work Composition (Anna Martin-Boyle et al., 2024)</a></li><li><a href=#68109--68346-understanding-the-effects-of-noise-in-text-to-sql-an-examination-of-the-bird-bench-benchmark-niklas-wretblad-et-al-2024>(68/109 | 68/346) Understanding the Effects of Noise in Text-to-SQL: An Examination of the BIRD-Bench Benchmark (Niklas Wretblad et al., 2024)</a></li><li><a href=#69109--69346-a-systematic-comparison-of-contextualized-word-embeddings-for-lexical-semantic-change-francesco-periti-et-al-2024>(69/109 | 69/346) A Systematic Comparison of Contextualized Word Embeddings for Lexical Semantic Change (Francesco Periti et al., 2024)</a></li><li><a href=#70109--70346-learning-to-edit-aligning-llms-with-knowledge-editing-yuxin-jiang-et-al-2024>(70/109 | 70/346) Learning to Edit: Aligning LLMs with Knowledge Editing (Yuxin Jiang et al., 2024)</a></li><li><a href=#71109--71346-genaudit-fixing-factual-errors-in-language-model-outputs-with-evidence-kundan-krishna-et-al-2024>(71/109 | 71/346) GenAudit: Fixing Factual Errors in Language Model Outputs with Evidence (Kundan Krishna et al., 2024)</a></li><li><a href=#72109--72346-confidence-matters-revisiting-intrinsic-self-correction-capabilities-of-large-language-models-loka-li-et-al-2024>(72/109 | 72/346) Confidence Matters: Revisiting Intrinsic Self-Correction Capabilities of Large Language Models (Loka Li et al., 2024)</a></li><li><a href=#73109--73346-emergent-word-order-universals-from-cognitively-motivated-language-models-tatsuki-kuribayashi-et-al-2024>(73/109 | 73/346) Emergent Word Order Universals from Cognitively-Motivated Language Models (Tatsuki Kuribayashi et al., 2024)</a></li><li><a href=#74109--74346-high-quality-data-to-text-generation-for-severely-under-resourced-languages-with-out-of-the-box-large-language-models-michela-lorandi-et-al-2024>(74/109 | 74/346) High-quality Data-to-Text Generation for Severely Under-Resourced Languages with Out-of-the-box Large Language Models (Michela Lorandi et al., 2024)</a></li><li><a href=#75109--75346-empirical-study-on-updating-key-value-memories-in-transformer-feed-forward-layers-zihan-qiu-et-al-2024>(75/109 | 75/346) Empirical Study on Updating Key-Value Memories in Transformer Feed-forward Layers (Zihan Qiu et al., 2024)</a></li><li><a href=#76109--76346-groot-adversarial-testing-for-generative-text-to-image-models-with-tree-based-semantic-transformation-yi-liu-et-al-2024>(76/109 | 76/346) Groot: Adversarial Testing for Generative Text-to-Image Models with Tree-based Semantic Transformation (Yi Liu et al., 2024)</a></li><li><a href=#77109--77346-can-llms-compute-with-reasons-harshit-sandilya-et-al-2024>(77/109 | 77/346) Can LLMs Compute with Reasons? (Harshit Sandilya et al., 2024)</a></li><li><a href=#78109--78346-are-llm-based-evaluators-confusing-nlg-quality-criteria-xinyu-hu-et-al-2024>(78/109 | 78/346) Are LLM-based Evaluators Confusing NLG Quality Criteria? (Xinyu Hu et al., 2024)</a></li><li><a href=#79109--79346-automatic-evaluation-for-mental-health-counseling-using-llms-anqi-li-et-al-2024>(79/109 | 79/346) Automatic Evaluation for Mental Health Counseling using LLMs (Anqi Li et al., 2024)</a></li><li><a href=#80109--80346-sola-solver-layer-adaption-of-llm-for-better-logic-reasoning-yu-zhang-et-al-2024>(80/109 | 80/346) SoLA: Solver-Layer Adaption of LLM for Better Logic Reasoning (Yu Zhang et al., 2024)</a></li><li><a href=#81109--81346-revisiting-knowledge-distillation-for-autoregressive-language-models-qihuang-zhong-et-al-2024>(81/109 | 81/346) Revisiting Knowledge Distillation for Autoregressive Language Models (Qihuang Zhong et al., 2024)</a></li><li><a href=#82109--82346-rose-doesnt-do-that-boosting-the-safety-of-instruction-tuned-large-language-models-with-reverse-prompt-contrastive-decoding-qihuang-zhong-et-al-2024>(82/109 | 82/346) ROSE Doesn&rsquo;t Do That: Boosting the Safety of Instruction-Tuned Large Language Models with Reverse Prompt Contrastive Decoding (Qihuang Zhong et al., 2024)</a></li><li><a href=#83109--83346-head-wise-shareable-attention-for-large-language-models-zouying-cao-et-al-2024>(83/109 | 83/346) Head-wise Shareable Attention for Large Language Models (Zouying Cao et al., 2024)</a></li><li><a href=#84109--84346-what-evidence-do-language-models-find-convincing-alexander-wan-et-al-2024>(84/109 | 84/346) What Evidence Do Language Models Find Convincing? (Alexander Wan et al., 2024)</a></li><li><a href=#85109--85346-rfbes-at-semeval-2024-task-8-investigating-syntactic-and-semantic-features-for-distinguishing-ai-generated-and-human-written-texts-mohammad-heydari-rad-et-al-2024>(85/109 | 85/346) RFBES at SemEval-2024 Task 8: Investigating Syntactic and Semantic Features for Distinguishing AI-Generated and Human-Written Texts (Mohammad Heydari Rad et al., 2024)</a></li><li><a href=#86109--86346-m2k-vdg-model-adaptive-multimodal-knowledge-anchor-enhanced-video-grounded-dialogue-generation-hongcheng-liu-et-al-2024>(86/109 | 86/346) M2K-VDG: Model-Adaptive Multimodal Knowledge Anchor Enhanced Video-grounded Dialogue Generation (Hongcheng Liu et al., 2024)</a></li><li><a href=#87109--87346-anygpt-unified-multimodal-llm-with-discrete-sequence-modeling-jun-zhan-et-al-2024>(87/109 | 87/346) AnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling (Jun Zhan et al., 2024)</a></li><li><a href=#88109--88346-triple-encoders-representations-that-fire-together-wire-together-justus-jonas-erker-et-al-2024>(88/109 | 88/346) Triple-Encoders: Representations That Fire Together, Wire Together (Justus-Jonas Erker et al., 2024)</a></li><li><a href=#89109--89346-hunflair2-in-a-cross-corpus-evaluation-of-biomedical-named-entity-recognition-and-normalization-tools-mario-sänger-et-al-2024>(89/109 | 89/346) HunFlair2 in a cross-corpus evaluation of biomedical named entity recognition and normalization tools (Mario Sänger et al., 2024)</a></li><li><a href=#90109--90346-language-model-adaptation-to-specialized-domains-through-selective-masking-based-on-genre-and-topical-characteristics-anas-belfathi-et-al-2024>(90/109 | 90/346) Language Model Adaptation to Specialized Domains through Selective Masking based on Genre and Topical Characteristics (Anas Belfathi et al., 2024)</a></li><li><a href=#91109--91346-comprehensive-cognitive-llm-agent-for-smartphone-gui-automation-xinbei-ma-et-al-2024>(91/109 | 91/346) Comprehensive Cognitive LLM Agent for Smartphone GUI Automation (Xinbei Ma et al., 2024)</a></li><li><a href=#92109--92346-have-seen-me-before-automating-dataset-updates-towards-reliable-and-timely-evaluation-jiahao-ying-et-al-2024>(92/109 | 92/346) Have Seen Me Before? Automating Dataset Updates Towards Reliable and Timely Evaluation (Jiahao Ying et al., 2024)</a></li><li><a href=#93109--93346-archer-a-human-labeled-text-to-sql-dataset-with-arithmetic-commonsense-and-hypothetical-reasoning-danna-zheng-et-al-2024>(93/109 | 93/346) Archer: A Human-Labeled Text-to-SQL Dataset with Arithmetic, Commonsense and Hypothetical Reasoning (Danna Zheng et al., 2024)</a></li><li><a href=#94109--94346-do-pre-trained-language-models-detect-and-understand-semantic-underspecification-ask-the-dust-frank-wildenburg-et-al-2024>(94/109 | 94/346) Do Pre-Trained Language Models Detect and Understand Semantic Underspecification? Ask the DUST! (Frank Wildenburg et al., 2024)</a></li><li><a href=#95109--95346-sequoia-scalable-robust-and-hardware-aware-speculative-decoding-zhuoming-chen-et-al-2024>(95/109 | 95/346) Sequoia: Scalable, Robust, and Hardware-aware Speculative Decoding (Zhuoming Chen et al., 2024)</a></li><li><a href=#96109--96346-zero-shot-vlms-for-hate-meme-detection-are-we-there-yet-naquee-rizwan-et-al-2024>(96/109 | 96/346) Zero shot VLMs for hate meme detection: Are we there yet? (Naquee Rizwan et al., 2024)</a></li><li><a href=#97109--97346-amplifying-training-data-exposure-through-fine-tuning-with-pseudo-labeled-memberships-myung-gyo-oh-et-al-2024>(97/109 | 97/346) Amplifying Training Data Exposure through Fine-Tuning with Pseudo-Labeled Memberships (Myung Gyo Oh et al., 2024)</a></li><li><a href=#98109--98346-purifying-large-language-models-by-ensembling-a-small-language-model-tianlin-li-et-al-2024>(98/109 | 98/346) Purifying Large Language Models by Ensembling a Small Language Model (Tianlin Li et al., 2024)</a></li><li><a href=#99109--99346-acquiring-clean-language-models-from-backdoor-poisoned-datasets-by-downscaling-frequency-space-zongru-wu-et-al-2024>(99/109 | 99/346) Acquiring Clean Language Models from Backdoor Poisoned Datasets by Downscaling Frequency Space (Zongru Wu et al., 2024)</a></li><li><a href=#100109--100346-compress-to-impress-unleashing-the-potential-of-compressive-memory-in-real-world-long-term-conversations-nuo-chen-et-al-2024>(100/109 | 100/346) Compress to Impress: Unleashing the Potential of Compressive Memory in Real-World Long-Term Conversations (Nuo Chen et al., 2024)</a></li><li><a href=#101109--101346-team-qust-at-semeval-2024-task-8-a-comprehensive-study-of-monolingual-and-multilingual-approaches-for-detecting-ai-generated-text-xiaoman-xu-et-al-2024>(101/109 | 101/346) Team QUST at SemEval-2024 Task 8: A Comprehensive Study of Monolingual and Multilingual Approaches for Detecting AI-generated Text (Xiaoman Xu et al., 2024)</a></li><li><a href=#102109--102346-semantic-textual-similarity-assessment-in-chest-x-ray-reports-using-a-domain-specific-cosine-based-metric-sayeh-gholipour-picha-et-al-2024>(102/109 | 102/346) Semantic Textual Similarity Assessment in Chest X-ray Reports Using a Domain-Specific Cosine-Based Metric (Sayeh Gholipour Picha et al., 2024)</a></li><li><a href=#103109--103346-evaluating-image-review-ability-of-vision-language-models-shigeki-saito-et-al-2024>(103/109 | 103/346) Evaluating Image Review Ability of Vision Language Models (Shigeki Saito et al., 2024)</a></li><li><a href=#104109--104346-evolving-ai-collectives-to-enhance-human-diversity-and-enable-self-regulation-shiyang-lai-et-al-2024>(104/109 | 104/346) Evolving AI Collectives to Enhance Human Diversity and Enable Self-Regulation (Shiyang Lai et al., 2024)</a></li><li><a href=#105109--105346-llm-agents-for-psychology-a-study-on-gamified-assessments-qisen-yang-et-al-2024>(105/109 | 105/346) LLM Agents for Psychology: A Study on Gamified Assessments (Qisen Yang et al., 2024)</a></li><li><a href=#106109--106346-karl-knowledge-aware-retrieval-and-representations-aid-retention-and-learning-in-students-matthew-shu-et-al-2024>(106/109 | 106/346) KARL: Knowledge-Aware Retrieval and Representations aid Retention and Learning in Students (Matthew Shu et al., 2024)</a></li><li><a href=#107109--107346-text-diffusion-with-reinforced-conditioning-yuxuan-liu-et-al-2024>(107/109 | 107/346) Text Diffusion with Reinforced Conditioning (Yuxuan Liu et al., 2024)</a></li><li><a href=#108109--108346-what-do-dialect-speakers-want-a-survey-of-attitudes-towards-language-technology-for-german-dialects-verena-blaschke-et-al-2024>(108/109 | 108/346) What Do Dialect Speakers Want? A Survey of Attitudes Towards Language Technology for German Dialects (Verena Blaschke et al., 2024)</a></li><li><a href=#109109--109346-causalgym-benchmarking-causal-interpretability-methods-on-linguistic-tasks-aryaman-arora-et-al-2024>(109/109 | 109/346) CausalGym: Benchmarking causal interpretability methods on linguistic tasks (Aryaman Arora et al., 2024)</a></li></ul></li><li><a href=#cslg-71>cs.LG (71)</a><ul><li><a href=#171--110346-a-critical-evaluation-of-ai-feedback-for-aligning-large-language-models-archit-sharma-et-al-2024>(1/71 | 110/346) A Critical Evaluation of AI Feedback for Aligning Large Language Models (Archit Sharma et al., 2024)</a></li><li><a href=#271--111346-spml-a-dsl-for-defending-language-models-against-prompt-attacks-reshabh-k-sharma-et-al-2024>(2/71 | 111/346) SPML: A DSL for Defending Language Models Against Prompt Attacks (Reshabh K Sharma et al., 2024)</a></li><li><a href=#371--112346-robust-clip-unsupervised-adversarial-fine-tuning-of-vision-embeddings-for-robust-large-vision-language-models-christian-schlarmann-et-al-2024>(3/71 | 112/346) Robust CLIP: Unsupervised Adversarial Fine-Tuning of Vision Embeddings for Robust Large Vision-Language Models (Christian Schlarmann et al., 2024)</a></li><li><a href=#471--113346-self-amplify-improving-small-language-models-with-self-post-hoc-explanations-milan-bhan-et-al-2024>(4/71 | 113/346) Self-AMPLIFY: Improving Small Language Models with Self Post Hoc Explanations (Milan Bhan et al., 2024)</a></li><li><a href=#571--114346-mafin-enhancing-black-box-embeddings-with-model-augmented-fine-tuning-mingtian-zhang-et-al-2024>(5/71 | 114/346) Mafin: Enhancing Black-Box Embeddings with Model Augmented Fine-tuning (Mingtian Zhang et al., 2024)</a></li><li><a href=#671--115346-hierarchical-bayes-approach-to-personalized-federated-unsupervised-learning-kaan-ozkara-et-al-2024>(6/71 | 115/346) Hierarchical Bayes Approach to Personalized Federated Unsupervised Learning (Kaan Ozkara et al., 2024)</a></li><li><a href=#771--116346-db-llm-accurate-dual-binarization-for-efficient-llms-hong-chen-et-al-2024>(7/71 | 116/346) DB-LLM: Accurate Dual-Binarization for Efficient LLMs (Hong Chen et al., 2024)</a></li><li><a href=#871--117346-towards-cross-domain-continual-learning-marcus-de-carvalho-et-al-2024>(8/71 | 117/346) Towards Cross-Domain Continual Learning (Marcus de Carvalho et al., 2024)</a></li><li><a href=#971--118346-uncertainty-quantification-in-fine-tuned-llms-using-lora-ensembles-oleksandr-balabanov-et-al-2024>(9/71 | 118/346) Uncertainty quantification in fine-tuned LLMs using LoRA ensembles (Oleksandr Balabanov et al., 2024)</a></li><li><a href=#1071--119346-wkvquant-quantizing-weight-and-keyvalue-cache-for-large-language-models-gains-more-yuxuan-yue-et-al-2024>(10/71 | 119/346) WKVQuant: Quantizing Weight and Key/Value Cache for Large Language Models Gains More (Yuxuan Yue et al., 2024)</a></li><li><a href=#1171--120346-unist-a-prompt-empowered-universal-model-for-urban-spatio-temporal-prediction-yuan-yuan-et-al-2024>(11/71 | 120/346) UniST: A Prompt-Empowered Universal Model for Urban Spatio-Temporal Prediction (Yuan Yuan et al., 2024)</a></li><li><a href=#1271--121346-tables-as-images-exploring-the-strengths-and-limitations-of-llms-on-multimodal-representations-of-tabular-data-naihao-deng-et-al-2024>(12/71 | 121/346) Tables as Images? Exploring the Strengths and Limitations of LLMs on Multimodal Representations of Tabular Data (Naihao Deng et al., 2024)</a></li><li><a href=#1371--122346-neuro-mimetic-task-free-unsupervised-online-learning-with-continual-self-organizing-maps-hitesh-vaidya-et-al-2024>(13/71 | 122/346) Neuro-mimetic Task-free Unsupervised Online Learning with Continual Self-Organizing Maps (Hitesh Vaidya et al., 2024)</a></li><li><a href=#1471--123346-synthetic-location-trajectory-generation-using-categorical-diffusion-models-simon-dirmeier-et-al-2024>(14/71 | 123/346) Synthetic location trajectory generation using categorical diffusion models (Simon Dirmeier et al., 2024)</a></li><li><a href=#1571--124346-endowing-pre-trained-graph-models-with-provable-fairness-zhongjian-zhang-et-al-2024>(15/71 | 124/346) Endowing Pre-trained Graph Models with Provable Fairness (Zhongjian Zhang et al., 2024)</a></li><li><a href=#1671--125346-a-generative-pre-training-framework-for-spatio-temporal-graph-transfer-learning-yuan-yuan-et-al-2024>(16/71 | 125/346) A Generative Pre-Training Framework for Spatio-Temporal Graph Transfer Learning (Yuan Yuan et al., 2024)</a></li><li><a href=#1771--126346-on-the-byzantine-resilience-of-distillation-based-federated-learning-christophe-roux-et-al-2024>(17/71 | 126/346) On the Byzantine-Resilience of Distillation-Based Federated Learning (Christophe Roux et al., 2024)</a></li><li><a href=#1871--127346-all-language-models-large-and-small-zhixun-chen-et-al-2024>(18/71 | 127/346) All Language Models Large and Small (Zhixun Chen et al., 2024)</a></li><li><a href=#1971--128346-reinforcement-learning-as-a-parsimonious-alternative-to-prediction-cascades-a-case-study-on-image-segmentation-bharat-srikishan-et-al-2024>(19/71 | 128/346) Reinforcement Learning as a Parsimonious Alternative to Prediction Cascades: A Case Study on Image Segmentation (Bharat Srikishan et al., 2024)</a></li><li><a href=#2071--129346-generative-semi-supervised-graph-anomaly-detection-hezhe-qiao-et-al-2024>(20/71 | 129/346) Generative Semi-supervised Graph Anomaly Detection (Hezhe Qiao et al., 2024)</a></li><li><a href=#2171--130346-locality-sensitive-hashing-based-efficient-point-transformer-with-applications-in-high-energy-physics-siqi-miao-et-al-2024>(21/71 | 130/346) Locality-Sensitive Hashing-Based Efficient Point Transformer with Applications in High-Energy Physics (Siqi Miao et al., 2024)</a></li><li><a href=#2271--131346-the-edge-of-reach-problem-in-offline-model-based-reinforcement-learning-anya-sims-et-al-2024>(22/71 | 131/346) The Edge-of-Reach Problem in Offline Model-Based Reinforcement Learning (Anya Sims et al., 2024)</a></li><li><a href=#2371--132346-convergence-of-gradient-descent-for-recurrent-neural-networks-a-nonasymptotic-analysis-semih-cayci-et-al-2024>(23/71 | 132/346) Convergence of Gradient Descent for Recurrent Neural Networks: A Nonasymptotic Analysis (Semih Cayci et al., 2024)</a></li><li><a href=#2471--133346-cluster-metric-sensitivity-to-irrelevant-features-miles-mccrory-et-al-2024>(24/71 | 133/346) Cluster Metric Sensitivity to Irrelevant Features (Miles McCrory et al., 2024)</a></li><li><a href=#2571--134346-ebft-effective-and-block-wise-fine-tuning-for-sparse-llms-song-guo-et-al-2024>(25/71 | 134/346) EBFT: Effective and Block-Wise Fine-Tuning for Sparse LLMs (Song Guo et al., 2024)</a></li><li><a href=#2671--135346-slade-detecting-dynamic-anomalies-in-edge-streams-without-labels-via-self-supervised-learning-jongha-lee-et-al-2024>(26/71 | 135/346) SLADE: Detecting Dynamic Anomalies in Edge Streams without Labels via Self-Supervised Learning (Jongha Lee et al., 2024)</a></li><li><a href=#2771--136346-microstructures-and-accuracy-of-graph-recall-by-large-language-models-yanbang-wang-et-al-2024>(27/71 | 136/346) Microstructures and Accuracy of Graph Recall by Large Language Models (Yanbang Wang et al., 2024)</a></li><li><a href=#2871--137346-induced-model-matching-how-restricted-models-can-help-larger-ones-usama-muneeb-et-al-2024>(28/71 | 137/346) Induced Model Matching: How Restricted Models Can Help Larger Ones (Usama Muneeb et al., 2024)</a></li><li><a href=#2971--138346-in-deep-reinforcement-learning-a-pruned-network-is-a-good-network-johan-obando-ceron-et-al-2024>(29/71 | 138/346) In deep reinforcement learning, a pruned network is a good network (Johan Obando-Ceron et al., 2024)</a></li><li><a href=#3071--139346-universal-physics-transformers-benedikt-alkin-et-al-2024>(30/71 | 139/346) Universal Physics Transformers (Benedikt Alkin et al., 2024)</a></li><li><a href=#3171--140346-generating-survival-interpretable-trajectories-and-data-andrei-v-konstantinov-et-al-2024>(31/71 | 140/346) Generating Survival Interpretable Trajectories and Data (Andrei V. Konstantinov et al., 2024)</a></li><li><a href=#3271--141346-towards-a-tailored-mixed-precision-sub-8bit-quantization-scheme-for-gated-recurrent-units-using-genetic-algorithms-riccardo-miccini-et-al-2024>(32/71 | 141/346) Towards a tailored mixed-precision sub-8bit quantization scheme for Gated Recurrent Units using Genetic Algorithms (Riccardo Miccini et al., 2024)</a></li><li><a href=#3371--142346-revisiting-data-augmentation-in-deep-reinforcement-learning-jianshu-hu-et-al-2024>(33/71 | 142/346) Revisiting Data Augmentation in Deep Reinforcement Learning (Jianshu Hu et al., 2024)</a></li><li><a href=#3471--143346-beyond-uniform-scaling-exploring-depth-heterogeneity-in-neural-architectures-akash-guna-r-t-et-al-2024>(34/71 | 143/346) Beyond Uniform Scaling: Exploring Depth Heterogeneity in Neural Architectures (Akash Guna R. T et al., 2024)</a></li><li><a href=#3571--144346-lora-training-in-the-ntk-regime-has-no-spurious-local-minima-uijeong-jang-et-al-2024>(35/71 | 144/346) LoRA Training in the NTK Regime has No Spurious Local Minima (Uijeong Jang et al., 2024)</a></li><li><a href=#3671--145346-parcv2-physics-aware-recurrent-convolutional-neural-networks-for-spatiotemporal-dynamics-modeling-phong-c-h-nguyen-et-al-2024>(36/71 | 145/346) PARCv2: Physics-aware Recurrent Convolutional Neural Networks for Spatiotemporal Dynamics Modeling (Phong C. H. Nguyen et al., 2024)</a></li><li><a href=#3771--146346-end-to-end-supervised-prediction-of-arbitrary-size-graphs-with-partially-masked-fused-gromov-wasserstein-matching-paul-krzakala-et-al-2024>(37/71 | 146/346) End-to-end Supervised Prediction of Arbitrary-size Graphs with Partially-Masked Fused Gromov-Wasserstein Matching (Paul Krzakala et al., 2024)</a></li><li><a href=#3871--147346-a-mechanistic-analysis-of-a-transformer-trained-on-a-symbolic-multi-step-reasoning-task-jannik-brinkmann-et-al-2024>(38/71 | 147/346) A Mechanistic Analysis of a Transformer Trained on a Symbolic Multi-Step Reasoning Task (Jannik Brinkmann et al., 2024)</a></li><li><a href=#3971--148346-self-guided-robust-graph-structure-refinement-yeonjun-in-et-al-2024>(39/71 | 148/346) Self-Guided Robust Graph Structure Refinement (Yeonjun In et al., 2024)</a></li><li><a href=#4071--149346-easy-as-abcs-unifying-boltzmann-q-learning-and-counterfactual-regret-minimization-luca-damico-wong-et-al-2024>(40/71 | 149/346) Easy as ABCs: Unifying Boltzmann Q-Learning and Counterfactual Regret Minimization (Luca D&rsquo;Amico-Wong et al., 2024)</a></li><li><a href=#4171--150346-offline-multi-task-transfer-rl-with-representational-penalization-avinandan-bose-et-al-2024>(41/71 | 150/346) Offline Multi-task Transfer RL with Representational Penalization (Avinandan Bose et al., 2024)</a></li><li><a href=#4271--151346-dynamic-environment-responsive-online-meta-learning-with-fairness-awareness-chen-zhao-et-al-2024>(42/71 | 151/346) Dynamic Environment Responsive Online Meta-Learning with Fairness Awareness (Chen Zhao et al., 2024)</a></li><li><a href=#4371--152346-refining-minimax-regret-for-unsupervised-environment-design-michael-beukman-et-al-2024>(43/71 | 152/346) Refining Minimax Regret for Unsupervised Environment Design (Michael Beukman et al., 2024)</a></li><li><a href=#4471--153346-dictionary-learning-improves-patch-free-circuit-discovery-in-mechanistic-interpretability-a-case-study-on-othello-gpt-zhengfu-he-et-al-2024>(44/71 | 153/346) Dictionary Learning Improves Patch-Free Circuit Discovery in Mechanistic Interpretability: A Case Study on Othello-GPT (Zhengfu He et al., 2024)</a></li><li><a href=#4571--154346-mlfef-machine-learning-fusion-model-with-empirical-formula-to-explore-the-momentum-in-competitive-sports-ruixin-peng-et-al-2024>(45/71 | 154/346) MLFEF: Machine Learning Fusion Model with Empirical Formula to Explore the Momentum in Competitive Sports (Ruixin Peng et al., 2024)</a></li><li><a href=#4671--155346-linear-bandits-with-polylogarithmic-minimax-regret-josep-lumbreras-et-al-2024>(46/71 | 155/346) Linear bandits with polylogarithmic minimax regret (Josep Lumbreras et al., 2024)</a></li><li><a href=#4771--156346-bayesian-active-learning-for-censored-regression-frederik-boe-hüttel-et-al-2024>(47/71 | 156/346) Bayesian Active Learning for Censored Regression (Frederik Boe Hüttel et al., 2024)</a></li><li><a href=#4871--157346-stochastic-approximation-with-delayed-updates-finite-time-rates-under-markovian-sampling-arman-adibi-et-al-2024>(48/71 | 157/346) Stochastic Approximation with Delayed Updates: Finite-Time Rates under Markovian Sampling (Arman Adibi et al., 2024)</a></li><li><a href=#4971--158346-generative-kaleidoscopic-networks-harsh-shrivastava-2024>(49/71 | 158/346) Generative Kaleidoscopic Networks (Harsh Shrivastava, 2024)</a></li><li><a href=#5071--159346-diagonalisation-sgd-fast--convergent-sgd-for-non-differentiable-models-via-reparameterisation-and-smoothing-dominik-wagner-et-al-2024>(50/71 | 159/346) Diagonalisation SGD: Fast & Convergent SGD for Non-Differentiable Models via Reparameterisation and Smoothing (Dominik Wagner et al., 2024)</a></li><li><a href=#5171--160346-gaussian-process-neural-additive-models-wei-zhang-et-al-2024>(51/71 | 160/346) Gaussian Process Neural Additive Models (Wei Zhang et al., 2024)</a></li><li><a href=#5271--161346-energy-efficient-edge-learning-via-joint-data-deepening-and-prefetching-sujin-kook-et-al-2024>(52/71 | 161/346) Energy-Efficient Edge Learning via Joint Data Deepening-and-Prefetching (Sujin Kook et al., 2024)</a></li><li><a href=#5371--162346-fairproof--confidential-and-certifiable-fairness-for-neural-networks-chhavi-yadav-et-al-2024>(53/71 | 162/346) FairProof : Confidential and Certifiable Fairness for Neural Networks (Chhavi Yadav et al., 2024)</a></li><li><a href=#5471--163346-dynamic-pricing-and-learning-with-long-term-reference-effects-shipra-agrawal-et-al-2024>(54/71 | 163/346) Dynamic Pricing and Learning with Long-term Reference Effects (Shipra Agrawal et al., 2024)</a></li><li><a href=#5571--164346-sdes-for-minimax-optimization-enea-monzio-compagnoni-et-al-2024>(55/71 | 164/346) SDEs for Minimax Optimization (Enea Monzio Compagnoni et al., 2024)</a></li><li><a href=#5671--165346-lora-efficient-low-rank-adaptation-of-large-models-soufiane-hayou-et-al-2024>(56/71 | 165/346) LoRA+: Efficient Low Rank Adaptation of Large Models (Soufiane Hayou et al., 2024)</a></li><li><a href=#5771--166346-bears-make-neuro-symbolic-models-aware-of-their-reasoning-shortcuts-emanuele-marconato-et-al-2024>(57/71 | 166/346) BEARS Make Neuro-Symbolic Models Aware of their Reasoning Shortcuts (Emanuele Marconato et al., 2024)</a></li><li><a href=#5871--167346-learning-discretized-bayesian-networks-with-gomea-damy-m-f-ha-et-al-2024>(58/71 | 167/346) Learning Discretized Bayesian Networks with GOMEA (Damy M. F. Ha et al., 2024)</a></li><li><a href=#5971--168346-federated-bayesian-network-ensembles-florian-van-daalen-et-al-2024>(59/71 | 168/346) Federated Bayesian Network Ensembles (Florian van Daalen et al., 2024)</a></li><li><a href=#6071--169346-interpretable-brain-inspired-representations-improve-rl-performance-on-visual-navigation-tasks-moritz-lange-et-al-2024>(60/71 | 169/346) Interpretable Brain-Inspired Representations Improve RL Performance on Visual Navigation Tasks (Moritz Lange et al., 2024)</a></li><li><a href=#6171--170346-privacy-preserving-low-rank-adaptation-for-latent-diffusion-models-zihao-luo-et-al-2024>(61/71 | 170/346) Privacy-Preserving Low-Rank Adaptation for Latent Diffusion Models (Zihao Luo et al., 2024)</a></li><li><a href=#6271--171346-mini-hes-a-parallelizable-second-order-latent-factor-analysis-model-jialiang-wang-et-al-2024>(62/71 | 171/346) Mini-Hes: A Parallelizable Second-order Latent Factor Analysis Model (Jialiang Wang et al., 2024)</a></li><li><a href=#6371--172346-predicting-trucking-accidents-with-truck-drivers-safety-climate-perception-across-companies-a-transfer-learning-approach-kailai-sun-et-al-2024>(63/71 | 172/346) Predicting trucking accidents with truck drivers &lsquo;safety climate perception across companies: A transfer learning approach (Kailai Sun et al., 2024)</a></li><li><a href=#6471--173346-vehicle-group-based-crash-risk-formation-and-propagation-analysis-for-expressways-tianheng-zhu-et-al-2024>(64/71 | 173/346) Vehicle-group-based Crash Risk Formation and Propagation Analysis for Expressways (Tianheng Zhu et al., 2024)</a></li><li><a href=#6571--174346-finite-time-error-analysis-of-online-model-based-q-learning-with-a-relaxed-sampling-model-han-dong-lim-et-al-2024>(65/71 | 174/346) Finite-Time Error Analysis of Online Model-Based Q-Learning with a Relaxed Sampling Model (Han-Dong Lim et al., 2024)</a></li><li><a href=#6671--175346-communication-efficient-distributed-learning-with-local-immediate-error-compensation-yifei-cheng-et-al-2024>(66/71 | 175/346) Communication-Efficient Distributed Learning with Local Immediate Error Compensation (Yifei Cheng et al., 2024)</a></li><li><a href=#6771--176346-towards-theoretical-understandings-of-self-consuming-generative-models-shi-fu-et-al-2024>(67/71 | 176/346) Towards Theoretical Understandings of Self-Consuming Generative Models (Shi Fu et al., 2024)</a></li><li><a href=#6871--177346-class-incremental-learning-for-time-series-benchmark-and-evaluation-zhongzheng-qiao-et-al-2024>(68/71 | 177/346) Class-incremental Learning for Time Series: Benchmark and Evaluation (Zhongzheng Qiao et al., 2024)</a></li><li><a href=#6971--178346-network-inversion-of-binarised-neural-nets-pirzada-suhail-et-al-2024>(69/71 | 178/346) Network Inversion of Binarised Neural Nets (Pirzada Suhail et al., 2024)</a></li><li><a href=#7071--179346-graph-based-virtual-sensing-from-sparse-and-partial-multivariate-observations-giovanni-de-felice-et-al-2024>(70/71 | 179/346) Graph-based Virtual Sensing from Sparse and Partial Multivariate Observations (Giovanni De Felice et al., 2024)</a></li><li><a href=#7171--180346-dynamic-multi-network-mining-of-tensor-time-series-kohei-obata-et-al-2024>(71/71 | 180/346) Dynamic Multi-Network Mining of Tensor Time Series (Kohei Obata et al., 2024)</a></li></ul></li><li><a href=#csai-9>cs.AI (9)</a><ul><li><a href=#19--181346-llm-as-prompter-low-resource-inductive-reasoning-on-arbitrary-knowledge-graphs-kai-wang-et-al-2024>(1/9 | 181/346) LLM as Prompter: Low-resource Inductive Reasoning on Arbitrary Knowledge Graphs (Kai Wang et al., 2024)</a></li><li><a href=#29--182346-hip-network-historical-information-passing-network-for-extrapolation-reasoning-on-temporal-knowledge-graph-yongquan-he-et-al-2024>(2/9 | 182/346) HIP Network: Historical Information Passing Network for Extrapolation Reasoning on Temporal Knowledge Graph (Yongquan He et al., 2024)</a></li><li><a href=#39--183346-a-survey-on-extractive-knowledge-graph-summarization-applications-approaches-evaluation-and-future-directions-xiaxia-wang-et-al-2024>(3/9 | 183/346) A Survey on Extractive Knowledge Graph Summarization: Applications, Approaches, Evaluation, and Future Directions (Xiaxia Wang et al., 2024)</a></li><li><a href=#49--184346-sstkg-simple-spatio-temporal-knowledge-graph-for-intepretable-and-versatile-dynamic-information-embedding-ruiyi-yang-et-al-2024>(4/9 | 184/346) SSTKG: Simple Spatio-Temporal Knowledge Graph for Intepretable and Versatile Dynamic Information Embedding (Ruiyi Yang et al., 2024)</a></li><li><a href=#59--185346-shall-we-talk-exploring-spontaneous-collaborations-of-competing-llm-agents-zengqing-wu-et-al-2024>(5/9 | 185/346) Shall We Talk: Exploring Spontaneous Collaborations of Competing LLM Agents (Zengqing Wu et al., 2024)</a></li><li><a href=#69--186346-grounding-from-an-ai-and-cognitive-science-lens-goonmeet-bajaj-et-al-2024>(6/9 | 186/346) Grounding from an AI and Cognitive Science Lens (Goonmeet Bajaj et al., 2024)</a></li><li><a href=#79--187346-worldcoder-a-model-based-llm-agent-building-world-models-by-writing-code-and-interacting-with-the-environment-hao-tang-et-al-2024>(7/9 | 187/346) WorldCoder, a Model-Based LLM Agent: Building World Models by Writing Code and Interacting with the Environment (Hao Tang et al., 2024)</a></li><li><a href=#89--188346-discerning-and-resolving-knowledge-conflicts-through-adaptive-decoding-with-contextual-information-entropy-constraint-xiaowei-yuan-et-al-2024>(8/9 | 188/346) Discerning and Resolving Knowledge Conflicts through Adaptive Decoding with Contextual Information-Entropy Constraint (Xiaowei Yuan et al., 2024)</a></li><li><a href=#99--189346-multifix-an-xai-friendly-feature-inducing-approach-to-building-models-from-multimodal-data-mafalda-malafaia-et-al-2024>(9/9 | 189/346) MultiFIX: An XAI-friendly feature inducing approach to building models from multimodal data (Mafalda Malafaia et al., 2024)</a></li></ul></li><li><a href=#cscr-9>cs.CR (9)</a><ul><li><a href=#19--190346-deepcode-ai-fix-fixing-security-vulnerabilities-with-large-language-models-berkay-berabi-et-al-2024>(1/9 | 190/346) DeepCode AI Fix: Fixing Security Vulnerabilities with Large Language Models (Berkay Berabi et al., 2024)</a></li><li><a href=#29--191346-stealing-the-invisible-unveiling-pre-trained-cnn-models-through-adversarial-examples-and-timing-side-channels-shubhi-shukla-et-al-2024>(2/9 | 191/346) Stealing the Invisible: Unveiling Pre-Trained CNN Models through Adversarial Examples and Timing Side-Channels (Shubhi Shukla et al., 2024)</a></li><li><a href=#39--192346-covrl-fuzzing-javascript-engines-with-coverage-guided-reinforcement-learning-for-llm-based-mutation-jueon-eom-et-al-2024>(3/9 | 192/346) CovRL: Fuzzing JavaScript Engines with Coverage-Guided Reinforcement Learning for LLM-based Mutation (Jueon Eom et al., 2024)</a></li><li><a href=#49--193346-evaluation-of-chatgpts-smart-contract-auditing-capabilities-based-on-chain-of-thought-yuying-du-et-al-2024>(4/9 | 193/346) Evaluation of ChatGPT&rsquo;s Smart Contract Auditing Capabilities Based on Chain of Thought (Yuying Du et al., 2024)</a></li><li><a href=#59--194346-an-empirical-evaluation-of-llms-for-solving-offensive-security-challenges-minghao-shao-et-al-2024>(5/9 | 194/346) An Empirical Evaluation of LLMs for Solving Offensive Security Challenges (Minghao Shao et al., 2024)</a></li><li><a href=#69--195346-defending-against-weight-poisoning-backdoor-attacks-for-parameter-efficient-fine-tuning-shuai-zhao-et-al-2024>(6/9 | 195/346) Defending Against Weight-Poisoning Backdoor Attacks for Parameter-Efficient Fine-Tuning (Shuai Zhao et al., 2024)</a></li><li><a href=#79--196346-deployment-of-advanced-and-intelligent-logistics-vehicles-with-enhanced-tracking-and-security-features-iqtiar-md-siddique-et-al-2024>(7/9 | 196/346) Deployment of Advanced and Intelligent Logistics Vehicles with Enhanced Tracking and Security Features (Iqtiar Md Siddique et al., 2024)</a></li><li><a href=#89--197346-an-interview-study-on-third-party-cyber-threat-hunting-processes-in-the-us-department-of-homeland-security-william-p-maxam-iii-et-al-2024>(8/9 | 197/346) An Interview Study on Third-Party Cyber Threat Hunting Processes in the U.S. Department of Homeland Security (William P. Maxam III et al., 2024)</a></li><li><a href=#99--198346-attack-tree-generation-via-process-mining-alyzia-maria-konsta-et-al-2024>(9/9 | 198/346) Attack Tree Generation via Process Mining (Alyzia-Maria Konsta et al., 2024)</a></li></ul></li><li><a href=#csse-3>cs.SE (3)</a><ul><li><a href=#13--199346-enhancing-large-language-models-for-text-to-testcase-generation-saranya-alagarsamy-et-al-2024>(1/3 | 199/346) Enhancing Large Language Models for Text-to-Testcase Generation (Saranya Alagarsamy et al., 2024)</a></li><li><a href=#23--200346-codeart-better-code-models-by-attention-regularization-when-symbols-are-lacking-zian-su-et-al-2024>(2/3 | 200/346) CodeArt: Better Code Models by Attention Regularization When Symbols Are Lacking (Zian Su et al., 2024)</a></li><li><a href=#33--201346-parallel-program-analysis-on-path-ranges-jan-haltermanna-et-al-2024>(3/3 | 201/346) Parallel Program Analysis on Path Ranges (Jan Haltermanna et al., 2024)</a></li></ul></li><li><a href=#csir-7>cs.IR (7)</a><ul><li><a href=#17--202346-feb4rag-evaluating-federated-search-in-the-context-of-retrieval-augmented-generation-shuai-wang-et-al-2024>(1/7 | 202/346) FeB4RAG: Evaluating Federated Search in the Context of Retrieval Augmented Generation (Shuai Wang et al., 2024)</a></li><li><a href=#27--203346-ask-optimal-questions-aligning-large-language-models-with-retrievers-preference-in-conversational-search-chanwoong-yoon-et-al-2024>(2/7 | 203/346) Ask Optimal Questions: Aligning Large Language Models with Retriever&rsquo;s Preference in Conversational Search (Chanwoong Yoon et al., 2024)</a></li><li><a href=#37--204346-explain-then-rank-scale-calibration-of-neural-rankers-using-natural-language-explanations-from-large-language-models-puxuan-yu-et-al-2024>(3/7 | 204/346) Explain then Rank: Scale Calibration of Neural Rankers Using Natural Language Explanations from Large Language Models (Puxuan Yu et al., 2024)</a></li><li><a href=#47--205346-leveraging-opposite-gender-interaction-ratio-as-a-path-towards-fairness-in-online-dating-recommendations-based-on-user-sexual-orientation-yuying-zhao-et-al-2024>(4/7 | 205/346) Leveraging Opposite Gender Interaction Ratio as a Path towards Fairness in Online Dating Recommendations Based on User Sexual Orientation (Yuying Zhao et al., 2024)</a></li><li><a href=#57--206346-large-language-models-for-stemming-promises-pitfalls-and-failures-shuai-wang-et-al-2024>(5/7 | 206/346) Large Language Models for Stemming: Promises, Pitfalls and Failures (Shuai Wang et al., 2024)</a></li><li><a href=#67--207346-heterogeneity-aware-cross-school-electives-recommendation-a-hybrid-federated-approach-chengyi-ju-et-al-2024>(6/7 | 207/346) Heterogeneity-aware Cross-school Electives Recommendation: a Hybrid Federated Approach (Chengyi Ju et al., 2024)</a></li><li><a href=#77--208346-trisampler-a-better-negative-sampling-principle-for-dense-retrieval-zhen-yang-et-al-2024>(7/7 | 208/346) TriSampler: A Better Negative Sampling Principle for Dense Retrieval (Zhen Yang et al., 2024)</a></li></ul></li><li><a href=#cscv-45>cs.CV (45)</a><ul><li><a href=#145--209346-physu-net-long-temporal-context-transformer-for-rppg-with-self-supervised-pre-training-marko-savic-et-al-2024>(1/45 | 209/346) PhySU-Net: Long Temporal Context Transformer for rPPG with Self-Supervised Pre-training (Marko Savic et al., 2024)</a></li><li><a href=#245--210346-a-lightweight-parallel-framework-for-blind-image-quality-assessment-qunyue-huang-et-al-2024>(2/45 | 210/346) A Lightweight Parallel Framework for Blind Image Quality Assessment (Qunyue Huang et al., 2024)</a></li><li><a href=#345--211346-adversarial-feature-alignment-balancing-robustness-and-accuracy-in-deep-learning-via-adversarial-training-leo-hyun-park-et-al-2024>(3/45 | 211/346) Adversarial Feature Alignment: Balancing Robustness and Accuracy in Deep Learning via Adversarial Training (Leo Hyun Park et al., 2024)</a></li><li><a href=#445--212346-direct-consistency-optimization-for-compositional-text-to-image-personalization-kyungmin-lee-et-al-2024>(4/45 | 212/346) Direct Consistency Optimization for Compositional Text-to-Image Personalization (Kyungmin Lee et al., 2024)</a></li><li><a href=#545--213346-note-notable-generation-of-patient-text-summaries-through-efficient-approach-based-on-direct-preference-optimization-imjin-ahn-et-al-2024>(5/45 | 213/346) NOTE: Notable generation Of patient Text summaries through Efficient approach based on direct preference optimization (Imjin Ahn et al., 2024)</a></li><li><a href=#645--214346-rock-classification-based-on-residual-networks-sining-zhoubian-et-al-2024>(6/45 | 214/346) Rock Classification Based on Residual Networks (Sining Zhoubian et al., 2024)</a></li><li><a href=#745--215346-chartx--chartvlm-a-versatile-benchmark-and-foundation-model-for-complicated-chart-reasoning-renqiu-xia-et-al-2024>(7/45 | 215/346) ChartX & ChartVLM: A Versatile Benchmark and Foundation Model for Complicated Chart Reasoning (Renqiu Xia et al., 2024)</a></li><li><a href=#845--216346-mm-survnet-deep-learning-based-survival-risk-stratification-in-breast-cancer-through-multimodal-data-fusion-raktim-kumar-mondol-et-al-2024>(8/45 | 216/346) MM-SurvNet: Deep Learning-Based Survival Risk Stratification in Breast Cancer Through Multimodal Data Fusion (Raktim Kumar Mondol et al., 2024)</a></li><li><a href=#945--217346-improving-deep-generative-models-on-many-to-one-image-to-image-translation-sagar-saxena-et-al-2024>(9/45 | 217/346) Improving Deep Generative Models on Many-To-One Image-to-Image Translation (Sagar Saxena et al., 2024)</a></li><li><a href=#1045--218346-open3dsg-open-vocabulary-3d-scene-graphs-from-point-clouds-with-queryable-objects-and-open-set-relationships-sebastian-koch-et-al-2024>(10/45 | 218/346) Open3DSG: Open-Vocabulary 3D Scene Graphs from Point Clouds with Queryable Objects and Open-Set Relationships (Sebastian Koch et al., 2024)</a></li><li><a href=#1145--219346-scaffolding-coordinates-to-promote-vision-language-coordination-in-large-multi-modal-models-xuanyu-lei-et-al-2024>(11/45 | 219/346) Scaffolding Coordinates to Promote Vision-Language Coordination in Large Multi-Modal Models (Xuanyu Lei et al., 2024)</a></li><li><a href=#1245--220346-fit-flexible-vision-transformer-for-diffusion-model-zeyu-lu-et-al-2024>(12/45 | 220/346) FiT: Flexible Vision Transformer for Diffusion Model (Zeyu Lu et al., 2024)</a></li><li><a href=#1345--221346-weakly-supervised-object-detection-in-chest-x-rays-with-differentiable-roi-proposal-networks-and-soft-roi-pooling-philip-müller-et-al-2024>(13/45 | 221/346) Weakly Supervised Object Detection in Chest X-Rays with Differentiable ROI Proposal Networks and Soft ROI Pooling (Philip Müller et al., 2024)</a></li><li><a href=#1445--222346-the-revolution-of-multimodal-large-language-models-a-survey-davide-caffagni-et-al-2024>(14/45 | 222/346) The (R)Evolution of Multimodal Large Language Models: A Survey (Davide Caffagni et al., 2024)</a></li><li><a href=#1545--223346-lvchat-facilitating-long-video-comprehension-yu-wang-et-al-2024>(15/45 | 223/346) LVCHAT: Facilitating Long Video Comprehension (Yu Wang et al., 2024)</a></li><li><a href=#1645--224346-integrating-knn-with-foundation-models-for-adaptable-and-privacy-aware-image-classification-sebastian-doerrich-et-al-2024>(16/45 | 224/346) Integrating kNN with Foundation Models for Adaptable and Privacy-Aware Image Classification (Sebastian Doerrich et al., 2024)</a></li><li><a href=#1745--225346-unlearncanvas-a-stylized-image-dataset-to-benchmark-machine-unlearning-for-diffusion-models-yihua-zhang-et-al-2024>(17/45 | 225/346) UnlearnCanvas: A Stylized Image Dataset to Benchmark Machine Unlearning for Diffusion Models (Yihua Zhang et al., 2024)</a></li><li><a href=#1845--226346-multilinear-mixture-of-experts-scalable-expert-specialization-through-factorization-james-oldfield-et-al-2024>(18/45 | 226/346) Multilinear Mixture of Experts: Scalable Expert Specialization through Factorization (James Oldfield et al., 2024)</a></li><li><a href=#1945--227346-system-identification-of-neural-systems-going-beyond-images-to-modelling-dynamics-mai-gamal-et-al-2024>(19/45 | 227/346) System Identification of Neural Systems: Going Beyond Images to Modelling Dynamics (Mai Gamal et al., 2024)</a></li><li><a href=#2045--228346-3d-vascular-segmentation-supervised-by-2d-annotation-of-maximum-intensity-projection-zhanqiang-guo-et-al-2024>(20/45 | 228/346) 3D Vascular Segmentation Supervised by 2D Annotation of Maximum Intensity Projection (Zhanqiang Guo et al., 2024)</a></li><li><a href=#2145--229346-iscute-instance-segmentation-of-cables-using-text-embedding-shir-kozlovsky-et-al-2024>(21/45 | 229/346) ISCUTE: Instance Segmentation of Cables Using Text Embedding (Shir Kozlovsky et al., 2024)</a></li><li><a href=#2245--230346-inmd-x-large-language-models-for-internal-medicine-doctors-hansle-gwon-et-al-2024>(22/45 | 230/346) InMD-X: Large Language Models for Internal Medicine Doctors (Hansle Gwon et al., 2024)</a></li><li><a href=#2345--231346-sdge-stereo-guided-depth-estimation-for-360-camera-sets-jialei-xu-et-al-2024>(23/45 | 231/346) SDGE: Stereo Guided Depth Estimation for 360° Camera Sets (Jialei Xu et al., 2024)</a></li><li><a href=#2445--232346-separating-common-from-salient-patterns-with-contrastive-representation-learning-robin-louiset-et-al-2024>(24/45 | 232/346) Separating common from salient patterns with Contrastive Representation Learning (Robin Louiset et al., 2024)</a></li><li><a href=#2545--233346-dilightnet-fine-grained-lighting-control-for-diffusion-based-image-generation-chong-zeng-et-al-2024>(25/45 | 233/346) DiLightNet: Fine-grained Lighting Control for Diffusion-based Image Generation (Chong Zeng et al., 2024)</a></li><li><a href=#2645--234346-feudal-networks-for-visual-navigation-faith-johnson-et-al-2024>(26/45 | 234/346) Feudal Networks for Visual Navigation (Faith Johnson et al., 2024)</a></li><li><a href=#2745--235346-avoiding-feature-suppression-in-contrastive-learning-learning-what-has-not-been-learned-before-jihai-zhang-et-al-2024>(27/45 | 235/346) Avoiding Feature Suppression in Contrastive Learning: Learning What Has Not Been Learned Before (Jihai Zhang et al., 2024)</a></li><li><a href=#2845--236346-langxai-integrating-large-vision-models-for-generating-textual-explanations-to-enhance-explainability-in-visual-perception-tasks-truong-thanh-hung-nguyen-et-al-2024>(28/45 | 236/346) LangXAI: Integrating Large Vision Models for Generating Textual Explanations to Enhance Explainability in Visual Perception Tasks (Truong Thanh Hung Nguyen et al., 2024)</a></li><li><a href=#2945--237346-drivevlm-the-convergence-of-autonomous-driving-and-large-vision-language-models-xiaoyu-tian-et-al-2024>(29/45 | 237/346) DriveVLM: The Convergence of Autonomous Driving and Large Vision-Language Models (Xiaoyu Tian et al., 2024)</a></li><li><a href=#3045--238346-human-video-translation-via-query-warping-haiming-zhu-et-al-2024>(30/45 | 238/346) Human Video Translation via Query Warping (Haiming Zhu et al., 2024)</a></li><li><a href=#3145--239346-surround-view-fisheye-optics-in-computer-vision-and-simulation-survey-and-challenges-daniel-jakab-et-al-2024>(31/45 | 239/346) Surround-View Fisheye Optics in Computer Vision and Simulation: Survey and Challenges (Daniel Jakab et al., 2024)</a></li><li><a href=#3245--240346-language-guided-image-reflection-separation-haofeng-zhong-et-al-2024>(32/45 | 240/346) Language-guided Image Reflection Separation (Haofeng Zhong et al., 2024)</a></li><li><a href=#3345--241346-comfusion-personalized-subject-generation-in-multiple-specific-scenes-from-single-image-yan-hong-et-al-2024>(33/45 | 241/346) ComFusion: Personalized Subject Generation in Multiple Specific Scenes From Single Image (Yan Hong et al., 2024)</a></li><li><a href=#3445--242346-wildfake-a-large-scale-challenging-dataset-for-ai-generated-images-detection-yan-hong-et-al-2024>(34/45 | 242/346) WildFake: A Large-scale Challenging Dataset for AI-Generated Images Detection (Yan Hong et al., 2024)</a></li><li><a href=#3545--243346-aicattack-adversarial-image-captioning-attack-with-attention-based-optimization-jiyao-li-et-al-2024>(35/45 | 243/346) AICAttack: Adversarial Image Captioning Attack with Attention-Based Optimization (Jiyao Li et al., 2024)</a></li><li><a href=#3645--244346-interpretable-embedding-for-ad-hoc-video-search-jiaxin-wu-et-al-2024>(36/45 | 244/346) Interpretable Embedding for Ad-hoc Video Search (Jiaxin Wu et al., 2024)</a></li><li><a href=#3745--245346-landmark-based-localization-using-stereo-vision-and-deep-learning-in-gps-denied-battlefield-environment-ganesh-sapkota-et-al-2024>(37/45 | 245/346) Landmark-based Localization using Stereo Vision and Deep Learning in GPS-Denied Battlefield Environment (Ganesh Sapkota et al., 2024)</a></li><li><a href=#3845--246346-uncertaintytrack-exploiting-detection-and-localization-uncertainty-in-multi-object-tracking-chang-won-lee-et-al-2024>(38/45 | 246/346) UncertaintyTrack: Exploiting Detection and Localization Uncertainty in Multi-Object Tracking (Chang Won Lee et al., 2024)</a></li><li><a href=#3945--247346-mixed-gaussian-flow-for-diverse-trajectory-prediction-jiahe-chen-et-al-2024>(39/45 | 247/346) Mixed Gaussian Flow for Diverse Trajectory Prediction (Jiahe Chen et al., 2024)</a></li><li><a href=#4045--248346-perceiving-longer-sequences-with-bi-directional-cross-attention-transformers-markus-hiller-et-al-2024>(40/45 | 248/346) Perceiving Longer Sequences With Bi-Directional Cross-Attention Transformers (Markus Hiller et al., 2024)</a></li><li><a href=#4145--249346-one2avatar-generative-implicit-head-avatar-for-few-shot-user-adaptation-zhixuan-yu-et-al-2024>(41/45 | 249/346) One2Avatar: Generative Implicit Head Avatar For Few-shot User Adaptation (Zhixuan Yu et al., 2024)</a></li><li><a href=#4245--250346-an-evaluation-of-deep-learning-based-stereo-dense-matching-dataset-shift-from-aerial-images-and-a-large-scale-stereo-dataset-teng-wu-et-al-2024>(42/45 | 250/346) An evaluation of Deep Learning based stereo dense matching dataset shift from aerial images and a large scale stereo dataset (Teng Wu et al., 2024)</a></li><li><a href=#4345--251346-binary-opacity-grids-capturing-fine-geometric-detail-for-mesh-based-view-synthesis-christian-reiser-et-al-2024>(43/45 | 251/346) Binary Opacity Grids: Capturing Fine Geometric Detail for Mesh-Based View Synthesis (Christian Reiser et al., 2024)</a></li><li><a href=#4445--252346-pushing-auto-regressive-models-for-3d-shape-generation-at-capacity-and-scalability-xuelin-qian-et-al-2024>(44/45 | 252/346) Pushing Auto-regressive Models for 3D Shape Generation at Capacity and Scalability (Xuelin Qian et al., 2024)</a></li><li><a href=#4545--253346-unveiling-the-depths-a-multi-modal-fusion-framework-for-challenging-scenarios-jialei-xu-et-al-2024>(45/45 | 253/346) Unveiling the Depths: A Multi-Modal Fusion Framework for Challenging Scenarios (Jialei Xu et al., 2024)</a></li></ul></li><li><a href=#cssd-5>cs.SD (5)</a><ul><li><a href=#15--254346-multimodal-emotion-recognition-from-raw-audio-with-sinc-convolution-xiaohui-zhang-et-al-2024>(1/5 | 254/346) Multimodal Emotion Recognition from Raw Audio with Sinc-convolution (Xiaohui Zhang et al., 2024)</a></li><li><a href=#25--255346-on-the-semantic-latent-space-of-diffusion-based-text-to-speech-models-miri-varshavsky-hassid-et-al-2024>(2/5 | 255/346) On the Semantic Latent Space of Diffusion-Based Text-to-Speech Models (Miri Varshavsky Hassid et al., 2024)</a></li><li><a href=#35--256346-secp-a-speech-enhancement-based-curation-pipeline-for-scalable-acquisition-of-clean-speech-adam-sabra-et-al-2024>(3/5 | 256/346) SECP: A Speech Enhancement-Based Curation Pipeline For Scalable Acquisition Of Clean Speech (Adam Sabra et al., 2024)</a></li><li><a href=#45--257346-unraveling-complex-data-diversity-in-underwater-acoustic-target-recognition-through-convolution-based-mixture-of-experts-yuan-xie-et-al-2024>(4/5 | 257/346) Unraveling Complex Data Diversity in Underwater Acoustic Target Recognition through Convolution-based Mixture of Experts (Yuan Xie et al., 2024)</a></li><li><a href=#55--258346-low-power-snn-based-audio-source-localisation-using-a-hilbert-transform-spike-encoding-scheme-saeid-haghighatshoar-et-al-2024>(5/5 | 258/346) Low-power SNN-based audio source localisation using a Hilbert Transform spike encoding scheme (Saeid Haghighatshoar et al., 2024)</a></li></ul></li><li><a href=#cssi-5>cs.SI (5)</a><ul><li><a href=#15--259346-attacks-on-node-attributes-in-graph-neural-networks-ying-xu-et-al-2024>(1/5 | 259/346) Attacks on Node Attributes in Graph Neural Networks (Ying Xu et al., 2024)</a></li><li><a href=#25--260346-analysis-of-persian-news-agencies-on-instagram-a-words-co-occurrence-graph-based-approach-mohammad-heydari-et-al-2024>(2/5 | 260/346) Analysis of Persian News Agencies on Instagram, A Words Co-occurrence Graph-based Approach (Mohammad Heydari et al., 2024)</a></li><li><a href=#35--261346-a-machine-learning-ensemble-model-for-the-detection-of-cyberbullying-abulkarim-faraj-alqahtani-et-al-2024>(3/5 | 261/346) A Machine Learning Ensemble Model for the Detection of Cyberbullying (Abulkarim Faraj Alqahtani et al., 2024)</a></li><li><a href=#45--262346-bridging-or-breaking-impact-of-intergroup-interactions-on-religious-polarization-rochana-chaturvedi-et-al-2024>(4/5 | 262/346) Bridging or Breaking: Impact of Intergroup Interactions on Religious Polarization (Rochana Chaturvedi et al., 2024)</a></li><li><a href=#55--263346-deep-structural-knowledge-exploitation-and-synergy-for-estimating-node-importance-value-on-heterogeneous-information-networks-yankai-chen-et-al-2024>(5/5 | 263/346) Deep Structural Knowledge Exploitation and Synergy for Estimating Node Importance Value on Heterogeneous Information Networks (Yankai Chen et al., 2024)</a></li></ul></li><li><a href=#csro-11>cs.RO (11)</a><ul><li><a href=#111--264346-sinvig-a-self-evolving-interactive-visual-agent-for-human-robot-interaction-jie-xu-et-al-2024>(1/11 | 264/346) SInViG: A Self-Evolving Interactive Visual Agent for Human-Robot Interaction (Jie Xu et al., 2024)</a></li><li><a href=#211--265346-decentralized-multi-robot-navigation-for-autonomous-surface-vehicles-with-distributional-reinforcement-learning-xi-lin-et-al-2024>(2/11 | 265/346) Decentralized Multi-Robot Navigation for Autonomous Surface Vehicles with Distributional Reinforcement Learning (Xi Lin et al., 2024)</a></li><li><a href=#311--266346-dio-dataset-of-3d-mesh-models-of-indoor-objects-for-robotics-and-computer-vision-applications-nillan-nimal-et-al-2024>(3/11 | 266/346) DIO: Dataset of 3D Mesh Models of Indoor Objects for Robotics and Computer Vision Applications (Nillan Nimal et al., 2024)</a></li><li><a href=#411--267346-learning-input-constrained-control-barrier-functions-for-guaranteed-safety-of-car-like-robots-sven-brüggemann-et-al-2024>(4/11 | 267/346) Learning Input Constrained Control Barrier Functions for Guaranteed Safety of Car-Like Robots (Sven Brüggemann et al., 2024)</a></li><li><a href=#511--268346-talk-through-it-end-user-directed-manipulation-learning-carl-winge-et-al-2024>(5/11 | 268/346) Talk Through It: End User Directed Manipulation Learning (Carl Winge et al., 2024)</a></li><li><a href=#611--269346-a-novel-framework-for-adaptive-stress-testing-of-autonomous-vehicles-in-highways-linh-trinh-et-al-2024>(6/11 | 269/346) A novel framework for adaptive stress testing of autonomous vehicles in highways (Linh Trinh et al., 2024)</a></li><li><a href=#711--270346-decentralized-lifelong-path-planning-for-multiple-ackerman-car-like-robots-teng-guo-et-al-2024>(7/11 | 270/346) Decentralized Lifelong Path Planning for Multiple Ackerman Car-Like Robots (Teng Guo et al., 2024)</a></li><li><a href=#811--271346-from-reals-to-logic-and-back-inventing-symbolic-vocabularies-actions-and-models-for-planning-from-raw-data-naman-shah-et-al-2024>(8/11 | 271/346) From Reals to Logic and Back: Inventing Symbolic Vocabularies, Actions and Models for Planning from Raw Data (Naman Shah et al., 2024)</a></li><li><a href=#911--272346-colrio-lidar-ranging-inertial-centralized-state-estimation-for-robotic-swarms-shipeng-zhong-et-al-2024>(9/11 | 272/346) CoLRIO: LiDAR-Ranging-Inertial Centralized State Estimation for Robotic Swarms (Shipeng Zhong et al., 2024)</a></li><li><a href=#1011--273346-targeted-parallelization-of-conflict-based-search-for-multi-robot-path-planning-teng-guo-et-al-2024>(10/11 | 273/346) Targeted Parallelization of Conflict-Based Search for Multi-Robot Path Planning (Teng Guo et al., 2024)</a></li><li><a href=#1111--274346-well-connected-set-and-its-application-to-multi-robot-path-planning-teng-guo-et-al-2024>(11/11 | 274/346) Well-Connected Set and Its Application to Multi-Robot Path Planning (Teng Guo et al., 2024)</a></li></ul></li><li><a href=#cshc-4>cs.HC (4)</a><ul><li><a href=#14--275346-imbue-improving-interpersonal-effectiveness-through-simulation-and-just-in-time-feedback-with-human-language-model-interaction-inna-wanyin-lin-et-al-2024>(1/4 | 275/346) IMBUE: Improving Interpersonal Effectiveness through Simulation and Just-in-time Feedback with Human-Language Model Interaction (Inna Wanyin Lin et al., 2024)</a></li><li><a href=#24--276346-dynamic-and-super-personalized-media-ecosystem-driven-by-generative-ai-unpredictable-plays-never-repeating-the-same-sungjun-ahn-et-al-2024>(2/4 | 276/346) Dynamic and Super-Personalized Media Ecosystem Driven by Generative AI: Unpredictable Plays Never Repeating The Same (Sungjun Ahn et al., 2024)</a></li><li><a href=#34--277346-enhancing-empathetic-response-generation-by-augmenting-llms-with-small-scale-empathetic-models-zhou-yang-et-al-2024>(3/4 | 277/346) Enhancing Empathetic Response Generation by Augmenting LLMs with Small-scale Empathetic Models (Zhou Yang et al., 2024)</a></li><li><a href=#44--278346-beyond-voice-assistants-exploring-advantages-and-risks-of-an-in-car-social-robot-in-real-driving-scenarios-yuanchao-li-et-al-2024>(4/4 | 278/346) Beyond Voice Assistants: Exploring Advantages and Risks of an In-Car Social Robot in Real Driving Scenarios (Yuanchao Li et al., 2024)</a></li></ul></li><li><a href=#eessas-3>eess.AS (3)</a><ul><li><a href=#13--279346-language-codec-reducing-the-gaps-between-discrete-codec-representation-and-speech-language-models-shengpeng-ji-et-al-2024>(1/3 | 279/346) Language-Codec: Reducing the Gaps Between Discrete Codec Representation and Speech Language Models (Shengpeng Ji et al., 2024)</a></li><li><a href=#23--280346-parameter-efficient-finetuning-for-speech-emotion-recognition-and-domain-adaptation-nineli-lashkarashvili-et-al-2024>(2/3 | 280/346) Parameter Efficient Finetuning for Speech Emotion Recognition and Domain Adaptation (Nineli Lashkarashvili et al., 2024)</a></li><li><a href=#33--281346-bayesian-parameter-efficient-fine-tuning-for-overcoming-catastrophic-forgetting-haolin-chen-et-al-2024>(3/3 | 281/346) Bayesian Parameter-Efficient Fine-Tuning for Overcoming Catastrophic Forgetting (Haolin Chen et al., 2024)</a></li></ul></li><li><a href=#csdb-2>cs.DB (2)</a><ul><li><a href=#12--282346-training-table-question-answering-via-sql-query-decomposition-raphaël-mouravieff-et-al-2024>(1/2 | 282/346) Training Table Question Answering via SQL Query Decomposition (Raphaël Mouravieff et al., 2024)</a></li><li><a href=#22--283346-structure-guided-large-language-model-for-sql-generation-qinggang-zhang-et-al-2024>(2/2 | 283/346) Structure Guided Large Language Model for SQL Generation (Qinggang Zhang et al., 2024)</a></li></ul></li><li><a href=#statml-6>stat.ML (6)</a><ul><li><a href=#16--284346-kernel-kmeans-clustering-splits-for-end-to-end-unsupervised-decision-trees-louis-ohl-et-al-2024>(1/6 | 284/346) Kernel KMeans clustering splits for end-to-end unsupervised decision trees (Louis Ohl et al., 2024)</a></li><li><a href=#26--285346-when-do-off-policy-and-on-policy-policy-gradient-methods-align-davide-mambelli-et-al-2024>(2/6 | 285/346) When Do Off-Policy and On-Policy Policy Gradient Methods Align? (Davide Mambelli et al., 2024)</a></li><li><a href=#36--286346-statistical-test-for-generated-hypotheses-by-diffusion-models-teruyuki-katsuoka-et-al-2024>(3/6 | 286/346) Statistical Test for Generated Hypotheses by Diffusion Models (Teruyuki Katsuoka et al., 2024)</a></li><li><a href=#46--287346-towards-ai-based-precision-oncology-a-machine-learning-framework-for-personalized-counterfactual-treatment-suggestions-based-on-multi-omics-data-manuel-schürch-et-al-2024>(4/6 | 287/346) Towards AI-Based Precision Oncology: A Machine Learning Framework for Personalized Counterfactual Treatment Suggestions based on Multi-Omics Data (Manuel Schürch et al., 2024)</a></li><li><a href=#56--288346-regularization-by-denoising-bayesian-model-and-langevin-within-split-gibbs-sampling-elhadji-c-faye-et-al-2024>(5/6 | 288/346) Regularization by denoising: Bayesian model and Langevin-within-split Gibbs sampling (Elhadji C. Faye et al., 2024)</a></li><li><a href=#66--289346-asymptotic-gaussian-fluctuations-of-eigenvectors-in-spectral-clustering-hugo-lebeau-et-al-2024>(6/6 | 289/346) Asymptotic Gaussian Fluctuations of Eigenvectors in Spectral Clustering (Hugo Lebeau et al., 2024)</a></li></ul></li><li><a href=#csgt-3>cs.GT (3)</a><ul><li><a href=#13--290346-automated-security-response-through-online-learning-with-adaptive-conjectures-kim-hammar-et-al-2024>(1/3 | 290/346) Automated Security Response through Online Learning with Adaptive Conjectures (Kim Hammar et al., 2024)</a></li><li><a href=#23--291346-integrating-dynamic-weighted-approach-with-fictitious-play-and-pure-counterfactual-regret-minimization-for-equilibrium-finding-qi-ju-et-al-2024>(2/3 | 291/346) Integrating Dynamic Weighted Approach with Fictitious Play and Pure Counterfactual Regret Minimization for Equilibrium Finding (Qi Ju et al., 2024)</a></li><li><a href=#33--292346-simple-mechanisms-for-utility-maximization-approximating-welfare-in-the-iid-unit-demand-setting-kira-goldner-et-al-2024>(3/3 | 292/346) Simple Mechanisms for Utility Maximization: Approximating Welfare in the I.I.D. Unit-Demand Setting (Kira Goldner et al., 2024)</a></li></ul></li><li><a href=#astro-phep-1>astro-ph.EP (1)</a><ul><li><a href=#11--293346-dbnets-a-publicly-available-deep-learning-tool-to-measure-the-masses-of-young-planets-in-dusty-protoplanetary-discs-alessandro-ruzza-et-al-2024>(1/1 | 293/346) DBNets: A publicly available deep learning tool to measure the masses of young planets in dusty protoplanetary discs (Alessandro Ruzza et al., 2024)</a></li></ul></li><li><a href=#eesssy-5>eess.SY (5)</a><ul><li><a href=#15--294346-an-adversarial-approach-to-evaluating-the-robustness-of-event-identification-models-obai-bahwal-et-al-2024>(1/5 | 294/346) An Adversarial Approach to Evaluating the Robustness of Event Identification Models (Obai Bahwal et al., 2024)</a></li><li><a href=#25--295346-an-index-policy-based-on-sarsa-and-q-learning-for-heterogeneous-smart-target-tracking-yuhang-hao-et-al-2024>(2/5 | 295/346) An Index Policy Based on Sarsa and Q-learning for Heterogeneous Smart Target Tracking (Yuhang Hao et al., 2024)</a></li><li><a href=#35--296346-impact-of-data-usage-for-forecasting-on-performance-of-model-predictive-control-in-buildings-with-smart-energy-storage-max-langtry-et-al-2024>(3/5 | 296/346) Impact of data usage for forecasting on performance of model predictive control in buildings with smart energy storage (Max Langtry et al., 2024)</a></li><li><a href=#45--297346-flexible-robust-optimal-bidding-of-renewable-virtual-power-plants-in-sequential-markets-hadi-nemati-et-al-2024>(4/5 | 297/346) Flexible Robust Optimal Bidding of Renewable Virtual Power Plants in Sequential Markets (Hadi Nemati et al., 2024)</a></li><li><a href=#55--298346-terahertz-user-centric-clustering-in-the-presence-of-beam-misalignment-khaled-humadi-et-al-2024>(5/5 | 298/346) Terahertz User-Centric Clustering in the Presence of Beam Misalignment (Khaled Humadi et al., 2024)</a></li></ul></li><li><a href=#csdc-5>cs.DC (5)</a><ul><li><a href=#15--299346-secure-federated-learning-across-heterogeneous-cloud-and-high-performance-computing-resources----a-case-study-on-federated-fine-tuning-of-llama-2-zilinghan-li-et-al-2024>(1/5 | 299/346) Secure Federated Learning Across Heterogeneous Cloud and High-Performance Computing Resources &ndash; A Case Study on Federated Fine-tuning of LLaMA 2 (Zilinghan Li et al., 2024)</a></li><li><a href=#25--300346-proximal-byzantine-consensus-roy-shadmon-et-al-2024>(2/5 | 300/346) Proximal Byzantine Consensus (Roy Shadmon et al., 2024)</a></li><li><a href=#35--301346-even-cycle-detection-in-the-randomized-and-quantum-congest-model-pierre-fraigniaud-et-al-2024>(3/5 | 301/346) Even-Cycle Detection in the Randomized and Quantum CONGEST Model (Pierre Fraigniaud et al., 2024)</a></li><li><a href=#45--302346-local-certification-of-forbidden-subgraphs-nicolas-bousquet-et-al-2024>(4/5 | 302/346) Local certification of forbidden subgraphs (Nicolas Bousquet et al., 2024)</a></li><li><a href=#55--303346-evaluating-versal-ai-engines-for-option-price-discovery-in-market-risk-analysis-mark-klaisoongnoen-et-al-2024>(5/5 | 303/346) Evaluating Versal AI Engines for option price discovery in market risk analysis (Mark Klaisoongnoen et al., 2024)</a></li></ul></li><li><a href=#csne-3>cs.NE (3)</a><ul><li><a href=#13--304346-hebbian-learning-based-orthogonal-projection-for-continual-learning-of-spiking-neural-networks-mingqing-xiao-et-al-2024>(1/3 | 304/346) Hebbian Learning based Orthogonal Projection for Continual Learning of Spiking Neural Networks (Mingqing Xiao et al., 2024)</a></li><li><a href=#23--305346-an-enhanced-teaching-learning-based-optimization-tlbo-with-grey-wolf-optimizer-gwo-for-text-feature-selection-and-clustering-mahsa-azarshab-et-al-2024>(2/3 | 305/346) An enhanced Teaching-Learning-Based Optimization (TLBO) with Grey Wolf Optimizer (GWO) for text feature selection and clustering (Mahsa Azarshab et al., 2024)</a></li><li><a href=#33--306346-function-class-learning-with-genetic-programming-towards-explainable-meta-learning-for-tumor-growth-functionals-e-m-c-sijben-et-al-2024>(3/3 | 306/346) Function Class Learning with Genetic Programming: Towards Explainable Meta Learning for Tumor Growth Functionals (E. M. C. Sijben et al., 2024)</a></li></ul></li><li><a href=#csit-3>cs.IT (3)</a><ul><li><a href=#13--307346-cooperative-backscatter-communications-with-reconfigurable-intelligent-surfaces-an-apsk-approach-qiang-li-et-al-2024>(1/3 | 307/346) Cooperative Backscatter Communications with Reconfigurable Intelligent Surfaces: An APSK Approach (Qiang Li et al., 2024)</a></li><li><a href=#23--308346-rate-splitting-multiple-access-for-transmissive-reconfigurable-intelligent-surface-transceiver-empowered-isac-system-ziwei-liu-et-al-2024>(2/3 | 308/346) Rate-Splitting Multiple Access for Transmissive Reconfigurable Intelligent Surface Transceiver Empowered ISAC System (Ziwei Liu et al., 2024)</a></li><li><a href=#33--309346-max-min-fairness-for-uplink-rate-splitting-multiple-access-with-finite-blocklength-jiawei-xu-et-al-2024>(3/3 | 309/346) Max-Min Fairness for Uplink Rate-Splitting Multiple Access with Finite Blocklength (Jiawei Xu et al., 2024)</a></li></ul></li><li><a href=#eesssp-2>eess.SP (2)</a><ul><li><a href=#12--310346-truncated-polynomial-expansion-based-detection-in-massive-mimo-a-model-driven-deep-learning-approach-kazem-izadinasab-et-al-2024>(1/2 | 310/346) Truncated Polynomial Expansion-Based Detection in Massive MIMO: A Model-Driven Deep Learning Approach (Kazem Izadinasab et al., 2024)</a></li><li><a href=#22--311346-a-simple-detection-and-identification-scheme-for-reconfigurable-intelligent-surfaces-aymen-khaleel-et-al-2024>(2/2 | 311/346) A Simple Detection and Identification Scheme For Reconfigurable Intelligent Surfaces (Aymen Khaleel et al., 2024)</a></li></ul></li><li><a href=#astro-phga-1>astro-ph.GA (1)</a><ul><li><a href=#11--312346-emulating-the-interstellar-medium-chemistry-with-neural-operators-lorenzo-branca-et-al-2024>(1/1 | 312/346) Emulating the interstellar medium chemistry with neural operators (Lorenzo Branca et al., 2024)</a></li></ul></li><li><a href=#astro-phsr-1>astro-ph.SR (1)</a><ul><li><a href=#11--313346-short-period-variables-in-tess-full-frame-image-light-curves-identified-via-convolutional-neural-networks-greg-olmschenk-et-al-2024>(1/1 | 313/346) Short-Period Variables in TESS Full-Frame Image Light Curves Identified via Convolutional Neural Networks (Greg Olmschenk et al., 2024)</a></li></ul></li><li><a href=#mathna-6>math.NA (6)</a><ul><li><a href=#16--314346-a-high-order-fully-well-balanced-unconditionally-positivity-preserving-finite-volume-framework-for-flood-simulations-mirco-ciallella-et-al-2024>(1/6 | 314/346) A high-order, fully well-balanced, unconditionally positivity-preserving finite volume framework for flood simulations (Mirco Ciallella et al., 2024)</a></li><li><a href=#26--315346-nonlinear-discrete-time-observers-with-physics-informed-neural-networks-hector-vargas-alvarez-et-al-2024>(2/6 | 315/346) Nonlinear Discrete-Time Observers with Physics-Informed Neural Networks (Hector Vargas Alvarez et al., 2024)</a></li><li><a href=#36--316346-lax-wendroff-flux-reconstruction-on-adaptive-curvilinear-meshes-with-error-based-time-stepping-for-hyperbolic-conservation-laws-arpit-babbar-et-al-2024>(3/6 | 316/346) Lax-Wendroff Flux Reconstruction on adaptive curvilinear meshes with error based time stepping for hyperbolic conservation laws (Arpit Babbar et al., 2024)</a></li><li><a href=#46--317346-solving-fluid-flow-problems-in-space-time-with-multiscale-stabilization-formulation-and-examples-biswajit-khara-et-al-2024>(4/6 | 317/346) Solving fluid flow problems in space-time with multiscale stabilization: formulation and examples (Biswajit Khara et al., 2024)</a></li><li><a href=#56--318346-a-unified-field-monolithic-fictitious-domain-finite-element-method-for-fluid-structure-contact-interactions-and-applications-to-deterministic-lateral-displacement-problems-cheng-wang-et-al-2024>(5/6 | 318/346) A Unified-Field Monolithic Fictitious Domain-Finite Element Method for Fluid-Structure-Contact Interactions and Applications to Deterministic Lateral Displacement Problems (Cheng Wang et al., 2024)</a></li><li><a href=#66--319346-analysis-of-the-picard-newton-iteration-for-the-navier-stokes-equations-global-stability-and-quadratic-convergence-sara-pollock-et-al-2024>(6/6 | 319/346) Analysis of the Picard-Newton iteration for the Navier-Stokes equations: global stability and quadratic convergence (Sara Pollock et al., 2024)</a></li></ul></li><li><a href=#physicscomp-ph-2>physics.comp-ph (2)</a><ul><li><a href=#12--320346-second-order-meanfield-approximation-for-calculating-dynamics-in-au-nanoparticle-networks-evan-wonisch-et-al-2024>(1/2 | 320/346) Second Order Meanfield Approximation for calculating Dynamics in Au-Nanoparticle Networks (Evan Wonisch et al., 2024)</a></li><li><a href=#22--321346-recent-extensions-of-the-zkcm-library-for-parallel-and-accurate-mps-simulation-of-quantum-circuits-akira-saitoh-2024>(2/2 | 321/346) Recent Extensions of the ZKCM Library for Parallel and Accurate MPS Simulation of Quantum Circuits (Akira SaiToh, 2024)</a></li></ul></li><li><a href=#mathoc-1>math.OC (1)</a><ul><li><a href=#11--322346-the-new-era-of-dynamic-pricing-synergizing-supervised-learning-and-quadratic-programming-gustavo-bramao-et-al-2024>(1/1 | 322/346) The New Era of Dynamic Pricing: Synergizing Supervised Learning and Quadratic Programming (Gustavo Bramao et al., 2024)</a></li></ul></li><li><a href=#csar-2>cs.AR (2)</a><ul><li><a href=#12--323346-towards-joint-optimization-for-dnn-architecture-and-configuration-for-compute-in-memory-hardware-souvik-kundu-et-al-2024>(1/2 | 323/346) Towards Joint Optimization for DNN Architecture and Configuration for Compute-In-Memory Hardware (Souvik Kundu et al., 2024)</a></li><li><a href=#22--324346-factor-machine-mixed-signal-architecture-for-fine-grained-graph-based-computing-piotr-dudek-2024>(2/2 | 324/346) Factor Machine: Mixed-signal Architecture for Fine-Grained Graph-Based Computing (Piotr Dudek, 2024)</a></li></ul></li><li><a href=#cslo-1>cs.LO (1)</a><ul><li><a href=#11--325346-automating-boundary-filling-in-cubical-agda-maximilian-doré-et-al-2024>(1/1 | 325/346) Automating Boundary Filling in Cubical Agda (Maximilian Doré et al., 2024)</a></li></ul></li><li><a href=#physicschem-ph-1>physics.chem-ph (1)</a><ul><li><a href=#11--326346-molecule-generation-and-optimization-for-efficient-fragrance-creation-bruno-c-l-rodrigues-et-al-2024>(1/1 | 326/346) Molecule Generation and Optimization for Efficient Fragrance Creation (Bruno C. L. Rodrigues et al., 2024)</a></li></ul></li><li><a href=#csma-2>cs.MA (2)</a><ul><li><a href=#12--327346-aligning-individual-and-collective-objectives-in-multi-agent-cooperation-yang-li-et-al-2024>(1/2 | 327/346) Aligning Individual and Collective Objectives in Multi-Agent Cooperation (Yang Li et al., 2024)</a></li><li><a href=#22--328346-a-conflict-aware-optimal-goal-assignment-algorithm-for-multi-robot-systems-aakash-et-al-2024>(2/2 | 328/346) A Conflict-Aware Optimal Goal Assignment Algorithm for Multi-Robot Systems (Aakash et al., 2024)</a></li></ul></li><li><a href=#mathco-2>math.CO (2)</a><ul><li><a href=#12--329346-conformally-rigid-graphs-stefan-steinerberger-et-al-2024>(1/2 | 329/346) Conformally rigid graphs (Stefan Steinerberger et al., 2024)</a></li><li><a href=#22--330346-lettericity-of-graphs-an-fpt-algorithm-and-a-bound-on-the-size-of-obstructions-bogdan-alecu-et-al-2024>(2/2 | 330/346) Lettericity of graphs: an FPT algorithm and a bound on the size of obstructions (Bogdan Alecu et al., 2024)</a></li></ul></li><li><a href=#csni-2>cs.NI (2)</a><ul><li><a href=#12--331346-optimize-energy-consumption-of-wireless-sensor-networks-by-using-modified-ant-colony-optimization-aco-yasameen-sajid-razooqi-et-al-2024>(1/2 | 331/346) Optimize Energy Consumption of Wireless Sensor Networks by using modified Ant Colony Optimization ACO (Yasameen Sajid Razooqi et al., 2024)</a></li><li><a href=#22--332346-strengths-and-weaknesses-of-the-etsi-adaptive-dcc-algorithm-a-proposal-for-improvement-ignacio-soto-et-al-2024>(2/2 | 332/346) Strengths and Weaknesses of the ETSI Adaptive DCC Algorithm: A Proposal for Improvement (Ignacio Soto et al., 2024)</a></li></ul></li><li><a href=#q-bionc-1>q-bio.NC (1)</a><ul><li><a href=#11--333346-structure-of-activity-in-multiregion-recurrent-neural-networks-david-g-clark-et-al-2024>(1/1 | 333/346) Structure of activity in multiregion recurrent neural networks (David G. Clark et al., 2024)</a></li></ul></li><li><a href=#cscy-1>cs.CY (1)</a><ul><li><a href=#11--334346-causal-equal-protection-as-algorithmic-fairness-marcello-di-bello-et-al-2024>(1/1 | 334/346) Causal Equal Protection as Algorithmic Fairness (Marcello Di Bello et al., 2024)</a></li></ul></li><li><a href=#q-biobm-1>q-bio.BM (1)</a><ul><li><a href=#11--335346-a-novel-molecule-generative-model-of-vae-combined-with-transformer-yasuhiro-yoshikai-et-al-2024>(1/1 | 335/346) A novel molecule generative model of VAE combined with Transformer (Yasuhiro Yoshikai et al., 2024)</a></li></ul></li><li><a href=#cscg-3>cs.CG (3)</a><ul><li><a href=#13--336346-two-online-map-matching-algorithms-based-on-analytic-hierarchy-process-and-fuzzy-logic-jeremy-j-lin-et-al-2024>(1/3 | 336/346) Two Online Map Matching Algorithms Based on Analytic Hierarchy Process and Fuzzy Logic (Jeremy J. Lin et al., 2024)</a></li><li><a href=#23--337346-flip-graphs-of-pseudo-triangulations-with-face-degree-at-most-4-maarten-löffler-et-al-2024>(2/3 | 337/346) Flip Graphs of Pseudo-Triangulations With Face Degree at Most 4 (Maarten Löffler et al., 2024)</a></li><li><a href=#33--338346-the-complexity-of-geodesic-spanners-using-steiner-points-sarita-de-berg-et-al-2024>(3/3 | 338/346) The Complexity of Geodesic Spanners using Steiner Points (Sarita de Berg et al., 2024)</a></li></ul></li><li><a href=#eessiv-1>eess.IV (1)</a><ul><li><a href=#11--339346-fod-swin-net-angular-super-resolution-of-fiber-orientation-distribution-using-a-transformer-based-deep-model-mateus-oliveira-da-silva-et-al-2024>(1/1 | 339/346) FOD-Swin-Net: angular super resolution of fiber orientation distribution using a transformer-based deep model (Mateus Oliveira da Silva et al., 2024)</a></li></ul></li><li><a href=#csdl-1>cs.DL (1)</a><ul><li><a href=#11--340346-thinking-outside-the-black-box-insights-from-a-digital-exhibition-in-the-humanities-sebastian-barzaghi-et-al-2024>(1/1 | 340/346) Thinking Outside the Black Box: Insights from a Digital Exhibition in the Humanities (Sebastian Barzaghi et al., 2024)</a></li></ul></li><li><a href=#mathst-1>math.ST (1)</a><ul><li><a href=#11--341346-on-the-fourier-coefficients-of-high-dimensional-random-geometric-graphs-kiril-bangachev-et-al-2024>(1/1 | 341/346) On The Fourier Coefficients of High-Dimensional Random Geometric Graphs (Kiril Bangachev et al., 2024)</a></li></ul></li><li><a href=#cspl-1>cs.PL (1)</a><ul><li><a href=#11--342346-ltl-learning-on-gpus-mojtaba-valizadeh-et-al-2024>(1/1 | 342/346) LTL learning on GPUs (Mojtaba Valizadeh et al., 2024)</a></li></ul></li><li><a href=#csds-4>cs.DS (4)</a><ul><li><a href=#14--343346-almost-linear-time-parameterized-algorithm-for-rankwidth-via-dynamic-rankwidth-tuukka-korhonen-et-al-2024>(1/4 | 343/346) Almost-linear time parameterized algorithm for rankwidth via dynamic rankwidth (Tuukka Korhonen et al., 2024)</a></li><li><a href=#24--344346-connectivity-labeling-in-faulty-colored-graphs-asaf-petruschka-et-al-2024>(2/4 | 344/346) Connectivity Labeling in Faulty Colored Graphs (Asaf Petruschka et al., 2024)</a></li><li><a href=#34--345346-collision-free-robot-scheduling-duncan-adamson-et-al-2024>(3/4 | 345/346) Collision-Free Robot Scheduling (Duncan Adamson et al., 2024)</a></li><li><a href=#44--346346-buffered-streaming-edge-partitioning-adil-chhabra-et-al-2024>(4/4 | 346/346) Buffered Streaming Edge Partitioning (Adil Chhabra et al., 2024)</a></li></ul></li></ul></nav></div></div></section></div><footer id=footer class="container-fluid text-center align-content-center footer pb-2"><div class="container pt-5"><div class="row text-left"><div class="col-md-4 col-sm-12"><h5>Navigation</h5><ul><li class=nav-item><a class=smooth-scroll href=https://akitenkrad.github.io/akitenkrad-blog/#about>About</a></li><li class=nav-item><a class=smooth-scroll href=https://akitenkrad.github.io/akitenkrad-blog/#recent-posts>Recent Posts</a></li></ul></div><div class="col-md-4 col-sm-12"><h5>Contact me:</h5><ul><li><a href=mailto:contact.to.akitenkrad@gmail.com target=_blank rel=noopener><span><i class="fas fa-envelope"></i></span> <span>contact.to.akitenkrad@gmail.com</span></a></li></ul></div></div></div><hr><div class=container><div class="row text-left"><div class=col-md-4><a id=theme href=https://github.com/hossainemruz/toha target=_blank rel=noopener><img src=/akitenkrad-blog/images/theme-logo_hu8376fd15465fef26ffe66b6bcf0ca686_13669_32x0_resize_box_3.png alt="Toha Theme Logo">
Toha</a></div><div class="col-md-4 text-center">© 2020 Akitenkrad.</div><div class="col-md-4 text-right"><a id=hugo href=https://gohugo.io/ target=_blank rel=noopener>Powered by
<img src=/akitenkrad-blog/images/hugo-logo.svg alt="Hugo Logo" height=18></a></div></div></div></footer><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/jquery-3.4.1.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/popper.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/bootstrap.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/navbar.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/plyr.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/main.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js></script><script src=https://akitenkrad.github.io/akitenkrad-blog/js/single.js></script><script>hljs.initHighlightingOnLoad()</script><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/katex/katex.min.css><script type=text/javascript defer src=https://akitenkrad.github.io/akitenkrad-blog/katex/katex.min.js></script><script type=text/javascript defer src=https://akitenkrad.github.io/akitenkrad-blog/katex/auto-render.min.js></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\[",right:"\\]",display:!0},{left:"\\(",right:"\\)",display:!1}],throwOnError:!0})})</script></body></html>