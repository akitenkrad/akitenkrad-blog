<!doctype html><html><head><title>arXiv @ 2024.02.27</title>
<meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/bootstrap.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/layouts/main.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/navigators/navbar.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/plyr.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/flag-icon.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/custom/style.css><link href=https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css rel=stylesheet integrity=sha384-9ndCyUaIbzAi2FUVXJi0CjmCapSmO7SnpJef0486qhLnuZ2cdeRhO02iuK6FUUVM crossorigin=anonymous><script async src=https://cdn.jsdelivr.net/npm/es-module-shims@1/dist/es-module-shims.min.js crossorigin=anonymous></script><script type=importmap>
{
  "imports": {
    "@popperjs/core": "https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.8/dist/esm/popper.min.js",
    "bootstrap": "https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.esm.min.js"
  }
}
</script><script src=https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js integrity=sha384-geWF76RCwLtnZ8qwWowPQNguL3RmwHVBC9FhGdlKrxdiJJigb/j/68SIy3Te4Bkz crossorigin=anonymous></script><script type=module src=https://akitenkrad.github.io/akitenkrad-blog/js/custom/custom.js></script><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Muli:wght@300;400;500;600"><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/fontawesome/css/all.min.css><link rel=icon type=image/png href=https://akitenkrad.github.io/akitenkrad-blog/images/favicons/favicon-96x96_huf1ee13f0caf27d1547f91fb46207d708_13005_42x0_resize_box_3.png><meta property="og:title" content="arXiv @ 2024.02.27"><meta property="og:description" content="Primary Categories cs.AI (2) cs.AR (2) cs.CE (1) cs.CL (29) cs.CR (7) cs.CV (18) cs.DB (1) cs.DL (1) cs.DS (1) cs.GR (1) cs.GT (1) cs.HC (1) cs.IR (3) cs.IT (5) cs.LG (25) cs.LO (1) cs.NE (1) cs.NI (2) cs.RO (4) cs.SD (2) cs.SE (5) cs.SI (2) eess.IV (2) math.AP (1) math.CO (2) q-fin.CP (1) stat.ML (1) Keywords keyword cs.CL cs.CV cs.LG Active Learning 1 Adversarial Attack 2 Adversarial Learning 1 Autoencoder 1 Automatic Evaluation 1 BERT 4 BLEU 1 Benchmarking 6 5 6 Black Box 1 ChatGPT 4 1 Chatbot 3 Clustering 1 Continual Learning 1 Continuous Time 2 Contrastive Learning 1 Convolution 2 5 Convolutional Neural Network 5 Counter-factual 1 1 Diffusion Model 1 Direct Preference Optimization 1 Distribution Shift 2 2 Domain Adaptation 1 Explainable AI 1 Federated Learning 3 Few-shot 1 1 Fine-tuning 6 4 2 Foundation Model 1 GPT 3 GPT-2 1 GPT-3 1 GPT-4 1 Gaussian Process 1 Gemini 1 Grammatical Error Correction 1 Graph 2 1 3 Graph Attention Networks 1 Graph Convolutional Network 2 Graph Neural Network 1 Grounding 1 Hallucination Detection 2 In-context Learning 5 Instruction Following 2 Instruction Tuning 2 LSTM 3 Language Generation 1 Large Language Model 37 4 2 Logistic Regression 1 Low-Resource 1 Meta Learning 2 Multi-modal 1 2 2 Named Entity Recognition 2 Natural Language Generation 1 Natural Language Inference 1 Natural Language Understanding 1 Neural Machine Translation 2 Object Detection 1 Out-of-distribution 1 Perplexity 1 Pre-trained Language Model 1 Prompt 4 2 Prompt Learning 1 1 Pruning 1 Quantization 2 Question Answering 2 1 Reasoning 3 1 Recommendation 1 Reinforcement Learning 3 1 2 Reinforcement Learning from Human Feedback 2 Relation Extraction 1 Representation Learning 1 Retrieval-Augmented Generation 2 Self-Attention 1 Self-supervised Learning 1 1 Sentiment Analysis 2 1 Simulation 3 Simulator 3 Speech-to-Speech Translation 2 Stemming 1 Summarization 1 Supervised Learning 3 3 Text Embedding 2 Text Generation 1 Text Understanding 1 Tokenization 1 Transfer Learning 1 3 Transformer 3 5 1 Unsupervised Learning 3 Variational Autoencoder 1 Vision Transformer 2 Weakly-supervised Learning 1 Word Embedding 1 Zero-shot 1 1 Zero-shot Learning 1 cs."><meta property="og:type" content="article"><meta property="og:url" content="https://akitenkrad.github.io/akitenkrad-blog/posts/arxiv/202402/20240227000000/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-02-27T00:00:00+00:00"><meta property="article:modified_time" content="2024-02-27T00:00:00+00:00"><meta name=description content="arXiv @ 2024.02.27"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/atom-one-dark.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/layouts/single.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/navigators/sidebar.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/style.css><script type=text/javascript src=//ajax.googleapis.com/ajax/libs/jquery/3.1.1/jquery.min.js></script><script language=JavaScript>$(document).ready(function(){$("a[href^='http']:not([href*='"+location.hostname+"'])").attr("target","_blank")})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-1MYYZQG0WE"></script><script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-1MYYZQG0WE",{anonymize_ip:!1})}</script></head><body data-spy=scroll data-target=#TableOfContents data-offset=80><div class="container-fluid bg-dimmed wrapper"><nav class="navbar navbar-expand-xl top-navbar final-navbar shadow" style=position:fixed><div class=container><button class="navbar-toggler navbar-light navbar-for-sidebar" id=sidebar-toggler type=button onclick=toggleSidebar()>
<span class=navbar-toggler-icon></span>
</button>
<button class="navbar-toggler navbar-light" id=toc-toggler type=button onclick=toggleTOC()>
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse lang-selector" id=top-nav-items><ul class="navbar-nav mr-auto"><li class=nav-item><a class=navbar-brand href=/akitenkrad-blog><img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png alt=Logo>
Akitenkrad's Blog</a></li></ul><ul class="navbar-nav ml-auto"><li class=nav-item><a class=nav-link href=/akitenkrad-blog#home>Home</a></li><li class=nav-item><a class=nav-link href=/akitenkrad-blog#about>About</a></li><li class=nav-item><a class=nav-link href=/akitenkrad-blog#recent-posts>Recent Posts</a></li><li class=nav-item><a class=nav-link id=blog-link href=https://akitenkrad.github.io/akitenkrad-blog/posts>Posts</a></li><li class=nav-item><a class=nav-link id=tag-link href=https://akitenkrad.github.io/akitenkrad-blog/tags>Tags</a></li></ul></div></div><img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png class=d-none id=main-logo alt=Logo>
<img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png class=d-none id=inverted-logo alt="Inverted Logo"></nav><section class=sidebar-section id=sidebar-section><div class=sidebar-holder><div class=sidebar id=sidebar><form class=mx-auto method=get action=/akitenkrad-blog/search><input type=text name=keyword placeholder=Search data-search id=search-box></form><div class=sidebar-tree><ul class=tree id=tree><li id=list-heading><a href=/posts data-filter=all>Posts</a></li><div class=subtree><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/>Papers</a><ul><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202205/>2022.05</a><ul><li><a href=/akitenkrad-blog/posts/papers/202205/20220518224923/ title="A Context-Aware Citation Recommendation Model with BERT and Graph Convolutional Networks">A Context-Aware Citation Recommendation Model with BERT and Graph Convolutional Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220508162318/ title="A Deep Cascade Model for Multi-Document Reading Comprehension">A Deep Cascade Model for Multi-Document Reading Comprehension</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220506021208/ title="A Primer in BERTology: What We Know About How BERT Works">A Primer in BERTology: What We Know About How BERT Works</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220529131339/ title="Attention Is All You Need">Attention Is All You Need</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220503010000/ title="DeBERTa: Decoding-Enhanced BERT with Disentangled Attention">DeBERTa: Decoding-Enhanced BERT with Disentangled Attention</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220505222900/ title="Dense Passage Retrieval for Open-Domain Question Answering">Dense Passage Retrieval for Open-Domain Question Answering</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220511010217/ title="Multi-Style Generative Reading Comprehension">Multi-Style Generative Reading Comprehension</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220530102936/ title="Neural Machine Translation of Rare Words with Subword Units">Neural Machine Translation of Rare Words with Subword Units</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220523223206/ title="RoBERTa: A Robustly Optimized BERT Pretraining Approach">RoBERTa: A Robustly Optimized BERT Pretraining Approach</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220520124748/ title="Semi-Supervised Classification with Graph Convolutional Networks">Semi-Supervised Classification with Graph Convolutional Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220509110738/ title="Survey on graph embeddings and their applications to machine learning problems on graphs">Survey on graph embeddings and their applications to machine learning problems on graphs</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220514151839/ title="UnitedQA: A Hybrid Approach for Open Domain Question Answering">UnitedQA: A Hybrid Approach for Open Domain Question Answering</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202206/>2022.06</a><ul><li><a href=/akitenkrad-blog/posts/papers/202206/20220612105422/ title="Attributed Network Embedding for Learning in a Dynamic Environment">Attributed Network Embedding for Learning in a Dynamic Environment</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220608085622/ title="CodeBERT: A Pre-Trained Model for Programming and Natural Languages">CodeBERT: A Pre-Trained Model for Programming and Natural Languages</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220618223844/ title="High-order Proximity Preserved Embedding for Dynamic Networks">High-order Proximity Preserved Embedding for Dynamic Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220602171700/ title="S-Net: From Answer Extraction to Answer Generation for Machine Reading Comprehension">S-Net: From Answer Extraction to Answer Generation for Machine Reading Comprehension</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202207/>2022.07</a><ul><li><a href=/akitenkrad-blog/posts/papers/202207/20220727145036/ title="Dynamic Heterogeneous Graph Embedding Using Hierarchical Attentions">Dynamic Heterogeneous Graph Embedding Using Hierarchical Attentions</a></li><li><a href=/akitenkrad-blog/posts/papers/202207/20220726163444/ title="Dynamic Network Embedding Survey">Dynamic Network Embedding Survey</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202208/>2022.08</a><ul><li><a href=/akitenkrad-blog/posts/papers/202208/20220802103319/ title="Modeling Dynamic Heterogeneous Network for Link Prediction using Hierarchical Attention with Temporal RNN">Modeling Dynamic Heterogeneous Network for Link Prediction using Hierarchical Attention with Temporal RNN</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202209/>2022.09</a><ul><li><a href=/akitenkrad-blog/posts/papers/202209/20220909180827/ title="Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting">Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202305/>2023.05</a><ul><li><a href=/akitenkrad-blog/posts/papers/202305/20230513094804/ title="Improving Language Understanding by Generative Pre-Training">Improving Language Understanding by Generative Pre-Training</a></li><li><a href=/akitenkrad-blog/posts/papers/202305/20230514200241/ title="Language Models are Unsupervised Multitask Learners">Language Models are Unsupervised Multitask Learners</a></li></ul></li><li><a href=/akitenkrad-blog/posts/papers/202306/ title=2023.06>2023.06</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/akitenkrad-blog/posts/arxiv/>arXiv</a><ul class=active><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202307/>2023.07</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230701000000/ title="arXiv @ 2023.07.01">arXiv @ 2023.07.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230702000000/ title="arXiv @ 2023.07.02">arXiv @ 2023.07.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230703000000/ title="arXiv @ 2023.07.03">arXiv @ 2023.07.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230704000000/ title="arXiv @ 2023.07.04">arXiv @ 2023.07.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230705000000/ title="arXiv @ 2023.07.05">arXiv @ 2023.07.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230706000000/ title="arXiv @ 2023.07.06">arXiv @ 2023.07.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230707000000/ title="arXiv @ 2023.07.07">arXiv @ 2023.07.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230708000000/ title="arXiv @ 2023.07.08">arXiv @ 2023.07.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230709000000/ title="arXiv @ 2023.07.09">arXiv @ 2023.07.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230710000000/ title="arXiv @ 2023.07.10">arXiv @ 2023.07.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230711000000/ title="arXiv @ 2023.07.11">arXiv @ 2023.07.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230712000000/ title="arXiv @ 2023.07.12">arXiv @ 2023.07.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230713000000/ title="arXiv @ 2023.07.13">arXiv @ 2023.07.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230714000000/ title="arXiv @ 2023.07.14">arXiv @ 2023.07.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230715000000/ title="arXiv @ 2023.07.15">arXiv @ 2023.07.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230716000000/ title="arXiv @ 2023.07.16">arXiv @ 2023.07.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230717000000/ title="arXiv @ 2023.07.17">arXiv @ 2023.07.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230718000000/ title="arXiv @ 2023.07.18">arXiv @ 2023.07.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230719000000/ title="arXiv @ 2023.07.19">arXiv @ 2023.07.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230720000000/ title="arXiv @ 2023.07.20">arXiv @ 2023.07.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230721000000/ title="arXiv @ 2023.07.21">arXiv @ 2023.07.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230722000000/ title="arXiv @ 2023.07.22">arXiv @ 2023.07.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230723000000/ title="arXiv @ 2023.07.23">arXiv @ 2023.07.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230724000000/ title="arXiv @ 2023.07.24">arXiv @ 2023.07.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230725000000/ title="arXiv @ 2023.07.25">arXiv @ 2023.07.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230726000000/ title="arXiv @ 2023.07.26">arXiv @ 2023.07.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230727000000/ title="arXiv @ 2023.07.27">arXiv @ 2023.07.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230728000000/ title="arXiv @ 2023.07.28">arXiv @ 2023.07.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230729000000/ title="arXiv @ 2023.07.29">arXiv @ 2023.07.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230730000000/ title="arXiv @ 2023.07.30">arXiv @ 2023.07.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230731000000/ title="arXiv @ 2023.07.31">arXiv @ 2023.07.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202308/>2023.08</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230801000000/ title="arXiv @ 2023.08.01">arXiv @ 2023.08.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230802000000/ title="arXiv @ 2023.08.02">arXiv @ 2023.08.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230803000000/ title="arXiv @ 2023.08.03">arXiv @ 2023.08.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230804000000/ title="arXiv @ 2023.08.04">arXiv @ 2023.08.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230805000000/ title="arXiv @ 2023.08.05">arXiv @ 2023.08.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230806000000/ title="arXiv @ 2023.08.06">arXiv @ 2023.08.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230807000000/ title="arXiv @ 2023.08.07">arXiv @ 2023.08.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230808000000/ title="arXiv @ 2023.08.08">arXiv @ 2023.08.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230809000000/ title="arXiv @ 2023.08.09">arXiv @ 2023.08.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230810000000/ title="arXiv @ 2023.08.10">arXiv @ 2023.08.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230811000000/ title="arXiv @ 2023.08.11">arXiv @ 2023.08.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230812000000/ title="arXiv @ 2023.08.12">arXiv @ 2023.08.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230813000000/ title="arXiv @ 2023.08.13">arXiv @ 2023.08.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230814000000/ title="arXiv @ 2023.08.14">arXiv @ 2023.08.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230815000000/ title="arXiv @ 2023.08.15">arXiv @ 2023.08.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230816000000/ title="arXiv @ 2023.08.16">arXiv @ 2023.08.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230817000000/ title="arXiv @ 2023.08.17">arXiv @ 2023.08.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230818000000/ title="arXiv @ 2023.08.18">arXiv @ 2023.08.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230819000000/ title="arXiv @ 2023.08.19">arXiv @ 2023.08.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230820000000/ title="arXiv @ 2023.08.20">arXiv @ 2023.08.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230821000000/ title="arXiv @ 2023.08.21">arXiv @ 2023.08.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230822000000/ title="arXiv @ 2023.08.22">arXiv @ 2023.08.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230823000000/ title="arXiv @ 2023.08.23">arXiv @ 2023.08.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230824000000/ title="arXiv @ 2023.08.24">arXiv @ 2023.08.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230825000000/ title="arXiv @ 2023.08.25">arXiv @ 2023.08.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230826000000/ title="arXiv @ 2023.08.26">arXiv @ 2023.08.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230827000000/ title="arXiv @ 2023.08.27">arXiv @ 2023.08.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230828000000/ title="arXiv @ 2023.08.28">arXiv @ 2023.08.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230829000000/ title="arXiv @ 2023.08.29">arXiv @ 2023.08.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230830000000/ title="arXiv @ 2023.08.30">arXiv @ 2023.08.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230831000000/ title="arXiv @ 2023.08.31">arXiv @ 2023.08.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202309/>2023.09</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230901000000/ title="arXiv @ 2023.09.01">arXiv @ 2023.09.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230902000000/ title="arXiv @ 2023.09.02">arXiv @ 2023.09.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230903000000/ title="arXiv @ 2023.09.03">arXiv @ 2023.09.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230904000000/ title="arXiv @ 2023.09.04">arXiv @ 2023.09.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230905000000/ title="arXiv @ 2023.09.05">arXiv @ 2023.09.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230906000000/ title="arXiv @ 2023.09.06">arXiv @ 2023.09.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230907000000/ title="arXiv @ 2023.09.07">arXiv @ 2023.09.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230908000000/ title="arXiv @ 2023.09.08">arXiv @ 2023.09.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230909000000/ title="arXiv @ 2023.09.09">arXiv @ 2023.09.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230910000000/ title="arXiv @ 2023.09.10">arXiv @ 2023.09.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230911000000/ title="arXiv @ 2023.09.11">arXiv @ 2023.09.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230912000000/ title="arXiv @ 2023.09.12">arXiv @ 2023.09.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230913000000/ title="arXiv @ 2023.09.13">arXiv @ 2023.09.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230914000000/ title="arXiv @ 2023.09.14">arXiv @ 2023.09.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230915000000/ title="arXiv @ 2023.09.15">arXiv @ 2023.09.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230916000000/ title="arXiv @ 2023.09.16">arXiv @ 2023.09.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230917000000/ title="arXiv @ 2023.09.17">arXiv @ 2023.09.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230918000000/ title="arXiv @ 2023.09.18">arXiv @ 2023.09.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230919000000/ title="arXiv @ 2023.09.19">arXiv @ 2023.09.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230920000000/ title="arXiv @ 2023.09.20">arXiv @ 2023.09.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230921000000/ title="arXiv @ 2023.09.21">arXiv @ 2023.09.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230922000000/ title="arXiv @ 2023.09.22">arXiv @ 2023.09.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230923000000/ title="arXiv @ 2023.09.23">arXiv @ 2023.09.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230924000000/ title="arXiv @ 2023.09.24">arXiv @ 2023.09.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230925000000/ title="arXiv @ 2023.09.25">arXiv @ 2023.09.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230926000000/ title="arXiv @ 2023.09.26">arXiv @ 2023.09.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230927000000/ title="arXiv @ 2023.09.27">arXiv @ 2023.09.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230928000000/ title="arXiv @ 2023.09.28">arXiv @ 2023.09.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230929000000/ title="arXiv @ 2023.09.29">arXiv @ 2023.09.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230930000000/ title="arXiv @ 2023.09.30">arXiv @ 2023.09.30</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202310/>2023.1</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231001000000/ title="arXiv @ 2023.10.01">arXiv @ 2023.10.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231002000000/ title="arXiv @ 2023.10.02">arXiv @ 2023.10.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231003000000/ title="arXiv @ 2023.10.03">arXiv @ 2023.10.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231004000000/ title="arXiv @ 2023.10.04">arXiv @ 2023.10.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231005000000/ title="arXiv @ 2023.10.05">arXiv @ 2023.10.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231006000000/ title="arXiv @ 2023.10.06">arXiv @ 2023.10.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231007000000/ title="arXiv @ 2023.10.07">arXiv @ 2023.10.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231008000000/ title="arXiv @ 2023.10.08">arXiv @ 2023.10.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231009000000/ title="arXiv @ 2023.10.09">arXiv @ 2023.10.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231010000000/ title="arXiv @ 2023.10.10">arXiv @ 2023.10.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231011000000/ title="arXiv @ 2023.10.11">arXiv @ 2023.10.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231012000000/ title="arXiv @ 2023.10.12">arXiv @ 2023.10.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231013000000/ title="arXiv @ 2023.10.13">arXiv @ 2023.10.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231014000000/ title="arXiv @ 2023.10.14">arXiv @ 2023.10.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231015000000/ title="arXiv @ 2023.10.15">arXiv @ 2023.10.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231016000000/ title="arXiv @ 2023.10.16">arXiv @ 2023.10.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231017000000/ title="arXiv @ 2023.10.17">arXiv @ 2023.10.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231018000000/ title="arXiv @ 2023.10.18">arXiv @ 2023.10.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231019000000/ title="arXiv @ 2023.10.19">arXiv @ 2023.10.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231020000000/ title="arXiv @ 2023.10.20">arXiv @ 2023.10.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231021000000/ title="arXiv @ 2023.10.21">arXiv @ 2023.10.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231022000000/ title="arXiv @ 2023.10.22">arXiv @ 2023.10.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231023000000/ title="arXiv @ 2023.10.23">arXiv @ 2023.10.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231024000000/ title="arXiv @ 2023.10.24">arXiv @ 2023.10.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231025000000/ title="arXiv @ 2023.10.25">arXiv @ 2023.10.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231026000000/ title="arXiv @ 2023.10.26">arXiv @ 2023.10.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231027000000/ title="arXiv @ 2023.10.27">arXiv @ 2023.10.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231028000000/ title="arXiv @ 2023.10.28">arXiv @ 2023.10.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231029000000/ title="arXiv @ 2023.10.29">arXiv @ 2023.10.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231030000000/ title="arXiv @ 2023.10.30">arXiv @ 2023.10.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231031000000/ title="arXiv @ 2023.10.31">arXiv @ 2023.10.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202311/>2023.11</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231101000000/ title="arXiv @ 2023.11.01">arXiv @ 2023.11.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231102000000/ title="arXiv @ 2023.11.02">arXiv @ 2023.11.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231103000000/ title="arXiv @ 2023.11.03">arXiv @ 2023.11.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231104000000/ title="arXiv @ 2023.11.04">arXiv @ 2023.11.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231105000000/ title="arXiv @ 2023.11.05">arXiv @ 2023.11.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231106000000/ title="arXiv @ 2023.11.06">arXiv @ 2023.11.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231107000000/ title="arXiv @ 2023.11.07">arXiv @ 2023.11.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231108000000/ title="arXiv @ 2023.11.08">arXiv @ 2023.11.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231109000000/ title="arXiv @ 2023.11.09">arXiv @ 2023.11.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231110000000/ title="arXiv @ 2023.11.10">arXiv @ 2023.11.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231111000000/ title="arXiv @ 2023.11.11">arXiv @ 2023.11.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231112000000/ title="arXiv @ 2023.11.12">arXiv @ 2023.11.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231113000000/ title="arXiv @ 2023.11.13">arXiv @ 2023.11.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231114000000/ title="arXiv @ 2023.11.14">arXiv @ 2023.11.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231115000000/ title="arXiv @ 2023.11.15">arXiv @ 2023.11.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231116000000/ title="arXiv @ 2023.11.16">arXiv @ 2023.11.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231117000000/ title="arXiv @ 2023.11.17">arXiv @ 2023.11.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231118000000/ title="arXiv @ 2023.11.18">arXiv @ 2023.11.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231119000000/ title="arXiv @ 2023.11.19">arXiv @ 2023.11.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231120000000/ title="arXiv @ 2023.11.20">arXiv @ 2023.11.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231121000000/ title="arXiv @ 2023.11.21">arXiv @ 2023.11.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231122000000/ title="arXiv @ 2023.11.22">arXiv @ 2023.11.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231123000000/ title="arXiv @ 2023.11.23">arXiv @ 2023.11.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231124000000/ title="arXiv @ 2023.11.24">arXiv @ 2023.11.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231125000000/ title="arXiv @ 2023.11.25">arXiv @ 2023.11.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231126000000/ title="arXiv @ 2023.11.26">arXiv @ 2023.11.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231127000000/ title="arXiv @ 2023.11.27">arXiv @ 2023.11.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231128000000/ title="arXiv @ 2023.11.28">arXiv @ 2023.11.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231129000000/ title="arXiv @ 2023.11.29">arXiv @ 2023.11.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231130000000/ title="arXiv @ 2023.11.30">arXiv @ 2023.11.30</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202312/>2023.12</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231201000000/ title="arXiv @ 2023.12.01">arXiv @ 2023.12.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231202000000/ title="arXiv @ 2023.12.02">arXiv @ 2023.12.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231203000000/ title="arXiv @ 2023.12.03">arXiv @ 2023.12.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231204000000/ title="arXiv @ 2023.12.04">arXiv @ 2023.12.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231205000000/ title="arXiv @ 2023.12.05">arXiv @ 2023.12.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231206000000/ title="arXiv @ 2023.12.06">arXiv @ 2023.12.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231207000000/ title="arXiv @ 2023.12.07">arXiv @ 2023.12.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231208000000/ title="arXiv @ 2023.12.08">arXiv @ 2023.12.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231209000000/ title="arXiv @ 2023.12.09">arXiv @ 2023.12.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231210000000/ title="arXiv @ 2023.12.10">arXiv @ 2023.12.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231211000000/ title="arXiv @ 2023.12.11">arXiv @ 2023.12.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231212000000/ title="arXiv @ 2023.12.12">arXiv @ 2023.12.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231213000000/ title="arXiv @ 2023.12.13">arXiv @ 2023.12.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231214000000/ title="arXiv @ 2023.12.14">arXiv @ 2023.12.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231215000000/ title="arXiv @ 2023.12.15">arXiv @ 2023.12.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231216000000/ title="arXiv @ 2023.12.16">arXiv @ 2023.12.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231217000000/ title="arXiv @ 2023.12.17">arXiv @ 2023.12.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231218000000/ title="arXiv @ 2023.12.18">arXiv @ 2023.12.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231219000000/ title="arXiv @ 2023.12.19">arXiv @ 2023.12.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231220000000/ title="arXiv @ 2023.12.20">arXiv @ 2023.12.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231221000000/ title="arXiv @ 2023.12.21">arXiv @ 2023.12.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231222000000/ title="arXiv @ 2023.12.22">arXiv @ 2023.12.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231223000000/ title="arXiv @ 2023.12.23">arXiv @ 2023.12.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231224000000/ title="arXiv @ 2023.12.24">arXiv @ 2023.12.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231225000000/ title="arXiv @ 2023.12.25">arXiv @ 2023.12.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231226000000/ title="arXiv @ 2023.12.26">arXiv @ 2023.12.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231227000000/ title="arXiv @ 2023.12.27">arXiv @ 2023.12.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231228000000/ title="arXiv @ 2023.12.28">arXiv @ 2023.12.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231229000000/ title="arXiv @ 2023.12.29">arXiv @ 2023.12.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231230000000/ title="arXiv @ 2023.12.30">arXiv @ 2023.12.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231231000000/ title="arXiv @ 2023.12.31">arXiv @ 2023.12.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202401/>2024.01</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240101000000/ title="arXiv @ 2024.01.01">arXiv @ 2024.01.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240102000000/ title="arXiv @ 2024.01.02">arXiv @ 2024.01.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240103000000/ title="arXiv @ 2024.01.03">arXiv @ 2024.01.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240104000000/ title="arXiv @ 2024.01.04">arXiv @ 2024.01.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240105000000/ title="arXiv @ 2024.01.05">arXiv @ 2024.01.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240106000000/ title="arXiv @ 2024.01.06">arXiv @ 2024.01.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240107000000/ title="arXiv @ 2024.01.07">arXiv @ 2024.01.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240108000000/ title="arXiv @ 2024.01.08">arXiv @ 2024.01.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240109000000/ title="arXiv @ 2024.01.09">arXiv @ 2024.01.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240110000000/ title="arXiv @ 2024.01.10">arXiv @ 2024.01.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240111000000/ title="arXiv @ 2024.01.11">arXiv @ 2024.01.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240112000000/ title="arXiv @ 2024.01.12">arXiv @ 2024.01.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240113000000/ title="arXiv @ 2024.01.13">arXiv @ 2024.01.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240114000000/ title="arXiv @ 2024.01.14">arXiv @ 2024.01.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240115000000/ title="arXiv @ 2024.01.15">arXiv @ 2024.01.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240116000000/ title="arXiv @ 2024.01.16">arXiv @ 2024.01.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240117000000/ title="arXiv @ 2024.01.17">arXiv @ 2024.01.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240118000000/ title="arXiv @ 2024.01.18">arXiv @ 2024.01.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240119000000/ title="arXiv @ 2024.01.19">arXiv @ 2024.01.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240120000000/ title="arXiv @ 2024.01.20">arXiv @ 2024.01.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240121000000/ title="arXiv @ 2024.01.21">arXiv @ 2024.01.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240122000000/ title="arXiv @ 2024.01.22">arXiv @ 2024.01.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240123000000/ title="arXiv @ 2024.01.23">arXiv @ 2024.01.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240124000000/ title="arXiv @ 2024.01.24">arXiv @ 2024.01.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240125000000/ title="arXiv @ 2024.01.25">arXiv @ 2024.01.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240126000000/ title="arXiv @ 2024.01.26">arXiv @ 2024.01.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240127000000/ title="arXiv @ 2024.01.27">arXiv @ 2024.01.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240128000000/ title="arXiv @ 2024.01.28">arXiv @ 2024.01.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240129000000/ title="arXiv @ 2024.01.29">arXiv @ 2024.01.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240130000000/ title="arXiv @ 2024.01.30">arXiv @ 2024.01.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240131000000/ title="arXiv @ 2024.01.31">arXiv @ 2024.01.31</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/akitenkrad-blog/posts/arxiv/202402/>2024.02</a><ul class=active><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240201000000/ title="arXiv @ 2024.02.01">arXiv @ 2024.02.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240202000000/ title="arXiv @ 2024.02.02">arXiv @ 2024.02.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240203000000/ title="arXiv @ 2024.02.03">arXiv @ 2024.02.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240204000000/ title="arXiv @ 2024.02.04">arXiv @ 2024.02.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240205000000/ title="arXiv @ 2024.02.05">arXiv @ 2024.02.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240206000000/ title="arXiv @ 2024.02.06">arXiv @ 2024.02.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240207000000/ title="arXiv @ 2024.02.07">arXiv @ 2024.02.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240208000000/ title="arXiv @ 2024.02.08">arXiv @ 2024.02.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240209000000/ title="arXiv @ 2024.02.09">arXiv @ 2024.02.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240210000000/ title="arXiv @ 2024.02.10">arXiv @ 2024.02.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240211000000/ title="arXiv @ 2024.02.11">arXiv @ 2024.02.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240212000000/ title="arXiv @ 2024.02.12">arXiv @ 2024.02.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240213000000/ title="arXiv @ 2024.02.13">arXiv @ 2024.02.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240214000000/ title="arXiv @ 2024.02.14">arXiv @ 2024.02.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240215000000/ title="arXiv @ 2024.02.15">arXiv @ 2024.02.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240216000000/ title="arXiv @ 2024.02.16">arXiv @ 2024.02.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240217000000/ title="arXiv @ 2024.02.17">arXiv @ 2024.02.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240218000000/ title="arXiv @ 2024.02.18">arXiv @ 2024.02.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240219000000/ title="arXiv @ 2024.02.19">arXiv @ 2024.02.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240220000000/ title="arXiv @ 2024.02.20">arXiv @ 2024.02.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240221000000/ title="arXiv @ 2024.02.21">arXiv @ 2024.02.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240222000000/ title="arXiv @ 2024.02.22">arXiv @ 2024.02.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240223000000/ title="arXiv @ 2024.02.23">arXiv @ 2024.02.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240224000000/ title="arXiv @ 2024.02.24">arXiv @ 2024.02.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240225000000/ title="arXiv @ 2024.02.25">arXiv @ 2024.02.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240226000000/ title="arXiv @ 2024.02.26">arXiv @ 2024.02.26</a></li><li><a class=active href=/akitenkrad-blog/posts/arxiv/202402/20240227000000/ title="arXiv @ 2024.02.27">arXiv @ 2024.02.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240228000000/ title="arXiv @ 2024.02.28">arXiv @ 2024.02.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240229000000/ title="arXiv @ 2024.02.29">arXiv @ 2024.02.29</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202403/>2024.03</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240301000000/ title="arXiv @ 2024.03.01">arXiv @ 2024.03.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240302000000/ title="arXiv @ 2024.03.02">arXiv @ 2024.03.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240303000000/ title="arXiv @ 2024.03.03">arXiv @ 2024.03.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240304000000/ title="arXiv @ 2024.03.04">arXiv @ 2024.03.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240305000000/ title="arXiv @ 2024.03.05">arXiv @ 2024.03.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240306000000/ title="arXiv @ 2024.03.06">arXiv @ 2024.03.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240307000000/ title="arXiv @ 2024.03.07">arXiv @ 2024.03.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240308000000/ title="arXiv @ 2024.03.08">arXiv @ 2024.03.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240309000000/ title="arXiv @ 2024.03.09">arXiv @ 2024.03.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240310000000/ title="arXiv @ 2024.03.10">arXiv @ 2024.03.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240311000000/ title="arXiv @ 2024.03.11">arXiv @ 2024.03.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240312000000/ title="arXiv @ 2024.03.12">arXiv @ 2024.03.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240313000000/ title="arXiv @ 2024.03.13">arXiv @ 2024.03.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240314000000/ title="arXiv @ 2024.03.14">arXiv @ 2024.03.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240315000000/ title="arXiv @ 2024.03.15">arXiv @ 2024.03.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240316000000/ title="arXiv @ 2024.03.16">arXiv @ 2024.03.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240317000000/ title="arXiv @ 2024.03.17">arXiv @ 2024.03.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240318000000/ title="arXiv @ 2024.03.18">arXiv @ 2024.03.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240319000000/ title="arXiv @ 2024.03.19">arXiv @ 2024.03.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240320000000/ title="arXiv @ 2024.03.20">arXiv @ 2024.03.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240321000000/ title="arXiv @ 2024.03.21">arXiv @ 2024.03.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240322000000/ title="arXiv @ 2024.03.22">arXiv @ 2024.03.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240323000000/ title="arXiv @ 2024.03.23">arXiv @ 2024.03.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240324000000/ title="arXiv @ 2024.03.24">arXiv @ 2024.03.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240325000000/ title="arXiv @ 2024.03.25">arXiv @ 2024.03.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240326000000/ title="arXiv @ 2024.03.26">arXiv @ 2024.03.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240327000000/ title="arXiv @ 2024.03.27">arXiv @ 2024.03.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240328000000/ title="arXiv @ 2024.03.28">arXiv @ 2024.03.28</a></li></ul></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/algorithms/>Algorithms</a><ul><li><a href=/akitenkrad-blog/posts/algorithms/bandit-algorithm-basic/ title="Bandit Algorithm Basic">Bandit Algorithm Basic</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/conference/>Conference</a><ul><li><a href=/akitenkrad-blog/posts/conference/acl/ title="Annual Meeting of the Association for Computational Linguistics">Annual Meeting of the Association for Computational Linguistics</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/>Figures</a><ul><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/202208/>2022.08</a><ul><li><a href=/akitenkrad-blog/posts/figures/202208/20220822092748/ title=1ヶ月当たりの実労働時間の推移>1ヶ月当たりの実労働時間の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220813115200/ title=コンビニエンスストアの店舗数の推移>コンビニエンスストアの店舗数の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220821113322/ title=地域別1世帯当たり1ヶ月間の支出額の推移>地域別1世帯当たり1ヶ月間の支出額の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220828111836/ title="夫婦別子供の有無別生活時間 (2016)">夫婦別子供の有無別生活時間 (2016)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220827103615/ title=家族類型の変遷>家族類型の変遷</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220815131016/ title="年齢別大学院入学者数 (2021)">年齢別大学院入学者数 (2021)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220818122628/ title=情報サービス業における企業特殊的人的資本（名目）の推移>情報サービス業における企業特殊的人的資本（名目）の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220824095127/ title=有業者の社会生活における活動の変化>有業者の社会生活における活動の変化</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220814175022/ title=東京23区の乗用車保有台数>東京23区の乗用車保有台数</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220817121117/ title="業種別企業特殊的人的資本 (2018)">業種別企業特殊的人的資本 (2018)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220825094111/ title=無業者の社会生活における活動の変化>無業者の社会生活における活動の変化</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220823104100/ title=睡眠時間の推移>睡眠時間の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220826102652/ title=総人口の推移>総人口の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220820225528/ title=都道府県別国公立別学校数・学生数>都道府県別国公立別学校数・学生数</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220816120845/ title="都道府県別消費者物価指数/教育（全国平均=100）">都道府県別消費者物価指数/教育（全国平均=100）</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220812173814/ title=金沢市の夏の月平均気温の遷移>金沢市の夏の月平均気温の遷移</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/202209/>2022.09</a><ul><li><a href=/akitenkrad-blog/posts/figures/202209/20220907122906/ title="ACL-2022 WordCloud">ACL-2022 WordCloud</a></li></ul></li></ul></li><li><a href=/akitenkrad-blog/posts/latex/ title="Latex Mathematics Syntax Guide">Latex Mathematics Syntax Guide</a></li><li><a href=/akitenkrad-blog/posts/markdown/ title="Markdown Sample">Markdown Sample</a></li></div></ul></div></div></div></section><section class=content-section id=content-section><div class=content><div class="container p-0 read-area"><div class="hero-area col-sm-12" id=hero-area style=background-image:url(/akitenkrad-blog/posts/arxiv/202402/20240227000000/hero.png)></div><div class=page-content><div class="author-profile ml-auto align-self-lg-center"><img class=rounded-circle src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_120x120_fit_box_3.png alt="Author Image"><h5 class=author-name></h5><p>Tuesday, Feb 27, 2024</p></div><div class=title><h1>arXiv @ 2024.02.27</h1></div><div class=taxonomy-terms><ul><li class=rounded><a href=/akitenkrad-blog/tags/arxiv class="btn, btn-sm">arXiv</a></li><li class=rounded><a href=/akitenkrad-blog/tags/published2024 class="btn, btn-sm">2024</a></li></ul></div><div class=post-content id=post-content><figure style=border:none;width:100%;display:flex;justify-content:center><iframe src=pie.html width=900 height=620 style=border:none></iframe></figure><h2 id=primary-categories>Primary Categories</h2><ul><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240227000000/#csai-2>cs.AI (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240227000000/#csar-2>cs.AR (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240227000000/#csce-1>cs.CE (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240227000000/#cscl-29>cs.CL (29)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240227000000/#cscr-7>cs.CR (7)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240227000000/#cscv-18>cs.CV (18)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240227000000/#csdb-1>cs.DB (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240227000000/#csdl-1>cs.DL (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240227000000/#csds-1>cs.DS (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240227000000/#csgr-1>cs.GR (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240227000000/#csgt-1>cs.GT (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240227000000/#cshc-1>cs.HC (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240227000000/#csir-3>cs.IR (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240227000000/#csit-5>cs.IT (5)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240227000000/#cslg-25>cs.LG (25)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240227000000/#cslo-1>cs.LO (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240227000000/#csne-1>cs.NE (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240227000000/#csni-2>cs.NI (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240227000000/#csro-4>cs.RO (4)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240227000000/#cssd-2>cs.SD (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240227000000/#csse-5>cs.SE (5)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240227000000/#cssi-2>cs.SI (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240227000000/#eessiv-2>eess.IV (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240227000000/#mathap-1>math.AP (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240227000000/#mathco-2>math.CO (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240227000000/#q-fincp-1>q-fin.CP (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240227000000/#statml-1>stat.ML (1)</a></li></ul><h2 id=keywords>Keywords</h2><table border=1 class=dataframe><thead><tr style=text-align:right><th>keyword</th><th>cs.CL</th><th>cs.CV</th><th>cs.LG</th></tr></thead><tbody><tr><td>Active Learning</td><td>1</td><td></td><td></td></tr><tr><td>Adversarial Attack</td><td></td><td>2</td><td></td></tr><tr><td>Adversarial Learning</td><td></td><td>1</td><td></td></tr><tr><td>Autoencoder</td><td></td><td></td><td>1</td></tr><tr><td>Automatic Evaluation</td><td>1</td><td></td><td></td></tr><tr><td>BERT</td><td>4</td><td></td><td></td></tr><tr><td>BLEU</td><td>1</td><td></td><td></td></tr><tr><td>Benchmarking</td><td>6</td><td>5</td><td>6</td></tr><tr><td>Black Box</td><td>1</td><td></td><td></td></tr><tr><td>ChatGPT</td><td>4</td><td></td><td>1</td></tr><tr><td>Chatbot</td><td>3</td><td></td><td></td></tr><tr><td>Clustering</td><td></td><td></td><td>1</td></tr><tr><td>Continual Learning</td><td></td><td>1</td><td></td></tr><tr><td>Continuous Time</td><td></td><td></td><td>2</td></tr><tr><td>Contrastive Learning</td><td></td><td></td><td>1</td></tr><tr><td>Convolution</td><td></td><td>2</td><td>5</td></tr><tr><td>Convolutional Neural Network</td><td></td><td></td><td>5</td></tr><tr><td>Counter-factual</td><td></td><td>1</td><td>1</td></tr><tr><td>Diffusion Model</td><td></td><td></td><td>1</td></tr><tr><td>Direct Preference Optimization</td><td>1</td><td></td><td></td></tr><tr><td>Distribution Shift</td><td>2</td><td></td><td>2</td></tr><tr><td>Domain Adaptation</td><td></td><td>1</td><td></td></tr><tr><td>Explainable AI</td><td></td><td>1</td><td></td></tr><tr><td>Federated Learning</td><td></td><td></td><td>3</td></tr><tr><td>Few-shot</td><td>1</td><td></td><td>1</td></tr><tr><td>Fine-tuning</td><td>6</td><td>4</td><td>2</td></tr><tr><td>Foundation Model</td><td></td><td></td><td>1</td></tr><tr><td>GPT</td><td>3</td><td></td><td></td></tr><tr><td>GPT-2</td><td>1</td><td></td><td></td></tr><tr><td>GPT-3</td><td>1</td><td></td><td></td></tr><tr><td>GPT-4</td><td>1</td><td></td><td></td></tr><tr><td>Gaussian Process</td><td></td><td></td><td>1</td></tr><tr><td>Gemini</td><td>1</td><td></td><td></td></tr><tr><td>Grammatical Error Correction</td><td>1</td><td></td><td></td></tr><tr><td>Graph</td><td>2</td><td>1</td><td>3</td></tr><tr><td>Graph Attention Networks</td><td></td><td></td><td>1</td></tr><tr><td>Graph Convolutional Network</td><td></td><td></td><td>2</td></tr><tr><td>Graph Neural Network</td><td>1</td><td></td><td></td></tr><tr><td>Grounding</td><td></td><td>1</td><td></td></tr><tr><td>Hallucination Detection</td><td>2</td><td></td><td></td></tr><tr><td>In-context Learning</td><td>5</td><td></td><td></td></tr><tr><td>Instruction Following</td><td>2</td><td></td><td></td></tr><tr><td>Instruction Tuning</td><td>2</td><td></td><td></td></tr><tr><td>LSTM</td><td></td><td></td><td>3</td></tr><tr><td>Language Generation</td><td>1</td><td></td><td></td></tr><tr><td>Large Language Model</td><td>37</td><td>4</td><td>2</td></tr><tr><td>Logistic Regression</td><td></td><td></td><td>1</td></tr><tr><td>Low-Resource</td><td>1</td><td></td><td></td></tr><tr><td>Meta Learning</td><td></td><td></td><td>2</td></tr><tr><td>Multi-modal</td><td>1</td><td>2</td><td>2</td></tr><tr><td>Named Entity Recognition</td><td>2</td><td></td><td></td></tr><tr><td>Natural Language Generation</td><td>1</td><td></td><td></td></tr><tr><td>Natural Language Inference</td><td>1</td><td></td><td></td></tr><tr><td>Natural Language Understanding</td><td>1</td><td></td><td></td></tr><tr><td>Neural Machine Translation</td><td>2</td><td></td><td></td></tr><tr><td>Object Detection</td><td></td><td>1</td><td></td></tr><tr><td>Out-of-distribution</td><td></td><td></td><td>1</td></tr><tr><td>Perplexity</td><td>1</td><td></td><td></td></tr><tr><td>Pre-trained Language Model</td><td>1</td><td></td><td></td></tr><tr><td>Prompt</td><td>4</td><td>2</td><td></td></tr><tr><td>Prompt Learning</td><td>1</td><td>1</td><td></td></tr><tr><td>Pruning</td><td></td><td></td><td>1</td></tr><tr><td>Quantization</td><td></td><td>2</td><td></td></tr><tr><td>Question Answering</td><td>2</td><td>1</td><td></td></tr><tr><td>Reasoning</td><td>3</td><td>1</td><td></td></tr><tr><td>Recommendation</td><td>1</td><td></td><td></td></tr><tr><td>Reinforcement Learning</td><td>3</td><td>1</td><td>2</td></tr><tr><td>Reinforcement Learning from Human Feedback</td><td>2</td><td></td><td></td></tr><tr><td>Relation Extraction</td><td>1</td><td></td><td></td></tr><tr><td>Representation Learning</td><td></td><td></td><td>1</td></tr><tr><td>Retrieval-Augmented Generation</td><td>2</td><td></td><td></td></tr><tr><td>Self-Attention</td><td></td><td>1</td><td></td></tr><tr><td>Self-supervised Learning</td><td></td><td>1</td><td>1</td></tr><tr><td>Sentiment Analysis</td><td>2</td><td></td><td>1</td></tr><tr><td>Simulation</td><td></td><td></td><td>3</td></tr><tr><td>Simulator</td><td></td><td></td><td>3</td></tr><tr><td>Speech-to-Speech Translation</td><td>2</td><td></td><td></td></tr><tr><td>Stemming</td><td></td><td>1</td><td></td></tr><tr><td>Summarization</td><td>1</td><td></td><td></td></tr><tr><td>Supervised Learning</td><td>3</td><td>3</td><td></td></tr><tr><td>Text Embedding</td><td>2</td><td></td><td></td></tr><tr><td>Text Generation</td><td>1</td><td></td><td></td></tr><tr><td>Text Understanding</td><td>1</td><td></td><td></td></tr><tr><td>Tokenization</td><td>1</td><td></td><td></td></tr><tr><td>Transfer Learning</td><td>1</td><td>3</td><td></td></tr><tr><td>Transformer</td><td>3</td><td>5</td><td>1</td></tr><tr><td>Unsupervised Learning</td><td></td><td>3</td><td></td></tr><tr><td>Variational Autoencoder</td><td></td><td></td><td>1</td></tr><tr><td>Vision Transformer</td><td></td><td>2</td><td></td></tr><tr><td>Weakly-supervised Learning</td><td></td><td>1</td><td></td></tr><tr><td>Word Embedding</td><td>1</td><td></td><td></td></tr><tr><td>Zero-shot</td><td>1</td><td></td><td>1</td></tr><tr><td>Zero-shot Learning</td><td></td><td></td><td>1</td></tr></tbody></table><script>$(function(){$("table").addClass("keyword-table table-bordered border-success"),$("table thead").addClass("sticky-top"),$("table tbody td").css("text-align","")})</script><h2 id=cssd-2>cs.SD (2)</h2><h3 id=12--1122-chatmusician-understanding-and-generating-music-intrinsically-with-llm-ruibin-yuan-et-al-2024>(1/2 | 1/122) ChatMusician: Understanding and Generating Music Intrinsically with LLM (Ruibin Yuan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ruibin Yuan, Hanfeng Lin, Yi Wang, Zeyue Tian, Shangda Wu, Tianhao Shen, Ge Zhang, Yuhang Wu, Cong Liu, Ziya Zhou, Ziyang Ma, Liumeng Xue, Ziyu Wang, Qin Liu, Tianyu Zheng, Yizhi Li, Yinghao Ma, Yiming Liang, Xiaowei Chi, Ruibo Liu, Zili Wang, Pengfei Li, Jingcheng Wu, Chenghua Lin, Qifeng Liu, Tao Jiang, Wenhao Huang, Wenhu Chen, Emmanouil Benetos, Jie Fu, Gus Xia, Roger Dannenberg, Wei Xue, Shiyin Kang, Yike Guo. (2024)<br><strong>ChatMusician: Understanding and Generating Music Intrinsically with LLM</strong><br><button class=copy-to-clipboard title="ChatMusician: Understanding and Generating Music Intrinsically with LLM" index=1>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-1 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SD<br>Categories: cs-AI, cs-CL, cs-LG, cs-MM, cs-SD, cs.SD, eess-AS<br>Keyword Score: 106<br>Keywords: Benchmarking, Fine-tuning, Multi-modal, Zero-shot, GPT, GPT-3, GPT-3.5, GPT-4, Massive Multitask Language Understanding (MMLU), Text Generation, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16153v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16153v1.pdf filename=2402.16153v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>While <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> demonstrate impressive capabilities in <b>text</b> <b>generation,</b> we find that their ability has yet to be generalized to music, humanity&rsquo;s creative language. We introduce ChatMusician, an open-source <b>LLM</b> that integrates intrinsic musical abilities. It is based on continual pre-training and <b>finetuning</b> LLaMA2 on a <b>text-compatible</b> <b>music</b> representation, ABC notation, and the music is treated as a second language. ChatMusician can understand and generate music with a pure <b>text</b> <b>tokenizer</b> without any external <b>multi-modal</b> neural structures or tokenizers. Interestingly, endowing musical abilities does not harm language abilities, even achieving a slightly higher <b>MMLU</b> score. Our model is capable of composing well-structured, full-length music, conditioned on <b>texts,</b> <b>chords,</b> melodies, motifs, musical forms, etc, surpassing <b>GPT-4</b> baseline. On our meticulously curated college-level music understanding <b>benchmark,</b> MusicTheoryBench, ChatMusician surpasses LLaMA2 and <b>GPT-3.5</b> on <b>zero-shot</b> setting by a noticeable margin. Our work reveals that <b>LLMs</b> can be an excellent compressor for music, but there remains significant territory to be conquered. We release our 4B token music-language corpora MusicPile, the collected MusicTheoryBench, code, model and demo in GitHub.</p></p class="citation"></blockquote><h3 id=22--2122-phonetic-and-lexical-discovery-of-a-canine-language-using-hubert-xingyuan-li-et-al-2024>(2/2 | 2/122) Phonetic and Lexical Discovery of a Canine Language using HuBERT (Xingyuan Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xingyuan Li, Sinong Wang, Zeyu Xie, Mengyue Wu, Kenny Q. Zhu. (2024)<br><strong>Phonetic and Lexical Discovery of a Canine Language using HuBERT</strong><br><button class=copy-to-clipboard title="Phonetic and Lexical Discovery of a Canine Language using HuBERT" index=2>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-2 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SD<br>Categories: cs-CL, cs-LG, cs-SD, cs.SD, eess-AS<br>Keyword Score: 20<br>Keywords: Self-supervised Learning, N-gram<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15985v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15985v1.pdf filename=2402.15985v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper delves into the pioneering exploration of potential communication patterns within dog vocalizations and transcends traditional linguistic analysis barriers, which heavily relies on human priori knowledge on limited datasets to find sound units in dog vocalization. We present a <b>self-supervised</b> approach with HuBERT, enabling the accurate classification of phoneme labels and the identification of vocal patterns that suggest a rudimentary vocabulary within dog vocalizations. Our findings indicate a significant acoustic consistency in these identified canine vocabulary, covering the entirety of observed dog vocalization sequences. We further develop a web-based dog vocalization labeling system. This system can highlight phoneme <b>n-grams,</b> present in the vocabulary, in the dog audio uploaded by users.</p></p class="citation"></blockquote><h2 id=cscl-29>cs.CL (29)</h2><h3 id=129--3122-citation-enhanced-generation-for-llm-based-chatbot-weitao-li-et-al-2024>(1/29 | 3/122) Citation-Enhanced Generation for LLM-based Chatbot (Weitao Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Weitao Li, Junkai Li, Weizhi Ma, Yang Liu. (2024)<br><strong>Citation-Enhanced Generation for LLM-based Chatbot</strong><br><button class=copy-to-clipboard title="Citation-Enhanced Generation for LLM-based Chatbot" index=3>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-3 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 83<br>Keywords: Benchmarking, Reinforcement Learning, Retrieval-Augmented Generation, Retrieval-Augmented Generation, Chatbot, Hallucination Detection, Natural Language Inference, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16063v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16063v2.pdf filename=2402.16063v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> exhibit powerful general intelligence across diverse scenarios, including their integration into <b>chatbots.</b> However, a vital challenge of <b>LLM-based</b> <b>chatbots</b> is that they may produce hallucinated content in responses, which significantly limits their applicability. Various efforts have been made to alleviate <b>hallucination,</b> <b>such</b> as <b>retrieval</b> <b>augmented</b> <b>generation</b> and <b>reinforcement</b> <b>learning</b> with human feedback, but most of them require additional training and data annotation. In this paper, we propose a novel post-hoc Citation-Enhanced Generation (CEG) approach combined with <b>retrieval</b> <b>argumentation.</b> <b>Unlike</b> previous studies that focus on preventing <b>hallucinations</b> <b>during</b> generation, our method addresses this issue in a post-hoc way. It incorporates a <b>retrieval</b> <b>module</b> <b>to</b> search for supporting documents relevant to the generated content, and employs a <b>natural</b> <b>language</b> <b>inference-based</b> citation generation module. Once the statements in the generated content lack of reference, our model can regenerate responses until all statements are supported by citations. Note that our method is a training-free plug-and-play plugin that is capable of various <b>LLMs.</b> Experiments on various <b>hallucination-related</b> <b>datasets</b> show our framework outperforms state-of-the-art methods in both <b>hallucination</b> <b>detection</b> and response regeneration on three <b>benchmarks.</b> Our codes and dataset will be publicly available.</p></p class="citation"></blockquote><h3 id=229--4122-graphwiz-an-instruction-following-language-model-for-graph-problems-nuo-chen-et-al-2024>(2/29 | 4/122) GraphWiz: An Instruction-Following Language Model for Graph Problems (Nuo Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nuo Chen, Yuhan Li, Jianheng Tang, Jia Li. (2024)<br><strong>GraphWiz: An Instruction-Following Language Model for Graph Problems</strong><br><button class=copy-to-clipboard title="GraphWiz: An Instruction-Following Language Model for Graph Problems" index=4>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-4 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 83<br>Keywords: Graph, Direct Preference Optimization, GPT, GPT-4, Instruction Following, Reasoning, Instruction Tuning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16029v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16029v1.pdf filename=2402.16029v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have achieved impressive success across several fields, but their proficiency in understanding and resolving complex <b>graph</b> problems is less explored. To bridge this gap, we introduce GraphInstruct, a novel and comprehensive <b>instruction-tuning</b> <b>dataset</b> designed to equip language models with the ability to tackle a broad spectrum of <b>graph</b> problems using explicit <b>reasoning</b> paths. Utilizing GraphInstruct, we build GraphWiz, an open-source language model capable of resolving various <b>graph</b> problem types while generating clear <b>reasoning</b> processes. To enhance the model&rsquo;s capability and reliability, we incorporate the <b>Direct</b> <b>Preference</b> <b>Optimization</b> (DPO) framework into the <b>graph</b> problem-solving context. The enhanced model, GraphWiz-DPO, achieves an average accuracy of 65% across nine tasks with different complexity levels, surpassing <b>GPT-4</b> which has an average accuracy of 43.8%. Moreover, our research delves into the delicate balance between training data volume and model performance, highlighting the potential for overfitting with increased data. We also explore the transferability of the model&rsquo;s <b>reasoning</b> ability across different <b>graph</b> tasks, indicating the model&rsquo;s adaptability and practical application potential. Our investigation offers a new blueprint and valuable insights for developing <b>LLMs</b> specialized in <b>graph</b> <b>reasoning</b> and problem-solving.</p></p class="citation"></blockquote><h3 id=329--5122-llms-with-chain-of-thought-are-non-causal-reasoners-guangsheng-bao-et-al-2024>(3/29 | 5/122) LLMs with Chain-of-Thought Are Non-Causal Reasoners (Guangsheng Bao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Guangsheng Bao, Hongbo Zhang, Linyi Yang, Cunxiang Wang, Yue Zhang. (2024)<br><strong>LLMs with Chain-of-Thought Are Non-Causal Reasoners</strong><br><button class=copy-to-clipboard title="LLMs with Chain-of-Thought Are Non-Causal Reasoners" index=5>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-5 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 80<br>Keywords: Fine-tuning, Reinforcement Learning, Supervised Learning, Reasoning, In-context Learning, In-context Learning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16048v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16048v1.pdf filename=2402.16048v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper explores the role of the Chain of Thought (CoT) in <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> <b>reasoning.</b> Despite its potential to improve task performance, our analysis reveals a surprising frequency of correct answers following incorrect CoTs and vice versa. We employ causal analysis to assess the cause-effect relationship between CoTs/instructions and answers in <b>LLMs,</b> uncovering the Structural Causal Model (SCM) that <b>LLMs</b> approximate. By comparing the implied SCM with that of human <b>reasoning,</b> we highlight discrepancies between <b>LLM</b> and human <b>reasoning</b> processes. We further examine the factors influencing the causal structure of the implied SCM, revealing that <b>in-context</b> <b>learning,</b> <b>supervised</b> <b>fine-tuning,</b> and <b>reinforcement</b> <b>learning</b> on human feedback significantly impact the causal relations. We release the code and results at <a href=https://github.com/StevenZHB/CoT_Causal_Analysis>https://github.com/StevenZHB/CoT_Causal_Analysis</a>.</p></p class="citation"></blockquote><h3 id=429--6122-likelihood-based-mitigation-of-evaluation-bias-in-large-language-models-masanari-ohi-et-al-2024>(4/29 | 6/122) Likelihood-based Mitigation of Evaluation Bias in Large Language Models (Masanari Ohi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Masanari Ohi, Masahiro Kaneko, Ryuto Koike, Mengsay Loem, Naoaki Okazaki. (2024)<br><strong>Likelihood-based Mitigation of Evaluation Bias in Large Language Models</strong><br><button class=copy-to-clipboard title="Likelihood-based Mitigation of Evaluation Bias in Large Language Models" index=6>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-6 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 80<br>Keywords: Few-shot, Grammatical Error Correction, Language Generation, Natural Language Generation, In-context Learning, In-context Learning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15987v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15987v1.pdf filename=2402.15987v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> are widely used to evaluate <b>natural</b> <b>language</b> <b>generation</b> tasks as automated metrics. However, the likelihood, a measure of <b>LLM&rsquo;s</b> plausibility for a sentence, can vary due to superficial differences in sentences, such as word order and sentence structure. It is therefore possible that there might be a likelihood bias if <b>LLMs</b> are used for evaluation: they might overrate sentences with higher likelihoods while underrating those with lower likelihoods. In this paper, we investigate the presence and impact of likelihood bias in <b>LLM-based</b> evaluators. We also propose a method to mitigate the likelihood bias. Our method utilizes highly biased instances as <b>few-shot</b> examples for <b>in-context</b> <b>learning.</b> Our experiments in evaluating the data-to-text and <b>grammatical</b> <b>error</b> <b>correction</b> tasks reveal that several <b>LLMs</b> we test display a likelihood bias. Furthermore, our proposed method successfully mitigates this bias, also improving evaluation performance (in terms of correlation of models with human scores) significantly.</p></p class="citation"></blockquote><h3 id=529--7122-from-noise-to-clarity-unraveling-the-adversarial-suffix-of-large-language-model-attacks-via-translation-of-text-embeddings-hao-wang-et-al-2024>(5/29 | 7/122) From Noise to Clarity: Unraveling the Adversarial Suffix of Large Language Model Attacks via Translation of Text Embeddings (Hao Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hao Wang, Hao Li, Minlie Huang, Lei Sha. (2024)<br><strong>From Noise to Clarity: Unraveling the Adversarial Suffix of Large Language Model Attacks via Translation of Text Embeddings</strong><br><button class=copy-to-clipboard title="From Noise to Clarity: Unraveling the Adversarial Suffix of Large Language Model Attacks via Translation of Text Embeddings" index=7>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-7 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 75<br>Keywords: Black Box, ChatGPT, Gemini, Large Language Model, Large Language Model, Perplexity, Prompt, Text Embedding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16006v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16006v1.pdf filename=2402.16006v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The safety defense methods of <b>Large</b> <b>language</b> <b>models(LLMs)</b> stays limited because the dangerous <b>prompts</b> are manually curated to just few known attack types, which fails to keep pace with emerging varieties. Recent studies found that attaching suffixes to harmful instructions can hack the defense of <b>LLMs</b> and lead to dangerous outputs. This method, while effective, leaves a gap in understanding the underlying mechanics of such adversarial suffix due to the non-readability and it can be relatively easily seen through by common defense methods such as <b>perplexity</b> filters.To cope with this challenge, in this paper, we propose an Adversarial Suffixes Embedding Translation Framework(ASETF) that are able to translate the unreadable adversarial suffixes into coherent, readable <b>text,</b> <b>which</b> makes it easier to understand and analyze the reasons behind harmful content generation by <b>large</b> <b>language</b> <b>models.</b> We conducted experiments on <b>LLMs</b> such as LLaMa2, Vicuna and using the Advbench dataset&rsquo;s harmful instructions. The results indicate that our method achieves a much better attack success rate to existing techniques, while significantly enhancing the textual fluency of the <b>prompts.</b> In addition, our approach can be generalized into a broader method for generating transferable adversarial suffixes that can successfully attack multiple <b>LLMs,</b> even <b>black-box</b> <b>LLMs,</b> such as <b>ChatGPT</b> and <b>Gemini.</b> As a result, the <b>prompts</b> generated through our method exhibit enriched semantic diversity, which potentially provides more adversarial examples for <b>LLM</b> defense methods.</p></p class="citation"></blockquote><h3 id=629--8122-higpt-heterogeneous-graph-language-model-jiabin-tang-et-al-2024>(6/29 | 8/122) HiGPT: Heterogeneous Graph Language Model (Jiabin Tang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiabin Tang, Yuhao Yang, Wei Wei, Lei Shi, Long Xia, Dawei Yin, Chao Huang. (2024)<br><strong>HiGPT: Heterogeneous Graph Language Model</strong><br><button class=copy-to-clipboard title="HiGPT: Heterogeneous Graph Language Model" index=8>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-8 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keyword Score: 63<br>Keywords: Graph, Graph Neural Network, Distribution Shift, Distribution Shift, Fine-tuning, Fine-tuning, In-context Learning, Instruction Tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16024v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16024v1.pdf filename=2402.16024v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Heterogeneous <b>graph</b> <b>learning</b> <b>aims</b> to capture complex relationships and diverse relational semantics among entities in a heterogeneous <b>graph</b> <b>to</b> <b>obtain</b> meaningful representations for nodes and edges. Recent advancements in heterogeneous <b>graph</b> <b>neural</b> <b>networks</b> (HGNNs) have achieved state-of-the-art performance by considering relation heterogeneity and using specialized message functions and aggregation rules. However, existing frameworks for heterogeneous <b>graph</b> <b>learning</b> <b>have</b> limitations in generalizing across diverse heterogeneous <b>graph</b> <b>datasets.</b> <b>Most</b> of these frameworks follow the &ldquo;pre-train&rdquo; and <b>&ldquo;fine-tune&rdquo;</b> paradigm on the same dataset, which restricts their capacity to adapt to new and unseen data. This raises the question: &ldquo;Can we generalize heterogeneous <b>graph</b> <b>models</b> <b>to</b> be well-adapted to diverse downstream learning tasks with <b>distribution</b> <b>shifts</b> in both node token sets and relation type heterogeneity?&rsquo;&rsquo; To tackle those challenges, we propose HiGPT, a general large <b>graph</b> <b>model</b> <b>with</b> Heterogeneous <b>graph</b> <b>instruction-tuning</b> <b>paradigm.</b> Our framework enables learning from arbitrary heterogeneous <b>graphs</b> <b>without</b> <b>the</b> need for any <b>fine-tuning</b> process from downstream datasets. To handle <b>distribution</b> <b>shifts</b> in heterogeneity, we introduce an <b>in-context</b> heterogeneous <b>graph</b> <b>tokenizer</b> <b>that</b> captures semantic relationships in different heterogeneous <b>graphs,</b> <b>facilitating</b> <b>model</b> adaptation. We incorporate a large corpus of heterogeneity-aware <b>graph</b> <b>instructions</b> <b>into</b> our HiGPT, enabling the model to effectively comprehend complex relation heterogeneity and distinguish between various types of <b>graph</b> <b>tokens.</b> <b>Furthermore,</b> we introduce the Mixture-of-Thought (MoT) <b>instruction</b> <b>augmentation</b> paradigm to mitigate data scarcity by generating diverse and informative <b>instructions.</b> <b>Through</b> comprehensive evaluations, our proposed framework demonstrates exceptional performance in terms of generalization performance.</p></p class="citation"></blockquote><h3 id=729--9122-distalaner-distantly-supervised-active-learning-augmented-named-entity-recognition-in-the-open-source-software-ecosystem-somnath-banerjee-et-al-2024>(7/29 | 9/122) DistALANER: Distantly Supervised Active Learning Augmented Named Entity Recognition in the Open Source Software Ecosystem (Somnath Banerjee et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Somnath Banerjee, Avik Dutta, Aaditya Agrawal, Rima Hazra, Animesh Mukherjee. (2024)<br><strong>DistALANER: Distantly Supervised Active Learning Augmented Named Entity Recognition in the Open Source Software Ecosystem</strong><br><button class=copy-to-clipboard title="DistALANER: Distantly Supervised Active Learning Augmented Named Entity Recognition in the Open Source Software Ecosystem" index=9>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-9 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 60<br>Keywords: Active Learning, Supervised Learning, Named Entity Recognition, Named Entity Recognition, Relation Extraction, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16159v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16159v1.pdf filename=2402.16159v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper proposes a novel <b>named</b> <b>entity</b> <b>recognition</b> <b>(NER)</b> technique specifically tailored for the open-source software systems. Our approach aims to address the scarcity of annotated software data by employing a comprehensive two-step distantly <b>supervised</b> annotation process. This process strategically leverages language heuristics, unique lookup tables, external knowledge sources, and an <b>active</b> <b>learning</b> approach. By harnessing these powerful techniques, we not only enhance model performance but also effectively mitigate the limitations associated with cost and the scarcity of expert annotators. It is noteworthy that our framework significantly outperforms the state-of-the-art <b>LLMs</b> by a substantial margin. We also show the effectiveness of <b>NER</b> in the downstream task of <b>relation</b> <b>extraction.</b></p></p class="citation"></blockquote><h3 id=829--10122-text-understanding-and-generation-using-transformer-models-for-intelligent-e-commerce-recommendations-yafei-xiang-et-al-2024>(8/29 | 10/122) Text Understanding and Generation Using Transformer Models for Intelligent E-commerce Recommendations (Yafei Xiang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yafei Xiang, Hanyi Yu, Yulu Gong, Shuning Huo, Mengran Zhu. (2024)<br><strong>Text Understanding and Generation Using Transformer Models for Intelligent E-commerce Recommendations</strong><br><button class=copy-to-clipboard title="Text Understanding and Generation Using Transformer Models for Intelligent E-commerce Recommendations" index=10>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-10 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 60<br>Keywords: Recommendation, Transformer, Sentiment Analysis, Large Language Model, Large Language Model, Text Understanding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16035v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16035v1.pdf filename=2402.16035v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the rapid development of artificial intelligence technology, <b>Transformer</b> structural pre-training model has become an important tool for <b>large</b> <b>language</b> <b>model</b> <b>(LLM)</b> tasks. In the field of e-commerce, these models are especially widely used, from <b>text</b> <b>understanding</b> to generating <b>recommendation</b> systems, which provide powerful technical support for improving user experience and optimizing service processes. This paper reviews the core application scenarios of <b>Transformer</b> pre-training model in e-commerce <b>text</b> <b>understanding</b> and <b>recommendation</b> generation, including but not limited to automatic generation of product descriptions, <b>sentiment</b> <b>analysis</b> of user comments, construction of personalized <b>recommendation</b> system and automated processing of customer service conversations. Through a detailed analysis of the model&rsquo;s working principle, implementation process, and application effects in specific cases, this paper emphasizes the unique advantages of pre-trained models in understanding complex user intentions and improving the quality of <b>recommendations.</b> In addition, the challenges and improvement directions for the future are also discussed, such as how to further improve the generalization ability of the model, the ability to handle <b>large-scale</b> <b>data</b> <b>sets,</b> and technical strategies to protect user privacy. Ultimately, the paper points out that the application of <b>Transformer</b> structural pre-training models in e-commerce has not only driven technological innovation, but also brought substantial benefits to merchants and consumers, and looking forward, these models will continue to play a key role in e-commerce and beyond.</p></p class="citation"></blockquote><h3 id=929--11122-dont-forget-your-reward-values-language-model-alignment-via-value-based-calibration-xin-mao-et-al-2024>(9/29 | 11/122) Don&rsquo;t Forget Your Reward Values: Language Model Alignment via Value-based Calibration (Xin Mao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xin Mao, Feng-Lin Li, Huimin Xu, Wei Zhang, Anh Tuan Luu. (2024)<br><strong>Don&rsquo;t Forget Your Reward Values: Language Model Alignment via Value-based Calibration</strong><br><button class=copy-to-clipboard title="Don't Forget Your Reward Values: Language Model Alignment via Value-based Calibration" index=11>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-11 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 60<br>Keywords: Reinforcement Learning, Reinforcement Learning from Human Feedback, Reinforcement Learning from Human Feedback, Large Language Model, Large Language Model, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16030v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16030v1.pdf filename=2402.16030v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>While <b>Reinforcement</b> <b>Learning</b> <b>from</b> <b>Human</b> <b>Feedback</b> <b>(RLHF)</b> significantly enhances the generation quality of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs),</b> recent studies have raised concerns regarding the complexity and instability associated with the Proximal Policy Optimization (PPO) algorithm, proposing a series of order-based calibration methods as viable alternatives. This paper delves further into current order-based methods, examining their inefficiencies in utilizing reward values and addressing misalignment issues. Building upon these findings, we propose a novel \textbf{V}alue-based \textbf{C}ali\textbf{B}ration (VCB) method to better align <b>LLMs</b> with human preferences. Experimental results demonstrate that VCB surpasses existing alignment methods on AI assistant and <b>summarization</b> datasets, providing impressive generalizability, robustness, and stability in diverse settings.</p></p class="citation"></blockquote><h3 id=1029--12122-from-text-to-transformation-a-comprehensive-review-of-large-language-models-versatility-pravneet-kaur-et-al-2024>(10/29 | 12/122) From Text to Transformation: A Comprehensive Review of Large Language Models&rsquo; Versatility (Pravneet Kaur et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Pravneet Kaur, Gautam Siddharth Kashyap, Ankit Kumar, Md Tabrez Nafis, Sandeep Kumar, Vikrant Shokeen. (2024)<br><strong>From Text to Transformation: A Comprehensive Review of Large Language Models&rsquo; Versatility</strong><br><button class=copy-to-clipboard title="From Text to Transformation: A Comprehensive Review of Large Language Models' Versatility" index=12>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-12 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: BERT, GPT, Transformer, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16142v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16142v1.pdf filename=2402.16142v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This groundbreaking study explores the expanse of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs),</b> such as Generative Pre-Trained <b>Transformer</b> <b>(GPT)</b> and Bidirectional Encoder Representations from <b>Transformers</b> <b>(BERT)</b> across varied domains ranging from technology, finance, healthcare to education. Despite their established prowess in Natural Language Processing (NLP), these <b>LLMs</b> have not been systematically examined for their impact on domains such as fitness, and holistic well-being, urban planning, climate modelling as well as disaster management. This review paper, in addition to furnishing a comprehensive analysis of the vast expanse and extent of <b>LLMs&rsquo;</b> utility in diverse domains, recognizes the research gaps and realms where the potential of <b>LLMs</b> is yet to be harnessed. This study uncovers innovative ways in which <b>LLMs</b> can leave a mark in the fields like fitness and wellbeing, urban planning, climate modelling and disaster response which could inspire future researches and applications in the said avenues.</p></p class="citation"></blockquote><h3 id=1129--13122-say-more-with-less-understanding-prompt-learning-behaviors-through-gist-compression-xinze-li-et-al-2024>(11/29 | 13/122) Say More with Less: Understanding Prompt Learning Behaviors through Gist Compression (Xinze Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xinze Li, Zhenghao Liu, Chenyan Xiong, Shi Yu, Yukun Yan, Shuo Wang, Ge Yu. (2024)<br><strong>Say More with Less: Understanding Prompt Learning Behaviors through Gist Compression</strong><br><button class=copy-to-clipboard title="Say More with Less: Understanding Prompt Learning Behaviors through Gist Compression" index=13>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-13 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: Fine-tuning, Large Language Model, Large Language Model, Prompt, Prompt Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16058v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16058v1.pdf filename=2402.16058v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> require lengthy <b>prompts</b> <b>as</b> the input context to produce output aligned with user intentions, a process that incurs extra costs during inference. In this paper, we propose the Gist COnditioned deCOding (Gist-COCO) model, introducing a novel method for compressing <b>prompts</b> <b>which</b> also can assist the <b>prompt</b> <b>interpretation</b> and engineering. Gist-COCO employs an encoder-decoder based language model and then incorporates an additional encoder as a plugin module to compress <b>prompts</b> <b>with</b> inputs using gist tokens. It <b>finetunes</b> the compression plugin module and uses the representations of gist tokens to emulate the raw <b>prompts</b> <b>in</b> the vanilla language model. By verbalizing the representations of gist tokens into gist <b>prompts,</b> <b>the</b> compression ability of Gist-COCO can be generalized to different <b>LLMs</b> with high compression rates. Our experiments demonstrate that Gist-COCO outperforms previous <b>prompt</b> <b>compression</b> models in both passage and instruction compression tasks. Further analysis on gist verbalization results suggests that our gist <b>prompts</b> <b>serve</b> different functions in aiding language models. They may directly provide potential answers, generate the chain-of-thought, or simply repeat the inputs. All data and codes are available at <a href=https://github.com/OpenMatch/Gist-COCO>https://github.com/OpenMatch/Gist-COCO</a> .</p></p class="citation"></blockquote><h3 id=1229--14122-deep-learning-approaches-for-improving-question-answering-systems-in-hepatocellular-carcinoma-research-shuning-huo-et-al-2024>(12/29 | 14/122) Deep Learning Approaches for Improving Question Answering Systems in Hepatocellular Carcinoma Research (Shuning Huo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shuning Huo, Yafei Xiang, Hanyi Yu, Mengran Zhu, Yulu Gong. (2024)<br><strong>Deep Learning Approaches for Improving Question Answering Systems in Hepatocellular Carcinoma Research</strong><br><button class=copy-to-clipboard title="Deep Learning Approaches for Improving Question Answering Systems in Hepatocellular Carcinoma Research" index=14>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-14 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 50<br>Keywords: BERT, GPT, GPT-3, Question Answering, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16038v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16038v1.pdf filename=2402.16038v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In recent years, advancements in natural language processing (NLP) have been fueled by deep learning techniques, particularly through the utilization of powerful computing resources like GPUs and TPUs. Models such as <b>BERT</b> and <b>GPT-3,</b> trained on vast amounts of data, have revolutionized language understanding and generation. These pre-trained models serve as robust bases for various tasks including semantic understanding, intelligent writing, and <b>reasoning,</b> paving the way for a more generalized form of artificial intelligence. NLP, as a vital application of AI, aims to bridge the gap between humans and computers through natural language interaction. This paper delves into the current landscape and future prospects of large-scale model-based NLP, focusing on the <b>question-answering</b> <b>systems</b> within this domain. Practical cases and developments in artificial intelligence-driven <b>question-answering</b> <b>systems</b> are analyzed to foster further exploration and research in the realm of large-scale NLP.</p></p class="citation"></blockquote><h3 id=1329--15122-direct-punjabi-to-english-speech-translation-using-discrete-units-prabhjot-kaur-et-al-2024>(13/29 | 15/122) Direct Punjabi to English speech translation using discrete units (Prabhjot Kaur et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Prabhjot Kaur, L. Andrew M. Bush, Weisong Shi. (2024)<br><strong>Direct Punjabi to English speech translation using discrete units</strong><br><button class=copy-to-clipboard title="Direct Punjabi to English speech translation using discrete units" index=15>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-15 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-SD, cs.CL, eess-AS<br>Keyword Score: 50<br>Keywords: Low-Resource, Transformer, Speech-to-Speech Translation, Speech-to-Speech Translation, BLEU<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15967v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15967v1.pdf filename=2402.15967v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Speech-to-speech</b> <b>translation</b> is yet to reach the same level of coverage as text-to-text translation systems. The current <b>speech</b> <b>technology</b> is highly limited in its coverage of over 7000 languages spoken worldwide, leaving more than half of the population deprived of such technology and shared experiences. With voice-assisted technology (such as social robots and <b>speech-to-text</b> <b>apps)</b> and auditory content (such as podcasts and lectures) on the rise, ensuring that the technology is available for all is more important than ever. <b>Speech</b> <b>translation</b> can play a vital role in mitigating technological disparity and creating a more inclusive society. With a motive to contribute towards <b>speech</b> <b>translation</b> research for <b>low-resource</b> languages, our work presents a direct <b>speech-to-speech</b> <b>translation</b> model for one of the Indic languages called Punjabi to English. Additionally, we explore the performance of using a discrete representation of <b>speech</b> <b>called</b> discrete acoustic units as input to the <b>Transformer-based</b> translation model. The model, abbreviated as Unit-to-Unit Translation (U2UT), takes a sequence of discrete units of the source language (the language being translated from) and outputs a sequence of discrete units of the target language (the language being translated to). Our results show that the U2UT model performs better than the <b>Speech-to-Unit</b> <b>Translation</b> (S2UT) model by a 3.69 <b>BLEU</b> score.</p></p class="citation"></blockquote><h3 id=1429--16122-hypotermqa-hypothetical-terms-dataset-for-benchmarking-hallucination-tendency-of-llms-cem-uluoglakci-et-al-2024>(14/29 | 16/122) HypoTermQA: Hypothetical Terms Dataset for Benchmarking Hallucination Tendency of LLMs (Cem Uluoglakci et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Cem Uluoglakci, Tugba Taskaya Temizel. (2024)<br><strong>HypoTermQA: Hypothetical Terms Dataset for Benchmarking Hallucination Tendency of LLMs</strong><br><button class=copy-to-clipboard title="HypoTermQA: Hypothetical Terms Dataset for Benchmarking Hallucination Tendency of LLMs" index=16>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-16 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 46<br>Keywords: Benchmarking, Benchmarking, Chatbot, Hallucination Detection, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16211v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16211v1.pdf filename=2402.16211v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Hallucinations</b> <b>pose</b> a significant challenge to the reliability and alignment of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs),</b> limiting their widespread acceptance beyond <b>chatbot</b> applications. Despite ongoing efforts, <b>hallucinations</b> <b>remain</b> a prevalent challenge in <b>LLMs.</b> The detection of <b>hallucinations</b> <b>itself</b> is also a formidable task, frequently requiring manual labeling or constrained evaluations. This paper introduces an automated scalable framework that combines <b>benchmarking</b> <b>LLMs&rsquo;</b> <b>hallucination</b> <b>tendencies</b> with efficient <b>hallucination</b> <b>detection.</b> We leverage <b>LLMs</b> to generate challenging tasks related to hypothetical phenomena, subsequently employing them as agents for efficient <b>hallucination</b> <b>detection.</b> The framework is domain-agnostic, allowing the use of any language model for <b>benchmark</b> creation or evaluation in any domain. We introduce the publicly available HypoTermQA <b>Benchmarking</b> Dataset, on which state-of-the-art models&rsquo; performance ranged between 3% and 11%, and evaluator agents demonstrated a 6% error rate in <b>hallucination</b> <b>prediction.</b> The proposed framework provides opportunities to test and improve <b>LLMs.</b> Additionally, it has the potential to generate <b>benchmarking</b> datasets tailored to specific domains, such as law, health, and finance.</p></p class="citation"></blockquote><h3 id=1529--17122-defending-large-language-models-against-jailbreak-attacks-via-semantic-smoothing-jiabao-ji-et-al-2024>(15/29 | 17/122) Defending Large Language Models against Jailbreak Attacks via Semantic Smoothing (Jiabao Ji et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiabao Ji, Bairu Hou, Alexander Robey, George J. Pappas, Hamed Hassani, Yang Zhang, Eric Wong, Shiyu Chang. (2024)<br><strong>Defending Large Language Models against Jailbreak Attacks via Semantic Smoothing</strong><br><button class=copy-to-clipboard title="Defending Large Language Models against Jailbreak Attacks via Semantic Smoothing" index=17>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-17 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 43<br>Keywords: Benchmarking, Instruction Following, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16192v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16192v2.pdf filename=2402.16192v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Aligned <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> are vulnerable to jailbreaking attacks, which bypass the safeguards of targeted <b>LLMs</b> and fool them into generating objectionable content. While initial defenses show promise against token-based threat models, there do not exist defenses that provide robustness against semantic attacks and avoid unfavorable trade-offs between robustness and nominal performance. To meet this need, we propose SEMANTICSMOOTH, a smoothing-based defense that aggregates the predictions of multiple semantically transformed copies of a given input <b>prompt.</b> Experimental results demonstrate that SEMANTICSMOOTH achieves state-of-the-art robustness against GCG, PAIR, and AutoDAN attacks while maintaining strong nominal performance on <b>instruction</b> <b>following</b> <b>benchmarks</b> such as InstructionFollowing and AlpacaEval. The codes will be publicly available at <a href=https://github.com/UCSB-NLP-Chang/SemanticSmooth>https://github.com/UCSB-NLP-Chang/SemanticSmooth</a>.</p></p class="citation"></blockquote><h3 id=1629--18122-ehrnoteqa-a-patient-specific-question-answering-benchmark-for-evaluating-large-language-models-in-clinical-settings-sunjun-kweon-et-al-2024>(16/29 | 18/122) EHRNoteQA: A Patient-Specific Question Answering Benchmark for Evaluating Large Language Models in Clinical Settings (Sunjun Kweon et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sunjun Kweon, Jiyoun Kim, Heeyoung Kwak, Dongchul Cha, Hangyul Yoon, Kwanghyun Kim, Seunghyun Won, Edward Choi. (2024)<br><strong>EHRNoteQA: A Patient-Specific Question Answering Benchmark for Evaluating Large Language Models in Clinical Settings</strong><br><button class=copy-to-clipboard title="EHRNoteQA: A Patient-Specific Question Answering Benchmark for Evaluating Large Language Models in Clinical Settings" index=18>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-18 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 43<br>Keywords: Automatic Evaluation, Benchmarking, Question Answering, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16040v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16040v2.pdf filename=2402.16040v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This study introduces EHRNoteQA, a novel patient-specific <b>question</b> <b>answering</b> <b>benchmark</b> tailored for evaluating <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> in clinical environments. Based on MIMIC-IV Electronic Health Record (EHR), a team of three medical professionals has curated the dataset comprising 962 unique <b>questions,</b> <b>each</b> linked to a specific patient&rsquo;s EHR clinical notes. What makes EHRNoteQA distinct from existing EHR-based <b>benchmarks</b> is as follows: Firstly, it is the first dataset to adopt a multi-choice <b>question</b> <b>answering</b> format, a design choice that effectively evaluates <b>LLMs</b> with reliable scores in the context of <b>automatic</b> <b>evaluation,</b> compared to other formats. Secondly, it requires an analysis of multiple clinical notes to answer a single <b>question,</b> <b>reflecting</b> the complex nature of real-world clinical decision-making where clinicians review extensive records of patient histories. Our comprehensive evaluation on various <b>large</b> <b>language</b> <b>models</b> showed that their scores on EHRNoteQA correlate more closely with their performance in addressing real-world medical <b>questions</b> <b>evaluated</b> by clinicians than their scores from other <b>LLM</b> <b>benchmarks.</b> This underscores the significance of EHRNoteQA in evaluating <b>LLMs</b> for medical applications and highlights its crucial role in facilitating the integration of <b>LLMs</b> into healthcare systems. The dataset will be made available to the public under PhysioNet credential access, promoting further research in this vital field.</p></p class="citation"></blockquote><h3 id=1729--19122-periodiclora-breaking-the-low-rank-bottleneck-in-lora-optimization-xiangdi-meng-et-al-2024>(17/29 | 19/122) PeriodicLoRA: Breaking the Low-Rank Bottleneck in LoRA Optimization (Xiangdi Meng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiangdi Meng, Damai Dai, Weiyao Luo, Zhe Yang, Shaoxiang Wu, Xiaochen Wang, Peiyi Wang, Qingxiu Dong, Liang Chen, Zhifang Sui. (2024)<br><strong>PeriodicLoRA: Breaking the Low-Rank Bottleneck in LoRA Optimization</strong><br><button class=copy-to-clipboard title="PeriodicLoRA: Breaking the Low-Rank Bottleneck in LoRA Optimization" index=19>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-19 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Fine-tuning, Supervised Learning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16141v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16141v1.pdf filename=2402.16141v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Supervised</b> <b>fine-tuning</b> is the most common method to adapt <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> to downstream tasks, but full <b>fine-tuning</b> <b>LLMs</b> requires massive computational resources. Recently, parameter-efficient <b>fine-tuning</b> (PEFT) methods have been widely studied due to its cost-effectiveness. LoRA is one of the most widely used methods, which assumes that the optimization process is essentially low-dimensional. Although LoRA <b>fine-tuning</b> is effective, there is still a performance gap compared to full <b>fine-tuning,</b> since its weight update is limited to low-rank matrices. In order to break the low-rank bottleneck in LoRA Optimization, we propose PeriodicLoRA (PLoRA), which accumulates low-rank update matrices multiple times to achieve a higher update rank. PLoRA has multiple training stages. During each stage, we still update only the LoRA weights. However, at the end of each stage, we unload the LoRA weights into the backbone parameters and then reinitialize the LoRA states. Experimental results show that PLoRA has stronger learning ability, approximately 1.8 times that of LoRA&rsquo;s learning ability at most, but it does not increase memory usage. Further, we introduce a momentum-based unloading strategy for PLoRA to mitigate the training instability.</p></p class="citation"></blockquote><h3 id=1829--20122-lstprompt-large-language-models-as-zero-shot-time-series-forecasters-by-long-short-term-prompting-haoxin-liu-et-al-2024>(18/29 | 20/122) LSTPrompt: Large Language Models as Zero-Shot Time Series Forecasters by Long-Short-Term Prompting (Haoxin Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haoxin Liu, Zhiyuan Zhao, Jindong Wang, Harshavardhan Kamarthi, B. Aditya Prakash. (2024)<br><strong>LSTPrompt: Large Language Models as Zero-Shot Time Series Forecasters by Long-Short-Term Prompting</strong><br><button class=copy-to-clipboard title="LSTPrompt: Large Language Models as Zero-Shot Time Series Forecasters by Long-Short-Term Prompting" index=20>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-20 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Zero-shot, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16132v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16132v1.pdf filename=2402.16132v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Time-series forecasting (TSF) finds broad applications in real-world scenarios. <b>Prompting</b> off-the-shelf <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> demonstrates strong <b>zero-shot</b> TSF capabilities while preserving computational efficiency. However, existing <b>prompting</b> methods oversimplify TSF as language next-token predictions, overlooking its dynamic nature and lack of integration with state-of-the-art <b>prompt</b> strategies such as Chain-of-Thought. Thus, we propose LSTPrompt, a novel approach for <b>prompting</b> <b>LLMs</b> in <b>zero-shot</b> TSF tasks. LSTPrompt decomposes TSF into short-term and long-term forecasting sub-tasks, tailoring <b>prompts</b> to each. LSTPrompt guides <b>LLMs</b> to regularly reassess forecasting mechanisms to enhance adaptability. Extensive evaluations demonstrate consistently better performance of LSTPrompt than existing <b>prompting</b> methods, and competitive results compared to foundation TSF models.</p></p class="citation"></blockquote><h3 id=1929--21122-detecting-machine-generated-texts-by-multi-population-aware-optimization-for-maximum-mean-discrepancy-shuhai-zhang-et-al-2024>(19/29 | 21/122) Detecting Machine-Generated Texts by Multi-Population Aware Optimization for Maximum Mean Discrepancy (Shuhai Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shuhai Zhang, Yiliao Song, Jiahao Yang, Yuanqing Li, Bo Han, Mingkui Tan. (2024)<br><strong>Detecting Machine-Generated Texts by Multi-Population Aware Optimization for Maximum Mean Discrepancy</strong><br><button class=copy-to-clipboard title="Detecting Machine-Generated Texts by Multi-Population Aware Optimization for Maximum Mean Discrepancy" index=21>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-21 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keyword Score: 40<br>Keywords: ChatGPT, GPT-2, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16041v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16041v2.pdf filename=2402.16041v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> such as <b>ChatGPT</b> have exhibited remarkable performance in generating human-like texts. However, machine-generated texts (MGTs) may carry critical risks, such as plagiarism issues, misleading information, or hallucination issues. Therefore, it is very urgent and important to detect MGTs in many situations. Unfortunately, it is challenging to distinguish MGTs and human-written texts because the distributional discrepancy between them is often very subtle due to the remarkable performance of <b>LLMs.</b> In this paper, we seek to exploit \textit{maximum mean discrepancy} (MMD) to address this issue in the sense that MMD can well identify distributional discrepancies. However, directly training a detector with MMD using diverse MGTs will incur a significantly increased variance of MMD since MGTs may contain \textit{multiple text populations} due to various <b>LLMs.</b> This will severely impair MMD&rsquo;s ability to measure the difference between two samples. To tackle this, we propose a novel \textit{multi-population} aware optimization method for MMD called MMD-MP, which can \textit{avoid variance increases} and thus improve the stability to measure the distributional discrepancy. Relying on MMD-MP, we develop two methods for paragraph-based and sentence-based detection, respectively. Extensive experiments on various <b>LLMs,</b> \eg, <b>GPT2</b> and <b>ChatGPT,</b> show superior detection performance of our MMD-MP. The source code is available at \url{https://github.com/ZSHsh98/MMD-MP}.</p></p class="citation"></blockquote><h3 id=2029--22122-emotion-classification-in-short-english-texts-using-deep-learning-techniques-siddhanth-bhat-2024>(20/29 | 22/122) Emotion Classification in Short English Texts using Deep Learning Techniques (Siddhanth Bhat, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Siddhanth Bhat. (2024)<br><strong>Emotion Classification in Short English Texts using Deep Learning Techniques</strong><br><button class=copy-to-clipboard title="Emotion Classification in Short English Texts using Deep Learning Techniques" index=22>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-22 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Transfer Learning, BERT, Text Embedding, Word Embedding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16034v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16034v1.pdf filename=2402.16034v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Detecting emotions in limited <b>text</b> <b>datasets</b> from under-resourced languages presents a formidable obstacle, demanding specialized frameworks and computational strategies. This study conducts a thorough examination of deep learning techniques for discerning emotions in short English <b>texts.</b> <b>Deep</b> learning approaches employ <b>transfer</b> <b>learning</b> and <b>word</b> <b>embedding,</b> notably <b>BERT,</b> to attain superior accuracy. To evaluate these methods, we introduce the &ldquo;SmallEnglishEmotions&rdquo; dataset, comprising 6372 varied short Persian <b>texts</b> <b>annotated</b> with five primary emotion categories. Our experiments reveal that <b>transfer</b> <b>learning</b> and <b>BERT-based</b> <b>text</b> <b>embedding</b> outperform alternative methods in accurately categorizing the <b>text</b> <b>in</b> the dataset.</p></p class="citation"></blockquote><h3 id=2129--23122-asem-enhancing-empathy-in-chatbot-through-attention-based-sentiment-and-emotion-modeling-omama-hamad-et-al-2024>(21/29 | 23/122) ASEM: Enhancing Empathy in Chatbot through Attention-based Sentiment and Emotion Modeling (Omama Hamad et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Omama Hamad, Ali Hamdi, Khaled Shaban. (2024)<br><strong>ASEM: Enhancing Empathy in Chatbot through Attention-based Sentiment and Emotion Modeling</strong><br><button class=copy-to-clipboard title="ASEM: Enhancing Empathy in Chatbot through Attention-based Sentiment and Emotion Modeling" index=23>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-23 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Chatbot, Sentiment Analysis, Text Generation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16194v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16194v1.pdf filename=2402.16194v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Effective feature representations play a critical role in enhancing the performance of <b>text</b> <b>generation</b> models that rely on deep neural networks. However, current approaches suffer from several drawbacks, such as the inability to capture the deep semantics of language and sensitivity to minor input variations, resulting in significant changes in the generated <b>text.</b> <b>In</b> this paper, we present a novel solution to these challenges by employing a mixture of experts, multiple encoders, to offer distinct perspectives on the emotional state of the user&rsquo;s utterance while simultaneously enhancing performance. We propose an end-to-end model architecture called ASEM that performs emotion analysis on top of <b>sentiment</b> <b>analysis</b> for open-domain <b>chatbots,</b> enabling the generation of empathetic responses that are fluent and relevant. In contrast to traditional attention mechanisms, the proposed model employs a specialized attention strategy that uniquely zeroes in on <b>sentiment</b> <b>and</b> emotion nuances within the user&rsquo;s utterance. This ensures the generation of context-rich representations tailored to the underlying emotional tone and <b>sentiment</b> <b>intricacies</b> of the <b>text.</b> <b>Our</b> approach outperforms existing methods for generating empathetic embeddings, providing empathetic and diverse responses. The performance of our proposed model significantly exceeds that of existing models, enhancing emotion detection accuracy by 6.2% and lexical diversity by 1.4%.</p></p class="citation"></blockquote><h3 id=2229--24122-fusechat-knowledge-fusion-of-chat-models-fanqi-wan-et-al-2024>(22/29 | 24/122) FuseChat: Knowledge Fusion of Chat Models (Fanqi Wan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fanqi Wan, Ziyi Yang, Longguang Zhong, Xiaojun Quan, Xinting Huang, Wei Bi. (2024)<br><strong>FuseChat: Knowledge Fusion of Chat Models</strong><br><button class=copy-to-clipboard title="FuseChat: Knowledge Fusion of Chat Models" index=24>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-24 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Fine-tuning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16107v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16107v2.pdf filename=2402.16107v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>While training <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> from scratch can indeed lead to models with distinct capabilities and strengths, this approach incurs substantial costs and may lead to potential redundancy in competencies. An alternative strategy is to combine existing <b>LLMs</b> into a more robust <b>LLM,</b> thereby diminishing the necessity for expensive pre-training. However, due to the diverse architectures of <b>LLMs,</b> direct parameter blending proves to be unfeasible. Recently, \textsc{FuseLLM} introduced the concept of knowledge fusion to transfer the collective knowledge of multiple structurally varied <b>LLMs</b> into a target <b>LLM</b> through lightweight continual training. In this report, we extend the scalability and flexibility of the \textsc{FuseLLM} framework to realize the fusion of chat <b>LLMs,</b> resulting in \textsc{FuseChat}. \textsc{FuseChat} comprises two main stages. Firstly, we undertake knowledge fusion for structurally and scale-varied source <b>LLMs</b> to derive multiple target <b>LLMs</b> of identical structure and size via lightweight <b>fine-tuning.</b> Then, these target <b>LLMs</b> are merged within the parameter space, wherein we propose a novel method for determining the merging weights based on the variation ratio of parameter matrices before and after <b>fine-tuning.</b> We validate our approach using three prominent chat <b>LLMs</b> with diverse architectures and scales, namely \texttt{NH2-Mixtral-8x7B}, \texttt{NH2-Solar-10.7B}, and \texttt{OpenChat-3.5-7B}. Experimental results spanning various chat domains demonstrate the superiority of \texttt{\textsc{FuseChat}-7B} across a broad spectrum of chat <b>LLMs</b> at 7B and 34B scales, even surpassing \texttt{GPT-3.5 (March)} and approaching \texttt{Mixtral-8x7B-Instruct}. Our code, model weights, and data are openly accessible at \url{https://github.com/fanqiwan/FuseLLM}.</p></p class="citation"></blockquote><h3 id=2329--25122-how-large-language-models-encode-context-knowledge-a-layer-wise-probing-study-tianjie-ju-et-al-2024>(23/29 | 25/122) How Large Language Models Encode Context Knowledge? A Layer-Wise Probing Study (Tianjie Ju et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tianjie Ju, Weiwei Sun, Wei Du, Xinwei Yuan, Zhaochun Ren, Gongshen Liu. (2024)<br><strong>How Large Language Models Encode Context Knowledge? A Layer-Wise Probing Study</strong><br><button class=copy-to-clipboard title="How Large Language Models Encode Context Knowledge? A Layer-Wise Probing Study" index=25>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-25 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: ChatGPT, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16061v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16061v1.pdf filename=2402.16061v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Previous work has showcased the intriguing capability of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> in retrieving facts and processing context knowledge. However, only limited research exists on the layer-wise capability of <b>LLMs</b> to encode knowledge, which challenges our understanding of their internal mechanisms. In this paper, we devote the first attempt to investigate the layer-wise capability of <b>LLMs</b> through probing tasks. We leverage the powerful generative capability of <b>ChatGPT</b> to construct probing datasets, providing diverse and coherent evidence corresponding to various facts. We employ $\mathcal V$-usable information as the validation metric to better reflect the capability in encoding context knowledge across different layers. Our experiments on conflicting and newly acquired knowledge show that <b>LLMs:</b> (1) prefer to encode more context knowledge in the upper layers; (2) primarily encode context knowledge within knowledge-related entity tokens at lower layers while progressively expanding more knowledge within other tokens at upper layers; and (3) gradually forget the earlier context knowledge retained within the intermediate layers when provided with irrelevant evidence. Code is publicly available at <a href=https://github.com/Jometeorie/probing_llama>https://github.com/Jometeorie/probing_llama</a>.</p></p class="citation"></blockquote><h3 id=2429--26122-tmt-tri-modal-translation-between-speech-image-and-text-by-processing-different-modalities-as-different-languages-minsu-kim-et-al-2024>(24/29 | 26/122) TMT: Tri-Modal Translation between Speech, Image, and Text by Processing Different Modalities as Different Languages (Minsu Kim et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Minsu Kim, Jee-weon Jung, Hyeongseop Rha, Soumi Maiti, Siddhant Arora, Xuankai Chang, Shinji Watanabe, Yong Man Ro. (2024)<br><strong>TMT: Tri-Modal Translation between Speech, Image, and Text by Processing Different Modalities as Different Languages</strong><br><button class=copy-to-clipboard title="TMT: Tri-Modal Translation between Speech, Image, and Text by Processing Different Modalities as Different Languages" index=26>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-26 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-CV, cs.CL, eess-AS<br>Keyword Score: 23<br>Keywords: Multi-modal, Neural Machine Translation, Tokenization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16021v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16021v1.pdf filename=2402.16021v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The capability to jointly process <b>multi-modal</b> information is becoming an essential task. However, the limited number of paired <b>multi-modal</b> data and the large computational requirements in <b>multi-modal</b> learning hinder the development. We propose a novel Tri-Modal Translation (TMT) model that translates between arbitrary modalities spanning speech, image, and text. We introduce a novel viewpoint, where we interpret different modalities as different languages, and treat <b>multi-modal</b> translation as a well-established <b>machine</b> <b>translation</b> problem. To this end, we tokenize speech and image data into discrete tokens, which provide a unified interface across modalities and significantly decrease the computational cost. In the proposed TMT, a <b>multi-modal</b> encoder-decoder conducts the core translation, whereas modality-specific processing is conducted only within the <b>tokenization</b> and detokenization stages. We evaluate the proposed TMT on all six modality translation tasks. TMT outperforms single model counterparts consistently, demonstrating that unifying tasks is beneficial not only for practicality but also for performance.</p></p class="citation"></blockquote><h3 id=2529--27122-c3-confidence-calibration-model-cascade-for-inference-efficient-cross-lingual-natural-language-understanding-taixi-lu-et-al-2024>(25/29 | 27/122) $C^3$: Confidence Calibration Model Cascade for Inference-Efficient Cross-Lingual Natural Language Understanding (Taixi Lu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Taixi Lu, Haoyu Wang, Huajie Shao, Jing Gao, Huaxiu Yao. (2024)<br><strong>$C^3$: Confidence Calibration Model Cascade for Inference-Efficient Cross-Lingual Natural Language Understanding</strong><br><button class=copy-to-clipboard title="$C^3$: Confidence Calibration Model Cascade for Inference-Efficient Cross-Lingual Natural Language Understanding" index=27>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-27 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 23<br>Keywords: Benchmarking, Natural Language Understanding, Pre-trained Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15991v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15991v1.pdf filename=2402.15991v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Cross-lingual <b>natural</b> <b>language</b> <b>understanding</b> (NLU) is a critical task in <b>natural</b> <b>language</b> <b>processing</b> (NLP). Recent advancements have seen multilingual <b>pre-trained</b> <b>language</b> <b>models</b> (mPLMs) significantly enhance the performance of these tasks. However, mPLMs necessitate substantial resources and incur high computational costs during inference, posing challenges for deployment in real-world and real-time systems. Existing model cascade methods seek to enhance inference efficiency by greedily selecting the lightest model capable of processing the current input from a variety of models, based on model confidence scores. Nonetheless, deep models tend to exhibit overconfidence, and confidence distributions vary across languages. This leads to the emission of confident but incorrect predictions by smaller models, hindering their ability to generalize effectively across test languages. In this study, we introduce a confidence calibration model cascade ($C^3$) method. This approach, simple yet effective, involves calibration prior to cascade inference, thereby enhancing cascade accuracy through more reliable predictions. Extensive experiments conducted on three cross-lingual <b>benchmarks</b> demonstrate that $C^3$ significantly outperforms all state-of-the-art baselines.</p></p class="citation"></blockquote><h3 id=2629--28122-instructedit-instruction-based-knowledge-editing-for-large-language-models-bozhong-tian-et-al-2024>(26/29 | 28/122) InstructEdit: Instruction-based Knowledge Editing for Large Language Models (Bozhong Tian et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bozhong Tian, Siyuan Cheng, Xiaozhuan Liang, Ningyu Zhang, Yi Hu, Kouying Xue, Yanjie Gou, Xi Chen, Huajun Chen. (2024)<br><strong>InstructEdit: Instruction-based Knowledge Editing for Large Language Models</strong><br><button class=copy-to-clipboard title="InstructEdit: Instruction-based Knowledge Editing for Large Language Models" index=28>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-28 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-CV, cs-HC, cs-LG, cs.CL<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16123v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16123v1.pdf filename=2402.16123v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Knowledge editing for <b>large</b> <b>language</b> <b>models</b> can offer an efficient solution to alter a model&rsquo;s behavior without negatively impacting the overall performance. However, the current approach encounters issues with limited generalizability across tasks, necessitating one distinct editor for each task, which significantly hinders the broader applications. To address this, we take the first step to analyze the multi-task generalization issue in knowledge editing. Specifically, we develop an instruction-based editing technique, termed InstructEdit, which facilitates the editor&rsquo;s adaptation to various task performances simultaneously using simple instructions. With only one unified editor for each <b>LLM,</b> we empirically demonstrate that InstructEdit can improve the editor&rsquo;s control, leading to an average 14.86% increase in Reliability in multi-task editing setting. Furthermore, experiments involving holdout unseen task illustrate that InstructEdit consistently surpass previous strong baselines. To further investigate the underlying mechanisms of instruction-based knowledge editing, we analyze the principal components of the editing gradient directions, which unveils that instructions can help control optimization direction with stronger OOD generalization. Code and datasets will be available in <a href=https://github.com/zjunlp/EasyEdit>https://github.com/zjunlp/EasyEdit</a>.</p></p class="citation"></blockquote><h3 id=2729--29122-hitting-proberty-with-non-linearity-and-more-avik-pal-et-al-2024>(27/29 | 29/122) Hitting &lsquo;Probe&rsquo;rty with Non-Linearity, and More (Avik Pal et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Avik Pal, Madhura Pawar. (2024)<br><strong>Hitting &lsquo;Probe&rsquo;rty with Non-Linearity, and More</strong><br><button class=copy-to-clipboard title="Hitting 'Probe'rty with Non-Linearity, and More" index=29>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-29 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 10<br>Keywords: BERT<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16168v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16168v1.pdf filename=2402.16168v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Structural probes learn a linear transformation to find how dependency trees are embedded in the hidden states of language models. This simple design may not allow for full exploitation of the structure of the encoded information. Hence, to investigate the structure of the encoded information to its full extent, we incorporate non-linear structural probes. We reformulate the design of non-linear structural probes introduced by White et al. making its design simpler yet effective. We also design a visualization framework that lets us qualitatively assess how strongly two words in a sentence are connected in the predicted dependency tree. We use this technique to understand which non-linear probe variant is good at encoding syntactical information. Additionally, we also use it to qualitatively investigate the structure of dependency trees that <b>BERT</b> encodes in each of its layers. We find that the radial basis function (RBF) is an effective non-linear probe for the <b>BERT</b> model than the linear probe.</p></p class="citation"></blockquote><h3 id=2829--30122-what-generative-artificial-intelligence-means-for-terminological-definitions-antonio-san-martín-2024>(28/29 | 30/122) What Generative Artificial Intelligence Means for Terminological Definitions (Antonio San Martín, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Antonio San Martín. (2024)<br><strong>What Generative Artificial Intelligence Means for Terminological Definitions</strong><br><button class=copy-to-clipboard title="What Generative Artificial Intelligence Means for Terminological Definitions" index=30>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-30 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 10<br>Keywords: ChatGPT<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16139v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16139v1.pdf filename=2402.16139v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper examines the impact of Generative Artificial Intelligence (GenAI) on the creation and consumption of terminological definitions. GenAI tools like <b>ChatGPT</b> present a mix of benefits and drawbacks compared to traditional terminological resources. <b>ChatGPT</b> excels in providing context-specific meanings in an interactive and customized fashion but faces challenges with accuracy. Terminological definitions in recognized resources will likely survive because of their reliability. From the point of view of the terminologist, tools like <b>ChatGPT</b> enable AI-assisted terminography, including post-editing terminography, as an approach blending AI efficiency with human expertise for faster definition creation.</p></p class="citation"></blockquote><h3 id=2929--31122-training-a-bilingual-language-model-by-mapping-tokens-onto-a-shared-character-space-aviad-rom-et-al-2024>(29/29 | 31/122) Training a Bilingual Language Model by Mapping Tokens onto a Shared Character Space (Aviad Rom et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Aviad Rom, Kfir Bar. (2024)<br><strong>Training a Bilingual Language Model by Mapping Tokens onto a Shared Character Space</strong><br><button class=copy-to-clipboard title="Training a Bilingual Language Model by Mapping Tokens onto a Shared Character Space" index=31>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-31 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keyword Score: 10<br>Keywords: Neural Machine Translation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16065v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16065v1.pdf filename=2402.16065v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We train a bilingual Arabic-Hebrew language model using a transliterated version of Arabic texts in Hebrew, to ensure both languages are represented in the same script. Given the morphological, structural similarities, and the extensive number of cognates shared among Arabic and Hebrew, we assess the performance of a language model that employs a unified script for both languages, on <b>machine</b> <b>translation</b> which requires cross-lingual knowledge. The results are promising: our model outperforms a contrasting model which keeps the Arabic texts in the Arabic script, demonstrating the efficacy of the transliteration step. Despite being trained on a dataset approximately 60% smaller than that of other existing language models, our model appears to deliver comparable performance in <b>machine</b> <b>translation</b> across both translation directions.</p></p class="citation"></blockquote><h2 id=cscr-7>cs.CR (7)</h2><h3 id=17--32122-drattack-prompt-decomposition-and-reconstruction-makes-powerful-llm-jailbreakers-xirui-li-et-al-2024>(1/7 | 32/122) DrAttack: Prompt Decomposition and Reconstruction Makes Powerful LLM Jailbreakers (Xirui Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xirui Li, Ruochen Wang, Minhao Cheng, Tianyi Zhou, Cho-Jui Hsieh. (2024)<br><strong>DrAttack: Prompt Decomposition and Reconstruction Makes Powerful LLM Jailbreakers</strong><br><button class=copy-to-clipboard title="DrAttack: Prompt Decomposition and Reconstruction Makes Powerful LLM Jailbreakers" index=32>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-32 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-AI, cs-CL, cs-CR, cs.CR<br>Keyword Score: 70<br>Keywords: GPT, GPT-4, In-context Learning, In-context Learning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16914v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16914v1.pdf filename=2402.16914v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The safety alignment of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> is vulnerable to both manual and automated jailbreak attacks, which adversarially trigger <b>LLMs</b> to output harmful content. However, current methods for jailbreaking <b>LLMs,</b> which nest entire harmful <b>prompts,</b> are not effective at concealing malicious intent and can be easily identified and rejected by well-aligned <b>LLMs.</b> This paper discovers that decomposing a malicious <b>prompt</b> into separated sub-prompts can effectively obscure its underlying malicious intent by presenting it in a fragmented, less detectable form, thereby addressing these limitations. We introduce an automatic <b>prompt</b> \textbf{D}ecomposition and \textbf{R}econstruction framework for jailbreak \textbf{Attack} (DrAttack). DrAttack includes three key components: (a) <code>Decomposition' of the original &lt;b>prompt&lt;/b> into sub-prompts, (b) </code>Reconstruction&rsquo; of these sub-prompts implicitly by <b>in-context</b> <b>learning</b> with semantically similar but harmless reassembling demo, and (c) a `Synonym Search&rsquo; of sub-prompts, aiming to find sub-prompts&rsquo; synonyms that maintain the original intent while jailbreaking <b>LLMs.</b> An extensive empirical study across multiple open-source and closed-source <b>LLMs</b> demonstrates that, with a significantly reduced number of queries, DrAttack obtains a substantial gain of success rate over prior SOTA <b>prompt-only</b> attackers. Notably, the success rate of 78.0% on <b>GPT-4</b> with merely 15 queries surpassed previous art by 33.1%.</p></p class="citation"></blockquote><h3 id=27--33122-attention-gan-for-anomaly-detection-a-cutting-edge-approach-to-cybersecurity-threat-management-mohammed-abo-sen-2024>(2/7 | 33/122) Attention-GAN for Anomaly Detection: A Cutting-Edge Approach to Cybersecurity Threat Management (Mohammed Abo Sen, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mohammed Abo Sen. (2024)<br><strong>Attention-GAN for Anomaly Detection: A Cutting-Edge Approach to Cybersecurity Threat Management</strong><br><button class=copy-to-clipboard title="Attention-GAN for Anomaly Detection: A Cutting-Edge Approach to Cybersecurity Threat Management" index=33>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-33 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-AI, cs-CR, cs.CR<br>Keyword Score: 53<br>Keywords: Anomaly Detection, Benchmarking, Data Augmentation, Generative Adversarial Network, Generative Adversarial Network, Knowledge Distillation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15945v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15945v2.pdf filename=2402.15945v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper proposes an innovative Attention-GAN framework for enhancing cybersecurity, focusing on <b>anomaly</b> <b>detection.</b> In response to the challenges posed by the constantly evolving nature of cyber threats, the proposed approach aims to generate diverse and realistic synthetic attack scenarios, thereby enriching the dataset and improving threat identification. Integrating attention mechanisms with <b>Generative</b> <b>Adversarial</b> <b>Networks</b> <b>(GANs)</b> is a key feature of the proposed method. The attention mechanism enhances the model&rsquo;s ability to focus on relevant features, essential for detecting subtle and complex attack patterns. In addition, <b>GANs</b> address the issue of <b>data</b> <b>scarcity</b> by generating additional varied attack <b>data,</b> <b>encompassing</b> known and emerging threats. This dual approach ensures that the system remains relevant and effective against the continuously evolving cyberattacks. The <b>KDD</b> Cup and CICIDS2017 datasets were used to validate this model, which exhibited significant improvements in <b>anomaly</b> <b>detection.</b> It achieved an accuracy of 99.69% on the <b>KDD</b> dataset and 97.93% on the CICIDS2017 dataset, with precision, recall, and F1-scores above 97%, demonstrating its effectiveness in recognizing complex attack patterns. This study contributes significantly to cybersecurity by providing a scalable and adaptable solution for <b>anomaly</b> <b>detection</b> in the face of sophisticated and dynamic cyber threats. The exploration of <b>GANs</b> for <b>data</b> <b>augmentation</b> highlights a promising direction for future research, particularly in situations where <b>data</b> <b>limitations</b> restrict the development of cybersecurity systems. The attention-GAN framework has emerged as a pioneering approach, setting a new <b>benchmark</b> for advanced cyber-defense strategies.</p></p class="citation"></blockquote><h3 id=37--34122-fedfdp-federated-learning-with-fairness-and-differential-privacy-xinpeng-ling-et-al-2024>(3/7 | 34/122) FedFDP: Federated Learning with Fairness and Differential Privacy (Xinpeng Ling et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xinpeng Ling, Jie Fu, Zhili Chen, Kuncan Wang, Huifa Li, Tong Cheng, Guanying Xu, Qin Li. (2024)<br><strong>FedFDP: Federated Learning with Fairness and Differential Privacy</strong><br><button class=copy-to-clipboard title="FedFDP: Federated Learning with Fairness and Differential Privacy" index=34>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-34 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs.CR<br>Keyword Score: 30<br>Keywords: Fairness, Federated Learning, Differential Privacy<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16028v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16028v1.pdf filename=2402.16028v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Federated</b> <b>learning</b> (FL) is a new machine learning paradigm to overcome the challenge of data silos and has garnered significant attention. However, through our observations, a globally effective trained model may performance disparities in different clients. This implies that the jointly trained models by clients may lead to unfair outcomes. On the other hand, relevant studies indicate that the transmission of gradients or models in <b>federated</b> <b>learning</b> can also give rise to privacy leakage issues, such as membership inference attacks. To address the first issue mentioned above, we propose a <b>federated</b> <b>algorithm</b> with <b>fairness,</b> termed FedFair. Building upon FedFair, we introduce privacy protection to form the FedFDP algorithm to address the second issue mentioned above. In FedFDP, we devise a <b>fairness-aware</b> clipping strategy to achieve <b>differential</b> <b>privacy</b> while adjusting <b>fairness.</b> Additionally, for the extra uploaded loss values, we present an adaptive clipping approach to maximize utility. Furthermore, we theoretically prove that our algorithm converges and ensures <b>differential</b> <b>privacy.</b> Lastly, Extensive experimental results demonstrate that FedFair and FedFDP significantly outperforms state-of-the-art solutions in terms of model performance and <b>fairness.</b> The code is accessible at <a href=https://anonymous.4open.science/r/FedFDP-E754>https://anonymous.4open.science/r/FedFDP-E754</a>.</p></p class="citation"></blockquote><h3 id=47--35122-how-to-privately-tune-hyperparameters-in-federated-learning-insights-from-a-benchmark-study-natalija-mitic-et-al-2024>(4/7 | 35/122) How to Privately Tune Hyperparameters in Federated Learning? Insights from a Benchmark Study (Natalija Mitic et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Natalija Mitic, Apostolos Pyrgelis, Sinem Sav. (2024)<br><strong>How to Privately Tune Hyperparameters in Federated Learning? Insights from a Benchmark Study</strong><br><button class=copy-to-clipboard title="How to Privately Tune Hyperparameters in Federated Learning? Insights from a Benchmark Study" index=35>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-35 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs.CR<br>Keyword Score: 16<br>Keywords: Benchmarking, Clustering, Federated Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16087v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16087v1.pdf filename=2402.16087v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we address the problem of privacy-preserving hyperparameter (HP) tuning for cross-silo <b>federated</b> <b>learning</b> (FL). We first perform a comprehensive measurement study that <b>benchmarks</b> various HP strategies suitable for FL. Our <b>benchmarks</b> show that the optimal parameters of the FL server, e.g., the learning rate, can be accurately and efficiently tuned based on the HPs found by each client on its local data. We demonstrate that HP averaging is suitable for iid settings, while density-based <b>clustering</b> can uncover the optimal set of parameters in non-iid ones. Then, to prevent information leakage from the exchange of the clients&rsquo; local HPs, we design and implement PrivTuna, a novel framework for privacy-preserving HP tuning using multiparty homomorphic encryption. We use PrivTuna to implement privacy-preserving <b>federated</b> <b>averaging</b> and density-based <b>clustering,</b> and we experimentally evaluate its performance demonstrating its computation/communication efficiency and its precision in tuning hyperparameters.</p></p class="citation"></blockquote><h3 id=57--36122-attacking-llm-watermarks-by-exploiting-their-strengths-qi-pang-et-al-2024>(5/7 | 36/122) Attacking LLM Watermarks by Exploiting Their Strengths (Qi Pang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qi Pang, Shengyuan Hu, Wenting Zheng, Virginia Smith. (2024)<br><strong>Attacking LLM Watermarks by Exploiting Their Strengths</strong><br><button class=copy-to-clipboard title="Attacking LLM Watermarks by Exploiting Their Strengths" index=36>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-36 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CL, cs-CR, cs-LG, cs.CR<br>Keyword Score: 10<br>Keywords: Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16187v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16187v1.pdf filename=2402.16187v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Advances in generative models have made it possible for AI-generated text, code, and images to mirror human-generated content in many applications. Watermarking, a technique that aims to embed information in the output of a model to verify its source, is useful for mitigating misuse of such AI-generated content. However, existing watermarking schemes remain surprisingly susceptible to attack. In particular, we show that desirable properties shared by existing <b>LLM</b> watermarking systems such as quality preservation, robustness, and public detection APIs can in turn make these systems vulnerable to various attacks. We rigorously study potential attacks in terms of common watermark design choices, and propose best practices and defenses for mitigation &ndash; establishing a set of practical guidelines for embedding and detection of <b>LLM</b> watermarks.</p></p class="citation"></blockquote><h3 id=67--37122-luataint-a-static-taint-analysis-system-for-web-interface-framework-vulnerability-of-iot-devices-jiahui-xiang-et-al-2024>(6/7 | 37/122) LuaTaint: A Static Taint Analysis System for Web Interface Framework Vulnerability of IoT Devices (Jiahui Xiang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiahui Xiang, Wenhai Wang, Tong Ye, Peiyu Liu. (2024)<br><strong>LuaTaint: A Static Taint Analysis System for Web Interface Framework Vulnerability of IoT Devices</strong><br><button class=copy-to-clipboard title="LuaTaint: A Static Taint Analysis System for Web Interface Framework Vulnerability of IoT Devices" index=37>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-37 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs-SE, cs.CR<br>Keyword Score: 10<br>Keywords: Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16043v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16043v1.pdf filename=2402.16043v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>IoT devices are currently facing continuous malicious attacks due to their widespread use. Among these IoT devices, web vulnerabilities are also widely exploited because of their inherent characteristics, such as improper permission controls and insecure interfaces. Recently, the embedded system web interface framework has become highly diverse, and specific vulnerabilities can arise if developers forget to detect user input parameters or if the detection process is not strict enough. Therefore, discovering vulnerabilities in the web interfaces of IoT devices accurately and comprehensively through an automated method is a major challenge. This paper aims to work out the challenge. We have developed an automated vulnerability detection system called LuaTaint for the typical web interface framework, LuCI. The system employs static taint analysis to address web security issues on mobile terminal platforms to ensure detection coverage. It integrates rules pertaining to page handler control logic within the taint detection process to improve its extensibility. We also implemented a post-processing step with the assistance of <b>large</b> <b>language</b> <b>models</b> to enhance accuracy and reduce the need for manual analysis. We have created a prototype of LuaTaint and tested it on 92 IoT firmwares from 8 well-known vendors. LuaTaint has discovered 68 unknown vulnerabilities.</p></p class="citation"></blockquote><h3 id=77--38122-an-adversarial-robustness-benchmark-for-enterprise-network-intrusion-detection-joão-vitorino-et-al-2024>(7/7 | 38/122) An Adversarial Robustness Benchmark for Enterprise Network Intrusion Detection (João Vitorino et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>João Vitorino, Miguel Silva, Eva Maia, Isabel Praça. (2024)<br><strong>An Adversarial Robustness Benchmark for Enterprise Network Intrusion Detection</strong><br><button class=copy-to-clipboard title="An Adversarial Robustness Benchmark for Enterprise Network Intrusion Detection" index=38>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-38 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs-LG, cs.CR<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16912v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16912v1.pdf filename=2402.16912v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>As cyber-attacks become more sophisticated, improving the robustness of Machine Learning (ML) models must be a priority for enterprises of all sizes. To reliably compare the robustness of different ML models for cyber-attack detection in enterprise computer networks, they must be evaluated in standardized conditions. This work presents a methodical adversarial robustness <b>benchmark</b> of multiple decision tree ensembles with constrained adversarial examples generated from standard datasets. The robustness of regularly and adversarially trained RF, XGB, LGBM, and EBM models was evaluated on the original CICIDS2017 dataset, a corrected version of it designated as NewCICIDS, and the HIKARI dataset, which contains more recent network traffic. NewCICIDS led to models with a better performance, especially XGB and EBM, but RF and LGBM were less robust against the more recent cyber-attacks of HIKARI. Overall, the robustness of the models to adversarial cyber-attack examples was improved without their generalization to regular traffic being affected, enabling a reliable detection of suspicious activity without costly increases of false alarms.</p></p class="citation"></blockquote><h2 id=csir-3>cs.IR (3)</h2><h3 id=13--39122-disentangled-graph-variational-auto-encoder-for-multimodal-recommendation-with-interpretability-xin-zhou-et-al-2024>(1/3 | 39/122) Disentangled Graph Variational Auto-Encoder for Multimodal Recommendation with Interpretability (Xin Zhou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xin Zhou, Chunyan Miao. (2024)<br><strong>Disentangled Graph Variational Auto-Encoder for Multimodal Recommendation with Interpretability</strong><br><button class=copy-to-clipboard title="Disentangled Graph Variational Auto-Encoder for Multimodal Recommendation with Interpretability" index=39>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-39 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs-MM, cs.IR<br>Keyword Score: 69<br>Keywords: Graph Convolutional Network, Graph, Convolution, Convolutional Neural Network, Multi-modal, Multi-modal, Mutual Information, Recommendation, Recommender System<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16110v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16110v1.pdf filename=2402.16110v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Multimodal</b> <b>recommender</b> <b>systems</b> amalgamate <b>multimodal</b> information (e.g., textual descriptions, images) into a collaborative filtering framework to provide more accurate <b>recommendations.</b> While the incorporation of <b>multimodal</b> information could enhance the interpretability of these systems, current <b>multimodal</b> models represent users and items utilizing entangled numerical vectors, rendering them arduous to interpret. To address this, we propose a Disentangled <b>Graph</b> <b>Variational</b> <b>Auto-Encoder</b> (DGVAE) that aims to enhance both model and <b>recommendation</b> interpretability. DGVAE initially projects <b>multimodal</b> information into textual contents, such as converting images to text, by harnessing state-of-the-art <b>multimodal</b> pre-training technologies. It then constructs a frozen item-item <b>graph</b> <b>and</b> <b>encodes</b> the contents and interactions into two sets of disentangled representations utilizing a simplified residual <b>graph</b> <b>convolutional</b> <b>network.</b> DGVAE further regularizes these disentangled representations through <b>mutual</b> <b>information</b> maximization, aligning the representations derived from the interactions between users and items with those learned from textual content. This alignment facilitates the interpretation of user binary interactions via text. Our empirical analysis conducted on three real-world datasets demonstrates that DGVAE significantly surpasses the performance of state-of-the-art baselines by a margin of 10.02%. We also furnish a case study from a real-world dataset to illustrate the interpretability of DGVAE. Code is available at: \url{https://github.com/enoche/DGVAE}.</p></p class="citation"></blockquote><h3 id=23--40122-ir2-information-regularization-for-information-retrieval-jianyou-wang-et-al-2024>(2/3 | 40/122) IR2: Information Regularization for Information Retrieval (Jianyou Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jianyou Wang, Kaicheng Wang, Xiaoyue Wang, Weili Cao, Ramamohan Paturi, Leon Bergen. (2024)<br><strong>IR2: Information Regularization for Information Retrieval</strong><br><button class=copy-to-clipboard title="IR2: Information Regularization for Information Retrieval" index=40>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-40 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-AI, cs-CL, cs-IR, cs-LG, cs.IR<br>Keyword Score: 20<br>Keywords: Information Retrieval, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16200v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16200v1.pdf filename=2402.16200v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Effective <b>information</b> <b>retrieval</b> (IR) in settings with limited training data, particularly for complex queries, remains a challenging task. This paper introduces IR2, <b>Information</b> <b>Regularization</b> for <b>Information</b> <b>Retrieval,</b> a technique for reducing overfitting during synthetic data generation. This approach, representing a novel application of regularization techniques in synthetic data creation for IR, is tested on three recent IR tasks characterized by complex queries: DORIS-MAE, ArguAna, and WhatsThatBook. Experimental results indicate that our regularization techniques not only outperform previous synthetic query generation methods on the tasks considered but also reduce cost by up to 50%. Furthermore, this paper categorizes and explores three regularization methods at different stages of the query synthesis pipeline-input, <b>prompt,</b> and output-each offering varying degrees of performance improvement compared to models where no regularization is applied. This provides a systematic approach for optimizing synthetic data generation in data-limited, complex-query IR scenarios. All code, <b>prompts</b> and synthetic data are available at <a href=https://github.com/Info-Regularization/Information-Regularization>https://github.com/Info-Regularization/Information-Regularization</a>.</p></p class="citation"></blockquote><h3 id=33--41122-pfeed-generating-near-real-time-personalized-feeds-using-precomputed-embedding-similarities-binyam-gebre-et-al-2024>(3/3 | 41/122) Pfeed: Generating near real-time personalized feeds using precomputed embedding similarities (Binyam Gebre et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Binyam Gebre, Karoliina Ranta, Stef van den Elzen, Ernst Kuiper, Thijs Baars, Tom Heskes. (2024)<br><strong>Pfeed: Generating near real-time personalized feeds using precomputed embedding similarities</strong><br><button class=copy-to-clipboard title="Pfeed: Generating near real-time personalized feeds using precomputed embedding similarities" index=41>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-41 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: H-3-3, cs-AI, cs-IR, cs-LG, cs.IR<br>Keyword Score: 10<br>Keywords: Recommender System<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16073v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16073v1.pdf filename=2402.16073v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In personalized <b>recommender</b> <b>systems,</b> embeddings are often used to encode customer actions and items, and retrieval is then performed in the embedding space using approximate nearest neighbor search. However, this approach can lead to two challenges: 1) user embeddings can restrict the diversity of interests captured and 2) the need to keep them up-to-date requires an expensive, real-time infrastructure. In this paper, we propose a method that overcomes these challenges in a practical, industrial setting. The method dynamically updates customer profiles and composes a feed every two minutes, employing precomputed embeddings and their respective similarities. We tested and deployed this method to personalise promotional items at Bol, one of the largest e-commerce platforms of the Netherlands and Belgium. The method enhanced customer engagement and experience, leading to a significant 4.9% uplift in conversions.</p></p class="citation"></blockquote><h2 id=cslg-25>cs.LG (25)</h2><h3 id=125--42122-building-flexible-machine-learning-models-for-scientific-computing-at-scale-tianyu-chen-et-al-2024>(1/25 | 42/122) Building Flexible Machine Learning Models for Scientific Computing at Scale (Tianyu Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tianyu Chen, Haoyi Zhou, Ying Li, Hao Wang, Chonghan Gao, Shanghang Zhang, Jianxin Li. (2024)<br><strong>Building Flexible Machine Learning Models for Scientific Computing at Scale</strong><br><button class=copy-to-clipboard title="Building Flexible Machine Learning Models for Scientific Computing at Scale" index=42>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-42 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 63<br>Keywords: Benchmarking, Few-shot, Fine-tuning, Foundation Model, Reinforcement Learning, Zero-shot, Zero-shot Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16014v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16014v1.pdf filename=2402.16014v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Foundation</b> <b>models</b> have revolutionized knowledge acquisition across domains, and our study introduces OmniArch, a paradigm-shifting approach designed for building <b>foundation</b> <b>models</b> in multi-physics scientific computing. OmniArch&rsquo;s pre-training involves a versatile pipeline that processes multi-physics spatio-temporal data, casting forward problem learning into scalable auto-regressive tasks, while our novel Physics-Informed <b>Reinforcement</b> <b>Learning</b> (PIRL) technique during <b>fine-tuning</b> ensures alignment with physical laws. Pre-trained on the comprehensive PDEBench dataset, OmniArch not only sets new performance <b>benchmarks</b> for 1D, 2D and 3D PDEs but also demonstrates exceptional adaptability to new physics via <b>few-shot</b> and <b>zero-shot</b> <b>learning</b> approaches. The model&rsquo;s representations further extend to inverse problem-solving, highlighting the transformative potential of AI-enabled Scientific Computing(AI4SC) <b>foundation</b> <b>models</b> for engineering applications and physics discovery.</p></p class="citation"></blockquote><h3 id=225--43122-deep-contrastive-graph-learning-with-clustering-oriented-guidance-mulin-chen-et-al-2024>(2/25 | 43/122) Deep Contrastive Graph Learning with Clustering-Oriented Guidance (Mulin Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mulin Chen, Bocheng Wang, Xuelong Li. (2024)<br><strong>Deep Contrastive Graph Learning with Clustering-Oriented Guidance</strong><br><button class=copy-to-clipboard title="Deep Contrastive Graph Learning with Clustering-Oriented Guidance" index=43>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-43 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 59<br>Keywords: Graph Convolutional Network, Graph Convolutional Network, Graph, Benchmarking, Clustering, Contrastive Learning, Convolution, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16012v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16012v1.pdf filename=2402.16012v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Graph</b> <b>Convolutional</b> <b>Network</b> <b>(GCN)</b> has exhibited remarkable potential in improving <b>graph-based</b> <b>clustering.</b> <b>To</b> handle the general <b>clustering</b> scenario without a prior <b>graph,</b> <b>these</b> <b>models</b> estimate an initial <b>graph</b> <b>beforehand</b> <b>to</b> apply <b>GCN.</b> Throughout the literature, we have witnessed that 1) most models focus on the initial <b>graph</b> <b>while</b> <b>neglecting</b> the original features. Therefore, the discriminability of the learned representation may be corrupted by a low-quality initial <b>graph;</b> <b>2)</b> <b>the</b> training procedure lacks effective <b>clustering</b> guidance, which may lead to the incorporation of <b>clustering-irrelevant</b> information into the learned <b>graph.</b> <b>To</b> <b>tackle</b> these problems, the Deep <b>Contrastive</b> <b>Graph</b> <b>Learning</b> <b>(DCGL)</b> model is proposed for general data <b>clustering.</b> Specifically, we establish a pseudo-siamese network, which incorporates auto-encoder with <b>GCN</b> to emphasize both the <b>graph</b> <b>structure</b> <b>and</b> the original features. On this basis, feature-level <b>contrastive</b> <b>learning</b> is introduced to enhance the discriminative capacity, and the relationship between samples and centroids is employed as the <b>clustering-oriented</b> guidance. Afterward, a two-branch <b>graph</b> <b>learning</b> <b>mechanism</b> is designed to extract the local and global structural relationships, which are further embedded into a unified <b>graph</b> <b>under</b> <b>the</b> cluster-level <b>contrastive</b> <b>guidance.</b> Experimental results on several <b>benchmark</b> datasets demonstrate the superiority of DCGL against state-of-the-art algorithms.</p></p class="citation"></blockquote><h3 id=325--44122-deepforge-leveraging-ai-for-microstructural-control-in-metal-forming-via-model-predictive-control-jan-petrik-et-al-2024>(3/25 | 44/122) DeepForge: Leveraging AI for Microstructural Control in Metal Forming via Model Predictive Control (Jan Petrik et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jan Petrik, Markus Bambach. (2024)<br><strong>DeepForge: Leveraging AI for Microstructural Control in Metal Forming via Model Predictive Control</strong><br><button class=copy-to-clipboard title="DeepForge: Leveraging AI for Microstructural Control in Metal Forming via Model Predictive Control" index=44>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-44 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs-SY, cs.LG, eess-SY<br>Keyword Score: 50<br>Keywords: Graph Attention Networks, Convolution, Convolutional Neural Network, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16119v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16119v1.pdf filename=2402.16119v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This study presents a novel method for microstructure control in closed die hot forging that combines Model Predictive Control (MPC) with a developed machine learning model called DeepForge. DeepForge uses an architecture that combines 1D <b>convolutional</b> <b>neural</b> <b>networks</b> and <b>gated</b> recurrent units. It uses surface temperature measurements of a workpiece as input to predict microstructure changes during forging. The paper also details DeepForge&rsquo;s architecture and the finite element <b>simulation</b> model used to generate the data set, using a three-stroke forging process. The results demonstrate DeepForge&rsquo;s ability to predict microstructure with a mean absolute error of 0.4$\pm$0.3%. In addition, the study explores the use of MPC to adjust inter-stroke wait times, effectively counteracting temperature disturbances to achieve a target grain size of less than 35 microns within a specific 2D region of the workpiece. These results are then verified experimentally, demonstrating a significant step towards improved control and quality in forging processes where temperature can be used as an additional degree of freedom in the process.</p></p class="citation"></blockquote><h3 id=425--45122-structural-knowledge-driven-meta-learning-for-task-offloading-in-vehicular-networks-with-integrated-communications-sensing-and-computing-ruijin-sun-et-al-2024>(4/25 | 45/122) Structural Knowledge-Driven Meta-Learning for Task Offloading in Vehicular Networks with Integrated Communications, Sensing and Computing (Ruijin Sun et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ruijin Sun, Yao Wen, Nan Cheng, Wei Wan, Rong Chai, Yilong Hui. (2024)<br><strong>Structural Knowledge-Driven Meta-Learning for Task Offloading in Vehicular Networks with Integrated Communications, Sensing and Computing</strong><br><button class=copy-to-clipboard title="Structural Knowledge-Driven Meta-Learning for Task Offloading in Vehicular Networks with Integrated Communications, Sensing and Computing" index=45>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-45 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs-NI, cs.LG<br>Keyword Score: 40<br>Keywords: Meta Learning, LSTM, LSTM, LSTM<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15972v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15972v1.pdf filename=2402.15972v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Task offloading is a potential solution to satisfy the strict requirements of computation-intensive and latency-sensitive vehicular applications due to the limited onboard computing resources. However, the overwhelming upload traffic may lead to unacceptable uploading time. To tackle this issue, for tasks taking environmental data as input, the data perceived by roadside units (RSU) equipped with several sensors can be directly exploited for computation, resulting in a novel task offloading paradigm with integrated communications, sensing and computing (I-CSC). With this paradigm, vehicles can select to upload their sensed data to RSUs or transmit computing instructions to RSUs during the offloading. By optimizing the computation mode and network resources, in this paper, we investigate an I-CSC-based task offloading problem to reduce the cost caused by resource consumption while guaranteeing the latency of each task. Although this non-convex problem can be handled by the alternating minimization (AM) algorithm that alternatively minimizes the divided four sub-problems, it leads to high computational complexity and local optimal solution. To tackle this challenge, we propose a creative structural knowledge-driven <b>meta-learning</b> <b>(SKDML)</b> method, involving both the model-based AM algorithm and neural networks. Specifically, borrowing the iterative structure of the AM algorithm, also referred to as structural knowledge, the proposed SKDML adopts <b>long</b> <b>short-term</b> <b>memory</b> <b>(LSTM)</b> network-based <b>meta-learning</b> <b>to</b> learn an adaptive optimizer for updating variables in each sub-problem, instead of the handcrafted counterpart in the AM algorithm.</p></p class="citation"></blockquote><h3 id=525--46122-hierarchical-energy-signatures-using-machine-learning-for-operational-visibility-and-diagnostics-in-automotive-manufacturing-ankur-verma-et-al-2024>(5/25 | 46/122) Hierarchical energy signatures using machine learning for operational visibility and diagnostics in automotive manufacturing (Ankur Verma et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ankur Verma, Seog-Chan Oh, Jorge Arinez, Soundar Kumara. (2024)<br><strong>Hierarchical energy signatures using machine learning for operational visibility and diagnostics in automotive manufacturing</strong><br><button class=copy-to-clipboard title="Hierarchical energy signatures using machine learning for operational visibility and diagnostics in automotive manufacturing" index=46>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-46 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 40<br>Keywords: Convolution, Convolutional Neural Network, Convolutional Neural Network, Logistic Regression<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15962v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15962v1.pdf filename=2402.15962v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Manufacturing energy consumption data contains important process signatures required for operational visibility and diagnostics. These signatures may be of different temporal scales, ranging from monthly to sub-second resolutions. We introduce a hierarchical machine learning approach to identify automotive process signatures from paint shop electricity consumption data at varying temporal scales (weekly and daily). A Multi-Layer Perceptron (MLP), a <b>Convolutional</b> <b>Neural</b> <b>Network</b> <b>(CNN),</b> and Principal Component Analysis (PCA) combined with <b>Logistic</b> <b>Regression</b> (LR) are used for the analysis. We validate the utility of the developed algorithms with subject matter experts for (i) better operational visibility, and (ii) identifying energy saving opportunities.</p></p class="citation"></blockquote><h3 id=625--47122-how-can-llm-guide-rl-a-value-based-approach-shenao-zhang-et-al-2024>(6/25 | 47/122) How Can LLM Guide RL? A Value-Based Approach (Shenao Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shenao Zhang, Sirui Zheng, Shuqi Ke, Zhihan Liu, Wanxin Jin, Jianbo Yuan, Yingxiang Yang, Hongxia Yang, Zhaoran Wang. (2024)<br><strong>How Can LLM Guide RL? A Value-Based Approach</strong><br><button class=copy-to-clipboard title="How Can LLM Guide RL? A Value-Based Approach" index=47>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-47 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Reinforcement Learning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16181v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16181v1.pdf filename=2402.16181v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Reinforcement</b> <b>learning</b> (RL) has become the de facto standard practice for sequential decision-making problems by improving future acting policies with feedback. However, RL algorithms may require extensive trial-and-error interactions to collect useful feedback for improvement. On the other hand, recent developments in <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have showcased impressive capabilities in language understanding and generation, yet they fall short in exploration and self-improvement capabilities for planning tasks, lacking the ability to autonomously refine their responses based on feedback. Therefore, in this paper, we study how the policy prior provided by the <b>LLM</b> can enhance the sample efficiency of RL algorithms. Specifically, we develop an algorithm named LINVIT that incorporates <b>LLM</b> guidance as a regularization factor in value-based RL, leading to significant reductions in the amount of data needed for learning, particularly when the difference between the ideal policy and the <b>LLM-informed</b> policy is small, which suggests that the initial policy is close to optimal, reducing the need for further exploration. Additionally, we present a practical algorithm SLINVIT that simplifies the construction of the value function and employs subgoals to reduce the search complexity. Our experiments across three interactive environments ALFWorld, InterCode, and BlocksWorld demonstrate that our method achieves state-of-the-art success rates and also surpasses previous RL and <b>LLM</b> approaches in terms of sample efficiency. Our code is available at <a href=https://github.com/agentification/Language-Integrated-VI>https://github.com/agentification/Language-Integrated-VI</a>.</p></p class="citation"></blockquote><h3 id=725--48122-trustworthy-personalized-bayesian-federated-learning-via-posterior-fine-tune-mengen-luo-et-al-2024>(7/25 | 48/122) Trustworthy Personalized Bayesian Federated Learning via Posterior Fine-Tune (Mengen Luo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mengen Luo, Chi Xu, Ercan Engin Kuruoglu. (2024)<br><strong>Trustworthy Personalized Bayesian Federated Learning via Posterior Fine-Tune</strong><br><button class=copy-to-clipboard title="Trustworthy Personalized Bayesian Federated Learning via Posterior Fine-Tune" index=48>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-48 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CV, cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Federated Learning, Fine-tuning, Out-of-distribution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16911v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16911v1.pdf filename=2402.16911v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Performance degradation owing to data heterogeneity and low output interpretability are the most significant challenges faced by <b>federated</b> <b>learning</b> in practical applications. Personalized <b>federated</b> <b>learning</b> diverges from traditional approaches, as it no longer seeks to train a single model, but instead tailors a unique personalized model for each client. However, previous work focused only on personalization from the perspective of neural network parameters and lack of robustness and interpretability. In this work, we establish a novel framework for personalized <b>federated</b> <b>learning,</b> incorporating Bayesian methodology which enhances the algorithm&rsquo;s ability to quantify uncertainty. Furthermore, we introduce normalizing flow to achieve personalization from the parameter posterior perspective and theoretically analyze the impact of normalizing flow on <b>out-of-distribution</b> (OOD) detection for Bayesian neural networks. Finally, we evaluated our approach on heterogeneous datasets, and the experimental results indicate that the new algorithm not only improves accuracy but also outperforms the baseline significantly in OOD detection due to the reliable output of the Bayesian approach.</p></p class="citation"></blockquote><h3 id=825--49122-more-than-routing-joint-gps-and-route-modeling-for-refine-trajectory-representation-learning-zhipeng-ma-et-al-2024>(8/25 | 49/122) More Than Routing: Joint GPS and Route Modeling for Refine Trajectory Representation Learning (Zhipeng Ma et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhipeng Ma, Zheyan Tu, Xinhai Chen, Yan Zhang, Deguo Xia, Guyue Zhou, Yilun Chen, Yu Zheng, Jiangtao Gong. (2024)<br><strong>More Than Routing: Joint GPS and Route Modeling for Refine Trajectory Representation Learning</strong><br><button class=copy-to-clipboard title="More Than Routing: Joint GPS and Route Modeling for Refine Trajectory Representation Learning" index=49>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-49 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 25<br>Keywords: Representation Learning, Self-supervised Learning, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16915v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16915v1.pdf filename=2402.16915v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Trajectory <b>representation</b> <b>learning</b> plays a pivotal role in supporting various downstream tasks. Traditional methods in order to filter the noise in GPS trajectories tend to focus on routing-based methods used to simplify the trajectories. However, this approach ignores the motion details contained in the GPS data, limiting the <b>representation</b> <b>capability</b> of trajectory <b>representation</b> <b>learning.</b> To fill this gap, we propose a novel <b>representation</b> <b>learning</b> framework that Joint GPS and Route Modelling based on <b>self-supervised</b> technology, namely JGRM. We consider GPS trajectory and route as the two modes of a single movement observation and fuse information through inter-modal information interaction. Specifically, we develop two encoders, each tailored to capture <b>representations</b> <b>of</b> route and GPS trajectories respectively. The <b>representations</b> <b>from</b> the two modalities are fed into a shared <b>transformer</b> for inter-modal information interaction. Eventually, we design three <b>self-supervised</b> tasks to train the model. We validate the effectiveness of the proposed method on two real datasets based on extensive experiments. The experimental results demonstrate that JGRM outperforms existing methods in both road segment <b>representation</b> <b>and</b> trajectory <b>representation</b> <b>tasks.</b> Our source code is available at Anonymous Github.</p></p class="citation"></blockquote><h3 id=925--50122-a-vae-based-framework-for-learning-multi-level-neural-granger-causal-connectivity-jiahe-lin-et-al-2024>(9/25 | 50/122) A VAE-based Framework for Learning Multi-Level Neural Granger-Causal Connectivity (Jiahe Lin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiahe Lin, Huitian Lei, George Michailidis. (2024)<br><strong>A VAE-based Framework for Learning Multi-Level Neural Granger-Causal Connectivity</strong><br><button class=copy-to-clipboard title="A VAE-based Framework for Learning Multi-Level Neural Granger-Causal Connectivity" index=50>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-50 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-ME, stat-ML<br>Keyword Score: 23<br>Keywords: Autoencoder, Benchmarking, Variational Autoencoder<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16131v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16131v1.pdf filename=2402.16131v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Granger causality has been widely used in various application domains to capture lead-lag relationships amongst the components of complex dynamical systems, and the focus in extant literature has been on a single dynamical system. In certain applications in macroeconomics and neuroscience, one has access to data from a collection of related such systems, wherein the modeling task of interest is to extract the shared common structure that is embedded across them, as well as to identify the idiosyncrasies within individual ones. This paper introduces a <b>Variational</b> <b>Autoencoder</b> (VAE) based framework that jointly learns Granger-causal relationships amongst components in a collection of related-yet-heterogeneous dynamical systems, and handles the aforementioned task in a principled way. The performance of the proposed framework is evaluated on several synthetic data settings and <b>benchmarked</b> against existing approaches designed for individual system learning. The method is further illustrated on a real dataset involving time series data from a neurophysiological experiment and produces interpretable results.</p></p class="citation"></blockquote><h3 id=1025--51122-combining-machine-learning-with-computational-fluid-dynamics-using-openfoam-and-smartsim-tomislav-maric-et-al-2024>(10/25 | 51/122) Combining Machine Learning with Computational Fluid Dynamics using OpenFOAM and SmartSim (Tomislav Maric et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tomislav Maric, Mohammed Elwardi Fadeli, Alessandro Rigazzi, Andrew Shao, Andre Weiner. (2024)<br><strong>Combining Machine Learning with Computational Fluid Dynamics using OpenFOAM and SmartSim</strong><br><button class=copy-to-clipboard title="Combining Machine Learning with Computational Fluid Dynamics using OpenFOAM and SmartSim" index=51>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-51 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, physics-flu-dyn<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16196v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16196v1.pdf filename=2402.16196v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Combining machine learning (ML) with computational fluid dynamics (CFD) opens many possibilities for improving <b>simulations</b> of technical and natural systems. However, CFD+ML algorithms require exchange of data, synchronization, and calculation on heterogeneous hardware, making their implementation for large-scale problems exceptionally challenging. We provide an effective and scalable solution to developing CFD+ML algorithms using open source software OpenFOAM and SmartSim. SmartSim provides an Orchestrator that significantly simplifies the programming of CFD+ML algorithms and a Redis database that ensures highly scalable data exchange between ML and CFD clients. We show how to leverage SmartSim to effectively couple different segments of OpenFOAM with ML, including pre/post-processing applications, solvers, function objects, and mesh motion solvers. We additionally provide an OpenFOAM sub-module with examples that can be used as starting points for real-world applications in CFD+ML.</p></p class="citation"></blockquote><h3 id=1125--52122-consensus-learning-a-novel-decentralised-ensemble-learning-paradigm-horia-magureanu-et-al-2024>(11/25 | 52/122) Consensus learning: A novel decentralised ensemble learning paradigm (Horia Magureanu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Horia Magureanu, Naïri Usher. (2024)<br><strong>Consensus learning: A novel decentralised ensemble learning paradigm</strong><br><button class=copy-to-clipboard title="Consensus learning: A novel decentralised ensemble learning paradigm" index=52>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-52 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-DC, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16157v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16157v1.pdf filename=2402.16157v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The widespread adoption of large-scale machine learning models in recent years highlights the need for distributed computing for efficiency and scalability. This work introduces a novel distributed machine learning paradigm &ndash; \emph{consensus learning} &ndash; which combines classical ensemble methods with consensus protocols deployed in peer-to-peer systems. These algorithms consist of two phases: first, participants develop their models and submit predictions for any new data inputs; second, the individual predictions are used as inputs for a communication phase, which is governed by a consensus protocol. Consensus learning ensures user data privacy, while also inheriting the safety measures against Byzantine attacks from the underlying consensus mechanism. We provide a detailed theoretical analysis for a particular consensus protocol and compare the performance of the consensus learning ensemble with centralised ensemble learning algorithms. The discussion is supplemented by various numerical <b>simulations,</b> which describe the robustness of the algorithms against Byzantine participants.</p></p class="citation"></blockquote><h3 id=1225--53122-informed-meta-learning-katarzyna-kobalczyk-et-al-2024>(12/25 | 53/122) Informed Meta-Learning (Katarzyna Kobalczyk et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Katarzyna Kobalczyk, Mihaela van der Schaar. (2024)<br><strong>Informed Meta-Learning</strong><br><button class=copy-to-clipboard title="Informed Meta-Learning" index=53>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-53 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Distribution Shift, Distribution Shift, Meta Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16105v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16105v1.pdf filename=2402.16105v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In noisy and low-data regimes prevalent in real-world applications, an outstanding challenge of machine learning lies in effectively incorporating inductive biases that promote data efficiency and robustness. <b>Meta-learning</b> <b>and</b> informed ML stand out as two approaches for incorporating prior knowledge into the ML pipeline. While the former relies on a purely data-driven source of priors, the latter is guided by a formal representation of expert knowledge. This paper introduces a novel hybrid paradigm, informed <b>meta-learning,</b> <b>seeking</b> complementarity in cross-task knowledge sharing of humans and machines. We establish the foundational components of informed <b>meta-learning</b> <b>and</b> present a concrete instantiation of this framework&ndash;the Informed Neural Process. Through a series of illustrative and larger-scale experiments, we demonstrate the potential benefits of informed <b>meta-learning</b> <b>in</b> improving data efficiency and robustness to observational noise, task <b>distribution</b> <b>shifts,</b> and heterogeneity.</p></p class="citation"></blockquote><h3 id=1325--54122-a-unified-fourier-slice-method-to-derive-ridgelet-transform-for-a-variety-of-depth-2-neural-networks-sho-sonoda-et-al-2024>(13/25 | 54/122) A unified Fourier slice method to derive ridgelet transform for a variety of depth-2 neural networks (Sho Sonoda et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sho Sonoda, Isao Ishikawa, Masahiro Ikeda. (2024)<br><strong>A unified Fourier slice method to derive ridgelet transform for a variety of depth-2 neural networks</strong><br><button class=copy-to-clipboard title="A unified Fourier slice method to derive ridgelet transform for a variety of depth-2 neural networks" index=54>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-54 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, math-FA, stat-ML<br>Keyword Score: 20<br>Keywords: Convolution, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15984v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15984v1.pdf filename=2402.15984v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>To investigate neural network parameters, it is easier to study the distribution of parameters than to study the parameters in each neuron. The ridgelet transform is a pseudo-inverse operator that maps a given function $f$ to the parameter distribution $\gamma$ so that a network $\mathtt{NN}[\gamma]$ reproduces $f$, i.e. $\mathtt{NN}[\gamma]=f$. For depth-2 fully-connected networks on a Euclidean space, the ridgelet transform has been discovered up to the closed-form expression, thus we could describe how the parameters are distributed. However, for a variety of modern neural network architectures, the closed-form expression has not been known. In this paper, we explain a systematic method using Fourier expressions to derive ridgelet transforms for a variety of modern networks such as networks on finite fields $\mathbb{F}_p$, group <b>convolutional</b> <b>networks</b> on abstract Hilbert space $\mathcal{H}$, fully-connected networks on noncompact symmetric spaces $G/K$, and pooling layers, or the $d$-plane ridgelet transform.</p></p class="citation"></blockquote><h3 id=1425--55122-behavioral-refinement-via-interpolant-based-policy-diffusion-kaiqi-chen-et-al-2024>(14/25 | 55/122) Behavioral Refinement via Interpolant-based Policy Diffusion (Kaiqi Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kaiqi Chen, Eugene Lim, Kelvin Lin, Yiyang Chen, Harold Soh. (2024)<br><strong>Behavioral Refinement via Interpolant-based Policy Diffusion</strong><br><button class=copy-to-clipboard title="Behavioral Refinement via Interpolant-based Policy Diffusion" index=55>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-55 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs-RO, cs.LG<br>Keyword Score: 19<br>Keywords: Diffusion Model, Benchmarking, Multi-modal, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16075v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16075v1.pdf filename=2402.16075v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Imitation learning empowers artificial agents to mimic behavior by learning from demonstrations. Recently, <b>diffusion</b> <b>models,</b> which have the ability to model high-dimensional and <b>multimodal</b> distributions, have shown impressive performance on imitation learning tasks. These models learn to shape a policy by diffusing actions (or states) from standard Gaussian noise. However, the target policy to be learned is often significantly different from Gaussian and this mismatch can result in poor performance when using a small number of <b>diffusion</b> <b>steps</b> (to improve inference speed) and under limited data. The key idea in this work is that initiating from a more informative source than Gaussian enables <b>diffusion</b> <b>methods</b> to overcome the above limitations. We contribute both theoretical results, a new method, and empirical findings that show the benefits of using an informative source policy. Our method, which we call BRIDGER, leverages the stochastic interpolants framework to bridge arbitrary policies, thus enabling a flexible approach towards imitation learning. It generalizes prior work in that standard Gaussians can still be applied, but other source policies can be used if available. In experiments on challenging <b>benchmarks,</b> BRIDGER outperforms state-of-the-art <b>diffusion</b> <b>policies</b> and we provide further analysis on design considerations when applying BRIDGER.</p></p class="citation"></blockquote><h3 id=1525--56122-beyond-spatio-temporal-representations-evolving-fourier-transform-for-temporal-graphs-anson-bastos-et-al-2024>(15/25 | 56/122) Beyond Spatio-Temporal Representations: Evolving Fourier Transform for Temporal Graphs (Anson Bastos et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Anson Bastos, Kuldeep Singh, Abhishek Nadgeri, Manish Singh, Toyotaro Suzumura. (2024)<br><strong>Beyond Spatio-Temporal Representations: Evolving Fourier Transform for Temporal Graphs</strong><br><button class=copy-to-clipboard title="Beyond Spatio-Temporal Representations: Evolving Fourier Transform for Temporal Graphs" index=56>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-56 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 16<br>Keywords: Graph, Benchmarking, Continuous Time, Continuous Time<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16078v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16078v1.pdf filename=2402.16078v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present the Evolving <b>Graph</b> Fourier Transform (EFT), the first invertible spectral transform that captures evolving representations on temporal <b>graphs.</b> We motivate our work by the inadequacy of existing methods for capturing the evolving <b>graph</b> spectra, which are also computationally expensive due to the temporal aspect along with the <b>graph</b> vertex domain. We view the problem as an optimization over the Laplacian of the <b>continuous</b> <b>time</b> dynamic <b>graph.</b> Additionally, we propose pseudo-spectrum relaxations that decompose the transformation process, making it highly computationally efficient. The EFT method adeptly captures the evolving <b>graph&rsquo;s</b> structural and positional properties, making it effective for downstream tasks on evolving <b>graphs.</b> Hence, as a reference implementation, we develop a simple neural model induced with EFT for capturing evolving <b>graph</b> spectra. We empirically validate our theoretical findings on a number of large-scale and standard temporal <b>graph</b> <b>benchmarks</b> and demonstrate that our model achieves state-of-the-art performance.</p></p class="citation"></blockquote><h3 id=1625--57122-deep-neural-network-initialization-with-sparsity-inducing-activations-ilan-price-et-al-2024>(16/25 | 57/122) Deep Neural Network Initialization with Sparsity Inducing Activations (Ilan Price et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ilan Price, Nicholas Daultry Ball, Samuel C. H. Lam, Adam C. Jones, Jared Tanner. (2024)<br><strong>Deep Neural Network Initialization with Sparsity Inducing Activations</strong><br><button class=copy-to-clipboard title="Deep Neural Network Initialization with Sparsity Inducing Activations" index=57>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-57 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Gaussian Process<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16184v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16184v1.pdf filename=2402.16184v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Inducing and leveraging sparse activations during training and inference is a promising avenue for improving the computational efficiency of deep networks, which is increasingly important as network sizes continue to grow and their application becomes more widespread. Here we use the large width <b>Gaussian</b> <b>process</b> limit to analyze the behaviour, at random initialization, of nonlinear activations that induce sparsity in the hidden outputs. A previously unreported form of training instability is proven for arguably two of the most natural candidates for hidden layer sparsification; those being a shifted ReLU ($\phi(x)=\max(0, x-\tau)$ for $\tau\ge 0$) and soft thresholding ($\phi(x)=0$ for $|x|\le\tau$ and $x-\text{sign}(x)\tau$ for $|x|>\tau$). We show that this instability is overcome by clipping the nonlinear activation magnitude, at a level prescribed by the shape of the associated <b>Gaussian</b> <b>process</b> variance map. Numerical experiments verify the theory and show that the proposed magnitude clipped sparsifying activations can be trained with training and test fractional sparsity as high as 85% while retaining close to full accuracy.</p></p class="citation"></blockquote><h3 id=1725--58122-bayesian-neural-network-for-personalized-federated-learning-parameter-selection-mengen-luo-et-al-2024>(17/25 | 58/122) Bayesian Neural Network For Personalized Federated Learning Parameter Selection (Mengen Luo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mengen Luo, Ercan Engin Kuruoglu. (2024)<br><strong>Bayesian Neural Network For Personalized Federated Learning Parameter Selection</strong><br><button class=copy-to-clipboard title="Bayesian Neural Network For Personalized Federated Learning Parameter Selection" index=58>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-58 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Federated Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16091v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16091v1.pdf filename=2402.16091v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Federated</b> <b>learning&rsquo;s</b> poor performance in the presence of heterogeneous data remains one of the most pressing issues in the field. Personalized <b>federated</b> <b>learning</b> departs from the conventional paradigm in which all clients employ the same model, instead striving to discover an individualized model for each client to address the heterogeneity in the data. One of such approach involves personalizing specific layers of neural networks. However, prior endeavors have not provided a dependable rationale, and some have selected personalized layers that are entirely distinct and conflicting. In this work, we take a step further by proposing personalization at the elemental level, rather than the traditional layer-level personalization. To select personalized parameters, we introduce Bayesian neural networks and rely on the uncertainty they offer to guide our selection of personalized parameters. Finally, we validate our algorithm&rsquo;s efficacy on several real-world datasets, demonstrating that our proposed approach outperforms existing baselines.</p></p class="citation"></blockquote><h3 id=1825--59122-impact-of-physical-activity-on-quality-of-life-during-pregnancy-a-causal-ml-approach-kianoosh-kazemi-et-al-2024>(18/25 | 59/122) Impact of Physical Activity on Quality of Life During Pregnancy: A Causal ML Approach (Kianoosh Kazemi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kianoosh Kazemi, Iina Ryhtä, Iman Azimi, Hannakaisa Niela-Vilen, Anna Axelin, Amir M. Rahmani, Pasi Liljeberg. (2024)<br><strong>Impact of Physical Activity on Quality of Life During Pregnancy: A Causal ML Approach</strong><br><button class=copy-to-clipboard title="Impact of Physical Activity on Quality of Life During Pregnancy: A Causal ML Approach" index=59>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-59 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-ME<br>Keyword Score: 10<br>Keywords: Counter-factual<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16909v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16909v1.pdf filename=2402.16909v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The concept of Quality of Life (QoL) refers to a holistic measurement of an individual&rsquo;s well-being, incorporating psychological and social aspects. Pregnant women, especially those with obesity and stress, often experience lower QoL. Physical activity (PA) has shown the potential to enhance the QoL. However, pregnant women who are overweight and obese rarely meet the recommended level of PA. Studies have investigated the relationship between PA and QoL during pregnancy using correlation-based approaches. These methods aim to discover spurious correlations between variables rather than causal relationships. Besides, the existing methods mainly rely on physical activity parameters and neglect the use of different factors such as maternal (medical) history and context data, leading to biased estimates. Furthermore, the estimations lack an understanding of mediators and <b>counterfactual</b> scenarios that might affect them. In this paper, we investigate the causal relationship between being physically active (treatment variable) and the QoL (outcome) during pregnancy and postpartum. To estimate the causal effect, we develop a Causal Machine Learning method, integrating causal discovery and causal inference components. The data for our investigation is derived from a long-term wearable-based health monitoring study focusing on overweight and obese pregnant women. The machine learning (meta-learner) estimation technique is used to estimate the causal effect. Our result shows that performing adequate physical activity during pregnancy and postpartum improves the QoL by units of 7.3 and 3.4 on average in physical health and psychological domains, respectively. In the final step, four refutation analysis techniques are employed to validate our estimation.</p></p class="citation"></blockquote><h3 id=1925--60122-spectrum-extraction-and-clipping-for-implicitly-linear-layers-ali-ebrahimpour-boroojeny-et-al-2024>(19/25 | 60/122) Spectrum Extraction and Clipping for Implicitly Linear Layers (Ali Ebrahimpour Boroojeny et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ali Ebrahimpour Boroojeny, Matus Telgarsky, Hari Sundaram. (2024)<br><strong>Spectrum Extraction and Clipping for Implicitly Linear Layers</strong><br><button class=copy-to-clipboard title="Spectrum Extraction and Clipping for Implicitly Linear Layers" index=60>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-60 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CV, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Convolution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16017v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16017v1.pdf filename=2402.16017v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We show the effectiveness of automatic differentiation in efficiently and correctly computing and controlling the spectrum of implicitly linear operators, a rich family of layer types including all standard <b>convolutional</b> and dense layers. We provide the first clipping method which is correct for general <b>convolution</b> layers, and illuminate the representational limitation that caused correctness issues in prior work. We study the effect of the batch normalization layers when concatenated with <b>convolutional</b> layers and show how our clipping method can be applied to their composition. By comparing the accuracy and performance of our algorithms to the state-of-the-art methods, using various experiments, we show they are more precise and efficient and lead to better generalization and adversarial robustness. We provide the code for using our methods at <a href=https://github.com/Ali-E/FastClip>https://github.com/Ali-E/FastClip</a>.</p></p class="citation"></blockquote><h3 id=2025--61122-a-machine-learning-approach-to-detect-customer-satisfaction-from-multiple-tweet-parameters-md-mahmudul-hasan-et-al-2024>(20/25 | 61/122) A Machine Learning Approach to Detect Customer Satisfaction From Multiple Tweet Parameters (Md Mahmudul Hasan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Md Mahmudul Hasan, Dr. Shaikh Anowarul Fattah. (2024)<br><strong>A Machine Learning Approach to Detect Customer Satisfaction From Multiple Tweet Parameters</strong><br><button class=copy-to-clipboard title="A Machine Learning Approach to Detect Customer Satisfaction From Multiple Tweet Parameters" index=61>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-61 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CL, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Sentiment Analysis<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15992v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15992v1.pdf filename=2402.15992v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Since internet technologies have advanced, one of the primary factors in company development is customer happiness. Online platforms have become prominent places for sharing reviews. Twitter is one of these platforms where customers frequently post their thoughts. Reviews of flights on these platforms have become a concern for the airline business. A positive review can help the company grow, while a negative one can quickly ruin its revenue and reputation. So it&rsquo;s vital for airline businesses to examine the feedback and experiences of their customers and enhance their services to remain competitive. But studying thousands of tweets and analyzing them to find the satisfaction of the customer is quite a difficult task. This tedious process can be made easier by using a machine learning approach to analyze tweets to determine client satisfaction levels. Some work has already been done on this strategy to automate the procedure using machine learning and deep learning techniques. However, they are all purely concerned with assessing the text&rsquo;s <b>sentiment.</b> <b>In</b> addition to the text, the tweet also includes the time, location, username, airline name, and so on. This additional information can be crucial for improving the model&rsquo;s outcome. To provide a machine learning based solution, this work has broadened its perspective to include these qualities. And it has come as no surprise that the additional features beyond text <b>sentiment</b> <b>analysis</b> produce better outcomes in machine learning based models.</p></p class="citation"></blockquote><h3 id=2125--62122-shaving-weights-with-occams-razor-bayesian-sparsification-for-neural-networks-using-the-marginal-likelihood-rayen-dhahri-et-al-2024>(21/25 | 62/122) Shaving Weights with Occam&rsquo;s Razor: Bayesian Sparsification for Neural Networks Using the Marginal Likelihood (Rayen Dhahri et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rayen Dhahri, Alexander Immer, Betrand Charpentier, Stephan Günnemann, Vincent Fortuin. (2024)<br><strong>Shaving Weights with Occam&rsquo;s Razor: Bayesian Sparsification for Neural Networks Using the Marginal Likelihood</strong><br><button class=copy-to-clipboard title="Shaving Weights with Occam's Razor: Bayesian Sparsification for Neural Networks Using the Marginal Likelihood" index=62>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-62 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-ML<br>Keyword Score: 10<br>Keywords: Pruning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15978v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15978v1.pdf filename=2402.15978v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Neural network sparsification is a promising avenue to save computational time and memory costs, especially in an age where many successful AI models are becoming too large to na"ively deploy on consumer hardware. While much work has focused on different weight <b>pruning</b> criteria, the overall sparsifiability of the network, i.e., its capacity to be pruned without quality loss, has often been overlooked. We present Sparsifiability via the Marginal likelihood (SpaM), a <b>pruning</b> framework that highlights the effectiveness of using the Bayesian marginal likelihood in conjunction with sparsity-inducing priors for making neural networks more sparsifiable. Our approach implements an automatic Occam&rsquo;s razor that selects the most sparsifiable model that still explains the data well, both for structured and unstructured sparsification. In addition, we demonstrate that the pre-computed posterior Hessian approximation used in the Laplace approximation can be re-used to define a cheap <b>pruning</b> criterion, which outperforms many existing (more expensive) approaches. We demonstrate the effectiveness of our framework, especially at high sparsity levels, across a range of different neural network architectures and datasets.</p></p class="citation"></blockquote><h3 id=2225--63122-codream-exchanging-dreams-instead-of-models-for-federated-aggregation-with-heterogeneous-models-abhishek-singh-et-al-2024>(22/25 | 63/122) CoDream: Exchanging dreams instead of models for federated aggregation with heterogeneous models (Abhishek Singh et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Abhishek Singh, Gauri Gupta, Ritvik Kapila, Yichuan Shi, Alex Dang, Sheshank Shankar, Mohammed Ehab, Ramesh Raskar. (2024)<br><strong>CoDream: Exchanging dreams instead of models for federated aggregation with heterogeneous models</strong><br><button class=copy-to-clipboard title="CoDream: Exchanging dreams instead of models for federated aggregation with heterogeneous models" index=63>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-63 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Federated Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15968v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15968v2.pdf filename=2402.15968v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Federated</b> <b>Learning</b> (FL) enables collaborative optimization of machine learning models across decentralized data by aggregating model parameters. Our approach extends this concept by aggregating &ldquo;knowledge&rdquo; derived from models, instead of model parameters. We present a novel framework called CoDream, where clients collaboratively optimize randomly initialized data using <b>federated</b> <b>optimization</b> in the input data space, similar to how randomly initialized model parameters are optimized in FL. Our key insight is that jointly optimizing this data can effectively capture the properties of the global data distribution. Sharing knowledge in data space offers numerous benefits: (1) model-agnostic collaborative learning, i.e., different clients can have different model architectures; (2) communication that is independent of the model size, eliminating scalability concerns with model parameters; (3) compatibility with secure aggregation, thus preserving the privacy benefits of <b>federated</b> <b>learning;</b> (4) allowing of adaptive optimization of knowledge shared for personalized learning. We empirically validate CoDream on standard FL tasks, demonstrating competitive performance despite not sharing model parameters. Our code: <a href=https://mitmedialab.github.io/codream.github.io/>https://mitmedialab.github.io/codream.github.io/</a></p></p class="citation"></blockquote><h3 id=2325--64122-greenllama-a-framework-for-detoxification-with-explanations-md-tawkat-islam-khondaker-et-al-2024>(23/25 | 64/122) GreenLLaMA: A Framework for Detoxification with Explanations (Md Tawkat Islam Khondaker et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Md Tawkat Islam Khondaker, Muhammad Abdul-Mageed, Laks V. S. Lakshmanan. (2024)<br><strong>GreenLLaMA: A Framework for Detoxification with Explanations</strong><br><button class=copy-to-clipboard title="GreenLLaMA: A Framework for Detoxification with Explanations" index=64>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-64 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CL, cs-CY, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: ChatGPT<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15951v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15951v1.pdf filename=2402.15951v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Prior works on detoxification are scattered in the sense that they do not cover all aspects of detoxification needed in a real-world scenario. Notably, prior works restrict the task of developing detoxification models to only a seen subset of platforms, leaving the question of how the models would perform on unseen platforms unexplored. Additionally, these works do not address non-detoxifiability, a phenomenon whereby the toxic text cannot be detoxified without altering the meaning. We propose GreenLLaMA, the first comprehensive end-to-end detoxification framework, which attempts to alleviate the aforementioned limitations. We first introduce a cross-platform pseudo-parallel corpus applying multi-step data processing and generation strategies leveraging <b>ChatGPT.</b> We then train a suite of detoxification models with our cross-platform corpus. We show that our detoxification models outperform the SoTA model trained with human-annotated parallel corpus. We further introduce explanation to promote transparency and trustworthiness. GreenLLaMA additionally offers a unique paraphrase detector especially dedicated for the detoxification task to tackle the non-detoxifiable cases. Through experimental analysis, we demonstrate the effectiveness of our cross-platform corpus and the robustness of GreenLLaMA against adversarial toxicity.</p></p class="citation"></blockquote><h3 id=2425--65122-pdetime-rethinking-long-term-multivariate-time-series-forecasting-from-the-perspective-of-partial-differential-equations-shiyi-qi-et-al-2024>(24/25 | 65/122) PDETime: Rethinking Long-Term Multivariate Time Series Forecasting from the perspective of partial differential equations (Shiyi Qi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shiyi Qi, Zenglin Xu, Yiduo Li, Liangjian Wen, Qingsong Wen, Qifan Wang, Yuan Qi. (2024)<br><strong>PDETime: Rethinking Long-Term Multivariate Time Series Forecasting from the perspective of partial differential equations</strong><br><button class=copy-to-clipboard title="PDETime: Rethinking Long-Term Multivariate Time Series Forecasting from the perspective of partial differential equations" index=65>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-65 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16913v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16913v1.pdf filename=2402.16913v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advancements in deep learning have led to the development of various models for long-term multivariate time-series forecasting (LMTF), many of which have shown promising results. Generally, the focus has been on historical-value-based models, which rely on past observations to predict future series. Notably, a new trend has emerged with time-index-based models, offering a more nuanced understanding of the continuous dynamics underlying time series. Unlike these two types of models that aggregate the information of spatial domains or temporal domains, in this paper, we consider multivariate time series as spatiotemporal data regularly sampled from a continuous dynamical system, which can be represented by partial differential equations (PDEs), with the spatial domain being fixed. Building on this perspective, we present PDETime, a novel LMTF model inspired by the principles of Neural PDE solvers, following the encoding-integration-decoding operations. Our extensive experimentation across seven diverse real-world LMTF datasets reveals that PDETime not only adapts effectively to the intrinsic spatiotemporal nature of the data but also sets new <b>benchmarks,</b> achieving state-of-the-art results</p></p class="citation"></blockquote><h3 id=2525--66122-a-step-by-step-introduction-to-the-implementation-of-automatic-differentiation-yu-hsueh-fang-et-al-2024>(25/25 | 66/122) A Step-by-step Introduction to the Implementation of Automatic Differentiation (Yu-Hsueh Fang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yu-Hsueh Fang, He-Zhe Lin, Jie-Jyun Liu, Chih-Jen Lin. (2024)<br><strong>A Step-by-step Introduction to the Implementation of Automatic Differentiation</strong><br><button class=copy-to-clipboard title="A Step-by-step Introduction to the Implementation of Automatic Differentiation" index=66>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-66 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16020v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16020v1.pdf filename=2402.16020v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Automatic differentiation is a key component in deep learning. This topic is well studied and excellent surveys such as Baydin et al. (2018) have been available to clearly describe the basic concepts. Further, sophisticated implementations of automatic differentiation are now an important part of popular deep learning frameworks. However, it is difficult, if not impossible, to directly teach students the implementation of existing systems due to the complexity. On the other hand, if the teaching stops at the basic concept, students fail to sense the realization of an implementation. For example, we often mention the computational <b>graph</b> in teaching automatic differentiation, but students wonder how to implement and use it. In this document, we partially fill the gap by giving a step by step introduction of implementing a simple automatic differentiation system. We streamline the mathematical concepts and the implementation. Further, we give the motivation behind each implementation detail, so the whole setting becomes very natural.</p></p class="citation"></blockquote><h2 id=cscv-18>cs.CV (18)</h2><h3 id=118--67122-lstp-language-guided-spatial-temporal-prompt-learning-for-long-form-video-text-understanding-yuxuan-wang-et-al-2024>(1/18 | 67/122) LSTP: Language-guided Spatial-Temporal Prompt Learning for Long-form Video-Text Understanding (Yuxuan Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuxuan Wang, Yueqian Wang, Pengfei Wu, Jianxin Liang, Dongyan Zhao, Zilong Zheng. (2024)<br><strong>LSTP: Language-guided Spatial-Temporal Prompt Learning for Long-form Video-Text Understanding</strong><br><button class=copy-to-clipboard title="LSTP: Language-guided Spatial-Temporal Prompt Learning for Long-form Video-Text Understanding" index=67>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-67 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CL, cs-CV, cs.CV<br>Keyword Score: 60<br>Keywords: Grounding, Question Answering, Large Language Model, Large Language Model, Prompt, Prompt Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16050v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16050v1.pdf filename=2402.16050v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Despite progress in video-language modeling, the computational challenge of interpreting long-form videos in response to task-specific linguistic queries persists, largely due to the complexity of high-dimensional video data and the misalignment between language and visual cues over space and time. To tackle this issue, we introduce a novel approach called Language-guided Spatial-Temporal <b>Prompt</b> <b>Learning</b> (LSTP). This approach features two key components: a Temporal <b>Prompt</b> <b>Sampler</b> (TPS) with optical flow prior that leverages temporal information to efficiently extract relevant video content, and a Spatial <b>Prompt</b> <b>Solver</b> (SPS) that adeptly captures the intricate spatial relationships between visual and textual elements. By harmonizing TPS and SPS with a cohesive training strategy, our framework significantly enhances computational efficiency, temporal understanding, and spatial-temporal alignment. Empirical evaluations across two challenging tasks&ndash;video <b>question</b> <b>answering</b> and temporal <b>question</b> <b>grounding</b> in videos&ndash;using a variety of video-language pretrainings (VLPs) and <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> demonstrate the superior performance, speed, and versatility of our proposed LSTP paradigm.</p></p class="citation"></blockquote><h3 id=218--68122-one-stage-prompt-based-continual-learning-youngeun-kim-et-al-2024>(2/18 | 68/122) One-stage Prompt-based Continual Learning (Youngeun Kim et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Youngeun Kim, Yuhang Li, Priyadarshini Panda. (2024)<br><strong>One-stage Prompt-based Continual Learning</strong><br><button class=copy-to-clipboard title="One-stage Prompt-based Continual Learning" index=68>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-68 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 53<br>Keywords: Vision Transformer, Benchmarking, Continual Learning, Transformer, Prompt, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16189v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16189v1.pdf filename=2402.16189v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Prompt-based</b> <b>Continual</b> <b>Learning</b> (PCL) has gained considerable attention as a promising <b>continual</b> <b>learning</b> solution as it achieves state-of-the-art performance while preventing privacy violation and memory overhead issues. Nonetheless, existing PCL approaches face significant computational burdens because of two <b>Vision</b> <b>Transformer</b> (ViT) feed-forward stages; one is for the query ViT that generates a <b>prompt</b> query to select <b>prompts</b> inside a <b>prompt</b> pool; the other one is a backbone ViT that mixes information between selected <b>prompts</b> and image tokens. To address this, we introduce a one-stage PCL framework by directly using the intermediate layer&rsquo;s token embedding as a <b>prompt</b> query. This design removes the need for an additional feed-forward stage for query ViT, resulting in ~50% computational cost reduction for both training and inference with marginal accuracy drop &lt; 1%. We further introduce a Query-Pool Regularization (QR) loss that regulates the relationship between the <b>prompt</b> query and the <b>prompt</b> pool to improve representation power. The QR loss is only applied during training time, so there is no computational overhead at inference from the QR loss. With the QR loss, our approach maintains ~ 50% computational cost reduction during inference as well as outperforms the prior two-stage PCL methods by ~1.4% on public class-incremental <b>continual</b> <b>learning</b> <b>benchmarks</b> including CIFAR-100, ImageNet-R, and DomainNet.</p></p class="citation"></blockquote><h3 id=318--69122-stochca-a-novel-approach-for-exploiting-pretrained-models-with-cross-attention-seungwon-seo-et-al-2024>(3/18 | 69/122) StochCA: A Novel Approach for Exploiting Pretrained Models with Cross-Attention (Seungwon Seo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Seungwon Seo, Suho Lee, Sangheum Hwang. (2024)<br><strong>StochCA: A Novel Approach for Exploiting Pretrained Models with Cross-Attention</strong><br><button class=copy-to-clipboard title="StochCA: A Novel Approach for Exploiting Pretrained Models with Cross-Attention" index=69>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-69 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 53<br>Keywords: Benchmarking, Fine-tuning, Fine-tuning, Transfer Learning, Transformer, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16092v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16092v1.pdf filename=2402.16092v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Utilizing large-scale pretrained models is a well-known strategy to enhance performance on various target tasks. It is typically achieved through <b>fine-tuning</b> pretrained models on target tasks. However, na"{\i}ve <b>fine-tuning</b> may not fully leverage knowledge embedded in pretrained models. In this study, we introduce a novel <b>fine-tuning</b> method, called stochastic cross-attention (StochCA), specific to <b>Transformer</b> architectures. This method modifies the <b>Transformer&rsquo;s</b> <b>self-attention</b> mechanism to selectively utilize knowledge from pretrained models during <b>fine-tuning.</b> Specifically, in each block, instead of <b>self-attention,</b> cross-attention is performed stochastically according to the predefined probability, where keys and values are extracted from the corresponding block of a pretrained model. By doing so, queries and channel-mixing multi-layer perceptron layers of a target model are <b>fine-tuned</b> to target tasks to learn how to effectively exploit rich representations of pretrained models. To verify the effectiveness of StochCA, extensive experiments are conducted on <b>benchmarks</b> in the areas of <b>transfer</b> <b>learning</b> and domain generalization, where the exploitation of pretrained models is critical. Our experimental results show the superiority of StochCA over state-of-the-art approaches in both areas. Furthermore, we demonstrate that StochCA is complementary to existing approaches, i.e., it can be combined with them to further improve performance. Our code is available at <a href=https://github.com/daintlab/stochastic_cross_attention>https://github.com/daintlab/stochastic_cross_attention</a></p></p class="citation"></blockquote><h3 id=418--70122-key-design-choices-in-source-free-unsupervised-domain-adaptation-an-in-depth-empirical-analysis-andrea-maracani-et-al-2024>(4/18 | 70/122) Key Design Choices in Source-Free Unsupervised Domain Adaptation: An In-depth Empirical Analysis (Andrea Maracani et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Andrea Maracani, Raffaello Camoriano, Elisa Maiettini, Davide Talon, Lorenzo Rosasco, Lorenzo Natale. (2024)<br><strong>Key Design Choices in Source-Free Unsupervised Domain Adaptation: An In-depth Empirical Analysis</strong><br><button class=copy-to-clipboard title="Key Design Choices in Source-Free Unsupervised Domain Adaptation: An In-depth Empirical Analysis" index=70>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-70 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 53<br>Keywords: Benchmarking, Fine-tuning, Self-supervised Learning, Supervised Learning, Unsupervised Learning, Domain Adaptation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16090v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16090v1.pdf filename=2402.16090v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This study provides a comprehensive <b>benchmark</b> framework for Source-Free <b>Unsupervised</b> <b>Domain</b> <b>Adaptation</b> (SF-UDA) in image classification, aiming to achieve a rigorous empirical understanding of the complex relationships between multiple key design factors in SF-UDA methods. The study empirically examines a diverse set of SF-UDA techniques, assessing their consistency across datasets, sensitivity to specific hyperparameters, and applicability across different families of backbone architectures. Moreover, it exhaustively evaluates pre-training datasets and strategies, particularly focusing on both <b>supervised</b> and <b>self-supervised</b> methods, as well as the impact of <b>fine-tuning</b> on the source <b>domain.</b> <b>Our</b> analysis also highlights gaps in existing <b>benchmark</b> practices, guiding SF-UDA research towards more effective and general approaches. It emphasizes the importance of backbone architecture and pre-training dataset selection on SF-UDA performance, serving as an essential reference and providing key insights. Lastly, we release the source code of our experimental framework. This facilitates the construction, training, and testing of SF-UDA methods, enabling systematic large-scale experimental analysis and supporting further research efforts in this field.</p></p class="citation"></blockquote><h3 id=518--71122-cross-resolution-land-cover-classification-using-outdated-products-and-transformers-huan-ni-et-al-2024>(5/18 | 71/122) Cross-Resolution Land Cover Classification Using Outdated Products and Transformers (Huan Ni et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Huan Ni, Yubin Zhao, Haiyan Guan, Cheng Jiang, Yongshi Jie, Xing Wang, Yiyang Shen. (2024)<br><strong>Cross-Resolution Land Cover Classification Using Outdated Products and Transformers</strong><br><button class=copy-to-clipboard title="Cross-Resolution Land Cover Classification Using Outdated Products and Transformers" index=71>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-71 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Supervised Learning, Unsupervised Learning, Weakly-supervised Learning, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16001v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16001v1.pdf filename=2402.16001v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Large-scale high-resolution land cover classification is a prerequisite for constructing Earth system models and addressing ecological and resource issues. Advancements in satellite sensor technology have led to an improvement in spatial resolution and wider coverage areas. Nevertheless, the lack of high-resolution labeled data is still a challenge, hindering the largescale application of land cover classification methods. In this paper, we propose a Transformerbased weakly <b>supervised</b> method for cross-resolution land cover classification using outdated data. First, to capture long-range dependencies without missing the fine-grained details of objects, we propose a U-Net-like <b>Transformer</b> based on a reverse difference mechanism (RDM) using dynamic sparse attention. Second, we propose an anti-noise loss calculation (ANLC) module based on optimal transport (OT). Anti-noise loss calculation identifies confident areas (CA) and vague areas (VA) based on the OT matrix, which relieves the impact of noises in outdated land cover products. By introducing a weakly <b>supervised</b> loss with weights and employing <b>unsupervised</b> loss, the RDM-based U-Net-like <b>Transformer</b> was trained. Remote sensing images with 1 m resolution and the corresponding ground-truths of six states in the United States were employed to validate the performance of the proposed method. The experiments utilized outdated land cover products with 30 m resolution from 2013 as training labels, and produced land cover maps with 1 m resolution from 2017. The results show the superiority of the proposed method compared to state-of-the-art methods. The code is available at <a href=https://github.com/yu-ni1989/ANLC-Former>https://github.com/yu-ni1989/ANLC-Former</a>.</p></p class="citation"></blockquote><h3 id=618--72122-unmasking-dementia-detection-by-masking-input-gradients-a-jsm-approach-to-model-interpretability-and-precision-yasmine-mustafa-et-al-2024>(6/18 | 72/122) Unmasking Dementia Detection by Masking Input Gradients: A JSM Approach to Model Interpretability and Precision (Yasmine Mustafa et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yasmine Mustafa, Tie Luo. (2024)<br><strong>Unmasking Dementia Detection by Masking Input Gradients: A JSM Approach to Model Interpretability and Precision</strong><br><button class=copy-to-clipboard title="Unmasking Dementia Detection by Masking Input Gradients: A JSM Approach to Model Interpretability and Precision" index=72>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-72 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 36<br>Keywords: Counter-factual, Explainable AI, Multi-modal, Multi-modal, Stemming<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16008v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16008v1.pdf filename=2402.16008v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The evolution of deep learning and artificial intelligence has significantly reshaped technological landscapes. However, their effective application in crucial sectors such as medicine demands more than just superior performance, but trustworthiness as well. While interpretability plays a pivotal role, existing <b>explainable</b> <b>AI</b> (XAI) approaches often do not reveal {\em Clever Hans} behavior where a model makes (ungeneralizable) correct predictions using spurious correlations or biases in data. Likewise, current post-hoc XAI methods are susceptible to generating unjustified <b>counterfactual</b> examples. In this paper, we approach XAI with an innovative {\em model debugging} methodology realized through Jacobian Saliency Map (JSM). To cast the problem into a concrete context, we employ Alzheimer&rsquo;s disease (AD) diagnosis as the use case, motivated by its significant impact on human lives and the formidable challenge in its early detection, <b>stemming</b> from the intricate nature of its progression. We introduce an interpretable, <b>multimodal</b> model for AD classification over its multi-stage progression, incorporating JSM as a modality-agnostic tool that provides insights into volumetric changes indicative of brain abnormalities. Our extensive evaluation including ablation study manifests the efficacy of using JSM for model debugging and interpretation, while significantly enhancing model accuracy as well.</p></p class="citation"></blockquote><h3 id=718--73122-task-specific-pretraining-with-noisy-labels-for-remote-sensing-image-segmentation-chenying-liu-et-al-2024>(7/18 | 73/122) Task Specific Pretraining with Noisy Labels for Remote sensing Image Segmentation (Chenying Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chenying Liu, Conrad Albrecht, Yi Wang, Xiao Xiang Zhu. (2024)<br><strong>Task Specific Pretraining with Noisy Labels for Remote sensing Image Segmentation</strong><br><button class=copy-to-clipboard title="Task Specific Pretraining with Noisy Labels for Remote sensing Image Segmentation" index=73>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-73 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Fine-tuning, Supervised Learning, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16164v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16164v1.pdf filename=2402.16164v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In recent years, self-supervision has drawn a lot of attention in remote sensing society due to its ability to reduce the demand of exact labels in <b>supervised</b> deep learning model training. Self-supervision methods generally utilize image-level information to pretrain models in an <b>unsupervised</b> fashion. Though these pretrained encoders show effectiveness in many downstream tasks, their performance on segmentation tasks is often not as good as that on classification tasks. On the other hand, many easily available label sources (e.g., automatic labeling tools and land cover land use products) exist, which can provide a large amount of noisy labels for segmentation model training. In this work, we propose to explore the under-exploited potential of noisy labels for segmentation task specific pretraining, and exam its robustness when confronted with mismatched categories and different decoders during <b>fine-tuning.</b> Specifically, we inspect the impacts of noisy labels on different layers in <b>supervised</b> model training to serve as the basis of our work. Experiments on two datasets indicate the effectiveness of task specific <b>supervised</b> pretraining with noisy labels. The findings are expected to shed light on new avenues for improving the accuracy and versatility of pretraining strategies for remote sensing image segmentation.</p></p class="citation"></blockquote><h3 id=818--74122-avi-talking-learning-audio-visual-instructions-for-expressive-3d-talking-face-generation-yasheng-sun-et-al-2024>(8/18 | 74/122) AVI-Talking: Learning Audio-Visual Instructions for Expressive 3D Talking Face Generation (Yasheng Sun et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yasheng Sun, Wenqing Chu, Hang Zhou, Kaisiyuan Wang, Hideki Koike. (2024)<br><strong>AVI-Talking: Learning Audio-Visual Instructions for Expressive 3D Talking Face Generation</strong><br><button class=copy-to-clipboard title="AVI-Talking: Learning Audio-Visual Instructions for Expressive 3D Talking Face Generation" index=74>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-74 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16124v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16124v1.pdf filename=2402.16124v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>While considerable progress has been made in achieving accurate lip synchronization for 3D speech-driven talking face generation, the task of incorporating expressive facial detail synthesis aligned with the speaker&rsquo;s speaking status remains challenging. Our goal is to directly leverage the inherent style information conveyed by human speech for generating an expressive talking face that aligns with the speaking status. In this paper, we propose AVI-Talking, an Audio-Visual Instruction system for expressive Talking face generation. This system harnesses the robust contextual <b>reasoning</b> and hallucination capability offered by <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> to instruct the realistic synthesis of 3D talking faces. Instead of directly learning facial movements from human speech, our two-stage strategy involves the <b>LLMs</b> first comprehending audio information and generating instructions implying expressive facial details seamlessly corresponding to the speech. Subsequently, a diffusion-based generative network executes these instructions. This two-stage process, coupled with the incorporation of <b>LLMs,</b> enhances model interpretability and provides users with flexibility to comprehend instructions and specify desired operations or modifications. Extensive experiments showcase the effectiveness of our approach in producing vivid talking faces with expressive facial movements and consistent emotional status.</p></p class="citation"></blockquote><h3 id=918--75122-towards-accurate-post-training-quantization-for-reparameterized-models-luoming-zhang-et-al-2024>(9/18 | 75/122) Towards Accurate Post-training Quantization for Reparameterized Models (Luoming Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Luoming Zhang, Yefei He, Wen Fei, Zhenyu Lou, Weijia Wu, YangWei Ying, Hong Zhou. (2024)<br><strong>Towards Accurate Post-training Quantization for Reparameterized Models</strong><br><button class=copy-to-clipboard title="Towards Accurate Post-training Quantization for Reparameterized Models" index=75>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-75 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Convolution, Quantization, Quantization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16121v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16121v1.pdf filename=2402.16121v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Model reparameterization is a widely accepted technique for improving inference speed without compromising performance. However, current Post-training <b>Quantization</b> (PTQ) methods often lead to significant accuracy degradation when applied to reparameterized models. This is primarily caused by channel-specific and sample-specific outliers, which appear only at specific samples and channels and impact on the selection of <b>quantization</b> parameters. To address this issue, we propose RepAPQ, a novel framework that preserves the accuracy of <b>quantized</b> reparameterization models. Different from previous frameworks using Mean Squared Error (MSE) as a measurement, we utilize Mean Absolute Error (MAE) to mitigate the influence of outliers on <b>quantization</b> parameters. Our framework comprises two main components: <b>Quantization</b> Protecting Reparameterization and Across-block Calibration. For effective calibration, <b>Quantization</b> Protecting Reparameterization combines multiple branches into a single <b>convolution</b> with an affine layer. During training, the affine layer accelerates convergence and amplifies the output of the <b>convolution</b> to better accommodate samples with outliers. Additionally, Across-block Calibration leverages the measurement of stage output as supervision to address the gradient problem introduced by MAE and enhance the interlayer correlation with <b>quantization</b> parameters. Comprehensive experiments demonstrate the effectiveness of RepAPQ across various models and tasks. Our framework outperforms previous methods by approximately 1% for 8-bit PTQ and 2% for 6-bit PTQ, showcasing its superior performance. The code is available at \url{https://github.com/ilur98/DLMC-QUANT}.</p></p class="citation"></blockquote><h3 id=1018--76122-adversarial-robust-transfer-learning-for-medical-imaging-via-domain-assimilation-xiaohui-chen-et-al-2024>(10/18 | 76/122) Adversarial-Robust Transfer Learning for Medical Imaging via Domain Assimilation (Xiaohui Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiaohui Chen, Tie Luo. (2024)<br><strong>Adversarial-Robust Transfer Learning for Medical Imaging via Domain Assimilation</strong><br><button class=copy-to-clipboard title="Adversarial-Robust Transfer Learning for Medical Imaging via Domain Assimilation" index=76>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-76 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 20<br>Keywords: Transfer Learning, Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16005v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16005v1.pdf filename=2402.16005v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the field of Medical Imaging, extensive research has been dedicated to leveraging its potential in uncovering critical diagnostic features in patients. Artificial Intelligence (AI)-driven medical diagnosis relies on sophisticated machine learning and deep learning models to analyze, detect, and identify diseases from medical images. Despite the remarkable performance of these models, characterized by high accuracy, they grapple with trustworthiness issues. The introduction of a subtle perturbation to the original image empowers adversaries to manipulate the prediction output, redirecting it to other targeted or untargeted classes. Furthermore, the scarcity of publicly available medical images, constituting a bottleneck for reliable training, has led contemporary algorithms to depend on pretrained models grounded on a large set of natural images &ndash; a practice referred to as <b>transfer</b> <b>learning.</b> However, a significant {\em domain discrepancy} exists between natural and medical images, which causes AI models resulting from <b>transfer</b> <b>learning</b> to exhibit heightened {\em vulnerability} to <b>adversarial</b> <b>attacks.</b> This paper proposes a {\em domain assimilation} approach that introduces texture and color adaptation into <b>transfer</b> <b>learning,</b> followed by a texture preservation component to suppress undesired distortion. We systematically analyze the performance of <b>transfer</b> <b>learning</b> in the face of various <b>adversarial</b> <b>attacks</b> under different data modalities, with the overarching goal of fortifying the model&rsquo;s robustness and security in medical imaging tasks. The results demonstrate high effectiveness in reducing attack efficacy, contributing toward more trustworthy <b>transfer</b> <b>learning</b> in biomedical applications.</p></p class="citation"></blockquote><h3 id=1118--77122-towards-robust-image-stitching-an-adaptive-resistance-learning-against-compatible-attacks-zhiying-jiang-et-al-2024>(11/18 | 77/122) Towards Robust Image Stitching: An Adaptive Resistance Learning against Compatible Attacks (Zhiying Jiang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhiying Jiang, Xingyuan Li, Jinyuan Liu, Xin Fan, Risheng Liu. (2024)<br><strong>Towards Robust Image Stitching: An Adaptive Resistance Learning against Compatible Attacks</strong><br><button class=copy-to-clipboard title="Towards Robust Image Stitching: An Adaptive Resistance Learning against Compatible Attacks" index=77>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-77 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Adversarial Learning, Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15959v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15959v1.pdf filename=2402.15959v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Image stitching seamlessly integrates images captured from varying perspectives into a single wide field-of-view image. Such integration not only broadens the captured scene but also augments holistic perception in computer vision applications. Given a pair of captured images, subtle perturbations and distortions which go unnoticed by the human visual system tend to attack the correspondence matching, impairing the performance of image stitching algorithms. In light of this challenge, this paper presents the first attempt to improve the robustness of image stitching against <b>adversarial</b> <b>attacks.</b> Specifically, we introduce a stitching-oriented attack~(SoA), tailored to amplify the alignment loss within overlapping regions, thereby targeting the feature matching procedure. To establish an attack resistant model, we delve into the robustness of stitching architecture and develop an adaptive <b>adversarial</b> <b>training~(AAT)</b> to balance attack resistance with stitching precision. In this way, we relieve the gap between the routine <b>adversarial</b> <b>training</b> and benign models, ensuring resilience without quality compromise. Comprehensive evaluation across real-world and synthetic datasets validate the deterioration of SoA on stitching performance. Furthermore, AAT emerges as a more robust solution against <b>adversarial</b> <b>perturbations,</b> delivering superior stitching results. Code is available at:https://github.com/Jzy2017/TRIS.</p></p class="citation"></blockquote><h3 id=1218--78122-gennbv-generalizable-next-best-view-policy-for-active-3d-reconstruction-xiao-chen-et-al-2024>(12/18 | 78/122) GenNBV: Generalizable Next-Best-View Policy for Active 3D Reconstruction (Xiao Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiao Chen, Quanyi Li, Tai Wang, Tianfan Xue, Jiangmiao Pang. (2024)<br><strong>GenNBV: Generalizable Next-Best-View Policy for Active 3D Reconstruction</strong><br><button class=copy-to-clipboard title="GenNBV: Generalizable Next-Best-View Policy for Active 3D Reconstruction" index=78>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-78 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-RO, cs.CV<br>Keyword Score: 13<br>Keywords: Benchmarking, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16174v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16174v1.pdf filename=2402.16174v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>While recent advances in neural radiance field enable realistic digitization for large-scale scenes, the image-capturing process is still time-consuming and labor-intensive. Previous works attempt to automate this process using the Next-Best-View (NBV) policy for active 3D reconstruction. However, the existing NBV policies heavily rely on hand-crafted criteria, limited action space, or per-scene optimized representations. These constraints limit their cross-dataset generalizability. To overcome them, we propose GenNBV, an end-to-end generalizable NBV policy. Our policy adopts a <b>reinforcement</b> <b>learning</b> (RL)-based framework and extends typical limited action space to 5D free space. It empowers our agent drone to scan from any viewpoint, and even interact with unseen geometries during training. To boost the cross-dataset generalizability, we also propose a novel multi-source state embedding, including geometric, semantic, and action representations. We establish a <b>benchmark</b> using the Isaac Gym simulator with the Houses3K and OmniObject3D datasets to evaluate this NBV policy. Experiments demonstrate that our policy achieves a 98.26% and 97.12% coverage ratio on unseen building-scale objects from these datasets, respectively, outperforming prior solutions.</p></p class="citation"></blockquote><h3 id=1318--79122-deep-homography-estimation-for-visual-place-recognition-feng-lu-et-al-2024>(13/18 | 79/122) Deep Homography Estimation for Visual Place Recognition (Feng Lu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Feng Lu, Shuting Dong, Lijun Zhang, Bingxi Liu, Xiangyuan Lan, Dongmei Jiang, Chun Yuan. (2024)<br><strong>Deep Homography Estimation for Visual Place Recognition</strong><br><button class=copy-to-clipboard title="Deep Homography Estimation for Visual Place Recognition" index=79>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-79 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 13<br>Keywords: Benchmarking, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16086v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16086v1.pdf filename=2402.16086v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Visual place recognition (VPR) is a fundamental task for many applications such as robot localization and augmented reality. Recently, the hierarchical VPR methods have received considerable attention due to the trade-off between accuracy and efficiency. They usually first use global features to retrieve the candidate images, then verify the spatial consistency of matched local features for re-ranking. However, the latter typically relies on the RANSAC algorithm for fitting homography, which is time-consuming and non-differentiable. This makes existing methods compromise to train the network only in global feature extraction. Here, we propose a <b>transformer-based</b> deep homography estimation (DHE) network that takes the dense feature map extracted by a backbone network as input and fits homography for fast and learnable geometric verification. Moreover, we design a re-projection error of inliers loss to train the DHE network without additional homography labels, which can also be jointly trained with the backbone network to help it extract the features that are more suitable for local matching. Extensive experiments on <b>benchmark</b> datasets show that our method can outperform several state-of-the-art methods. And it is more than one order of magnitude faster than the mainstream hierarchical VPR methods using RANSAC. The code is released at <a href=https://github.com/Lu-Feng/DHE-VPR>https://github.com/Lu-Feng/DHE-VPR</a>.</p></p class="citation"></blockquote><h3 id=1418--80122-diving-deep-into-regions-exploiting-regional-information-transformer-for-single-image-deraining-baiang-li-et-al-2024>(14/18 | 80/122) Diving Deep into Regions: Exploiting Regional Information Transformer for Single Image Deraining (Baiang Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Baiang Li, Zhao Zhang, Huan Zheng, Xiaogang Xu, Yanyan Wei, Jingyi Zhang, Jicong Fan, Meng Wang. (2024)<br><strong>Diving Deep into Regions: Exploiting Regional Information Transformer for Single Image Deraining</strong><br><button class=copy-to-clipboard title="Diving Deep into Regions: Exploiting Regional Information Transformer for Single Image Deraining" index=80>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-80 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16033v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16033v1.pdf filename=2402.16033v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Transformer-based</b> Single Image Deraining (SID) methods have achieved remarkable success, primarily attributed to their robust capability in capturing long-range interactions. However, we&rsquo;ve noticed that current methods handle rain-affected and unaffected regions concurrently, overlooking the disparities between these areas, resulting in confusion between rain streaks and background parts, and inabilities to obtain effective interactions, ultimately resulting in suboptimal deraining outcomes. To address the above issue, we introduce the Region <b>Transformer</b> (Regformer), a novel SID method that underlines the importance of independently processing rain-affected and unaffected regions while considering their combined impact for high-quality image reconstruction. The crux of our method is the innovative Region <b>Transformer</b> Block (RTB), which integrates a Region Masked Attention (RMA) mechanism and a Mixed Gate Forward Block (MGFB). Our RTB is used for attention selection of rain-affected and unaffected regions and local modeling of mixed scales. The RMA generates attention maps tailored to these two regions and their interactions, enabling our model to capture comprehensive features essential for rain removal. To better recover high-frequency textures and capture more local details, we develop the MGFB as a compensation module to complete local mixed scale modeling. Extensive experiments demonstrate that our model reaches state-of-the-art performance, significantly improving the image deraining quality. Our code and trained models are publicly available.</p></p class="citation"></blockquote><h3 id=1518--81122-semi-supervised-open-world-object-detection-sahal-shaji-mullappilly-et-al-2024>(15/18 | 81/122) Semi-supervised Open-World Object Detection (Sahal Shaji Mullappilly et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sahal Shaji Mullappilly, Abhishek Singh Gehlot, Rao Muhammad Anwer, Fahad Shahbaz Khan, Hisham Cholakkal. (2024)<br><strong>Semi-supervised Open-World Object Detection</strong><br><button class=copy-to-clipboard title="Semi-supervised Open-World Object Detection" index=81>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-81 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Object Detection<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16013v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16013v1.pdf filename=2402.16013v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Conventional open-world <b>object</b> <b>detection</b> (OWOD) problem setting first distinguishes known and unknown classes and then later incrementally learns the unknown <b>objects</b> <b>when</b> introduced with labels in the subsequent tasks. However, the current OWOD formulation heavily relies on the external human oracle for knowledge input during the incremental learning stages. Such reliance on run-time makes this formulation less realistic in a real-world deployment. To address this, we introduce a more realistic formulation, named semi-supervised open-world detection (SS-OWOD), that reduces the annotation cost by casting the incremental learning stages of OWOD in a semi-supervised manner. We demonstrate that the performance of the state-of-the-art OWOD detector dramatically deteriorates in the proposed SS-OWOD setting. Therefore, we introduce a novel SS-OWOD detector, named SS-OWFormer, that utilizes a feature-alignment scheme to better align the <b>object</b> <b>query</b> representations between the original and augmented images to leverage the large unlabeled and few labeled data. We further introduce a pseudo-labeling scheme for unknown detection that exploits the inherent capability of decoder <b>object</b> <b>queries</b> to capture <b>object-specific</b> <b>information.</b> We demonstrate the effectiveness of our SS-OWOD problem setting and approach for remote sensing <b>object</b> <b>detection,</b> proposing carefully curated splits and baseline performance evaluations. Our experiments on 4 datasets including MS COCO, PASCAL, Objects365 and DOTA demonstrate the effectiveness of our approach. Our source code, models and splits are available here - <a href=https://github.com/sahalshajim/SS-OWFormer>https://github.com/sahalshajim/SS-OWFormer</a></p></p class="citation"></blockquote><h3 id=1618--82122-an-image-enhancement-method-for-improving-small-intestinal-villi-clarity-shaojie-zhang-et-al-2024>(16/18 | 82/122) An Image Enhancement Method for Improving Small Intestinal Villi Clarity (Shaojie Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shaojie Zhang, Yinghui Wang, Peixuan Liu, Wei Li, Jinlong Yang, Tao Yan, Yukai Wang, Liangyi Huang, Mingfeng Wang, Ibragim R. Atadjanov. (2024)<br><strong>An Image Enhancement Method for Improving Small Intestinal Villi Clarity</strong><br><button class=copy-to-clipboard title="An Image Enhancement Method for Improving Small Intestinal Villi Clarity" index=82>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-82 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Convolution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15977v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15977v1.pdf filename=2402.15977v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents, for the first time, an image enhancement methodology designed to enhance the clarity of small intestinal villi in Wireless Capsule Endoscopy (WCE) images. This method first separates the low-frequency and high-frequency components of small intestinal villi images using guided filtering. Subsequently, an adaptive light gain factor is generated based on the low-frequency component, and an adaptive gradient gain factor is derived from the <b>convolution</b> results of the Laplacian operator in different regions of small intestinal villi images. The obtained light gain factor and gradient gain factor are then combined to enhance the high-frequency components. Finally, the enhanced high-frequency component is fused with the original image to achieve adaptive sharpening of the edges of WCE small intestinal villi images. The experiments affirm that, compared to established WCE image enhancement methods, our approach not only accentuates the edge details of WCE small intestine villi images but also skillfully suppresses noise amplification, thereby preventing the occurrence of edge overshooting.</p></p class="citation"></blockquote><h3 id=1718--83122-voloc-visual-place-recognition-by-querying-compressed-lidar-map-xudong-cai-et-al-2024>(17/18 | 83/122) VOLoc: Visual Place Recognition by Querying Compressed Lidar Map (Xudong Cai et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xudong Cai, Yongcai Wang, Zhe Huang, Yu Shao, Deying Li. (2024)<br><strong>VOLoc: Visual Place Recognition by Querying Compressed Lidar Map</strong><br><button class=copy-to-clipboard title="VOLoc: Visual Place Recognition by Querying Compressed Lidar Map" index=83>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-83 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Transfer Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15961v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15961v1.pdf filename=2402.15961v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The availability of city-scale Lidar maps enables the potential of city-scale place recognition using mobile cameras. However, the city-scale Lidar maps generally need to be compressed for storage efficiency, which increases the difficulty of direct visual place recognition in compressed Lidar maps. This paper proposes VOLoc, an accurate and efficient visual place recognition method that exploits geometric similarity to directly query the compressed Lidar map via the real-time captured image sequence. In the offline phase, VOLoc compresses the Lidar maps using a \emph{Geometry-Preserving Compressor} (GPC), in which the compression is reversible, a crucial requirement for the downstream 6DoF pose estimation. In the online phase, VOLoc proposes an online Geometric Recovery Module (GRM), which is composed of online Visual Odometry (VO) and a point cloud optimization module, such that the local scene structure around the camera is online recovered to build the \emph{Querying Point Cloud} (QPC). Then the QPC is compressed by the same GPC, and is aggregated into a global descriptor by an attention-based aggregation module, to query the compressed Lidar map in the vector space. A <b>transfer</b> <b>learning</b> mechanism is also proposed to improve the accuracy and the generality of the aggregation network. Extensive evaluations show that VOLoc provides localization accuracy even better than the Lidar-to-Lidar place recognition, setting up a new record for utilizing the compressed Lidar map by low-end mobile cameras. The code are publicly available at <a href=https://github.com/Master-cai/VOLoc>https://github.com/Master-cai/VOLoc</a>.</p></p class="citation"></blockquote><h3 id=1818--84122-vistec-video-modeling-for-sports-technique-recognition-and-tactical-analysis-yuchen-he-et-al-2024>(18/18 | 84/122) ViSTec: Video Modeling for Sports Technique Recognition and Tactical Analysis (Yuchen He et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuchen He, Zeqing Yuan, Yihong Wu, Liqi Cheng, Dazhen Deng, Yingcai Wu. (2024)<br><strong>ViSTec: Video Modeling for Sports Technique Recognition and Tactical Analysis</strong><br><button class=copy-to-clipboard title="ViSTec: Video Modeling for Sports Technique Recognition and Tactical Analysis" index=84>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-84 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15952v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15952v1.pdf filename=2402.15952v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The immense popularity of racket sports has fueled substantial demand in tactical analysis with broadcast videos. However, existing manual methods require laborious annotation, and recent attempts leveraging video perception models are limited to low-level annotations like ball trajectories, overlooking tactics that necessitate an understanding of stroke techniques. State-of-the-art action segmentation models also struggle with technique recognition due to frequent occlusions and motion-induced blurring in racket sports videos. To address these challenges, We propose ViSTec, a Video-based Sports Technique recognition model inspired by human cognition that synergizes sparse visual data with rich contextual insights. Our approach integrates a <b>graph</b> to explicitly model strategic knowledge in stroke sequences and enhance technique recognition with contextual inductive bias. A two-stage action perception model is jointly trained to align with the contextual knowledge in the <b>graph.</b> Experiments demonstrate that our method outperforms existing models by a significant margin. Case studies with experts from the Chinese national table tennis team validate our model&rsquo;s capacity to automate analysis for technical actions and tactical strategies. More details are available at: <a href=https://ViSTec2024.github.io/>https://ViSTec2024.github.io/</a>.</p></p class="citation"></blockquote><h2 id=csro-4>cs.RO (4)</h2><h3 id=14--85122-robocodex-multimodal-code-generation-for-robotic-behavior-synthesis-yao-mu-et-al-2024>(1/4 | 85/122) RoboCodeX: Multimodal Code Generation for Robotic Behavior Synthesis (Yao Mu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yao Mu, Junting Chen, Qinglong Zhang, Shoufa Chen, Qiaojun Yu, Chongjian Ge, Runjian Chen, Zhixuan Liang, Mengkang Hu, Chaofan Tao, Peize Sun, Haibao Yu, Chao Yang, Wenqi Shao, Wenhai Wang, Jifeng Dai, Yu Qiao, Mingyu Ding, Ping Luo. (2024)<br><strong>RoboCodeX: Multimodal Code Generation for Robotic Behavior Synthesis</strong><br><button class=copy-to-clipboard title="RoboCodeX: Multimodal Code Generation for Robotic Behavior Synthesis" index=85>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-85 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-CV, cs-RO, cs.RO<br>Keyword Score: 56<br>Keywords: Fine-tuning, Multi-modal, Multi-modal, Supervised Learning, Code Generation, Reasoning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16117v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16117v1.pdf filename=2402.16117v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Robotic behavior synthesis, the problem of understanding <b>multimodal</b> inputs and generating precise physical control for robots, is an important part of Embodied AI. Despite successes in applying <b>multimodal</b> <b>large</b> <b>language</b> <b>models</b> for high-level understanding, it remains challenging to translate these conceptual understandings into detailed robotic actions while achieving generalization across various scenarios. In this paper, we propose a tree-structured <b>multimodal</b> <b>code</b> <b>generation</b> framework for generalized robotic behavior synthesis, termed RoboCodeX. RoboCodeX decomposes high-level human instructions into multiple object-centric manipulation units consisting of physical preferences such as affordance and safety constraints, and applies <b>code</b> <b>generation</b> to introduce generalization ability across various robotics platforms. To further enhance the capability to map conceptual and perceptual understanding into control commands, a specialized <b>multimodal</b> <b>reasoning</b> dataset is collected for pre-training and an iterative self-updating methodology is introduced for <b>supervised</b> <b>fine-tuning.</b> Extensive experiments demonstrate that RoboCodeX achieves state-of-the-art performance in both simulators and real robots on four different kinds of manipulation tasks and one navigation task.</p></p class="citation"></blockquote><h3 id=24--86122-harnessing-the-synergy-between-pushing-grasping-and-throwing-to-enhance-object-manipulation-in-cluttered-scenarios-hamidreza-kasaei-et-al-2024>(2/4 | 86/122) Harnessing the Synergy between Pushing, Grasping, and Throwing to Enhance Object Manipulation in Cluttered Scenarios (Hamidreza Kasaei et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hamidreza Kasaei, Mohammadreza Kasaei. (2024)<br><strong>Harnessing the Synergy between Pushing, Grasping, and Throwing to Enhance Object Manipulation in Cluttered Scenarios</strong><br><button class=copy-to-clipboard title="Harnessing the Synergy between Pushing, Grasping, and Throwing to Enhance Object Manipulation in Cluttered Scenarios" index=86>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-86 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 30<br>Keywords: Reinforcement Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16045v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16045v1.pdf filename=2402.16045v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this work, we delve into the intricate synergy among non-prehensile actions like pushing, and prehensile actions such as grasping and throwing, within the domain of robotic manipulation. We introduce an innovative approach to learning these synergies by leveraging model-free deep <b>reinforcement</b> <b>learning.</b> The robot&rsquo;s workflow involves detecting the pose of the target object and the basket at each time step, predicting the optimal push configuration to isolate the target object, determining the appropriate grasp configuration, and inferring the necessary parameters for an accurate throw into the basket. This empowers robots to skillfully reconfigure cluttered scenarios through pushing, creating space for collision-free grasping actions. Simultaneously, we integrate throwing behavior, showcasing how this action significantly extends the robot&rsquo;s operational reach. Ensuring safety, we developed a <b>simulation</b> environment in Gazebo for robot training, applying the learned policy directly to our real robot. Notably, this work represents a pioneering effort to learn the synergy between pushing, grasping, and throwing actions. Extensive experimentation in both simulated and real-robot scenarios substantiates the effectiveness of our approach across diverse settings. Our approach achieves a success rate exceeding 80% in both simulated and real-world scenarios. A video showcasing our experiments is available online at: <a href=https://youtu.be/q1l4BJVDbRw>https://youtu.be/q1l4BJVDbRw</a></p></p class="citation"></blockquote><h3 id=34--87122-iklink-end-effector-trajectory-tracking-with-minimal-reconfigurations-yeping-wang-et-al-2024>(3/4 | 87/122) IKLink: End-Effector Trajectory Tracking with Minimal Reconfigurations (Yeping Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yeping Wang, Carter Sifferman, Michael Gleicher. (2024)<br><strong>IKLink: End-Effector Trajectory Tracking with Minimal Reconfigurations</strong><br><button class=copy-to-clipboard title="IKLink: End-Effector Trajectory Tracking with Minimal Reconfigurations" index=87>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-87 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 23<br>Keywords: Graph, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16154v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16154v1.pdf filename=2402.16154v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Many applications require a robot to accurately track reference end-effector trajectories. Certain trajectories may not be tracked as single, continuous paths due to the robot&rsquo;s kinematic constraints or obstacles elsewhere in the environment. In this situation, it becomes necessary to divide the trajectory into shorter segments. Each such division introduces a reconfiguration, in which the robot deviates from the reference trajectory, repositions itself in configuration space, and then resumes task execution. The occurrence of reconfigurations should be minimized because they increase the time and energy usage. In this paper, we present IKLink, a method for finding joint motions to track reference end-effector trajectories while executing minimal reconfigurations. Our <b>graph-based</b> method generates a diverse set of Inverse Kinematics (IK) solutions for every waypoint on the reference trajectory and utilizes a dynamic programming algorithm to find the globally optimal motion by linking the IK solutions. We demonstrate the effectiveness of IKLink through a <b>simulation</b> experiment and an illustrative demonstration using a physical robot.</p></p class="citation"></blockquote><h3 id=44--88122-optimizing-base-placement-of-surgical-robot-kinematics-data-driven-approach-by-analyzing-working-pattern-jeonghyeon-yoon-et-al-2024>(4/4 | 88/122) Optimizing Base Placement of Surgical Robot: Kinematics Data-Driven Approach by Analyzing Working Pattern (Jeonghyeon Yoon et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jeonghyeon Yoon, Junhyun Park, Hyojae Park, Hakyoon Lee, Sangwon Lee, Minho Hwang. (2024)<br><strong>Optimizing Base Placement of Surgical Robot: Kinematics Data-Driven Approach by Analyzing Working Pattern</strong><br><button class=copy-to-clipboard title="Optimizing Base Placement of Surgical Robot: Kinematics Data-Driven Approach by Analyzing Working Pattern" index=88>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-88 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 3<br>Keywords: Clustering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16101v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16101v1.pdf filename=2402.16101v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In robot-assisted minimally invasive surgery (RAMIS), optimal placement of the surgical robot&rsquo;s base is crucial for successful surgery. Improper placement can hinder performance due to manipulator limitations and inaccessible workspaces. Traditionally, trained medical staff rely on experience for base placement, but this approach lacks objectivity. This paper proposes a novel method to determine the optimal base pose based on the individual surgeon&rsquo;s working pattern. The proposed method analyzes recorded end-effector poses using machine-learning based <b>clustering</b> technique to identify key positions and orientations preferred by the surgeon. To address joint limits and singularities problems, we introduce two scoring metrics: joint margin score and manipulability score. We then train a multi-layer perceptron (MLP) regressor to predict the optimal base pose based on these scores. Evaluation in a simulated environment using the da Vinci Research Kit (dVRK) showed unique base pose-score maps for four volunteers, highlighting the individuality of working patterns. After conducting tests on the base poses identified using the proposed method, we confirmed that they have a score approximately 28.2% higher than when the robots were placed randomly, with respect to the score we defined. This emphasizes the need for operator-specific optimization in RAMIS base placement.</p></p class="citation"></blockquote><h2 id=csse-5>cs.SE (5)</h2><h3 id=15--89122-rethinking-software-engineering-in-the-era-of-foundation-models-a-curated-catalogue-of-challenges-in-the-development-of-trustworthy-fmware-ahmed-e-hassan-et-al-2024>(1/5 | 89/122) Rethinking Software Engineering in the Era of Foundation Models: A Curated Catalogue of Challenges in the Development of Trustworthy FMware (Ahmed E. Hassan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ahmed E. Hassan, Dayi Lin, Gopi Krishnan Rajbahadur, Keheliya Gallaba, Filipe R. Cogo, Boyuan Chen, Haoxiang Zhang, Kishanthan Thangarajah, Gustavo Ansaldi Oliva, Jiahuei Lin, Wali Mohammad Abdullah, Zhen Ming Jiang. (2024)<br><strong>Rethinking Software Engineering in the Era of Foundation Models: A Curated Catalogue of Challenges in the Development of Trustworthy FMware</strong><br><button class=copy-to-clipboard title="Rethinking Software Engineering in the Era of Foundation Models: A Curated Catalogue of Challenges in the Development of Trustworthy FMware" index=89>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-89 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-AI, cs-SE, cs.SE<br>Keyword Score: 40<br>Keywords: Foundation Model, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15943v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15943v1.pdf filename=2402.15943v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Foundation</b> <b>models</b> (FMs), such as <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs),</b> have revolutionized software development by enabling new use cases and business models. We refer to software built using FMs as FMware. The unique properties of FMware (e.g., <b>prompts,</b> agents, and the need for orchestration), coupled with the intrinsic limitations of FMs (e.g., hallucination) lead to a completely new set of software engineering challenges. Based on our industrial experience, we identified 10 key SE4FMware challenges that have caused enterprise FMware development to be unproductive, costly, and risky. In this paper, we discuss these challenges in detail and state the path for innovation that we envision. Next, we present FMArts, which is our long-term effort towards creating a cradle-to-grave platform for the engineering of trustworthy FMware. Finally, we (i) show how the unique properties of FMArts enabled us to design and develop a complex FMware for a <b>large</b> <b>customer</b> <b>in</b> a timely manner and (ii) discuss the lessons that we learned in doing so. We hope that the disclosure of the aforementioned challenges and our associated efforts to tackle them will not only raise awareness but also promote deeper and further discussions, knowledge sharing, and innovative solutions across the software engineering discipline.</p></p class="citation"></blockquote><h3 id=25--90122-ldb-a-large-language-model-debugger-via-verifying-runtime-execution-step-by-step-li-zhong-et-al-2024>(2/5 | 90/122) LDB: A Large Language Model Debugger via Verifying Runtime Execution Step-by-step (Li Zhong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Li Zhong, Zilong Wang, Jingbo Shang. (2024)<br><strong>LDB: A Large Language Model Debugger via Verifying Runtime Execution Step-by-step</strong><br><button class=copy-to-clipboard title="LDB: A Large Language Model Debugger via Verifying Runtime Execution Step-by-step" index=90>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-90 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-AI, cs-CL, cs-SE, cs.SE<br>Keyword Score: 33<br>Keywords: Benchmarking, Code Generation, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16906v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16906v1.pdf filename=2402.16906v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> are leading significant progress in <b>code</b> <b>generation.</b> Beyond one-pass <b>code</b> <b>generation,</b> recent works further integrate unit tests and program verifiers into <b>LLMs</b> to iteratively refine the generated programs. However, these works consider the generated programs as an indivisible entity, which falls short for <b>LLMs</b> in debugging the programs, especially when the programs contain complex logic flows and data operations. In contrast, when human developers debug programs, they typically set breakpoints and selectively examine runtime execution information. The execution flow and the intermediate variables play a crucial role in the debugging process, yet they are underutilized in the existing literature on <b>code</b> <b>generation.</b> In this study, we introduce <b>Large</b> <b>Language</b> <b>Model</b> Debugger (LDB), a novel debugging framework that enables <b>LLMs</b> to refine their generated programs with the runtime execution information. Specifically, LDB segments the programs into basic blocks and tracks the values of intermediate variables after each block throughout the runtime execution. This allows <b>LLMs</b> to concentrate on simpler <b>code</b> <b>units</b> within the overall execution flow, verify their correctness against the task description block by block, and efficiently pinpoint any potential errors. Experiments demonstrate that LDB consistently enhances the baseline performance by up to 9.8% across the HumanEval, MBPP, and TransCoder <b>benchmarks,</b> archiving new state-of-the-art performance in <b>code</b> <b>debugging</b> for various <b>LLM</b> selections.</p></p class="citation"></blockquote><h3 id=35--91122-nesy-is-alive-and-well-a-llm-driven-symbolic-approach-for-better-code-comment-data-generation-and-classification-hanna-abi-akl-2024>(3/5 | 91/122) NeSy is alive and well: A LLM-driven symbolic approach for better code comment data generation and classification (Hanna Abi Akl, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hanna Abi Akl. (2024)<br><strong>NeSy is alive and well: A LLM-driven symbolic approach for better code comment data generation and classification</strong><br><button class=copy-to-clipboard title="NeSy is alive and well: A LLM-driven symbolic approach for better code comment data generation and classification" index=91>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-91 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-AI, cs-SE, cs.SE<br>Keyword Score: 30<br>Keywords: Data Augmentation, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16910v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16910v1.pdf filename=2402.16910v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present a neuro-symbolic (NeSy) workflow combining a symbolic-based learning technique with a <b>large</b> <b>language</b> <b>model</b> <b>(LLM)</b> agent to generate synthetic <b>data</b> <b>for</b> code comment classification in the C programming language. We also show how generating controlled synthetic <b>data</b> <b>using</b> this workflow fixes some of the notable weaknesses of <b>LLM-based</b> generation and increases the performance of classical machine learning models on the code comment classification task. Our best model, a Neural Network, achieves a Macro-F1 score of 91.412% with an increase of 1.033% after <b>data</b> <b>augmentation.</b></p></p class="citation"></blockquote><h3 id=45--92122-language-models-for-code-completion-a-practical-evaluation-maliheh-izadi-et-al-2024>(4/5 | 92/122) Language Models for Code Completion: A Practical Evaluation (Maliheh Izadi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Maliheh Izadi, Jonathan Katzy, Tim van Dam, Marc Otten, Razvan Mihai Popescu, Arie van Deursen. (2024)<br><strong>Language Models for Code Completion: A Practical Evaluation</strong><br><button class=copy-to-clipboard title="Language Models for Code Completion: A Practical Evaluation" index=92>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-92 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-LG, cs-PL, cs-SE, cs.SE<br>Keyword Score: 13<br>Keywords: Benchmarking, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16197v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16197v1.pdf filename=2402.16197v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Transformer-based</b> language models for automatic code completion have shown great promise so far, yet the evaluation of these models rarely uses real data. This study provides both quantitative and qualitative assessments of three public code language models when completing real-world code. We first developed an open-source IDE extension, Code4Me, for the online evaluation of the models. We collected real auto-completion usage data for over a year from more than 1200 users, resulting in over 600K valid completions. These models were then evaluated using six standard metrics across twelve programming languages. Next, we conducted a qualitative study of 1690 real-world completion requests to identify the reasons behind the poor model performance. A comparative analysis of the models&rsquo; performance in online and offline settings was also performed, using <b>benchmark</b> synthetic datasets and two masking strategies. Our findings suggest that while developers utilize code completion across various languages, the best results are achieved for mainstream languages such as Python and Java. InCoder outperformed the other models across all programming languages, highlighting the significance of training data and objectives. Our study also revealed that offline evaluations do not accurately reflect real-world scenarios. Upon qualitative analysis of the model&rsquo;s predictions, we found that 66.3% of failures were due to the models&rsquo; limitations, 24.4% occurred due to inappropriate model usage in a development context, and 9.3% were valid requests that developers overwrote. Given these findings, we propose several strategies to overcome the current limitations. These include refining training objectives, improving resilience to typographical errors, adopting hybrid approaches, and enhancing implementations and usability.</p></p class="citation"></blockquote><h3 id=55--93122-an-empirical-study-of-challenges-in-machine-learning-asset-management-zhimin-zhao-et-al-2024>(5/5 | 93/122) An Empirical Study of Challenges in Machine Learning Asset Management (Zhimin Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhimin Zhao, Yihao Chen, Abdul Ali Bangash, Bram Adams, Ahmed E. Hassan. (2024)<br><strong>An Empirical Study of Challenges in Machine Learning Asset Management</strong><br><button class=copy-to-clipboard title="An Empirical Study of Challenges in Machine Learning Asset Management" index=93>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-93 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-AI, cs-SE, cs.SE<br>Keyword Score: 3<br>Keywords: Clustering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15990v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15990v2.pdf filename=2402.15990v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In machine learning (ML), efficient asset management, including ML models, datasets, algorithms, and tools, is vital for resource optimization, consistent performance, and a streamlined development lifecycle. This enables quicker iterations, adaptability, reduced development-to-deployment time, and reliable outputs. Despite existing research, a significant knowledge gap remains in operational challenges like model versioning, data traceability, and collaboration, which are crucial for the success of ML projects. Our study aims to address this gap by analyzing 15,065 posts from developer forums and platforms, employing a mixed-method approach to classify inquiries, extract challenges using BERTopic, and identify solutions through open card sorting and BERTopic <b>clustering.</b> We uncover 133 topics related to asset management challenges, grouped into 16 macro-topics, with software dependency, model deployment, and model training being the most discussed. We also find 79 solution topics, categorized under 18 macro-topics, highlighting software dependency, feature development, and file management as key solutions. This research underscores the need for further exploration of identified pain points and the importance of collaborative efforts across academia, industry, and the research community.</p></p class="citation"></blockquote><h2 id=eessiv-2>eess.IV (2)</h2><h3 id=12--94122-integrating-preprocessing-methods-and-convolutional-neural-networks-for-effective-tumor-detection-in-medical-imaging-ha-anh-vu-2024>(1/2 | 94/122) Integrating Preprocessing Methods and Convolutional Neural Networks for Effective Tumor Detection in Medical Imaging (Ha Anh Vu, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ha Anh Vu. (2024)<br><strong>Integrating Preprocessing Methods and Convolutional Neural Networks for Effective Tumor Detection in Medical Imaging</strong><br><button class=copy-to-clipboard title="Integrating Preprocessing Methods and Convolutional Neural Networks for Effective Tumor Detection in Medical Imaging" index=94>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-94 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: 62H30, I-4-9, cs-CV, eess-IV, eess.IV<br>Keyword Score: 33<br>Keywords: Clustering, Convolution, Convolutional Neural Network, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16221v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16221v1.pdf filename=2402.16221v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This research presents a machine-learning approach for tumor detection in medical images using <b>convolutional</b> <b>neural</b> <b>networks</b> <b>(CNNs).</b> The study focuses on preprocessing techniques to enhance image features relevant to tumor detection, followed by developing and training a <b>CNN</b> model for accurate classification. Various image processing techniques, including Gaussian smoothing, bilateral filtering, and K-means <b>clustering,</b> are employed to preprocess the input images and highlight tumor regions. The <b>CNN</b> model is trained and evaluated on a dataset of medical images, with augmentation and data generators utilized to enhance model generalization. Experimental results demonstrate the effectiveness of the proposed approach in accurately detecting tumors in medical images, paving the way for improved diagnostic tools in healthcare.</p></p class="citation"></blockquote><h3 id=22--95122-diffusion-posterior-proximal-sampling-for-image-restoration-hongjie-wu-et-al-2024>(2/2 | 95/122) Diffusion Posterior Proximal Sampling for Image Restoration (Hongjie Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hongjie Wu, Linchao He, Mingqin Zhang, Dongdong Chen, Kunming Luo, Mengting Luo, Ji-Zhe Zhou, Hu Chen, Jiancheng Lv. (2024)<br><strong>Diffusion Posterior Proximal Sampling for Image Restoration</strong><br><button class=copy-to-clipboard title="Diffusion Posterior Proximal Sampling for Image Restoration" index=95>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-95 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, cs-LG, eess-IV, eess.IV<br>Keyword Score: 10<br>Keywords: Diffusion Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16907v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16907v1.pdf filename=2402.16907v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Diffusion</b> <b>models</b> have demonstrated remarkable efficacy in generating high-quality samples. Existing <b>diffusion-based</b> <b>image</b> restoration algorithms exploit pre-trained <b>diffusion</b> <b>models</b> to leverage data priors, yet they still preserve elements inherited from the unconditional generation paradigm. These strategies initiate the denoising process with pure white noise and incorporate random noise at each generative step, leading to over-smoothed results. In this paper, we introduce a refined paradigm for <b>diffusion-based</b> <b>image</b> restoration. Specifically, we opt for a sample consistent with the measurement identity at each generative step, exploiting the sampling selection as an avenue for output stability and enhancement. Besides, we start the restoration process with an initialization combined with the measurement signal, providing supplementary information to better align the generative process. Extensive experimental results and analyses validate the effectiveness of our proposed approach across diverse image restoration tasks.</p></p class="citation"></blockquote><h2 id=cssi-2>cs.SI (2)</h2><h3 id=12--96122-towards-fair-graph-anomaly-detection-problem-new-datasets-and-evaluation-neng-kai-nigel-neo-et-al-2024>(1/2 | 96/122) Towards Fair Graph Anomaly Detection: Problem, New Datasets, and Evaluation (Neng Kai Nigel Neo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Neng Kai Nigel Neo, Yeon-Chang Lee, Yiqiao Jin, Sang-Wook Kim, Srijan Kumar. (2024)<br><strong>Towards Fair Graph Anomaly Detection: Problem, New Datasets, and Evaluation</strong><br><button class=copy-to-clipboard title="Towards Fair Graph Anomaly Detection: Problem, New Datasets, and Evaluation" index=96>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-96 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SI<br>Categories: cs-LG, cs-SI, cs.SI<br>Keyword Score: 33<br>Keywords: Graph Anomaly Detection, Graph, Anomaly Detection, Fairness<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15988v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15988v1.pdf filename=2402.15988v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The Fair <b>Graph</b> <b>Anomaly</b> <b>Detection</b> (FairGAD) problem aims to accurately detect anomalous nodes in an input <b>graph</b> <b>while</b> <b>ensuring</b> <b>fairness</b> and avoiding biased predictions against individuals from sensitive subgroups such as gender or political leanings. <b>Fairness</b> in <b>graphs</b> <b>is</b> <b>particularly</b> crucial in <b>anomaly</b> <b>detection</b> areas such as misinformation detection in search/ranking systems, where decision outcomes can significantly affect individuals. However, the current literature does not comprehensively discuss this problem, nor does it provide realistic datasets that encompass actual <b>graph</b> <b>structures,</b> <b>anomaly</b> <b>labels,</b> and sensitive attributes for research in FairGAD. To bridge this gap, we introduce a formal definition of the FairGAD problem and present two novel <b>graph</b> <b>datasets</b> <b>constructed</b> from the globally prominent social media platforms Reddit and Twitter. These datasets comprise 1.2 million and 400,000 edges associated with 9,000 and 47,000 nodes, respectively, and leverage political leanings as sensitive attributes and misinformation spreaders as <b>anomaly</b> <b>labels.</b> We demonstrate that our FairGAD datasets significantly differ from the synthetic datasets used currently by the research community. These new datasets offer significant values for FairGAD by providing realistic data that captures the intricacies of social networks. Using our datasets, we investigate the performance-fairness trade-off in eleven existing GAD and non-graph AD methods on five state-of-the-art <b>fairness</b> methods, which sheds light on their effectiveness and limitations in addressing the FairGAD problem.</p></p class="citation"></blockquote><h3 id=22--97122-signed-graph-representation-learning-a-survey-zeyu-zhang-et-al-2024>(2/2 | 97/122) Signed Graph Representation Learning: A Survey (Zeyu Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zeyu Zhang, Peiyao Zhao, Xin Li, Jiamou Liu, Xinrui Zhang, Junjie Huang, Xiaofeng Zhu. (2024)<br><strong>Signed Graph Representation Learning: A Survey</strong><br><button class=copy-to-clipboard title="Signed Graph Representation Learning: A Survey" index=97>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-97 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SI<br>Categories: cs-SI, cs.SI<br>Keyword Score: 8<br>Keywords: Graph, Representation Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15980v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15980v1.pdf filename=2402.15980v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the prevalence of social media, the connectedness between people has been greatly enhanced. Real-world relations between users on social media are often not limited to expressing positive ties such as friendship, trust, and agreement, but they also reflect negative ties such as enmity, mistrust, and disagreement, which can be well modelled by signed <b>graphs.</b> Signed <b>Graph</b> <b>Representation</b> <b>Learning</b> (SGRL) is an effective approach to analyze the complex patterns in real-world signed <b>graphs</b> with the co-existence of positive and negative links. In recent years, SGRL has witnesses fruitful results. SGRL tries to allocate low-dimensional <b>representations</b> <b>to</b> nodes and edges which could preserve the <b>graph</b> structure, attribute and some collective properties, e.g., balance theory and status theory. To the best of knowledge, there is no survey paper about SGRL up to now. In this paper, we present a broad review of SGRL methods and discuss some future research directions.</p></p class="citation"></blockquote><h2 id=csit-5>cs.IT (5)</h2><h3 id=15--98122-hpe-transformer-learning-to-optimize-multi-group-multicast-beamforming-under-nonconvex-qos-constraints-yang-li-et-al-2024>(1/5 | 98/122) HPE Transformer: Learning to Optimize Multi-Group Multicast Beamforming Under Nonconvex QoS Constraints (Yang Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yang Li, Ya-Feng Liu. (2024)<br><strong>HPE Transformer: Learning to Optimize Multi-Group Multicast Beamforming Under Nonconvex QoS Constraints</strong><br><button class=copy-to-clipboard title="HPE Transformer: Learning to Optimize Multi-Group Multicast Beamforming Under Nonconvex QoS Constraints" index=98>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-98 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, eess-SP, math-IT<br>Keyword Score: 30<br>Keywords: Simulation, Simulator, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16081v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16081v1.pdf filename=2402.16081v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper studies the quality-of-service (QoS) constrained multi-group multicast beamforming design problem, where each multicast group is composed of a number of users requiring the same content. Due to the nonconvex QoS constraints, this problem is nonconvex and NP-hard. While existing optimization-based iterative algorithms can obtain a suboptimal solution, their iterative nature results in large computational complexity and delay. To facilitate real-time implementations, this paper proposes a deep learning-based approach, which consists of a beamforming structure assisted problem transformation and a customized neural network architecture named hierarchical permutation equivariance (HPE) <b>transformer.</b> The proposed HPE <b>transformer</b> is proved to be permutation equivariant with respect to the users within each multicast group, and also permutation equivariant with respect to different multicast groups. <b>Simulation</b> results demonstrate that the proposed HPE <b>transformer</b> outperforms state-of-the-art optimization-based and deep learning-based approaches for multi-group multicast beamforming design in terms of the total transmit power, the constraint violation, and the computational time. In addition, the proposed HPE <b>transformer</b> achieves pretty good generalization performance on different numbers of users, different numbers of multicast groups, and different signal-to-interference-plus-noise ratio targets.</p></p class="citation"></blockquote><h3 id=25--99122-enhancing-xurllc-with-rsma-assisted-massive-mimo-networks-performance-analysis-and-optimization-yuang-chen-et-al-2024>(2/5 | 99/122) Enhancing xURLLC with RSMA-Assisted Massive-MIMO Networks: Performance Analysis and Optimization (Yuang Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuang Chen, Hancheng Lu, Chenwu Zhang, Yansha Deng, Arumugam Nallanathan. (2024)<br><strong>Enhancing xURLLC with RSMA-Assisted Massive-MIMO Networks: Performance Analysis and Optimization</strong><br><button class=copy-to-clipboard title="Enhancing xURLLC with RSMA-Assisted Massive-MIMO Networks: Performance Analysis and Optimization" index=99>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-99 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, eess-SP, math-IT<br>Keyword Score: 30<br>Keywords: Simulation, Simulator, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16027v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16027v1.pdf filename=2402.16027v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Massive interconnection has sparked people&rsquo;s envisioning for next-generation ultra-reliable and low-latency communications (xURLLC), <b>prompting</b> the design of customized next-generation advanced transceivers (NGAT). Rate-splitting multiple access (RSMA) has emerged as a pivotal technology for NGAT design, given its robustness to imperfect channel state information (CSI) and resilience to quality of service (QoS). Additionally, xURLLC urgently appeals to large-scale access techniques, thus massive multiple-input multiple-output (mMIMO) is anticipated to integrate with RSMA to enhance xURLLC. In this paper, we develop an innovative RSMA-assisted massive-MIMO xURLLC (RSMA-mMIMO-xURLLC) network architecture tailored to accommodate xURLLC&rsquo;s critical QoS constraints in finite blocklength (FBL) regimes. Leveraging uplink pilot training under imperfect CSI at the transmitter, we estimate channel gains and customize linear precoders for efficient downlink short-packet data transmission. Subsequently, we formulate a joint rate-splitting, beamforming, and transmit antenna selection optimization problem to maximize the total effective transmission rate (ETR). Addressing this multi-variable coupled non-convex problem, we decompose it into three corresponding subproblems and propose a low-complexity joint iterative algorithm for efficient optimization. Extensive <b>simulations</b> substantiate that compared with non-orthogonal multiple access (NOMA) and space division multiple access (SDMA), the developed architecture improves the total ETR by 15.3% and 41.91%, respectively, as well as accommodates larger-scale access.</p></p class="citation"></blockquote><h3 id=35--100122-on-a-class-of-greedy-sparse-recovery-algorithms----a-high-dimensional-approach-gang-li-et-al-2024>(3/5 | 100/122) On A Class of Greedy Sparse Recovery Algorithms &ndash; A High Dimensional Approach (Gang Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Gang Li, Qiuwei Li, Shuang Li, Wu Angela Li. (2024)<br><strong>On A Class of Greedy Sparse Recovery Algorithms &ndash; A High Dimensional Approach</strong><br><button class=copy-to-clipboard title="On A Class of Greedy Sparse Recovery Algorithms -- A High Dimensional Approach" index=100>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-100 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, eess-SP, math-IT<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15944v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15944v1.pdf filename=2402.15944v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Sparse signal recovery deals with finding the sparest solution of an under-determined linear system $x = Qs$. In this paper, we propose a novel greedy approach to addressing the challenges from such a problem. Such an approach is based on a characterization of solutions to the system, which allows us to work on the sparse recovery in the $s$-space directly with a given measure. With $l_2$-based measure, two OMP-type algorithms are proposed, which significantly outperform the classical OMP algorithm in terms of recovery accuracy while maintaining comparable computational complexity. An $l_1$-based algorithm, denoted as $\text{Alg}_{GBP}$ (greedy basis pursuit) algorithm, is derived. Such an algorithm significantly outperforms the classical BP algorithm. A CoSaMP-type algorithm is also proposed to further enhance the performance of the two proposed OMP-type algorithms. The superior performance of our proposed algorithms is demonstrated through extensive numerical <b>simulations</b> using synthetic data as well as video signals, highlighting their potential for various applications in compressed sensing and signal processing.</p></p class="citation"></blockquote><h3 id=45--101122-molecular-code-division-multiple-access-signaling-detection-and-performance-weidong-gao-et-al-2024>(4/5 | 101/122) Molecular Code-Division Multiple-Access: Signaling, Detection, and Performance (Weidong Gao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Weidong Gao, Lu Shi, Lie-Liang Yang. (2024)<br><strong>Molecular Code-Division Multiple-Access: Signaling, Detection, and Performance</strong><br><button class=copy-to-clipboard title="Molecular Code-Division Multiple-Access: Signaling, Detection, and Performance" index=101>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-101 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, eess-SP, math-IT<br>Keyword Score: 10<br>Keywords: Fairness<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16097v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16097v1.pdf filename=2402.16097v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>To accomplish relatively complex tasks, in Internet of Bio-Nano Things (IoBNT), information collected by different nano-machines (NMs) is usually sent via multiple-access channels to fusion centers (FCs) for further processing. Relying on two types of molecules, in this paper, a molecular code-division multiple-access (MoCDMA) scheme is designed for multiple NMs to simultaneously send information to an access-point (AP) in a diffusive molecular communications (DMC) environment. We assume that different NMs may have different distances from AP, which generates `near-far&rsquo; effect. Correspondingly, the uniform and channel-inverse based molecular emission schemes are proposed for NMs to emit information molecules. To facilitate the design of different signal detection schemes, the received signals by AP are represented in different forms. Specifically, by considering the limited computational power of nano-machines, three low-complexity detectors are designed in the principles of matched-filtering (MF), zero-forcing (ZF), and minimum mean-square error (MMSE). The noise characteristics in MoCDMA systems and the complexity of various detection schemes are analyzed. The error performance of the MoCDMA systems with various molecular emission and detection schemes is demonstrated and compared. Our studies and performance results demonstrate that MoCDMA constitutes a promising scheme for supporting multiple-access transmission in DMC, while the channel-inverse based transmission can ensure the <b>fairness</b> of communication qualities (FoCQ) among different NMs. Furthermore, different detection schemes may be implemented to attain a good trade-off between implementation complexity and communication reliability.</p></p class="citation"></blockquote><h3 id=55--102122-multi-access-distributed-computing-models-from-map-reduce-arrays-shanuja-sasi-et-al-2024>(5/5 | 102/122) Multi-access Distributed Computing Models from Map-Reduce Arrays (Shanuja Sasi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shanuja Sasi, Onur Günlü, B. Sundar Rajan. (2024)<br><strong>Multi-access Distributed Computing Models from Map-Reduce Arrays</strong><br><button class=copy-to-clipboard title="Multi-access Distributed Computing Models from Map-Reduce Arrays" index=102>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-102 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, math-IT<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16140v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16140v1.pdf filename=2402.16140v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A novel distributed computing model called &ldquo;Multi-access Distributed Computing (MADC)&rdquo; was recently introduced in <a href=http://www.ar>http://www.ar</a>Xiv:2206.12851. In this paper, we represent MADC models via 2-layered bipartite <b>graphs</b> called Map-Reduce <b>Graphs</b> (MRGs) and a set of arrays called Map-Reduce Arrays (MRAs) inspired from the Placement Delivery Arrays (PDAs) used in the coded caching literature. The connection between MRAs and MRGs is established, thereby exploring new topologies and providing coded shuffling schemes for the MADC models with MRGs using the structure of MRAs. A novel \textit{Nearest Neighbor Connect-MRG (NNC-MRG)} is explored and a coding scheme is provided for MADC models with NNC-MRG, exploiting the connections between MRAs and PDAs. Moreover, CT is generalized to Generalized Combinatorial-MRG (GC-MRG). A set of $g-$regular MRAs is provided which corresponds to the existing scheme for MADC models with CT and extended those to generate another set of MRAs to represent MADC models with GC-MRG. A lower bound on the computation-communication curve for MADC model with GC-MRG under homogeneous setting is derived and certain cases are explored where the existing scheme is optimal under CT. One of the major limitations of the existing scheme for CT is that it requires an exponentially large number of reducer nodes and input files for large $\Lambda$. This can be overcome by representing CT by MRAs, where coding schemes can be derived even if some of the reducer nodes are not present. Another way of tackling this is by using a different MRG, specifically NNC-MRG, where the number of reducer nodes and files required are significantly smaller compared to CT. Hence, the advantages are two-fold, which is achievable at the expense of a slight increase in the communication load.</p></p class="citation"></blockquote><h2 id=statml-1>stat.ML (1)</h2><h3 id=11--103122-distribution-free-fair-federated-learning-with-small-samples-qichuan-yin-et-al-2024>(1/1 | 103/122) Distribution-Free Fair Federated Learning with Small Samples (Qichuan Yin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qichuan Yin, Junzhou Huang, Huaxiu Yao, Linjun Zhang. (2024)<br><strong>Distribution-Free Fair Federated Learning with Small Samples</strong><br><button class=copy-to-clipboard title="Distribution-Free Fair Federated Learning with Small Samples" index=103>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-103 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-CY, cs-LG, stat-ML, stat.ML<br>Keyword Score: 23<br>Keywords: Fairness, Federated Learning, Sample Size<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16158v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16158v1.pdf filename=2402.16158v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>As <b>federated</b> <b>learning</b> gains increasing importance in real-world applications due to its capacity for decentralized data training, addressing <b>fairness</b> concerns across demographic groups becomes critically important. However, most existing machine learning algorithms for ensuring <b>fairness</b> are designed for centralized data environments and generally require large-sample and distributional assumptions, underscoring the urgent need for <b>fairness</b> techniques adapted for decentralized and heterogeneous systems with finite-sample and distribution-free guarantees. To address this issue, this paper introduces FedFaiREE, a post-processing algorithm developed specifically for distribution-free fair learning in decentralized settings with small <b>samples.</b> <b>Our</b> approach accounts for unique challenges in decentralized environments, such as client heterogeneity, communication costs, and small <b>sample</b> <b>sizes.</b> We provide rigorous theoretical guarantees for both <b>fairness</b> and accuracy, and our experimental results further provide robust empirical validation for our proposed method.</p></p class="citation"></blockquote><h2 id=csce-1>cs.CE (1)</h2><h3 id=11--104122-accurate-predictions-of-keyhole-depths-using-machine-learning-aided-simulations-jiahui-zhang-et-al-2024>(1/1 | 104/122) Accurate predictions of keyhole depths using machine learning-aided simulations (Jiahui Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiahui Zhang, Runbo Jiang, Kangming Li, Pengyu Chen, Xiao Shang, Zhiying Liu, Jason Hattrick-Simpers, Brian J. Simonds, Qianglong Wei, Hongze Wang, Tao Sun, Anthony D. Rollett, Yu Zou. (2024)<br><strong>Accurate predictions of keyhole depths using machine learning-aided simulations</strong><br><button class=copy-to-clipboard title="Accurate predictions of keyhole depths using machine learning-aided simulations" index=104>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-104 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CE<br>Categories: cond-mat-mtrl-sci, cs-CE, cs.CE<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16190v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16190v1.pdf filename=2402.16190v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The keyhole phenomenon is widely observed in laser materials processing, including laser welding, remelting, cladding, drilling, and additive manufacturing. Keyhole-induced defects, primarily pores, dramatically affect the performance of final products, impeding the broad use of these laser-based technologies. The formation of these pores is typically associated with the dynamic behavior of the keyhole. So far, the accurate characterization and prediction of keyhole features, particularly keyhole depth, as a function of time has been a challenging task. In situ characterization of keyhole dynamic behavior using a synchrotron X-ray is complicated and expensive. Current <b>simulations</b> are hindered by their poor accuracies in predicting keyhole depths due to the lack of real-time laser absorptance data. Here, we develop a machine learning-aided <b>simulation</b> method that allows us to accurately predict keyhole depth over a wide range of processing parameters. Based on titanium and aluminum alloys, two commonly used engineering materials as examples, we achieve an accuracy with an error margin of 10 %, surpassing those simulated using other existing models (with an error margin in a range of 50-200 %). Our machine learning-aided <b>simulation</b> method is affordable and readily deployable for a large variety of materials, opening new doors to eliminate or reduce defects for a wide range of laser materials processing techniques.</p></p class="citation"></blockquote><h2 id=csgr-1>cs.GR (1)</h2><h3 id=11--105122-cinematographic-camera-diffusion-model-hongda-jiang-et-al-2024>(1/1 | 105/122) Cinematographic Camera Diffusion Model (Hongda Jiang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hongda Jiang, Xi Wang, Marc Christie, Libin Liu, Baoquan Chen. (2024)<br><strong>Cinematographic Camera Diffusion Model</strong><br><button class=copy-to-clipboard title="Cinematographic Camera Diffusion Model" index=105>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-105 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.GR<br>Categories: cs-GR, cs.GR<br>Keyword Score: 20<br>Keywords: Diffusion Model, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16143v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16143v1.pdf filename=2402.16143v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Designing effective camera trajectories in virtual 3D environments is a challenging task even for experienced animators. Despite an elaborate film grammar, forged through years of experience, that enables the specification of camera motions through cinematographic properties (framing, shots sizes, angles, motions), there are endless possibilities in deciding how to place and move cameras with characters. Dealing with these possibilities is part of the complexity of the problem. While numerous techniques have been proposed in the literature (optimization-based solving, encoding of empirical rules, learning from real examples,&mldr;), the results either lack variety or ease of control. In this paper, we propose a cinematographic camera <b>diffusion</b> <b>model</b> using a <b>transformer-based</b> architecture to handle temporality and exploit the stochasticity of <b>diffusion</b> <b>models</b> to generate diverse and qualitative trajectories conditioned by high-level textual descriptions. We extend the work by integrating keyframing constraints and the ability to blend naturally between motions using latent interpolation, in a way to augment the degree of control of the designers. We demonstrate the strengths of this text-to-camera motion approach through qualitative and quantitative experiments and gather feedback from professional artists. The code and data are available at \URL{https://github.com/jianghd1996/Camera-control}.</p></p class="citation"></blockquote><h2 id=csai-2>cs.AI (2)</h2><h3 id=12--106122-pidformer-transformer-meets-control-theory-tam-nguyen-et-al-2024>(1/2 | 106/122) PIDformer: Transformer Meets Control Theory (Tam Nguyen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tam Nguyen, César A. Uribe, Tan M. Nguyen, Richard G. Baraniuk. (2024)<br><strong>PIDformer: Transformer Meets Control Theory</strong><br><button class=copy-to-clipboard title="PIDformer: Transformer Meets Control Theory" index=106>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-106 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-SY, cs.AI, eess-SY<br>Keyword Score: 20<br>Keywords: Transformer, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15989v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15989v1.pdf filename=2402.15989v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this work, we address two main shortcomings of <b>transformer</b> architectures: input corruption and rank collapse in their output representation. We unveil <b>self-attention</b> as an autonomous state-space model that inherently promotes smoothness in its solutions, leading to lower-rank outputs and diminished representation capacity. Moreover, the steady-state solution of the model is sensitive to input perturbations. We incorporate a Proportional-Integral-Derivative (PID) closed-loop feedback control system with a reference point into the model to improve robustness and representation capacity. This integration aims to preserve high-frequency details while bolstering model stability, rendering it more noise-resilient. The resulting controlled state-space model is theoretically proven robust and adept at addressing the rank collapse. Motivated by this control framework, we derive a novel class of <b>transformers,</b> PID-controlled <b>Transformer</b> (PIDformer), aimed at improving robustness and mitigating the rank-collapse issue inherent in softmax <b>transformers.</b> We empirically evaluate the model for advantages and robustness against baseline <b>transformers</b> across various practical tasks, including object classification, image segmentation, and language modeling.</p></p class="citation"></blockquote><h3 id=22--107122-budget-constrained-tool-learning-with-planning-yuanhang-zheng-et-al-2024>(2/2 | 107/122) Budget-Constrained Tool Learning with Planning (Yuanhang Zheng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuanhang Zheng, Peng Li, Ming Yan, Ji Zhang, Fei Huang, Yang Liu. (2024)<br><strong>Budget-Constrained Tool Learning with Planning</strong><br><button class=copy-to-clipboard title="Budget-Constrained Tool Learning with Planning" index=107>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-107 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 10<br>Keywords: Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15960v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15960v1.pdf filename=2402.15960v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Despite intensive efforts devoted to tool learning, the problem of budget-constrained tool learning, which focuses on resolving user queries within a specific budget constraint, has been widely overlooked. This paper proposes a novel method for budget-constrained tool learning. Our approach involves creating a preferable plan under the budget constraint before utilizing the tools. This plan outlines the feasible tools and the maximum number of times they can be employed, offering a comprehensive overview of the tool learning process for <b>large</b> <b>language</b> <b>models.</b> This allows them to allocate the budget from a broader perspective. To devise the plan without incurring significant extra costs, we suggest initially estimating the usefulness of the candidate tools based on past experience. Subsequently, we employ dynamic programming to formulate the plan. Experimental results demonstrate that our method can be integrated with various tool learning methods, significantly enhancing their effectiveness under strict budget constraints.</p></p class="citation"></blockquote><h2 id=csar-2>cs.AR (2)</h2><h3 id=12--108122-sustainable-supercomputing-for-ai-gpu-power-capping-at-hpc-scale-dan-zhao-et-al-2024>(1/2 | 108/122) Sustainable Supercomputing for AI: GPU Power Capping at HPC Scale (Dan Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dan Zhao, Siddharth Samsi, Joseph McDonald, Baolin Li, David Bestor, Michael Jones, Devesh Tiwari, Vijay Gadepally. (2024)<br><strong>Sustainable Supercomputing for AI: GPU Power Capping at HPC Scale</strong><br><button class=copy-to-clipboard title="Sustainable Supercomputing for AI: GPU Power Capping at HPC Scale" index=108>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-108 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AR<br>Categories: cs-AI, cs-AR, cs-DC, cs.AR<br>Keyword Score: 20<br>Keywords: Fine-tuning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18593v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18593v1.pdf filename=2402.18593v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>As research and deployment of AI grows, the computational burden to support and sustain its progress inevitably does too. To train or <b>fine-tune</b> state-of-the-art models in NLP, computer vision, etc., some form of AI hardware acceleration is virtually a requirement. Recent <b>large</b> <b>language</b> <b>models</b> require considerable resources to train and deploy, resulting in significant energy usage, potential carbon emissions, and massive demand for GPUs and other hardware accelerators. However, this surge carries <b>large</b> <b>implications</b> <b>for</b> energy sustainability at the HPC/datacenter level. In this paper, we study the aggregate effect of power-capping GPUs on GPU temperature and power draw at a research supercomputing center. With the right amount of power-capping, we show significant decreases in both temperature and power draw, reducing power consumption and potentially improving hardware life-span with minimal impact on job performance. While power-capping reduces power draw by design, the aggregate system-wide effect on overall energy consumption is less clear; for instance, if users notice job performance degradation from GPU power-caps, they may request additional GPU-jobs to compensate, negating any energy savings or even worsening energy consumption. To our knowledge, our work is the first to conduct and make available a detailed analysis of the effects of GPU power-capping at the supercomputing scale. We hope our work will inspire HPCs/datacenters to further explore, evaluate, and communicate the impact of power-capping AI hardware accelerators for more sustainable AI.</p></p class="citation"></blockquote><h3 id=22--109122-encodingnet-a-novel-encoding-based-mac-design-for-efficient-neural-network-acceleration-bo-liu-et-al-2024>(2/2 | 109/122) EncodingNet: A Novel Encoding-based MAC Design for Efficient Neural Network Acceleration (Bo Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bo Liu, Grace Li Zhang, Xunzhao Yin, Ulf Schlichtmann, Bing Li. (2024)<br><strong>EncodingNet: A Novel Encoding-based MAC Design for Efficient Neural Network Acceleration</strong><br><button class=copy-to-clipboard title="EncodingNet: A Novel Encoding-based MAC Design for Efficient Neural Network Acceleration" index=109>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-109 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AR<br>Categories: cs-AR, cs-CE, cs-LG, cs.AR<br>Keyword Score: 10<br>Keywords: Quantization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.18595v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.18595v1.pdf filename=2402.18595v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep neural networks (DNNs) have achieved great breakthroughs in many fields such as image classification and natural language processing. However, the execution of DNNs needs to conduct massive numbers of multiply-accumulate (MAC) operations on hardware and thus incurs a large power consumption. To address this challenge, we propose a novel digital MAC design based on encoding. In this new design, the multipliers are replaced by simple logic gates to project the results onto a wide bit representation. These bits carry individual position weights, which can be trained for specific neural networks to enhance inference accuracy. The outputs of the new multipliers are added by bit-wise weighted accumulation and the accumulation results are compatible with existing computing platforms accelerating neural networks with either uniform or non-uniform <b>quantization.</b> Since the multiplication function is replaced by simple logic projection, the critical paths in the resulting circuits become much shorter. Correspondingly, pipelining stages in the MAC array can be reduced, leading to a significantly smaller area as well as a better power efficiency. The proposed design has been synthesized and verified by ResNet18-Cifar10, ResNet20-Cifar100 and ResNet50-ImageNet. The experimental results confirmed the reduction of circuit area by up to 79.63% and the reduction of power consumption of executing DNNs by up to 70.18%, while the accuracy of the neural networks can still be well maintained.</p></p class="citation"></blockquote><h2 id=csni-2>cs.NI (2)</h2><h3 id=12--110122-learning-with-semantics-towards-a-semantics-aware-routing-anomaly-detection-system-yihao-chen-et-al-2024>(1/2 | 110/122) Learning with Semantics: Towards a Semantics-Aware Routing Anomaly Detection System (Yihao Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yihao Chen, Qilei Yin, Qi Li, Zhuotao Liu, Ke Xu, Yi Xu, Mingwei Xu, Ziqian Liu, Jianping Wu. (2024)<br><strong>Learning with Semantics: Towards a Semantics-Aware Routing Anomaly Detection System</strong><br><button class=copy-to-clipboard title="Learning with Semantics: Towards a Semantics-Aware Routing Anomaly Detection System" index=110>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-110 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NI<br>Categories: cs-NI, cs.NI<br>Keyword Score: 15<br>Keywords: Anomaly Detection, Representation Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16025v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16025v1.pdf filename=2402.16025v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>BGP is the de facto inter-domain routing protocol to ensure global connectivity of the Internet. However, various reasons, such as deliberate attacks or misconfigurations, could cause BGP routing anomalies. Traditional methods for BGP routing <b>anomaly</b> <b>detection</b> require significant manual investigation of routes by network operators. Although machine learning has been applied to automate the process, prior arts typically impose significant training overhead (such as large-scale data labeling and feature crafting), and only produce uninterpretable results. To address these limitations, this paper presents a routing <b>anomaly</b> <b>detection</b> system centering around a novel network <b>representation</b> <b>learning</b> model named BEAM. The core design of BEAM is to accurately learn the unique properties (defined as \emph{routing role}) of each Autonomous System (AS) in the Internet by incorporating BGP semantics. As a result, routing <b>anomaly</b> <b>detection,</b> given BEAM, is reduced to a matter of discovering unexpected routing role churns upon observing new route announcements. We implement a prototype of our routing <b>anomaly</b> <b>detection</b> system and extensively evaluate its performance. The experimental results, based on 18 real-world RouteViews datasets containing over 11 billion route announcement records, demonstrate that our system can detect all previously-confirmed routing anomalies, while only introducing at most five false alarms every 180 million route announcements. We also deploy our system at a large ISP to perform real-world detection for one month. During the course of deployment, our system detects 497 true anomalies in the wild with an average of only 1.65 false alarms per day.</p></p class="citation"></blockquote><h3 id=22--111122-communication-traffic-characteristics-reveal-an-iot-devices-identity-rajarshi-roy-chowdhury-et-al-2024>(2/2 | 111/122) Communication Traffic Characteristics Reveal an IoT Devices Identity (Rajarshi Roy Chowdhury et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rajarshi Roy Chowdhury, Debashish Roy, Pg Emeroylariffion Abas. (2024)<br><strong>Communication Traffic Characteristics Reveal an IoT Devices Identity</strong><br><button class=copy-to-clipboard title="Communication Traffic Characteristics Reveal an IoT Devices Identity" index=111>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-111 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NI<br>Categories: F-2-2; I-2-7, cs-AI, cs-NI, cs.NI<br>Keyword Score: 10<br>Keywords: Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16173v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16173v1.pdf filename=2402.16173v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Internet of Things (IoT) is one of the technological advancements of the twenty-first century which can improve living standards. However, it also imposes new types of security challenges, including device authentication, traffic types classification, and malicious traffic identification, in the network domain. Traditionally, internet protocol (IP) and media access control (MAC) addresses are utilized for identifying network-connected devices in a network, whilst these addressing schemes are prone to be compromised, including spoofing attacks and MAC randomization. Therefore, device identification using only explicit identifiers is a challenging task. Accurate device identification plays a key role in securing a network. In this paper, a <b>supervised</b> machine learning-based device fingerprinting (DFP) model has been proposed for identifying network-connected IoT devices using only communication traffic characteristics (or implicit identifiers). A single transmission control protocol/internet protocol (TCP/IP) packet header features have been utilized for generating unique fingerprints, with the fingerprints represented as a vector of 22 features. Experimental results have shown that the proposed DFP method achieves over 98% in classifying individual IoT devices using the UNSW dataset with 22 smart-home IoT devices. This signifies that the proposed approach is invaluable to network operators in making their networks more secure.</p></p class="citation"></blockquote><h2 id=csne-1>cs.NE (1)</h2><h3 id=11--112122-efficient-online-learning-for-networks-of-two-compartment-spiking-neurons-yujia-yin-et-al-2024>(1/1 | 112/122) Efficient Online Learning for Networks of Two-Compartment Spiking Neurons (Yujia Yin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yujia Yin, Xinyi Chen, Chenxiang Ma, Jibin Wu, Kay Chen Tan. (2024)<br><strong>Efficient Online Learning for Networks of Two-Compartment Spiking Neurons</strong><br><button class=copy-to-clipboard title="Efficient Online Learning for Networks of Two-Compartment Spiking Neurons" index=112>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-112 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NE<br>Categories: cs-NE, cs.NE<br>Keyword Score: 13<br>Keywords: Benchmarking, Stemming<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15969v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15969v1.pdf filename=2402.15969v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The brain-inspired Spiking Neural Networks (SNNs) have garnered considerable research interest due to their superior performance and energy efficiency in processing temporal signals. Recently, a novel multi-compartment spiking neuron model, namely the Two-Compartment LIF (TC-LIF) model, has been proposed and exhibited a remarkable capacity for sequential modelling. However, training the TC-LIF model presents challenges <b>stemming</b> from the large memory consumption and the issue of gradient vanishing associated with the Backpropagation Through Time (BPTT) algorithm. To address these challenges, online learning methodologies emerge as a promising solution. Yet, to date, the application of online learning methods in SNNs has been predominantly confined to simplified Leaky Integrate-and-Fire (LIF) neuron models. In this paper, we present a novel online learning method specifically tailored for networks of TC-LIF neurons. Additionally, we propose a refined TC-LIF neuron model called Adaptive TC-LIF, which is carefully designed to enhance temporal information integration in online learning scenarios. Extensive experiments, conducted on various sequential <b>benchmarks,</b> demonstrate that our approach successfully preserves the superior sequential modeling capabilities of the TC-LIF neuron while incorporating the training efficiency and hardware friendliness of online learning. As a result, it offers a multitude of opportunities to leverage neuromorphic solutions for processing temporal signals.</p></p class="citation"></blockquote><h2 id=csgt-1>cs.GT (1)</h2><h3 id=11--113122-egalitarian-price-of-fairness-for-indivisible-goods-karen-frilya-celine-et-al-2024>(1/1 | 113/122) Egalitarian Price of Fairness for Indivisible Goods (Karen Frilya Celine et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Karen Frilya Celine, Muhammad Ayaz Dzulfikar, Ivan Adrian Koswara. (2024)<br><strong>Egalitarian Price of Fairness for Indivisible Goods</strong><br><button class=copy-to-clipboard title="Egalitarian Price of Fairness for Indivisible Goods" index=113>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-113 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.GT<br>Categories: 91B32, F-m, cs-GT, cs-MA, cs.GT<br>Keyword Score: 10<br>Keywords: Fairness<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16145v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16145v1.pdf filename=2402.16145v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the context of fair division, the concept of price of <b>fairness</b> has been introduced to quantify the loss of welfare when we have to satisfy some <b>fairness</b> condition. In other words, it is the price we have to pay to guarantee <b>fairness.</b> Various settings of fair division have been considered previously; we extend to the setting of indivisible goods by using egalitarian welfare as the welfare measure, instead of the commonly used utilitarian welfare. We provide lower and upper bounds for various <b>fairness</b> and efficiency conditions such as envy-freeness up to one good (EF1) and maximum Nash welfare (MNW).</p></p class="citation"></blockquote><h2 id=cshc-1>cs.HC (1)</h2><h3 id=11--114122-towards-mixed-reality-as-the-everyday-computing-paradigm-challenges--design-recommendations-amir-reza-asadi-et-al-2024>(1/1 | 114/122) Towards Mixed Reality as the Everyday Computing Paradigm: Challenges & Design Recommendations (Amir Reza Asadi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Amir Reza Asadi, Reza Hemadi. (2024)<br><strong>Towards Mixed Reality as the Everyday Computing Paradigm: Challenges & Design Recommendations</strong><br><button class=copy-to-clipboard title="Towards Mixed Reality as the Everyday Computing Paradigm: Challenges & Design Recommendations" index=114>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-114 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-HC, cs.HC<br>Keyword Score: 10<br>Keywords: Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15974v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15974v1.pdf filename=2402.15974v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This research presents a proof-of-concept prototype of an all-in-one mixed reality application platform, developed to investigate the needs and expectations of users from mixed reality systems. The study involved an extensive user study with 1,052 participants, including the collection of diaries from 6 users and conducting interviews with 51 participants to gain deeper insights into their experiences. The findings from the interviews revealed that directly porting current user flows into 3D environments was not well-received by the target users. Instead, users expressed a clear preference for alternative 3D interactions along with the continued use of 2D interfaces. This study provides insights for understanding user preferences and interactions in mixed reality systems, and design <b>recommendations</b> to facilitate the mass adoption of MR systems.</p></p class="citation"></blockquote><h2 id=csdb-1>cs.DB (1)</h2><h3 id=11--115122-convolution-and-cross-correlation-of-count-sketches-enables-fast-cardinality-estimation-of-multi-join-queries-mike-heddes-et-al-2024>(1/1 | 115/122) Convolution and Cross-Correlation of Count Sketches Enables Fast Cardinality Estimation of Multi-Join Queries (Mike Heddes et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mike Heddes, Igor Nunes, Tony Givargis, Alex Nicolau. (2024)<br><strong>Convolution and Cross-Correlation of Count Sketches Enables Fast Cardinality Estimation of Multi-Join Queries</strong><br><button class=copy-to-clipboard title="Convolution and Cross-Correlation of Count Sketches Enables Fast Cardinality Estimation of Multi-Join Queries" index=115>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-115 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DB<br>Categories: cs-DB, cs.DB<br>Keyword Score: 10<br>Keywords: Convolution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15953v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15953v1.pdf filename=2402.15953v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the increasing rate of data generated by critical systems, estimating functions on streaming data has become essential. This demand has driven numerous advancements in algorithms designed to efficiently query and analyze one or more data streams while operating under memory constraints. The primary challenge arises from the rapid influx of new items, requiring algorithms that enable efficient incremental processing of streams in order to keep up. A prominent algorithm in this domain is the AMS sketch. Originally developed to estimate the second frequency moment of a data stream, it can also estimate the cardinality of the equi-join between two relations. Since then, two important advancements are the Count sketch, a method which significantly improves upon the sketch update time, and secondly, an extension of the AMS sketch to accommodate multi-join queries. However, combining the strengths of these methods to maintain sketches for multi-join queries while ensuring fast update times is a non-trivial task, and has remained an open problem for decades as highlighted in the existing literature. In this work, we successfully address this problem by introducing a novel sketching method which has fast updates, even for sketches capable of accurately estimating the cardinality of complex multi-join queries. We prove that our estimator is unbiased and has the same error guarantees as the AMS-based method. Our experimental results confirm the significant improvement in update time complexity, resulting in orders of magnitude faster estimates, with equal or better estimation accuracy.</p></p class="citation"></blockquote><h2 id=csdl-1>cs.DL (1)</h2><h3 id=11--116122-pst-bench-tracing-and-benchmarking-the-source-of-publications-fanjin-zhang-et-al-2024>(1/1 | 116/122) PST-Bench: Tracing and Benchmarking the Source of Publications (Fanjin Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fanjin Zhang, Kun Cao, Yukuo Cen, Jifan Yu, Da Yin, Jie Tang. (2024)<br><strong>PST-Bench: Tracing and Benchmarking the Source of Publications</strong><br><button class=copy-to-clipboard title="PST-Bench: Tracing and Benchmarking the Source of Publications" index=116>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-116 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DL<br>Categories: cs-CL, cs-DL, cs.DL<br>Keyword Score: 6<br>Keywords: Benchmarking, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16009v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16009v1.pdf filename=2402.16009v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Tracing the source of research papers is a fundamental yet challenging task for researchers. The billion-scale citation relations between papers hinder researchers from understanding the evolution of science efficiently. To date, there is still a lack of an accurate and scalable dataset constructed by professional researchers to identify the direct source of their studied papers, based on which automatic algorithms can be developed to expand the evolutionary knowledge of science. In this paper, we study the problem of paper source tracing (PST) and construct a high-quality and ever-increasing dataset PST-Bench in computer science. Based on PST-Bench, we reveal several intriguing discoveries, such as the differing evolution patterns across various topics. An exploration of various methods underscores the hardness of PST-Bench, pinpointing potential directions on this topic. The dataset and codes have been available at <a href=https://github.com/THUDM/paper-source-trace>https://github.com/THUDM/paper-source-trace</a>.</p></p class="citation"></blockquote><h2 id=mathap-1>math.AP (1)</h2><h3 id=11--117122-direct-and-inverse-scattering-in-a-three-dimensional-planar-waveguide-yan-chang-et-al-2024>(1/1 | 117/122) Direct and Inverse scattering in a three-dimensional planar waveguide (Yan Chang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yan Chang, Yukun Guo, Yue Zhao. (2024)<br><strong>Direct and Inverse scattering in a three-dimensional planar waveguide</strong><br><button class=copy-to-clipboard title="Direct and Inverse scattering in a three-dimensional planar waveguide" index=117>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-117 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.AP<br>Categories: cs-NA, math-AP, math-NA, math.AP<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15955v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15955v1.pdf filename=2402.15955v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we study the direct and inverse scattering of the Schr"odinger equation in a three-dimensional planar waveguide. For the direct problem, we derive a resonance-free region and resolvent estimates for the resolvent of the Schr"odinger operator in such a <b>geometry.</b> Based on the analysis of the resolvent, several inverse problems are investigated. First, given the potential function, we prove the uniqueness of the inverse source problem with multi-frequency data. We also develop a Fourier-based method to reconstruct the source function. The capability of this method is numerically illustrated by examples. Second, the uniqueness and increased stability of an inverse potential problem from data generated by incident waves are achieved in the absence of the source function. To derive the stability estimate, we use an argument of quantitative analytic continuation in complex theory. Third, we prove the uniqueness of simultaneously determining the source and potential by active boundary data generated by incident waves. In these inverse problems, we only use the limited lateral Dirichlet boundary data at multiple wavenumbers within a finite interval.</p></p class="citation"></blockquote><h2 id=mathco-2>math.CO (2)</h2><h3 id=12--118122-branch-depth-is-minor-closure-of-contraction-deletion-depth-marcin-briański-et-al-2024>(1/2 | 118/122) Branch-depth is minor closure of contraction-deletion-depth (Marcin Briański et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Marcin Briański, Daniel Kráľ, Kristýna Pekárková. (2024)<br><strong>Branch-depth is minor closure of contraction-deletion-depth</strong><br><button class=copy-to-clipboard title="Branch-depth is minor closure of contraction-deletion-depth" index=118>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-118 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.CO<br>Categories: cs-DM, math-CO, math.CO<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16215v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16215v1.pdf filename=2402.16215v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The notion of branch-depth for matroids was introduced by DeVos, Kwon and Oum as the matroid analogue of the tree-depth of <b>graphs.</b> The contraction-deletion-depth, another tree-depth like parameter of matroids, is the number of recursive steps needed to decompose a matroid by contractions and deletions to single elements. Any matroid with contraction-deletion-depth at most d has branch-depth at most d. However, the two notions are not functionally equivalent as contraction-deletion-depth of matroids with branch-depth two can be arbitrarily large. We show that the two notions are functionally equivalent for representable matroids when minor closures are considered. Namely, an F-representable matroid has small branch-depth if and only if it is a minor of an F-representable matroid with small contraction-deletion-depth. This implies that any class of F-representable matroids has bounded branch-depth if and only if it is a subclass of the minor closure of a class of F-representable matroids with bounded contraction-deletion-depth.</p></p class="citation"></blockquote><h3 id=22--119122-list-coloring-of-some-cayley-graphs-using-kernel-perfections-prajnanaswaroopa-s-2024>(2/2 | 119/122) List Coloring of some Cayley graphs using Kernel perfections (Prajnanaswaroopa S, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Prajnanaswaroopa S. (2024)<br><strong>List Coloring of some Cayley graphs using Kernel perfections</strong><br><button class=copy-to-clipboard title="List Coloring of some Cayley graphs using Kernel perfections" index=119>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-119 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.CO<br>Categories: 05C15, 05C25, G-2-1; G-2-2, cs-DM, cs-DS, math-CO, math.CO<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16047v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16047v1.pdf filename=2402.16047v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we try to determine exact or bounds on the choosability, or list chromatic numbers of some Cayley <b>graphs,</b> typically some Unitary Cayley <b>graphs</b> and Cayley <b>graphs</b> on Dihedral groups.</p></p class="citation"></blockquote><h2 id=csds-1>cs.DS (1)</h2><h3 id=11--120122-enhanced-graph-pattern-matching-nicola-cotumaccio-2024>(1/1 | 120/122) Enhanced Graph Pattern Matching (Nicola Cotumaccio, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nicola Cotumaccio. (2024)<br><strong>Enhanced Graph Pattern Matching</strong><br><button class=copy-to-clipboard title="Enhanced Graph Pattern Matching" index=120>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-120 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DS<br>Categories: cs-DS, cs.DS<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16205v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16205v1.pdf filename=2402.16205v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Pattern matching queries on strings can be solved in linear time by Knuth-Morris-Pratt (KMP) algorithm. In 1973, Weiner introduced the suffix tree of a string [FOCS 1973] and showed that the seemingly more difficult problem of computing matching statistics can also be solved in liner time. Pattern matching queries on <b>graphs</b> are inherently more difficult: under the Orthogonal Vector hypothesis, the <b>graph</b> pattern matching problem cannot be solved in subquadratic time [TALG 2023]. The complexity of <b>graph</b> pattern matching can be parameterized by the topological complexity of the considered <b>graph,</b> which is captured by a parameter $ p $ [JACM 2023]. In this paper, we show that, as in the string setting, computing matching statistics on <b>graph</b> is as difficult as solving standard pattern matching queries. To this end, we introduce a notion of longest common prefix (LCP) array for arbitrary <b>graphs.</b></p></p class="citation"></blockquote><h2 id=cslo-1>cs.LO (1)</h2><h3 id=11--121122-effective-mso-definability-for-tree-width-bounded-models-of-an-inductive-separation-logic-of-relations-lucas-bueri-et-al-2024>(1/1 | 121/122) Effective MSO-Definability for Tree-width Bounded Models of an Inductive Separation Logic of Relations (Lucas Bueri et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lucas Bueri, Radu Iosif, Florian Zuleger. (2024)<br><strong>Effective MSO-Definability for Tree-width Bounded Models of an Inductive Separation Logic of Relations</strong><br><button class=copy-to-clipboard title="Effective MSO-Definability for Tree-width Bounded Models of an Inductive Separation Logic of Relations" index=121>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-121 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LO<br>Categories: cs-FL, cs-LO, cs.LO<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16150v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16150v1.pdf filename=2402.16150v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A class of <b>graph</b> languages is definable in Monadic Second-Order logic (MSO) if and only if it consists of sets of models of MSO formul{\ae}. If, moreover, there is a computable bound on the tree-widths of the <b>graphs</b> in each such set, the satisfiability and entailment problems are decidable, by Courcelle&rsquo;s Theorem. This motivates the comparison of other <b>graph</b> logics to MSO. In this paper, we consider the MSO definability of a Separation Logic of Relations (SLR) that describes simple hyper-graphs, in which each sequence of vertices is attached to at most one edge with a given label. Our logic SLR uses inductive predicates whose recursive definitions consist of existentially quantified separated conjunctions of relation and predicate atoms. The main contribution of this paper is an expressive fragment of SLR that describes bounded tree-width sets of <b>graphs</b> which can, moreover, be effectively translated into MSO.</p></p class="citation"></blockquote><h2 id=q-fincp-1>q-fin.CP (1)</h2><h3 id=11--122122-optimizing-portfolio-management-and-risk-assessment-in-digital-assets-using-deep-learning-for-predictive-analysis-qishuo-cheng-et-al-2024>(1/1 | 122/122) Optimizing Portfolio Management and Risk Assessment in Digital Assets Using Deep Learning for Predictive Analysis (Qishuo Cheng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qishuo Cheng, Le Yang, Jiajian Zheng, Miao Tian, Duan Xin. (2024)<br><strong>Optimizing Portfolio Management and Risk Assessment in Digital Assets Using Deep Learning for Predictive Analysis</strong><br><button class=copy-to-clipboard title="Optimizing Portfolio Management and Risk Assessment in Digital Assets Using Deep Learning for Predictive Analysis" index=122>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-122 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: q-fin.CP<br>Categories: cs-CE, cs-LG, q-fin-CP, q-fin.CP<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15994v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15994v1.pdf filename=2402.15994v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Portfolio management issues have been extensively studied in the field of artificial intelligence in recent years, but existing deep learning-based quantitative trading methods have some areas where they could be improved. First of all, the prediction mode of stocks is singular; often, only one trading expert is trained by a model, and the trading decision is solely based on the prediction results of the model. Secondly, the data source used by the model is relatively simple, and only considers the data of the stock itself, ignoring the impact of the whole market risk on the stock. In this paper, the DQN algorithm is introduced into asset management portfolios in a novel and straightforward way, and the performance greatly exceeds the <b>benchmark,</b> which fully proves the effectiveness of the DRL algorithm in portfolio management. This also inspires us to consider the complexity of financial problems, and the use of algorithms should be fully combined with the problems to adapt. Finally, in this paper, the strategy is implemented by selecting the assets and actions with the largest Q value. Since different assets are trained separately as environments, there may be a phenomenon of Q value drift among different assets (different assets have different Q value distribution areas), which may easily lead to incorrect asset selection. Consider adding constraints so that the Q values of different assets share a Q value distribution to improve results.</p></p class="citation"></blockquote></div><div class="row pl-3 pr-3"><div class="col-md-6 share-buttons"></div></div><hr><div class="row next-prev-navigator"><div class="col-md-6 previous-article"><a href=/akitenkrad-blog/posts/arxiv/202402/20240226000000/ title="arXiv @ 2024.02.26" class="btn btn-outline-info"><div><i class="fas fa-chevron-circle-left"></i> Prev</div><div class=next-prev-text>arXiv @ 2024.02.26</div></a></div><div class="col-md-6 next-article"><a href=/akitenkrad-blog/posts/arxiv/202402/20240228000000/ title="arXiv @ 2024.02.28" class="btn btn-outline-info"><div>Next <i class="fas fa-chevron-circle-right"></i></div><div class=next-prev-text>arXiv @ 2024.02.28</div></a></div></div><hr></div></div></div><a id=scroll-to-top class=btn><i class="fas fa-chevron-circle-up"></i></a></section><section class=toc-section id=toc-section><div class=toc-holder><h5 class="text-center pl-3">Table of Contents</h5><hr><div class=toc><nav id=TableOfContents><ul><li><a href=#primary-categories>Primary Categories</a></li><li><a href=#keywords>Keywords</a></li><li><a href=#cssd-2>cs.SD (2)</a><ul><li><a href=#12--1122-chatmusician-understanding-and-generating-music-intrinsically-with-llm-ruibin-yuan-et-al-2024>(1/2 | 1/122) ChatMusician: Understanding and Generating Music Intrinsically with LLM (Ruibin Yuan et al., 2024)</a></li><li><a href=#22--2122-phonetic-and-lexical-discovery-of-a-canine-language-using-hubert-xingyuan-li-et-al-2024>(2/2 | 2/122) Phonetic and Lexical Discovery of a Canine Language using HuBERT (Xingyuan Li et al., 2024)</a></li></ul></li><li><a href=#cscl-29>cs.CL (29)</a><ul><li><a href=#129--3122-citation-enhanced-generation-for-llm-based-chatbot-weitao-li-et-al-2024>(1/29 | 3/122) Citation-Enhanced Generation for LLM-based Chatbot (Weitao Li et al., 2024)</a></li><li><a href=#229--4122-graphwiz-an-instruction-following-language-model-for-graph-problems-nuo-chen-et-al-2024>(2/29 | 4/122) GraphWiz: An Instruction-Following Language Model for Graph Problems (Nuo Chen et al., 2024)</a></li><li><a href=#329--5122-llms-with-chain-of-thought-are-non-causal-reasoners-guangsheng-bao-et-al-2024>(3/29 | 5/122) LLMs with Chain-of-Thought Are Non-Causal Reasoners (Guangsheng Bao et al., 2024)</a></li><li><a href=#429--6122-likelihood-based-mitigation-of-evaluation-bias-in-large-language-models-masanari-ohi-et-al-2024>(4/29 | 6/122) Likelihood-based Mitigation of Evaluation Bias in Large Language Models (Masanari Ohi et al., 2024)</a></li><li><a href=#529--7122-from-noise-to-clarity-unraveling-the-adversarial-suffix-of-large-language-model-attacks-via-translation-of-text-embeddings-hao-wang-et-al-2024>(5/29 | 7/122) From Noise to Clarity: Unraveling the Adversarial Suffix of Large Language Model Attacks via Translation of Text Embeddings (Hao Wang et al., 2024)</a></li><li><a href=#629--8122-higpt-heterogeneous-graph-language-model-jiabin-tang-et-al-2024>(6/29 | 8/122) HiGPT: Heterogeneous Graph Language Model (Jiabin Tang et al., 2024)</a></li><li><a href=#729--9122-distalaner-distantly-supervised-active-learning-augmented-named-entity-recognition-in-the-open-source-software-ecosystem-somnath-banerjee-et-al-2024>(7/29 | 9/122) DistALANER: Distantly Supervised Active Learning Augmented Named Entity Recognition in the Open Source Software Ecosystem (Somnath Banerjee et al., 2024)</a></li><li><a href=#829--10122-text-understanding-and-generation-using-transformer-models-for-intelligent-e-commerce-recommendations-yafei-xiang-et-al-2024>(8/29 | 10/122) Text Understanding and Generation Using Transformer Models for Intelligent E-commerce Recommendations (Yafei Xiang et al., 2024)</a></li><li><a href=#929--11122-dont-forget-your-reward-values-language-model-alignment-via-value-based-calibration-xin-mao-et-al-2024>(9/29 | 11/122) Don&rsquo;t Forget Your Reward Values: Language Model Alignment via Value-based Calibration (Xin Mao et al., 2024)</a></li><li><a href=#1029--12122-from-text-to-transformation-a-comprehensive-review-of-large-language-models-versatility-pravneet-kaur-et-al-2024>(10/29 | 12/122) From Text to Transformation: A Comprehensive Review of Large Language Models&rsquo; Versatility (Pravneet Kaur et al., 2024)</a></li><li><a href=#1129--13122-say-more-with-less-understanding-prompt-learning-behaviors-through-gist-compression-xinze-li-et-al-2024>(11/29 | 13/122) Say More with Less: Understanding Prompt Learning Behaviors through Gist Compression (Xinze Li et al., 2024)</a></li><li><a href=#1229--14122-deep-learning-approaches-for-improving-question-answering-systems-in-hepatocellular-carcinoma-research-shuning-huo-et-al-2024>(12/29 | 14/122) Deep Learning Approaches for Improving Question Answering Systems in Hepatocellular Carcinoma Research (Shuning Huo et al., 2024)</a></li><li><a href=#1329--15122-direct-punjabi-to-english-speech-translation-using-discrete-units-prabhjot-kaur-et-al-2024>(13/29 | 15/122) Direct Punjabi to English speech translation using discrete units (Prabhjot Kaur et al., 2024)</a></li><li><a href=#1429--16122-hypotermqa-hypothetical-terms-dataset-for-benchmarking-hallucination-tendency-of-llms-cem-uluoglakci-et-al-2024>(14/29 | 16/122) HypoTermQA: Hypothetical Terms Dataset for Benchmarking Hallucination Tendency of LLMs (Cem Uluoglakci et al., 2024)</a></li><li><a href=#1529--17122-defending-large-language-models-against-jailbreak-attacks-via-semantic-smoothing-jiabao-ji-et-al-2024>(15/29 | 17/122) Defending Large Language Models against Jailbreak Attacks via Semantic Smoothing (Jiabao Ji et al., 2024)</a></li><li><a href=#1629--18122-ehrnoteqa-a-patient-specific-question-answering-benchmark-for-evaluating-large-language-models-in-clinical-settings-sunjun-kweon-et-al-2024>(16/29 | 18/122) EHRNoteQA: A Patient-Specific Question Answering Benchmark for Evaluating Large Language Models in Clinical Settings (Sunjun Kweon et al., 2024)</a></li><li><a href=#1729--19122-periodiclora-breaking-the-low-rank-bottleneck-in-lora-optimization-xiangdi-meng-et-al-2024>(17/29 | 19/122) PeriodicLoRA: Breaking the Low-Rank Bottleneck in LoRA Optimization (Xiangdi Meng et al., 2024)</a></li><li><a href=#1829--20122-lstprompt-large-language-models-as-zero-shot-time-series-forecasters-by-long-short-term-prompting-haoxin-liu-et-al-2024>(18/29 | 20/122) LSTPrompt: Large Language Models as Zero-Shot Time Series Forecasters by Long-Short-Term Prompting (Haoxin Liu et al., 2024)</a></li><li><a href=#1929--21122-detecting-machine-generated-texts-by-multi-population-aware-optimization-for-maximum-mean-discrepancy-shuhai-zhang-et-al-2024>(19/29 | 21/122) Detecting Machine-Generated Texts by Multi-Population Aware Optimization for Maximum Mean Discrepancy (Shuhai Zhang et al., 2024)</a></li><li><a href=#2029--22122-emotion-classification-in-short-english-texts-using-deep-learning-techniques-siddhanth-bhat-2024>(20/29 | 22/122) Emotion Classification in Short English Texts using Deep Learning Techniques (Siddhanth Bhat, 2024)</a></li><li><a href=#2129--23122-asem-enhancing-empathy-in-chatbot-through-attention-based-sentiment-and-emotion-modeling-omama-hamad-et-al-2024>(21/29 | 23/122) ASEM: Enhancing Empathy in Chatbot through Attention-based Sentiment and Emotion Modeling (Omama Hamad et al., 2024)</a></li><li><a href=#2229--24122-fusechat-knowledge-fusion-of-chat-models-fanqi-wan-et-al-2024>(22/29 | 24/122) FuseChat: Knowledge Fusion of Chat Models (Fanqi Wan et al., 2024)</a></li><li><a href=#2329--25122-how-large-language-models-encode-context-knowledge-a-layer-wise-probing-study-tianjie-ju-et-al-2024>(23/29 | 25/122) How Large Language Models Encode Context Knowledge? A Layer-Wise Probing Study (Tianjie Ju et al., 2024)</a></li><li><a href=#2429--26122-tmt-tri-modal-translation-between-speech-image-and-text-by-processing-different-modalities-as-different-languages-minsu-kim-et-al-2024>(24/29 | 26/122) TMT: Tri-Modal Translation between Speech, Image, and Text by Processing Different Modalities as Different Languages (Minsu Kim et al., 2024)</a></li><li><a href=#2529--27122-c3-confidence-calibration-model-cascade-for-inference-efficient-cross-lingual-natural-language-understanding-taixi-lu-et-al-2024>(25/29 | 27/122) $C^3$: Confidence Calibration Model Cascade for Inference-Efficient Cross-Lingual Natural Language Understanding (Taixi Lu et al., 2024)</a></li><li><a href=#2629--28122-instructedit-instruction-based-knowledge-editing-for-large-language-models-bozhong-tian-et-al-2024>(26/29 | 28/122) InstructEdit: Instruction-based Knowledge Editing for Large Language Models (Bozhong Tian et al., 2024)</a></li><li><a href=#2729--29122-hitting-proberty-with-non-linearity-and-more-avik-pal-et-al-2024>(27/29 | 29/122) Hitting &lsquo;Probe&rsquo;rty with Non-Linearity, and More (Avik Pal et al., 2024)</a></li><li><a href=#2829--30122-what-generative-artificial-intelligence-means-for-terminological-definitions-antonio-san-martín-2024>(28/29 | 30/122) What Generative Artificial Intelligence Means for Terminological Definitions (Antonio San Martín, 2024)</a></li><li><a href=#2929--31122-training-a-bilingual-language-model-by-mapping-tokens-onto-a-shared-character-space-aviad-rom-et-al-2024>(29/29 | 31/122) Training a Bilingual Language Model by Mapping Tokens onto a Shared Character Space (Aviad Rom et al., 2024)</a></li></ul></li><li><a href=#cscr-7>cs.CR (7)</a><ul><li><a href=#17--32122-drattack-prompt-decomposition-and-reconstruction-makes-powerful-llm-jailbreakers-xirui-li-et-al-2024>(1/7 | 32/122) DrAttack: Prompt Decomposition and Reconstruction Makes Powerful LLM Jailbreakers (Xirui Li et al., 2024)</a></li><li><a href=#27--33122-attention-gan-for-anomaly-detection-a-cutting-edge-approach-to-cybersecurity-threat-management-mohammed-abo-sen-2024>(2/7 | 33/122) Attention-GAN for Anomaly Detection: A Cutting-Edge Approach to Cybersecurity Threat Management (Mohammed Abo Sen, 2024)</a></li><li><a href=#37--34122-fedfdp-federated-learning-with-fairness-and-differential-privacy-xinpeng-ling-et-al-2024>(3/7 | 34/122) FedFDP: Federated Learning with Fairness and Differential Privacy (Xinpeng Ling et al., 2024)</a></li><li><a href=#47--35122-how-to-privately-tune-hyperparameters-in-federated-learning-insights-from-a-benchmark-study-natalija-mitic-et-al-2024>(4/7 | 35/122) How to Privately Tune Hyperparameters in Federated Learning? Insights from a Benchmark Study (Natalija Mitic et al., 2024)</a></li><li><a href=#57--36122-attacking-llm-watermarks-by-exploiting-their-strengths-qi-pang-et-al-2024>(5/7 | 36/122) Attacking LLM Watermarks by Exploiting Their Strengths (Qi Pang et al., 2024)</a></li><li><a href=#67--37122-luataint-a-static-taint-analysis-system-for-web-interface-framework-vulnerability-of-iot-devices-jiahui-xiang-et-al-2024>(6/7 | 37/122) LuaTaint: A Static Taint Analysis System for Web Interface Framework Vulnerability of IoT Devices (Jiahui Xiang et al., 2024)</a></li><li><a href=#77--38122-an-adversarial-robustness-benchmark-for-enterprise-network-intrusion-detection-joão-vitorino-et-al-2024>(7/7 | 38/122) An Adversarial Robustness Benchmark for Enterprise Network Intrusion Detection (João Vitorino et al., 2024)</a></li></ul></li><li><a href=#csir-3>cs.IR (3)</a><ul><li><a href=#13--39122-disentangled-graph-variational-auto-encoder-for-multimodal-recommendation-with-interpretability-xin-zhou-et-al-2024>(1/3 | 39/122) Disentangled Graph Variational Auto-Encoder for Multimodal Recommendation with Interpretability (Xin Zhou et al., 2024)</a></li><li><a href=#23--40122-ir2-information-regularization-for-information-retrieval-jianyou-wang-et-al-2024>(2/3 | 40/122) IR2: Information Regularization for Information Retrieval (Jianyou Wang et al., 2024)</a></li><li><a href=#33--41122-pfeed-generating-near-real-time-personalized-feeds-using-precomputed-embedding-similarities-binyam-gebre-et-al-2024>(3/3 | 41/122) Pfeed: Generating near real-time personalized feeds using precomputed embedding similarities (Binyam Gebre et al., 2024)</a></li></ul></li><li><a href=#cslg-25>cs.LG (25)</a><ul><li><a href=#125--42122-building-flexible-machine-learning-models-for-scientific-computing-at-scale-tianyu-chen-et-al-2024>(1/25 | 42/122) Building Flexible Machine Learning Models for Scientific Computing at Scale (Tianyu Chen et al., 2024)</a></li><li><a href=#225--43122-deep-contrastive-graph-learning-with-clustering-oriented-guidance-mulin-chen-et-al-2024>(2/25 | 43/122) Deep Contrastive Graph Learning with Clustering-Oriented Guidance (Mulin Chen et al., 2024)</a></li><li><a href=#325--44122-deepforge-leveraging-ai-for-microstructural-control-in-metal-forming-via-model-predictive-control-jan-petrik-et-al-2024>(3/25 | 44/122) DeepForge: Leveraging AI for Microstructural Control in Metal Forming via Model Predictive Control (Jan Petrik et al., 2024)</a></li><li><a href=#425--45122-structural-knowledge-driven-meta-learning-for-task-offloading-in-vehicular-networks-with-integrated-communications-sensing-and-computing-ruijin-sun-et-al-2024>(4/25 | 45/122) Structural Knowledge-Driven Meta-Learning for Task Offloading in Vehicular Networks with Integrated Communications, Sensing and Computing (Ruijin Sun et al., 2024)</a></li><li><a href=#525--46122-hierarchical-energy-signatures-using-machine-learning-for-operational-visibility-and-diagnostics-in-automotive-manufacturing-ankur-verma-et-al-2024>(5/25 | 46/122) Hierarchical energy signatures using machine learning for operational visibility and diagnostics in automotive manufacturing (Ankur Verma et al., 2024)</a></li><li><a href=#625--47122-how-can-llm-guide-rl-a-value-based-approach-shenao-zhang-et-al-2024>(6/25 | 47/122) How Can LLM Guide RL? A Value-Based Approach (Shenao Zhang et al., 2024)</a></li><li><a href=#725--48122-trustworthy-personalized-bayesian-federated-learning-via-posterior-fine-tune-mengen-luo-et-al-2024>(7/25 | 48/122) Trustworthy Personalized Bayesian Federated Learning via Posterior Fine-Tune (Mengen Luo et al., 2024)</a></li><li><a href=#825--49122-more-than-routing-joint-gps-and-route-modeling-for-refine-trajectory-representation-learning-zhipeng-ma-et-al-2024>(8/25 | 49/122) More Than Routing: Joint GPS and Route Modeling for Refine Trajectory Representation Learning (Zhipeng Ma et al., 2024)</a></li><li><a href=#925--50122-a-vae-based-framework-for-learning-multi-level-neural-granger-causal-connectivity-jiahe-lin-et-al-2024>(9/25 | 50/122) A VAE-based Framework for Learning Multi-Level Neural Granger-Causal Connectivity (Jiahe Lin et al., 2024)</a></li><li><a href=#1025--51122-combining-machine-learning-with-computational-fluid-dynamics-using-openfoam-and-smartsim-tomislav-maric-et-al-2024>(10/25 | 51/122) Combining Machine Learning with Computational Fluid Dynamics using OpenFOAM and SmartSim (Tomislav Maric et al., 2024)</a></li><li><a href=#1125--52122-consensus-learning-a-novel-decentralised-ensemble-learning-paradigm-horia-magureanu-et-al-2024>(11/25 | 52/122) Consensus learning: A novel decentralised ensemble learning paradigm (Horia Magureanu et al., 2024)</a></li><li><a href=#1225--53122-informed-meta-learning-katarzyna-kobalczyk-et-al-2024>(12/25 | 53/122) Informed Meta-Learning (Katarzyna Kobalczyk et al., 2024)</a></li><li><a href=#1325--54122-a-unified-fourier-slice-method-to-derive-ridgelet-transform-for-a-variety-of-depth-2-neural-networks-sho-sonoda-et-al-2024>(13/25 | 54/122) A unified Fourier slice method to derive ridgelet transform for a variety of depth-2 neural networks (Sho Sonoda et al., 2024)</a></li><li><a href=#1425--55122-behavioral-refinement-via-interpolant-based-policy-diffusion-kaiqi-chen-et-al-2024>(14/25 | 55/122) Behavioral Refinement via Interpolant-based Policy Diffusion (Kaiqi Chen et al., 2024)</a></li><li><a href=#1525--56122-beyond-spatio-temporal-representations-evolving-fourier-transform-for-temporal-graphs-anson-bastos-et-al-2024>(15/25 | 56/122) Beyond Spatio-Temporal Representations: Evolving Fourier Transform for Temporal Graphs (Anson Bastos et al., 2024)</a></li><li><a href=#1625--57122-deep-neural-network-initialization-with-sparsity-inducing-activations-ilan-price-et-al-2024>(16/25 | 57/122) Deep Neural Network Initialization with Sparsity Inducing Activations (Ilan Price et al., 2024)</a></li><li><a href=#1725--58122-bayesian-neural-network-for-personalized-federated-learning-parameter-selection-mengen-luo-et-al-2024>(17/25 | 58/122) Bayesian Neural Network For Personalized Federated Learning Parameter Selection (Mengen Luo et al., 2024)</a></li><li><a href=#1825--59122-impact-of-physical-activity-on-quality-of-life-during-pregnancy-a-causal-ml-approach-kianoosh-kazemi-et-al-2024>(18/25 | 59/122) Impact of Physical Activity on Quality of Life During Pregnancy: A Causal ML Approach (Kianoosh Kazemi et al., 2024)</a></li><li><a href=#1925--60122-spectrum-extraction-and-clipping-for-implicitly-linear-layers-ali-ebrahimpour-boroojeny-et-al-2024>(19/25 | 60/122) Spectrum Extraction and Clipping for Implicitly Linear Layers (Ali Ebrahimpour Boroojeny et al., 2024)</a></li><li><a href=#2025--61122-a-machine-learning-approach-to-detect-customer-satisfaction-from-multiple-tweet-parameters-md-mahmudul-hasan-et-al-2024>(20/25 | 61/122) A Machine Learning Approach to Detect Customer Satisfaction From Multiple Tweet Parameters (Md Mahmudul Hasan et al., 2024)</a></li><li><a href=#2125--62122-shaving-weights-with-occams-razor-bayesian-sparsification-for-neural-networks-using-the-marginal-likelihood-rayen-dhahri-et-al-2024>(21/25 | 62/122) Shaving Weights with Occam&rsquo;s Razor: Bayesian Sparsification for Neural Networks Using the Marginal Likelihood (Rayen Dhahri et al., 2024)</a></li><li><a href=#2225--63122-codream-exchanging-dreams-instead-of-models-for-federated-aggregation-with-heterogeneous-models-abhishek-singh-et-al-2024>(22/25 | 63/122) CoDream: Exchanging dreams instead of models for federated aggregation with heterogeneous models (Abhishek Singh et al., 2024)</a></li><li><a href=#2325--64122-greenllama-a-framework-for-detoxification-with-explanations-md-tawkat-islam-khondaker-et-al-2024>(23/25 | 64/122) GreenLLaMA: A Framework for Detoxification with Explanations (Md Tawkat Islam Khondaker et al., 2024)</a></li><li><a href=#2425--65122-pdetime-rethinking-long-term-multivariate-time-series-forecasting-from-the-perspective-of-partial-differential-equations-shiyi-qi-et-al-2024>(24/25 | 65/122) PDETime: Rethinking Long-Term Multivariate Time Series Forecasting from the perspective of partial differential equations (Shiyi Qi et al., 2024)</a></li><li><a href=#2525--66122-a-step-by-step-introduction-to-the-implementation-of-automatic-differentiation-yu-hsueh-fang-et-al-2024>(25/25 | 66/122) A Step-by-step Introduction to the Implementation of Automatic Differentiation (Yu-Hsueh Fang et al., 2024)</a></li></ul></li><li><a href=#cscv-18>cs.CV (18)</a><ul><li><a href=#118--67122-lstp-language-guided-spatial-temporal-prompt-learning-for-long-form-video-text-understanding-yuxuan-wang-et-al-2024>(1/18 | 67/122) LSTP: Language-guided Spatial-Temporal Prompt Learning for Long-form Video-Text Understanding (Yuxuan Wang et al., 2024)</a></li><li><a href=#218--68122-one-stage-prompt-based-continual-learning-youngeun-kim-et-al-2024>(2/18 | 68/122) One-stage Prompt-based Continual Learning (Youngeun Kim et al., 2024)</a></li><li><a href=#318--69122-stochca-a-novel-approach-for-exploiting-pretrained-models-with-cross-attention-seungwon-seo-et-al-2024>(3/18 | 69/122) StochCA: A Novel Approach for Exploiting Pretrained Models with Cross-Attention (Seungwon Seo et al., 2024)</a></li><li><a href=#418--70122-key-design-choices-in-source-free-unsupervised-domain-adaptation-an-in-depth-empirical-analysis-andrea-maracani-et-al-2024>(4/18 | 70/122) Key Design Choices in Source-Free Unsupervised Domain Adaptation: An In-depth Empirical Analysis (Andrea Maracani et al., 2024)</a></li><li><a href=#518--71122-cross-resolution-land-cover-classification-using-outdated-products-and-transformers-huan-ni-et-al-2024>(5/18 | 71/122) Cross-Resolution Land Cover Classification Using Outdated Products and Transformers (Huan Ni et al., 2024)</a></li><li><a href=#618--72122-unmasking-dementia-detection-by-masking-input-gradients-a-jsm-approach-to-model-interpretability-and-precision-yasmine-mustafa-et-al-2024>(6/18 | 72/122) Unmasking Dementia Detection by Masking Input Gradients: A JSM Approach to Model Interpretability and Precision (Yasmine Mustafa et al., 2024)</a></li><li><a href=#718--73122-task-specific-pretraining-with-noisy-labels-for-remote-sensing-image-segmentation-chenying-liu-et-al-2024>(7/18 | 73/122) Task Specific Pretraining with Noisy Labels for Remote sensing Image Segmentation (Chenying Liu et al., 2024)</a></li><li><a href=#818--74122-avi-talking-learning-audio-visual-instructions-for-expressive-3d-talking-face-generation-yasheng-sun-et-al-2024>(8/18 | 74/122) AVI-Talking: Learning Audio-Visual Instructions for Expressive 3D Talking Face Generation (Yasheng Sun et al., 2024)</a></li><li><a href=#918--75122-towards-accurate-post-training-quantization-for-reparameterized-models-luoming-zhang-et-al-2024>(9/18 | 75/122) Towards Accurate Post-training Quantization for Reparameterized Models (Luoming Zhang et al., 2024)</a></li><li><a href=#1018--76122-adversarial-robust-transfer-learning-for-medical-imaging-via-domain-assimilation-xiaohui-chen-et-al-2024>(10/18 | 76/122) Adversarial-Robust Transfer Learning for Medical Imaging via Domain Assimilation (Xiaohui Chen et al., 2024)</a></li><li><a href=#1118--77122-towards-robust-image-stitching-an-adaptive-resistance-learning-against-compatible-attacks-zhiying-jiang-et-al-2024>(11/18 | 77/122) Towards Robust Image Stitching: An Adaptive Resistance Learning against Compatible Attacks (Zhiying Jiang et al., 2024)</a></li><li><a href=#1218--78122-gennbv-generalizable-next-best-view-policy-for-active-3d-reconstruction-xiao-chen-et-al-2024>(12/18 | 78/122) GenNBV: Generalizable Next-Best-View Policy for Active 3D Reconstruction (Xiao Chen et al., 2024)</a></li><li><a href=#1318--79122-deep-homography-estimation-for-visual-place-recognition-feng-lu-et-al-2024>(13/18 | 79/122) Deep Homography Estimation for Visual Place Recognition (Feng Lu et al., 2024)</a></li><li><a href=#1418--80122-diving-deep-into-regions-exploiting-regional-information-transformer-for-single-image-deraining-baiang-li-et-al-2024>(14/18 | 80/122) Diving Deep into Regions: Exploiting Regional Information Transformer for Single Image Deraining (Baiang Li et al., 2024)</a></li><li><a href=#1518--81122-semi-supervised-open-world-object-detection-sahal-shaji-mullappilly-et-al-2024>(15/18 | 81/122) Semi-supervised Open-World Object Detection (Sahal Shaji Mullappilly et al., 2024)</a></li><li><a href=#1618--82122-an-image-enhancement-method-for-improving-small-intestinal-villi-clarity-shaojie-zhang-et-al-2024>(16/18 | 82/122) An Image Enhancement Method for Improving Small Intestinal Villi Clarity (Shaojie Zhang et al., 2024)</a></li><li><a href=#1718--83122-voloc-visual-place-recognition-by-querying-compressed-lidar-map-xudong-cai-et-al-2024>(17/18 | 83/122) VOLoc: Visual Place Recognition by Querying Compressed Lidar Map (Xudong Cai et al., 2024)</a></li><li><a href=#1818--84122-vistec-video-modeling-for-sports-technique-recognition-and-tactical-analysis-yuchen-he-et-al-2024>(18/18 | 84/122) ViSTec: Video Modeling for Sports Technique Recognition and Tactical Analysis (Yuchen He et al., 2024)</a></li></ul></li><li><a href=#csro-4>cs.RO (4)</a><ul><li><a href=#14--85122-robocodex-multimodal-code-generation-for-robotic-behavior-synthesis-yao-mu-et-al-2024>(1/4 | 85/122) RoboCodeX: Multimodal Code Generation for Robotic Behavior Synthesis (Yao Mu et al., 2024)</a></li><li><a href=#24--86122-harnessing-the-synergy-between-pushing-grasping-and-throwing-to-enhance-object-manipulation-in-cluttered-scenarios-hamidreza-kasaei-et-al-2024>(2/4 | 86/122) Harnessing the Synergy between Pushing, Grasping, and Throwing to Enhance Object Manipulation in Cluttered Scenarios (Hamidreza Kasaei et al., 2024)</a></li><li><a href=#34--87122-iklink-end-effector-trajectory-tracking-with-minimal-reconfigurations-yeping-wang-et-al-2024>(3/4 | 87/122) IKLink: End-Effector Trajectory Tracking with Minimal Reconfigurations (Yeping Wang et al., 2024)</a></li><li><a href=#44--88122-optimizing-base-placement-of-surgical-robot-kinematics-data-driven-approach-by-analyzing-working-pattern-jeonghyeon-yoon-et-al-2024>(4/4 | 88/122) Optimizing Base Placement of Surgical Robot: Kinematics Data-Driven Approach by Analyzing Working Pattern (Jeonghyeon Yoon et al., 2024)</a></li></ul></li><li><a href=#csse-5>cs.SE (5)</a><ul><li><a href=#15--89122-rethinking-software-engineering-in-the-era-of-foundation-models-a-curated-catalogue-of-challenges-in-the-development-of-trustworthy-fmware-ahmed-e-hassan-et-al-2024>(1/5 | 89/122) Rethinking Software Engineering in the Era of Foundation Models: A Curated Catalogue of Challenges in the Development of Trustworthy FMware (Ahmed E. Hassan et al., 2024)</a></li><li><a href=#25--90122-ldb-a-large-language-model-debugger-via-verifying-runtime-execution-step-by-step-li-zhong-et-al-2024>(2/5 | 90/122) LDB: A Large Language Model Debugger via Verifying Runtime Execution Step-by-step (Li Zhong et al., 2024)</a></li><li><a href=#35--91122-nesy-is-alive-and-well-a-llm-driven-symbolic-approach-for-better-code-comment-data-generation-and-classification-hanna-abi-akl-2024>(3/5 | 91/122) NeSy is alive and well: A LLM-driven symbolic approach for better code comment data generation and classification (Hanna Abi Akl, 2024)</a></li><li><a href=#45--92122-language-models-for-code-completion-a-practical-evaluation-maliheh-izadi-et-al-2024>(4/5 | 92/122) Language Models for Code Completion: A Practical Evaluation (Maliheh Izadi et al., 2024)</a></li><li><a href=#55--93122-an-empirical-study-of-challenges-in-machine-learning-asset-management-zhimin-zhao-et-al-2024>(5/5 | 93/122) An Empirical Study of Challenges in Machine Learning Asset Management (Zhimin Zhao et al., 2024)</a></li></ul></li><li><a href=#eessiv-2>eess.IV (2)</a><ul><li><a href=#12--94122-integrating-preprocessing-methods-and-convolutional-neural-networks-for-effective-tumor-detection-in-medical-imaging-ha-anh-vu-2024>(1/2 | 94/122) Integrating Preprocessing Methods and Convolutional Neural Networks for Effective Tumor Detection in Medical Imaging (Ha Anh Vu, 2024)</a></li><li><a href=#22--95122-diffusion-posterior-proximal-sampling-for-image-restoration-hongjie-wu-et-al-2024>(2/2 | 95/122) Diffusion Posterior Proximal Sampling for Image Restoration (Hongjie Wu et al., 2024)</a></li></ul></li><li><a href=#cssi-2>cs.SI (2)</a><ul><li><a href=#12--96122-towards-fair-graph-anomaly-detection-problem-new-datasets-and-evaluation-neng-kai-nigel-neo-et-al-2024>(1/2 | 96/122) Towards Fair Graph Anomaly Detection: Problem, New Datasets, and Evaluation (Neng Kai Nigel Neo et al., 2024)</a></li><li><a href=#22--97122-signed-graph-representation-learning-a-survey-zeyu-zhang-et-al-2024>(2/2 | 97/122) Signed Graph Representation Learning: A Survey (Zeyu Zhang et al., 2024)</a></li></ul></li><li><a href=#csit-5>cs.IT (5)</a><ul><li><a href=#15--98122-hpe-transformer-learning-to-optimize-multi-group-multicast-beamforming-under-nonconvex-qos-constraints-yang-li-et-al-2024>(1/5 | 98/122) HPE Transformer: Learning to Optimize Multi-Group Multicast Beamforming Under Nonconvex QoS Constraints (Yang Li et al., 2024)</a></li><li><a href=#25--99122-enhancing-xurllc-with-rsma-assisted-massive-mimo-networks-performance-analysis-and-optimization-yuang-chen-et-al-2024>(2/5 | 99/122) Enhancing xURLLC with RSMA-Assisted Massive-MIMO Networks: Performance Analysis and Optimization (Yuang Chen et al., 2024)</a></li><li><a href=#35--100122-on-a-class-of-greedy-sparse-recovery-algorithms----a-high-dimensional-approach-gang-li-et-al-2024>(3/5 | 100/122) On A Class of Greedy Sparse Recovery Algorithms &ndash; A High Dimensional Approach (Gang Li et al., 2024)</a></li><li><a href=#45--101122-molecular-code-division-multiple-access-signaling-detection-and-performance-weidong-gao-et-al-2024>(4/5 | 101/122) Molecular Code-Division Multiple-Access: Signaling, Detection, and Performance (Weidong Gao et al., 2024)</a></li><li><a href=#55--102122-multi-access-distributed-computing-models-from-map-reduce-arrays-shanuja-sasi-et-al-2024>(5/5 | 102/122) Multi-access Distributed Computing Models from Map-Reduce Arrays (Shanuja Sasi et al., 2024)</a></li></ul></li><li><a href=#statml-1>stat.ML (1)</a><ul><li><a href=#11--103122-distribution-free-fair-federated-learning-with-small-samples-qichuan-yin-et-al-2024>(1/1 | 103/122) Distribution-Free Fair Federated Learning with Small Samples (Qichuan Yin et al., 2024)</a></li></ul></li><li><a href=#csce-1>cs.CE (1)</a><ul><li><a href=#11--104122-accurate-predictions-of-keyhole-depths-using-machine-learning-aided-simulations-jiahui-zhang-et-al-2024>(1/1 | 104/122) Accurate predictions of keyhole depths using machine learning-aided simulations (Jiahui Zhang et al., 2024)</a></li></ul></li><li><a href=#csgr-1>cs.GR (1)</a><ul><li><a href=#11--105122-cinematographic-camera-diffusion-model-hongda-jiang-et-al-2024>(1/1 | 105/122) Cinematographic Camera Diffusion Model (Hongda Jiang et al., 2024)</a></li></ul></li><li><a href=#csai-2>cs.AI (2)</a><ul><li><a href=#12--106122-pidformer-transformer-meets-control-theory-tam-nguyen-et-al-2024>(1/2 | 106/122) PIDformer: Transformer Meets Control Theory (Tam Nguyen et al., 2024)</a></li><li><a href=#22--107122-budget-constrained-tool-learning-with-planning-yuanhang-zheng-et-al-2024>(2/2 | 107/122) Budget-Constrained Tool Learning with Planning (Yuanhang Zheng et al., 2024)</a></li></ul></li><li><a href=#csar-2>cs.AR (2)</a><ul><li><a href=#12--108122-sustainable-supercomputing-for-ai-gpu-power-capping-at-hpc-scale-dan-zhao-et-al-2024>(1/2 | 108/122) Sustainable Supercomputing for AI: GPU Power Capping at HPC Scale (Dan Zhao et al., 2024)</a></li><li><a href=#22--109122-encodingnet-a-novel-encoding-based-mac-design-for-efficient-neural-network-acceleration-bo-liu-et-al-2024>(2/2 | 109/122) EncodingNet: A Novel Encoding-based MAC Design for Efficient Neural Network Acceleration (Bo Liu et al., 2024)</a></li></ul></li><li><a href=#csni-2>cs.NI (2)</a><ul><li><a href=#12--110122-learning-with-semantics-towards-a-semantics-aware-routing-anomaly-detection-system-yihao-chen-et-al-2024>(1/2 | 110/122) Learning with Semantics: Towards a Semantics-Aware Routing Anomaly Detection System (Yihao Chen et al., 2024)</a></li><li><a href=#22--111122-communication-traffic-characteristics-reveal-an-iot-devices-identity-rajarshi-roy-chowdhury-et-al-2024>(2/2 | 111/122) Communication Traffic Characteristics Reveal an IoT Devices Identity (Rajarshi Roy Chowdhury et al., 2024)</a></li></ul></li><li><a href=#csne-1>cs.NE (1)</a><ul><li><a href=#11--112122-efficient-online-learning-for-networks-of-two-compartment-spiking-neurons-yujia-yin-et-al-2024>(1/1 | 112/122) Efficient Online Learning for Networks of Two-Compartment Spiking Neurons (Yujia Yin et al., 2024)</a></li></ul></li><li><a href=#csgt-1>cs.GT (1)</a><ul><li><a href=#11--113122-egalitarian-price-of-fairness-for-indivisible-goods-karen-frilya-celine-et-al-2024>(1/1 | 113/122) Egalitarian Price of Fairness for Indivisible Goods (Karen Frilya Celine et al., 2024)</a></li></ul></li><li><a href=#cshc-1>cs.HC (1)</a><ul><li><a href=#11--114122-towards-mixed-reality-as-the-everyday-computing-paradigm-challenges--design-recommendations-amir-reza-asadi-et-al-2024>(1/1 | 114/122) Towards Mixed Reality as the Everyday Computing Paradigm: Challenges & Design Recommendations (Amir Reza Asadi et al., 2024)</a></li></ul></li><li><a href=#csdb-1>cs.DB (1)</a><ul><li><a href=#11--115122-convolution-and-cross-correlation-of-count-sketches-enables-fast-cardinality-estimation-of-multi-join-queries-mike-heddes-et-al-2024>(1/1 | 115/122) Convolution and Cross-Correlation of Count Sketches Enables Fast Cardinality Estimation of Multi-Join Queries (Mike Heddes et al., 2024)</a></li></ul></li><li><a href=#csdl-1>cs.DL (1)</a><ul><li><a href=#11--116122-pst-bench-tracing-and-benchmarking-the-source-of-publications-fanjin-zhang-et-al-2024>(1/1 | 116/122) PST-Bench: Tracing and Benchmarking the Source of Publications (Fanjin Zhang et al., 2024)</a></li></ul></li><li><a href=#mathap-1>math.AP (1)</a><ul><li><a href=#11--117122-direct-and-inverse-scattering-in-a-three-dimensional-planar-waveguide-yan-chang-et-al-2024>(1/1 | 117/122) Direct and Inverse scattering in a three-dimensional planar waveguide (Yan Chang et al., 2024)</a></li></ul></li><li><a href=#mathco-2>math.CO (2)</a><ul><li><a href=#12--118122-branch-depth-is-minor-closure-of-contraction-deletion-depth-marcin-briański-et-al-2024>(1/2 | 118/122) Branch-depth is minor closure of contraction-deletion-depth (Marcin Briański et al., 2024)</a></li><li><a href=#22--119122-list-coloring-of-some-cayley-graphs-using-kernel-perfections-prajnanaswaroopa-s-2024>(2/2 | 119/122) List Coloring of some Cayley graphs using Kernel perfections (Prajnanaswaroopa S, 2024)</a></li></ul></li><li><a href=#csds-1>cs.DS (1)</a><ul><li><a href=#11--120122-enhanced-graph-pattern-matching-nicola-cotumaccio-2024>(1/1 | 120/122) Enhanced Graph Pattern Matching (Nicola Cotumaccio, 2024)</a></li></ul></li><li><a href=#cslo-1>cs.LO (1)</a><ul><li><a href=#11--121122-effective-mso-definability-for-tree-width-bounded-models-of-an-inductive-separation-logic-of-relations-lucas-bueri-et-al-2024>(1/1 | 121/122) Effective MSO-Definability for Tree-width Bounded Models of an Inductive Separation Logic of Relations (Lucas Bueri et al., 2024)</a></li></ul></li><li><a href=#q-fincp-1>q-fin.CP (1)</a><ul><li><a href=#11--122122-optimizing-portfolio-management-and-risk-assessment-in-digital-assets-using-deep-learning-for-predictive-analysis-qishuo-cheng-et-al-2024>(1/1 | 122/122) Optimizing Portfolio Management and Risk Assessment in Digital Assets Using Deep Learning for Predictive Analysis (Qishuo Cheng et al., 2024)</a></li></ul></li></ul></nav></div></div></section></div><footer id=footer class="container-fluid text-center align-content-center footer pb-2"><div class="container pt-5"><div class="row text-left"><div class="col-md-4 col-sm-12"><h5>Navigation</h5><ul><li class=nav-item><a class=smooth-scroll href=https://akitenkrad.github.io/akitenkrad-blog/#about>About</a></li><li class=nav-item><a class=smooth-scroll href=https://akitenkrad.github.io/akitenkrad-blog/#recent-posts>Recent Posts</a></li></ul></div><div class="col-md-4 col-sm-12"><h5>Contact me:</h5><ul><li><a href=mailto:contact.to.akitenkrad@gmail.com target=_blank rel=noopener><span><i class="fas fa-envelope"></i></span> <span>contact.to.akitenkrad@gmail.com</span></a></li></ul></div></div></div><hr><div class=container><div class="row text-left"><div class=col-md-4><a id=theme href=https://github.com/hossainemruz/toha target=_blank rel=noopener><img src=/akitenkrad-blog/images/theme-logo_hu8376fd15465fef26ffe66b6bcf0ca686_13669_32x0_resize_box_3.png alt="Toha Theme Logo">
Toha</a></div><div class="col-md-4 text-center">© 2020 Akitenkrad.</div><div class="col-md-4 text-right"><a id=hugo href=https://gohugo.io/ target=_blank rel=noopener>Powered by
<img src=/akitenkrad-blog/images/hugo-logo.svg alt="Hugo Logo" height=18></a></div></div></div></footer><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/jquery-3.4.1.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/popper.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/bootstrap.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/navbar.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/plyr.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/main.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js></script><script src=https://akitenkrad.github.io/akitenkrad-blog/js/single.js></script><script>hljs.initHighlightingOnLoad()</script><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/katex/katex.min.css><script type=text/javascript defer src=https://akitenkrad.github.io/akitenkrad-blog/katex/katex.min.js></script><script type=text/javascript defer src=https://akitenkrad.github.io/akitenkrad-blog/katex/auto-render.min.js></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\[",right:"\\]",display:!0},{left:"\\(",right:"\\)",display:!1}],throwOnError:!0})})</script></body></html>