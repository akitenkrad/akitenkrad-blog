<!doctype html><html><head><title>arXiv @ 2024.02.26</title>
<meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/bootstrap.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/layouts/main.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/navigators/navbar.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/plyr.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/flag-icon.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/custom/style.css><link href=https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css rel=stylesheet integrity=sha384-9ndCyUaIbzAi2FUVXJi0CjmCapSmO7SnpJef0486qhLnuZ2cdeRhO02iuK6FUUVM crossorigin=anonymous><script async src=https://cdn.jsdelivr.net/npm/es-module-shims@1/dist/es-module-shims.min.js crossorigin=anonymous></script><script type=importmap>
{
  "imports": {
    "@popperjs/core": "https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.8/dist/esm/popper.min.js",
    "bootstrap": "https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.esm.min.js"
  }
}
</script><script src=https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js integrity=sha384-geWF76RCwLtnZ8qwWowPQNguL3RmwHVBC9FhGdlKrxdiJJigb/j/68SIy3Te4Bkz crossorigin=anonymous></script><script type=module src=https://akitenkrad.github.io/akitenkrad-blog/js/custom/custom.js></script><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Muli:wght@300;400;500;600"><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/fontawesome/css/all.min.css><link rel=icon type=image/png href=https://akitenkrad.github.io/akitenkrad-blog/images/favicons/favicon-96x96_huf1ee13f0caf27d1547f91fb46207d708_13005_42x0_resize_box_3.png><meta property="og:title" content="arXiv @ 2024.02.26"><meta property="og:description" content="Primary Categories cs.AI (6) cs.CG (1) cs.CL (21) cs.CR (5) cs.CV (22) cs.CY (1) cs.DC (1) cs.DL (1) cs.DM (1) cs.DS (2) cs.GT (1) cs.HC (3) cs.IR (2) cs.IT (1) cs.LG (26) cs.LO (1) cs.NI (2) cs.PL (1) cs.RO (2) cs.SD (1) cs.SE (2) cs.SI (1) econ.TH (1) eess.IV (2) eess.SP (1) eess.SY (4) math.CO (1) math.NA (1) q-bio.GN (1) quant-ph (1) stat.ML (1) Keywords keyword cs.CL cs.CV cs.LG Active Learning 1 Adversarial Attack 1 1 Autoencoder 1 BERT 1 Bandit Algorithm 2 Benchmarking 5 7 3 Black Box 3 ChatGPT 4 1 Clustering 3 Continuous Time 2 Contrastive Learning 1 3 1 Convolution 3 1 Convolutional Neural Network 6 Data Augmentation 2 Dense Retrieval 1 Diffusion Model 1 Document Ranking 1 Emotion Recognition 1 Essay Scoring 1 Explainable AI 1 Federated Learning 1 1 Few-shot 4 1 Few-shot Learning 2 1 Fine-tuning 9 6 4 Foundation Model 1 GPT 2 1 GPT-3 1 GPT-3."><meta property="og:type" content="article"><meta property="og:url" content="https://akitenkrad.github.io/akitenkrad-blog/posts/arxiv/202402/20240226000000/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-02-26T00:00:00+00:00"><meta property="article:modified_time" content="2024-02-26T00:00:00+00:00"><meta name=description content="arXiv @ 2024.02.26"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/atom-one-dark.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/layouts/single.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/navigators/sidebar.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/style.css><script type=text/javascript src=//ajax.googleapis.com/ajax/libs/jquery/3.1.1/jquery.min.js></script><script language=JavaScript>$(document).ready(function(){$("a[href^='http']:not([href*='"+location.hostname+"'])").attr("target","_blank")})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-1MYYZQG0WE"></script><script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-1MYYZQG0WE",{anonymize_ip:!1})}</script></head><body data-spy=scroll data-target=#TableOfContents data-offset=80><div class="container-fluid bg-dimmed wrapper"><nav class="navbar navbar-expand-xl top-navbar final-navbar shadow" style=position:fixed><div class=container><button class="navbar-toggler navbar-light navbar-for-sidebar" id=sidebar-toggler type=button onclick=toggleSidebar()>
<span class=navbar-toggler-icon></span>
</button>
<button class="navbar-toggler navbar-light" id=toc-toggler type=button onclick=toggleTOC()>
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse lang-selector" id=top-nav-items><ul class="navbar-nav mr-auto"><li class=nav-item><a class=navbar-brand href=/akitenkrad-blog><img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png alt=Logo>
Akitenkrad's Blog</a></li></ul><ul class="navbar-nav ml-auto"><li class=nav-item><a class=nav-link href=/akitenkrad-blog#home>Home</a></li><li class=nav-item><a class=nav-link href=/akitenkrad-blog#about>About</a></li><li class=nav-item><a class=nav-link href=/akitenkrad-blog#recent-posts>Recent Posts</a></li><li class=nav-item><a class=nav-link id=blog-link href=https://akitenkrad.github.io/akitenkrad-blog/posts>Posts</a></li><li class=nav-item><a class=nav-link id=tag-link href=https://akitenkrad.github.io/akitenkrad-blog/tags>Tags</a></li></ul></div></div><img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png class=d-none id=main-logo alt=Logo>
<img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png class=d-none id=inverted-logo alt="Inverted Logo"></nav><section class=sidebar-section id=sidebar-section><div class=sidebar-holder><div class=sidebar id=sidebar><form class=mx-auto method=get action=/akitenkrad-blog/search><input type=text name=keyword placeholder=Search data-search id=search-box></form><div class=sidebar-tree><ul class=tree id=tree><li id=list-heading><a href=/posts data-filter=all>Posts</a></li><div class=subtree><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/>Papers</a><ul><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202205/>2022.05</a><ul><li><a href=/akitenkrad-blog/posts/papers/202205/20220518224923/ title="A Context-Aware Citation Recommendation Model with BERT and Graph Convolutional Networks">A Context-Aware Citation Recommendation Model with BERT and Graph Convolutional Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220508162318/ title="A Deep Cascade Model for Multi-Document Reading Comprehension">A Deep Cascade Model for Multi-Document Reading Comprehension</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220506021208/ title="A Primer in BERTology: What We Know About How BERT Works">A Primer in BERTology: What We Know About How BERT Works</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220529131339/ title="Attention Is All You Need">Attention Is All You Need</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220503010000/ title="DeBERTa: Decoding-Enhanced BERT with Disentangled Attention">DeBERTa: Decoding-Enhanced BERT with Disentangled Attention</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220505222900/ title="Dense Passage Retrieval for Open-Domain Question Answering">Dense Passage Retrieval for Open-Domain Question Answering</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220511010217/ title="Multi-Style Generative Reading Comprehension">Multi-Style Generative Reading Comprehension</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220530102936/ title="Neural Machine Translation of Rare Words with Subword Units">Neural Machine Translation of Rare Words with Subword Units</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220523223206/ title="RoBERTa: A Robustly Optimized BERT Pretraining Approach">RoBERTa: A Robustly Optimized BERT Pretraining Approach</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220520124748/ title="Semi-Supervised Classification with Graph Convolutional Networks">Semi-Supervised Classification with Graph Convolutional Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220509110738/ title="Survey on graph embeddings and their applications to machine learning problems on graphs">Survey on graph embeddings and their applications to machine learning problems on graphs</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220514151839/ title="UnitedQA: A Hybrid Approach for Open Domain Question Answering">UnitedQA: A Hybrid Approach for Open Domain Question Answering</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202206/>2022.06</a><ul><li><a href=/akitenkrad-blog/posts/papers/202206/20220612105422/ title="Attributed Network Embedding for Learning in a Dynamic Environment">Attributed Network Embedding for Learning in a Dynamic Environment</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220608085622/ title="CodeBERT: A Pre-Trained Model for Programming and Natural Languages">CodeBERT: A Pre-Trained Model for Programming and Natural Languages</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220618223844/ title="High-order Proximity Preserved Embedding for Dynamic Networks">High-order Proximity Preserved Embedding for Dynamic Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220602171700/ title="S-Net: From Answer Extraction to Answer Generation for Machine Reading Comprehension">S-Net: From Answer Extraction to Answer Generation for Machine Reading Comprehension</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202207/>2022.07</a><ul><li><a href=/akitenkrad-blog/posts/papers/202207/20220727145036/ title="Dynamic Heterogeneous Graph Embedding Using Hierarchical Attentions">Dynamic Heterogeneous Graph Embedding Using Hierarchical Attentions</a></li><li><a href=/akitenkrad-blog/posts/papers/202207/20220726163444/ title="Dynamic Network Embedding Survey">Dynamic Network Embedding Survey</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202208/>2022.08</a><ul><li><a href=/akitenkrad-blog/posts/papers/202208/20220802103319/ title="Modeling Dynamic Heterogeneous Network for Link Prediction using Hierarchical Attention with Temporal RNN">Modeling Dynamic Heterogeneous Network for Link Prediction using Hierarchical Attention with Temporal RNN</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202209/>2022.09</a><ul><li><a href=/akitenkrad-blog/posts/papers/202209/20220909180827/ title="Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting">Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202305/>2023.05</a><ul><li><a href=/akitenkrad-blog/posts/papers/202305/20230513094804/ title="Improving Language Understanding by Generative Pre-Training">Improving Language Understanding by Generative Pre-Training</a></li><li><a href=/akitenkrad-blog/posts/papers/202305/20230514200241/ title="Language Models are Unsupervised Multitask Learners">Language Models are Unsupervised Multitask Learners</a></li></ul></li><li><a href=/akitenkrad-blog/posts/papers/202306/ title=2023.06>2023.06</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/akitenkrad-blog/posts/arxiv/>arXiv</a><ul class=active><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202307/>2023.07</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230701000000/ title="arXiv @ 2023.07.01">arXiv @ 2023.07.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230702000000/ title="arXiv @ 2023.07.02">arXiv @ 2023.07.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230703000000/ title="arXiv @ 2023.07.03">arXiv @ 2023.07.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230704000000/ title="arXiv @ 2023.07.04">arXiv @ 2023.07.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230705000000/ title="arXiv @ 2023.07.05">arXiv @ 2023.07.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230706000000/ title="arXiv @ 2023.07.06">arXiv @ 2023.07.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230707000000/ title="arXiv @ 2023.07.07">arXiv @ 2023.07.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230708000000/ title="arXiv @ 2023.07.08">arXiv @ 2023.07.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230709000000/ title="arXiv @ 2023.07.09">arXiv @ 2023.07.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230710000000/ title="arXiv @ 2023.07.10">arXiv @ 2023.07.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230711000000/ title="arXiv @ 2023.07.11">arXiv @ 2023.07.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230712000000/ title="arXiv @ 2023.07.12">arXiv @ 2023.07.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230713000000/ title="arXiv @ 2023.07.13">arXiv @ 2023.07.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230714000000/ title="arXiv @ 2023.07.14">arXiv @ 2023.07.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230715000000/ title="arXiv @ 2023.07.15">arXiv @ 2023.07.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230716000000/ title="arXiv @ 2023.07.16">arXiv @ 2023.07.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230717000000/ title="arXiv @ 2023.07.17">arXiv @ 2023.07.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230718000000/ title="arXiv @ 2023.07.18">arXiv @ 2023.07.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230719000000/ title="arXiv @ 2023.07.19">arXiv @ 2023.07.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230720000000/ title="arXiv @ 2023.07.20">arXiv @ 2023.07.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230721000000/ title="arXiv @ 2023.07.21">arXiv @ 2023.07.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230722000000/ title="arXiv @ 2023.07.22">arXiv @ 2023.07.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230723000000/ title="arXiv @ 2023.07.23">arXiv @ 2023.07.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230724000000/ title="arXiv @ 2023.07.24">arXiv @ 2023.07.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230725000000/ title="arXiv @ 2023.07.25">arXiv @ 2023.07.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230726000000/ title="arXiv @ 2023.07.26">arXiv @ 2023.07.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230727000000/ title="arXiv @ 2023.07.27">arXiv @ 2023.07.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230728000000/ title="arXiv @ 2023.07.28">arXiv @ 2023.07.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230729000000/ title="arXiv @ 2023.07.29">arXiv @ 2023.07.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230730000000/ title="arXiv @ 2023.07.30">arXiv @ 2023.07.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230731000000/ title="arXiv @ 2023.07.31">arXiv @ 2023.07.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202308/>2023.08</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230801000000/ title="arXiv @ 2023.08.01">arXiv @ 2023.08.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230802000000/ title="arXiv @ 2023.08.02">arXiv @ 2023.08.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230803000000/ title="arXiv @ 2023.08.03">arXiv @ 2023.08.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230804000000/ title="arXiv @ 2023.08.04">arXiv @ 2023.08.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230805000000/ title="arXiv @ 2023.08.05">arXiv @ 2023.08.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230806000000/ title="arXiv @ 2023.08.06">arXiv @ 2023.08.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230807000000/ title="arXiv @ 2023.08.07">arXiv @ 2023.08.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230808000000/ title="arXiv @ 2023.08.08">arXiv @ 2023.08.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230809000000/ title="arXiv @ 2023.08.09">arXiv @ 2023.08.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230810000000/ title="arXiv @ 2023.08.10">arXiv @ 2023.08.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230811000000/ title="arXiv @ 2023.08.11">arXiv @ 2023.08.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230812000000/ title="arXiv @ 2023.08.12">arXiv @ 2023.08.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230813000000/ title="arXiv @ 2023.08.13">arXiv @ 2023.08.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230814000000/ title="arXiv @ 2023.08.14">arXiv @ 2023.08.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230815000000/ title="arXiv @ 2023.08.15">arXiv @ 2023.08.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230816000000/ title="arXiv @ 2023.08.16">arXiv @ 2023.08.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230817000000/ title="arXiv @ 2023.08.17">arXiv @ 2023.08.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230818000000/ title="arXiv @ 2023.08.18">arXiv @ 2023.08.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230819000000/ title="arXiv @ 2023.08.19">arXiv @ 2023.08.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230820000000/ title="arXiv @ 2023.08.20">arXiv @ 2023.08.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230821000000/ title="arXiv @ 2023.08.21">arXiv @ 2023.08.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230822000000/ title="arXiv @ 2023.08.22">arXiv @ 2023.08.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230823000000/ title="arXiv @ 2023.08.23">arXiv @ 2023.08.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230824000000/ title="arXiv @ 2023.08.24">arXiv @ 2023.08.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230825000000/ title="arXiv @ 2023.08.25">arXiv @ 2023.08.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230826000000/ title="arXiv @ 2023.08.26">arXiv @ 2023.08.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230827000000/ title="arXiv @ 2023.08.27">arXiv @ 2023.08.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230828000000/ title="arXiv @ 2023.08.28">arXiv @ 2023.08.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230829000000/ title="arXiv @ 2023.08.29">arXiv @ 2023.08.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230830000000/ title="arXiv @ 2023.08.30">arXiv @ 2023.08.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230831000000/ title="arXiv @ 2023.08.31">arXiv @ 2023.08.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202309/>2023.09</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230901000000/ title="arXiv @ 2023.09.01">arXiv @ 2023.09.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230902000000/ title="arXiv @ 2023.09.02">arXiv @ 2023.09.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230903000000/ title="arXiv @ 2023.09.03">arXiv @ 2023.09.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230904000000/ title="arXiv @ 2023.09.04">arXiv @ 2023.09.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230905000000/ title="arXiv @ 2023.09.05">arXiv @ 2023.09.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230906000000/ title="arXiv @ 2023.09.06">arXiv @ 2023.09.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230907000000/ title="arXiv @ 2023.09.07">arXiv @ 2023.09.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230908000000/ title="arXiv @ 2023.09.08">arXiv @ 2023.09.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230909000000/ title="arXiv @ 2023.09.09">arXiv @ 2023.09.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230910000000/ title="arXiv @ 2023.09.10">arXiv @ 2023.09.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230911000000/ title="arXiv @ 2023.09.11">arXiv @ 2023.09.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230912000000/ title="arXiv @ 2023.09.12">arXiv @ 2023.09.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230913000000/ title="arXiv @ 2023.09.13">arXiv @ 2023.09.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230914000000/ title="arXiv @ 2023.09.14">arXiv @ 2023.09.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230915000000/ title="arXiv @ 2023.09.15">arXiv @ 2023.09.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230916000000/ title="arXiv @ 2023.09.16">arXiv @ 2023.09.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230917000000/ title="arXiv @ 2023.09.17">arXiv @ 2023.09.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230918000000/ title="arXiv @ 2023.09.18">arXiv @ 2023.09.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230919000000/ title="arXiv @ 2023.09.19">arXiv @ 2023.09.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230920000000/ title="arXiv @ 2023.09.20">arXiv @ 2023.09.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230921000000/ title="arXiv @ 2023.09.21">arXiv @ 2023.09.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230922000000/ title="arXiv @ 2023.09.22">arXiv @ 2023.09.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230923000000/ title="arXiv @ 2023.09.23">arXiv @ 2023.09.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230924000000/ title="arXiv @ 2023.09.24">arXiv @ 2023.09.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230925000000/ title="arXiv @ 2023.09.25">arXiv @ 2023.09.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230926000000/ title="arXiv @ 2023.09.26">arXiv @ 2023.09.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230927000000/ title="arXiv @ 2023.09.27">arXiv @ 2023.09.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230928000000/ title="arXiv @ 2023.09.28">arXiv @ 2023.09.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230929000000/ title="arXiv @ 2023.09.29">arXiv @ 2023.09.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230930000000/ title="arXiv @ 2023.09.30">arXiv @ 2023.09.30</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202310/>2023.1</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231001000000/ title="arXiv @ 2023.10.01">arXiv @ 2023.10.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231002000000/ title="arXiv @ 2023.10.02">arXiv @ 2023.10.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231003000000/ title="arXiv @ 2023.10.03">arXiv @ 2023.10.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231004000000/ title="arXiv @ 2023.10.04">arXiv @ 2023.10.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231005000000/ title="arXiv @ 2023.10.05">arXiv @ 2023.10.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231006000000/ title="arXiv @ 2023.10.06">arXiv @ 2023.10.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231007000000/ title="arXiv @ 2023.10.07">arXiv @ 2023.10.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231008000000/ title="arXiv @ 2023.10.08">arXiv @ 2023.10.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231009000000/ title="arXiv @ 2023.10.09">arXiv @ 2023.10.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231010000000/ title="arXiv @ 2023.10.10">arXiv @ 2023.10.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231011000000/ title="arXiv @ 2023.10.11">arXiv @ 2023.10.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231012000000/ title="arXiv @ 2023.10.12">arXiv @ 2023.10.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231013000000/ title="arXiv @ 2023.10.13">arXiv @ 2023.10.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231014000000/ title="arXiv @ 2023.10.14">arXiv @ 2023.10.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231015000000/ title="arXiv @ 2023.10.15">arXiv @ 2023.10.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231016000000/ title="arXiv @ 2023.10.16">arXiv @ 2023.10.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231017000000/ title="arXiv @ 2023.10.17">arXiv @ 2023.10.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231018000000/ title="arXiv @ 2023.10.18">arXiv @ 2023.10.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231019000000/ title="arXiv @ 2023.10.19">arXiv @ 2023.10.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231020000000/ title="arXiv @ 2023.10.20">arXiv @ 2023.10.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231021000000/ title="arXiv @ 2023.10.21">arXiv @ 2023.10.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231022000000/ title="arXiv @ 2023.10.22">arXiv @ 2023.10.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231023000000/ title="arXiv @ 2023.10.23">arXiv @ 2023.10.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231024000000/ title="arXiv @ 2023.10.24">arXiv @ 2023.10.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231025000000/ title="arXiv @ 2023.10.25">arXiv @ 2023.10.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231026000000/ title="arXiv @ 2023.10.26">arXiv @ 2023.10.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231027000000/ title="arXiv @ 2023.10.27">arXiv @ 2023.10.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231028000000/ title="arXiv @ 2023.10.28">arXiv @ 2023.10.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231029000000/ title="arXiv @ 2023.10.29">arXiv @ 2023.10.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231030000000/ title="arXiv @ 2023.10.30">arXiv @ 2023.10.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231031000000/ title="arXiv @ 2023.10.31">arXiv @ 2023.10.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202311/>2023.11</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231101000000/ title="arXiv @ 2023.11.01">arXiv @ 2023.11.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231102000000/ title="arXiv @ 2023.11.02">arXiv @ 2023.11.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231103000000/ title="arXiv @ 2023.11.03">arXiv @ 2023.11.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231104000000/ title="arXiv @ 2023.11.04">arXiv @ 2023.11.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231105000000/ title="arXiv @ 2023.11.05">arXiv @ 2023.11.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231106000000/ title="arXiv @ 2023.11.06">arXiv @ 2023.11.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231107000000/ title="arXiv @ 2023.11.07">arXiv @ 2023.11.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231108000000/ title="arXiv @ 2023.11.08">arXiv @ 2023.11.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231109000000/ title="arXiv @ 2023.11.09">arXiv @ 2023.11.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231110000000/ title="arXiv @ 2023.11.10">arXiv @ 2023.11.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231111000000/ title="arXiv @ 2023.11.11">arXiv @ 2023.11.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231112000000/ title="arXiv @ 2023.11.12">arXiv @ 2023.11.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231113000000/ title="arXiv @ 2023.11.13">arXiv @ 2023.11.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231114000000/ title="arXiv @ 2023.11.14">arXiv @ 2023.11.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231115000000/ title="arXiv @ 2023.11.15">arXiv @ 2023.11.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231116000000/ title="arXiv @ 2023.11.16">arXiv @ 2023.11.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231117000000/ title="arXiv @ 2023.11.17">arXiv @ 2023.11.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231118000000/ title="arXiv @ 2023.11.18">arXiv @ 2023.11.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231119000000/ title="arXiv @ 2023.11.19">arXiv @ 2023.11.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231120000000/ title="arXiv @ 2023.11.20">arXiv @ 2023.11.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231121000000/ title="arXiv @ 2023.11.21">arXiv @ 2023.11.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231122000000/ title="arXiv @ 2023.11.22">arXiv @ 2023.11.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231123000000/ title="arXiv @ 2023.11.23">arXiv @ 2023.11.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231124000000/ title="arXiv @ 2023.11.24">arXiv @ 2023.11.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231125000000/ title="arXiv @ 2023.11.25">arXiv @ 2023.11.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231126000000/ title="arXiv @ 2023.11.26">arXiv @ 2023.11.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231127000000/ title="arXiv @ 2023.11.27">arXiv @ 2023.11.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231128000000/ title="arXiv @ 2023.11.28">arXiv @ 2023.11.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231129000000/ title="arXiv @ 2023.11.29">arXiv @ 2023.11.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231130000000/ title="arXiv @ 2023.11.30">arXiv @ 2023.11.30</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202312/>2023.12</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231201000000/ title="arXiv @ 2023.12.01">arXiv @ 2023.12.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231202000000/ title="arXiv @ 2023.12.02">arXiv @ 2023.12.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231203000000/ title="arXiv @ 2023.12.03">arXiv @ 2023.12.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231204000000/ title="arXiv @ 2023.12.04">arXiv @ 2023.12.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231205000000/ title="arXiv @ 2023.12.05">arXiv @ 2023.12.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231206000000/ title="arXiv @ 2023.12.06">arXiv @ 2023.12.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231207000000/ title="arXiv @ 2023.12.07">arXiv @ 2023.12.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231208000000/ title="arXiv @ 2023.12.08">arXiv @ 2023.12.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231209000000/ title="arXiv @ 2023.12.09">arXiv @ 2023.12.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231210000000/ title="arXiv @ 2023.12.10">arXiv @ 2023.12.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231211000000/ title="arXiv @ 2023.12.11">arXiv @ 2023.12.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231212000000/ title="arXiv @ 2023.12.12">arXiv @ 2023.12.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231213000000/ title="arXiv @ 2023.12.13">arXiv @ 2023.12.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231214000000/ title="arXiv @ 2023.12.14">arXiv @ 2023.12.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231215000000/ title="arXiv @ 2023.12.15">arXiv @ 2023.12.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231216000000/ title="arXiv @ 2023.12.16">arXiv @ 2023.12.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231217000000/ title="arXiv @ 2023.12.17">arXiv @ 2023.12.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231218000000/ title="arXiv @ 2023.12.18">arXiv @ 2023.12.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231219000000/ title="arXiv @ 2023.12.19">arXiv @ 2023.12.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231220000000/ title="arXiv @ 2023.12.20">arXiv @ 2023.12.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231221000000/ title="arXiv @ 2023.12.21">arXiv @ 2023.12.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231222000000/ title="arXiv @ 2023.12.22">arXiv @ 2023.12.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231223000000/ title="arXiv @ 2023.12.23">arXiv @ 2023.12.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231224000000/ title="arXiv @ 2023.12.24">arXiv @ 2023.12.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231225000000/ title="arXiv @ 2023.12.25">arXiv @ 2023.12.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231226000000/ title="arXiv @ 2023.12.26">arXiv @ 2023.12.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231227000000/ title="arXiv @ 2023.12.27">arXiv @ 2023.12.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231228000000/ title="arXiv @ 2023.12.28">arXiv @ 2023.12.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231229000000/ title="arXiv @ 2023.12.29">arXiv @ 2023.12.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231230000000/ title="arXiv @ 2023.12.30">arXiv @ 2023.12.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231231000000/ title="arXiv @ 2023.12.31">arXiv @ 2023.12.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202401/>2024.01</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240101000000/ title="arXiv @ 2024.01.01">arXiv @ 2024.01.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240102000000/ title="arXiv @ 2024.01.02">arXiv @ 2024.01.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240103000000/ title="arXiv @ 2024.01.03">arXiv @ 2024.01.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240104000000/ title="arXiv @ 2024.01.04">arXiv @ 2024.01.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240105000000/ title="arXiv @ 2024.01.05">arXiv @ 2024.01.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240106000000/ title="arXiv @ 2024.01.06">arXiv @ 2024.01.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240107000000/ title="arXiv @ 2024.01.07">arXiv @ 2024.01.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240108000000/ title="arXiv @ 2024.01.08">arXiv @ 2024.01.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240109000000/ title="arXiv @ 2024.01.09">arXiv @ 2024.01.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240110000000/ title="arXiv @ 2024.01.10">arXiv @ 2024.01.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240111000000/ title="arXiv @ 2024.01.11">arXiv @ 2024.01.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240112000000/ title="arXiv @ 2024.01.12">arXiv @ 2024.01.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240113000000/ title="arXiv @ 2024.01.13">arXiv @ 2024.01.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240114000000/ title="arXiv @ 2024.01.14">arXiv @ 2024.01.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240115000000/ title="arXiv @ 2024.01.15">arXiv @ 2024.01.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240116000000/ title="arXiv @ 2024.01.16">arXiv @ 2024.01.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240117000000/ title="arXiv @ 2024.01.17">arXiv @ 2024.01.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240118000000/ title="arXiv @ 2024.01.18">arXiv @ 2024.01.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240119000000/ title="arXiv @ 2024.01.19">arXiv @ 2024.01.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240120000000/ title="arXiv @ 2024.01.20">arXiv @ 2024.01.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240121000000/ title="arXiv @ 2024.01.21">arXiv @ 2024.01.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240122000000/ title="arXiv @ 2024.01.22">arXiv @ 2024.01.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240123000000/ title="arXiv @ 2024.01.23">arXiv @ 2024.01.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240124000000/ title="arXiv @ 2024.01.24">arXiv @ 2024.01.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240125000000/ title="arXiv @ 2024.01.25">arXiv @ 2024.01.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240126000000/ title="arXiv @ 2024.01.26">arXiv @ 2024.01.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240127000000/ title="arXiv @ 2024.01.27">arXiv @ 2024.01.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240128000000/ title="arXiv @ 2024.01.28">arXiv @ 2024.01.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240129000000/ title="arXiv @ 2024.01.29">arXiv @ 2024.01.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240130000000/ title="arXiv @ 2024.01.30">arXiv @ 2024.01.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240131000000/ title="arXiv @ 2024.01.31">arXiv @ 2024.01.31</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/akitenkrad-blog/posts/arxiv/202402/>2024.02</a><ul class=active><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240201000000/ title="arXiv @ 2024.02.01">arXiv @ 2024.02.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240202000000/ title="arXiv @ 2024.02.02">arXiv @ 2024.02.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240203000000/ title="arXiv @ 2024.02.03">arXiv @ 2024.02.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240204000000/ title="arXiv @ 2024.02.04">arXiv @ 2024.02.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240205000000/ title="arXiv @ 2024.02.05">arXiv @ 2024.02.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240206000000/ title="arXiv @ 2024.02.06">arXiv @ 2024.02.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240207000000/ title="arXiv @ 2024.02.07">arXiv @ 2024.02.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240208000000/ title="arXiv @ 2024.02.08">arXiv @ 2024.02.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240209000000/ title="arXiv @ 2024.02.09">arXiv @ 2024.02.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240210000000/ title="arXiv @ 2024.02.10">arXiv @ 2024.02.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240211000000/ title="arXiv @ 2024.02.11">arXiv @ 2024.02.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240212000000/ title="arXiv @ 2024.02.12">arXiv @ 2024.02.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240213000000/ title="arXiv @ 2024.02.13">arXiv @ 2024.02.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240214000000/ title="arXiv @ 2024.02.14">arXiv @ 2024.02.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240215000000/ title="arXiv @ 2024.02.15">arXiv @ 2024.02.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240216000000/ title="arXiv @ 2024.02.16">arXiv @ 2024.02.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240217000000/ title="arXiv @ 2024.02.17">arXiv @ 2024.02.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240218000000/ title="arXiv @ 2024.02.18">arXiv @ 2024.02.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240219000000/ title="arXiv @ 2024.02.19">arXiv @ 2024.02.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240220000000/ title="arXiv @ 2024.02.20">arXiv @ 2024.02.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240221000000/ title="arXiv @ 2024.02.21">arXiv @ 2024.02.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240222000000/ title="arXiv @ 2024.02.22">arXiv @ 2024.02.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240223000000/ title="arXiv @ 2024.02.23">arXiv @ 2024.02.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240224000000/ title="arXiv @ 2024.02.24">arXiv @ 2024.02.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240225000000/ title="arXiv @ 2024.02.25">arXiv @ 2024.02.25</a></li><li><a class=active href=/akitenkrad-blog/posts/arxiv/202402/20240226000000/ title="arXiv @ 2024.02.26">arXiv @ 2024.02.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240227000000/ title="arXiv @ 2024.02.27">arXiv @ 2024.02.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240228000000/ title="arXiv @ 2024.02.28">arXiv @ 2024.02.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240229000000/ title="arXiv @ 2024.02.29">arXiv @ 2024.02.29</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202403/>2024.03</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240301000000/ title="arXiv @ 2024.03.01">arXiv @ 2024.03.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240302000000/ title="arXiv @ 2024.03.02">arXiv @ 2024.03.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240303000000/ title="arXiv @ 2024.03.03">arXiv @ 2024.03.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240304000000/ title="arXiv @ 2024.03.04">arXiv @ 2024.03.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240305000000/ title="arXiv @ 2024.03.05">arXiv @ 2024.03.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240306000000/ title="arXiv @ 2024.03.06">arXiv @ 2024.03.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240307000000/ title="arXiv @ 2024.03.07">arXiv @ 2024.03.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240308000000/ title="arXiv @ 2024.03.08">arXiv @ 2024.03.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240309000000/ title="arXiv @ 2024.03.09">arXiv @ 2024.03.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240310000000/ title="arXiv @ 2024.03.10">arXiv @ 2024.03.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240311000000/ title="arXiv @ 2024.03.11">arXiv @ 2024.03.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240312000000/ title="arXiv @ 2024.03.12">arXiv @ 2024.03.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240313000000/ title="arXiv @ 2024.03.13">arXiv @ 2024.03.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240314000000/ title="arXiv @ 2024.03.14">arXiv @ 2024.03.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240315000000/ title="arXiv @ 2024.03.15">arXiv @ 2024.03.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240316000000/ title="arXiv @ 2024.03.16">arXiv @ 2024.03.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240317000000/ title="arXiv @ 2024.03.17">arXiv @ 2024.03.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240318000000/ title="arXiv @ 2024.03.18">arXiv @ 2024.03.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240319000000/ title="arXiv @ 2024.03.19">arXiv @ 2024.03.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240320000000/ title="arXiv @ 2024.03.20">arXiv @ 2024.03.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240321000000/ title="arXiv @ 2024.03.21">arXiv @ 2024.03.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240322000000/ title="arXiv @ 2024.03.22">arXiv @ 2024.03.22</a></li></ul></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/algorithms/>Algorithms</a><ul><li><a href=/akitenkrad-blog/posts/algorithms/bandit-algorithm-basic/ title="Bandit Algorithm Basic">Bandit Algorithm Basic</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/conference/>Conference</a><ul><li><a href=/akitenkrad-blog/posts/conference/acl/ title="Annual Meeting of the Association for Computational Linguistics">Annual Meeting of the Association for Computational Linguistics</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/>Figures</a><ul><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/202208/>2022.08</a><ul><li><a href=/akitenkrad-blog/posts/figures/202208/20220822092748/ title=1ヶ月当たりの実労働時間の推移>1ヶ月当たりの実労働時間の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220813115200/ title=コンビニエンスストアの店舗数の推移>コンビニエンスストアの店舗数の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220821113322/ title=地域別1世帯当たり1ヶ月間の支出額の推移>地域別1世帯当たり1ヶ月間の支出額の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220828111836/ title="夫婦別子供の有無別生活時間 (2016)">夫婦別子供の有無別生活時間 (2016)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220827103615/ title=家族類型の変遷>家族類型の変遷</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220815131016/ title="年齢別大学院入学者数 (2021)">年齢別大学院入学者数 (2021)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220818122628/ title=情報サービス業における企業特殊的人的資本（名目）の推移>情報サービス業における企業特殊的人的資本（名目）の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220824095127/ title=有業者の社会生活における活動の変化>有業者の社会生活における活動の変化</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220814175022/ title=東京23区の乗用車保有台数>東京23区の乗用車保有台数</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220817121117/ title="業種別企業特殊的人的資本 (2018)">業種別企業特殊的人的資本 (2018)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220825094111/ title=無業者の社会生活における活動の変化>無業者の社会生活における活動の変化</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220823104100/ title=睡眠時間の推移>睡眠時間の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220826102652/ title=総人口の推移>総人口の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220820225528/ title=都道府県別国公立別学校数・学生数>都道府県別国公立別学校数・学生数</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220816120845/ title="都道府県別消費者物価指数/教育（全国平均=100）">都道府県別消費者物価指数/教育（全国平均=100）</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220812173814/ title=金沢市の夏の月平均気温の遷移>金沢市の夏の月平均気温の遷移</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/202209/>2022.09</a><ul><li><a href=/akitenkrad-blog/posts/figures/202209/20220907122906/ title="ACL-2022 WordCloud">ACL-2022 WordCloud</a></li></ul></li></ul></li><li><a href=/akitenkrad-blog/posts/latex/ title="Latex Mathematics Syntax Guide">Latex Mathematics Syntax Guide</a></li><li><a href=/akitenkrad-blog/posts/markdown/ title="Markdown Sample">Markdown Sample</a></li></div></ul></div></div></div></section><section class=content-section id=content-section><div class=content><div class="container p-0 read-area"><div class="hero-area col-sm-12" id=hero-area style=background-image:url(/akitenkrad-blog/posts/arxiv/202402/20240226000000/hero.png)></div><div class=page-content><div class="author-profile ml-auto align-self-lg-center"><img class=rounded-circle src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_120x120_fit_box_3.png alt="Author Image"><h5 class=author-name></h5><p>Monday, Feb 26, 2024</p></div><div class=title><h1>arXiv @ 2024.02.26</h1></div><div class=taxonomy-terms><ul><li class=rounded><a href=/akitenkrad-blog/tags/arxiv class="btn, btn-sm">arXiv</a></li><li class=rounded><a href=/akitenkrad-blog/tags/published2024 class="btn, btn-sm">2024</a></li></ul></div><div class=post-content id=post-content><figure style=border:none;width:100%;display:flex;justify-content:center><iframe src=pie.html width=900 height=620 style=border:none></iframe></figure><h2 id=primary-categories>Primary Categories</h2><ul><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240226000000/#csai-6>cs.AI (6)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240226000000/#cscg-1>cs.CG (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240226000000/#cscl-21>cs.CL (21)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240226000000/#cscr-5>cs.CR (5)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240226000000/#cscv-22>cs.CV (22)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240226000000/#cscy-1>cs.CY (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240226000000/#csdc-1>cs.DC (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240226000000/#csdl-1>cs.DL (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240226000000/#csdm-1>cs.DM (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240226000000/#csds-2>cs.DS (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240226000000/#csgt-1>cs.GT (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240226000000/#cshc-3>cs.HC (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240226000000/#csir-2>cs.IR (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240226000000/#csit-1>cs.IT (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240226000000/#cslg-26>cs.LG (26)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240226000000/#cslo-1>cs.LO (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240226000000/#csni-2>cs.NI (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240226000000/#cspl-1>cs.PL (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240226000000/#csro-2>cs.RO (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240226000000/#cssd-1>cs.SD (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240226000000/#csse-2>cs.SE (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240226000000/#cssi-1>cs.SI (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240226000000/#econth-1>econ.TH (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240226000000/#eessiv-2>eess.IV (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240226000000/#eesssp-1>eess.SP (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240226000000/#eesssy-4>eess.SY (4)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240226000000/#mathco-1>math.CO (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240226000000/#mathna-1>math.NA (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240226000000/#q-biogn-1>q-bio.GN (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240226000000/#quant-ph-1>quant-ph (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240226000000/#statml-1>stat.ML (1)</a></li></ul><h2 id=keywords>Keywords</h2><table border=1 class=dataframe><thead><tr style=text-align:right><th>keyword</th><th>cs.CL</th><th>cs.CV</th><th>cs.LG</th></tr></thead><tbody><tr><td>Active Learning</td><td></td><td></td><td>1</td></tr><tr><td>Adversarial Attack</td><td></td><td>1</td><td>1</td></tr><tr><td>Autoencoder</td><td></td><td></td><td>1</td></tr><tr><td>BERT</td><td>1</td><td></td><td></td></tr><tr><td>Bandit Algorithm</td><td></td><td></td><td>2</td></tr><tr><td>Benchmarking</td><td>5</td><td>7</td><td>3</td></tr><tr><td>Black Box</td><td>3</td><td></td><td></td></tr><tr><td>ChatGPT</td><td>4</td><td>1</td><td></td></tr><tr><td>Clustering</td><td></td><td></td><td>3</td></tr><tr><td>Continuous Time</td><td></td><td></td><td>2</td></tr><tr><td>Contrastive Learning</td><td>1</td><td>3</td><td>1</td></tr><tr><td>Convolution</td><td></td><td>3</td><td>1</td></tr><tr><td>Convolutional Neural Network</td><td></td><td>6</td><td></td></tr><tr><td>Data Augmentation</td><td>2</td><td></td><td></td></tr><tr><td>Dense Retrieval</td><td>1</td><td></td><td></td></tr><tr><td>Diffusion Model</td><td></td><td>1</td><td></td></tr><tr><td>Document Ranking</td><td>1</td><td></td><td></td></tr><tr><td>Emotion Recognition</td><td></td><td>1</td><td></td></tr><tr><td>Essay Scoring</td><td>1</td><td></td><td></td></tr><tr><td>Explainable AI</td><td></td><td>1</td><td></td></tr><tr><td>Federated Learning</td><td></td><td>1</td><td>1</td></tr><tr><td>Few-shot</td><td>4</td><td>1</td><td></td></tr><tr><td>Few-shot Learning</td><td>2</td><td>1</td><td></td></tr><tr><td>Fine-tuning</td><td>9</td><td>6</td><td>4</td></tr><tr><td>Foundation Model</td><td></td><td>1</td><td></td></tr><tr><td>GPT</td><td>2</td><td>1</td><td></td></tr><tr><td>GPT-3</td><td>1</td><td></td><td></td></tr><tr><td>GPT-3.5</td><td>1</td><td></td><td></td></tr><tr><td>GPT-4</td><td>1</td><td>1</td><td></td></tr><tr><td>Gaussian Process</td><td></td><td></td><td>1</td></tr><tr><td>Gemini</td><td>1</td><td></td><td></td></tr><tr><td>Generative Adversarial Network</td><td></td><td>2</td><td>2</td></tr><tr><td>Geometry</td><td></td><td></td><td>1</td></tr><tr><td>Grammatical Error Correction</td><td>2</td><td></td><td></td></tr><tr><td>Graph</td><td>1</td><td>1</td><td>3</td></tr><tr><td>Graph Contrastive Learning</td><td></td><td></td><td>2</td></tr><tr><td>Graph Neural Network</td><td></td><td></td><td>2</td></tr><tr><td>In-context Learning</td><td></td><td></td><td>2</td></tr><tr><td>Information Retrieval</td><td></td><td></td><td>1</td></tr><tr><td>Instruction Following</td><td></td><td>1</td><td></td></tr><tr><td>Instruction Tuning</td><td></td><td>1</td><td>1</td></tr><tr><td>Knowledge Distillation</td><td>1</td><td>2</td><td></td></tr><tr><td>LLaMA</td><td>2</td><td></td><td>1</td></tr><tr><td>LSTM</td><td></td><td></td><td>3</td></tr><tr><td>Language Generation</td><td>1</td><td></td><td></td></tr><tr><td>Large Language Model</td><td>24</td><td>2</td><td>6</td></tr><tr><td>Logistic Regression</td><td></td><td></td><td>1</td></tr><tr><td>Low-Resource</td><td>1</td><td></td><td></td></tr><tr><td>Markov Decision Process</td><td></td><td></td><td>1</td></tr><tr><td>Mathematical Reasoning</td><td>1</td><td></td><td></td></tr><tr><td>Mistral</td><td>1</td><td></td><td></td></tr><tr><td>Multi-modal</td><td>4</td><td>10</td><td></td></tr><tr><td>Multiple Instance Learning</td><td></td><td>1</td><td></td></tr><tr><td>Offline Reinforcement Learning</td><td></td><td></td><td>1</td></tr><tr><td>Out-of-distribution</td><td></td><td>1</td><td>1</td></tr><tr><td>Parameter Sharing</td><td></td><td></td><td>1</td></tr><tr><td>Pre-trained Language Model</td><td>1</td><td></td><td></td></tr><tr><td>Probabilistic Model</td><td></td><td></td><td>1</td></tr><tr><td>Prompt</td><td>10</td><td>3</td><td></td></tr><tr><td>Prompt Learning</td><td>1</td><td>1</td><td></td></tr><tr><td>Pruning</td><td>1</td><td></td><td></td></tr><tr><td>Question Answering</td><td>1</td><td>1</td><td></td></tr><tr><td>Reasoning</td><td>5</td><td>2</td><td></td></tr><tr><td>Recurrent Neural Network</td><td>2</td><td></td><td></td></tr><tr><td>Reinforcement Learning</td><td></td><td>1</td><td>5</td></tr><tr><td>Relation Extraction</td><td>1</td><td></td><td></td></tr><tr><td>RoBERTa</td><td>1</td><td></td><td></td></tr><tr><td>Self-Attention</td><td></td><td>1</td><td></td></tr><tr><td>Self-supervised Learning</td><td></td><td></td><td>2</td></tr><tr><td>Semi-Supervised Learning</td><td></td><td>1</td><td></td></tr><tr><td>Simulation</td><td></td><td>2</td><td>3</td></tr><tr><td>Simulator</td><td></td><td>2</td><td>3</td></tr><tr><td>Slot Filling</td><td>1</td><td></td><td></td></tr><tr><td>Stochastic Gradient Descent</td><td></td><td></td><td>2</td></tr><tr><td>Style Transfer</td><td></td><td>2</td><td></td></tr><tr><td>Summarization</td><td>1</td><td></td><td>1</td></tr><tr><td>Supervised Learning</td><td></td><td>1</td><td></td></tr><tr><td>Text Summarization</td><td>1</td><td></td><td></td></tr><tr><td>Text2image</td><td></td><td>1</td><td></td></tr><tr><td>Transformer</td><td></td><td>3</td><td>1</td></tr><tr><td>Unsupervised Learning</td><td></td><td>1</td><td>3</td></tr><tr><td>Variational Autoencoder</td><td></td><td></td><td>1</td></tr><tr><td>Vision Transformer</td><td></td><td>2</td><td></td></tr><tr><td>Vision-and-Language</td><td>2</td><td>5</td><td></td></tr><tr><td>Visual Question Answering</td><td></td><td>2</td><td></td></tr><tr><td>Zero-shot</td><td>3</td><td>2</td><td>1</td></tr><tr><td>falcon</td><td>1</td><td></td><td></td></tr><tr><td>human-in-the-loop</td><td></td><td></td><td>1</td></tr></tbody></table><script>$(function(){$("table").addClass("keyword-table table-bordered border-success"),$("table thead").addClass("sticky-top"),$("table tbody td").css("text-align","")})</script><h2 id=cscl-21>cs.CL (21)</h2><h3 id=121--1117-prompt-perturbation-consistency-learning-for-robust-language-models-yao-qiang-et-al-2024>(1/21 | 1/117) Prompt Perturbation Consistency Learning for Robust Language Models (Yao Qiang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yao Qiang, Subhrangshu Nandi, Ninareh Mehrabi, Greg Ver Steeg, Anoop Kumar, Anna Rumshisky, Aram Galstyan. (2024)<br><strong>Prompt Perturbation Consistency Learning for Robust Language Models</strong><br><button class=copy-to-clipboard title="Prompt Perturbation Consistency Learning for Robust Language Models" index=1>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-1 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keyword Score: 100<br>Keywords: Data Augmentation, Fine-tuning, Fine-tuning, Question Answering, Slot Filling, Text Summarization, Large Language Model, Large Language Model, Prompt, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15833v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15833v1.pdf filename=2402.15833v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have demonstrated impressive performance on a number of natural language processing tasks, such as <b>question</b> <b>answering</b> and <b>text</b> <b>summarization.</b> However, their performance on sequence labeling tasks such as intent classification and <b>slot</b> <b>filling</b> (IC-SF), which is a central component in personal assistant systems, lags significantly behind discriminative models. Furthermore, there is a lack of substantive research on the robustness of <b>LLMs</b> to various perturbations in the input <b>prompts.</b> The contributions of this paper are three-fold. First, we show that <b>fine-tuning</b> sufficiently <b>large</b> <b>LLMs</b> <b>can</b> produce IC-SF performance comparable to discriminative models. Next, we systematically analyze the performance deterioration of those <b>fine-tuned</b> models due to three distinct yet relevant types of input perturbations - oronyms, synonyms, and paraphrasing. Finally, we propose an efficient mitigation approach, <b>Prompt</b> Perturbation Consistency Learning (PPCL), which works by regularizing the divergence between losses from clean and perturbed samples. Our experiments demonstrate that PPCL can recover on average 59% and 69% of the performance drop for IC and SF tasks, respectively. Furthermore, PPCL beats the <b>data</b> <b>augmentation</b> approach while using ten times fewer augmented <b>data</b> <b>samples.</b></p></p class="citation"></blockquote><h3 id=221--2117-linguistic-intelligence-in-large-language-models-for-telecommunications-tasnim-ahmed-et-al-2024>(2/21 | 2/117) Linguistic Intelligence in Large Language Models for Telecommunications (Tasnim Ahmed et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tasnim Ahmed, Nicola Piovesan, Antonio De Domenico, Salimur Choudhury. (2024)<br><strong>Linguistic Intelligence in Large Language Models for Telecommunications</strong><br><button class=copy-to-clipboard title="Linguistic Intelligence in Large Language Models for Telecommunications" index=2>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-2 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 90<br>Keywords: Fine-tuning, Zero-shot, ChatGPT, Mistral, falcon, Language Generation, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15818v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15818v1.pdf filename=2402.15818v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have emerged as a significant advancement in the field of Natural <b>Language</b> <b>Processing</b> (NLP), demonstrating remarkable capabilities in <b>language</b> <b>generation</b> and other <b>language-centric</b> <b>tasks.</b> Despite their evaluation across a multitude of analytical and <b>reasoning</b> tasks in various scientific domains, a comprehensive exploration of their knowledge and understanding within the realm of natural <b>language</b> <b>tasks</b> in the telecommunications domain is still needed. This study, therefore, seeks to evaluate the knowledge and understanding capabilities of <b>LLMs</b> within this domain. To achieve this, we conduct an exhaustive <b>zero-shot</b> evaluation of four prominent <b>LLMs-Llama-2,</b> <b>Falcon,</b> <b>Mistral,</b> and Zephyr. These models require fewer resources than <b>ChatGPT,</b> making them suitable for resource-constrained environments. Their performance is compared with state-of-the-art, <b>fine-tuned</b> models. To the best of our knowledge, this is the first work to extensively evaluate and compare the understanding of <b>LLMs</b> across multiple <b>language-centric</b> <b>tasks</b> in this domain. Our evaluation reveals that <b>zero-shot</b> <b>LLMs</b> can achieve performance levels comparable to the current state-of-the-art <b>fine-tuned</b> models. This indicates that pretraining on extensive text corpora equips <b>LLMs</b> with a degree of specialization, even within the telecommunications domain. We also observe that no single <b>LLM</b> consistently outperforms others, and the performance of different <b>LLMs</b> can fluctuate. Although their performance lags behind <b>fine-tuned</b> models, our findings underscore the potential of <b>LLMs</b> as a valuable resource for understanding various aspects of this field that lack <b>large</b> <b>annotated</b> <b>data.</b></p></p class="citation"></blockquote><h3 id=321--3117-evaluating-prompting-strategies-for-grammatical-error-correction-based-on-language-proficiency-min-zeng-et-al-2024>(3/21 | 3/117) Evaluating Prompting Strategies for Grammatical Error Correction Based on Language Proficiency (Min Zeng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Min Zeng, Jiexin Kuang, Mengyang Qiu, Jayoung Song, Jungyeul Park. (2024)<br><strong>Evaluating Prompting Strategies for Grammatical Error Correction Based on Language Proficiency</strong><br><button class=copy-to-clipboard title="Evaluating Prompting Strategies for Grammatical Error Correction Based on Language Proficiency" index=3>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-3 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 80<br>Keywords: Few-shot, Fine-tuning, Fine-tuning, Zero-shot, Grammatical Error Correction, Grammatical Error Correction, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15930v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15930v1.pdf filename=2402.15930v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The writing examples of English language learners may be different from those of native speakers. Given that there is a significant differences in second language (L2) learners&rsquo; error types by their proficiency levels, this paper attempts to reduce overcorrection by examining the interaction between <b>LLM&rsquo;s</b> performance and L2 language proficiency. Our method focuses on <b>zero-shot</b> and <b>few-shot</b> <b>prompting</b> and <b>fine-tuning</b> models for <b>GEC</b> for learners of English as a foreign language based on the different proficiency. We investigate <b>GEC</b> results and find that overcorrection happens primarily in advanced language learners&rsquo; writing (proficiency C) rather than proficiency A (a beginner level) and proficiency B (an intermediate level). <b>Fine-tuned</b> <b>LLMs,</b> and even <b>few-shot</b> <b>prompting</b> with writing examples of English learners, actually tend to exhibit decreased recall measures. To make our claim concrete, we conduct a comprehensive examination of <b>GEC</b> outcomes and their evaluation results based on language proficiency.</p></p class="citation"></blockquote><h3 id=421--4117-look-before-you-leap-problem-elaboration-prompting-improves-mathematical-reasoning-in-large-language-models-haoran-liao-et-al-2024>(4/21 | 4/117) Look Before You Leap: Problem Elaboration Prompting Improves Mathematical Reasoning in Large Language Models (Haoran Liao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haoran Liao, Jidong Tian, Shaohua Hu, Hao He, Yaohui Jin. (2024)<br><strong>Look Before You Leap: Problem Elaboration Prompting Improves Mathematical Reasoning in Large Language Models</strong><br><button class=copy-to-clipboard title="Look Before You Leap: Problem Elaboration Prompting Improves Mathematical Reasoning in Large Language Models" index=4>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-4 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 80<br>Keywords: GPT, GPT-3, GPT-3.5, Mathematical Reasoning, Reasoning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15764v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15764v1.pdf filename=2402.15764v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models~(LLMs)</b> have exhibited impressive performance across NLP tasks. So far they still face challenges in complex <b>reasoning</b> tasks and can be sensitive to input context. Despite significant efforts have been invested in enhancing <b>reasoning</b> process and improving prefix-prompts robustness, the crucial role of problem context has been overlooked. In this study, we propose a new approach to improve the <b>mathematical</b> <b>capacities</b> of <b>LLMs,</b> named Problem Elaboration Prompting~(PEP). Specifically, PEP decomposes and elucidates the problem context before <b>reasoning,</b> thus enhancing the global context modeling and reducing the parsing difficulties. Experiments on datasets demonstrate promising performances on complex <b>reasoning</b> and indicate the beneficial impact for ill-formed problems. For instance, with the <b>GPT-3.5</b> model~(\texttt{text-davinci-003}), we observed a 9.93% improvement with greedy decoding and 8.80% improvement with self-consistency on GSM8k compared to the standard CoT. With ChatGPT~(\texttt{turbo}) and PEP, we achieve SOTA performances on SVAMP with 86.2% and GSM8k with 90.98%.</p></p class="citation"></blockquote><h3 id=521--5117-making-pre-trained-language-models-better-continual-few-shot-relation-extractors-shengkun-ma-et-al-2024>(5/21 | 5/117) Making Pre-trained Language Models Better Continual Few-Shot Relation Extractors (Shengkun Ma et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shengkun Ma, Jiale Han, Yi Liang, Bo Cheng. (2024)<br><strong>Making Pre-trained Language Models Better Continual Few-Shot Relation Extractors</strong><br><button class=copy-to-clipboard title="Making Pre-trained Language Models Better Continual Few-Shot Relation Extractors" index=5>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-5 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 80<br>Keywords: Contrastive Learning, Few-shot, Low-Resource, ChatGPT, Relation Extraction, Pre-trained Language Model, Prompt, Prompt Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15713v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15713v1.pdf filename=2402.15713v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Continual <b>Few-shot</b> <b>Relation</b> <b>Extraction</b> (CFRE) is a practical problem that requires the model to continuously learn novel <b>relations</b> <b>while</b> avoiding forgetting old ones with few labeled training data. The primary challenges are catastrophic forgetting and overfitting. This paper harnesses <b>prompt</b> <b>learning</b> to explore the implicit capabilities of <b>pre-trained</b> <b>language</b> <b>models</b> to address the above two challenges, thereby making language models better continual <b>few-shot</b> <b>relation</b> <b>extractors.</b> Specifically, we propose a <b>Contrastive</b> <b>Prompt</b> <b>Learning</b> framework, which designs <b>prompt</b> <b>representation</b> to acquire more generalized knowledge that can be easily adapted to old and new categories, and margin-based <b>contrastive</b> <b>learning</b> to focus more on hard samples, therefore alleviating catastrophic forgetting and overfitting issues. To further remedy overfitting in <b>low-resource</b> scenarios, we introduce an effective memory augmentation strategy that employs well-crafted <b>prompts</b> <b>to</b> guide <b>ChatGPT</b> in generating diverse samples. Extensive experiments demonstrate that our method outperforms state-of-the-art methods by a large margin and significantly mitigates catastrophic forgetting and overfitting in <b>low-resource</b> scenarios.</p></p class="citation"></blockquote><h3 id=621--6117-leveraging-chatgpt-in-pharmacovigilance-event-extraction-an-empirical-study-zhaoyue-sun-et-al-2024>(6/21 | 6/117) Leveraging ChatGPT in Pharmacovigilance Event Extraction: An Empirical Study (Zhaoyue Sun et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhaoyue Sun, Gabriele Pergola, Byron C. Wallace, Yulan He. (2024)<br><strong>Leveraging ChatGPT in Pharmacovigilance Event Extraction: An Empirical Study</strong><br><button class=copy-to-clipboard title="Leveraging ChatGPT in Pharmacovigilance Event Extraction: An Empirical Study" index=6>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-6 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 70<br>Keywords: Data Augmentation, Fine-tuning, Fine-tuning, ChatGPT, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15663v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15663v1.pdf filename=2402.15663v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the advent of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs),</b> there has been growing interest in exploring their potential for medical applications. This research aims to investigate the ability of <b>LLMs,</b> specifically <b>ChatGPT,</b> in the context of pharmacovigilance event extraction, of which the main goal is to identify and extract adverse events or potential therapeutic events from textual medical sources. We conduct extensive experiments to assess the performance of <b>ChatGPT</b> in the pharmacovigilance event extraction task, employing various <b>prompts</b> and demonstration selection strategies. The findings demonstrate that while <b>ChatGPT</b> demonstrates reasonable performance with appropriate demonstration selection strategies, it still falls short compared to fully <b>fine-tuned</b> small models. Additionally, we explore the potential of leveraging <b>ChatGPT</b> for <b>data</b> <b>augmentation.</b> However, our investigation reveals that the inclusion of synthesized <b>data</b> <b>into</b> <b>fine-tuning</b> may lead to a decrease in performance, possibly attributed to noise in the <b>ChatGPT-generated</b> labels. To mitigate this, we explore different filtering strategies and find that, with the proper approach, more stable performance can be achieved, although constant improvement remains elusive.</p></p class="citation"></blockquote><h3 id=721--7117-sportqa-a-benchmark-for-sports-understanding-in-large-language-models-haotian-xia-et-al-2024>(7/21 | 7/117) SportQA: A Benchmark for Sports Understanding in Large Language Models (Haotian Xia et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haotian Xia, Zhengbang Yang, Yuqing Wang, Rhys Tracy, Yun Zhao, Dongdong Huang, Zezhi Chen, Yan Zhu, Yuan-fang Wang, Weining Shen. (2024)<br><strong>SportQA: A Benchmark for Sports Understanding in Large Language Models</strong><br><button class=copy-to-clipboard title="SportQA: A Benchmark for Sports Understanding in Large Language Models" index=7>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-7 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 63<br>Keywords: Benchmarking, Few-shot, Few-shot Learning, Reasoning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15862v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15862v1.pdf filename=2402.15862v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A deep understanding of sports, a field rich in strategic and dynamic content, is crucial for advancing Natural Language Processing (NLP). This holds particular significance in the context of evaluating and advancing <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs),</b> given the existing gap in specialized <b>benchmarks.</b> To bridge this gap, we introduce SportQA, a novel <b>benchmark</b> specifically designed for evaluating <b>LLMs</b> in the context of sports understanding. SportQA encompasses over 70,000 multiple-choice questions across three distinct difficulty levels, each targeting different aspects of sports knowledge from basic historical facts to intricate, scenario-based <b>reasoning</b> tasks. We conducted a thorough evaluation of prevalent <b>LLMs,</b> mainly utilizing <b>few-shot</b> <b>learning</b> paradigms supplemented by chain-of-thought (CoT) <b>prompting.</b> Our results reveal that while <b>LLMs</b> exhibit competent performance in basic sports knowledge, they struggle with more complex, scenario-based sports <b>reasoning,</b> lagging behind human expertise. The introduction of SportQA marks a significant step forward in NLP, offering a tool for assessing and enhancing sports understanding in <b>LLMs.</b></p></p class="citation"></blockquote><h3 id=821--8117-gaokao-mm-a-chinese-human-level-benchmark-for-multimodal-models-evaluation-yi-zong-et-al-2024>(8/21 | 8/117) GAOKAO-MM: A Chinese Human-Level Benchmark for Multimodal Models Evaluation (Yi Zong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yi Zong, Xipeng Qiu. (2024)<br><strong>GAOKAO-MM: A Chinese Human-Level Benchmark for Multimodal Models Evaluation</strong><br><button class=copy-to-clipboard title="GAOKAO-MM: A Chinese Human-Level Benchmark for Multimodal Models Evaluation" index=8>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-8 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-CV, cs.CL<br>Keyword Score: 62<br>Keywords: Graph, Benchmarking, Multi-modal, Multi-modal, GPT, GPT-4, Gemini, Reasoning, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15745v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15745v1.pdf filename=2402.15745v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The Large <b>Vision-Language</b> Models (LVLMs) have demonstrated great abilities in image perception and language understanding. However, existing <b>multimodal</b> <b>benchmarks</b> focus on primary perception abilities and commonsense knowledge which are insufficient to reflect the comprehensive capabilities of LVLMs. We propose GAOKAO-MM, a <b>multimodal</b> <b>benchmark</b> based on the Chinese College Entrance Examination (GAOKAO), comprising of 8 subjects and 12 types of images, such as diagrams, function <b>graphs,</b> maps and photos. GAOKAO-MM derives from native Chinese context and sets human-level requirements for the model&rsquo;s abilities, including perception, understanding, knowledge and <b>reasoning.</b> We evaluate 10 LVLMs and find that the accuracies of all of them are lower than 50%, with <b>GPT-4-Vison</b> (48.1%), Qwen-VL-Plus (41.2%) and <b>Gemini-Pro-Vision</b> (35.1%) ranking in the top three positions. The results of our multi-dimension analysis indicate that LVLMs have moderate distance towards Artificial General Intelligence (AGI) and provide insights facilitating the development of multilingual LVLMs.</p></p class="citation"></blockquote><h3 id=921--9117-exploring-failure-cases-in-multimodal-reasoning-about-physical-dynamics-sadaf-ghaffari-et-al-2024>(9/21 | 9/117) Exploring Failure Cases in Multimodal Reasoning About Physical Dynamics (Sadaf Ghaffari et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sadaf Ghaffari, Nikhil Krishnaswamy. (2024)<br><strong>Exploring Failure Cases in Multimodal Reasoning About Physical Dynamics</strong><br><button class=copy-to-clipboard title="Exploring Failure Cases in Multimodal Reasoning About Physical Dynamics" index=9>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-9 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 56<br>Keywords: Knowledge Distillation, Multi-modal, Multi-modal, Zero-shot, Reasoning, Large Language Model, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15654v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15654v1.pdf filename=2402.15654v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we present an exploration of <b>LLMs&rsquo;</b> abilities to problem solve with physical <b>reasoning</b> in situated environments. We construct a simple simulated environment and demonstrate examples of where, in a <b>zero-shot</b> setting, both text and <b>multimodal</b> <b>LLMs</b> display atomic world knowledge about various objects but fail to compose this knowledge in correct solutions for an object manipulation and placement task. We also use BLIP, a <b>vision-language</b> model trained with more sophisticated cross-modal attention, to identify cases relevant to object physical properties that that model fails to ground. Finally, we present a procedure for discovering the relevant properties of objects in the environment and propose a method to <b>distill</b> this knowledge back into the <b>LLM.</b></p></p class="citation"></blockquote><h3 id=1021--10117-dental-severity-assessment-through-few-shot-learning-and-sbert-fine-tuning-mohammad-dehghani-2024>(10/21 | 10/117) Dental Severity Assessment through Few-shot Learning and SBERT Fine-tuning (Mohammad Dehghani, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mohammad Dehghani. (2024)<br><strong>Dental Severity Assessment through Few-shot Learning and SBERT Fine-tuning</strong><br><button class=copy-to-clipboard title="Dental Severity Assessment through Few-shot Learning and SBERT Fine-tuning" index=10>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-10 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Few-shot, Few-shot Learning, Fine-tuning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15755v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15755v1.pdf filename=2402.15755v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Dental diseases have a significant impact on a considerable portion of the population, leading to various health issues that can detrimentally affect individuals&rsquo; overall well-being. The integration of automated systems in oral healthcare has become increasingly crucial. Machine learning approaches offer a viable solution to address challenges such as diagnostic difficulties, inefficiencies, and errors in oral disease diagnosis. These methods prove particularly useful when physicians struggle to predict or diagnose diseases at their early stages. In this study, thirteen different machine learning, deep learning, and <b>large</b> <b>language</b> <b>models</b> were employed to determine the severity level of oral health issues based on radiologists&rsquo; reports. The results revealed that the <b>Few-shot</b> <b>learning</b> with SBERT and Multi-Layer Perceptron model outperformed all other models across various experiments, achieving an impressive accuracy of 94.1% as the best result. Consequently, this model exhibits promise as a reliable tool for evaluating the severity of oral diseases, enabling patients to receive more effective treatment and aiding healthcare professionals in making informed decisions regarding resource allocation and the management of high-risk patients.</p></p class="citation"></blockquote><h3 id=1121--11117-hd-eval-aligning-large-language-model-evaluators-through-hierarchical-criteria-decomposition-yuxuan-liu-et-al-2024>(11/21 | 11/117) HD-Eval: Aligning Large Language Model Evaluators Through Hierarchical Criteria Decomposition (Yuxuan Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuxuan Liu, Tianchi Yang, Shaohan Huang, Zihan Zhang, Haizhen Huang, Furu Wei, Weiwei Deng, Feng Sun, Qi Zhang. (2024)<br><strong>HD-Eval: Aligning Large Language Model Evaluators Through Hierarchical Criteria Decomposition</strong><br><button class=copy-to-clipboard title="HD-Eval: Aligning Large Language Model Evaluators Through Hierarchical Criteria Decomposition" index=11>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-11 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Pruning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15754v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15754v1.pdf filename=2402.15754v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have emerged as a promising alternative to expensive human evaluations. However, the alignment and coverage of <b>LLM-based</b> evaluations are often limited by the scope and potential bias of the evaluation <b>prompts</b> and criteria. To address this challenge, we propose HD-Eval, a novel framework that iteratively aligns <b>LLM-based</b> evaluators with human preference via Hierarchical Criteria Decomposition. HD-Eval inherits the essence from the evaluation mindset of human experts and enhances the alignment of <b>LLM-based</b> evaluators by decomposing a given evaluation task into finer-grained criteria, aggregating them according to estimated human preferences, <b>pruning</b> insignificant criteria with attribution, and further decomposing significant criteria. By integrating these steps within an iterative alignment training process, we obtain a hierarchical decomposition of criteria that comprehensively captures aspects of natural language at multiple levels of granularity. Implemented as a white box, the human preference-guided aggregator is efficient to train and more explainable than relying solely on <b>prompting,</b> and its independence from model parameters makes it applicable to closed-source <b>LLMs.</b> Extensive experiments on three evaluation domains demonstrate the superiority of HD-Eval in further aligning state-of-the-art evaluators and providing deeper insights into the explanation of evaluation results and the task itself.</p></p class="citation"></blockquote><h3 id=1221--12117-generalization-or-memorization-data-contamination-and-trustworthy-evaluation-for-large-language-models-yihong-dong-et-al-2024>(12/21 | 12/117) Generalization or Memorization: Data Contamination and Trustworthy Evaluation for Large Language Models (Yihong Dong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yihong Dong, Xue Jiang, Huanyu Liu, Zhi Jin, Ge Li. (2024)<br><strong>Generalization or Memorization: Data Contamination and Trustworthy Evaluation for Large Language Models</strong><br><button class=copy-to-clipboard title="Generalization or Memorization: Data Contamination and Trustworthy Evaluation for Large Language Models" index=12>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-12 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-CR, cs-LG, cs-SE, cs.CL<br>Keyword Score: 38<br>Keywords: Benchmarking, Black Box, ChatGPT, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15938v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15938v1.pdf filename=2402.15938v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent statements about the impressive capabilities of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> are usually supported by evaluating on open-access <b>benchmarks.</b> Considering the vast size and wide-ranging sources of <b>LLMs&rsquo;</b> training data, it could explicitly or implicitly include test data, leading to <b>LLMs</b> being more susceptible to data contamination. However, due to the opacity of training data, the <b>black-box</b> <b>access</b> of models, and the rapid growth of synthetic training data, detecting and mitigating data contamination for <b>LLMs</b> faces significant challenges. In this paper, we propose CDD, which stands for Contamination Detection via output Distribution for <b>LLMs.</b> CDD necessitates only the sampled texts to detect data contamination, by identifying the peakedness of <b>LLM&rsquo;s</b> output distribution. To mitigate the impact of data contamination in evaluation, we also present TED: Trustworthy Evaluation via output Distribution, based on the correction of <b>LLM&rsquo;s</b> output distribution. To facilitate this study, we introduce two <b>benchmarks,</b> i.e., DetCon and ComiEval, for data contamination detection and contamination mitigation evaluation tasks. Extensive experimental results show that CDD achieves the average relative improvements of 21.8%-30.2% over other contamination detection approaches in terms of Accuracy, F1 Score, and AUC metrics, and can effectively detect contamination caused by the variants of test data. TED significantly mitigates performance improvements up to 66.9% attributed to data contamination across 24 settings and 21 contamination degrees. In real-world applications, we reveal that <b>ChatGPT</b> exhibits a high potential to suffer from data contamination on HumanEval <b>benchmark.</b></p></p class="citation"></blockquote><h3 id=1321--13117-semeval-2024-task-8-weighted-layer-averaging-roberta-for-black-box-machine-generated-text-detection-ayan-datta-et-al-2024>(13/21 | 13/117) SemEval-2024 Task 8: Weighted Layer Averaging RoBERTa for Black-Box Machine-Generated Text Detection (Ayan Datta et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ayan Datta, Aryan Chandramania, Radhika Mamidi. (2024)<br><strong>SemEval-2024 Task 8: Weighted Layer Averaging RoBERTa for Black-Box Machine-Generated Text Detection</strong><br><button class=copy-to-clipboard title="SemEval-2024 Task 8: Weighted Layer Averaging RoBERTa for Black-Box Machine-Generated Text Detection" index=13>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-13 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 35<br>Keywords: Black Box, RoBERTa, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15873v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15873v1.pdf filename=2402.15873v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This document contains the details of the authors&rsquo; submission to the proceedings of SemEval 2024&rsquo;s Task 8: Multigenerator, Multidomain, and Multilingual <b>Black-Box</b> <b>Machine-Generated</b> Text Detection Subtask A (monolingual) and B. Detection of machine-generated text is becoming an increasingly important task, with the advent of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs).</b> In this document, we lay out the techniques utilized for performing the same, along with the results obtained.</p></p class="citation"></blockquote><h3 id=1421--14117-foot-in-the-door-understanding-large-language-model-jailbreaking-via-cognitive-psychology-zhenhua-wang-et-al-2024>(14/21 | 14/117) Foot In The Door: Understanding Large Language Model Jailbreaking via Cognitive Psychology (Zhenhua Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhenhua Wang, Wei Xie, Baosheng Wang, Enze Wang, Zhiwen Gui, Shuoyoucheng Ma, Kai Chen. (2024)<br><strong>Foot In The Door: Understanding Large Language Model Jailbreaking via Cognitive Psychology</strong><br><button class=copy-to-clipboard title="Foot In The Door: Understanding Large Language Model Jailbreaking via Cognitive Psychology" index=14>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-14 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 35<br>Keywords: Black Box, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15690v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15690v1.pdf filename=2402.15690v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have gradually become the gateway for people to acquire new knowledge. However, attackers can break the model&rsquo;s security protection (&ldquo;jail&rdquo;) to access restricted information, which is called &ldquo;jailbreaking.&rdquo; Previous studies have shown the weakness of current <b>LLMs</b> when confronted with such jailbreaking attacks. Nevertheless, comprehension of the intrinsic decision-making mechanism within the <b>LLMs</b> upon receipt of jailbreak <b>prompts</b> is noticeably lacking. Our research provides a psychological explanation of the jailbreak <b>prompts.</b> Drawing on cognitive consistency theory, we argue that the key to jailbreak is guiding the <b>LLM</b> to achieve cognitive coordination in an erroneous direction. Further, we propose an automatic <b>black-box</b> <b>jailbreaking</b> method based on the Foot-in-the-Door (FITD) technique. This method progressively induces the model to answer harmful questions via multi-step incremental <b>prompts.</b> We instantiated a prototype system to evaluate the jailbreaking effectiveness on 8 advanced <b>LLMs,</b> yielding an average success rate of 83.9%. This study builds a psychological perspective on the explanatory insights into the intrinsic decision-making logic of <b>LLMs.</b></p></p class="citation"></blockquote><h3 id=1521--15117-chimera-a-lossless-decoding-method-for-accelerating-large-language-models-inference-by-fusing-all-tokens-ziqian-zeng-et-al-2024>(15/21 | 15/117) Chimera: A Lossless Decoding Method for Accelerating Large Language Models Inference by Fusing all Tokens (Ziqian Zeng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ziqian Zeng, Jiahong Yu, Qianshi Pang, Zihao Wang, Huiping Zhuang, Cen Chen. (2024)<br><strong>Chimera: A Lossless Decoding Method for Accelerating Large Language Models Inference by Fusing all Tokens</strong><br><button class=copy-to-clipboard title="Chimera: A Lossless Decoding Method for Accelerating Large Language Models Inference by Fusing all Tokens" index=15>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-15 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: LLaMA, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15758v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15758v1.pdf filename=2402.15758v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have demonstrated remarkable capabilities across various tasks. However, their widespread application is hindered by the resource-intensive decoding process. To address this challenge, current approaches have incorporated additional decoding heads to enable parallel prediction of multiple subsequent tokens, thereby achieving inference acceleration. Nevertheless, the accuracy of these decoding heads falls short of the auto-regressive decoding approach. In light of these limitations, we propose Chimera, a novel framework specifically designed for speculative sampling. Within this framework, we introduce a lightweight draft model that effectively utilizes previously generated tokens to predict subsequent words. To ensure both accuracy and efficiency, we present two strategies within the lightweight draft model. Firstly, we focus on capturing short-range dependencies at the bottom layer. Secondly, we leverage the readily available representations from the original <b>LLM.Through</b> empirical evaluation on the Vicuna and <b>LlaMA-2</b> series, Chimera demonstrates impressive results, achieving an average latency speedup ratio of 2.7x compared to the vanilla auto-regressive decoding approach. This highlights the potential of our proposed framework in significantly improving the efficiency of <b>large</b> <b>language</b> <b>models</b> during the decoding process.</p></p class="citation"></blockquote><h3 id=1621--16117-multicontrievers-analysis-of-dense-retrieval-representations-seraphina-goldfarb-tarrant-et-al-2024>(16/21 | 16/117) MultiContrievers: Analysis of Dense Retrieval Representations (Seraphina Goldfarb-Tarrant et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Seraphina Goldfarb-Tarrant, Pedro Rodriguez, Jane Dwivedi-Yu, Patrick Lewis. (2024)<br><strong>MultiContrievers: Analysis of Dense Retrieval Representations</strong><br><button class=copy-to-clipboard title="MultiContrievers: Analysis of Dense Retrieval Representations" index=16>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-16 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-IR, cs.CL<br>Keyword Score: 23<br>Keywords: Benchmarking, Dense Retrieval, BERT<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15925v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15925v1.pdf filename=2402.15925v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Dense</b> <b>retrievers</b> compress source documents into (possibly lossy) vector representations, yet there is little analysis of what information is lost versus preserved, and how it affects downstream tasks. We conduct the first analysis of the information captured by <b>dense</b> <b>retrievers</b> compared to the language models they are based on (e.g., <b>BERT</b> versus Contriever). We use 25 MultiBert checkpoints as randomized initialisations to train MultiContrievers, a set of 25 contriever models. We test whether specific pieces of information &ndash; such as gender and occupation &ndash; can be extracted from contriever vectors of wikipedia-like documents. We measure this extractability via information theoretic probing. We then examine the relationship of extractability to performance and gender bias, as well as the sensitivity of these results to many random initialisations and data shuffles. We find that (1) contriever models have significantly increased extractability, but extractability usually correlates poorly with <b>benchmark</b> performance 2) gender bias is present, but is not caused by the contriever representations 3) there is high sensitivity to both random initialisation and to data shuffle, suggesting that future retrieval research should test across a wider spread of both.</p></p class="citation"></blockquote><h3 id=1721--17117-frustratingly-simple-prompting-based-text-denoising-jungyeul-park-et-al-2024>(17/21 | 17/117) Frustratingly Simple Prompting-based Text Denoising (Jungyeul Park et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jungyeul Park, Mengyang Qiu. (2024)<br><strong>Frustratingly Simple Prompting-based Text Denoising</strong><br><button class=copy-to-clipboard title="Frustratingly Simple Prompting-based Text Denoising" index=17>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-17 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Essay Scoring, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15931v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15931v1.pdf filename=2402.15931v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper introduces a novel perspective on the automated <b>essay</b> <b>scoring</b> (AES) task, challenging the conventional view of the ASAP dataset as a static entity. Employing simple text denoising techniques using <b>prompting,</b> we explore the dynamic potential within the dataset. While acknowledging the previous emphasis on building regression systems, our paper underscores how making minor changes to a dataset through text denoising can enhance the final results.</p></p class="citation"></blockquote><h3 id=1821--18117-mathwell-generating-educational-math-word-problems-at-scale-bryan-r-christ-et-al-2024>(18/21 | 18/117) MATHWELL: Generating Educational Math Word Problems at Scale (Bryan R Christ et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bryan R Christ, Jonathan Kropko, Thomas Hartvigsen. (2024)<br><strong>MATHWELL: Generating Educational Math Word Problems at Scale</strong><br><button class=copy-to-clipboard title="MATHWELL: Generating Educational Math Word Problems at Scale" index=18>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-18 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Fine-tuning, LLaMA<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15861v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15861v2.pdf filename=2402.15861v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Math word problems are critical K-8 educational tools, but writing them is time-consuming and requires domain expertise. We suggest that language models can support K-8 math education by automatically generating problems at scale. To be educational, generated problems must be 1) solvable, 2) accurate, and 3) appropriate. Existing datasets are unlabeled for these criteria, making them ill-suited for training problem generators. We introduce MATHWELL, a <b>Llama-2</b> (70B) model iteratively <b>finetuned</b> to generate K-8 math word problems using data from expert annotation. Using MATHWELL, we generate the largest English word problem dataset with Program of Thought (PoT) rationales to date, containing 20,490 problems. 3,484 are scored by domain experts who find MATHWELL has a 40% higher share of problems that have executable solutions and meet all criteria than alternatives, with 74% of its problems with executable solutions being solvable, accurate, and appropriate. We release our model, data, and annotations.</p></p class="citation"></blockquote><h3 id=1921--19117-a-theoretical-result-on-the-inductive-bias-of-rnn-language-models-anej-svete-et-al-2024>(19/21 | 19/117) A Theoretical Result on the Inductive Bias of RNN Language Models (Anej Svete et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Anej Svete, Robin Shing Moon Chan, Ryan Cotterell. (2024)<br><strong>A Theoretical Result on the Inductive Bias of RNN Language Models</strong><br><button class=copy-to-clipboard title="A Theoretical Result on the Inductive Bias of RNN Language Models" index=19>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-19 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CC, cs-CL, cs-LG, cs.CL<br>Keyword Score: 20<br>Keywords: Recurrent Neural Network, Recurrent Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15814v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15814v1.pdf filename=2402.15814v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent work by Hewitt et al. (2020) provides a possible interpretation of the empirical success of <b>recurrent</b> <b>neural</b> <b>networks</b> <b>(RNNs)</b> as language models (LMs). It shows that <b>RNNs</b> can efficiently represent bounded hierarchical structures that are prevalent in human language. This suggests that <b>RNNs&rsquo;</b> success might be linked to their ability to model hierarchy. However, a closer inspection of Hewitt et al.&rsquo;s (2020) construction shows that it is not limited to hierarchical LMs, posing the question of what \emph{other classes} of LMs can be efficiently represented by <b>RNNs.</b> To this end, we generalize their construction to show that <b>RNNs</b> can efficiently represent a larger class of LMs: Those that can be represented by a pushdown automaton with a bounded stack and a generalized stack update function. This is analogous to an automaton that keeps a memory of a fixed number of symbols and updates the memory with a simple update mechanism. Altogether, the efficiency in representing a diverse class of non-hierarchical LMs posits a lack of concrete cognitive and human-language-centered inductive biases in <b>RNNs.</b></p></p class="citation"></blockquote><h3 id=2021--20117-query-augmentation-by-decoding-semantics-from-brain-signals-ziyi-ye-et-al-2024>(20/21 | 20/117) Query Augmentation by Decoding Semantics from Brain Signals (Ziyi Ye et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ziyi Ye, Jingtao Zhan, Qingyao Ai, Yiqun Liu, Maarten de Rijke, Christina Lioma, Tuukka Ruotsalo. (2024)<br><strong>Query Augmentation by Decoding Semantics from Brain Signals</strong><br><button class=copy-to-clipboard title="Query Augmentation by Decoding Semantics from Brain Signals" index=20>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-20 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-IR, cs.CL<br>Keyword Score: 20<br>Keywords: Document Ranking, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15708v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15708v1.pdf filename=2402.15708v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Query augmentation is a crucial technique for refining semantically imprecise queries. Traditionally, query augmentation relies on extracting information from initially retrieved, potentially relevant <b>documents.</b> <b>If</b> the quality of the initially retrieved <b>documents</b> <b>is</b> low, then the effectiveness of query augmentation would be limited as well. We propose Brain-Aug, which enhances a query by incorporating semantic information decoded from brain signals. BrainAug generates the continuation of the original query with a <b>prompt</b> constructed with brain signal information and a ranking-oriented inference approach. Experimental results on fMRI (functional magnetic resonance imaging) datasets show that Brain-Aug produces semantically more accurate queries, leading to improved <b>document</b> <b>ranking</b> performance. Such improvement brought by brain signals is particularly notable for ambiguous queries.</p></p class="citation"></blockquote><h3 id=2121--21117-measuring-bargaining-abilities-of-llms-a-benchmark-and-a-buyer-enhancement-method-tian-xia-et-al-2024>(21/21 | 21/117) Measuring Bargaining Abilities of LLMs: A Benchmark and A Buyer-Enhancement Method (Tian Xia et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tian Xia, Zhiwei He, Tong Ren, Yibo Miao, Zhuosheng Zhang, Yang Yang, Rui Wang. (2024)<br><strong>Measuring Bargaining Abilities of LLMs: A Benchmark and A Buyer-Enhancement Method</strong><br><button class=copy-to-clipboard title="Measuring Bargaining Abilities of LLMs: A Benchmark and A Buyer-Enhancement Method" index=21>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-21 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-GT, cs.CL<br>Keyword Score: 13<br>Keywords: Benchmarking, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15813v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15813v2.pdf filename=2402.15813v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Bargaining is an important and unique part of negotiation between humans. As <b>LLM-driven</b> agents learn to negotiate and act like real humans, how to evaluate agents&rsquo; bargaining abilities remains an open problem. For the first time, we formally described the Bargaining task as an asymmetric incomplete information game, defining the gains of the Buyer and Seller in multiple bargaining processes. It allows us to quantitatively assess an agent&rsquo;s performance in the Bargain task. We collected a real product price dataset, AmazonHistoryPrice, and conducted evaluations of various <b>LLM</b> agents&rsquo; bargaining abilities. We find that playing a Buyer is much harder than a Seller, and increasing model size can not effectively improve the Buyer&rsquo;s performance. To address the challenge, we propose a novel approach called OG-Narrator that integrates a deterministic Offer Generator to control the price range of Buyer&rsquo;s offers, and an <b>LLM</b> Narrator to create natural language sentences for generated offers. Experimental results show that OG-Narrator improves the buyer&rsquo;s deal rates from 26.67% to 88.88% and brings a ten times of multiplication of profits on all baselines, even a model that has not been aligned.</p></p class="citation"></blockquote><h2 id=cscv-22>cs.CV (22)</h2><h3 id=122--22117-res-vmamba-fine-grained-food-category-visual-classification-using-selective-state-space-models-with-deep-residual-learning-chi-sheng-chen-et-al-2024>(1/22 | 22/117) Res-VMamba: Fine-Grained Food Category Visual Classification Using Selective State Space Models with Deep Residual Learning (Chi-Sheng Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chi-Sheng Chen, Guan-Ying Chen, Dong Zhou, Di Jiang, Dai-Shi Chen. (2024)<br><strong>Res-VMamba: Fine-Grained Food Category Visual Classification Using Selective State Space Models with Deep Residual Learning</strong><br><button class=copy-to-clipboard title="Res-VMamba: Fine-Grained Food Category Visual Classification Using Selective State Space Models with Deep Residual Learning" index=22>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-22 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 73<br>Keywords: Vision Transformer, Benchmarking, Convolution, Convolutional Neural Network, Convolutional Neural Network, Transformer, Self-Attention, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15761v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15761v1.pdf filename=2402.15761v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Food classification is the foundation for developing food <b>vision</b> <b>tasks</b> and plays a key role in the burgeoning field of computational nutrition. Due to the complexity of food requiring fine-grained classification, recent academic research mainly modifies <b>Convolutional</b> <b>Neural</b> <b>Networks</b> <b>(CNNs)</b> and/or <b>Vision</b> <b>Transformers</b> (ViTs) to perform food category classification. However, to learn fine-grained features, the <b>CNN</b> backbone needs additional structural design, whereas ViT, containing the <b>self-attention</b> module, has increased computational complexity. In recent months, a new Sequence State Space (S4) model, through a Selection mechanism and computation with a Scan (S6), colloquially termed Mamba, has demonstrated superior performance and computation efficiency compared to the <b>Transformer</b> architecture. The VMamba model, which incorporates the Mamba mechanism into image tasks (such as classification), currently establishes the state-of-the-art (SOTA) on the ImageNet dataset. In this research, we introduce an academically underestimated food dataset CNFOOD-241, and pioneer the integration of a residual learning framework within the VMamba model to concurrently harness both global and local state features inherent in the original VMamba architectural design. The research results show that VMamba surpasses current SOTA models in fine-grained and food classification. The proposed Res-VMamba further improves the classification accuracy to 79.54% without pretrained weight. Our findings elucidate that our proposed methodology establishes a new <b>benchmark</b> for SOTA performance in food recognition on the CNFOOD-241 dataset. The code can be obtained on GitHub: <a href=https://github.com/ChiShengChen/ResVMamba>https://github.com/ChiShengChen/ResVMamba</a>.</p></p class="citation"></blockquote><h3 id=222--23117-explainable-contrastive-and-cost-sensitive-learning-for-cervical-cancer-classification-ashfiqun-mustari-et-al-2024>(2/22 | 23/117) Explainable Contrastive and Cost-Sensitive Learning for Cervical Cancer Classification (Ashfiqun Mustari et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ashfiqun Mustari, Rushmia Ahmed, Afsara Tasnim, Jakia Sultana Juthi, G M Shahariar. (2024)<br><strong>Explainable Contrastive and Cost-Sensitive Learning for Cervical Cancer Classification</strong><br><button class=copy-to-clipboard title="Explainable Contrastive and Cost-Sensitive Learning for Cervical Cancer Classification" index=23>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-23 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-LG, cs.CV<br>Keyword Score: 70<br>Keywords: Contrastive Learning, Convolution, Convolutional Neural Network, Convolutional Neural Network, Explainable AI, Fine-tuning, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15905v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15905v1.pdf filename=2402.15905v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper proposes an efficient system for classifying cervical cancer cells using pre-trained <b>convolutional</b> <b>neural</b> <b>networks</b> <b>(CNNs).</b> We first <b>fine-tune</b> five pre-trained <b>CNNs</b> and minimize the overall cost of misclassification by prioritizing accuracy for certain classes that have higher associated costs or importance. To further enhance the performance of the models, <b>supervised</b> <b>contrastive</b> <b>learning</b> is included to make the models more adept at capturing important features and patterns. Extensive experimentation are conducted to evaluate the proposed system on the SIPaKMeD dataset. The experimental results demonstrate the effectiveness of the developed system, achieving an accuracy of 97.29%. To make our system more trustworthy, we have employed several <b>explainable</b> <b>AI</b> techniques to interpret how the models reached a specific decision. The implementation of the system can be found at - <a href=https://github.com/isha-67/CervicalCancerStudy>https://github.com/isha-67/CervicalCancerStudy</a>.</p></p class="citation"></blockquote><h3 id=322--24117-increasing-sam-zero-shot-performance-on-multimodal-medical-images-using-gpt-4-generated-descriptive-prompts-without-human-annotation-zekun-jiang-et-al-2024>(3/22 | 24/117) Increasing SAM Zero-Shot Performance on Multimodal Medical Images Using GPT-4 Generated Descriptive Prompts Without Human Annotation (Zekun Jiang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zekun Jiang, Dongjie Cheng, Ziyuan Qin, Jun Gao, Qicheng Lao, Kang Li, Le Zhang. (2024)<br><strong>Increasing SAM Zero-Shot Performance on Multimodal Medical Images Using GPT-4 Generated Descriptive Prompts Without Human Annotation</strong><br><button class=copy-to-clipboard title="Increasing SAM Zero-Shot Performance on Multimodal Medical Images Using GPT-4 Generated Descriptive Prompts Without Human Annotation" index=24>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-24 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 66<br>Keywords: Multi-modal, Multi-modal, Zero-shot, GPT, GPT-4, Large Language Model, Prompt, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15759v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15759v1.pdf filename=2402.15759v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This study develops and evaluates a novel <b>multimodal</b> medical image <b>zero-shot</b> segmentation algorithm named Text-Visual-Prompt SAM (TV-SAM) without any manual annotations. TV-SAM incorporates and integrates <b>large</b> <b>language</b> <b>model</b> <b>GPT-4,</b> Vision Language Model GLIP, and Segment Anything Model (SAM), to autonomously generate descriptive text <b>prompts</b> and visual bounding box <b>prompts</b> from medical images, thereby enhancing SAM for <b>zero-shot</b> segmentation. Comprehensive evaluations are implemented on seven public datasets encompassing eight imaging modalities to demonstrate that TV-SAM can effectively segment unseen targets across various modalities without additional training, significantly outperforming SAM AUTO and GSAM, closely matching the performance of SAM BBOX with gold standard bounding box <b>prompts,</b> and surpassing the state-of-the-art on specific datasets like ISIC and WBC. The study indicates that TV-SAM serves as an effective <b>multimodal</b> medical image <b>zero-shot</b> segmentation algorithm, highlighting the significant contribution of <b>GPT-4</b> to <b>zero-shot</b> segmentation. By integrating foundational models such as <b>GPT-4,</b> GLIP, and SAM, it could enhance the capability to address complex problems in specialized domains. The code is available at: <a href=https://github.com/JZK00/TV-SAM>https://github.com/JZK00/TV-SAM</a>.</p></p class="citation"></blockquote><h3 id=422--25117-bridging-the-gap-between-2d-and-3d-visual-question-answering-a-fusion-approach-for-3d-vqa-wentao-mo-et-al-2024>(4/22 | 25/117) Bridging the Gap between 2D and 3D Visual Question Answering: A Fusion Approach for 3D VQA (Wentao Mo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wentao Mo, Yang Liu. (2024)<br><strong>Bridging the Gap between 2D and 3D Visual Question Answering: A Fusion Approach for 3D VQA</strong><br><button class=copy-to-clipboard title="Bridging the Gap between 2D and 3D Visual Question Answering: A Fusion Approach for 3D VQA" index=25>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-25 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CL, cs-CV, cs-LG, cs.CV<br>Keyword Score: 63<br>Keywords: Multi-modal, Transformer, Question Answering, Reasoning, Visual Question Answering, Visual Question Answering, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15933v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15933v1.pdf filename=2402.15933v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In 3D <b>Visual</b> <b>Question</b> <b>Answering</b> (3D <b>VQA),</b> the scarcity of fully annotated data and limited <b>visual</b> <b>content</b> <b>diversity</b> hampers the generalization to novel scenes and 3D concepts (e.g., only around 800 scenes are utilized in ScanQA and SQA dataset). Current approaches resort supplement 3D <b>reasoning</b> with 2D information. However, these methods face challenges: either they use top-down 2D views that introduce overly complex and sometimes <b>question-irrelevant</b> <b>visual</b> <b>clues,</b> <b>or</b> they rely on globally aggregated scene/image-level representations from 2D VLMs, losing the fine-grained <b>vision-language</b> correlations. To overcome these limitations, our approach utilizes <b>question-conditional</b> <b>2D</b> view selection procedure, pinpointing semantically relevant 2D inputs for crucial <b>visual</b> <b>clues.</b> <b>We</b> then integrate this 2D knowledge into the 3D-VQA system via a two-branch <b>Transformer</b> structure. This structure, featuring a Twin-Transformer design, compactly combines 2D and 3D modalities and captures fine-grained correlations between modalities, allowing them mutually augmenting each other. Integrating proposed mechanisms above, we present BridgeQA, that offers a fresh perspective on <b>multi-modal</b> <b>transformer-based</b> architectures for 3D-VQA. Experiments validate that BridgeQA achieves state-of-the-art on 3D-VQA datasets and significantly outperforms existing solutions. Code is available at $\href{https://github.com/matthewdm0816/BridgeQA}{\text{this URL}}$.</p></p class="citation"></blockquote><h3 id=522--26117-navid-video-based-vlm-plans-the-next-step-for-vision-and-language-navigation-jiazhao-zhang-et-al-2024>(5/22 | 26/117) NaVid: Video-based VLM Plans the Next Step for Vision-and-Language Navigation (Jiazhao Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiazhao Zhang, Kunyu Wang, Rongtao Xu, Gengze Zhou, Yicong Hong, Xiaomeng Fang, Qi Wu, Zhizheng Zhang, Wang He. (2024)<br><strong>NaVid: Video-based VLM Plans the Next Step for Vision-and-Language Navigation</strong><br><button class=copy-to-clipboard title="NaVid: Video-based VLM Plans the Next Step for Vision-and-Language Navigation" index=26>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-26 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-RO, cs.CV<br>Keyword Score: 60<br>Keywords: Out-of-distribution, Simulation, Simulator, Instruction Following, Vision-and-Language, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15852v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15852v2.pdf filename=2402.15852v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Vision-and-Language</b> Navigation (VLN) stands as a key research problem of Embodied AI, aiming at enabling agents to navigate in unseen environments following linguistic <b>instructions.</b> <b>In</b> this field, generalization is a long-standing challenge, either to <b>out-of-distribution</b> scenes or from Sim to Real. In this paper, we propose NaVid, a video-based large vision language model (VLM), to mitigate such a generalization gap. NaVid makes the first endeavour to showcase the capability of VLMs to achieve state-of-the-art level navigation performance without any maps, odometer and depth inputs. Following human <b>instruction,</b> <b>NaVid</b> only requires an on-the-fly video stream from a monocular RGB camera equipped on the robot to output the next-step action. Our formulation mimics how humans navigate and naturally gets rid of the problems introduced by odometer noises, and the Sim2Real gaps from map or depth inputs. Moreover, our video-based approach can effectively encode the historical observations of robots as spatio-temporal contexts for decision-making and <b>instruction</b> <b>following.</b> We train NaVid with 550k navigation samples collected from VLN-CE trajectories, including action-planning and <b>instruction-reasoning</b> <b>samples,</b> along with 665k large-scale web data. Extensive experiments show that NaVid achieves SOTA performance in <b>simulation</b> environments and the real world, demonstrating superior cross-dataset and Sim2Real transfer. We thus believe our proposed VLM approach plans the next step for not only the navigation agents but also this research field.</p></p class="citation"></blockquote><h3 id=622--27117-multimodal-instruction-tuning-with-conditional-mixture-of-lora-ying-shen-et-al-2024>(6/22 | 27/117) Multimodal Instruction Tuning with Conditional Mixture of LoRA (Ying Shen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ying Shen, Zhiyang Xu, Qifan Wang, Yu Cheng, Wenpeng Yin, Lifu Huang. (2024)<br><strong>Multimodal Instruction Tuning with Conditional Mixture of LoRA</strong><br><button class=copy-to-clipboard title="Multimodal Instruction Tuning with Conditional Mixture of LoRA" index=27>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-27 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 56<br>Keywords: Fine-tuning, Fine-tuning, Multi-modal, Multi-modal, Zero-shot, Instruction Tuning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15896v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15896v1.pdf filename=2402.15896v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Multimodal</b> <b>Large</b> <b>Language</b> <b>Models</b> (MLLMs) have demonstrated remarkable proficiency in diverse tasks across different domains, with an increasing focus on improving their <b>zero-shot</b> generalization capabilities for unseen <b>multimodal</b> tasks. <b>Multimodal</b> <b>instruction</b> <b>tuning</b> has emerged as a successful strategy for achieving <b>zero-shot</b> generalization by <b>fine-tuning</b> pre-trained models on diverse <b>multimodal</b> tasks through <b>instructions.</b> <b>As</b> MLLMs grow in complexity and size, the need for parameter-efficient <b>fine-tuning</b> methods like Low-Rank Adaption (LoRA), which <b>fine-tunes</b> with a minimal set of parameters, becomes essential. However, applying LoRA in <b>multimodal</b> <b>instruction</b> <b>tuning</b> presents the challenge of task interference, which leads to performance degradation, especially when dealing with a broad array of <b>multimodal</b> tasks. To address this, this paper introduces a novel approach that integrates <b>multimodal</b> <b>instruction</b> <b>tuning</b> with Conditional Mixture-of-LoRA (MixLoRA). It innovates upon LoRA by dynamically constructing low-rank adaptation matrices tailored to the unique demands of each input instance, aiming to mitigate task interference. Experimental results on various <b>multimodal</b> evaluation datasets indicate that MixLoRA not only outperforms the conventional LoRA with the same or even higher ranks, demonstrating its efficacy and adaptability in diverse <b>multimodal</b> tasks.</p></p class="citation"></blockquote><h3 id=722--28117-parameter-efficient-prompt-learning-for-3d-point-cloud-understanding-hongyu-sun-et-al-2024>(7/22 | 28/117) Parameter-efficient Prompt Learning for 3D Point Cloud Understanding (Hongyu Sun et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hongyu Sun, Yongcai Wang, Wang Chen, Haoran Deng, Deying Li. (2024)<br><strong>Parameter-efficient Prompt Learning for 3D Point Cloud Understanding</strong><br><button class=copy-to-clipboard title="Parameter-efficient Prompt Learning for 3D Point Cloud Understanding" index=28>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-28 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 53<br>Keywords: Few-shot, Few-shot Learning, Fine-tuning, Multi-modal, Prompt, Prompt Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15823v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15823v1.pdf filename=2402.15823v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents a parameter-efficient <b>prompt</b> <b>tuning</b> method, named PPT, to adapt a large <b>multi-modal</b> model for 3D point cloud understanding. Existing strategies are quite expensive in computation and storage, and depend on time-consuming <b>prompt</b> <b>engineering.</b> We address the problems from three aspects. Firstly, a PromptLearner module is devised to replace hand-crafted <b>prompts</b> <b>with</b> learnable contexts to automate the <b>prompt</b> <b>tuning</b> process. Then, we lock the pre-trained backbone instead of adopting the full <b>fine-tuning</b> paradigm to substantially improve the parameter efficiency. Finally, a lightweight PointAdapter module is arranged near target tasks to enhance <b>prompt</b> <b>tuning</b> for 3D point cloud understanding. Comprehensive experiments are conducted to demonstrate the superior parameter and data efficiency of the proposed method.Meanwhile, we obtain new records on 4 public datasets and multiple 3D tasks, i.e., point cloud recognition, <b>few-shot</b> <b>learning,</b> and part segmentation. The implementation is available at <a href=https://github.com/auniquesun/PPT>https://github.com/auniquesun/PPT</a>.</p></p class="citation"></blockquote><h3 id=822--29117-clipose-category-level-object-pose-estimation-with-pre-trained-vision-language-knowledge-xiao-lin-et-al-2024>(8/22 | 29/117) CLIPose: Category-Level Object Pose Estimation with Pre-trained Vision-Language Knowledge (Xiao Lin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiao Lin, Minghao Zhu, Ronghao Dang, Guangliang Zhou, Shaolong Shu, Feng Lin, Chengju Liu, Qijun Chen. (2024)<br><strong>CLIPose: Category-Level Object Pose Estimation with Pre-trained Vision-Language Knowledge</strong><br><button class=copy-to-clipboard title="CLIPose: Category-Level Object Pose Estimation with Pre-trained Vision-Language Knowledge" index=29>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-29 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 46<br>Keywords: Benchmarking, Contrastive Learning, Fine-tuning, Multi-modal, Prompt, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15726v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15726v1.pdf filename=2402.15726v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Most of existing category-level object pose estimation methods devote to learning the object category information from point cloud modality. However, the scale of 3D datasets is limited due to the high cost of 3D data collection and annotation. Consequently, the category features extracted from these limited point cloud samples may not be comprehensive. This motivates us to investigate whether we can draw on knowledge of other modalities to obtain category information. Inspired by this motivation, we propose CLIPose, a novel 6D pose framework that employs the pre-trained <b>vision-language</b> model to develop better learning of object category information, which can fully leverage abundant semantic knowledge in image and text modalities. To make the 3D encoder learn category-specific features more efficiently, we align representations of three modalities in feature space via <b>multi-modal</b> <b>contrastive</b> <b>learning.</b> In addition to exploiting the pre-trained knowledge of the CLIP&rsquo;s model, we also expect it to be more sensitive with pose parameters. Therefore, we introduce a <b>prompt</b> tuning approach to <b>fine-tune</b> image encoder while we incorporate rotations and translations information in the text descriptions. CLIPose achieves state-of-the-art performance on two mainstream <b>benchmark</b> datasets, REAL275 and CAMERA25, and runs in real-time during inference (40FPS).</p></p class="citation"></blockquote><h3 id=922--30117-irconstyle-image-restoration-framework-using-contrastive-learning-and-style-transfer-dongqi-fan-et-al-2024>(9/22 | 30/117) IRConStyle: Image Restoration Framework Using Contrastive Learning and Style Transfer (Dongqi Fan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dongqi Fan, Xin Zhao, Liang Chang. (2024)<br><strong>IRConStyle: Image Restoration Framework Using Contrastive Learning and Style Transfer</strong><br><button class=copy-to-clipboard title="IRConStyle: Image Restoration Framework Using Contrastive Learning and Style Transfer" index=30>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-30 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 43<br>Keywords: Benchmarking, Contrastive Learning, Convolutional Neural Network, Transformer, Style Transfer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15784v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15784v2.pdf filename=2402.15784v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recently, the <b>contrastive</b> <b>learning</b> paradigm has achieved remarkable success in high-level tasks such as classification, detection, and segmentation. However, <b>contrastive</b> <b>learning</b> applied in low-level tasks, like image restoration, is limited, and its effectiveness is uncertain. This raises a question: Why does the <b>contrastive</b> <b>learning</b> paradigm not yield satisfactory results in image restoration? In this paper, we conduct in-depth analyses and propose three guidelines to address the above question. In addition, inspired by <b>style</b> <b>transfer</b> and based on <b>contrastive</b> <b>learning,</b> we propose a novel module for image restoration called \textbf{ConStyle}, which can be efficiently integrated into any U-Net structure network. By leveraging the flexibility of ConStyle, we develop a \textbf{general restoration network} for image restoration. ConStyle and the general restoration network together form an image restoration framework, namely \textbf{IRConStyle}. To demonstrate the capability and compatibility of ConStyle, we replace the general restoration network with <b>transformer-based,</b> <b>CNN-based,</b> and MLP-based networks, respectively. We perform extensive experiments on various image restoration tasks, including denoising, deblurring, deraining, and dehazing. The results on 19 <b>benchmarks</b> demonstrate that ConStyle can be integrated with any U-Net-based network and significantly enhance performance. For instance, ConStyle NAFNet significantly outperforms the original NAFNet on SOTS outdoor (dehazing) and Rain100H (deraining) datasets, with PSNR improvements of 4.16 dB and 3.58 dB with 85% fewer parameters.</p></p class="citation"></blockquote><h3 id=1022--31117-sequential-visual-and-semantic-consistency-for-semi-supervised-text-recognition-mingkun-yang-et-al-2024>(10/22 | 31/117) Sequential Visual and Semantic Consistency for Semi-supervised Text Recognition (Mingkun Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mingkun Yang, Biao Yang, Minghui Liao, Yingying Zhu, Xiang Bai. (2024)<br><strong>Sequential Visual and Semantic Consistency for Semi-supervised Text Recognition</strong><br><button class=copy-to-clipboard title="Sequential Visual and Semantic Consistency for Semi-supervised Text Recognition" index=31>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-31 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 33<br>Keywords: Benchmarking, Reinforcement Learning, Semi-Supervised Learning, Text2image<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15806v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15806v1.pdf filename=2402.15806v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Scene text recognition (STR) is a challenging task that requires large-scale annotated data for training. However, collecting and labeling real text images is expensive and time-consuming, which limits the availability of real data. Therefore, most existing STR methods resort to synthetic data, which may introduce domain discrepancy and degrade the performance of STR models. To alleviate this problem, recent <b>semi-supervised</b> <b>STR</b> methods exploit unlabeled real data by enforcing character-level consistency regularization between weakly and strongly augmented views of the same image. However, these methods neglect word-level consistency, which is crucial for sequence recognition tasks. This paper proposes a novel <b>semi-supervised</b> <b>learning</b> method for STR that incorporates word-level consistency regularization from both visual and semantic aspects. Specifically, we devise a shortest path alignment module to align the sequential visual features of different views and minimize their distance. Moreover, we adopt a <b>reinforcement</b> <b>learning</b> framework to optimize the semantic similarity of the predicted strings in the embedding space. We conduct extensive experiments on several standard and challenging STR <b>benchmarks</b> and demonstrate the superiority of our proposed method over existing <b>semi-supervised</b> <b>STR</b> methods.</p></p class="citation"></blockquote><h3 id=1122--32117-gimefive-towards-interpretable-facial-emotion-classification-jiawen-wang-et-al-2024>(11/22 | 32/117) GiMeFive: Towards Interpretable Facial Emotion Classification (Jiawen Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiawen Wang, Leah Kawka. (2024)<br><strong>GiMeFive: Towards Interpretable Facial Emotion Classification</strong><br><button class=copy-to-clipboard title="GiMeFive: Towards Interpretable Facial Emotion Classification" index=32>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-32 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 33<br>Keywords: Benchmarking, Convolution, Convolutional Neural Network, Emotion Recognition<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15662v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15662v1.pdf filename=2402.15662v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep <b>convolutional</b> <b>neural</b> <b>networks</b> have been shown to successfully recognize facial <b>emotions</b> <b>for</b> the past years in the realm of computer vision. However, the existing detection approaches are not always reliable or explainable, we here propose our model GiMeFive with interpretations, i.e., via layer activations and gradient-weighted class activation mapping. We compare against the state-of-the-art methods to classify the six facial <b>emotions.</b> <b>Empirical</b> results show that our model outperforms the previous methods in terms of accuracy on two Facial <b>Emotion</b> <b>Recognition</b> (FER) <b>benchmarks</b> and our aggregated FER GiMeFive. Furthermore, we explain our work in real-world image and video examples, as well as real-time live camera streams. Our code and supplementary material are available at https: //github.com/werywjw/SEP-CVDL.</p></p class="citation"></blockquote><h3 id=1222--33117-rauca-a-novel-physical-adversarial-attack-on-vehicle-detectors-via-robust-and-accurate-camouflage-generation-jiawei-zhou-et-al-2024>(12/22 | 33/117) RAUCA: A Novel Physical Adversarial Attack on Vehicle Detectors via Robust and Accurate Camouflage Generation (Jiawei Zhou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiawei Zhou, Linye Lyu, Daojing He, Yu Li. (2024)<br><strong>RAUCA: A Novel Physical Adversarial Attack on Vehicle Detectors via Robust and Accurate Camouflage Generation</strong><br><button class=copy-to-clipboard title="RAUCA: A Novel Physical Adversarial Attack on Vehicle Detectors via Robust and Accurate Camouflage Generation" index=33>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-33 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Simulation, Simulator, Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15853v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15853v1.pdf filename=2402.15853v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Adversarial</b> <b>camouflage</b> is a widely used physical attack against vehicle detectors for its superiority in multi-view attack performance. One promising approach involves using differentiable neural renderers to facilitate <b>adversarial</b> <b>camouflage</b> optimization through gradient back-propagation. However, existing methods often struggle to capture environmental characteristics during the rendering process or produce <b>adversarial</b> <b>textures</b> that can precisely map to the target vehicle, resulting in suboptimal attack performance. Moreover, these approaches neglect diverse weather conditions, reducing the efficacy of generated camouflage across varying weather scenarios. To tackle these challenges, we propose a robust and accurate camouflage generation method, namely RAUCA. The core of RAUCA is a novel neural rendering component, Neural Renderer Plus (NRP), which can accurately project vehicle textures and render images with environmental characteristics such as lighting and weather. In addition, we integrate a multi-weather dataset for camouflage generation, leveraging the NRP to enhance the attack robustness. Experimental results on six popular object detectors show that RAUCA consistently outperforms existing methods in both <b>simulation</b> and real-world settings.</p></p class="citation"></blockquote><h3 id=1322--34117-multiple-instance-learning-for-glioma-diagnosis-using-hematoxylin-and-eosin-whole-slide-images-an-indian-cohort-study-ekansh-chauhan-et-al-2024>(13/22 | 34/117) Multiple Instance Learning for Glioma Diagnosis using Hematoxylin and Eosin Whole Slide Images: An Indian cohort Study (Ekansh Chauhan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ekansh Chauhan, Amit Sharma, Megha S Uppin, C. V. Jawahar, Vinod P. K. (2024)<br><strong>Multiple Instance Learning for Glioma Diagnosis using Hematoxylin and Eosin Whole Slide Images: An Indian cohort Study</strong><br><button class=copy-to-clipboard title="Multiple Instance Learning for Glioma Diagnosis using Hematoxylin and Eosin Whole Slide Images: An Indian cohort Study" index=34>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-34 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 23<br>Keywords: Benchmarking, Multiple Instance Learning, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15832v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15832v1.pdf filename=2402.15832v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Brain tumors represent a severe and life-threatening condition, demanding precise diagnosis and tailored treatment strategies. This study advances patient care with findings from rigorous <b>multiple-instance-learning</b> <b>experimentations</b> <b>across</b> various feature extractors and aggregators in brain tumor histopathology. It establishes new performance <b>benchmarks</b> in glioma subtype classification across <b>multiple</b> <b>datasets,</b> <b>including</b> a novel dataset focused on the Indian demographic (IPD-Brain), providing a valuable resource for existing research. Using a ResNet-50, pretrained on histopathology datasets, for feature extraction, combined with DTFD feature aggregator, our approach achieves state-of-the-art AUCs of 88.08 on IPD-Brain and 95.81 on TCGA-Brain dataset respectively for three-way glioma subtype classification. Moreover, it establishes new <b>benchmarks</b> in grading and detecting IHC molecular biomarkers (IDH1 (mutant R132H), TP53, ATRX, Ki-67) through H&amp;E stained whole slide images for the IPD-Brain dataset. The work also highlights a significant correlation between the model decision-making processes and the diagnostic <b>reasoning</b> of pathologists, underscoring its capability to mimic professional diagnostic procedures.</p></p class="citation"></blockquote><h3 id=1422--35117-enhanced-droplet-analysis-using-generative-adversarial-networks-tan-hanh-pham-et-al-2024>(14/22 | 35/117) Enhanced Droplet Analysis Using Generative Adversarial Networks (Tan-Hanh Pham et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tan-Hanh Pham, Kim-Doang Nguyen. (2024)<br><strong>Enhanced Droplet Analysis Using Generative Adversarial Networks</strong><br><button class=copy-to-clipboard title="Enhanced Droplet Analysis Using Generative Adversarial Networks" index=35>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-35 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Generative Adversarial Network, Generative Adversarial Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15909v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15909v1.pdf filename=2402.15909v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Precision devices play an important role in enhancing production quality and productivity in agricultural systems. Therefore, the optimization of these devices is essential in precision agriculture. Recently, with the advancements of deep learning, there have been several studies aiming to harness its capabilities for improving spray system performance. However, the effectiveness of these methods heavily depends on the size of the training dataset, which is expensive and time-consuming to collect. To address the challenge of insufficient training samples, this paper proposes an alternative solution by generating artificial images of droplets using <b>generative</b> <b>adversarial</b> <b>networks</b> <b>(GAN).</b> The <b>GAN</b> model is trained by using a small dataset captured by a high-speed camera and capable of generating images with progressively increasing resolution. The results demonstrate that the model can generate high-quality images with the size of $1024\times1024$. Furthermore, this research leverages recent advancements in computer vision and deep learning to develop a light droplet detector using the synthetic dataset. As a result, the detection model achieves a 16.06% increase in mean average precision (mAP) when utilizing the synthetic dataset. To the best of our knowledge, this work stands as the first to employ a <b>generative</b> <b>model</b> <b>for</b> augmenting droplet detection. Its significance lies not only in optimizing nozzle design for constructing efficient spray systems but also in addressing the common challenge of insufficient data in various precision agriculture tasks. This work offers a critical contribution to conserving resources while striving for optimal and sustainable agricultural practices.</p></p class="citation"></blockquote><h3 id=1522--36117-hir-diff-unsupervised-hyperspectral-image-restoration-via-improved-diffusion-models-li-pang-et-al-2024>(15/22 | 36/117) HIR-Diff: Unsupervised Hyperspectral Image Restoration Via Improved Diffusion Models (Li Pang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Li Pang, Xiangyu Rui, Long Cui, Hongzhong Wang, Deyu Meng, Xiangyong Cao. (2024)<br><strong>HIR-Diff: Unsupervised Hyperspectral Image Restoration Via Improved Diffusion Models</strong><br><button class=copy-to-clipboard title="HIR-Diff: Unsupervised Hyperspectral Image Restoration Via Improved Diffusion Models" index=36>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-36 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV, eess-IV<br>Keyword Score: 20<br>Keywords: Diffusion Model, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15865v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15865v1.pdf filename=2402.15865v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Hyperspectral image (HSI) restoration aims at recovering clean images from degraded observations and plays a vital role in downstream tasks. Existing model-based methods have limitations in accurately modeling the complex image characteristics with handcraft priors, and deep learning-based methods suffer from poor generalization ability. To alleviate these issues, this paper proposes an <b>unsupervised</b> HSI restoration framework with pre-trained <b>diffusion</b> <b>model</b> (HIR-Diff), which restores the clean HSIs from the product of two low-rank components, i.e., the reduced image and the coefficient matrix. Specifically, the reduced image, which has a low spectral dimension, lies in the image field and can be inferred from our improved <b>diffusion</b> <b>model</b> where a new guidance function with total variation (TV) prior is designed to ensure that the reduced image can be well sampled. The coefficient matrix can be effectively pre-estimated based on singular value decomposition (SVD) and rank-revealing QR (RRQR) factorization. Furthermore, a novel exponential noise schedule is proposed to accelerate the restoration process (about 5$\times$ acceleration for denoising) with little performance decrease. Extensive experimental results validate the superiority of our method in both performance and speed on a variety of HSI restoration tasks, including HSI denoising, noisy HSI super-resolution, and noisy HSI inpainting. The code is available at <a href=https://github.com/LiPang/HIRDiff>https://github.com/LiPang/HIRDiff</a>.</p></p class="citation"></blockquote><h3 id=1622--37117-dart-depth-enhanced-accurate-and-real-time-background-matting-hanxi-li-et-al-2024>(16/22 | 37/117) DART: Depth-Enhanced Accurate and Real-Time Background Matting (Hanxi Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hanxi Li, Guofeng Li, Bo Li, Lin Wu, Yan Cheng. (2024)<br><strong>DART: Depth-Enhanced Accurate and Real-Time Background Matting</strong><br><button class=copy-to-clipboard title="DART: Depth-Enhanced Accurate and Real-Time Background Matting" index=37>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-37 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Knowledge Distillation, Knowledge Distillation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15820v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15820v1.pdf filename=2402.15820v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Matting with a static background, often referred to as ``Background Matting" (BGM), has garnered significant attention within the computer vision community due to its pivotal role in various practical applications like webcasting and photo editing. Nevertheless, achieving highly accurate background matting remains a formidable challenge, primarily owing to the limitations inherent in conventional RGB images. These limitations manifest in the form of susceptibility to varying lighting conditions and unforeseen shadows. In this paper, we leverage the rich depth information provided by the RGB-Depth (RGB-D) cameras to enhance background matting performance in real-time, dubbed DART. Firstly, we adapt the original RGB-based BGM algorithm to incorporate depth information. The resulting model&rsquo;s output undergoes refinement through Bayesian inference, incorporating a background depth prior. The posterior prediction is then translated into a &ldquo;trimap,&rdquo; which is subsequently fed into a state-of-the-art matting algorithm to generate more precise alpha mattes. To ensure real-time matting capabilities, a critical requirement for many real-world applications, we <b>distill</b> the backbone of our model from a larger and more versatile BGM network. Our experiments demonstrate the superior performance of the proposed method. Moreover, thanks to the <b>distillation</b> operation, our method achieves a remarkable processing speed of 33 frames per second (fps) on a mid-range edge-computing device. This high efficiency underscores DART&rsquo;s immense potential for deployment in mobile applications}</p></p class="citation"></blockquote><h3 id=1722--38117-intelligent-director-an-automatic-framework-for-dynamic-visual-composition-using-chatgpt-sixiao-zheng-et-al-2024>(17/22 | 38/117) Intelligent Director: An Automatic Framework for Dynamic Visual Composition using ChatGPT (Sixiao Zheng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sixiao Zheng, Jingyang Huo, Yu Wang, Yanwei Fu. (2024)<br><strong>Intelligent Director: An Automatic Framework for Dynamic Visual Composition using ChatGPT</strong><br><button class=copy-to-clipboard title="Intelligent Director: An Automatic Framework for Dynamic Visual Composition using ChatGPT" index=38>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-38 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-MM, cs.CV<br>Keyword Score: 20<br>Keywords: ChatGPT, Style Transfer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15746v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15746v1.pdf filename=2402.15746v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the rise of short video platforms represented by TikTok, the trend of users expressing their creativity through photos and videos has increased dramatically. However, ordinary users lack the professional skills to produce high-quality videos using professional creation software. To meet the demand for intelligent and user-friendly video creation tools, we propose the Dynamic Visual Composition (DVC) task, an interesting and challenging task that aims to automatically integrate various media elements based on user requirements and create storytelling videos. We propose an Intelligent Director framework, utilizing LENS to generate descriptions for images and video frames and combining <b>ChatGPT</b> to generate coherent captions while recommending appropriate music names. Then, the best-matched music is obtained through music retrieval. Then, materials such as captions, images, videos, and music are integrated to seamlessly synthesize the video. Finally, we apply AnimeGANv2 for <b>style</b> <b>transfer.</b> We construct UCF101-DVC and Personal Album datasets and verified the effectiveness of our framework in solving DVC through qualitative and quantitative comparisons, along with user studies, demonstrating its substantial potential.</p></p class="citation"></blockquote><h3 id=1822--39117-general-purpose-image-encoder-dinov2-for-medical-image-registration-xinrui-song-et-al-2024>(18/22 | 39/117) General Purpose Image Encoder DINOv2 for Medical Image Registration (Xinrui Song et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xinrui Song, Xuanang Xu, Pingkun Yan. (2024)<br><strong>General Purpose Image Encoder DINOv2 for Medical Image Registration</strong><br><button class=copy-to-clipboard title="General Purpose Image Encoder DINOv2 for Medical Image Registration" index=39>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-39 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Fine-tuning, Foundation Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15687v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15687v1.pdf filename=2402.15687v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Existing medical image registration algorithms rely on either dataset specific training or local texture-based features to align images. The former cannot be reliably implemented without large modality-specific training datasets, while the latter lacks global semantics thus could be easily trapped at local minima. In this paper, we present a training-free deformable image registration method, DINO-Reg, leveraging a general purpose image encoder DINOv2 for image feature extraction. The DINOv2 encoder was trained using the ImageNet data containing natural images. We used the pretrained DINOv2 without any <b>finetuning.</b> Our method feeds the DINOv2 encoded features into a discrete optimizer to find the optimal deformable registration field. We conducted a series of experiments to understand the behavior and role of such a general purpose image encoder in the application of image registration. Combined with handcrafted features, our method won the first place in the recent OncoReg Challenge. To our knowledge, this is the first application of general vision <b>foundation</b> <b>models</b> in medical image registration.</p></p class="citation"></blockquote><h3 id=1922--40117-fedmm-federated-multi-modal-learning-with-modality-heterogeneity-in-computational-pathology-yuanzhe-peng-et-al-2024>(19/22 | 40/117) FedMM: Federated Multi-Modal Learning with Modality Heterogeneity in Computational Pathology (Yuanzhe Peng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuanzhe Peng, Jieming Bian, Jie Xu. (2024)<br><strong>FedMM: Federated Multi-Modal Learning with Modality Heterogeneity in Computational Pathology</strong><br><button class=copy-to-clipboard title="FedMM: Federated Multi-Modal Learning with Modality Heterogeneity in Computational Pathology" index=40>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-40 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-DC, cs.CV<br>Keyword Score: 16<br>Keywords: Federated Learning, Multi-modal, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15858v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15858v1.pdf filename=2402.15858v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The fusion of complementary <b>multimodal</b> information is crucial in computational pathology for accurate diagnostics. However, existing <b>multimodal</b> learning approaches necessitate access to users&rsquo; raw data, posing substantial privacy risks. While <b>Federated</b> <b>Learning</b> (FL) serves as a privacy-preserving alternative, it falls short in addressing the challenges posed by heterogeneous (yet possibly overlapped) modalities data across various hospitals. To bridge this gap, we propose a <b>Federated</b> <b>Multi-Modal</b> (FedMM) learning framework that federatedly trains multiple single-modal feature extractors to enhance subsequent classification performance instead of existing FL that aims to train a unified <b>multimodal</b> fusion model. Any participating hospital, even with small-scale datasets or limited devices, can leverage these <b>federated</b> <b>trained</b> extractors to perform local downstream tasks (e.g., classification) while ensuring data privacy. Through comprehensive evaluations of two publicly available datasets, we demonstrate that FedMM notably outperforms two baselines in accuracy and AUC metrics.</p></p class="citation"></blockquote><h3 id=2022--41117-multi-object-tracking-by-hierarchical-visual-representations-jinkun-cao-et-al-2024>(20/22 | 41/117) Multi-Object Tracking by Hierarchical Visual Representations (Jinkun Cao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jinkun Cao, Jiangmiao Pang, Kris Kitani. (2024)<br><strong>Multi-Object Tracking by Hierarchical Visual Representations</strong><br><button class=copy-to-clipboard title="Multi-Object Tracking by Hierarchical Visual Representations" index=41>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-41 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15895v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15895v1.pdf filename=2402.15895v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We propose a new visual hierarchical representation paradigm for multi-object tracking. It is more effective to discriminate between objects by attending to objects&rsquo; compositional visual regions and contrasting with the background contextual information instead of sticking to only the semantic visual cue such as bounding boxes. This compositional-semantic-contextual hierarchy is flexible to be integrated in different appearance-based multi-object tracking methods. We also propose an attention-based visual feature module to fuse the hierarchical visual representations. The proposed method achieves state-of-the-art accuracy and time efficiency among query-based methods on multiple multi-object tracking <b>benchmarks.</b></p></p class="citation"></blockquote><h3 id=2122--42117-multi-graph-graph-matching-for-coronary-artery-semantic-labeling-chen-zhao-et-al-2024>(21/22 | 42/117) Multi-graph Graph Matching for Coronary Artery Semantic Labeling (Chen Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chen Zhao, Zhihui Xu, Pukar Baral, Michel Esposito, Weihua Zhou. (2024)<br><strong>Multi-graph Graph Matching for Coronary Artery Semantic Labeling</strong><br><button class=copy-to-clipboard title="Multi-graph Graph Matching for Coronary Artery Semantic Labeling" index=42>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-42 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15894v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15894v1.pdf filename=2402.15894v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Coronary artery disease (CAD) stands as the leading cause of death worldwide, and invasive coronary angiography (ICA) remains the gold standard for assessing vascular anatomical information. However, deep learning-based methods encounter challenges in generating semantic labels for arterial segments, primarily due to the morphological similarity between arterial branches. To address this challenge, we model the vascular tree as a <b>graph</b> and propose a multi-graph <b>graph</b> matching (MGM) algorithm for coronary artery semantic labeling. The MGM algorithm assesses the similarity between arterials in multiple vascular tree <b>graphs,</b> taking into account the cycle consistency between each pair of <b>graphs.</b> This ensures that unannotated arterial segments are appropriately labeled by matching them with annotated segments. Through the incorporation of anatomical <b>graph</b> structure, radiomics features, and semantic mapping, the proposed MGM model achieves an impressive accuracy of 0.9471 for coronary artery semantic labeling. This approach presents a novel tool for coronary artery analysis using ICA videos, offering valuable insights into vascular health and pathology.</p></p class="citation"></blockquote><h3 id=2222--43117-deeplight-reconstructing-high-resolution-observations-of-nighttime-light-with-multi-modal-remote-sensing-data-lixian-zhang-et-al-2024>(22/22 | 43/117) DeepLight: Reconstructing High-Resolution Observations of Nighttime Light With Multi-Modal Remote Sensing Data (Lixian Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lixian Zhang, Runmin Dong, Shuai Yuan, Jinxiao Zhang, Mengxuan Chen, Juepeng Zheng, Haohuan Fu. (2024)<br><strong>DeepLight: Reconstructing High-Resolution Observations of Nighttime Light With Multi-Modal Remote Sensing Data</strong><br><button class=copy-to-clipboard title="DeepLight: Reconstructing High-Resolution Observations of Nighttime Light With Multi-Modal Remote Sensing Data" index=43>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-43 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV, eess-IV<br>Keyword Score: 3<br>Keywords: Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15659v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15659v1.pdf filename=2402.15659v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Nighttime light (NTL) remote sensing observation serves as a unique proxy for quantitatively assessing progress toward meeting a series of Sustainable Development Goals (SDGs), such as poverty estimation, urban sustainable development, and carbon emission. However, existing NTL observations often suffer from pervasive degradation and inconsistency, limiting their utility for computing the indicators defined by the SDGs. In this study, we propose a novel approach to reconstruct high-resolution NTL images using <b>multi-modal</b> remote sensing data. To support this research endeavor, we introduce DeepLightMD, a comprehensive dataset comprising data from five heterogeneous sensors, offering fine spatial resolution and rich spectral information at a national scale. Additionally, we present DeepLightSR, a calibration-aware method for building bridges between spatially heterogeneous modality data in the multi-modality super-resolution. DeepLightSR integrates calibration-aware alignment, an auxiliary-to-main multi-modality fusion, and an auxiliary-embedded refinement to effectively address spatial heterogeneity, fuse diversely representative features, and enhance performance in $8\times$ super-resolution (SR) tasks. Extensive experiments demonstrate the superiority of DeepLightSR over 8 competing methods, as evidenced by improvements in PSNR (2.01 dB $ \sim $ 13.25 dB) and PIQE (0.49 $ \sim $ 9.32). Our findings underscore the practical significance of our proposed dataset and model in reconstructing high-resolution NTL data, supporting efficiently and quantitatively assessing the SDG progress.</p></p class="citation"></blockquote><h2 id=cslg-26>cs.LG (26)</h2><h3 id=126--44117-predicting-outcomes-in-video-games-with-long-short-term-memory-networks-kittimate-chulajata-et-al-2024>(1/26 | 44/117) Predicting Outcomes in Video Games with Long Short Term Memory Networks (Kittimate Chulajata et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kittimate Chulajata, Sean Wu, Fabien Scalzo, Eun Sang Cha. (2024)<br><strong>Predicting Outcomes in Video Games with Long Short Term Memory Networks</strong><br><button class=copy-to-clipboard title="Predicting Outcomes in Video Games with Long Short Term Memory Networks" index=44>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-44 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs-MM, cs.LG<br>Keyword Score: 63<br>Keywords: Benchmarking, LSTM, LSTM, LSTM, Transformer, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15923v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15923v1.pdf filename=2402.15923v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Forecasting winners in E-sports with real-time analytics has the potential to further engage audiences watching major tournament events. However, making such real-time predictions is challenging due to unpredictable variables within the game involving diverse player strategies and decision-making. Our work attempts to enhance audience engagement within video game tournaments by introducing a real-time method of predicting wins. Our <b>Long</b> <b>Short</b> <b>Term</b> <b>Memory</b> Network <b>(LSTMs)</b> based approach enables efficient predictions of win-lose outcomes by only using the health indicator of each player as a time series. As a proof of concept, we evaluate our model&rsquo;s performance within a classic, two-player arcade game, Super Street Fighter II Turbo. We also <b>benchmark</b> our method against state of the art methods for time series forecasting; i.e. <b>Transformer</b> models found in <b>large</b> <b>language</b> <b>models</b> <b>(LLMs).</b> Finally, we open-source our data set and code in hopes of furthering work in predictive analysis for arcade games.</p></p class="citation"></blockquote><h3 id=226--45117-data-efficient-operator-learning-via-unsupervised-pretraining-and-in-context-learning-wuyang-chen-et-al-2024>(2/26 | 45/117) Data-Efficient Operator Learning via Unsupervised Pretraining and In-Context Learning (Wuyang Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wuyang Chen, Jialin Song, Pu Ren, Shashank Subramanian, Dmitriy Morozov, Michael W. Mahoney. (2024)<br><strong>Data-Efficient Operator Learning via Unsupervised Pretraining and In-Context Learning</strong><br><button class=copy-to-clipboard title="Data-Efficient Operator Learning via Unsupervised Pretraining and In-Context Learning" index=45>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-45 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-ML<br>Keyword Score: 60<br>Keywords: Out-of-distribution, Simulation, Simulator, Unsupervised Learning, In-context Learning, In-context Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15734v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15734v1.pdf filename=2402.15734v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent years have witnessed the promise of coupling machine learning methods and physical domain-specific insight for solving scientific problems based on partial differential equations (PDEs). However, being data-intensive, these methods still require a large amount of PDE data. This reintroduces the need for expensive numerical PDE solutions, partially undermining the original goal of avoiding these expensive <b>simulations.</b> In this work, seeking data efficiency, we design <b>unsupervised</b> pretraining and <b>in-context</b> <b>learning</b> methods for PDE operator learning. To reduce the need for training data with simulated solutions, we pretrain neural operators on unlabeled PDE data using reconstruction-based proxy tasks. To improve <b>out-of-distribution</b> performance, we further assist neural operators in flexibly leveraging <b>in-context</b> <b>learning</b> methods, without incurring extra training costs or designs. Extensive empirical evaluations on a diverse set of PDEs demonstrate that our method is highly data-efficient, more generalizable, and even outperforms conventional vision-pretrained models.</p></p class="citation"></blockquote><h3 id=326--46117-overcoming-pitfalls-in-graph-contrastive-learning-evaluation-toward-comprehensive-benchmarks-qian-ma-et-al-2024>(3/26 | 46/117) Overcoming Pitfalls in Graph Contrastive Learning Evaluation: Toward Comprehensive Benchmarks (Qian Ma et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qian Ma, Hongliang Chi, Hengrui Zhang, Kay Liu, Zhiwei Zhang, Lu Cheng, Suhang Wang, Philip S. Yu, Yao Ma. (2024)<br><strong>Overcoming Pitfalls in Graph Contrastive Learning Evaluation: Toward Comprehensive Benchmarks</strong><br><button class=copy-to-clipboard title="Overcoming Pitfalls in Graph Contrastive Learning Evaluation: Toward Comprehensive Benchmarks" index=46>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-46 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 56<br>Keywords: Graph, Graph Contrastive Learning, Graph Contrastive Learning, Benchmarking, Contrastive Learning, Self-supervised Learning, Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15680v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15680v1.pdf filename=2402.15680v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The rise of <b>self-supervised</b> <b>learning,</b> which operates without the need for labeled data, has garnered significant interest within the <b>graph</b> <b>learning</b> <b>community.</b> This enthusiasm has led to the development of numerous <b>Graph</b> <b>Contrastive</b> <b>Learning</b> <b>(GCL)</b> techniques, all aiming to create a versatile <b>graph</b> <b>encoder</b> <b>that</b> leverages the wealth of unlabeled data for various downstream tasks. However, the current evaluation standards for <b>GCL</b> approaches are flawed due to the need for extensive hyper-parameter tuning during pre-training and the reliance on a single downstream task for assessment. These flaws can skew the evaluation away from the intended goals, potentially leading to misleading conclusions. In our paper, we thoroughly examine these shortcomings and offer fresh perspectives on how <b>GCL</b> methods are affected by hyper-parameter choices and the choice of downstream tasks for their evaluation. Additionally, we introduce an enhanced evaluation framework designed to more accurately gauge the effectiveness, consistency, and overall capability of <b>GCL</b> methods.</p></p class="citation"></blockquote><h3 id=426--47117-prolora-partial-rotation-empowers-more-parameter-efficient-lora-sheng-wang-et-al-2024>(4/26 | 47/117) PRoLoRA: Partial Rotation Empowers More Parameter-Efficient LoRA (Sheng Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sheng Wang, Boyang Xue, Jiacheng Ye, Jiyue Jiang, Liheng Chen, Lingpeng Kong, Chuan Wu. (2024)<br><strong>PRoLoRA: Partial Rotation Empowers More Parameter-Efficient LoRA</strong><br><button class=copy-to-clipboard title="PRoLoRA: Partial Rotation Empowers More Parameter-Efficient LoRA" index=47>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-47 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 50<br>Keywords: Fine-tuning, Parameter Sharing, Instruction Tuning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16902v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16902v1.pdf filename=2402.16902v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the rapid scaling of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs),</b> serving numerous LoRAs concurrently has become increasingly impractical, leading to unaffordable costs and necessitating more <b>parameter-efficient</b> <b>finetuning</b> methods. In this work, we introduce Partially Rotation-enhanced Low-Rank Adaptation (PRoLoRA), an intra-layer sharing mechanism comprising four essential components: broadcast reduction, rotation enhancement, partially-sharing refinement, and rectified initialization strategy. As a superset of LoRA, PRoLoRA pertains its advantages, and effectively circumvent the drawbacks of peer <b>parameter-sharing</b> <b>methods</b> with superior model capacity, practical feasibility, and broad applicability. Empirical experiments demonstrate the remarkably higher <b>parameter</b> <b>efficiency</b> of PRoLoRA in both specific <b>parameter</b> <b>budget</b> and performance target scenarios, and its scalability to larger <b>LLMs.</b> Notably, with one time less trainable <b>parameters,</b> <b>PRoLoRA</b> still outperforms LoRA on multiple <b>instruction</b> <b>tuning</b> datasets. Subsequently, an ablation study is conducted to validate the necessity of individual components and highlight the superiority of PRoLoRA over three potential variants. Hopefully, the conspicuously higher <b>parameter</b> <b>efficiency</b> can establish PRoLoRA as a resource-friendly alternative to LoRA.</p></p class="citation"></blockquote><h3 id=526--48117-sparse-mezo-less-parameters-for-better-performance-in-zeroth-order-llm-fine-tuning-yong-liu-et-al-2024>(5/26 | 48/117) Sparse MeZO: Less Parameters for Better Performance in Zeroth-Order LLM Fine-Tuning (Yong Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yong Liu, Zirui Zhu, Chaoyu Gong, Minhao Cheng, Cho-Jui Hsieh, Yang You. (2024)<br><strong>Sparse MeZO: Less Parameters for Better Performance in Zeroth-Order LLM Fine-Tuning</strong><br><button class=copy-to-clipboard title="Sparse MeZO: Less Parameters for Better Performance in Zeroth-Order LLM Fine-Tuning" index=48>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-48 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CL, cs-LG, cs.LG<br>Keyword Score: 50<br>Keywords: Fine-tuning, Fine-tuning, LLaMA, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15751v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15751v1.pdf filename=2402.15751v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>While <b>fine-tuning</b> <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> for specific tasks often yields impressive results, it comes at the cost of memory inefficiency due to back-propagation in gradient-based training. Memory-efficient Zeroth-order (MeZO) optimizers, recently proposed to address this issue, only require forward passes during training, making them more memory-friendly. However, the quality of gradient estimates in zeroth order optimization often depends on the data dimensionality, potentially explaining why MeZO still exhibits significant performance drops compared to standard <b>fine-tuning</b> across various tasks. Inspired by the success of Parameter-Efficient <b>Fine-Tuning</b> (PEFT), this paper introduces Sparse MeZO, a novel memory-efficient zeroth-order optimization approach that applies ZO only to a carefully chosen subset of parameters. We propose a simple yet effective parameter selection scheme that yields significant performance gains with Sparse-MeZO. Additionally, we develop a memory-optimized implementation for sparse masking, ensuring the algorithm requires only inference-level memory consumption, allowing Sparse-MeZO to <b>fine-tune</b> <b>LLaMA-30b</b> on a single A100 GPU. Experimental results illustrate that Sparse-MeZO consistently improves both performance and convergence speed over MeZO without any overhead. For example, it achieves a 9% absolute accuracy improvement and 3.5x speedup over MeZO on the RTE task.</p></p class="citation"></blockquote><h3 id=626--49117-pretraining-strategy-for-neural-potentials-zehua-zhang-et-al-2024>(6/26 | 49/117) Pretraining Strategy for Neural Potentials (Zehua Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zehua Zhang, Zijie Li, Amir Barati Farimani. (2024)<br><strong>Pretraining Strategy for Neural Potentials</strong><br><button class=copy-to-clipboard title="Pretraining Strategy for Neural Potentials" index=49>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-49 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, physics-chem-ph<br>Keyword Score: 33<br>Keywords: Graph, Graph Neural Network, Graph Neural Network, Fine-tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15921v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15921v1.pdf filename=2402.15921v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We propose a mask pretraining method for <b>Graph</b> <b>Neural</b> <b>Networks</b> <b>(GNNs)</b> to improve their performance on fitting potential energy surfaces, particularly in water systems. <b>GNNs</b> are pretrained by recovering spatial information related to masked-out atoms from molecules, then transferred and <b>finetuned</b> on atomic forcefields. Through such pretraining, <b>GNNs</b> learn meaningful prior about structural and underlying physical information of molecule systems that are useful for downstream tasks. From comprehensive experiments and ablation studies, we show that the proposed method improves the accuracy and convergence speed compared to <b>GNNs</b> trained from scratch or using other pretraining techniques such as denoising. On the other hand, our pretraining method is suitable for both energy-centric and force-centric <b>GNNs.</b> This approach showcases its potential to enhance the performance and data efficiency of <b>GNNs</b> in fitting molecular force fields.</p></p class="citation"></blockquote><h3 id=726--50117-large-stepsize-gradient-descent-for-logistic-loss-non-monotonicity-of-the-loss-improves-optimization-efficiency-jingfeng-wu-et-al-2024>(7/26 | 50/117) Large Stepsize Gradient Descent for Logistic Loss: Non-Monotonicity of the Loss Improves Optimization Efficiency (Jingfeng Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jingfeng Wu, Peter L. Bartlett, Matus Telgarsky, Bin Yu. (2024)<br><strong>Large Stepsize Gradient Descent for Logistic Loss: Non-Monotonicity of the Loss Improves Optimization Efficiency</strong><br><button class=copy-to-clipboard title="Large Stepsize Gradient Descent for Logistic Loss: Non-Monotonicity of the Loss Improves Optimization Efficiency" index=50>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-50 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-ML<br>Keyword Score: 30<br>Keywords: Logistic Regression, Stochastic Gradient Descent, Stochastic Gradient Descent<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15926v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15926v1.pdf filename=2402.15926v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We consider gradient descent (GD) with a constant stepsize applied to <b>logistic</b> <b>regression</b> with linearly separable data, where the constant stepsize $\eta$ is so large that the loss initially oscillates. We show that GD exits this initial oscillatory phase rapidly &ndash; in $\mathcal{O}(\eta)$ steps &ndash; and subsequently achieves an $\tilde{\mathcal{O}}(1 / (\eta t) )$ convergence rate after $t$ additional steps. Our results imply that, given a budget of $T$ steps, GD can achieve an accelerated loss of $\tilde{\mathcal{O}}(1/T^2)$ with an aggressive stepsize $\eta:= \Theta( T)$, without any use of momentum or variable stepsize schedulers. Our proof technique is versatile and also handles general classification loss functions (where exponential tails are needed for the $\tilde{\mathcal{O}}(1/T^2)$ acceleration), nonlinear predictors in the neural tangent kernel regime, and online <b>stochastic</b> <b>gradient</b> <b>descent</b> <b>(SGD)</b> with a large stepsize, under suitable separability conditions.</p></p class="citation"></blockquote><h3 id=826--51117-esfl-efficient-split-federated-learning-over-resource-constrained-heterogeneous-wireless-devices-guangyu-zhu-et-al-2024>(8/26 | 51/117) ESFL: Efficient Split Federated Learning over Resource-Constrained Heterogeneous Wireless Devices (Guangyu Zhu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Guangyu Zhu, Yiqin Deng, Xianhao Chen, Haixia Zhang, Yuguang Fang, Tan F. Wong. (2024)<br><strong>ESFL: Efficient Split Federated Learning over Resource-Constrained Heterogeneous Wireless Devices</strong><br><button class=copy-to-clipboard title="ESFL: Efficient Split Federated Learning over Resource-Constrained Heterogeneous Wireless Devices" index=51>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-51 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs-NI, cs.LG<br>Keyword Score: 30<br>Keywords: Federated Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15903v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15903v1.pdf filename=2402.15903v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Federated</b> <b>learning</b> (FL) allows multiple parties (distributed devices) to train a machine learning model without sharing raw data. How to effectively and efficiently utilize the resources on devices and the central server is a highly interesting yet challenging problem. In this paper, we propose an efficient split <b>federated</b> <b>learning</b> algorithm (ESFL) to take full advantage of the powerful computing capabilities at a central server under a split <b>federated</b> <b>learning</b> framework with heterogeneous end devices (EDs). By splitting the model into different submodels between the server and EDs, our approach jointly optimizes user-side workload and server-side computing resource allocation by considering users&rsquo; heterogeneity. We formulate the whole optimization problem as a mixed-integer non-linear program, which is an NP-hard problem, and develop an iterative approach to obtain an approximate solution efficiently. Extensive <b>simulations</b> have been conducted to validate the significantly increased efficiency of our ESFL approach compared with standard <b>federated</b> <b>learning,</b> split learning, and splitfed learning.</p></p class="citation"></blockquote><h3 id=926--52117-a-generative-machine-learning-model-for-material-microstructure-3d-reconstruction-and-performance-evaluation-yilin-zheng-et-al-2024>(9/26 | 52/117) A Generative Machine Learning Model for Material Microstructure 3D Reconstruction and Performance Evaluation (Yilin Zheng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yilin Zheng, Zhigong Song. (2024)<br><strong>A Generative Machine Learning Model for Material Microstructure 3D Reconstruction and Performance Evaluation</strong><br><button class=copy-to-clipboard title="A Generative Machine Learning Model for Material Microstructure 3D Reconstruction and Performance Evaluation" index=52>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-52 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cond-mat-mtrl-sci, cs-CV, cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Convolution, Generative Adversarial Network, Generative Adversarial Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15815v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15815v1.pdf filename=2402.15815v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The reconstruction of 3D microstructures from 2D slices is considered to hold significant value in predicting the spatial structure and physical properties of materials.The dimensional extension from 2D to 3D is viewed as a highly challenging inverse problem from the current technological perspective.Recently,methods based on <b>generative</b> <b>adversarial</b> <b>networks</b> have garnered widespread attention.However,they are still hampered by numerous limitations,including oversimplified models,a requirement for a substantial number of training samples,and difficulties in achieving model convergence during training.In light of this,a novel <b>generative</b> <b>model</b> <b>that</b> integrates the multiscale properties of U-net with and the <b>generative</b> <b>capabilities</b> <b>of</b> <b>GAN</b> has been proposed.Based on this,the innovative construction of a multi-scale channel aggregation module,a multi-scale hierarchical feature aggregation module and a <b>convolutional</b> block attention mechanism can better capture the properties of the material microstructure and extract the image information.The model&rsquo;s accuracy is further improved by combining the image regularization loss with the Wasserstein distance loss.In addition,this study utilizes the anisotropy index to accurately distinguish the nature of the image,which can clearly determine the isotropy and anisotropy of the image.It is also the first time that the generation quality of material samples from different domains is evaluated and the performance of the model itself is compared.The experimental results demonstrate that the present model not only shows a very high similarity between the generated 3D structures and real samples but is also highly consistent with real data in terms of statistical data analysis.</p></p class="citation"></blockquote><h3 id=1026--53117-batch-active-learning-of-reward-functions-from-human-preferences-erdem-bıyık-et-al-2024>(10/26 | 53/117) Batch Active Learning of Reward Functions from Human Preferences (Erdem Bıyık et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Erdem Bıyık, Nima Anari, Dorsa Sadigh. (2024)<br><strong>Batch Active Learning of Reward Functions from Human Preferences</strong><br><button class=copy-to-clipboard title="Batch Active Learning of Reward Functions from Human Preferences" index=53>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-53 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs-RO, cs.LG, stat-ML<br>Keyword Score: 30<br>Keywords: Active Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15757v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15757v1.pdf filename=2402.15757v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Data generation and labeling are often expensive in robot learning. Preference-based learning is a concept that enables reliable labeling by querying users with preference questions. <b>Active</b> <b>querying</b> methods are commonly employed in preference-based learning to generate more informative data at the expense of parallelization and computation time. In this paper, we develop a set of novel algorithms, batch <b>active</b> <b>preference-based</b> learning methods, that enable efficient learning of reward functions using as few data samples as possible while still having short query generation times and also retaining parallelizability. We introduce a method based on determinantal point processes (DPP) for <b>active</b> <b>batch</b> generation and several heuristic-based alternatives. Finally, we present our experimental results for a variety of robotics tasks in <b>simulation.</b> Our results suggest that our batch <b>active</b> <b>learning</b> algorithm requires only a few queries that are computed in a short amount of time. We showcase one of our algorithms in a study to learn human users&rsquo; preferences.</p></p class="citation"></blockquote><h3 id=1126--54117-is-offline-decision-making-possible-with-only-few-samples-reliable-decisions-in-data-starved-bandits-via-trust-region-enhancement-ruiqi-zhang-et-al-2024>(11/26 | 54/117) Is Offline Decision Making Possible with Only Few Samples? Reliable Decisions in Data-Starved Bandits via Trust Region Enhancement (Ruiqi Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ruiqi Zhang, Yuexiang Zhai, Andrea Zanette. (2024)<br><strong>Is Offline Decision Making Possible with Only Few Samples? Reliable Decisions in Data-Starved Bandits via Trust Region Enhancement</strong><br><button class=copy-to-clipboard title="Is Offline Decision Making Possible with Only Few Samples? Reliable Decisions in Data-Starved Bandits via Trust Region Enhancement" index=54>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-54 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG, stat-ML<br>Keyword Score: 30<br>Keywords: Bandit Algorithm, Offline Reinforcement Learning, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15703v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15703v1.pdf filename=2402.15703v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>What can an agent learn in a stochastic Multi-Armed <b>Bandit</b> (MAB) problem from a dataset that contains just a single sample for each arm? Surprisingly, in this work, we demonstrate that even in such a data-starved setting it may still be possible to find a policy competitive with the optimal one. This paves the way to reliable decision-making in settings where critical decisions must be made by relying only on a handful of samples. Our analysis reveals that \emph{stochastic policies can be substantially better} than deterministic ones for <b>offline</b> <b>decision-making.</b> <b>Focusing</b> on <b>offline</b> <b>multi-armed</b> <b>bandits,</b> we design an algorithm called Trust Region of Uncertainty for Stochastic policy enhancemenT (TRUST) which is quite different from the predominant value-based lower confidence bound approach. Its design is enabled by localization laws, critical radii, and relative pessimism. We prove that its sample complexity is comparable to that of LCB on minimax problems while being substantially lower on problems with very few samples. Finally, we consider an application to <b>offline</b> <b>reinforcement</b> <b>learning</b> in the special case where the logging policies are known.</p></p class="citation"></blockquote><h3 id=1226--55117-clustering-in-dynamic-environments-a-framework-for-benchmark-dataset-generation-with-heterogeneous-changes-danial-yazdani-et-al-2024>(12/26 | 55/117) Clustering in Dynamic Environments: A Framework for Benchmark Dataset Generation With Heterogeneous Changes (Danial Yazdani et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Danial Yazdani, Juergen Branke, Mohammad Sadegh Khorshidi, Mohammad Nabi Omidvar, Xiaodong Li, Amir H. Gandomi, Xin Yao. (2024)<br><strong>Clustering in Dynamic Environments: A Framework for Benchmark Dataset Generation With Heterogeneous Changes</strong><br><button class=copy-to-clipboard title="Clustering in Dynamic Environments: A Framework for Benchmark Dataset Generation With Heterogeneous Changes" index=55>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-55 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs-NE, cs.LG<br>Keyword Score: 26<br>Keywords: Benchmarking, Clustering, Unsupervised Learning, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15731v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15731v1.pdf filename=2402.15731v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Clustering</b> in dynamic environments is of increasing importance, with broad applications ranging from real-time data analysis and online <b>unsupervised</b> <b>learning</b> to dynamic facility location problems. While meta-heuristics have shown promising effectiveness in static <b>clustering</b> tasks, their application for tracking optimal <b>clustering</b> solutions or robust <b>clustering</b> over time in dynamic environments remains largely underexplored. This is partly due to a lack of dynamic datasets with diverse, controllable, and realistic dynamic characteristics, hindering systematic performance evaluations of <b>clustering</b> algorithms in various dynamic scenarios. This deficiency leads to a gap in our understanding and capability to effectively design algorithms for <b>clustering</b> in dynamic environments. To bridge this gap, this paper introduces the Dynamic Dataset Generator (DDG). DDG features multiple dynamic Gaussian components integrated with a range of heterogeneous, local, and global changes. These changes vary in spatial and temporal severity, patterns, and domain of influence, providing a comprehensive tool for simulating a wide range of dynamic scenarios.</p></p class="citation"></blockquote><h3 id=1326--56117-reward-design-for-justifiable-sequential-decision-making-aleksa-sukovic-et-al-2024>(13/26 | 56/117) Reward Design for Justifiable Sequential Decision-Making (Aleksa Sukovic et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Aleksa Sukovic, Goran Radanovic. (2024)<br><strong>Reward Design for Justifiable Sequential Decision-Making</strong><br><button class=copy-to-clipboard title="Reward Design for Justifiable Sequential Decision-Making" index=56>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-56 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Reinforcement Learning, human-in-the-loop<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15826v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15826v1.pdf filename=2402.15826v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Equipping agents with the capacity to justify made decisions using supporting evidence represents a cornerstone of accountable decision-making. Furthermore, ensuring that justifications are in line with human expectations and societal norms is vital, especially in high-stakes situations such as healthcare. In this work, we propose the use of a debate-based reward model for <b>reinforcement</b> <b>learning</b> agents, where the outcome of a zero-sum debate game quantifies the justifiability of a decision in a particular state. This reward model is then used to train a justifiable policy, whose decisions can be more easily corroborated with supporting evidence. In the debate game, two argumentative agents take turns providing supporting evidence for two competing decisions. Given the proposed evidence, a proxy of a human judge evaluates which decision is better justified. We demonstrate the potential of our approach in learning policies for prescribing and justifying treatment decisions of septic patients. We show that augmenting the reward with the feedback signal generated by the debate-based reward model yields policies highly favored by the judge when compared to the policy obtained solely from the environment rewards, while hardly sacrificing any performance. Moreover, in terms of the overall performance and justifiability of trained policies, the debate-based feedback is comparable to the feedback obtained from an ideal judge proxy that evaluates decisions using the full information encoded in the state. This suggests that the debate game outputs key information contained in states that is most relevant for evaluating decisions, which in turn substantiates the practicality of combining our approach with <b>human-in-the-loop</b> evaluations. Lastly, we showcase that agents trained via multi-agent debate learn to propose evidence that is resilient to refutations and closely aligns with human preferences.</p></p class="citation"></blockquote><h3 id=1426--57117-optimal-zero-shot-detector-for-multi-armed-attacks-federica-granese-et-al-2024>(14/26 | 57/117) Optimal Zero-Shot Detector for Multi-Armed Attacks (Federica Granese et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Federica Granese, Marco Romanelli, Pablo Piantanida. (2024)<br><strong>Optimal Zero-Shot Detector for Multi-Armed Attacks</strong><br><button class=copy-to-clipboard title="Optimal Zero-Shot Detector for Multi-Armed Attacks" index=57>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-57 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CR, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Zero-shot, Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15808v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15808v2.pdf filename=2402.15808v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper explores a scenario in which a malicious actor employs a multi-armed attack strategy to manipulate data samples, offering them various avenues to introduce noise into the dataset. Our central objective is to protect the data by detecting any alterations to the input. We approach this defensive strategy with utmost caution, operating in an environment where the defender possesses significantly less information compared to the attacker. Specifically, the defender is unable to utilize any data samples for training a defense model or verifying the integrity of the channel. Instead, the defender relies exclusively on a set of pre-existing detectors readily available &ldquo;off the shelf&rdquo;. To tackle this challenge, we derive an innovative information-theoretic defense approach that optimally aggregates the decisions made by these detectors, eliminating the need for any training data. We further explore a practical use-case scenario for empirical evaluation, where the attacker possesses a pre-trained classifier and launches well-known <b>adversarial</b> <b>attacks</b> against it. Our experiments highlight the effectiveness of our proposed solution, even in scenarios that deviate from the optimal setup.</p></p class="citation"></blockquote><h3 id=1526--58117-truly-no-regret-learning-in-constrained-mdps-adrian-müller-et-al-2024>(15/26 | 58/117) Truly No-Regret Learning in Constrained MDPs (Adrian Müller et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Adrian Müller, Pragnya Alatur, Volkan Cevher, Giorgia Ramponi, Niao He. (2024)<br><strong>Truly No-Regret Learning in Constrained MDPs</strong><br><button class=copy-to-clipboard title="Truly No-Regret Learning in Constrained MDPs" index=58>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-58 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-ML<br>Keyword Score: 20<br>Keywords: Markov Decision Process, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15776v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15776v1.pdf filename=2402.15776v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Constrained Markov decision processes (CMDPs) are a common way to model safety constraints in <b>reinforcement</b> <b>learning.</b> State-of-the-art methods for efficiently solving CMDPs are based on primal-dual algorithms. For these algorithms, all currently known regret bounds allow for error cancellations &ndash; one can compensate for a constraint violation in one round with a strict constraint satisfaction in another. This makes the online learning process unsafe since it only guarantees safety for the final (mixture) policy but not during learning. As Efroni et al. (2020) pointed out, it is an open question whether primal-dual algorithms can provably achieve sublinear regret if we do not allow error cancellations. In this paper, we give the first affirmative answer. We first generalize a result on last-iterate convergence of regularized primal-dual schemes to CMDPs with multiple constraints. Building upon this insight, we propose a model-based primal-dual algorithm to learn in an unknown CMDP. We prove that our algorithm achieves sublinear regret without error cancellations.</p></p class="citation"></blockquote><h3 id=1626--59117-a-prior-estimates-for-deep-residual-network-in-continuous-time-reinforcement-learning-shuyu-yin-et-al-2024>(16/26 | 59/117) A prior Estimates for Deep Residual Network in Continuous-time Reinforcement Learning (Shuyu Yin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shuyu Yin, Qixuan Zhou, Fei Wen, Tao Luo. (2024)<br><strong>A prior Estimates for Deep Residual Network in Continuous-time Reinforcement Learning</strong><br><button class=copy-to-clipboard title="A prior Estimates for Deep Residual Network in Continuous-time Reinforcement Learning" index=59>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-59 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Continuous Time, Continuous Time, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16899v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16899v1.pdf filename=2402.16899v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep <b>reinforcement</b> <b>learning</b> excels in numerous large-scale practical applications. However, existing performance analyses ignores the unique characteristics of <b>continuous-time</b> <b>control</b> problems, is unable to directly estimate the generalization error of the Bellman optimal loss and require a boundedness assumption. Our work focuses on <b>continuous-time</b> <b>control</b> problems and proposes a method that is applicable to all such problems where the transition function satisfies semi-group and Lipschitz properties. Under this method, we can directly analyze the \emph{a priori} generalization error of the Bellman optimal loss. The core of this method lies in two transformations of the loss function. To complete the transformation, we propose a decomposition method for the maximum operator. Additionally, this analysis method does not require a boundedness assumption. Finally, we obtain an \emph{a priori} generalization error without the curse of dimensionality.</p></p class="citation"></blockquote><h3 id=1726--60117-a-statistical-analysis-of-wasserstein-autoencoders-for-intrinsically-low-dimensional-data-saptarshi-chakraborty-et-al-2024>(17/26 | 60/117) A Statistical Analysis of Wasserstein Autoencoders for Intrinsically Low-dimensional Data (Saptarshi Chakraborty et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Saptarshi Chakraborty, Peter L. Bartlett. (2024)<br><strong>A Statistical Analysis of Wasserstein Autoencoders for Intrinsically Low-dimensional Data</strong><br><button class=copy-to-clipboard title="A Statistical Analysis of Wasserstein Autoencoders for Intrinsically Low-dimensional Data" index=60>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-60 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, math-ST, stat-ML, stat-TH<br>Keyword Score: 20<br>Keywords: Autoencoder, Variational Autoencoder<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15710v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15710v1.pdf filename=2402.15710v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Variational</b> <b>Autoencoders</b> (VAEs) have gained significant popularity among researchers as a powerful tool for understanding unknown distributions based on limited samples. This popularity stems partly from their impressive performance and partly from their ability to provide meaningful feature representations in the latent space. Wasserstein <b>Autoencoders</b> (WAEs), a variant of VAEs, aim to not only improve model efficiency but also interpretability. However, there has been limited focus on analyzing their statistical guarantees. The matter is further complicated by the fact that the data distributions to which WAEs are applied - such as natural images - are often presumed to possess an underlying low-dimensional structure within a high-dimensional feature space, which current theory does not adequately account for, rendering known bounds inefficient. To bridge the gap between the theory and practice of WAEs, in this paper, we show that WAEs can learn the data distributions when the network architectures are properly chosen. We show that the convergence rates of the expected excess risk in the number of samples for WAEs are independent of the high feature dimension, instead relying only on the intrinsic dimension of the data distribution.</p></p class="citation"></blockquote><h3 id=1826--61117-scalable-volt-var-optimization-using-rllib-impala-framework-a-reinforcement-learning-approach-alaa-selim-et-al-2024>(18/26 | 61/117) Scalable Volt-VAR Optimization using RLlib-IMPALA Framework: A Reinforcement Learning Approach (Alaa Selim et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Alaa Selim, Yanzhu Ye, Junbo Zhao, Bo Yang. (2024)<br><strong>Scalable Volt-VAR Optimization using RLlib-IMPALA Framework: A Reinforcement Learning Approach</strong><br><button class=copy-to-clipboard title="Scalable Volt-VAR Optimization using RLlib-IMPALA Framework: A Reinforcement Learning Approach" index=61>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-61 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs-SY, cs.LG, eess-SY<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15932v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15932v1.pdf filename=2402.15932v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the rapidly evolving domain of electrical power systems, the Volt-VAR optimization (VVO) is increasingly critical, especially with the burgeoning integration of renewable energy sources. Traditional approaches to learning-based VVO in expansive and dynamically changing power systems are often hindered by computational complexities. To address this challenge, our research presents a novel framework that harnesses the potential of Deep <b>Reinforcement</b> <b>Learning</b> (DRL), specifically utilizing the Importance Weighted Actor-Learner Architecture (IMPALA) algorithm, executed on the RAY platform. This framework, built upon RLlib-an industry-standard in <b>Reinforcement</b> <b>Learning-ingeniously</b> capitalizes on the distributed computing capabilities and advanced hyperparameter tuning offered by RAY. This design significantly expedites the exploration and exploitation phases in the VVO solution space. Our empirical results demonstrate that our approach not only surpasses existing DRL methods in achieving superior reward outcomes but also manifests a remarkable tenfold reduction in computational requirements. The integration of our DRL agent with the RAY platform facilitates the creation of RLlib-IMPALA, a novel framework that efficiently uses RAY&rsquo;s resources to improve system adaptability and control. RLlib-IMPALA leverages RAY&rsquo;s toolkit to enhance analytical capabilities and significantly speeds up training to become more than 10 times faster than other state-of-the-art DRL methods.</p></p class="citation"></blockquote><h3 id=1926--62117-a-novel-data-generation-scheme-for-surrogate-modelling-with-deep-operator-networks-shivam-choubey-et-al-2024>(19/26 | 62/117) A novel data generation scheme for surrogate modelling with deep operator networks (Shivam Choubey et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shivam Choubey, Birupaksha Pal, Manish Agrawal. (2024)<br><strong>A novel data generation scheme for surrogate modelling with deep operator networks</strong><br><button class=copy-to-clipboard title="A novel data generation scheme for surrogate modelling with deep operator networks" index=62>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-62 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs-NA, cs.LG, math-NA<br>Keyword Score: 10<br>Keywords: Gaussian Process<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16903v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16903v1.pdf filename=2402.16903v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Operator-based neural network architectures such as DeepONets have emerged as a promising tool for the surrogate modeling of physical systems. In general, towards operator surrogate modeling, the training data is generated by solving the PDEs using techniques such as Finite Element Method (FEM). The computationally intensive nature of data generation is one of the biggest bottleneck in deploying these surrogate models for practical applications. In this study, we propose a novel methodology to alleviate the computational burden associated with training data generation for DeepONets. Unlike existing literature, the proposed framework for data generation does not use any partial differential equation integration strategy, thereby significantly reducing the computational cost associated with generating training dataset for DeepONet. In the proposed strategy, first, the output field is generated randomly, satisfying the boundary conditions using <b>Gaussian</b> <b>Process</b> Regression (GPR). From the output field, the input source field can be calculated easily using finite difference techniques. The proposed methodology can be extended to other operator learning methods, making the approach widely applicable. To validate the proposed approach, we employ the heat equations as the model problem and develop the surrogate model for numerous boundary value problems.</p></p class="citation"></blockquote><h3 id=2026--63117-low-rank-bandits-via-tight-two-to-infinity-singular-subspace-recovery-yassir-jedra-et-al-2024>(20/26 | 63/117) Low-Rank Bandits via Tight Two-to-Infinity Singular Subspace Recovery (Yassir Jedra et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yassir Jedra, William Réveillard, Stefan Stojanovic, Alexandre Proutiere. (2024)<br><strong>Low-Rank Bandits via Tight Two-to-Infinity Singular Subspace Recovery</strong><br><button class=copy-to-clipboard title="Low-Rank Bandits via Tight Two-to-Infinity Singular Subspace Recovery" index=63>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-63 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-ML<br>Keyword Score: 10<br>Keywords: Bandit Algorithm<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15739v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15739v1.pdf filename=2402.15739v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We study contextual <b>bandits</b> with low-rank structure where, in each round, if the (context, arm) pair $(i,j)\in [m]\times [n]$ is selected, the learner observes a noisy sample of the $(i,j)$-th entry of an unknown low-rank reward matrix. Successive contexts are generated randomly in an i.i.d. manner and are revealed to the learner. For such <b>bandits,</b> we present efficient algorithms for policy evaluation, best policy identification and regret minimization. For policy evaluation and best policy identification, we show that our algorithms are nearly minimax optimal. For instance, the number of samples required to return an $\varepsilon$-optimal policy with probability at least $1-\delta$ typically scales as ${m+n\over \varepsilon^2}\log(1/\delta)$. Our regret minimization algorithm enjoys minimax guarantees scaling as $r^{7/4}(m+n)^{3/4}\sqrt{T}$, which improves over existing algorithms. All the proposed algorithms consist of two phases: they first leverage spectral methods to estimate the left and right singular subspaces of the low-rank reward matrix. We show that these estimates enjoy tight error guarantees in the two-to-infinity norm. This in turn allows us to reformulate our problems as a misspecified linear <b>bandit</b> problem with dimension roughly $r(m+n)$ and misspecification controlled by the subspace recovery error, as well as to design the second phase of our algorithms efficiently.</p></p class="citation"></blockquote><h3 id=2126--64117-operator-learning-algorithms-and-analysis-nikola-b-kovachki-et-al-2024>(21/26 | 64/117) Operator Learning: Algorithms and Analysis (Nikola B. Kovachki et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nikola B. Kovachki, Samuel Lanthaler, Andrew M. Stuart. (2024)<br><strong>Operator Learning: Algorithms and Analysis</strong><br><button class=copy-to-clipboard title="Operator Learning: Algorithms and Analysis" index=64>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-64 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs-NA, cs.LG, math-NA<br>Keyword Score: 10<br>Keywords: Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15715v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15715v1.pdf filename=2402.15715v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Operator learning refers to the application of ideas from machine learning to approximate (typically nonlinear) operators mapping between Banach spaces of functions. Such operators often arise from physical models expressed in terms of partial differential equations (PDEs). In this context, such approximate operators hold great potential as efficient surrogate models to complement traditional numerical methods in many-query tasks. Being data-driven, they also enable model discovery when a mathematical description in terms of a PDE is not available. This review focuses primarily on neural operators, built on the success of deep neural networks in the approximation of functions defined on finite dimensional Euclidean spaces. Empirically, neural operators have shown success in a variety of applications, but our theoretical understanding remains incomplete. This review article <b>summarizes</b> recent progress and the current state of our theoretical understanding of neural operators, focusing on an approximation theoretic point of view.</p></p class="citation"></blockquote><h3 id=2226--65117-orthogonal-gradient-boosting-for-simpler-additive-rule-ensembles-fan-yang-et-al-2024>(22/26 | 65/117) Orthogonal Gradient Boosting for Simpler Additive Rule Ensembles (Fan Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fan Yang, Pierre Le Bodic, Michael Kamp, Mario Boley. (2024)<br><strong>Orthogonal Gradient Boosting for Simpler Additive Rule Ensembles</strong><br><button class=copy-to-clipboard title="Orthogonal Gradient Boosting for Simpler Additive Rule Ensembles" index=65>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-65 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-ML<br>Keyword Score: 10<br>Keywords: Probabilistic Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15691v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15691v1.pdf filename=2402.15691v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Gradient boosting of prediction rules is an efficient approach to learn potentially interpretable yet accurate <b>probabilistic</b> <b>models.</b> However, actual interpretability requires to limit the number and size of the generated rules, and existing boosting variants are not designed for this purpose. Though corrective boosting refits all rule weights in each iteration to minimise prediction risk, the included rule conditions tend to be sub-optimal, because commonly used objective functions fail to anticipate this refitting. Here, we address this issue by a new objective function that measures the angle between the risk gradient vector and the projection of the condition output vector onto the orthogonal complement of the already selected conditions. This approach correctly approximate the ideal update of adding the risk gradient itself to the model and favours the inclusion of more general and thus shorter rules. As we demonstrate using a wide range of prediction tasks, this significantly improves the comprehensibility/accuracy trade-off of the fitted ensemble. Additionally, we show how objective values for related rule conditions can be computed incrementally to avoid any substantial computational overhead of the new method.</p></p class="citation"></blockquote><h3 id=2326--66117-universal-model-in-online-customer-service-shu-ting-pi-et-al-2024>(23/26 | 66/117) Universal Model in Online Customer Service (Shu-Ting Pi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shu-Ting Pi, Cheng-Ping Hsieh, Qun Liu, Yuying Zhu. (2024)<br><strong>Universal Model in Online Customer Service</strong><br><button class=copy-to-clipboard title="Universal Model in Online Customer Service" index=66>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-66 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-IR, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Information Retrieval<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15666v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15666v1.pdf filename=2402.15666v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Building machine learning models can be a time-consuming process that often takes several months to implement in typical business scenarios. To ensure consistent model performance and account for variations in data distribution, regular retraining is necessary. This paper introduces a solution for improving online customer service in e-commerce by presenting a universal model for predict-ing labels based on customer questions, without requiring training. Our novel approach involves using machine learning techniques to tag customer questions in transcripts and create a repository of questions and corresponding labels. When a customer requests assistance, an <b>information</b> <b>retrieval</b> model searches the repository for similar questions, and statistical analysis is used to predict the corresponding label. By eliminating the need for individual model training and maintenance, our approach reduces both the model development cycle and costs. The repository only requires periodic updating to maintain accuracy.</p></p class="citation"></blockquote><h3 id=2426--67117-anchor-free-clustering-based-on-anchor-graph-factorization-shikun-mei-et-al-2024>(24/26 | 67/117) Anchor-free Clustering based on Anchor Graph Factorization (Shikun Mei et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shikun Mei, Fangfang Li, Quanxue Gao, Ming Yang. (2024)<br><strong>Anchor-free Clustering based on Anchor Graph Factorization</strong><br><button class=copy-to-clipboard title="Anchor-free Clustering based on Anchor Graph Factorization" index=67>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-67 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 6<br>Keywords: Graph, Clustering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15688v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15688v1.pdf filename=2402.15688v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Anchor-based methods are a pivotal approach in handling <b>clustering</b> of large-scale data. However, these methods typically entail two distinct stages: selecting anchor points and constructing an anchor <b>graph.</b> This bifurcation, along with the initialization of anchor points, significantly influences the overall performance of the algorithm. To mitigate these issues, we introduce a novel method termed Anchor-free <b>Clustering</b> based on Anchor <b>Graph</b> Factorization (AFCAGF). AFCAGF innovates in learning the anchor <b>graph,</b> requiring only the computation of pairwise distances between samples. This process, achievable through straightforward optimization, circumvents the necessity for explicit selection of anchor points. More concretely, our approach enhances the Fuzzy k-means <b>clustering</b> algorithm (FKM), introducing a new manifold learning technique that obviates the need for initializing cluster centers. Additionally, we evolve the concept of the membership matrix between cluster centers and samples in FKM into an anchor <b>graph</b> encompassing multiple anchor points and samples. Employing Non-negative Matrix Factorization (NMF) on this anchor <b>graph</b> allows for the direct derivation of cluster labels, thereby eliminating the requirement for further post-processing steps. To solve the method proposed, we implement an alternating optimization algorithm that ensures convergence. Empirical evaluations on various real-world datasets underscore the superior efficacy of our algorithm compared to traditional approaches.</p></p class="citation"></blockquote><h3 id=2526--68117-field-based-molecule-generation-alexandru-dumitrescu-et-al-2024>(25/26 | 68/117) Field-based Molecule Generation (Alexandru Dumitrescu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Alexandru Dumitrescu, Dani Korpela, Markus Heinonen, Yogesh Verma, Valerii Iakovlev, Vikas Garg, Harri Lähdesmäki. (2024)<br><strong>Field-based Molecule Generation</strong><br><button class=copy-to-clipboard title="Field-based Molecule Generation" index=68>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-68 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, physics-chem-ph, q-bio-BM<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15864v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15864v1.pdf filename=2402.15864v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This work introduces FMG, a field-based model for drug-like molecule generation. We show how the flexibility of this method provides crucial advantages over the prevalent, point-cloud based methods, and achieves competitive molecular stability generation. We tackle optical isomerism (enantiomers), a previously omitted molecular property that is crucial for drug safety and effectiveness, and thus account for all molecular <b>geometry</b> aspects. We demonstrate how previous methods are invariant to a group of transformations that includes enantiomer pairs, leading them invariant to the molecular R and S configurations, while our field-based generative model captures this property.</p></p class="citation"></blockquote><h3 id=2626--69117-scalable-density-based-clustering-with-random-projections-haochuan-xu-et-al-2024>(26/26 | 69/117) Scalable Density-based Clustering with Random Projections (Haochuan Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haochuan Xu, Ninh Pham. (2024)<br><strong>Scalable Density-based Clustering with Random Projections</strong><br><button class=copy-to-clipboard title="Scalable Density-based Clustering with Random Projections" index=69>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-69 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CV, cs-LG, cs.LG<br>Keyword Score: 3<br>Keywords: Clustering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15679v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15679v1.pdf filename=2402.15679v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present sDBSCAN, a scalable density-based <b>clustering</b> algorithm in high dimensions with cosine distance. Utilizing the neighborhood-preserving property of random projections, sDBSCAN can quickly identify core points and their neighborhoods, the primary hurdle of density-based <b>clustering.</b> Theoretically, sDBSCAN outputs a <b>clustering</b> structure similar to DBSCAN under mild conditions with high probability. To further facilitate sDBSCAN, we present sOPTICS, a scalable OPTICS for interactive exploration of the intrinsic <b>clustering</b> structure. We also extend sDBSCAN and sOPTICS to L2, L1, $\chi^2$, and Jensen-Shannon distances via random kernel features. Empirically, sDBSCAN is significantly faster and provides higher accuracy than many other <b>clustering</b> algorithms on real-world million-point data sets. On these data sets, sDBSCAN and sOPTICS run in a few minutes, while the scikit-learn&rsquo;s counterparts demand several hours or cannot run due to memory constraints.</p></p class="citation"></blockquote><h2 id=cscr-5>cs.CR (5)</h2><h3 id=15--70117-prp-propagating-universal-perturbations-to-attack-large-language-model-guard-rails-neal-mangaokar-et-al-2024>(1/5 | 70/117) PRP: Propagating Universal Perturbations to Attack Large Language Model Guard-Rails (Neal Mangaokar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Neal Mangaokar, Ashish Hooda, Jihye Choi, Shreyas Chandrashekaran, Kassem Fawaz, Somesh Jha, Atul Prakash. (2024)<br><strong>PRP: Propagating Universal Perturbations to Attack Large Language Model Guard-Rails</strong><br><button class=copy-to-clipboard title="PRP: Propagating Universal Perturbations to Attack Large Language Model Guard-Rails" index=70>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-70 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CL, cs-CR, cs.CR<br>Keyword Score: 60<br>Keywords: GPT, GPT-3, GPT-3.5, LLaMA, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15911v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15911v1.pdf filename=2402.15911v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> are typically aligned to be harmless to humans. Unfortunately, recent work has shown that such models are susceptible to automated jailbreak attacks that induce them to generate harmful content. More recent <b>LLMs</b> often incorporate an additional layer of defense, a Guard Model, which is a second <b>LLM</b> that is designed to check and moderate the output response of the primary <b>LLM.</b> Our key contribution is to show a novel attack strategy, PRP, that is successful against several open-source (e.g., <b>Llama</b> 2) and closed-source (e.g., <b>GPT</b> 3.5) implementations of Guard Models. PRP leverages a two step prefix-based attack that operates by (a) constructing a universal adversarial prefix for the Guard Model, and (b) propagating this prefix to the response. We find that this procedure is effective across multiple threat models, including ones in which the adversary has no access to the Guard Model at all. Our work suggests that further advances are required on defenses and Guard Models before they can be considered effective.</p></p class="citation"></blockquote><h3 id=25--71117-llms-can-defend-themselves-against-jailbreaking-in-a-practical-manner-a-vision-paper-daoyuan-wu-et-al-2024>(2/5 | 71/117) LLMs Can Defend Themselves Against Jailbreaking in a Practical Manner: A Vision Paper (Daoyuan Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Daoyuan Wu, Shuai Wang, Yang Liu, Ning Liu. (2024)<br><strong>LLMs Can Defend Themselves Against Jailbreaking in a Practical Manner: A Vision Paper</strong><br><button class=copy-to-clipboard title="LLMs Can Defend Themselves Against Jailbreaking in a Practical Manner: A Vision Paper" index=71>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-71 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-AI, cs-CR, cs.CR<br>Keyword Score: 60<br>Keywords: GPT, GPT-3, Large Language Model, Large Language Model, Prompt, Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15727v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15727v1.pdf filename=2402.15727v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Jailbreaking is an emerging <b>adversarial</b> <b>attack</b> that bypasses the safety alignment deployed in off-the-shelf <b>large</b> <b>language</b> <b>models</b> <b>(LLMs).</b> A considerable amount of research exists proposing more effective jailbreak attacks, including the recent Greedy Coordinate Gradient (GCG) attack, jailbreak template-based attacks such as using &ldquo;Do-Anything-Now&rdquo; (DAN), and multilingual jailbreak. In contrast, the defensive side has been relatively less explored. This paper proposes a lightweight yet practical defense called SELFDEFEND, which can defend against all existing jailbreak attacks with minimal delay for jailbreak <b>prompts</b> and negligible delay for normal user <b>prompts.</b> Our key insight is that regardless of the kind of jailbreak strategies employed, they eventually need to include a harmful <b>prompt</b> (e.g., &ldquo;how to make a bomb&rdquo;) in the <b>prompt</b> sent to <b>LLMs,</b> and we found that existing <b>LLMs</b> can effectively recognize such harmful <b>prompts</b> that violate their safety policies. Based on this insight, we design a shadow stack that concurrently checks whether a harmful <b>prompt</b> exists in the user <b>prompt</b> and triggers a checkpoint in the normal stack once a token of &ldquo;No&rdquo; or a harmful <b>prompt</b> is output. The latter could also generate an explainable <b>LLM</b> response to <b>adversarial</b> <b>prompts.</b> We demonstrate our idea of SELFDEFEND works in various jailbreak scenarios through manual analysis in <b>GPT-3.5/4.</b> We also list three future directions to further enhance SELFDEFEND.</p></p class="citation"></blockquote><h3 id=35--72117-gait-based-privacy-protection-for-smart-wearable-devices-yu-su-et-al-2024>(3/5 | 72/117) Gait-Based Privacy Protection for Smart Wearable Devices (Yu Su et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yu Su, Yongjiao Li, Zhu Cao. (2024)<br><strong>Gait-Based Privacy Protection for Smart Wearable Devices</strong><br><button class=copy-to-clipboard title="Gait-Based Privacy Protection for Smart Wearable Devices" index=72>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-72 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs.CR<br>Keyword Score: 20<br>Keywords: LSTM, LSTM<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15797v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15797v1.pdf filename=2402.15797v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Smart wearable devices (SWDs) collect and store sensitive daily information of many people. Its primary method of identification is still the password unlocking method. However, several studies have shown serious security flaws in that method, which makes the privacy and security concerns of SWDs particularly urgent. Gait identification is well suited for SWDs because its built-in sensors can provide data support for identification. However, existing gait identification methods have low accuracy and neglect to protect the privacy of gait features. In addition, the SWD can be used as an internet of things device for users to share data. But few studies have used gait feature-based encryption schemes to protect the privacy of message interactions between SWDs and other devices. In this paper, we propose a gait identification network, a bi-directional <b>long</b> <b>short-term</b> <b>memory</b> <b>network</b> with an attention mechanism (ABLSTM), to improve the identification accuracy and a stochastic orthogonal transformation (SOT) scheme to protect the extracted gait features from leakage. In the experiments, ABLSTM achieves an accuracy of 95.28%, reducing previous error rate by 19.3%. The SOT scheme is proved to be resistant to the chosen plaintext attack (CPA) and is 30% faster than previous methods. A biometric-based encryption scheme is proposed to enable secure message interactions using gait features as keys after the gait identification stage is passed, and offers better protection of the gait features compared to previous schemes.</p></p class="citation"></blockquote><h3 id=45--73117-privacy-preserving-state-estimation-in-the-presence-of-eavesdroppers-a-survey-xinhao-yan-et-al-2024>(4/5 | 73/117) Privacy-Preserving State Estimation in the Presence of Eavesdroppers: A Survey (Xinhao Yan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xinhao Yan, Guanzhong Zhou, Daniel E. Quevedo, Carlos Murguia, Bo Chen, Hailong Huang. (2024)<br><strong>Privacy-Preserving State Estimation in the Presence of Eavesdroppers: A Survey</strong><br><button class=copy-to-clipboard title="Privacy-Preserving State Estimation in the Presence of Eavesdroppers: A Survey" index=73>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-73 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs-SY, cs.CR, eess-SY<br>Keyword Score: 10<br>Keywords: Differential Privacy<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15738v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15738v1.pdf filename=2402.15738v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Networked systems are increasingly the target of cyberattacks that exploit vulnerabilities within digital communications, embedded hardware, and software. Arguably, the simplest class of attacks &ndash; and often the first type before launching destructive integrity attacks &ndash; are eavesdropping attacks, which aim to infer information by collecting system data and exploiting it for malicious purposes. A key technology of networked systems is state estimation, which leverages sensing and actuation data and first-principles models to enable trajectory planning, real-time monitoring, and control. However, state estimation can also be exploited by eavesdroppers to identify models and reconstruct states with the aim of, e.g., launching integrity (stealthy) attacks and inferring sensitive information. It is therefore crucial to protect disclosed system data to avoid an accurate state estimation by eavesdroppers. This survey presents a comprehensive review of existing literature on privacy-preserving state estimation methods, while also identifying potential limitations and research gaps. Our primary focus revolves around three types of methods: cryptography, data perturbation, and transmission scheduling, with particular emphasis on Kalman-like filters. Within these categories, we delve into the concepts of homomorphic encryption and <b>differential</b> <b>privacy,</b> which have been extensively investigated in recent years in the context of privacy-preserving state estimation. Finally, we shed light on several technical and fundamental challenges surrounding current methods and propose potential directions for future research.</p></p class="citation"></blockquote><h3 id=55--74117-cryptanalysis-and-improvement-of-multimodal-data-encryption-by-machine-learning-based-system-zakaria-tolba-2024>(5/5 | 74/117) Cryptanalysis and improvement of multimodal data encryption by machine-learning-based system (Zakaria Tolba, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zakaria Tolba. (2024)<br><strong>Cryptanalysis and improvement of multimodal data encryption by machine-learning-based system</strong><br><button class=copy-to-clipboard title="Cryptanalysis and improvement of multimodal data encryption by machine-learning-based system" index=74>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-74 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-AI, cs-CR, cs-IR, cs.CR<br>Keyword Score: 6<br>Keywords: Multi-modal, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15779v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15779v1.pdf filename=2402.15779v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the rising popularity of the internet and the widespread use of networks and information systems via the cloud and data centers, the privacy and security of individuals and organizations have become extremely crucial. In this perspective, encryption consolidates effective technologies that can effectively fulfill these requirements by protecting public information exchanges. To achieve these aims, the researchers used a wide assortment of encryption algorithms to accommodate the varied requirements of this field, as well as focusing on complex mathematical issues during their work to substantially complicate the encrypted communication mechanism. as much as possible to preserve personal information while significantly reducing the possibility of attacks. Depending on how complex and distinct the requirements established by these various applications are, the potential of trying to break them continues to occur, and systems for evaluating and verifying the cryptographic algorithms implemented continue to be necessary. The best approach to analyzing an encryption algorithm is to identify a practical and efficient technique to break it or to learn ways to detect and repair weak aspects in algorithms, which is known as cryptanalysis. Experts in cryptanalysis have discovered several methods for breaking the cipher, such as discovering a critical vulnerability in mathematical equations to derive the secret key or determining the plaintext from the ciphertext. There are various attacks against secure cryptographic algorithms in the literature, and the strategies and mathematical solutions widely employed empower cryptanalysts to demonstrate their findings, identify weaknesses, and diagnose maintenance failures in algorithms.</p></p class="citation"></blockquote><h2 id=csai-6>cs.AI (6)</h2><h3 id=16--75117-how-do-humans-write-code-large-models-do-it-the-same-way-too-long-li-2024>(1/6 | 75/117) How Do Humans Write Code? Large Models Do It the Same Way Too (Long Li, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Long Li. (2024)<br><strong>How Do Humans Write Code? Large Models Do It the Same Way Too</strong><br><button class=copy-to-clipboard title="How Do Humans Write Code? Large Models Do It the Same Way Too" index=75>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-75 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CL, cs-PL, cs.AI<br>Keyword Score: 60<br>Keywords: LLaMA, Code Generation, Natural Language Inference, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15729v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15729v1.pdf filename=2402.15729v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> often make errors when performing numerical calculations. In contrast to traditional chain-of-thought <b>reasoning,</b> the program-of-thoughts approach involves generating executable <b>code</b> <b>to</b> solve problems. By executing this <b>code,</b> <b>it</b> achieves more precise results. Using generated executable <b>code</b> <b>instead</b> of <b>natural</b> <b>language</b> <b>can</b> reduce computational errors. However, we observe that when <b>LLMs</b> solve mathematical problems using <b>code,</b> <b>they</b> tend to generate more incorrect <b>reasoning</b> than when using <b>natural</b> <b>language.</b> <b>To</b> address this issue, we propose Human-Think Language (HTL), a straightforward yet highly efficient approach inspired by human coding practices. The approach first generates problem-solving methods described in the <b>natural</b> <b>language</b> <b>by</b> the model, then converts them into <b>code,</b> <b>mirroring</b> the process where people think through the logic in <b>natural</b> <b>language</b> <b>before</b> writing it as <b>code.</b> <b>Additionally,</b> it utilizes the Proximal Policy Optimization (PPO) algorithm, enabling it to provide feedback to itself based on the correctness of mathematical answers, much like humans do. Finally, we introduce a focus-attention mechanism that masks the question segment, enhancing its reliance on <b>natural</b> <b>language</b> <b>inference</b> solutions during <b>code</b> <b>generation.</b> We conduct our experiments without introducing any additional information, and the results across five mathematical calculation datasets showcase the effectiveness of our approach. Notably, on the NumGLUE dataset, the <b>LlaMA-2-7B-based</b> model achieves a superior performance rate (75.1%) compared to the previous best performance with the <b>LlaMA-2-70B</b> model (74.4%).</p></p class="citation"></blockquote><h3 id=26--76117-quacer-c-quantitative-certification-of-knowledge-comprehension-in-llms-isha-chaudhary-et-al-2024>(2/6 | 76/117) QuaCer-C: Quantitative Certification of Knowledge Comprehension in LLMs (Isha Chaudhary et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Isha Chaudhary, Vedaant V. Jain, Gagandeep Singh. (2024)<br><strong>QuaCer-C: Quantitative Certification of Knowledge Comprehension in LLMs</strong><br><button class=copy-to-clipboard title="QuaCer-C: Quantitative Certification of Knowledge Comprehension in LLMs" index=76>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-76 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CL, cs-LG, cs.AI<br>Keyword Score: 53<br>Keywords: Benchmarking, LLaMA, Mistral, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15929v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15929v1.pdf filename=2402.15929v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have demonstrated impressive performance on several <b>benchmarks.</b> However, traditional studies do not provide formal guarantees on the performance of <b>LLMs.</b> In this work, we propose a novel certification framework for <b>LLM,</b> QuaCer-C, wherein we formally certify the knowledge-comprehension capabilities of popular <b>LLMs.</b> Our certificates are quantitative - they consist of high-confidence, tight bounds on the probability that the target <b>LLM</b> gives the correct answer on any relevant knowledge comprehension <b>prompt.</b> Our certificates for the <b>Llama,</b> Vicuna, and <b>Mistral</b> <b>LLMs</b> indicate that the knowledge comprehension capability improves with an increase in the number of parameters and that the <b>Mistral</b> model is less performant than the rest in this evaluation.</p></p class="citation"></blockquote><h3 id=36--77117-stepwise-self-consistent-mathematical-reasoning-with-large-language-models-zilong-zhao-et-al-2024>(3/6 | 77/117) Stepwise Self-Consistent Mathematical Reasoning with Large Language Models (Zilong Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zilong Zhao, Yao Rong, Dongyang Guo, Emek Gözlüklü, Emir Gülboy, Enkelejda Kasneci. (2024)<br><strong>Stepwise Self-Consistent Mathematical Reasoning with Large Language Models</strong><br><button class=copy-to-clipboard title="Stepwise Self-Consistent Mathematical Reasoning with Large Language Models" index=77>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-77 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CL, cs-LG, cs.AI<br>Keyword Score: 41<br>Keywords: Graph, Benchmarking, Knowledge Graph, Mathematical Reasoning, Reasoning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17786v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17786v1.pdf filename=2402.17786v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Using <b>Large</b> <b>Language</b> <b>Models</b> for complex <b>mathematical</b> <b>reasoning</b> is difficult, primarily due to the complexity of multi-step <b>reasoning.</b> The main challenges of this process include (1) selecting critical intermediate results to advance the procedure, and (2) limited exploration of potential solutions. To address these issues, we introduce a novel algorithm, namely Stepwise Self-Consistent Chain-of-Thought (SSC-CoT). SSC-CoT employs a strategy of selecting intermediate steps based on the intersection of various <b>reasoning</b> chains. Additionally, SSC-CoT enables the model to discover critical intermediate steps by querying a <b>knowledge</b> <b>graph</b> comprising relevant domain <b>knowledge.</b> <b>To</b> validate SSC-CoT, we present a new dataset, TriMaster100, tailored for complex trigonometry problems. This dataset contains 100 questions, with each solution broken down into scored intermediate steps, facilitating a comprehensive evaluation of the <b>mathematical</b> <b>reasoning</b> process. On TriMaster100, SSC-CoT triples the effectiveness of the state-of-the-art methods. Furthermore, we <b>benchmark</b> SSC-CoT on the widely recognized complex <b>mathematical</b> <b>question</b> dataset, MATH level 5, and it surpasses the second-best method by 7.2% in accuracy. Code and the TriMaster100 dataset can be found at: <a href=https://github.com/zhao-zilong/ssc-cot>https://github.com/zhao-zilong/ssc-cot</a>.</p></p class="citation"></blockquote><h3 id=46--78117-enforcing-temporal-constraints-on-generative-agent-behavior-with-reactive-synthesis-raven-rothkopf-et-al-2024>(4/6 | 78/117) Enforcing Temporal Constraints on Generative Agent Behavior with Reactive Synthesis (Raven Rothkopf et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Raven Rothkopf, Hannah Tongxin Zeng, Mark Santolucito. (2024)<br><strong>Enforcing Temporal Constraints on Generative Agent Behavior with Reactive Synthesis</strong><br><button class=copy-to-clipboard title="Enforcing Temporal Constraints on Generative Agent Behavior with Reactive Synthesis" index=78>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-78 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-LG, cs-LO, cs.AI<br>Keyword Score: 30<br>Keywords: Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16905v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16905v1.pdf filename=2402.16905v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The surge in popularity of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> has opened doors for new approaches to the creation of interactive agents. However, managing the temporal behavior of such agents over the course of an interaction remains challenging. The stateful, long-term horizon and quantitative <b>reasoning</b> required for coherent agent behavior does not fit well into the <b>LLM</b> paradigm. We propose a combination of formal logic-based program synthesis and <b>LLM</b> content generation to create generative agents that adhere to temporal constraints. Our approach uses Temporal Stream Logic (TSL) to generate an automaton that enforces a temporal structure on an agent and leaves the details of each action for a moment in time to an <b>LLM.</b> By using TSL, we are able to augment the generative agent where users have a higher level of guarantees on behavior, better interpretability of the system, and more ability to build agents in a modular way. We evaluate our approach on different tasks involved in creating a coherent interactive agent specialized for various application domains. We found that over all of the tasks, our approach using TSL achieves at least 96% adherence, whereas the pure <b>LLM-based</b> approach demonstrates as low as 14.67% adherence.</p></p class="citation"></blockquote><h3 id=56--79117-hal-eval-a-universal-and-fine-grained-hallucination-evaluation-framework-for-large-vision-language-models-chaoya-jiang-et-al-2024>(5/6 | 79/117) Hal-Eval: A Universal and Fine-grained Hallucination Evaluation Framework for Large Vision Language Models (Chaoya Jiang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chaoya Jiang, Wei Ye, Mengfan Dong, Hongrui Jia, Haiyang Xu, Ming Yan, Ji Zhang, Shikun Zhang. (2024)<br><strong>Hal-Eval: A Universal and Fine-grained Hallucination Evaluation Framework for Large Vision Language Models</strong><br><button class=copy-to-clipboard title="Hal-Eval: A Universal and Fine-grained Hallucination Evaluation Framework for Large Vision Language Models" index=79>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-79 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CL, cs.AI<br>Keyword Score: 23<br>Keywords: Benchmarking, Large Language Model, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15721v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15721v1.pdf filename=2402.15721v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Large Vision Language Models exhibit remarkable capabilities but struggle with hallucinations inconsistencies between images and their descriptions. Previous hallucination evaluation studies on LVLMs have identified hallucinations in terms of objects, attributes, and relations but overlooked complex hallucinations that create an entire narrative around a fictional entity. In this paper, we introduce a refined taxonomy of hallucinations, featuring a new category: Event Hallucination. We then utilize advanced <b>LLMs</b> to generate and filter fine grained hallucinatory data consisting of various types of hallucinations, with a particular focus on event hallucinations, laying the groundwork for integrating discriminative and generative evaluation methods within our universal evaluation framework. The proposed <b>benchmark</b> distinctively assesses LVLMs ability to tackle a broad spectrum of hallucinations, making it a reliable and comprehensive tool for gauging LVLMs efficacy in handling hallucinations. We will release our code and data.</p></p class="citation"></blockquote><h3 id=66--80117-empowering-large-language-model-agents-through-action-learning-haiteng-zhao-et-al-2024>(6/6 | 80/117) Empowering Large Language Model Agents through Action Learning (Haiteng Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haiteng Zhao, Chang Ma, Guoyin Wang, Jing Su, Lingpeng Kong, Jingjing Xu, Zhi-Hong Deng, Hongxia Yang. (2024)<br><strong>Empowering Large Language Model Agents through Action Learning</strong><br><button class=copy-to-clipboard title="Empowering Large Language Model Agents through Action Learning" index=80>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-80 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CL, cs.AI<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15809v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15809v1.pdf filename=2402.15809v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Model</b> <b>(LLM)</b> Agents have recently garnered increasing interest yet they are limited in their ability to learn from trial and error, a key element of intelligent behavior. In this work, we argue that the capacity to learn new actions from experience is fundamental to the advancement of learning in <b>LLM</b> agents. While humans naturally expand their action spaces and develop skills through experiential learning, <b>LLM</b> agents typically operate within fixed action spaces, limiting their potential for growth. To address these challenges, our study explores open-action learning for language agents. We introduce a framework LearnAct with an iterative learning strategy to create and improve actions in the form of Python functions. In each iteration, <b>LLM</b> revises and updates the currently available actions based on the errors identified in unsuccessful training tasks, thereby enhancing action effectiveness. Our experimental evaluations across Robotic Planning and Alfworld environments reveal that after learning on a few training task instances, our approach to open-action learning markedly improves agent performance for the type of task (by 32 percent in AlfWorld compared to ReAct+Reflexion, for instance) highlighting the importance of experiential action learning in the development of more intelligent <b>LLM</b> agents.</p></p class="citation"></blockquote><h2 id=csdc-1>cs.DC (1)</h2><h3 id=11--81117-minions-accelerating-large-language-model-inference-with-adaptive-and-collective-speculative-decoding-siqi-wang-et-al-2024>(1/1 | 81/117) Minions: Accelerating Large Language Model Inference with Adaptive and Collective Speculative Decoding (Siqi Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Siqi Wang, Hailong Yang, Xuezhu Wang, Tongxuan Liu, Pengbo Wang, Xuning Liang, Kejie Ma, Tianyu Feng, Xin You, Yongjun Bao, Yi Liu, Zhongzhi Luan, Depei Qian. (2024)<br><strong>Minions: Accelerating Large Language Model Inference with Adaptive and Collective Speculative Decoding</strong><br><button class=copy-to-clipboard title="Minions: Accelerating Large Language Model Inference with Adaptive and Collective Speculative Decoding" index=81>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-81 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DC<br>Categories: cs-DC, cs.DC<br>Keyword Score: 50<br>Keywords: Fine-tuning, Pruning, Quantization, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15678v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15678v1.pdf filename=2402.15678v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLM)</b> have recently attracted surging interest due to their outstanding capabilities across various domains. However, enabling efficient <b>LLM</b> inference is challenging due to its autoregressive decoding that generates tokens only one at a time. Although research works apply <b>pruning</b> or <b>quantization</b> to speed up <b>LLM</b> inference, they typically require <b>fine-tuning</b> the <b>LLM,</b> incurring significant time and economic costs. Meanwhile, speculative decoding has been proposed to use small speculative models (SSMs) to accelerate the inference of <b>LLM.</b> However, the low acceptance rate of SSM and the high verification cost of <b>LLM</b> prohibit further performance improvement of inference. In this paper, we propose Minions, an <b>LLM</b> inference system that accelerates <b>LLM</b> inference with a collective and adaptive speculative generation. Specifically, Minions proposes a majority-voted mechanism to leverage multiple SSMs to jointly speculate the outputs of <b>LLM,</b> which improves the inference performance without introducing prohibitive computation costs for <b>LLM.</b> To better trade off the number of tokens speculated from SSM and the verification cost of <b>LLM,</b> Minions proposes an adaptive mechanism to dynamically determine the optimal speculation length of SSM, which can achieve better inference performance across different models, datasets, and hyper-parameters. In addition, Minions decouples the SSM decoding and <b>LLM</b> verification efficiently and adopts a pipelined execution mechanism to further improve the inference performance of <b>LLM.</b> By comparing with the state-of-the-art <b>LLM</b> inference systems, we demonstrate that Minions can achieve higher inference throughput and lower inference time.</p></p class="citation"></blockquote><h2 id=quant-ph-1>quant-ph (1)</h2><h3 id=11--82117-lower-bounds-for-quantum-inspired-classical-algorithms-via-communication-complexity-nikhil-s-mande-et-al-2024>(1/1 | 82/117) Lower bounds for quantum-inspired classical algorithms via communication complexity (Nikhil S. Mande et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nikhil S. Mande, Changpeng Shao. (2024)<br><strong>Lower bounds for quantum-inspired classical algorithms via communication complexity</strong><br><button class=copy-to-clipboard title="Lower bounds for quantum-inspired classical algorithms via communication complexity" index=82>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-82 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: quant-ph<br>Categories: cs-CC, quant-ph, quant-ph<br>Keyword Score: 43<br>Keywords: Clustering, Recommendation, Simulation, Simulator, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15686v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15686v1.pdf filename=2402.15686v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Quantum-inspired classical algorithms provide us with a new way to understand the computational power of quantum computers for practically-relevant problems, especially in machine learning. In the past several years, numerous efficient algorithms for various tasks have been found, while an analysis of lower bounds is still missing. Using communication complexity, in this work we propose the first method to study lower bounds for these tasks. We mainly focus on lower bounds for solving linear regressions, <b>supervised</b> <b>clustering,</b> principal component analysis, <b>recommendation</b> systems, and Hamiltonian <b>simulations.</b> More precisely, we show that for linear regressions, in the row-sparse case, the lower bound is quadratic in the Frobenius norm of the underlying matrix, which is tight. In the dense case, with an extra assumption on the accuracy we obtain that the lower bound is quartic in the Frobenius norm, which matches the upper bound. For <b>supervised</b> <b>clustering,</b> we obtain a tight lower bound that is quartic in the Frobenius norm. For the other three tasks, we obtain a lower bound that is quadratic in the Frobenius norm, and the known upper bound is quartic in the Frobenius norm. Through this research, we find that large quantum speedup can exist for sparse, high-rank, well-conditioned matrix-related problems. Finally, we extend our method to study lower bounds analysis of quantum query algorithms for matrix-related problems. Some applications are given.</p></p class="citation"></blockquote><h2 id=cssd-1>cs.SD (1)</h2><h3 id=11--83117-bytecomposer-a-human-like-melody-composition-method-based-on-language-model-agent-xia-liang-et-al-2024>(1/1 | 83/117) ByteComposer: a Human-like Melody Composition Method based on Language Model Agent (Xia Liang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xia Liang, Jiaju Lin, Xinjian Du. (2024)<br><strong>ByteComposer: a Human-like Melody Composition Method based on Language Model Agent</strong><br><button class=copy-to-clipboard title="ByteComposer: a Human-like Melody Composition Method based on Language Model Agent" index=83>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-83 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SD<br>Categories: cs-AI, cs-SD, cs.SD, eess-AS<br>Keyword Score: 36<br>Keywords: Multi-modal, Multi-modal, GPT-4, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17785v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17785v1.pdf filename=2402.17785v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLM)</b> have shown encouraging progress in <b>multimodal</b> understanding and generation tasks. However, how to design a human-aligned and interpretable melody composition system is still under-explored. To solve this problem, we propose ByteComposer, an agent framework emulating a human&rsquo;s creative pipeline in four separate steps : &ldquo;Conception Analysis - Draft Composition - Self-Evaluation and Modification - Aesthetic Selection&rdquo;. This framework seamlessly blends the interactive and knowledge-understanding features of <b>LLMs</b> with existing symbolic music generation models, thereby achieving a melody composition agent comparable to human creators. We conduct extensive experiments on <b>GPT4</b> and several open-source <b>large</b> <b>language</b> <b>models,</b> which substantiate our framework&rsquo;s effectiveness. Furthermore, professional music composers were engaged in multi-dimensional evaluations, the final results demonstrated that across various facets of music composition, ByteComposer agent attains the level of a novice melody composer.</p></p class="citation"></blockquote><h2 id=csro-2>cs.RO (2)</h2><h3 id=12--84117-discretionary-lane-change-decision-and-control-via-parameterized-soft-actor-critic-for-hybrid-action-space-yuan-lin-et-al-2024>(1/2 | 84/117) Discretionary Lane-Change Decision and Control via Parameterized Soft Actor-Critic for Hybrid Action Space (Yuan Lin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuan Lin, Xiao Liu, Zishun Zheng, Liyao Wang. (2024)<br><strong>Discretionary Lane-Change Decision and Control via Parameterized Soft Actor-Critic for Hybrid Action Space</strong><br><button class=copy-to-clipboard title="Discretionary Lane-Change Decision and Control via Parameterized Soft Actor-Critic for Hybrid Action Space" index=84>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-84 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs-SY, cs.RO, eess-SY<br>Keyword Score: 30<br>Keywords: Reinforcement Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15790v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15790v1.pdf filename=2402.15790v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This study focuses on a crucial task in the field of autonomous driving, autonomous lane change. Autonomous lane change plays a pivotal role in improving traffic flow, alleviating driver burden, and reducing the risk of traffic accidents. However, due to the complexity and uncertainty of lane-change scenarios, the functionality of autonomous lane change still faces challenges. In this research, we conduct autonomous lane-change <b>simulations</b> using both Deep <b>Reinforcement</b> <b>Learning</b> (DRL) and Model Predictive Control (MPC). Specifically, we propose the Parameterized Soft Actor-Critic (PASAC) algorithm to train a DRL-based lane-change strategy to output both discrete lane-change decision and continuous longitudinal vehicle acceleration. We also use MPC for lane selection based on predictive car-following costs for different lanes. For the first time, we compare the performance of DRL and MPC in the context of lane-change decision. <b>Simulation</b> results indicate that, under the same reward/cost functions and traffic flow, both MPC and PASAC achieve a collision rate of 0%. PASAC demonstrates comparable performance to MPC in terms of episodic rewards/costs and average vehicle speeds.</p></p class="citation"></blockquote><h3 id=22--85117-phyplan-compositional-and-adaptive-physical-task-reasoning-with-physics-informed-skill-networks-for-robot-manipulators-harshil-vagadia-et-al-2024>(2/2 | 85/117) PhyPlan: Compositional and Adaptive Physical Task Reasoning with Physics-Informed Skill Networks for Robot Manipulators (Harshil Vagadia et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Harshil Vagadia, Mudit Chopra, Abhinav Barnawal, Tamajit Banerjee, Shreshth Tuli, Souvik Chakraborty, Rohan Paul. (2024)<br><strong>PhyPlan: Compositional and Adaptive Physical Task Reasoning with Physics-Informed Skill Networks for Robot Manipulators</strong><br><button class=copy-to-clipboard title="PhyPlan: Compositional and Adaptive Physical Task Reasoning with Physics-Informed Skill Networks for Robot Manipulators" index=85>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-85 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-RO, cs.RO<br>Keyword Score: 10<br>Keywords: Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15767v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15767v1.pdf filename=2402.15767v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Given the task of positioning a ball-like object to a goal region beyond direct reach, humans can often throw, slide, or rebound objects against the wall to attain the goal. However, enabling robots to reason similarly is non-trivial. Existing methods for physical <b>reasoning</b> are data-hungry and struggle with complexity and uncertainty inherent in the real world. This paper presents PhyPlan, a novel physics-informed planning framework that combines physics-informed neural networks (PINNs) with modified Monte Carlo Tree Search (MCTS) to enable embodied agents to perform dynamic physical tasks. PhyPlan leverages PINNs to simulate and predict outcomes of actions in a fast and accurate manner and uses MCTS for planning. It dynamically determines whether to consult a PINN-based simulator (coarse but fast) or engage directly with the actual environment (fine but slow) to determine optimal policy. Evaluation with robots in simulated 3D environments demonstrates the ability of our approach to solve 3D-physical <b>reasoning</b> tasks involving the composition of dynamic skills. Quantitatively, PhyPlan excels in several aspects: (i) it achieves lower regret when learning novel tasks compared to state-of-the-art, (ii) it expedites skill learning and enhances the speed of physical <b>reasoning,</b> (iii) it demonstrates higher data efficiency compared to a physics un-informed approach.</p></p class="citation"></blockquote><h2 id=eessiv-2>eess.IV (2)</h2><h3 id=12--86117-a-heterogeneous-dynamic-convolutional-neural-network-for-image-super-resolution-chunwei-tian-et-al-2024>(1/2 | 86/117) A Heterogeneous Dynamic Convolutional Neural Network for Image Super-resolution (Chunwei Tian et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chunwei Tian, Xuanyu Zhang, Jia Ren, Wangmeng Zuo, Yanning Zhang, Chia-Wen Lin. (2024)<br><strong>A Heterogeneous Dynamic Convolutional Neural Network for Image Super-resolution</strong><br><button class=copy-to-clipboard title="A Heterogeneous Dynamic Convolutional Neural Network for Image Super-resolution" index=86>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-86 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 30<br>Keywords: Convolution, Convolutional Neural Network, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15704v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15704v1.pdf filename=2402.15704v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Convolutional</b> <b>neural</b> <b>networks</b> can automatically learn features via deep network architectures and given input samples. However, robustness of obtained models may have challenges in varying scenes. Bigger differences of a network architecture are beneficial to extract more complementary structural information to enhance robustness of an obtained super-resolution model. In this paper, we present a heterogeneous dynamic <b>convolutional</b> <b>network</b> <b>in</b> image super-resolution (HDSRNet). To capture more information, HDSRNet is implemented by a heterogeneous parallel network. The upper network can facilitate more contexture information via stacked heterogeneous blocks to improve effects of image super-resolution. Each heterogeneous block is composed of a combination of a dilated, dynamic, common <b>convolutional</b> <b>layers,</b> <b>ReLU</b> and residual learning operation. It can not only adaptively adjust parameters, according to different inputs, but also prevent long-term dependency problem. The lower network utilizes a symmetric architecture to enhance relations of different layers to mine more structural information, which is complementary with a upper network for image super-resolution. The relevant experimental results show that the proposed HDSRNet is effective to deal with image resolving. The code of HDSRNet can be obtained at <a href=https://github.com/hellloxiaotian/HDSRNet>https://github.com/hellloxiaotian/HDSRNet</a>.</p></p class="citation"></blockquote><h3 id=22--87117-sandwich-gan-image-reconstruction-from-phase-mask-based-anti-dazzle-imaging-xiaopeng-peng-et-al-2024>(2/2 | 87/117) Sandwich GAN: Image Reconstruction from Phase Mask based Anti-dazzle Imaging (Xiaopeng Peng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiaopeng Peng, Erin F. Fleet, Abbie T. Watnik, Grover A. Swartzlander. (2024)<br><strong>Sandwich GAN: Image Reconstruction from Phase Mask based Anti-dazzle Imaging</strong><br><button class=copy-to-clipboard title="Sandwich GAN: Image Reconstruction from Phase Mask based Anti-dazzle Imaging" index=87>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-87 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV, physics-optics<br>Keyword Score: 20<br>Keywords: Generative Adversarial Network, Generative Adversarial Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15919v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15919v1.pdf filename=2402.15919v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Conventional camera systems are susceptible to the adverse effects of laser dazzle, which may over-saturate an image or cause permanent damage to pixels. To address this problem, we developed an approach combining point spread function engineering whereby a wavefront-coded mask in the pupil plane blurs both the laser and scene, together with a deep neural sandwich network. In addition to protecting the sensor, our approach jointly removes the laser from the scene and reconstructs a satisfactory deblurred image. Image recovery is achieved by wrapping two <b>generative</b> <b>adversarial</b> <b>networks</b> <b>(GANs)</b> around a learnable non-blind image deconvolution module. We trained the Sandwich <b>GAN</b> (SGAN) to suppress the peak laser irradiance as high as $10^6$ times the sensor saturation threshold - the point at which the bare system without the phase mask may exhibit damage. The end-to-end training includes physics-based modeling of the imaging system whereby a laser having an arbitrary angle of incidence is superimposed on images from a large publicly available library. The trained system was validated in the laboratory for laser strengths up to $10^4$ times the saturation value. The proposed image restoration model quantitatively and qualitatively outperforms other methods for a wide range of scene contents, illumination conditions, laser strengths, and noise characteristics.</p></p class="citation"></blockquote><h2 id=csdl-1>cs.DL (1)</h2><h3 id=11--88117-oag-bench-a-human-curated-benchmark-for-academic-graph-mining-fanjin-zhang-et-al-2024>(1/1 | 88/117) OAG-Bench: A Human-Curated Benchmark for Academic Graph Mining (Fanjin Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fanjin Zhang, Shijie Shi, Yifan Zhu, Bo Chen, Yukuo Cen, Jifan Yu, Yelin Chen, Lulu Wang, Qingfei Zhao, Yuqing Cheng, Tianyi Han, Yuwei An, Dan Zhang, Weng Lam Tam, Kun Cao, Yunhe Pang, Xinyu Guan, Huihui Yuan, Jian Song, Xiaoyan Li, Yuxiao Dong, Jie Tang. (2024)<br><strong>OAG-Bench: A Human-Curated Benchmark for Academic Graph Mining</strong><br><button class=copy-to-clipboard title="OAG-Bench: A Human-Curated Benchmark for Academic Graph Mining" index=88>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-88 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DL<br>Categories: cs-CL, cs-DL, cs-LG, cs.DL<br>Keyword Score: 26<br>Keywords: Graph, Benchmarking, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15810v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15810v1.pdf filename=2402.15810v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the rapid proliferation of scientific literature, versatile academic knowledge services increasingly rely on comprehensive academic <b>graph</b> mining. Despite the availability of public academic <b>graphs,</b> <b>benchmarks,</b> and datasets, these resources often fall short in multi-aspect and fine-grained annotations, are constrained to specific task types and domains, or lack underlying real academic <b>graphs.</b> In this paper, we present OAG-Bench, a comprehensive, multi-aspect, and fine-grained human-curated <b>benchmark</b> based on the Open Academic <b>Graph</b> (OAG). OAG-Bench covers 10 tasks, 20 datasets, 70+ baselines, and 120+ experimental results to date. We propose new data annotation strategies for certain tasks and offer a suite of data pre-processing codes, algorithm implementations, and standardized evaluation protocols to facilitate academic <b>graph</b> mining. Extensive experiments reveal that even advanced algorithms like <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> encounter difficulties in addressing key challenges in certain tasks, such as paper source tracing and scholar profiling. We also introduce the Open Academic <b>Graph</b> Challenge (OAG-Challenge) to encourage community input and sharing. We envisage that OAG-Bench can serve as a common ground for the community to evaluate and compare algorithms in academic <b>graph</b> mining, thereby accelerating algorithm development and advancement in this field. OAG-Bench is accessible at <a href=https://www.aminer.cn/data/>https://www.aminer.cn/data/</a>.</p></p class="citation"></blockquote><h2 id=csds-2>cs.DS (2)</h2><h3 id=12--89117-pruned-pivot-correlation-clustering-algorithm-for-dynamic-parallel-and-local-computation-models-mina-dalirrooyfard-et-al-2024>(1/2 | 89/117) Pruned Pivot: Correlation Clustering Algorithm for Dynamic, Parallel, and Local Computation Models (Mina Dalirrooyfard et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mina Dalirrooyfard, Konstantin Makarychev, Slobodan Mitrović. (2024)<br><strong>Pruned Pivot: Correlation Clustering Algorithm for Dynamic, Parallel, and Local Computation Models</strong><br><button class=copy-to-clipboard title="Pruned Pivot: Correlation Clustering Algorithm for Dynamic, Parallel, and Local Computation Models" index=89>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-89 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DS<br>Categories: cs-DS, cs.DS<br>Keyword Score: 26<br>Keywords: Graph, Clustering, Unsupervised Learning, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15668v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15668v1.pdf filename=2402.15668v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Given a <b>graph</b> with positive and negative edge labels, the correlation <b>clustering</b> problem aims to cluster the nodes so to minimize the total number of between-cluster positive and within-cluster negative edges. This problem has many applications in data mining, particularly in <b>unsupervised</b> <b>learning.</b> Inspired by the prevalence of large <b>graphs</b> and constantly changing data in modern applications, we study correlation <b>clustering</b> in dynamic, parallel (MPC), and local computation (LCA) settings. We design an approach that improves state-of-the-art runtime complexities in all these settings. In particular, we provide the first fully dynamic algorithm that runs in an expected amortized constant time, without any dependence on the <b>graph</b> size. Moreover, our algorithm essentially matches the approximation guarantee of the celebrated Pivot algorithm.</p></p class="citation"></blockquote><h3 id=22--90117-tree-decompositions-meet-induced-matchings-beyond-max-weight-independent-set-paloma-t-lima-et-al-2024>(2/2 | 90/117) Tree decompositions meet induced matchings: beyond Max Weight Independent Set (Paloma T. Lima et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Paloma T. Lima, Martin Milanič, Peter Muršič, Karolina Okrasa, Paweł Rzążewski, Kenny Štorgel. (2024)<br><strong>Tree decompositions meet induced matchings: beyond Max Weight Independent Set</strong><br><button class=copy-to-clipboard title="Tree decompositions meet induced matchings: beyond Max Weight Independent Set" index=90>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-90 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DS<br>Categories: 05C85 (Primary), 68Q25, 68R10, 05C05, 05C12, 05C38, 05C15, 05C40,
05C70, 05C75, 05C76, 05C83 (Secondary), cs-DM, cs-DS, cs.DS, math-CO<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15834v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15834v1.pdf filename=2402.15834v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>For a tree decomposition $\mathcal{T}$ of a <b>graph</b> $G$, by $\mu(\mathcal{T})$ we denote the size of a largest induced matching in $G$ all of whose edges intersect one bag of $\mathcal{T}$. Induced matching treewidth of a <b>graph</b> $G$ is the minimum value of $\mu(\mathcal{T})$ over all tree decompositions $\mathcal{T}$ of $G$. Yolov [SODA 2018] proved that Max Weight Independent Set can be solved in polynomial time for <b>graphs</b> of bounded induced matching treewidth. In this paper we explore what other problems are tractable in such classes of <b>graphs.</b> As our main result, we give a polynomial-time algorithm for Min Weight Feedback Vertex Set. We also provide some positive results concerning packing induced subgraphs, which in particular imply a PTAS for the problem of finding a largest induced subgraph of bounded treewidth. These results suggest that in <b>graphs</b> of bounded induced matching treewidth, one could find in polynomial time a maximum-weight induced subgraph of bounded treewidth satisfying a given CMSO$_2$ formula. We conjecture that such a result indeed holds and prove it for <b>graphs</b> of bounded tree-independence number, which form a rich and important family of subclasses of <b>graphs</b> of bounded induced matching treewidth. We complement these algorithmic results with a number of complexity and structural results concerning induced matching treewidth.</p></p class="citation"></blockquote><h2 id=cslo-1>cs.LO (1)</h2><h3 id=11--91117-regular-resolution-effectively-simulates-resolution-sam-buss-et-al-2024>(1/1 | 91/117) Regular resolution effectively simulates resolution (Sam Buss et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sam Buss, Emre Yolcu. (2024)<br><strong>Regular resolution effectively simulates resolution</strong><br><button class=copy-to-clipboard title="Regular resolution effectively simulates resolution" index=91>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-91 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LO<br>Categories: cs-CC, cs-LO, cs.LO<br>Keyword Score: 25<br>Keywords: Black Box, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15871v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15871v1.pdf filename=2402.15871v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Regular resolution is a refinement of the resolution proof system requiring that no variable be resolved on more than once along any path in the proof. It is known that there exist sequences of formulas that require exponential-size proofs in regular resolution while admitting polynomial-size proofs in resolution. Thus, with respect to the usual notion of <b>simulation,</b> regular resolution is separated from resolution. An alternative, and weaker, notion for comparing proof systems is that of an &ldquo;effective <b>simulation,"</b> which allows the translation of the formula along with the proof when moving between proof systems. We prove that regular resolution is equivalent to resolution under effective <b>simulations.</b> As a corollary, we recover in a <b>black-box</b> <b>fashion</b> a recent result on the hardness of automating regular resolution.</p></p class="citation"></blockquote><h2 id=csir-2>cs.IR (2)</h2><h3 id=12--92117-listt5-listwise-reranking-with-fusion-in-decoder-improves-zero-shot-retrieval-soyoung-yoon-et-al-2024>(1/2 | 92/117) ListT5: Listwise Reranking with Fusion-in-Decoder Improves Zero-shot Retrieval (Soyoung Yoon et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Soyoung Yoon, Eunbi Choi, Jiyeon Kim, Yireun Kim, Hyeongu Yun, Seung-won Hwang. (2024)<br><strong>ListT5: Listwise Reranking with Fusion-in-Decoder Improves Zero-shot Retrieval</strong><br><button class=copy-to-clipboard title="ListT5: Listwise Reranking with Fusion-in-Decoder Improves Zero-shot Retrieval" index=92>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-92 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 23<br>Keywords: Benchmarking, Rerank, Zero-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15838v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15838v2.pdf filename=2402.15838v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We propose ListT5, a novel <b>reranking</b> approach based on Fusion-in-Decoder (FiD) that handles multiple candidate passages at both train and inference time. We also introduce an efficient inference framework for listwise ranking based on m-ary tournament sort with output caching. We evaluate and compare our model on the BEIR <b>benchmark</b> for <b>zero-shot</b> retrieval task, demonstrating that ListT5 (1) outperforms the state-of-the-art RankT5 baseline with a notable +1.3 gain in the average NDCG@10 score, (2) has an efficiency comparable to pointwise ranking models and surpasses the efficiency of previous listwise ranking models, and (3) overcomes the lost-in-the-middle problem of previous listwise rerankers. Our code, model checkpoints, and the evaluation framework are fully open-sourced at \url{https://github.com/soyoung97/ListT5}.</p></p class="citation"></blockquote><h3 id=22--93117-debiased-model-based-interactive-recommendation-zijian-li-et-al-2024>(2/2 | 93/117) Debiased Model-based Interactive Recommendation (Zijian Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zijian Li, Ruichu Cai, Haiqin Huang, Sili Zhang, Yuguang Yan, Zhifeng Hao, Zhenghua Dong. (2024)<br><strong>Debiased Model-based Interactive Recommendation</strong><br><button class=copy-to-clipboard title="Debiased Model-based Interactive Recommendation" index=93>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-93 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs-LG, cs.IR<br>Keyword Score: 20<br>Keywords: Contrastive Learning, Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15819v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15819v1.pdf filename=2402.15819v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Existing model-based interactive <b>recommendation</b> systems are trained by querying a world model to capture the user preference, but learning the world model from historical logged data will easily suffer from bias issues such as popularity bias and sampling bias. This is why some debiased methods have been proposed recently. However, two essential drawbacks still remain: 1) ignoring the dynamics of the time-varying popularity results in a false reweighting of items. 2) taking the unknown samples as negative samples in negative sampling results in the sampling bias. To overcome these two drawbacks, we develop a model called \textbf{i}dentifiable \textbf{D}ebiased \textbf{M}odel-based \textbf{I}nteractive \textbf{R}ecommendation (\textbf{iDMIR} in short). In iDMIR, for the first drawback, we devise a debiased causal world model based on the causal mechanism of the time-varying <b>recommendation</b> generation process with identification guarantees; for the second drawback, we devise a debiased <b>contrastive</b> <b>policy,</b> which coincides with the debiased <b>contrastive</b> <b>learning</b> and avoids sampling bias. Moreover, we demonstrate that the proposed method not only outperforms several latest interactive <b>recommendation</b> algorithms but also enjoys diverse <b>recommendation</b> performance.</p></p class="citation"></blockquote><h2 id=csni-2>cs.NI (2)</h2><h3 id=12--94117-dedicated-restricted-target-wake-time-for-real-time-applications-in-wi-fi-7-andrey-belogaev-et-al-2024>(1/2 | 94/117) Dedicated Restricted Target Wake Time for Real-Time Applications in Wi-Fi 7 (Andrey Belogaev et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Andrey Belogaev, Xiaoman Shen, Chun Pan, Xingfeng Jiang, Chris Blondia, Jeroen Famaey. (2024)<br><strong>Dedicated Restricted Target Wake Time for Real-Time Applications in Wi-Fi 7</strong><br><button class=copy-to-clipboard title="Dedicated Restricted Target Wake Time for Real-Time Applications in Wi-Fi 7" index=94>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-94 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NI<br>Categories: cs-NI, cs.NI<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15900v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15900v1.pdf filename=2402.15900v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Real-time applications (RTA) tend to play a crucial role in people&rsquo;s everyday life. Such applications are among the key use cases for the next generations of wireless technologies. RTA applications are characterized by strict guaranteed delay requirements (in the order of a few milliseconds). One of the pillars of enabling RTA in next-generation Wi-Fi standards is Restricted Target Wake Time (R-TWT), which provides Wi-Fi stations exclusive channel access within negotiated service periods (SPs). If each RTA data flow uses dedicated SPs for data transmission, they are completely isolated from each other and do not experience any contention. To ensure the satisfaction of RTA QoS requirements while minimizing the channel airtime consumption, it is important to properly select the R-TWT parameters, namely the duration of SPs and the period between SPs. In this paper, we develop a mathematical model that estimates the delay probability distribution and packet loss probability for a given set of network, traffic and R-TWT parameters. Using this model, the access point can select the optimal R-TWT parameters for the given QoS requirements. The high accuracy of the model is proven by means of <b>simulation.</b></p></p class="citation"></blockquote><h3 id=22--95117-balancedn-load-balancing-allocation-of-interest-for-fast-discovery-in-content-centric-networks-murali-gunti-et-al-2024>(2/2 | 95/117) BalanceDN: Load-Balancing Allocation of Interest for Fast Discovery in Content Centric Networks (Murali Gunti et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Murali Gunti, Roberto Rojas-Cessa. (2024)<br><strong>BalanceDN: Load-Balancing Allocation of Interest for Fast Discovery in Content Centric Networks</strong><br><button class=copy-to-clipboard title="BalanceDN: Load-Balancing Allocation of Interest for Fast Discovery in Content Centric Networks" index=95>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-95 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NI<br>Categories: cs-NI, cs.NI<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15844v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15844v1.pdf filename=2402.15844v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In Named Data Networking (NDN), data is identified by unique names instead of IP addresses, and routers use the names of the content to forward Interest packets towards the producers of the requested content. However, the current content search mechanism in NDN is complex and slow. This mechanism not only creates congestion but also hinders practical deployment due to its slowness and cumbersome nature. To address this issue, we propose a methodology, called BalanceDN, that distributes content through the network such that sought content can be found quickly. BalanceDN uses a distributed allocation of resolvers as those used by the domain name system but differs in how content is distributed. Our approach avoids flooding the network with pending interest requests and also eliminates the need for blind search when the location of content is unknown. We tested our approach on ndnSIM; a <b>simulation</b> platform for NDN. The results show that the proposed routing scheme utilizes far fewer network resources compared to the NDN network when retrieving content. The proposed scheme accomplishes this performance gain by leveraging a load-balanced hashing mechanism to distribute and locate the name of the content on the distributed nameserver lookup service nodes.</p></p class="citation"></blockquote><h2 id=csgt-1>cs.GT (1)</h2><h3 id=11--96117-differentially-private-bayesian-persuasion-yuqi-pan-et-al-2024>(1/1 | 96/117) Differentially Private Bayesian Persuasion (Yuqi Pan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuqi Pan, Zhiwei Steven Wu, Haifeng Xu, Shuran Zheng. (2024)<br><strong>Differentially Private Bayesian Persuasion</strong><br><button class=copy-to-clipboard title="Differentially Private Bayesian Persuasion" index=96>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-96 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.GT<br>Categories: cs-GT, cs.GT<br>Keyword Score: 20<br>Keywords: Recommendation, Differential Privacy<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15872v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15872v1.pdf filename=2402.15872v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The tension between persuasion and privacy preservation is common in real-world settings. Online platforms should protect the privacy of web users whose data they collect, even as they seek to disclose information about these data to selling advertising spaces. Similarly, hospitals may share patient data to attract research investments with the obligation to preserve patients&rsquo; privacy. To deal with these issues, we develop a framework to study Bayesian persuasion under <b>differential</b> <b>privacy</b> constraints, where the sender must design an optimal signaling scheme for persuasion while guaranteeing the privacy of each agent&rsquo;s private information in the database. To understand how privacy constraints affect information disclosure, we explore two perspectives within Bayesian persuasion: one views the mechanism as releasing a posterior about the private data, while the other views it as sending an action <b>recommendation.</b> The posterior-based formulation helps consider privacy-utility tradeoffs, quantifying how the tightness of privacy constraints impacts the sender&rsquo;s optimal utility. For any instance in a common utility function family and a wide range of privacy levels, a significant constant utility gap can be found between any two of the three conditions: $\epsilon$-differential privacy constraint, relaxation $(\epsilon,\delta)$-differential privacy constraint, and no privacy constraint. We further geometrically characterize optimal signaling schemes under different types of constraints ($\epsilon$-differential privacy, $(\epsilon,\delta)$-differential privacy and Renyi <b>differential</b> <b>privacy),</b> all of which can be seen as finding concave hulls in constrained posterior regions. Meanwhile, by taking the action-based view of persuasion, we provide polynomial-time algorithms for computing optimal differentially private signaling schemes, as long as a mild homogeneous condition is met.</p></p class="citation"></blockquote><h2 id=cscy-1>cs.CY (1)</h2><h3 id=11--97117-from-cobit-to-iso-42001-evaluating-cybersecurity-frameworks-for-opportunities-risks-and-regulatory-compliance-in-commercializing-large-language-models-timothy-r-mcintosh-et-al-2024>(1/1 | 97/117) From COBIT to ISO 42001: Evaluating Cybersecurity Frameworks for Opportunities, Risks, and Regulatory Compliance in Commercializing Large Language Models (Timothy R. McIntosh et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Timothy R. McIntosh, Teo Susnjak, Tong Liu, Paul Watters, Raza Nowrozy, Malka N. Halgamuge. (2024)<br><strong>From COBIT to ISO 42001: Evaluating Cybersecurity Frameworks for Opportunities, Risks, and Regulatory Compliance in Commercializing Large Language Models</strong><br><button class=copy-to-clipboard title="From COBIT to ISO 42001: Evaluating Cybersecurity Frameworks for Opportunities, Risks, and Regulatory Compliance in Commercializing Large Language Models" index=97>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-97 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CY<br>Categories: cs-AI, cs-CY, cs.CY<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15770v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15770v1.pdf filename=2402.15770v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This study investigated the integration readiness of four predominant cybersecurity Governance, Risk and Compliance (GRC) frameworks - NIST CSF 2.0, COBIT 2019, ISO 27001:2022, and the latest ISO 42001:2023 - for the opportunities, risks, and regulatory compliance when adopting <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs),</b> using qualitative content analysis and expert validation. Our analysis, with both <b>LLMs</b> and human experts in the loop, uncovered potential for <b>LLM</b> integration together with inadequacies in <b>LLM</b> risk oversight of those frameworks. Comparative gap analysis has highlighted that the new ISO 42001:2023, specifically designed for Artificial Intelligence (AI) management systems, provided most comprehensive facilitation for <b>LLM</b> opportunities, whereas COBIT 2019 aligned most closely with the impending European Union AI Act. Nonetheless, our findings suggested that all evaluated frameworks would benefit from enhancements to more effectively and more comprehensively address the multifaceted risks associated with <b>LLMs,</b> indicating a critical and time-sensitive need for their continuous evolution. We propose integrating human-expert-in-the-loop validation processes as crucial for enhancing cybersecurity frameworks to support secure and compliant <b>LLM</b> integration, and discuss implications for the continuous evolution of cybersecurity GRC frameworks to support the secure integration of <b>LLMs.</b></p></p class="citation"></blockquote><h2 id=cshc-3>cs.HC (3)</h2><h3 id=13--98117-areeg_chars-dataset-for-envisioned-speech-recognition-using-eeg-for-arabic-characters-hazem-darwish-et-al-2024>(1/3 | 98/117) ArEEG_Chars: Dataset for Envisioned Speech Recognition using EEG for Arabic Characters (Hazem Darwish et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hazem Darwish, Abdalrahman Al Malah, Khloud Al Jallad, Nada Ghneim. (2024)<br><strong>ArEEG_Chars: Dataset for Envisioned Speech Recognition using EEG for Arabic Characters</strong><br><button class=copy-to-clipboard title="ArEEG_Chars: Dataset for Envisioned Speech Recognition using EEG for Arabic Characters" index=98>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-98 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-CL, cs-HC, cs-LG, cs.HC, eess-SP<br>Keyword Score: 20<br>Keywords: LSTM, Automatic Speech Recognition<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15733v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15733v1.pdf filename=2402.15733v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Brain-Computer-Interface (BCI) has been a hot research topic in the last few years that could help paralyzed people in their lives. Several researches were done to classify electroencephalography (EEG) signals automatically into English characters and words. Arabic language is one of the most used languages around the world. However, to the best of our knowledge, there is no dataset for Arabic characters EEG signals. In this paper, we have created an EEG dataset for Arabic characters and named it ArEEG_Chars. Moreover, several experiments were done on ArEEG_Chars using deep learning. Best results were achieved using <b>LSTM</b> and reached an accuracy of 97%. ArEEG_Chars dataset will be public for researchers.</p></p class="citation"></blockquote><h3 id=23--99117-applied-user-research-in-virtual-reality-tools-methods-and-challenges-leonie-bensch-et-al-2024>(2/3 | 99/117) Applied User Research in Virtual Reality: Tools, Methods, and Challenges (Leonie Bensch et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Leonie Bensch, Andrea Casini, Aidan Cowley, Florian Dufresne, Enrico Guerra, Paul de Medeiros, Tommy Nilsson, Flavie Rometsch, Andreas Treuer, Anna Vock. (2024)<br><strong>Applied User Research in Virtual Reality: Tools, Methods, and Challenges</strong><br><button class=copy-to-clipboard title="Applied User Research in Virtual Reality: Tools, Methods, and Challenges" index=99>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-99 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: 93B51, 97M50, H-1-2; I-3-8; J-4; J-m; K-8-2; J-6, cs-HC, cs-MM, cs.HC<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15695v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15695v1.pdf filename=2402.15695v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This chapter explores the practice of conducting user research studies and design assessments in virtual reality (VR). An overview of key VR hardware and software tools is provided, including game engines, such as Unity and Unreal Engine. Qualitative and quantitative research methods, along with their various synergies with VR, are likewise discussed, and some of the challenges associated with VR, such as limited sensory stimulation, are reflected upon. VR is proving particularly useful in the context of space systems development, where its utilisation offers a cost-effective and secure method for simulating extraterrestrial environments, allowing for rapid prototyping and evaluation of innovative concepts under representative operational conditions. To illustrate this, we present a case study detailing the application of VR to aid aerospace engineers testing their ideas with end-users and stakeholders during early design stages of the European Space Agency&rsquo;s (ESA) prospective Argonaut lunar lander. This case study demonstrates the effectiveness of VR <b>simulations</b> in gathering important feedback concerning the operability of the Argonaut lander in poor lighting conditions as well as surfacing relevant ergonomics considerations and constraints. The chapter concludes by discussing the strengths and weaknesses associated with VR-based user studies and proposes future research directions, emphasising the necessity for novel VR interfaces to overcome existing technical limitations.</p></p class="citation"></blockquote><h3 id=33--100117-touching-the-moon-leveraging-passive-haptics-embodiment-and-presence-for-operational-assessments-in-virtual-reality-florian-dufresne-et-al-2024>(3/3 | 100/117) Touching the Moon: Leveraging Passive Haptics, Embodiment and Presence for Operational Assessments in Virtual Reality (Florian Dufresne et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Florian Dufresne, Tommy Nilsson, Geoffrey Gorisse, Enrico Guerra, André Zenner, Olivier Christmann, Leonie Bensch, Nikolai Anton Callus, Aidan Cowley. (2024)<br><strong>Touching the Moon: Leveraging Passive Haptics, Embodiment and Presence for Operational Assessments in Virtual Reality</strong><br><button class=copy-to-clipboard title="Touching the Moon: Leveraging Passive Haptics, Embodiment and Presence for Operational Assessments in Virtual Reality" index=100>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-100 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: 93B51, 97M50, H-1-2; I-3-8; J-4; J-m; K-8-2; J-6, cs-HC, cs.HC<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15694v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15694v1.pdf filename=2402.15694v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Space agencies are in the process of drawing up carefully thought-out Concepts of Operations (ConOps) for future human missions on the Moon. These are typically assessed and validated through costly and logistically demanding analogue field studies. While interactive <b>simulations</b> in Virtual Reality (VR) offer a comparatively cost-effective alternative, they have faced criticism for lacking the fidelity of real-world deployments. This paper explores the applicability of passive haptic interfaces in bridging the gap between simulated and real-world ConOps assessments. Leveraging passive haptic props (equipment mockup and astronaut gloves), we virtually recreated the Apollo 12 mission procedure and assessed it with experienced astronauts and other space experts. Quantitative and qualitative findings indicate that haptics increased presence and embodiment, thus improving perceived <b>simulation</b> fidelity and validity of user reflections. We conclude by discussing the potential role of passive haptic modalities in facilitating early-stage ConOps assessments for human endeavours on the Moon and beyond.</p></p class="citation"></blockquote><h2 id=eesssy-4>eess.SY (4)</h2><h3 id=14--101117-design-and-implementation-of-low-cost-electric-vehicles-evs-supercharger-a-comprehensive-review-md-khaledur-rahman-et-al-2024>(1/4 | 101/117) Design and Implementation of Low-Cost Electric Vehicles (Evs) Supercharger: A Comprehensive Review (Md Khaledur Rahman et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Md Khaledur Rahman, Faysal Amin Tanvir, Md Saiful Islam, Md Shameem Ahsan, Manam Ahmed. (2024)<br><strong>Design and Implementation of Low-Cost Electric Vehicles (Evs) Supercharger: A Comprehensive Review</strong><br><button class=copy-to-clipboard title="Design and Implementation of Low-Cost Electric Vehicles (Evs) Supercharger: A Comprehensive Review" index=101>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-101 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 20<br>Keywords: Probabilistic Model, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15728v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15728v1.pdf filename=2402.15728v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This article presents a <b>probabilistic</b> <b>modeling</b> method utilizing smart meter data and an innovative agent-based simulator for electric vehicles (EVs). The aim is to assess the effects of different cost-driven EV charging strategies on the power distribution network (PDN). We investigate the effects of a 40% EV adoption on three parts of Frederiksberg&rsquo;s low voltage distribution network (LVDN), a densely urbanized municipality in Denmark. Our findings indicate that cable and <b>transformer</b> overloading especially pose a challenge. However, the impact of EVs varies significantly between each LVDN area and charging scenario. Across scenarios and LVDNs, the share of cables facing congestion ranges between 5% and 60%. It is also revealed that time-of-use (ToU)-based and single-day cost-minimized charging could be beneficial for LVDNs with moderate EV adoption rates. In contrast, multiple-day optimization will likely lead to severe congestion, as such strategies concentrate demand on a single day that would otherwise be distributed over several days, thus raising concerns about how to prevent it. The broader implications of our research suggest that, despite initial worries primarily centered on congestion due to unregulated charging during peak hours, a transition to cost-based smart charging, propelled by an increasing awareness of time-dependent electricity prices, may lead to a significant rise in charging synchronization, bringing about undesirable consequences for the power distribution network (PDN).</p></p class="citation"></blockquote><h3 id=24--102117-consensus-seeking-in-diffusive-multidimensional-networks-with-a-repeated-interaction-pattern-and-time-delays-hoang-huy-vu-et-al-2024>(2/4 | 102/117) Consensus seeking in diffusive multidimensional networks with a repeated interaction pattern and time-delays (Hoang Huy Vu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hoang Huy Vu, Quyen Ngoc Nguyen, Chuong Van Nguyen, Tuynh Van Pham, Minh Hoang Trinh. (2024)<br><strong>Consensus seeking in diffusive multidimensional networks with a repeated interaction pattern and time-delays</strong><br><button class=copy-to-clipboard title="Consensus seeking in diffusive multidimensional networks with a repeated interaction pattern and time-delays" index=102>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-102 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-MA, cs-SY, eess-SY, eess.SY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15677v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15677v1.pdf filename=2402.15677v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper studies a consensus problem in multidimensional networks having the same agent-to-agent interaction pattern under both intra- and cross-layer time delays. Several conditions for the agents to globally asymptotically achieve a consensus are derived, which involve the overall network&rsquo;s structure, the local interacting pattern, and the values of the time delays. The validity of these conditions is proved by direct eigenvalue evaluation and supported by numerical <b>simulations.</b></p></p class="citation"></blockquote><h3 id=34--103117-concurrent-learning-of-policy-and-unknown-safety-constraints-in-reinforcement-learning-lunet-yifru-et-al-2024>(3/4 | 103/117) Concurrent Learning of Policy and Unknown Safety Constraints in Reinforcement Learning (Lunet Yifru et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lunet Yifru, Ali Baheri. (2024)<br><strong>Concurrent Learning of Policy and Unknown Safety Constraints in Reinforcement Learning</strong><br><button class=copy-to-clipboard title="Concurrent Learning of Policy and Unknown Safety Constraints in Reinforcement Learning" index=103>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-103 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-LG, cs-SY, eess-SY, eess.SY<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15893v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15893v2.pdf filename=2402.15893v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Reinforcement</b> <b>learning</b> (RL) has revolutionized decision-making across a wide range of domains over the past few decades. Yet, deploying RL policies in real-world scenarios presents the crucial challenge of ensuring safety. Traditional safe RL approaches have predominantly focused on incorporating predefined safety constraints into the policy learning process. However, this reliance on predefined safety constraints poses limitations in dynamic and unpredictable real-world settings where such constraints may not be available or sufficiently adaptable. Bridging this gap, we propose a novel approach that concurrently learns a safe RL control policy and identifies the unknown safety constraint parameters of a given environment. Initializing with a parametric signal temporal logic (pSTL) safety specification and a small initial labeled dataset, we frame the problem as a bilevel optimization task, intricately integrating constrained policy optimization, using a Lagrangian-variant of the twin delayed deep deterministic policy gradient (TD3) algorithm, with Bayesian optimization for optimizing parameters for the given pSTL safety specification. Through experimentation in comprehensive case studies, we validate the efficacy of this approach across varying forms of environmental constraints, consistently yielding safe RL policies with high returns. Furthermore, our findings indicate successful learning of STL safety constraint parameters, exhibiting a high degree of conformity with true environmental safety constraints. The performance of our model closely mirrors that of an ideal scenario that possesses complete prior knowledge of safety constraints, demonstrating its proficiency in accurately identifying environmental safety constraints and learning safe policies that adhere to those constraints.</p></p class="citation"></blockquote><h3 id=44--104117-analysis-of-off-policy-multi-step-td-learning-with-linear-function-approximation-donghwan-lee-2024>(4/4 | 104/117) Analysis of Off-Policy Multi-Step TD-Learning with Linear Function Approximation (Donghwan Lee, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Donghwan Lee. (2024)<br><strong>Analysis of Off-Policy Multi-Step TD-Learning with Linear Function Approximation</strong><br><button class=copy-to-clipboard title="Analysis of Off-Policy Multi-Step TD-Learning with Linear Function Approximation" index=104>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-104 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-LG, cs-SY, eess-SY, eess.SY<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15781v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15781v1.pdf filename=2402.15781v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper analyzes multi-step TD-learning algorithms within the `deadly triad&rsquo; scenario, characterized by linear function approximation, off-policy learning, and bootstrapping. In particular, we prove that n-step TD-learning algorithms converge to a solution as the sampling horizon n increases sufficiently. The paper is divided into two parts. In the first part, we comprehensively examine the fundamental properties of their model-based deterministic counterparts, including projected value iteration, gradient descent algorithms, and the control theoretic approach, which can be viewed as prototype deterministic algorithms whose analysis plays a pivotal role in understanding and developing their model-free <b>reinforcement</b> <b>learning</b> counterparts. In particular, we prove that these algorithms converge to meaningful solutions when n is sufficiently large. Based on these findings, two n-step TD-learning algorithms are proposed and analyzed, which can be seen as the model-free <b>reinforcement</b> <b>learning</b> counterparts of the gradient and control theoretic algorithms.</p></p class="citation"></blockquote><h2 id=statml-1>stat.ML (1)</h2><h3 id=11--105117-a-duality-analysis-of-kernel-ridge-regression-in-the-noiseless-regime-jihao-long-et-al-2024>(1/1 | 105/117) A Duality Analysis of Kernel Ridge Regression in the Noiseless Regime (Jihao Long et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jihao Long, Xiaojun Peng, Lei Wu. (2024)<br><strong>A Duality Analysis of Kernel Ridge Regression in the Noiseless Regime</strong><br><button class=copy-to-clipboard title="A Duality Analysis of Kernel Ridge Regression in the Noiseless Regime" index=105>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-105 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-LG, stat-ML, stat.ML<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15718v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15718v1.pdf filename=2402.15718v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we conduct a comprehensive analysis of generalization properties of Kernel Ridge Regression (KRR) in the noiseless regime, a scenario crucial to scientific computing, where data are often generated via computer <b>simulations.</b> We prove that KRR can attain the minimax optimal rate, which depends on both the eigenvalue decay of the associated kernel and the relative smoothness of target functions. Particularly, when the eigenvalue decays exponentially fast, KRR achieves the spectral accuracy, i.e., a convergence rate faster than any polynomial. Moreover, the numerical experiments well corroborate our theoretical findings. Our proof leverages a novel extension of the duality framework introduced by Chen et al. (2023), which could be useful in analyzing kernel-based methods beyond the scope of this work.</p></p class="citation"></blockquote><h2 id=eesssp-1>eess.SP (1)</h2><h3 id=11--106117-multimodal-sleep-apnea-detection-with-missing-or-noisy-modalities-hamed-fayyaz-et-al-2024>(1/1 | 106/117) Multimodal Sleep Apnea Detection with Missing or Noisy Modalities (Hamed Fayyaz et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hamed Fayyaz, Abigail Strang, Niharika S. D&rsquo;Souza, Rahmatollah Beheshti. (2024)<br><strong>Multimodal Sleep Apnea Detection with Missing or Noisy Modalities</strong><br><button class=copy-to-clipboard title="Multimodal Sleep Apnea Detection with Missing or Noisy Modalities" index=106>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-106 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SP<br>Categories: cs-LG, eess-SP, eess.SP<br>Keyword Score: 16<br>Keywords: Multi-modal, Multi-modal, Event Detection<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.17788v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.17788v1.pdf filename=2402.17788v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Polysomnography (PSG) is a type of sleep study that records <b>multimodal</b> physiological signals and is widely used for purposes such as sleep staging and respiratory <b>event</b> <b>detection.</b> Conventional machine learning methods assume that each sleep study is associated with a fixed set of observed modalities and that all modalities are available for each sample. However, noisy and missing modalities are a common issue in real-world clinical settings. In this study, we propose a comprehensive pipeline aiming to compensate for the missing or noisy modalities when performing sleep apnea detection. Unlike other existing studies, our proposed model works with any combination of available modalities. Our experiments show that the proposed model outperforms other state-of-the-art approaches in sleep apnea detection using various subsets of available data and different levels of noise, and maintains its high performance (AUROC>0.9) even in the presence of high levels of noise or missingness. This is especially relevant in settings where the level of noise and missingness is high (such as pediatric or outside-of-clinic scenarios).</p></p class="citation"></blockquote><h2 id=csse-2>cs.SE (2)</h2><h3 id=12--107117-advancing-bdd-software-testing-dynamic-scenario-re-usability-and-step-auto-complete-for-cucumber-framework-a-h-mughal-2024>(1/2 | 107/117) Advancing BDD Software Testing: Dynamic Scenario Re-Usability And Step Auto-Complete For Cucumber Framework (A. H. Mughal, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>A. H. Mughal. (2024)<br><strong>Advancing BDD Software Testing: Dynamic Scenario Re-Usability And Step Auto-Complete For Cucumber Framework</strong><br><button class=copy-to-clipboard title="Advancing BDD Software Testing: Dynamic Scenario Re-Usability And Step Auto-Complete For Cucumber Framework" index=107>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-107 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-CY, cs-PL, cs-SE, cs.SE<br>Keyword Score: 10<br>Keywords: Question Answering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15928v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15928v1.pdf filename=2402.15928v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents and implements the re-usability of scenarios within scenarios for behavior-driven development (BDD) Gherkin test scripts in the Cucumber Java framework. Though the focus of the presented work is on scenario re-usability through an implementation within the Cucumber BDD Java framework, the paper also dives a little into the limitations of Cucumber single-threaded scenario execution model. This implementation increases the modularity and efficiency of the test suite. The paper also discusses VSCode step definition auto-completion integration, simplifying the test script writing process. This functionality is handy to Quality Assurance(QA) test writers, allowing instant access to relevant step definitions. In addition, the use of these methods in a popular continuous integration and delivery platform Jenkins as a Maven Java project is discussed. This integration with Jenkins, facilitates for more efficient test automation for continuous deployment scenarios. Empirical research and practical applications reveal significant improvements in the speed and efficiency of test writing, which is especially valuable for large and complex software projects. Integrating these methods into traditional sequential BDD practices paves the way towards more effective, efficient, and sustainable test automation strategies.</p></p class="citation"></blockquote><h3 id=22--108117-importance-guided-data-augmentation-for-neural-based-code-understanding-zeming-dong-et-al-2024>(2/2 | 108/117) Importance Guided Data Augmentation for Neural-Based Code Understanding (Zeming Dong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zeming Dong, Qiang Hu, Xiaofei Xie, Maxime Cordy, Mike Papadakis, Jianjun Zhao. (2024)<br><strong>Importance Guided Data Augmentation for Neural-Based Code Understanding</strong><br><button class=copy-to-clipboard title="Importance Guided Data Augmentation for Neural-Based Code Understanding" index=108>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-108 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-AI, cs-SE, cs.SE<br>Keyword Score: 10<br>Keywords: Data Augmentation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15769v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15769v1.pdf filename=2402.15769v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Pre-trained code models lead the era of code intelligence. Many models have been designed with impressive performance recently. However, one important problem, <b>data</b> <b>augmentation</b> for code <b>data</b> <b>that</b> automatically helps developers prepare training <b>data</b> <b>lacks</b> study in the field of code learning. In this paper, we introduce a general <b>data</b> <b>augmentation</b> framework, GenCode, to enhance the training of code understanding models. GenCode follows a generation-and-selection paradigm to prepare useful training codes. Specifically, it uses code transformation techniques to generate new code candidates first and then selects important ones as the training <b>data</b> <b>by</b> importance metrics. To evaluate the effectiveness of GenCode with a general importance metric &ndash; loss value, we conduct experiments on four code understanding tasks (e.g., code clone detection) and three pre-trained code models (e.g., CodeT5). Compared to the state-of-the-art (SOTA) code augmentation method, MixCode, GenCode produces code models with 2.92% higher accuracy and 4.90% robustness on average.</p></p class="citation"></blockquote><h2 id=econth-1>econ.TH (1)</h2><h3 id=11--109117-optimal-budget-aggregation-with-single-peaked-preferences-felix-brandt-et-al-2024>(1/1 | 109/117) Optimal Budget Aggregation with Single-Peaked Preferences (Felix Brandt et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Felix Brandt, Matthias Greger, Erel Segal-Halevi, Warut Suksompong. (2024)<br><strong>Optimal Budget Aggregation with Single-Peaked Preferences</strong><br><button class=copy-to-clipboard title="Optimal Budget Aggregation with Single-Peaked Preferences" index=109>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-109 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: econ.TH<br>Categories: cs-GT, econ-TH, econ.TH<br>Keyword Score: 10<br>Keywords: Fairness<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15904v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15904v1.pdf filename=2402.15904v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We study the problem of aggregating distributions, such as budget proposals, into a collective distribution. An ideal aggregation mechanism would be Pareto efficient, strategyproof, and fair. Most previous work assumes that agents evaluate budgets according to the $\ell_1$ distance to their ideal budget. We investigate and compare different models from the larger class of star-shaped utility functions - a multi-dimensional generalization of single-peaked preferences. For the case of two alternatives, we extend existing results by proving that under very general assumptions, the uniform phantom mechanism is the only strategyproof mechanism that satisfies proportionality - a minimal notion of <b>fairness</b> introduced by Freeman et al. (2021). Moving to the case of more than two alternatives, we establish sweeping impossibilities for $\ell_1$ and $\ell_\infty$ disutilities: no mechanism satisfies efficiency, strategyproofness, and proportionality. We then propose a new kind of star-shaped utilities based on evaluating budgets by the ratios of shares between a given budget and an ideal budget. For these utilities, efficiency, strategyproofness, and <b>fairness</b> become compatible. In particular, we prove that the mechanism that maximizes the Nash product of individual utilities is characterized by group-strategyproofness and a core-based <b>fairness</b> condition.</p></p class="citation"></blockquote><h2 id=q-biogn-1>q-bio.GN (1)</h2><h3 id=11--110117-fgbert-function-driven-pre-trained-gene-language-model-for-metagenomics-chenrui-duan-et-al-2024>(1/1 | 110/117) FGBERT: Function-Driven Pre-trained Gene Language Model for Metagenomics (ChenRui Duan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>ChenRui Duan, Zelin Zang, Yongjie Xu, Hang He, Zihan Liu, Zijia Song, Ju-Sheng Zheng, Stan Z. Li. (2024)<br><strong>FGBERT: Function-Driven Pre-trained Gene Language Model for Metagenomics</strong><br><button class=copy-to-clipboard title="FGBERT: Function-Driven Pre-trained Gene Language Model for Metagenomics" index=110>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-110 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: q-bio.GN<br>Categories: cs-AI, cs-LG, q-bio-GN, q-bio.GN<br>Keyword Score: 10<br>Keywords: Contrastive Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16901v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16901v1.pdf filename=2402.16901v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Metagenomic data, comprising mixed multi-species genomes, are prevalent in diverse environments like oceans and soils, significantly impacting human health and ecological functions. However, current research relies on K-mer representations, limiting the capture of structurally relevant gene contexts. To address these limitations and further our understanding of complex relationships between metagenomic sequences and their functions, we introduce a protein-based gene representation as a context-aware and structure-relevant tokenizer. Our approach includes Masked Gene Modeling (MGM) for gene group-level pre-training, providing insights into inter-gene contextual information, and Triple Enhanced Metagenomic <b>Contrastive</b> <b>Learning</b> (TEM-CL) for gene-level pre-training to model gene sequence-function relationships. MGM and TEM-CL constitute our novel metagenomic language model {\NAME}, pre-trained on 100 million metagenomic sequences. We demonstrate the superiority of our proposed {\NAME} on eight datasets.</p></p class="citation"></blockquote><h2 id=cssi-1>cs.SI (1)</h2><h3 id=11--111117-mim-reasoner-learning-with-theoretical-guarantees-for-multiplex-influence-maximization-nguyen-do-et-al-2024>(1/1 | 111/117) MIM-Reasoner: Learning with Theoretical Guarantees for Multiplex Influence Maximization (Nguyen Do et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nguyen Do, Tanmoy Chowdhury, Chen Ling, Liang Zhao, My T. Thai. (2024)<br><strong>MIM-Reasoner: Learning with Theoretical Guarantees for Multiplex Influence Maximization</strong><br><button class=copy-to-clipboard title="MIM-Reasoner: Learning with Theoretical Guarantees for Multiplex Influence Maximization" index=111>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-111 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SI<br>Categories: cs-AI, cs-LG, cs-SI, cs.SI, math-PR, stat-ML<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.16898v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.16898v1.pdf filename=2402.16898v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Multiplex influence maximization (MIM) asks us to identify a set of seed users such as to maximize the expected number of influenced users in a multiplex network. MIM has been one of central research topics, especially in nowadays social networking landscape where users participate in multiple online social networks (OSNs) and their influences can propagate among several OSNs simultaneously. Although there exist a couple combinatorial algorithms to MIM, learning-based solutions have been desired due to its generalization ability to heterogeneous networks and their diversified propagation characteristics. In this paper, we introduce MIM-Reasoner, coupling <b>reinforcement</b> <b>learning</b> with probabilistic graphical model, which effectively captures the complex propagation process within and between layers of a given multiplex network, thereby tackling the most challenging problem in MIM. We establish a theoretical guarantee for MIM-Reasoner as well as conduct extensive analyses on both synthetic and real-world datasets to validate our MIM-Reasoner&rsquo;s performance.</p></p class="citation"></blockquote><h2 id=cspl-1>cs.PL (1)</h2><h3 id=11--112117-formally-verified-c-code-generation-from-hybrid-communicating-sequential-processes-shuling-wang-et-al-2024>(1/1 | 112/117) Formally Verified C Code Generation from Hybrid Communicating Sequential Processes (Shuling Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shuling Wang, Zekun Ji, Bohua Zhan, Xiong Xu, Qiang Gao, Naijun Zhan. (2024)<br><strong>Formally Verified C Code Generation from Hybrid Communicating Sequential Processes</strong><br><button class=copy-to-clipboard title="Formally Verified C Code Generation from Hybrid Communicating Sequential Processes" index=112>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-112 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.PL<br>Categories: cs-PL, cs.PL<br>Keyword Score: 10<br>Keywords: Code Generation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15674v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15674v2.pdf filename=2402.15674v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Hybrid Communicating Sequential Processes (HCSP) is a formal model for hybrid systems, including primitives for evolution along an ordinary differential equation (ODE), communication, and parallel composition. <b>Code</b> <b>generation</b> is needed to convert HCSP models into <b>code</b> <b>that</b> can be executed in practice, and the correctness of this conversion is essential to ensure that the generated <b>code</b> <b>accurately</b> reflects the formal model. In this paper, we propose a <b>code</b> <b>generation</b> algorithm from HCSP to C with POSIX library for concurrency. The main difficulties include how to bridge the gap between the synchronized communication model in HCSP and the use of mutexes for synchronization in C, and how to discretize evolution along ODEs and support interrupt of ODE evolution by communication. To prove the correctness of <b>code</b> <b>generation,</b> we define a formal semantics for POSIX C, and build transition system models for both HCSP and C programs. We then define an approximate bisimulation relation between traces of transition systems, and show that under certain robustness conditions for HCSP, the generated C program is approximately bisimilar to the original model. Finally, we evaluate the <b>code</b> <b>generation</b> algorithm on a detailed model for automatic cruise control, showing its utility on real-world examples.</p></p class="citation"></blockquote><h2 id=cscg-1>cs.CG (1)</h2><h3 id=11--113117-an-on-log-n-time-approximation-scheme-for-geometric-many-to-many-matching-sayan-bandyapadhyay-et-al-2024>(1/1 | 113/117) An $O(n \log n)$-Time Approximation Scheme for Geometric Many-to-Many Matching (Sayan Bandyapadhyay et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sayan Bandyapadhyay, Jie Xue. (2024)<br><strong>An $O(n \log n)$-Time Approximation Scheme for Geometric Many-to-Many Matching</strong><br><button class=copy-to-clipboard title="An $O(n \log n)$-Time Approximation Scheme for Geometric Many-to-Many Matching" index=113>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-113 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CG<br>Categories: cs-CG, cs.CG<br>Keyword Score: 8<br>Keywords: Graph, Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15837v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15837v1.pdf filename=2402.15837v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Geometric matching is an important topic in computational <b>geometry</b> and has been extensively studied over decades. In this paper, we study a geometric-matching problem, known as geometric many-to-many matching. In this problem, the input is a set $S$ of $n$ colored points in $\mathbb{R}^d$, which implicitly defines a <b>graph</b> $G = (S,E(S))$ where $E(S) = {(p,q): p,q \in S \text{ have different colors}}$, and the goal is to compute a minimum-cost subset $E^* \subseteq E(S)$ of edges that cover all points in $S$. Here the cost of $E^<em>$ is the sum of the costs of all edges in $E^</em>$, where the cost of a single edge $e$ is the Euclidean distance (or more generally, the $L_p$-distance) between the two endpoints of $e$. Our main result is a $(1+\varepsilon)$-approximation algorithm with an optimal running time $O_\varepsilon(n \log n)$ for geometric many-to-many matching in any fixed dimension, which works under any $L_p$-norm. This is the first near-linear approximation scheme for the problem in any $d \geq 2$. Prior to this work, only the bipartite case of geometric many-to-many matching was considered in $\mathbb{R}^1$ and $\mathbb{R}^2$, and the best known approximation scheme in $\mathbb{R}^2$ takes $O_\varepsilon(n^{1.5} \cdot \mathsf{poly}(\log n))$ time.</p></p class="citation"></blockquote><h2 id=mathna-1>math.NA (1)</h2><h3 id=11--114117-interpolation-based-immersogeometric-analysis-methods-for-multi-material-and-multi-physics-problems-jennifer-e-fromm-et-al-2024>(1/1 | 114/117) Interpolation-based immersogeometric analysis methods for multi-material and multi-physics problems (Jennifer E. Fromm et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jennifer E. Fromm, Nils Wunsch, Kurt Maute, John A. Evans, Jiun-Shyan Chen. (2024)<br><strong>Interpolation-based immersogeometric analysis methods for multi-material and multi-physics problems</strong><br><button class=copy-to-clipboard title="Interpolation-based immersogeometric analysis methods for multi-material and multi-physics problems" index=114>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-114 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: cs-NA, math-NA, math.NA<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15937v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15937v1.pdf filename=2402.15937v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Immersed boundary methods are high-order accurate computational tools used to model geometrically complex problems in computational mechanics. While traditional finite element methods require the construction of high-quality boundary-fitted meshes, immersed boundary methods instead embed the computational domain in a background grid. Interpolation-based immersed boundary methods augment existing finite element software to non-invasively implement immersed boundary capabilities through extraction. Extraction interpolates the background basis as a linear combination of Lagrange polynomials defined on a foreground mesh, creating an interpolated basis that can be easily integrated by existing methods. This work extends the interpolation-based immersed boundary method to multi-material and multi-physics problems. Beginning from level-set descriptions of domain geometries, Heaviside enrichment is implemented to accommodate discontinuities in state variable fields across material interfaces. Adaptive refinement with truncated hierarchical B-splines is used to both improve interface <b>geometry</b> representations and resolve large solution gradients near interfaces. Multi-physics problems typically involve coupled fields where each field has unique discretization requirements. This work presents a novel discretization method for coupled problems through the application of extraction, using a single foreground mesh for all fields. Numerical examples illustrate optimal convergence rates for this method in both 2D and 3D, for heat conduction, linear elasticity, and a coupled thermo-mechanical problem. The utility of this method is demonstrated through image-based analysis of a composite sample, where in addition to circumventing typical meshing difficulties, this method reduces the required degrees of freedom compared to classical boundary-fitted finite element methods.</p></p class="citation"></blockquote><h2 id=csit-1>cs.IT (1)</h2><h3 id=11--115117-study-of-noncoherent-sparse-subarrays-for-direction-finding-based-on-low-rank-and-sparse-recovery-w-leite-et-al-2024>(1/1 | 115/117) Study of Noncoherent Sparse Subarrays for Direction Finding Based on Low-Rank and Sparse Recovery (W. Leite et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>W. Leite, R. C. de Lamare. (2024)<br><strong>Study of Noncoherent Sparse Subarrays for Direction Finding Based on Low-Rank and Sparse Recovery</strong><br><button class=copy-to-clipboard title="Study of Noncoherent Sparse Subarrays for Direction Finding Based on Low-Rank and Sparse Recovery" index=115>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-115 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, eess-SP, math-IT<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15681v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15681v1.pdf filename=2402.15681v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper investigates the problem of noncoherent direction-of-arrival (DOA) estimation using different sparse subarrays. In particular, we present a Multiple Measurements Vector (MMV) model for noncoherent DOA estimation based on a low-rank and sparse recovery optimization problem. Moreover, we develop two different practical strategies to obtain sparse arrays and subarrays: i) the subarrays are generated from a main sparse array <b>geometry</b> (Type-I sparse array), and ii) the sparse subarrays that are directly designed and grouped together to generate the whole sparse array (Type-II sparse array). Numerical results demonstrate that the proposed MMV model can benefit from multiple data records and that Type-II sparse noncoherent arrays are superior in performance for DOA estimation</p></p class="citation"></blockquote><h2 id=mathco-1>math.CO (1)</h2><h3 id=11--116117-on-the-finiteness-of-k-vertex-critical-2p_2-free-graphs-with-forbidden-induced-squids-or-bulls-melvin-adekanye-et-al-2024>(1/1 | 116/117) On the finiteness of $k$-vertex-critical $2P_2$-free graphs with forbidden induced squids or bulls (Melvin Adekanye et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Melvin Adekanye, Christopher Bury, Ben Cameron, Thaler Knodel. (2024)<br><strong>On the finiteness of $k$-vertex-critical $2P_2$-free graphs with forbidden induced squids or bulls</strong><br><button class=copy-to-clipboard title="On the finiteness of $k$-vertex-critical $2P_2$-free graphs with forbidden induced squids or bulls" index=116>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-116 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.CO<br>Categories: cs-DM, math-CO, math.CO<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15908v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15908v1.pdf filename=2402.15908v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A <b>graph</b> is $k$-vertex-critical if $\chi(G)=k$ but $\chi(G-v)&lt;k$ for all $v\in V(G)$ and $(G,H)$-free if it contains no induced subgraph isomorphic to $G$ or $H$. We show that there are only finitely many $k$-vertex-critical $(2P_2,H)$-free <b>graphs</b> for all $k$ when $H$ is isomorphic to any of the following <b>graphs</b> of order $5$: $bull$, $chair$, $claw+P_1$, or $\overline{diamond+P_1}$. The latter three are corollaries of more general results where $H$ is isomorphic to $(m, \ell)$-$squid$ for $m=3,4$ and any $\ell\ge 1$ where an $(m,\ell)$-$squid$ is the <b>graph</b> obtained from an $m$-cycle by attaching $\ell$ leaves to a single vertex of the cycle. For each of the <b>graphs</b> $H$ above and any fixed $k$, our results imply the existence of polynomial-time certifying algorithms for deciding the $k$-colourability problem for $(2P_2,H)$-free <b>graphs.</b> Further, our structural classifications allow us to exhaustively generate, with aid of computer search, all $k$-vertex-critical $(2P_2,H)$-free <b>graphs</b> for $k\le 7$ when $H=bull$ or $H=(4,1)$-$squid$ (also known as $banner$).</p></p class="citation"></blockquote><h2 id=csdm-1>cs.DM (1)</h2><h3 id=11--117117-symbolic-listings-as-computation-hamilton-sawczuk-et-al-2024>(1/1 | 117/117) Symbolic Listings as Computation (Hamilton Sawczuk et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hamilton Sawczuk, Edinah Gnang. (2024)<br><strong>Symbolic Listings as Computation</strong><br><button class=copy-to-clipboard title="Symbolic Listings as Computation" index=117>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-117 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DM<br>Categories: cs-DM, cs.DM<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.15885v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.15885v1.pdf filename=2402.15885v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We propose an algebraic model of computation which formally relates symbolic listings, complexity of Boolean functions, and low depth arithmetic circuit complexity. In this model algorithms are arithmetic formula expressing symbolic listings of YES instances of Boolean functions, and computation is executed via partial differential operators. We consider the Chow rank of an arithmetic formula as a measure of complexity and establish the Chow rank of multilinear polynomials with totally non-overlapping monomial support. We also provide Chow rank non-decreasing transformations from sets of <b>graphs</b> to sets of functional <b>graphs.</b></p></p class="citation"></blockquote></div><div class="row pl-3 pr-3"><div class="col-md-6 share-buttons"></div></div><hr><div class="row next-prev-navigator"><div class="col-md-6 previous-article"><a href=/akitenkrad-blog/posts/arxiv/202402/20240225000000/ title="arXiv @ 2024.02.25" class="btn btn-outline-info"><div><i class="fas fa-chevron-circle-left"></i> Prev</div><div class=next-prev-text>arXiv @ 2024.02.25</div></a></div><div class="col-md-6 next-article"><a href=/akitenkrad-blog/posts/arxiv/202402/20240227000000/ title="arXiv @ 2024.02.27" class="btn btn-outline-info"><div>Next <i class="fas fa-chevron-circle-right"></i></div><div class=next-prev-text>arXiv @ 2024.02.27</div></a></div></div><hr></div></div></div><a id=scroll-to-top class=btn><i class="fas fa-chevron-circle-up"></i></a></section><section class=toc-section id=toc-section><div class=toc-holder><h5 class="text-center pl-3">Table of Contents</h5><hr><div class=toc><nav id=TableOfContents><ul><li><a href=#primary-categories>Primary Categories</a></li><li><a href=#keywords>Keywords</a></li><li><a href=#cscl-21>cs.CL (21)</a><ul><li><a href=#121--1117-prompt-perturbation-consistency-learning-for-robust-language-models-yao-qiang-et-al-2024>(1/21 | 1/117) Prompt Perturbation Consistency Learning for Robust Language Models (Yao Qiang et al., 2024)</a></li><li><a href=#221--2117-linguistic-intelligence-in-large-language-models-for-telecommunications-tasnim-ahmed-et-al-2024>(2/21 | 2/117) Linguistic Intelligence in Large Language Models for Telecommunications (Tasnim Ahmed et al., 2024)</a></li><li><a href=#321--3117-evaluating-prompting-strategies-for-grammatical-error-correction-based-on-language-proficiency-min-zeng-et-al-2024>(3/21 | 3/117) Evaluating Prompting Strategies for Grammatical Error Correction Based on Language Proficiency (Min Zeng et al., 2024)</a></li><li><a href=#421--4117-look-before-you-leap-problem-elaboration-prompting-improves-mathematical-reasoning-in-large-language-models-haoran-liao-et-al-2024>(4/21 | 4/117) Look Before You Leap: Problem Elaboration Prompting Improves Mathematical Reasoning in Large Language Models (Haoran Liao et al., 2024)</a></li><li><a href=#521--5117-making-pre-trained-language-models-better-continual-few-shot-relation-extractors-shengkun-ma-et-al-2024>(5/21 | 5/117) Making Pre-trained Language Models Better Continual Few-Shot Relation Extractors (Shengkun Ma et al., 2024)</a></li><li><a href=#621--6117-leveraging-chatgpt-in-pharmacovigilance-event-extraction-an-empirical-study-zhaoyue-sun-et-al-2024>(6/21 | 6/117) Leveraging ChatGPT in Pharmacovigilance Event Extraction: An Empirical Study (Zhaoyue Sun et al., 2024)</a></li><li><a href=#721--7117-sportqa-a-benchmark-for-sports-understanding-in-large-language-models-haotian-xia-et-al-2024>(7/21 | 7/117) SportQA: A Benchmark for Sports Understanding in Large Language Models (Haotian Xia et al., 2024)</a></li><li><a href=#821--8117-gaokao-mm-a-chinese-human-level-benchmark-for-multimodal-models-evaluation-yi-zong-et-al-2024>(8/21 | 8/117) GAOKAO-MM: A Chinese Human-Level Benchmark for Multimodal Models Evaluation (Yi Zong et al., 2024)</a></li><li><a href=#921--9117-exploring-failure-cases-in-multimodal-reasoning-about-physical-dynamics-sadaf-ghaffari-et-al-2024>(9/21 | 9/117) Exploring Failure Cases in Multimodal Reasoning About Physical Dynamics (Sadaf Ghaffari et al., 2024)</a></li><li><a href=#1021--10117-dental-severity-assessment-through-few-shot-learning-and-sbert-fine-tuning-mohammad-dehghani-2024>(10/21 | 10/117) Dental Severity Assessment through Few-shot Learning and SBERT Fine-tuning (Mohammad Dehghani, 2024)</a></li><li><a href=#1121--11117-hd-eval-aligning-large-language-model-evaluators-through-hierarchical-criteria-decomposition-yuxuan-liu-et-al-2024>(11/21 | 11/117) HD-Eval: Aligning Large Language Model Evaluators Through Hierarchical Criteria Decomposition (Yuxuan Liu et al., 2024)</a></li><li><a href=#1221--12117-generalization-or-memorization-data-contamination-and-trustworthy-evaluation-for-large-language-models-yihong-dong-et-al-2024>(12/21 | 12/117) Generalization or Memorization: Data Contamination and Trustworthy Evaluation for Large Language Models (Yihong Dong et al., 2024)</a></li><li><a href=#1321--13117-semeval-2024-task-8-weighted-layer-averaging-roberta-for-black-box-machine-generated-text-detection-ayan-datta-et-al-2024>(13/21 | 13/117) SemEval-2024 Task 8: Weighted Layer Averaging RoBERTa for Black-Box Machine-Generated Text Detection (Ayan Datta et al., 2024)</a></li><li><a href=#1421--14117-foot-in-the-door-understanding-large-language-model-jailbreaking-via-cognitive-psychology-zhenhua-wang-et-al-2024>(14/21 | 14/117) Foot In The Door: Understanding Large Language Model Jailbreaking via Cognitive Psychology (Zhenhua Wang et al., 2024)</a></li><li><a href=#1521--15117-chimera-a-lossless-decoding-method-for-accelerating-large-language-models-inference-by-fusing-all-tokens-ziqian-zeng-et-al-2024>(15/21 | 15/117) Chimera: A Lossless Decoding Method for Accelerating Large Language Models Inference by Fusing all Tokens (Ziqian Zeng et al., 2024)</a></li><li><a href=#1621--16117-multicontrievers-analysis-of-dense-retrieval-representations-seraphina-goldfarb-tarrant-et-al-2024>(16/21 | 16/117) MultiContrievers: Analysis of Dense Retrieval Representations (Seraphina Goldfarb-Tarrant et al., 2024)</a></li><li><a href=#1721--17117-frustratingly-simple-prompting-based-text-denoising-jungyeul-park-et-al-2024>(17/21 | 17/117) Frustratingly Simple Prompting-based Text Denoising (Jungyeul Park et al., 2024)</a></li><li><a href=#1821--18117-mathwell-generating-educational-math-word-problems-at-scale-bryan-r-christ-et-al-2024>(18/21 | 18/117) MATHWELL: Generating Educational Math Word Problems at Scale (Bryan R Christ et al., 2024)</a></li><li><a href=#1921--19117-a-theoretical-result-on-the-inductive-bias-of-rnn-language-models-anej-svete-et-al-2024>(19/21 | 19/117) A Theoretical Result on the Inductive Bias of RNN Language Models (Anej Svete et al., 2024)</a></li><li><a href=#2021--20117-query-augmentation-by-decoding-semantics-from-brain-signals-ziyi-ye-et-al-2024>(20/21 | 20/117) Query Augmentation by Decoding Semantics from Brain Signals (Ziyi Ye et al., 2024)</a></li><li><a href=#2121--21117-measuring-bargaining-abilities-of-llms-a-benchmark-and-a-buyer-enhancement-method-tian-xia-et-al-2024>(21/21 | 21/117) Measuring Bargaining Abilities of LLMs: A Benchmark and A Buyer-Enhancement Method (Tian Xia et al., 2024)</a></li></ul></li><li><a href=#cscv-22>cs.CV (22)</a><ul><li><a href=#122--22117-res-vmamba-fine-grained-food-category-visual-classification-using-selective-state-space-models-with-deep-residual-learning-chi-sheng-chen-et-al-2024>(1/22 | 22/117) Res-VMamba: Fine-Grained Food Category Visual Classification Using Selective State Space Models with Deep Residual Learning (Chi-Sheng Chen et al., 2024)</a></li><li><a href=#222--23117-explainable-contrastive-and-cost-sensitive-learning-for-cervical-cancer-classification-ashfiqun-mustari-et-al-2024>(2/22 | 23/117) Explainable Contrastive and Cost-Sensitive Learning for Cervical Cancer Classification (Ashfiqun Mustari et al., 2024)</a></li><li><a href=#322--24117-increasing-sam-zero-shot-performance-on-multimodal-medical-images-using-gpt-4-generated-descriptive-prompts-without-human-annotation-zekun-jiang-et-al-2024>(3/22 | 24/117) Increasing SAM Zero-Shot Performance on Multimodal Medical Images Using GPT-4 Generated Descriptive Prompts Without Human Annotation (Zekun Jiang et al., 2024)</a></li><li><a href=#422--25117-bridging-the-gap-between-2d-and-3d-visual-question-answering-a-fusion-approach-for-3d-vqa-wentao-mo-et-al-2024>(4/22 | 25/117) Bridging the Gap between 2D and 3D Visual Question Answering: A Fusion Approach for 3D VQA (Wentao Mo et al., 2024)</a></li><li><a href=#522--26117-navid-video-based-vlm-plans-the-next-step-for-vision-and-language-navigation-jiazhao-zhang-et-al-2024>(5/22 | 26/117) NaVid: Video-based VLM Plans the Next Step for Vision-and-Language Navigation (Jiazhao Zhang et al., 2024)</a></li><li><a href=#622--27117-multimodal-instruction-tuning-with-conditional-mixture-of-lora-ying-shen-et-al-2024>(6/22 | 27/117) Multimodal Instruction Tuning with Conditional Mixture of LoRA (Ying Shen et al., 2024)</a></li><li><a href=#722--28117-parameter-efficient-prompt-learning-for-3d-point-cloud-understanding-hongyu-sun-et-al-2024>(7/22 | 28/117) Parameter-efficient Prompt Learning for 3D Point Cloud Understanding (Hongyu Sun et al., 2024)</a></li><li><a href=#822--29117-clipose-category-level-object-pose-estimation-with-pre-trained-vision-language-knowledge-xiao-lin-et-al-2024>(8/22 | 29/117) CLIPose: Category-Level Object Pose Estimation with Pre-trained Vision-Language Knowledge (Xiao Lin et al., 2024)</a></li><li><a href=#922--30117-irconstyle-image-restoration-framework-using-contrastive-learning-and-style-transfer-dongqi-fan-et-al-2024>(9/22 | 30/117) IRConStyle: Image Restoration Framework Using Contrastive Learning and Style Transfer (Dongqi Fan et al., 2024)</a></li><li><a href=#1022--31117-sequential-visual-and-semantic-consistency-for-semi-supervised-text-recognition-mingkun-yang-et-al-2024>(10/22 | 31/117) Sequential Visual and Semantic Consistency for Semi-supervised Text Recognition (Mingkun Yang et al., 2024)</a></li><li><a href=#1122--32117-gimefive-towards-interpretable-facial-emotion-classification-jiawen-wang-et-al-2024>(11/22 | 32/117) GiMeFive: Towards Interpretable Facial Emotion Classification (Jiawen Wang et al., 2024)</a></li><li><a href=#1222--33117-rauca-a-novel-physical-adversarial-attack-on-vehicle-detectors-via-robust-and-accurate-camouflage-generation-jiawei-zhou-et-al-2024>(12/22 | 33/117) RAUCA: A Novel Physical Adversarial Attack on Vehicle Detectors via Robust and Accurate Camouflage Generation (Jiawei Zhou et al., 2024)</a></li><li><a href=#1322--34117-multiple-instance-learning-for-glioma-diagnosis-using-hematoxylin-and-eosin-whole-slide-images-an-indian-cohort-study-ekansh-chauhan-et-al-2024>(13/22 | 34/117) Multiple Instance Learning for Glioma Diagnosis using Hematoxylin and Eosin Whole Slide Images: An Indian cohort Study (Ekansh Chauhan et al., 2024)</a></li><li><a href=#1422--35117-enhanced-droplet-analysis-using-generative-adversarial-networks-tan-hanh-pham-et-al-2024>(14/22 | 35/117) Enhanced Droplet Analysis Using Generative Adversarial Networks (Tan-Hanh Pham et al., 2024)</a></li><li><a href=#1522--36117-hir-diff-unsupervised-hyperspectral-image-restoration-via-improved-diffusion-models-li-pang-et-al-2024>(15/22 | 36/117) HIR-Diff: Unsupervised Hyperspectral Image Restoration Via Improved Diffusion Models (Li Pang et al., 2024)</a></li><li><a href=#1622--37117-dart-depth-enhanced-accurate-and-real-time-background-matting-hanxi-li-et-al-2024>(16/22 | 37/117) DART: Depth-Enhanced Accurate and Real-Time Background Matting (Hanxi Li et al., 2024)</a></li><li><a href=#1722--38117-intelligent-director-an-automatic-framework-for-dynamic-visual-composition-using-chatgpt-sixiao-zheng-et-al-2024>(17/22 | 38/117) Intelligent Director: An Automatic Framework for Dynamic Visual Composition using ChatGPT (Sixiao Zheng et al., 2024)</a></li><li><a href=#1822--39117-general-purpose-image-encoder-dinov2-for-medical-image-registration-xinrui-song-et-al-2024>(18/22 | 39/117) General Purpose Image Encoder DINOv2 for Medical Image Registration (Xinrui Song et al., 2024)</a></li><li><a href=#1922--40117-fedmm-federated-multi-modal-learning-with-modality-heterogeneity-in-computational-pathology-yuanzhe-peng-et-al-2024>(19/22 | 40/117) FedMM: Federated Multi-Modal Learning with Modality Heterogeneity in Computational Pathology (Yuanzhe Peng et al., 2024)</a></li><li><a href=#2022--41117-multi-object-tracking-by-hierarchical-visual-representations-jinkun-cao-et-al-2024>(20/22 | 41/117) Multi-Object Tracking by Hierarchical Visual Representations (Jinkun Cao et al., 2024)</a></li><li><a href=#2122--42117-multi-graph-graph-matching-for-coronary-artery-semantic-labeling-chen-zhao-et-al-2024>(21/22 | 42/117) Multi-graph Graph Matching for Coronary Artery Semantic Labeling (Chen Zhao et al., 2024)</a></li><li><a href=#2222--43117-deeplight-reconstructing-high-resolution-observations-of-nighttime-light-with-multi-modal-remote-sensing-data-lixian-zhang-et-al-2024>(22/22 | 43/117) DeepLight: Reconstructing High-Resolution Observations of Nighttime Light With Multi-Modal Remote Sensing Data (Lixian Zhang et al., 2024)</a></li></ul></li><li><a href=#cslg-26>cs.LG (26)</a><ul><li><a href=#126--44117-predicting-outcomes-in-video-games-with-long-short-term-memory-networks-kittimate-chulajata-et-al-2024>(1/26 | 44/117) Predicting Outcomes in Video Games with Long Short Term Memory Networks (Kittimate Chulajata et al., 2024)</a></li><li><a href=#226--45117-data-efficient-operator-learning-via-unsupervised-pretraining-and-in-context-learning-wuyang-chen-et-al-2024>(2/26 | 45/117) Data-Efficient Operator Learning via Unsupervised Pretraining and In-Context Learning (Wuyang Chen et al., 2024)</a></li><li><a href=#326--46117-overcoming-pitfalls-in-graph-contrastive-learning-evaluation-toward-comprehensive-benchmarks-qian-ma-et-al-2024>(3/26 | 46/117) Overcoming Pitfalls in Graph Contrastive Learning Evaluation: Toward Comprehensive Benchmarks (Qian Ma et al., 2024)</a></li><li><a href=#426--47117-prolora-partial-rotation-empowers-more-parameter-efficient-lora-sheng-wang-et-al-2024>(4/26 | 47/117) PRoLoRA: Partial Rotation Empowers More Parameter-Efficient LoRA (Sheng Wang et al., 2024)</a></li><li><a href=#526--48117-sparse-mezo-less-parameters-for-better-performance-in-zeroth-order-llm-fine-tuning-yong-liu-et-al-2024>(5/26 | 48/117) Sparse MeZO: Less Parameters for Better Performance in Zeroth-Order LLM Fine-Tuning (Yong Liu et al., 2024)</a></li><li><a href=#626--49117-pretraining-strategy-for-neural-potentials-zehua-zhang-et-al-2024>(6/26 | 49/117) Pretraining Strategy for Neural Potentials (Zehua Zhang et al., 2024)</a></li><li><a href=#726--50117-large-stepsize-gradient-descent-for-logistic-loss-non-monotonicity-of-the-loss-improves-optimization-efficiency-jingfeng-wu-et-al-2024>(7/26 | 50/117) Large Stepsize Gradient Descent for Logistic Loss: Non-Monotonicity of the Loss Improves Optimization Efficiency (Jingfeng Wu et al., 2024)</a></li><li><a href=#826--51117-esfl-efficient-split-federated-learning-over-resource-constrained-heterogeneous-wireless-devices-guangyu-zhu-et-al-2024>(8/26 | 51/117) ESFL: Efficient Split Federated Learning over Resource-Constrained Heterogeneous Wireless Devices (Guangyu Zhu et al., 2024)</a></li><li><a href=#926--52117-a-generative-machine-learning-model-for-material-microstructure-3d-reconstruction-and-performance-evaluation-yilin-zheng-et-al-2024>(9/26 | 52/117) A Generative Machine Learning Model for Material Microstructure 3D Reconstruction and Performance Evaluation (Yilin Zheng et al., 2024)</a></li><li><a href=#1026--53117-batch-active-learning-of-reward-functions-from-human-preferences-erdem-bıyık-et-al-2024>(10/26 | 53/117) Batch Active Learning of Reward Functions from Human Preferences (Erdem Bıyık et al., 2024)</a></li><li><a href=#1126--54117-is-offline-decision-making-possible-with-only-few-samples-reliable-decisions-in-data-starved-bandits-via-trust-region-enhancement-ruiqi-zhang-et-al-2024>(11/26 | 54/117) Is Offline Decision Making Possible with Only Few Samples? Reliable Decisions in Data-Starved Bandits via Trust Region Enhancement (Ruiqi Zhang et al., 2024)</a></li><li><a href=#1226--55117-clustering-in-dynamic-environments-a-framework-for-benchmark-dataset-generation-with-heterogeneous-changes-danial-yazdani-et-al-2024>(12/26 | 55/117) Clustering in Dynamic Environments: A Framework for Benchmark Dataset Generation With Heterogeneous Changes (Danial Yazdani et al., 2024)</a></li><li><a href=#1326--56117-reward-design-for-justifiable-sequential-decision-making-aleksa-sukovic-et-al-2024>(13/26 | 56/117) Reward Design for Justifiable Sequential Decision-Making (Aleksa Sukovic et al., 2024)</a></li><li><a href=#1426--57117-optimal-zero-shot-detector-for-multi-armed-attacks-federica-granese-et-al-2024>(14/26 | 57/117) Optimal Zero-Shot Detector for Multi-Armed Attacks (Federica Granese et al., 2024)</a></li><li><a href=#1526--58117-truly-no-regret-learning-in-constrained-mdps-adrian-müller-et-al-2024>(15/26 | 58/117) Truly No-Regret Learning in Constrained MDPs (Adrian Müller et al., 2024)</a></li><li><a href=#1626--59117-a-prior-estimates-for-deep-residual-network-in-continuous-time-reinforcement-learning-shuyu-yin-et-al-2024>(16/26 | 59/117) A prior Estimates for Deep Residual Network in Continuous-time Reinforcement Learning (Shuyu Yin et al., 2024)</a></li><li><a href=#1726--60117-a-statistical-analysis-of-wasserstein-autoencoders-for-intrinsically-low-dimensional-data-saptarshi-chakraborty-et-al-2024>(17/26 | 60/117) A Statistical Analysis of Wasserstein Autoencoders for Intrinsically Low-dimensional Data (Saptarshi Chakraborty et al., 2024)</a></li><li><a href=#1826--61117-scalable-volt-var-optimization-using-rllib-impala-framework-a-reinforcement-learning-approach-alaa-selim-et-al-2024>(18/26 | 61/117) Scalable Volt-VAR Optimization using RLlib-IMPALA Framework: A Reinforcement Learning Approach (Alaa Selim et al., 2024)</a></li><li><a href=#1926--62117-a-novel-data-generation-scheme-for-surrogate-modelling-with-deep-operator-networks-shivam-choubey-et-al-2024>(19/26 | 62/117) A novel data generation scheme for surrogate modelling with deep operator networks (Shivam Choubey et al., 2024)</a></li><li><a href=#2026--63117-low-rank-bandits-via-tight-two-to-infinity-singular-subspace-recovery-yassir-jedra-et-al-2024>(20/26 | 63/117) Low-Rank Bandits via Tight Two-to-Infinity Singular Subspace Recovery (Yassir Jedra et al., 2024)</a></li><li><a href=#2126--64117-operator-learning-algorithms-and-analysis-nikola-b-kovachki-et-al-2024>(21/26 | 64/117) Operator Learning: Algorithms and Analysis (Nikola B. Kovachki et al., 2024)</a></li><li><a href=#2226--65117-orthogonal-gradient-boosting-for-simpler-additive-rule-ensembles-fan-yang-et-al-2024>(22/26 | 65/117) Orthogonal Gradient Boosting for Simpler Additive Rule Ensembles (Fan Yang et al., 2024)</a></li><li><a href=#2326--66117-universal-model-in-online-customer-service-shu-ting-pi-et-al-2024>(23/26 | 66/117) Universal Model in Online Customer Service (Shu-Ting Pi et al., 2024)</a></li><li><a href=#2426--67117-anchor-free-clustering-based-on-anchor-graph-factorization-shikun-mei-et-al-2024>(24/26 | 67/117) Anchor-free Clustering based on Anchor Graph Factorization (Shikun Mei et al., 2024)</a></li><li><a href=#2526--68117-field-based-molecule-generation-alexandru-dumitrescu-et-al-2024>(25/26 | 68/117) Field-based Molecule Generation (Alexandru Dumitrescu et al., 2024)</a></li><li><a href=#2626--69117-scalable-density-based-clustering-with-random-projections-haochuan-xu-et-al-2024>(26/26 | 69/117) Scalable Density-based Clustering with Random Projections (Haochuan Xu et al., 2024)</a></li></ul></li><li><a href=#cscr-5>cs.CR (5)</a><ul><li><a href=#15--70117-prp-propagating-universal-perturbations-to-attack-large-language-model-guard-rails-neal-mangaokar-et-al-2024>(1/5 | 70/117) PRP: Propagating Universal Perturbations to Attack Large Language Model Guard-Rails (Neal Mangaokar et al., 2024)</a></li><li><a href=#25--71117-llms-can-defend-themselves-against-jailbreaking-in-a-practical-manner-a-vision-paper-daoyuan-wu-et-al-2024>(2/5 | 71/117) LLMs Can Defend Themselves Against Jailbreaking in a Practical Manner: A Vision Paper (Daoyuan Wu et al., 2024)</a></li><li><a href=#35--72117-gait-based-privacy-protection-for-smart-wearable-devices-yu-su-et-al-2024>(3/5 | 72/117) Gait-Based Privacy Protection for Smart Wearable Devices (Yu Su et al., 2024)</a></li><li><a href=#45--73117-privacy-preserving-state-estimation-in-the-presence-of-eavesdroppers-a-survey-xinhao-yan-et-al-2024>(4/5 | 73/117) Privacy-Preserving State Estimation in the Presence of Eavesdroppers: A Survey (Xinhao Yan et al., 2024)</a></li><li><a href=#55--74117-cryptanalysis-and-improvement-of-multimodal-data-encryption-by-machine-learning-based-system-zakaria-tolba-2024>(5/5 | 74/117) Cryptanalysis and improvement of multimodal data encryption by machine-learning-based system (Zakaria Tolba, 2024)</a></li></ul></li><li><a href=#csai-6>cs.AI (6)</a><ul><li><a href=#16--75117-how-do-humans-write-code-large-models-do-it-the-same-way-too-long-li-2024>(1/6 | 75/117) How Do Humans Write Code? Large Models Do It the Same Way Too (Long Li, 2024)</a></li><li><a href=#26--76117-quacer-c-quantitative-certification-of-knowledge-comprehension-in-llms-isha-chaudhary-et-al-2024>(2/6 | 76/117) QuaCer-C: Quantitative Certification of Knowledge Comprehension in LLMs (Isha Chaudhary et al., 2024)</a></li><li><a href=#36--77117-stepwise-self-consistent-mathematical-reasoning-with-large-language-models-zilong-zhao-et-al-2024>(3/6 | 77/117) Stepwise Self-Consistent Mathematical Reasoning with Large Language Models (Zilong Zhao et al., 2024)</a></li><li><a href=#46--78117-enforcing-temporal-constraints-on-generative-agent-behavior-with-reactive-synthesis-raven-rothkopf-et-al-2024>(4/6 | 78/117) Enforcing Temporal Constraints on Generative Agent Behavior with Reactive Synthesis (Raven Rothkopf et al., 2024)</a></li><li><a href=#56--79117-hal-eval-a-universal-and-fine-grained-hallucination-evaluation-framework-for-large-vision-language-models-chaoya-jiang-et-al-2024>(5/6 | 79/117) Hal-Eval: A Universal and Fine-grained Hallucination Evaluation Framework for Large Vision Language Models (Chaoya Jiang et al., 2024)</a></li><li><a href=#66--80117-empowering-large-language-model-agents-through-action-learning-haiteng-zhao-et-al-2024>(6/6 | 80/117) Empowering Large Language Model Agents through Action Learning (Haiteng Zhao et al., 2024)</a></li></ul></li><li><a href=#csdc-1>cs.DC (1)</a><ul><li><a href=#11--81117-minions-accelerating-large-language-model-inference-with-adaptive-and-collective-speculative-decoding-siqi-wang-et-al-2024>(1/1 | 81/117) Minions: Accelerating Large Language Model Inference with Adaptive and Collective Speculative Decoding (Siqi Wang et al., 2024)</a></li></ul></li><li><a href=#quant-ph-1>quant-ph (1)</a><ul><li><a href=#11--82117-lower-bounds-for-quantum-inspired-classical-algorithms-via-communication-complexity-nikhil-s-mande-et-al-2024>(1/1 | 82/117) Lower bounds for quantum-inspired classical algorithms via communication complexity (Nikhil S. Mande et al., 2024)</a></li></ul></li><li><a href=#cssd-1>cs.SD (1)</a><ul><li><a href=#11--83117-bytecomposer-a-human-like-melody-composition-method-based-on-language-model-agent-xia-liang-et-al-2024>(1/1 | 83/117) ByteComposer: a Human-like Melody Composition Method based on Language Model Agent (Xia Liang et al., 2024)</a></li></ul></li><li><a href=#csro-2>cs.RO (2)</a><ul><li><a href=#12--84117-discretionary-lane-change-decision-and-control-via-parameterized-soft-actor-critic-for-hybrid-action-space-yuan-lin-et-al-2024>(1/2 | 84/117) Discretionary Lane-Change Decision and Control via Parameterized Soft Actor-Critic for Hybrid Action Space (Yuan Lin et al., 2024)</a></li><li><a href=#22--85117-phyplan-compositional-and-adaptive-physical-task-reasoning-with-physics-informed-skill-networks-for-robot-manipulators-harshil-vagadia-et-al-2024>(2/2 | 85/117) PhyPlan: Compositional and Adaptive Physical Task Reasoning with Physics-Informed Skill Networks for Robot Manipulators (Harshil Vagadia et al., 2024)</a></li></ul></li><li><a href=#eessiv-2>eess.IV (2)</a><ul><li><a href=#12--86117-a-heterogeneous-dynamic-convolutional-neural-network-for-image-super-resolution-chunwei-tian-et-al-2024>(1/2 | 86/117) A Heterogeneous Dynamic Convolutional Neural Network for Image Super-resolution (Chunwei Tian et al., 2024)</a></li><li><a href=#22--87117-sandwich-gan-image-reconstruction-from-phase-mask-based-anti-dazzle-imaging-xiaopeng-peng-et-al-2024>(2/2 | 87/117) Sandwich GAN: Image Reconstruction from Phase Mask based Anti-dazzle Imaging (Xiaopeng Peng et al., 2024)</a></li></ul></li><li><a href=#csdl-1>cs.DL (1)</a><ul><li><a href=#11--88117-oag-bench-a-human-curated-benchmark-for-academic-graph-mining-fanjin-zhang-et-al-2024>(1/1 | 88/117) OAG-Bench: A Human-Curated Benchmark for Academic Graph Mining (Fanjin Zhang et al., 2024)</a></li></ul></li><li><a href=#csds-2>cs.DS (2)</a><ul><li><a href=#12--89117-pruned-pivot-correlation-clustering-algorithm-for-dynamic-parallel-and-local-computation-models-mina-dalirrooyfard-et-al-2024>(1/2 | 89/117) Pruned Pivot: Correlation Clustering Algorithm for Dynamic, Parallel, and Local Computation Models (Mina Dalirrooyfard et al., 2024)</a></li><li><a href=#22--90117-tree-decompositions-meet-induced-matchings-beyond-max-weight-independent-set-paloma-t-lima-et-al-2024>(2/2 | 90/117) Tree decompositions meet induced matchings: beyond Max Weight Independent Set (Paloma T. Lima et al., 2024)</a></li></ul></li><li><a href=#cslo-1>cs.LO (1)</a><ul><li><a href=#11--91117-regular-resolution-effectively-simulates-resolution-sam-buss-et-al-2024>(1/1 | 91/117) Regular resolution effectively simulates resolution (Sam Buss et al., 2024)</a></li></ul></li><li><a href=#csir-2>cs.IR (2)</a><ul><li><a href=#12--92117-listt5-listwise-reranking-with-fusion-in-decoder-improves-zero-shot-retrieval-soyoung-yoon-et-al-2024>(1/2 | 92/117) ListT5: Listwise Reranking with Fusion-in-Decoder Improves Zero-shot Retrieval (Soyoung Yoon et al., 2024)</a></li><li><a href=#22--93117-debiased-model-based-interactive-recommendation-zijian-li-et-al-2024>(2/2 | 93/117) Debiased Model-based Interactive Recommendation (Zijian Li et al., 2024)</a></li></ul></li><li><a href=#csni-2>cs.NI (2)</a><ul><li><a href=#12--94117-dedicated-restricted-target-wake-time-for-real-time-applications-in-wi-fi-7-andrey-belogaev-et-al-2024>(1/2 | 94/117) Dedicated Restricted Target Wake Time for Real-Time Applications in Wi-Fi 7 (Andrey Belogaev et al., 2024)</a></li><li><a href=#22--95117-balancedn-load-balancing-allocation-of-interest-for-fast-discovery-in-content-centric-networks-murali-gunti-et-al-2024>(2/2 | 95/117) BalanceDN: Load-Balancing Allocation of Interest for Fast Discovery in Content Centric Networks (Murali Gunti et al., 2024)</a></li></ul></li><li><a href=#csgt-1>cs.GT (1)</a><ul><li><a href=#11--96117-differentially-private-bayesian-persuasion-yuqi-pan-et-al-2024>(1/1 | 96/117) Differentially Private Bayesian Persuasion (Yuqi Pan et al., 2024)</a></li></ul></li><li><a href=#cscy-1>cs.CY (1)</a><ul><li><a href=#11--97117-from-cobit-to-iso-42001-evaluating-cybersecurity-frameworks-for-opportunities-risks-and-regulatory-compliance-in-commercializing-large-language-models-timothy-r-mcintosh-et-al-2024>(1/1 | 97/117) From COBIT to ISO 42001: Evaluating Cybersecurity Frameworks for Opportunities, Risks, and Regulatory Compliance in Commercializing Large Language Models (Timothy R. McIntosh et al., 2024)</a></li></ul></li><li><a href=#cshc-3>cs.HC (3)</a><ul><li><a href=#13--98117-areeg_chars-dataset-for-envisioned-speech-recognition-using-eeg-for-arabic-characters-hazem-darwish-et-al-2024>(1/3 | 98/117) ArEEG_Chars: Dataset for Envisioned Speech Recognition using EEG for Arabic Characters (Hazem Darwish et al., 2024)</a></li><li><a href=#23--99117-applied-user-research-in-virtual-reality-tools-methods-and-challenges-leonie-bensch-et-al-2024>(2/3 | 99/117) Applied User Research in Virtual Reality: Tools, Methods, and Challenges (Leonie Bensch et al., 2024)</a></li><li><a href=#33--100117-touching-the-moon-leveraging-passive-haptics-embodiment-and-presence-for-operational-assessments-in-virtual-reality-florian-dufresne-et-al-2024>(3/3 | 100/117) Touching the Moon: Leveraging Passive Haptics, Embodiment and Presence for Operational Assessments in Virtual Reality (Florian Dufresne et al., 2024)</a></li></ul></li><li><a href=#eesssy-4>eess.SY (4)</a><ul><li><a href=#14--101117-design-and-implementation-of-low-cost-electric-vehicles-evs-supercharger-a-comprehensive-review-md-khaledur-rahman-et-al-2024>(1/4 | 101/117) Design and Implementation of Low-Cost Electric Vehicles (Evs) Supercharger: A Comprehensive Review (Md Khaledur Rahman et al., 2024)</a></li><li><a href=#24--102117-consensus-seeking-in-diffusive-multidimensional-networks-with-a-repeated-interaction-pattern-and-time-delays-hoang-huy-vu-et-al-2024>(2/4 | 102/117) Consensus seeking in diffusive multidimensional networks with a repeated interaction pattern and time-delays (Hoang Huy Vu et al., 2024)</a></li><li><a href=#34--103117-concurrent-learning-of-policy-and-unknown-safety-constraints-in-reinforcement-learning-lunet-yifru-et-al-2024>(3/4 | 103/117) Concurrent Learning of Policy and Unknown Safety Constraints in Reinforcement Learning (Lunet Yifru et al., 2024)</a></li><li><a href=#44--104117-analysis-of-off-policy-multi-step-td-learning-with-linear-function-approximation-donghwan-lee-2024>(4/4 | 104/117) Analysis of Off-Policy Multi-Step TD-Learning with Linear Function Approximation (Donghwan Lee, 2024)</a></li></ul></li><li><a href=#statml-1>stat.ML (1)</a><ul><li><a href=#11--105117-a-duality-analysis-of-kernel-ridge-regression-in-the-noiseless-regime-jihao-long-et-al-2024>(1/1 | 105/117) A Duality Analysis of Kernel Ridge Regression in the Noiseless Regime (Jihao Long et al., 2024)</a></li></ul></li><li><a href=#eesssp-1>eess.SP (1)</a><ul><li><a href=#11--106117-multimodal-sleep-apnea-detection-with-missing-or-noisy-modalities-hamed-fayyaz-et-al-2024>(1/1 | 106/117) Multimodal Sleep Apnea Detection with Missing or Noisy Modalities (Hamed Fayyaz et al., 2024)</a></li></ul></li><li><a href=#csse-2>cs.SE (2)</a><ul><li><a href=#12--107117-advancing-bdd-software-testing-dynamic-scenario-re-usability-and-step-auto-complete-for-cucumber-framework-a-h-mughal-2024>(1/2 | 107/117) Advancing BDD Software Testing: Dynamic Scenario Re-Usability And Step Auto-Complete For Cucumber Framework (A. H. Mughal, 2024)</a></li><li><a href=#22--108117-importance-guided-data-augmentation-for-neural-based-code-understanding-zeming-dong-et-al-2024>(2/2 | 108/117) Importance Guided Data Augmentation for Neural-Based Code Understanding (Zeming Dong et al., 2024)</a></li></ul></li><li><a href=#econth-1>econ.TH (1)</a><ul><li><a href=#11--109117-optimal-budget-aggregation-with-single-peaked-preferences-felix-brandt-et-al-2024>(1/1 | 109/117) Optimal Budget Aggregation with Single-Peaked Preferences (Felix Brandt et al., 2024)</a></li></ul></li><li><a href=#q-biogn-1>q-bio.GN (1)</a><ul><li><a href=#11--110117-fgbert-function-driven-pre-trained-gene-language-model-for-metagenomics-chenrui-duan-et-al-2024>(1/1 | 110/117) FGBERT: Function-Driven Pre-trained Gene Language Model for Metagenomics (ChenRui Duan et al., 2024)</a></li></ul></li><li><a href=#cssi-1>cs.SI (1)</a><ul><li><a href=#11--111117-mim-reasoner-learning-with-theoretical-guarantees-for-multiplex-influence-maximization-nguyen-do-et-al-2024>(1/1 | 111/117) MIM-Reasoner: Learning with Theoretical Guarantees for Multiplex Influence Maximization (Nguyen Do et al., 2024)</a></li></ul></li><li><a href=#cspl-1>cs.PL (1)</a><ul><li><a href=#11--112117-formally-verified-c-code-generation-from-hybrid-communicating-sequential-processes-shuling-wang-et-al-2024>(1/1 | 112/117) Formally Verified C Code Generation from Hybrid Communicating Sequential Processes (Shuling Wang et al., 2024)</a></li></ul></li><li><a href=#cscg-1>cs.CG (1)</a><ul><li><a href=#11--113117-an-on-log-n-time-approximation-scheme-for-geometric-many-to-many-matching-sayan-bandyapadhyay-et-al-2024>(1/1 | 113/117) An $O(n \log n)$-Time Approximation Scheme for Geometric Many-to-Many Matching (Sayan Bandyapadhyay et al., 2024)</a></li></ul></li><li><a href=#mathna-1>math.NA (1)</a><ul><li><a href=#11--114117-interpolation-based-immersogeometric-analysis-methods-for-multi-material-and-multi-physics-problems-jennifer-e-fromm-et-al-2024>(1/1 | 114/117) Interpolation-based immersogeometric analysis methods for multi-material and multi-physics problems (Jennifer E. Fromm et al., 2024)</a></li></ul></li><li><a href=#csit-1>cs.IT (1)</a><ul><li><a href=#11--115117-study-of-noncoherent-sparse-subarrays-for-direction-finding-based-on-low-rank-and-sparse-recovery-w-leite-et-al-2024>(1/1 | 115/117) Study of Noncoherent Sparse Subarrays for Direction Finding Based on Low-Rank and Sparse Recovery (W. Leite et al., 2024)</a></li></ul></li><li><a href=#mathco-1>math.CO (1)</a><ul><li><a href=#11--116117-on-the-finiteness-of-k-vertex-critical-2p_2-free-graphs-with-forbidden-induced-squids-or-bulls-melvin-adekanye-et-al-2024>(1/1 | 116/117) On the finiteness of $k$-vertex-critical $2P_2$-free graphs with forbidden induced squids or bulls (Melvin Adekanye et al., 2024)</a></li></ul></li><li><a href=#csdm-1>cs.DM (1)</a><ul><li><a href=#11--117117-symbolic-listings-as-computation-hamilton-sawczuk-et-al-2024>(1/1 | 117/117) Symbolic Listings as Computation (Hamilton Sawczuk et al., 2024)</a></li></ul></li></ul></nav></div></div></section></div><footer id=footer class="container-fluid text-center align-content-center footer pb-2"><div class="container pt-5"><div class="row text-left"><div class="col-md-4 col-sm-12"><h5>Navigation</h5><ul><li class=nav-item><a class=smooth-scroll href=https://akitenkrad.github.io/akitenkrad-blog/#about>About</a></li><li class=nav-item><a class=smooth-scroll href=https://akitenkrad.github.io/akitenkrad-blog/#recent-posts>Recent Posts</a></li></ul></div><div class="col-md-4 col-sm-12"><h5>Contact me:</h5><ul><li><a href=mailto:contact.to.akitenkrad@gmail.com target=_blank rel=noopener><span><i class="fas fa-envelope"></i></span> <span>contact.to.akitenkrad@gmail.com</span></a></li></ul></div></div></div><hr><div class=container><div class="row text-left"><div class=col-md-4><a id=theme href=https://github.com/hossainemruz/toha target=_blank rel=noopener><img src=/akitenkrad-blog/images/theme-logo_hu8376fd15465fef26ffe66b6bcf0ca686_13669_32x0_resize_box_3.png alt="Toha Theme Logo">
Toha</a></div><div class="col-md-4 text-center">© 2020 Akitenkrad.</div><div class="col-md-4 text-right"><a id=hugo href=https://gohugo.io/ target=_blank rel=noopener>Powered by
<img src=/akitenkrad-blog/images/hugo-logo.svg alt="Hugo Logo" height=18></a></div></div></div></footer><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/jquery-3.4.1.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/popper.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/bootstrap.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/navbar.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/plyr.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/main.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js></script><script src=https://akitenkrad.github.io/akitenkrad-blog/js/single.js></script><script>hljs.initHighlightingOnLoad()</script><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/katex/katex.min.css><script type=text/javascript defer src=https://akitenkrad.github.io/akitenkrad-blog/katex/katex.min.js></script><script type=text/javascript defer src=https://akitenkrad.github.io/akitenkrad-blog/katex/auto-render.min.js></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\[",right:"\\]",display:!0},{left:"\\(",right:"\\)",display:!1}],throwOnError:!0})})</script></body></html>