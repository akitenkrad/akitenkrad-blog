<!doctype html><html><head><title>arXiv @ 2024.02.09</title>
<meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/bootstrap.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/layouts/main.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/navigators/navbar.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/plyr.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/flag-icon.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/custom/style.css><link href=https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css rel=stylesheet integrity=sha384-9ndCyUaIbzAi2FUVXJi0CjmCapSmO7SnpJef0486qhLnuZ2cdeRhO02iuK6FUUVM crossorigin=anonymous><script async src=https://cdn.jsdelivr.net/npm/es-module-shims@1/dist/es-module-shims.min.js crossorigin=anonymous></script><script type=importmap>
{
  "imports": {
    "@popperjs/core": "https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.8/dist/esm/popper.min.js",
    "bootstrap": "https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.esm.min.js"
  }
}
</script><script src=https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js integrity=sha384-geWF76RCwLtnZ8qwWowPQNguL3RmwHVBC9FhGdlKrxdiJJigb/j/68SIy3Te4Bkz crossorigin=anonymous></script><script type=module src=https://akitenkrad.github.io/akitenkrad-blog/js/custom/custom.js></script><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Muli:wght@300;400;500;600"><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/fontawesome/css/all.min.css><link rel=icon type=image/png href=https://akitenkrad.github.io/akitenkrad-blog/images/favicons/favicon-96x96_huf1ee13f0caf27d1547f91fb46207d708_13005_42x0_resize_box_3.png><meta property="og:title" content="arXiv @ 2024.02.09"><meta property="og:description" content="Primary Categories cond-mat.mtrl-sci (1) cs.AI (10) cs.CL (29) cs.CR (4) cs.CV (38) cs.DC (2) cs.DL (1) cs.FL (1) cs.GR (1) cs.GT (1) cs.HC (6) cs.IR (7) cs.IT (4) cs.LG (65) cs.MA (2) cs.NE (2) cs.NI (3) cs.RO (13) cs.SD (2) cs.SE (8) cs.SI (1) eess.IV (7) eess.SY (5) math.FA (1) math.NA (2) math.OC (3) math.ST (1) physics.chem-ph (1) physics.flu-dyn (2) q-bio.BM (2) quant-ph (2) stat.ME (2) stat.ML (7) Keywords keyword cs."><meta property="og:type" content="article"><meta property="og:url" content="https://akitenkrad.github.io/akitenkrad-blog/posts/arxiv/202402/20240209000000/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-02-09T00:00:00+00:00"><meta property="article:modified_time" content="2024-02-09T00:00:00+00:00"><meta name=description content="arXiv @ 2024.02.09"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/atom-one-dark.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/layouts/single.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/navigators/sidebar.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/style.css><script type=text/javascript src=//ajax.googleapis.com/ajax/libs/jquery/3.1.1/jquery.min.js></script><script language=JavaScript>$(document).ready(function(){$("a[href^='http']:not([href*='"+location.hostname+"'])").attr("target","_blank")})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-1MYYZQG0WE"></script><script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-1MYYZQG0WE",{anonymize_ip:!1})}</script></head><body data-spy=scroll data-target=#TableOfContents data-offset=80><div class="container-fluid bg-dimmed wrapper"><nav class="navbar navbar-expand-xl top-navbar final-navbar shadow" style=position:fixed><div class=container><button class="navbar-toggler navbar-light navbar-for-sidebar" id=sidebar-toggler type=button onclick=toggleSidebar()>
<span class=navbar-toggler-icon></span>
</button>
<button class="navbar-toggler navbar-light" id=toc-toggler type=button onclick=toggleTOC()>
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse lang-selector" id=top-nav-items><ul class="navbar-nav mr-auto"><li class=nav-item><a class=navbar-brand href=/akitenkrad-blog><img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png alt=Logo>
Akitenkrad's Blog</a></li></ul><ul class="navbar-nav ml-auto"><li class=nav-item><a class=nav-link href=/akitenkrad-blog#home>Home</a></li><li class=nav-item><a class=nav-link href=/akitenkrad-blog#about>About</a></li><li class=nav-item><a class=nav-link href=/akitenkrad-blog#recent-posts>Recent Posts</a></li><li class=nav-item><a class=nav-link id=blog-link href=https://akitenkrad.github.io/akitenkrad-blog/posts>Posts</a></li><li class=nav-item><a class=nav-link id=tag-link href=https://akitenkrad.github.io/akitenkrad-blog/tags>Tags</a></li></ul></div></div><img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png class=d-none id=main-logo alt=Logo>
<img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png class=d-none id=inverted-logo alt="Inverted Logo"></nav><section class=sidebar-section id=sidebar-section><div class=sidebar-holder><div class=sidebar id=sidebar><form class=mx-auto method=get action=/akitenkrad-blog/search><input type=text name=keyword placeholder=Search data-search id=search-box></form><div class=sidebar-tree><ul class=tree id=tree><li id=list-heading><a href=/posts data-filter=all>Posts</a></li><div class=subtree><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/>Papers</a><ul><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202205/>2022.05</a><ul><li><a href=/akitenkrad-blog/posts/papers/202205/20220518224923/ title="A Context-Aware Citation Recommendation Model with BERT and Graph Convolutional Networks">A Context-Aware Citation Recommendation Model with BERT and Graph Convolutional Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220508162318/ title="A Deep Cascade Model for Multi-Document Reading Comprehension">A Deep Cascade Model for Multi-Document Reading Comprehension</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220506021208/ title="A Primer in BERTology: What We Know About How BERT Works">A Primer in BERTology: What We Know About How BERT Works</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220529131339/ title="Attention Is All You Need">Attention Is All You Need</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220503010000/ title="DeBERTa: Decoding-Enhanced BERT with Disentangled Attention">DeBERTa: Decoding-Enhanced BERT with Disentangled Attention</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220505222900/ title="Dense Passage Retrieval for Open-Domain Question Answering">Dense Passage Retrieval for Open-Domain Question Answering</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220511010217/ title="Multi-Style Generative Reading Comprehension">Multi-Style Generative Reading Comprehension</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220530102936/ title="Neural Machine Translation of Rare Words with Subword Units">Neural Machine Translation of Rare Words with Subword Units</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220523223206/ title="RoBERTa: A Robustly Optimized BERT Pretraining Approach">RoBERTa: A Robustly Optimized BERT Pretraining Approach</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220520124748/ title="Semi-Supervised Classification with Graph Convolutional Networks">Semi-Supervised Classification with Graph Convolutional Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220509110738/ title="Survey on graph embeddings and their applications to machine learning problems on graphs">Survey on graph embeddings and their applications to machine learning problems on graphs</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220514151839/ title="UnitedQA: A Hybrid Approach for Open Domain Question Answering">UnitedQA: A Hybrid Approach for Open Domain Question Answering</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202206/>2022.06</a><ul><li><a href=/akitenkrad-blog/posts/papers/202206/20220612105422/ title="Attributed Network Embedding for Learning in a Dynamic Environment">Attributed Network Embedding for Learning in a Dynamic Environment</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220608085622/ title="CodeBERT: A Pre-Trained Model for Programming and Natural Languages">CodeBERT: A Pre-Trained Model for Programming and Natural Languages</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220618223844/ title="High-order Proximity Preserved Embedding for Dynamic Networks">High-order Proximity Preserved Embedding for Dynamic Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220602171700/ title="S-Net: From Answer Extraction to Answer Generation for Machine Reading Comprehension">S-Net: From Answer Extraction to Answer Generation for Machine Reading Comprehension</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202207/>2022.07</a><ul><li><a href=/akitenkrad-blog/posts/papers/202207/20220727145036/ title="Dynamic Heterogeneous Graph Embedding Using Hierarchical Attentions">Dynamic Heterogeneous Graph Embedding Using Hierarchical Attentions</a></li><li><a href=/akitenkrad-blog/posts/papers/202207/20220726163444/ title="Dynamic Network Embedding Survey">Dynamic Network Embedding Survey</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202208/>2022.08</a><ul><li><a href=/akitenkrad-blog/posts/papers/202208/20220802103319/ title="Modeling Dynamic Heterogeneous Network for Link Prediction using Hierarchical Attention with Temporal RNN">Modeling Dynamic Heterogeneous Network for Link Prediction using Hierarchical Attention with Temporal RNN</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202209/>2022.09</a><ul><li><a href=/akitenkrad-blog/posts/papers/202209/20220909180827/ title="Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting">Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202305/>2023.05</a><ul><li><a href=/akitenkrad-blog/posts/papers/202305/20230513094804/ title="Improving Language Understanding by Generative Pre-Training">Improving Language Understanding by Generative Pre-Training</a></li><li><a href=/akitenkrad-blog/posts/papers/202305/20230514200241/ title="Language Models are Unsupervised Multitask Learners">Language Models are Unsupervised Multitask Learners</a></li></ul></li><li><a href=/akitenkrad-blog/posts/papers/202306/ title=2023.06>2023.06</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/akitenkrad-blog/posts/arxiv/>arXiv</a><ul class=active><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202307/>2023.07</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230701000000/ title="arXiv @ 2023.07.01">arXiv @ 2023.07.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230702000000/ title="arXiv @ 2023.07.02">arXiv @ 2023.07.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230703000000/ title="arXiv @ 2023.07.03">arXiv @ 2023.07.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230704000000/ title="arXiv @ 2023.07.04">arXiv @ 2023.07.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230705000000/ title="arXiv @ 2023.07.05">arXiv @ 2023.07.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230706000000/ title="arXiv @ 2023.07.06">arXiv @ 2023.07.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230707000000/ title="arXiv @ 2023.07.07">arXiv @ 2023.07.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230708000000/ title="arXiv @ 2023.07.08">arXiv @ 2023.07.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230709000000/ title="arXiv @ 2023.07.09">arXiv @ 2023.07.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230710000000/ title="arXiv @ 2023.07.10">arXiv @ 2023.07.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230711000000/ title="arXiv @ 2023.07.11">arXiv @ 2023.07.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230712000000/ title="arXiv @ 2023.07.12">arXiv @ 2023.07.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230713000000/ title="arXiv @ 2023.07.13">arXiv @ 2023.07.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230714000000/ title="arXiv @ 2023.07.14">arXiv @ 2023.07.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230715000000/ title="arXiv @ 2023.07.15">arXiv @ 2023.07.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230716000000/ title="arXiv @ 2023.07.16">arXiv @ 2023.07.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230717000000/ title="arXiv @ 2023.07.17">arXiv @ 2023.07.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230718000000/ title="arXiv @ 2023.07.18">arXiv @ 2023.07.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230719000000/ title="arXiv @ 2023.07.19">arXiv @ 2023.07.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230720000000/ title="arXiv @ 2023.07.20">arXiv @ 2023.07.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230721000000/ title="arXiv @ 2023.07.21">arXiv @ 2023.07.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230722000000/ title="arXiv @ 2023.07.22">arXiv @ 2023.07.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230723000000/ title="arXiv @ 2023.07.23">arXiv @ 2023.07.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230724000000/ title="arXiv @ 2023.07.24">arXiv @ 2023.07.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230725000000/ title="arXiv @ 2023.07.25">arXiv @ 2023.07.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230726000000/ title="arXiv @ 2023.07.26">arXiv @ 2023.07.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230727000000/ title="arXiv @ 2023.07.27">arXiv @ 2023.07.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230728000000/ title="arXiv @ 2023.07.28">arXiv @ 2023.07.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230729000000/ title="arXiv @ 2023.07.29">arXiv @ 2023.07.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230730000000/ title="arXiv @ 2023.07.30">arXiv @ 2023.07.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230731000000/ title="arXiv @ 2023.07.31">arXiv @ 2023.07.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202308/>2023.08</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230801000000/ title="arXiv @ 2023.08.01">arXiv @ 2023.08.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230802000000/ title="arXiv @ 2023.08.02">arXiv @ 2023.08.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230803000000/ title="arXiv @ 2023.08.03">arXiv @ 2023.08.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230804000000/ title="arXiv @ 2023.08.04">arXiv @ 2023.08.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230805000000/ title="arXiv @ 2023.08.05">arXiv @ 2023.08.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230806000000/ title="arXiv @ 2023.08.06">arXiv @ 2023.08.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230807000000/ title="arXiv @ 2023.08.07">arXiv @ 2023.08.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230808000000/ title="arXiv @ 2023.08.08">arXiv @ 2023.08.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230809000000/ title="arXiv @ 2023.08.09">arXiv @ 2023.08.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230810000000/ title="arXiv @ 2023.08.10">arXiv @ 2023.08.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230811000000/ title="arXiv @ 2023.08.11">arXiv @ 2023.08.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230812000000/ title="arXiv @ 2023.08.12">arXiv @ 2023.08.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230813000000/ title="arXiv @ 2023.08.13">arXiv @ 2023.08.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230814000000/ title="arXiv @ 2023.08.14">arXiv @ 2023.08.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230815000000/ title="arXiv @ 2023.08.15">arXiv @ 2023.08.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230816000000/ title="arXiv @ 2023.08.16">arXiv @ 2023.08.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230817000000/ title="arXiv @ 2023.08.17">arXiv @ 2023.08.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230818000000/ title="arXiv @ 2023.08.18">arXiv @ 2023.08.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230819000000/ title="arXiv @ 2023.08.19">arXiv @ 2023.08.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230820000000/ title="arXiv @ 2023.08.20">arXiv @ 2023.08.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230821000000/ title="arXiv @ 2023.08.21">arXiv @ 2023.08.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230822000000/ title="arXiv @ 2023.08.22">arXiv @ 2023.08.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230823000000/ title="arXiv @ 2023.08.23">arXiv @ 2023.08.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230824000000/ title="arXiv @ 2023.08.24">arXiv @ 2023.08.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230825000000/ title="arXiv @ 2023.08.25">arXiv @ 2023.08.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230826000000/ title="arXiv @ 2023.08.26">arXiv @ 2023.08.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230827000000/ title="arXiv @ 2023.08.27">arXiv @ 2023.08.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230828000000/ title="arXiv @ 2023.08.28">arXiv @ 2023.08.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230829000000/ title="arXiv @ 2023.08.29">arXiv @ 2023.08.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230830000000/ title="arXiv @ 2023.08.30">arXiv @ 2023.08.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230831000000/ title="arXiv @ 2023.08.31">arXiv @ 2023.08.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202309/>2023.09</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230901000000/ title="arXiv @ 2023.09.01">arXiv @ 2023.09.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230902000000/ title="arXiv @ 2023.09.02">arXiv @ 2023.09.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230903000000/ title="arXiv @ 2023.09.03">arXiv @ 2023.09.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230904000000/ title="arXiv @ 2023.09.04">arXiv @ 2023.09.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230905000000/ title="arXiv @ 2023.09.05">arXiv @ 2023.09.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230906000000/ title="arXiv @ 2023.09.06">arXiv @ 2023.09.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230907000000/ title="arXiv @ 2023.09.07">arXiv @ 2023.09.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230908000000/ title="arXiv @ 2023.09.08">arXiv @ 2023.09.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230909000000/ title="arXiv @ 2023.09.09">arXiv @ 2023.09.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230910000000/ title="arXiv @ 2023.09.10">arXiv @ 2023.09.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230911000000/ title="arXiv @ 2023.09.11">arXiv @ 2023.09.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230912000000/ title="arXiv @ 2023.09.12">arXiv @ 2023.09.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230913000000/ title="arXiv @ 2023.09.13">arXiv @ 2023.09.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230914000000/ title="arXiv @ 2023.09.14">arXiv @ 2023.09.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230915000000/ title="arXiv @ 2023.09.15">arXiv @ 2023.09.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230916000000/ title="arXiv @ 2023.09.16">arXiv @ 2023.09.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230917000000/ title="arXiv @ 2023.09.17">arXiv @ 2023.09.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230918000000/ title="arXiv @ 2023.09.18">arXiv @ 2023.09.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230919000000/ title="arXiv @ 2023.09.19">arXiv @ 2023.09.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230920000000/ title="arXiv @ 2023.09.20">arXiv @ 2023.09.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230921000000/ title="arXiv @ 2023.09.21">arXiv @ 2023.09.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230922000000/ title="arXiv @ 2023.09.22">arXiv @ 2023.09.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230923000000/ title="arXiv @ 2023.09.23">arXiv @ 2023.09.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230924000000/ title="arXiv @ 2023.09.24">arXiv @ 2023.09.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230925000000/ title="arXiv @ 2023.09.25">arXiv @ 2023.09.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230926000000/ title="arXiv @ 2023.09.26">arXiv @ 2023.09.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230927000000/ title="arXiv @ 2023.09.27">arXiv @ 2023.09.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230928000000/ title="arXiv @ 2023.09.28">arXiv @ 2023.09.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230929000000/ title="arXiv @ 2023.09.29">arXiv @ 2023.09.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230930000000/ title="arXiv @ 2023.09.30">arXiv @ 2023.09.30</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202310/>2023.1</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231001000000/ title="arXiv @ 2023.10.01">arXiv @ 2023.10.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231002000000/ title="arXiv @ 2023.10.02">arXiv @ 2023.10.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231003000000/ title="arXiv @ 2023.10.03">arXiv @ 2023.10.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231004000000/ title="arXiv @ 2023.10.04">arXiv @ 2023.10.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231005000000/ title="arXiv @ 2023.10.05">arXiv @ 2023.10.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231006000000/ title="arXiv @ 2023.10.06">arXiv @ 2023.10.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231007000000/ title="arXiv @ 2023.10.07">arXiv @ 2023.10.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231008000000/ title="arXiv @ 2023.10.08">arXiv @ 2023.10.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231009000000/ title="arXiv @ 2023.10.09">arXiv @ 2023.10.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231010000000/ title="arXiv @ 2023.10.10">arXiv @ 2023.10.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231011000000/ title="arXiv @ 2023.10.11">arXiv @ 2023.10.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231012000000/ title="arXiv @ 2023.10.12">arXiv @ 2023.10.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231013000000/ title="arXiv @ 2023.10.13">arXiv @ 2023.10.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231014000000/ title="arXiv @ 2023.10.14">arXiv @ 2023.10.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231015000000/ title="arXiv @ 2023.10.15">arXiv @ 2023.10.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231016000000/ title="arXiv @ 2023.10.16">arXiv @ 2023.10.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231017000000/ title="arXiv @ 2023.10.17">arXiv @ 2023.10.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231018000000/ title="arXiv @ 2023.10.18">arXiv @ 2023.10.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231019000000/ title="arXiv @ 2023.10.19">arXiv @ 2023.10.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231020000000/ title="arXiv @ 2023.10.20">arXiv @ 2023.10.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231021000000/ title="arXiv @ 2023.10.21">arXiv @ 2023.10.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231022000000/ title="arXiv @ 2023.10.22">arXiv @ 2023.10.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231023000000/ title="arXiv @ 2023.10.23">arXiv @ 2023.10.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231024000000/ title="arXiv @ 2023.10.24">arXiv @ 2023.10.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231025000000/ title="arXiv @ 2023.10.25">arXiv @ 2023.10.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231026000000/ title="arXiv @ 2023.10.26">arXiv @ 2023.10.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231027000000/ title="arXiv @ 2023.10.27">arXiv @ 2023.10.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231028000000/ title="arXiv @ 2023.10.28">arXiv @ 2023.10.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231029000000/ title="arXiv @ 2023.10.29">arXiv @ 2023.10.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231030000000/ title="arXiv @ 2023.10.30">arXiv @ 2023.10.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231031000000/ title="arXiv @ 2023.10.31">arXiv @ 2023.10.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202311/>2023.11</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231101000000/ title="arXiv @ 2023.11.01">arXiv @ 2023.11.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231102000000/ title="arXiv @ 2023.11.02">arXiv @ 2023.11.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231103000000/ title="arXiv @ 2023.11.03">arXiv @ 2023.11.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231104000000/ title="arXiv @ 2023.11.04">arXiv @ 2023.11.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231105000000/ title="arXiv @ 2023.11.05">arXiv @ 2023.11.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231106000000/ title="arXiv @ 2023.11.06">arXiv @ 2023.11.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231107000000/ title="arXiv @ 2023.11.07">arXiv @ 2023.11.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231108000000/ title="arXiv @ 2023.11.08">arXiv @ 2023.11.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231109000000/ title="arXiv @ 2023.11.09">arXiv @ 2023.11.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231110000000/ title="arXiv @ 2023.11.10">arXiv @ 2023.11.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231111000000/ title="arXiv @ 2023.11.11">arXiv @ 2023.11.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231112000000/ title="arXiv @ 2023.11.12">arXiv @ 2023.11.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231113000000/ title="arXiv @ 2023.11.13">arXiv @ 2023.11.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231114000000/ title="arXiv @ 2023.11.14">arXiv @ 2023.11.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231115000000/ title="arXiv @ 2023.11.15">arXiv @ 2023.11.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231116000000/ title="arXiv @ 2023.11.16">arXiv @ 2023.11.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231117000000/ title="arXiv @ 2023.11.17">arXiv @ 2023.11.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231118000000/ title="arXiv @ 2023.11.18">arXiv @ 2023.11.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231119000000/ title="arXiv @ 2023.11.19">arXiv @ 2023.11.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231120000000/ title="arXiv @ 2023.11.20">arXiv @ 2023.11.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231121000000/ title="arXiv @ 2023.11.21">arXiv @ 2023.11.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231122000000/ title="arXiv @ 2023.11.22">arXiv @ 2023.11.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231123000000/ title="arXiv @ 2023.11.23">arXiv @ 2023.11.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231124000000/ title="arXiv @ 2023.11.24">arXiv @ 2023.11.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231125000000/ title="arXiv @ 2023.11.25">arXiv @ 2023.11.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231126000000/ title="arXiv @ 2023.11.26">arXiv @ 2023.11.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231127000000/ title="arXiv @ 2023.11.27">arXiv @ 2023.11.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231128000000/ title="arXiv @ 2023.11.28">arXiv @ 2023.11.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231129000000/ title="arXiv @ 2023.11.29">arXiv @ 2023.11.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231130000000/ title="arXiv @ 2023.11.30">arXiv @ 2023.11.30</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202312/>2023.12</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231201000000/ title="arXiv @ 2023.12.01">arXiv @ 2023.12.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231202000000/ title="arXiv @ 2023.12.02">arXiv @ 2023.12.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231203000000/ title="arXiv @ 2023.12.03">arXiv @ 2023.12.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231204000000/ title="arXiv @ 2023.12.04">arXiv @ 2023.12.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231205000000/ title="arXiv @ 2023.12.05">arXiv @ 2023.12.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231206000000/ title="arXiv @ 2023.12.06">arXiv @ 2023.12.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231207000000/ title="arXiv @ 2023.12.07">arXiv @ 2023.12.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231208000000/ title="arXiv @ 2023.12.08">arXiv @ 2023.12.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231209000000/ title="arXiv @ 2023.12.09">arXiv @ 2023.12.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231210000000/ title="arXiv @ 2023.12.10">arXiv @ 2023.12.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231211000000/ title="arXiv @ 2023.12.11">arXiv @ 2023.12.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231212000000/ title="arXiv @ 2023.12.12">arXiv @ 2023.12.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231213000000/ title="arXiv @ 2023.12.13">arXiv @ 2023.12.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231214000000/ title="arXiv @ 2023.12.14">arXiv @ 2023.12.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231215000000/ title="arXiv @ 2023.12.15">arXiv @ 2023.12.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231216000000/ title="arXiv @ 2023.12.16">arXiv @ 2023.12.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231217000000/ title="arXiv @ 2023.12.17">arXiv @ 2023.12.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231218000000/ title="arXiv @ 2023.12.18">arXiv @ 2023.12.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231219000000/ title="arXiv @ 2023.12.19">arXiv @ 2023.12.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231220000000/ title="arXiv @ 2023.12.20">arXiv @ 2023.12.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231221000000/ title="arXiv @ 2023.12.21">arXiv @ 2023.12.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231222000000/ title="arXiv @ 2023.12.22">arXiv @ 2023.12.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231223000000/ title="arXiv @ 2023.12.23">arXiv @ 2023.12.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231224000000/ title="arXiv @ 2023.12.24">arXiv @ 2023.12.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231225000000/ title="arXiv @ 2023.12.25">arXiv @ 2023.12.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231226000000/ title="arXiv @ 2023.12.26">arXiv @ 2023.12.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231227000000/ title="arXiv @ 2023.12.27">arXiv @ 2023.12.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231228000000/ title="arXiv @ 2023.12.28">arXiv @ 2023.12.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231229000000/ title="arXiv @ 2023.12.29">arXiv @ 2023.12.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231230000000/ title="arXiv @ 2023.12.30">arXiv @ 2023.12.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231231000000/ title="arXiv @ 2023.12.31">arXiv @ 2023.12.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202401/>2024.01</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240101000000/ title="arXiv @ 2024.01.01">arXiv @ 2024.01.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240102000000/ title="arXiv @ 2024.01.02">arXiv @ 2024.01.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240103000000/ title="arXiv @ 2024.01.03">arXiv @ 2024.01.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240104000000/ title="arXiv @ 2024.01.04">arXiv @ 2024.01.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240105000000/ title="arXiv @ 2024.01.05">arXiv @ 2024.01.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240106000000/ title="arXiv @ 2024.01.06">arXiv @ 2024.01.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240107000000/ title="arXiv @ 2024.01.07">arXiv @ 2024.01.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240108000000/ title="arXiv @ 2024.01.08">arXiv @ 2024.01.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240109000000/ title="arXiv @ 2024.01.09">arXiv @ 2024.01.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240110000000/ title="arXiv @ 2024.01.10">arXiv @ 2024.01.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240111000000/ title="arXiv @ 2024.01.11">arXiv @ 2024.01.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240112000000/ title="arXiv @ 2024.01.12">arXiv @ 2024.01.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240113000000/ title="arXiv @ 2024.01.13">arXiv @ 2024.01.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240114000000/ title="arXiv @ 2024.01.14">arXiv @ 2024.01.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240115000000/ title="arXiv @ 2024.01.15">arXiv @ 2024.01.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240116000000/ title="arXiv @ 2024.01.16">arXiv @ 2024.01.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240117000000/ title="arXiv @ 2024.01.17">arXiv @ 2024.01.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240118000000/ title="arXiv @ 2024.01.18">arXiv @ 2024.01.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240119000000/ title="arXiv @ 2024.01.19">arXiv @ 2024.01.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240120000000/ title="arXiv @ 2024.01.20">arXiv @ 2024.01.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240121000000/ title="arXiv @ 2024.01.21">arXiv @ 2024.01.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240122000000/ title="arXiv @ 2024.01.22">arXiv @ 2024.01.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240123000000/ title="arXiv @ 2024.01.23">arXiv @ 2024.01.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240124000000/ title="arXiv @ 2024.01.24">arXiv @ 2024.01.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240125000000/ title="arXiv @ 2024.01.25">arXiv @ 2024.01.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240126000000/ title="arXiv @ 2024.01.26">arXiv @ 2024.01.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240127000000/ title="arXiv @ 2024.01.27">arXiv @ 2024.01.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240128000000/ title="arXiv @ 2024.01.28">arXiv @ 2024.01.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240129000000/ title="arXiv @ 2024.01.29">arXiv @ 2024.01.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240130000000/ title="arXiv @ 2024.01.30">arXiv @ 2024.01.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240131000000/ title="arXiv @ 2024.01.31">arXiv @ 2024.01.31</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/akitenkrad-blog/posts/arxiv/202402/>2024.02</a><ul class=active><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240201000000/ title="arXiv @ 2024.02.01">arXiv @ 2024.02.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240202000000/ title="arXiv @ 2024.02.02">arXiv @ 2024.02.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240203000000/ title="arXiv @ 2024.02.03">arXiv @ 2024.02.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240204000000/ title="arXiv @ 2024.02.04">arXiv @ 2024.02.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240205000000/ title="arXiv @ 2024.02.05">arXiv @ 2024.02.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240206000000/ title="arXiv @ 2024.02.06">arXiv @ 2024.02.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240207000000/ title="arXiv @ 2024.02.07">arXiv @ 2024.02.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240208000000/ title="arXiv @ 2024.02.08">arXiv @ 2024.02.08</a></li><li><a class=active href=/akitenkrad-blog/posts/arxiv/202402/20240209000000/ title="arXiv @ 2024.02.09">arXiv @ 2024.02.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240210000000/ title="arXiv @ 2024.02.10">arXiv @ 2024.02.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240211000000/ title="arXiv @ 2024.02.11">arXiv @ 2024.02.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240212000000/ title="arXiv @ 2024.02.12">arXiv @ 2024.02.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240213000000/ title="arXiv @ 2024.02.13">arXiv @ 2024.02.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240214000000/ title="arXiv @ 2024.02.14">arXiv @ 2024.02.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240215000000/ title="arXiv @ 2024.02.15">arXiv @ 2024.02.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240216000000/ title="arXiv @ 2024.02.16">arXiv @ 2024.02.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240217000000/ title="arXiv @ 2024.02.17">arXiv @ 2024.02.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240218000000/ title="arXiv @ 2024.02.18">arXiv @ 2024.02.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240219000000/ title="arXiv @ 2024.02.19">arXiv @ 2024.02.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240220000000/ title="arXiv @ 2024.02.20">arXiv @ 2024.02.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240221000000/ title="arXiv @ 2024.02.21">arXiv @ 2024.02.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240222000000/ title="arXiv @ 2024.02.22">arXiv @ 2024.02.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240223000000/ title="arXiv @ 2024.02.23">arXiv @ 2024.02.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240224000000/ title="arXiv @ 2024.02.24">arXiv @ 2024.02.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240225000000/ title="arXiv @ 2024.02.25">arXiv @ 2024.02.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240226000000/ title="arXiv @ 2024.02.26">arXiv @ 2024.02.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240227000000/ title="arXiv @ 2024.02.27">arXiv @ 2024.02.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240228000000/ title="arXiv @ 2024.02.28">arXiv @ 2024.02.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240229000000/ title="arXiv @ 2024.02.29">arXiv @ 2024.02.29</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202403/>2024.03</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240301000000/ title="arXiv @ 2024.03.01">arXiv @ 2024.03.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240302000000/ title="arXiv @ 2024.03.02">arXiv @ 2024.03.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240303000000/ title="arXiv @ 2024.03.03">arXiv @ 2024.03.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240304000000/ title="arXiv @ 2024.03.04">arXiv @ 2024.03.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240305000000/ title="arXiv @ 2024.03.05">arXiv @ 2024.03.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240306000000/ title="arXiv @ 2024.03.06">arXiv @ 2024.03.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240307000000/ title="arXiv @ 2024.03.07">arXiv @ 2024.03.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240308000000/ title="arXiv @ 2024.03.08">arXiv @ 2024.03.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240309000000/ title="arXiv @ 2024.03.09">arXiv @ 2024.03.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240310000000/ title="arXiv @ 2024.03.10">arXiv @ 2024.03.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240311000000/ title="arXiv @ 2024.03.11">arXiv @ 2024.03.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202403/20240312000000/ title="arXiv @ 2024.03.12">arXiv @ 2024.03.12</a></li></ul></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/algorithms/>Algorithms</a><ul><li><a href=/akitenkrad-blog/posts/algorithms/bandit-algorithm-basic/ title="Bandit Algorithm Basic">Bandit Algorithm Basic</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/conference/>Conference</a><ul><li><a href=/akitenkrad-blog/posts/conference/acl/ title="Annual Meeting of the Association for Computational Linguistics">Annual Meeting of the Association for Computational Linguistics</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/>Figures</a><ul><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/202208/>2022.08</a><ul><li><a href=/akitenkrad-blog/posts/figures/202208/20220822092748/ title=1ヶ月当たりの実労働時間の推移>1ヶ月当たりの実労働時間の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220813115200/ title=コンビニエンスストアの店舗数の推移>コンビニエンスストアの店舗数の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220821113322/ title=地域別1世帯当たり1ヶ月間の支出額の推移>地域別1世帯当たり1ヶ月間の支出額の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220828111836/ title="夫婦別子供の有無別生活時間 (2016)">夫婦別子供の有無別生活時間 (2016)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220827103615/ title=家族類型の変遷>家族類型の変遷</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220815131016/ title="年齢別大学院入学者数 (2021)">年齢別大学院入学者数 (2021)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220818122628/ title=情報サービス業における企業特殊的人的資本（名目）の推移>情報サービス業における企業特殊的人的資本（名目）の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220824095127/ title=有業者の社会生活における活動の変化>有業者の社会生活における活動の変化</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220814175022/ title=東京23区の乗用車保有台数>東京23区の乗用車保有台数</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220817121117/ title="業種別企業特殊的人的資本 (2018)">業種別企業特殊的人的資本 (2018)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220825094111/ title=無業者の社会生活における活動の変化>無業者の社会生活における活動の変化</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220823104100/ title=睡眠時間の推移>睡眠時間の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220826102652/ title=総人口の推移>総人口の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220820225528/ title=都道府県別国公立別学校数・学生数>都道府県別国公立別学校数・学生数</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220816120845/ title="都道府県別消費者物価指数/教育（全国平均=100）">都道府県別消費者物価指数/教育（全国平均=100）</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220812173814/ title=金沢市の夏の月平均気温の遷移>金沢市の夏の月平均気温の遷移</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/202209/>2022.09</a><ul><li><a href=/akitenkrad-blog/posts/figures/202209/20220907122906/ title="ACL-2022 WordCloud">ACL-2022 WordCloud</a></li></ul></li></ul></li><li><a href=/akitenkrad-blog/posts/latex/ title="Latex Mathematics Syntax Guide">Latex Mathematics Syntax Guide</a></li><li><a href=/akitenkrad-blog/posts/markdown/ title="Markdown Sample">Markdown Sample</a></li></div></ul></div></div></div></section><section class=content-section id=content-section><div class=content><div class="container p-0 read-area"><div class="hero-area col-sm-12" id=hero-area style=background-image:url(/akitenkrad-blog/posts/arxiv/202402/20240209000000/hero.png)></div><div class=page-content><div class="author-profile ml-auto align-self-lg-center"><img class=rounded-circle src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_120x120_fit_box_3.png alt="Author Image"><h5 class=author-name></h5><p>Friday, Feb 9, 2024</p></div><div class=title><h1>arXiv @ 2024.02.09</h1></div><div class=taxonomy-terms><ul><li class=rounded><a href=/akitenkrad-blog/tags/arxiv class="btn, btn-sm">arXiv</a></li><li class=rounded><a href=/akitenkrad-blog/tags/published2024 class="btn, btn-sm">2024</a></li></ul></div><div class=post-content id=post-content><figure style=border:none;width:100%;display:flex;justify-content:center><iframe src=pie.html width=900 height=620 style=border:none></iframe></figure><h2 id=primary-categories>Primary Categories</h2><ul><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240209000000/#cond-matmtrl-sci-1>cond-mat.mtrl-sci (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240209000000/#csai-10>cs.AI (10)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240209000000/#cscl-29>cs.CL (29)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240209000000/#cscr-4>cs.CR (4)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240209000000/#cscv-38>cs.CV (38)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240209000000/#csdc-2>cs.DC (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240209000000/#csdl-1>cs.DL (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240209000000/#csfl-1>cs.FL (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240209000000/#csgr-1>cs.GR (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240209000000/#csgt-1>cs.GT (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240209000000/#cshc-6>cs.HC (6)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240209000000/#csir-7>cs.IR (7)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240209000000/#csit-4>cs.IT (4)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240209000000/#cslg-65>cs.LG (65)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240209000000/#csma-2>cs.MA (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240209000000/#csne-2>cs.NE (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240209000000/#csni-3>cs.NI (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240209000000/#csro-13>cs.RO (13)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240209000000/#cssd-2>cs.SD (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240209000000/#csse-8>cs.SE (8)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240209000000/#cssi-1>cs.SI (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240209000000/#eessiv-7>eess.IV (7)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240209000000/#eesssy-5>eess.SY (5)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240209000000/#mathfa-1>math.FA (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240209000000/#mathna-2>math.NA (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240209000000/#mathoc-3>math.OC (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240209000000/#mathst-1>math.ST (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240209000000/#physicschem-ph-1>physics.chem-ph (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240209000000/#physicsflu-dyn-2>physics.flu-dyn (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240209000000/#q-biobm-2>q-bio.BM (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240209000000/#quant-ph-2>quant-ph (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240209000000/#statme-2>stat.ME (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240209000000/#statml-7>stat.ML (7)</a></li></ul><h2 id=keywords>Keywords</h2><table border=1 class=dataframe><thead><tr style=text-align:right><th>keyword</th><th>cs.AI</th><th>cs.CL</th><th>cs.CV</th><th>cs.LG</th><th>cs.RO</th></tr></thead><tbody><tr><td>Alpaca</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Aspect-based Sentiment Analysis</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Autoencoder</td><td></td><td></td><td></td><td>2</td><td></td></tr><tr><td>BERT</td><td></td><td>2</td><td></td><td></td><td></td></tr><tr><td>BLEU</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Bag-of-Words</td><td></td><td>1</td><td></td><td>1</td><td></td></tr><tr><td>Bandit Algorithm</td><td></td><td></td><td></td><td>2</td><td></td></tr><tr><td>ChatGPT</td><td></td><td>2</td><td></td><td></td><td></td></tr><tr><td>Chatbot</td><td></td><td></td><td></td><td>2</td><td></td></tr><tr><td>Code Generation</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Contrastive Learning</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Convolution</td><td></td><td></td><td>3</td><td>4</td><td></td></tr><tr><td>Convolutional Neural Network</td><td></td><td></td><td>4</td><td>5</td><td></td></tr><tr><td>Counter-factual</td><td>1</td><td></td><td></td><td>2</td><td></td></tr><tr><td>Curriculum Learning</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Data Augmentation</td><td>2</td><td>1</td><td></td><td>2</td><td></td></tr><tr><td>Dialogue System</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Disambiguation</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Domain Adaptation</td><td></td><td></td><td>3</td><td></td><td></td></tr><tr><td>Essay Scoring</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Explainable AI</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Fairness</td><td>1</td><td></td><td></td><td>3</td><td></td></tr><tr><td>Few-shot</td><td></td><td>2</td><td>2</td><td>2</td><td></td></tr><tr><td>Few-shot Learning</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Fine-tuning</td><td>1</td><td>6</td><td>8</td><td>13</td><td></td></tr><tr><td>Foundation Model</td><td></td><td></td><td>2</td><td>1</td><td></td></tr><tr><td>GPT</td><td></td><td>5</td><td>1</td><td>2</td><td></td></tr><tr><td>GPT-3</td><td></td><td>3</td><td></td><td>1</td><td></td></tr><tr><td>GPT-3.5</td><td></td><td>3</td><td></td><td>1</td><td></td></tr><tr><td>GPT-4</td><td></td><td>3</td><td></td><td>1</td><td></td></tr><tr><td>Gaussian Process</td><td></td><td></td><td></td><td>1</td><td>1</td></tr><tr><td>Grammatical Error Correction</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Graph Attention Networks</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Graph Convolutional Network</td><td></td><td></td><td></td><td>2</td><td></td></tr><tr><td>Graph Neural Network</td><td></td><td></td><td></td><td>12</td><td></td></tr><tr><td>Grounding</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Hierarchical Clustering</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Human Intervention</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Image2text</td><td></td><td></td><td>2</td><td></td><td></td></tr><tr><td>In-context Learning</td><td></td><td>2</td><td></td><td>2</td><td>2</td></tr><tr><td>Information Retrieval</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Knowledge Distillation</td><td></td><td>2</td><td>7</td><td>1</td><td></td></tr><tr><td>LLaMA</td><td></td><td>2</td><td></td><td>1</td><td></td></tr><tr><td>Label Smoothing</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Large Language Model</td><td>8</td><td>38</td><td>3</td><td>22</td><td>1</td></tr><tr><td>Logistic Regression</td><td></td><td></td><td></td><td>2</td><td></td></tr><tr><td>Low-Resource</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Machine Unlearning</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Masked Language Model</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Message-Passing</td><td></td><td></td><td></td><td>2</td><td></td></tr><tr><td>Meta Learning</td><td></td><td></td><td>2</td><td></td><td></td></tr><tr><td>Multi-modal</td><td></td><td>4</td><td>7</td><td>6</td><td>2</td></tr><tr><td>Named Entity Recognition</td><td></td><td>2</td><td></td><td></td><td></td></tr><tr><td>Natural Language Explanation</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Natural Language Inference</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Neural Machine Translation</td><td></td><td>2</td><td></td><td></td><td></td></tr><tr><td>Node Classification</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Object Detection</td><td></td><td></td><td>5</td><td></td><td></td></tr><tr><td>Offline Reinforcement Learning</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Online Reinforcement Learning</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Open-Domain Dialogue</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Open-Domain Question Answering</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Optical Character Recognition</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Out-of-distribution</td><td>1</td><td></td><td>1</td><td>3</td><td>1</td></tr><tr><td>Out-of-domain</td><td></td><td>1</td><td>1</td><td></td><td></td></tr><tr><td>PaLM</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Perplexity</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Pre-trained Language Model</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Prompt</td><td>1</td><td>3</td><td>6</td><td>3</td><td></td></tr><tr><td>Prompt Learning</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Pruning</td><td></td><td></td><td></td><td>2</td><td></td></tr><tr><td>Quantization</td><td></td><td></td><td></td><td>4</td><td></td></tr><tr><td>Question Answering</td><td>1</td><td>2</td><td>2</td><td></td><td></td></tr><tr><td>Reasoning</td><td>4</td><td>6</td><td></td><td>1</td><td>1</td></tr><tr><td>Recommendation</td><td></td><td></td><td></td><td>2</td><td></td></tr><tr><td>Recurrent Neural Network</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Reinforcement Learning</td><td>3</td><td>2</td><td></td><td>14</td><td>4</td></tr><tr><td>Scaling Law</td><td></td><td></td><td></td><td>2</td><td></td></tr><tr><td>Self-Attention</td><td></td><td></td><td>4</td><td>1</td><td></td></tr><tr><td>Self-supervised Learning</td><td></td><td></td><td>1</td><td>1</td><td></td></tr><tr><td>Self-supervised Pre-training</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Semi-Supervised Learning</td><td></td><td></td><td>2</td><td></td><td></td></tr><tr><td>Sentence Embedding</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Sentiment Analysis</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Simulation</td><td>2</td><td></td><td>2</td><td>5</td><td>8</td></tr><tr><td>Simulator</td><td>2</td><td></td><td>2</td><td>5</td><td>8</td></tr><tr><td>Stochastic Gradient Descent</td><td></td><td></td><td></td><td>5</td><td></td></tr><tr><td>Summarization</td><td></td><td>2</td><td>2</td><td>2</td><td>1</td></tr><tr><td>Supervised Learning</td><td></td><td>3</td><td>3</td><td>8</td><td>1</td></tr><tr><td>TF-IDF</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Text Generation</td><td></td><td>3</td><td></td><td></td><td></td></tr><tr><td>Text Summarization</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Text2image</td><td></td><td></td><td>3</td><td></td><td></td></tr><tr><td>Topic Model</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Transfer Learning</td><td></td><td></td><td></td><td>2</td><td></td></tr><tr><td>Transformer</td><td></td><td>3</td><td>7</td><td>13</td><td>1</td></tr><tr><td>Unsupervised Learning</td><td></td><td></td><td>2</td><td>2</td><td></td></tr><tr><td>Vision-and-Language</td><td></td><td>1</td><td>4</td><td>4</td><td>1</td></tr><tr><td>Weakly Supervised Learning</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Weakly-supervised Learning</td><td></td><td></td><td>3</td><td></td><td></td></tr><tr><td>Word2vec</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Zero-shot</td><td></td><td>4</td><td>3</td><td></td><td>3</td></tr></tbody></table><script>$(function(){$("table").addClass("keyword-table table-bordered border-success"),$("table thead").addClass("sticky-top"),$("table tbody td").css("text-align","")})</script><h2 id=cslg-65>cs.LG (65)</h2><h3 id=1236-graph-neural-networks-as-fast-and-high-fidelity-emulators-for-finite-element-ice-sheet-modeling-maryam-rahnemoonfar-et-al-2024>(1/236) Graph Neural Networks as Fast and High-fidelity Emulators for Finite-Element Ice Sheet Modeling (Maryam Rahnemoonfar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Maryam Rahnemoonfar, Younghyun Koo. (2024)<br><strong>Graph Neural Networks as Fast and High-fidelity Emulators for Finite-Element Ice Sheet Modeling</strong><br><button class=copy-to-clipboard title="Graph Neural Networks as Fast and High-fidelity Emulators for Finite-Element Ice Sheet Modeling" index=1>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-1 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CE, cs-LG, cs.LG<br>Keyword Score: 110<br>Keywords: Graph Attention Networks, Graph Convolutional Network, Graph Convolutional Network, Graph Neural Network, Graph Neural Network, Convolution, Convolutional Neural Network, Convolutional Neural Network, Convolutional Neural Network, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05291v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05291v1.pdf filename=2402.05291v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Although the finite element approach of the Ice-sheet and Sea-level System Model (ISSM) solves ice dynamics problems governed by Stokes equations quickly and accurately, such numerical modeling requires intensive computation on central processing units (CPU). In this study, we develop <b>graph</b> <b>neural</b> <b>networks</b> <b>(GNN)</b> as fast surrogate models to preserve the finite element structure of ISSM. Using the 20-year transient <b>simulations</b> in the Pine Island Glacier (PIG), we train and test three <b>GNNs:</b> <b>graph</b> <b>convolutional</b> <b>network</b> <b>(GCN),</b> <b>graph</b> <b>attention</b> <b>network</b> <b>(GAT),</b> and equivariant <b>graph</b> <b>convolutional</b> <b>network</b> <b>(EGCN).</b> These <b>GNNs</b> reproduce ice thickness and velocity with better accuracy than the classic <b>convolutional</b> <b>neural</b> <b>network</b> <b>(CNN)</b> and multi-layer perception (MLP). In particular, <b>GNNs</b> successfully capture the ice mass loss and acceleration induced by higher basal melting rates in the PIG. When our <b>GNN</b> emulators are implemented on graphic processing units (GPUs), they show up to 50 times faster computational time than the CPU-based ISSM <b>simulation.</b></p></p class="citation"></blockquote><h3 id=2236-l4q-parameter-efficient-quantization-aware-training-on-large-language-models-via-lora-wise-lsq-hyesung-jeon-et-al-2024>(2/236) L4Q: Parameter Efficient Quantization-Aware Training on Large Language Models via LoRA-wise LSQ (Hyesung Jeon et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hyesung Jeon, Yulhwa Kim, Jae-joon Kim. (2024)<br><strong>L4Q: Parameter Efficient Quantization-Aware Training on Large Language Models via LoRA-wise LSQ</strong><br><button class=copy-to-clipboard title="L4Q: Parameter Efficient Quantization-Aware Training on Large Language Models via LoRA-wise LSQ" index=2>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-2 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CL, cs-LG, cs.LG<br>Keyword Score: 90<br>Keywords: Few-shot, Fine-tuning, Quantization, Quantization, LLaMA, In-context Learning, In-context Learning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04902v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04902v1.pdf filename=2402.04902v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Post-training <b>quantization</b> (PTQ) and <b>quantization-aware</b> training (QAT) methods are gaining popularity in mitigating the high memory and computational costs associated with <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs).</b> In resource-constrained scenarios, PTQ, with its reduced training overhead, is often preferred over QAT, despite the latter&rsquo;s potential for higher accuracy. Meanwhile, parameter-efficient <b>fine-tuning</b> (PEFT) methods like low-rank adaptation (LoRA) have been introduced, and recent efforts have explored <b>quantization-aware</b> PEFT techniques. However, these approaches may lack generality due to their reliance on the pre-quantized model&rsquo;s configuration. Their effectiveness may be compromised by non-linearly <b>quantized</b> or mixed-precision weights, and the retraining of specific <b>quantization</b> parameters might impede optimal performance. To address these challenges, we propose L4Q, an algorithm for parameter-efficient <b>quantization-aware</b> training. L4Q leverages LoRA-wise learned <b>quantization</b> step size for <b>LLMs,</b> aiming to enhance generality. The simultaneous <b>quantization-and-fine-tuning</b> process of L4Q is applicable to high-precision models, yielding linearly <b>quantized</b> weights with superior accuracy. Our experiments, conducted on the <b>LLaMA</b> and LLaMA2 model families using an instructional dataset, showcase L4Q&rsquo;s capabilities in language comprehension and <b>few-shot</b> <b>in-context</b> <b>learning,</b> achieving sub-4-bit precision while maintaining comparable training times to applying PEFT on a <b>quantized</b> model.</p></p class="citation"></blockquote><h3 id=3236-de-amplifying-bias-from-differential-privacy-in-language-model-fine-tuning-sanjari-srivastava-et-al-2024>(3/236) De-amplifying Bias from Differential Privacy in Language Model Fine-tuning (Sanjari Srivastava et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sanjari Srivastava, Piotr Mardziel, Zhikhun Zhang, Archana Ahlawat, Anupam Datta, John C Mitchell. (2024)<br><strong>De-amplifying Bias from Differential Privacy in Language Model Fine-tuning</strong><br><button class=copy-to-clipboard title="De-amplifying Bias from Differential Privacy in Language Model Fine-tuning" index=3>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-3 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CR, cs-CY, cs-LG, cs.LG, stat-ME<br>Keyword Score: 70<br>Keywords: Counter-factual, Data Augmentation, Fairness, Fine-tuning, Fine-tuning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04489v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04489v1.pdf filename=2402.04489v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Fairness</b> and privacy are two important values machine learning (ML) practitioners often seek to operationalize in models. <b>Fairness</b> aims to reduce model bias for social/demographic sub-groups. Privacy via differential privacy (DP) mechanisms, on the other hand, limits the impact of any individual&rsquo;s training <b>data</b> <b>on</b> the resulting model. The trade-offs between privacy and <b>fairness</b> goals of trustworthy ML pose a challenge to those wishing to address both. We show that DP amplifies gender, racial, and religious bias when <b>fine-tuning</b> <b>large</b> <b>language</b> <b>models</b> <b>(LLMs),</b> producing models more biased than ones <b>fine-tuned</b> without DP. We find the cause of the amplification to be a disparity in convergence of gradients across sub-groups. Through the case of binary gender bias, we demonstrate that <b>Counterfactual</b> <b>Data</b> <b>Augmentation</b> (CDA), a known method for addressing bias, also mitigates bias amplification by DP. As a consequence, DP and CDA together can be used to <b>fine-tune</b> models while maintaining both <b>fairness</b> and privacy.</p></p class="citation"></blockquote><h3 id=4236-hydragen-high-throughput-llm-inference-with-shared-prefixes-jordan-juravsky-et-al-2024>(4/236) Hydragen: High-Throughput LLM Inference with Shared Prefixes (Jordan Juravsky et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jordan Juravsky, Bradley Brown, Ryan Ehrlich, Daniel Y. Fu, Christopher Ré, Azalia Mirhoseini. (2024)<br><strong>Hydragen: High-Throughput LLM Inference with Shared Prefixes</strong><br><button class=copy-to-clipboard title="Hydragen: High-Throughput LLM Inference with Shared Prefixes" index=4>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-4 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 60<br>Keywords: Few-shot, Transformer, Chatbot, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05099v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05099v1.pdf filename=2402.05099v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Transformer-based</b> <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> are now deployed to hundreds of millions of users. <b>LLM</b> inference is commonly performed on batches of sequences that share a prefix, such as <b>few-shot</b> examples or a <b>chatbot</b> system <b>prompt.</b> Decoding in this <b>large-batch</b> <b>setting</b> <b>can</b> be bottlenecked by the attention operation, which reads <b>large</b> <b>key-value</b> <b>(KV)</b> caches from memory and computes inefficient matrix-vector products for every sequence in the batch. In this work, we introduce Hydragen, a hardware-aware exact implementation of attention with shared prefixes. Hydragen computes attention over the shared prefix and unique suffixes separately. This decomposition enables efficient prefix attention by batching queries together across sequences, reducing redundant memory reads and enabling the use of hardware-friendly matrix multiplications. Our method can improve end-to-end <b>LLM</b> throughput by up to 32x against competitive baselines, with speedup growing with the batch size and shared prefix length. Hydragen also enables the use of very long shared contexts: with a high batch size, increasing the prefix length from 1K to 16K tokens decreases Hydragen throughput by less than 15%, while the throughput of baselines drops by over 90%. Hydragen generalizes beyond simple prefix-suffix decomposition and can be applied to tree-based <b>prompt</b> sharing patterns, allowing us to further reduce inference time on competitive programming problems by 55%.</p></p class="citation"></blockquote><h3 id=5236-open-vocabulary-calibration-for-vision-language-models-shuoyuan-wang-et-al-2024>(5/236) Open-Vocabulary Calibration for Vision-Language Models (Shuoyuan Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shuoyuan Wang, Jindong Wang, Guoqing Wang, Bob Zhang, Kaiyang Zhou, Hongxin Wei. (2024)<br><strong>Open-Vocabulary Calibration for Vision-Language Models</strong><br><button class=copy-to-clipboard title="Open-Vocabulary Calibration for Vision-Language Models" index=5>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-5 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 60<br>Keywords: Fine-tuning, Fine-tuning, Chatbot, Prompt, Prompt Learning, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04655v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04655v1.pdf filename=2402.04655v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Vision-language</b> models (VLMs) have emerged as formidable tools, showing their strong capability in handling various open-vocabulary tasks in image recognition, text-driven visual content generation, and visual <b>chatbots,</b> to name a few. In recent years, considerable efforts and resources have been devoted to adaptation methods for improving downstream performance of VLMs, particularly on parameter-efficient <b>fine-tuning</b> methods like <b>prompt</b> <b>learning.</b> However, a crucial aspect that has been largely overlooked is the confidence calibration problem in <b>fine-tuned</b> VLMs, which could greatly reduce reliability when deploying such models in the real world. This paper bridges the gap by systematically investigating the confidence calibration problem in the context of <b>prompt</b> <b>learning</b> and reveals that existing calibration methods are insufficient to address the problem, especially in the open-vocabulary setting. To solve the problem, we present a simple and effective approach called Distance-Aware Calibration (DAC), which is based on scaling the temperature using as guidance the distance between predicted text labels and base classes. The experiments with 7 distinct <b>prompt</b> <b>learning</b> methods applied across 11 diverse downstream datasets demonstrate the effectiveness of DAC, which achieves high efficacy without sacrificing the inference speed.</p></p class="citation"></blockquote><h3 id=6236-grandmaster-level-chess-without-search-anian-ruoss-et-al-2024>(6/236) Grandmaster-Level Chess Without Search (Anian Ruoss et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Anian Ruoss, Grégoire Delétang, Sourabh Medapati, Jordi Grau-Moya, Li Kevin Wenliang, Elliot Catt, John Reid, Tim Genewein. (2024)<br><strong>Grandmaster-Level Chess Without Search</strong><br><button class=copy-to-clipboard title="Grandmaster-Level Chess Without Search" index=6>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-6 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG, stat-ML<br>Keyword Score: 60<br>Keywords: Supervised Learning, Supervised Learning, GPT, GPT-3, GPT-3.5, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04494v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04494v1.pdf filename=2402.04494v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The recent breakthrough successes in machine learning are mainly attributed to scale: namely large-scale attention-based architectures and datasets of unprecedented scale. This paper investigates the impact of training at scale for chess. Unlike traditional chess engines that rely on complex heuristics, explicit search, or a combination of both, we train a 270M parameter <b>transformer</b> model with <b>supervised</b> <b>learning</b> on a dataset of 10 million chess games. We annotate each board in the dataset with action-values provided by the powerful Stockfish 16 engine, leading to roughly 15 billion data points. Our largest model reaches a Lichess blitz Elo of 2895 against humans, and successfully solves a series of challenging chess puzzles, without any domain-specific tweaks or explicit search algorithms. We also show that our model outperforms AlphaZero&rsquo;s policy and value networks (without MCTS) and <b>GPT-3.5-turbo-instruct.</b> A systematic investigation of model and dataset size shows that strong chess performance only arises at sufficient scale. To validate our results, we perform an extensive series of ablations of design choices and hyperparameters.</p></p class="citation"></blockquote><h3 id=7236-ranksum-an-unsupervised-extractive-text-summarization-based-on-rank-fusion-a-joshi-et-al-2024>(7/236) RankSum An unsupervised extractive text summarization based on rank fusion (A. Joshi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>A. Joshi, E. Fidalgo, E. Alegre, R. Alaiz-Rodriguez. (2024)<br><strong>RankSum An unsupervised extractive text summarization based on rank fusion</strong><br><button class=copy-to-clipboard title="RankSum An unsupervised extractive text summarization based on rank fusion" index=7>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-7 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 50<br>Keywords: Topic Model, Unsupervised Learning, Sentence Embedding, Text Summarization, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05976v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05976v1.pdf filename=2402.05976v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we propose Ranksum, an approach for extractive <b>text</b> <b>summarization</b> of single documents based on the rank fusion of four multi-dimensional <b>sentence</b> <b>features</b> extracted for each <b>sentence:</b> <b>topic</b> <b>information,</b> semantic content, significant keywords, and position. The Ranksum obtains the <b>sentence</b> <b>saliency</b> rankings corresponding to each feature in an <b>unsupervised</b> way followed by the weighted fusion of the four scores to rank the <b>sentences</b> <b>according</b> to their significance. The scores are generated in completely <b>unsupervised</b> way, and a labeled document set is required to learn the fusion weights. Since we found that the fusion weights can generalize to other datasets, we consider the Ranksum as an <b>unsupervised</b> approach. To determine <b>topic</b> <b>rank,</b> we employ probabilistic <b>topic</b> <b>models</b> whereas semantic information is captured using <b>sentence</b> <b>embeddings.</b> To derive rankings using <b>sentence</b> <b>embeddings,</b> we utilize Siamese networks to produce abstractive <b>sentence</b> <b>representation</b> and then we formulate a novel strategy to arrange them in their order of importance. A graph-based strategy is applied to find the significant keywords and related <b>sentence</b> <b>rankings</b> in the document. We also formulate a <b>sentence</b> <b>novelty</b> measure based on bigrams, trigrams, and <b>sentence</b> <b>embeddings</b> to eliminate redundant <b>sentences</b> <b>from</b> the summary. The ranks of all the <b>sentences</b> <b>computed</b> for each feature are finally fused to get the final score for each <b>sentence</b> <b>in</b> the document. We evaluate our approach on publicly available <b>summarization</b> datasets CNN/DailyMail and DUC 2002. Experimental results show that our approach outperforms other existing state-of-the-art <b>summarization</b> methods.</p></p class="citation"></blockquote><h3 id=8236-classifying-spam-emails-using-agglomerative-hierarchical-clustering-and-a-topic-based-approach-f-janez-martino-et-al-2024>(8/236) Classifying spam emails using agglomerative hierarchical clustering and a topic-based approach (F. Janez-Martino et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>F. Janez-Martino, R. Alaiz-Rodriguez, V. Gonzalez-Castro, E. Fidalgo, E. Alegre. (2024)<br><strong>Classifying spam emails using agglomerative hierarchical clustering and a topic-based approach</strong><br><button class=copy-to-clipboard title="Classifying spam emails using agglomerative hierarchical clustering and a topic-based approach" index=8>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-8 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 50<br>Keywords: Hierarchical Clustering, Logistic Regression, Bag-of-Words, Word2vec, TF-IDF<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05296v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05296v1.pdf filename=2402.05296v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Spam emails are unsolicited, annoying and sometimes harmful messages which may contain malware, phishing or hoaxes. Unlike most studies that address the design of efficient anti-spam filters, we approach the spam email problem from a different and novel perspective. Focusing on the needs of cybersecurity units, we follow a topic-based approach for addressing the classification of spam email into multiple categories. We propose SPEMC-15K-E and SPEMC-15K-S, two novel datasets with approximately 15K emails each in English and Spanish, respectively, and we label them using agglomerative <b>hierarchical</b> <b>clustering</b> into 11 classes. We evaluate 16 pipelines, combining four text representation techniques -Term Frequency-Inverse Document Frequency <b>(TF-IDF),</b> Bag of Words, <b>Word2Vec</b> and BERT- and four classifiers: Support Vector Machine, N"aive Bayes, Random Forest and <b>Logistic</b> <b>Regression.</b> Experimental results show that the highest performance is achieved with <b>TF-IDF</b> and LR for the English dataset, with a F1 score of 0.953 and an accuracy of 94.6%, and while for the Spanish dataset, <b>TF-IDF</b> with NB yields a F1 score of 0.945 and 98.5% accuracy. Regarding the processing time, <b>TF-IDF</b> with LR leads to the fastest classification, processing an English and Spanish spam email in and on average, respectively.</p></p class="citation"></blockquote><h3 id=9236-opening-the-ai-black-box-program-synthesis-via-mechanistic-interpretability-eric-j-michaud-et-al-2024>(9/236) Opening the AI black box: program synthesis via mechanistic interpretability (Eric J. Michaud et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Eric J. Michaud, Isaac Liao, Vedang Lad, Ziming Liu, Anish Mudide, Chloe Loughridge, Zifan Carl Guo, Tara Rezaei Kheirkhah, Mateja Vukelić, Max Tegmark. (2024)<br><strong>Opening the AI black box: program synthesis via mechanistic interpretability</strong><br><button class=copy-to-clipboard title="Opening the AI black box: program synthesis via mechanistic interpretability" index=9>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-9 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 50<br>Keywords: Autoencoder, GPT, GPT-4, Recurrent Neural Network, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05110v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05110v1.pdf filename=2402.05110v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present MIPS, a novel method for program synthesis based on automated mechanistic interpretability of neural networks trained to perform the desired task, auto-distilling the learned algorithm into Python code. We test MIPS on a benchmark of 62 algorithmic tasks that can be learned by an <b>RNN</b> and find it highly complementary to <b>GPT-4:</b> MIPS solves 32 of them, including 13 that are not solved by <b>GPT-4</b> (which also solves 30). MIPS uses an integer <b>autoencoder</b> to convert the <b>RNN</b> into a finite state machine, then applies Boolean or integer symbolic regression to capture the learned algorithm. As opposed to <b>large</b> <b>language</b> <b>models,</b> this program synthesis technique makes no use of (and is therefore not limited by) human training data such as algorithms and code from GitHub. We discuss opportunities and challenges for scaling up this approach to make machine-learned models more interpretable and trustworthy.</p></p class="citation"></blockquote><h3 id=10236-nito-neural-implicit-fields-for-resolution-free-topology-optimization-amin-heyrani-nobari-et-al-2024>(10/236) NITO: Neural Implicit Fields for Resolution-free Topology Optimization (Amin Heyrani Nobari et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Amin Heyrani Nobari, Giorgio Giannone, Lyle Regenwetter, Faez Ahmed. (2024)<br><strong>NITO: Neural Implicit Fields for Resolution-free Topology Optimization</strong><br><button class=copy-to-clipboard title="NITO: Neural Implicit Fields for Resolution-free Topology Optimization" index=10>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-10 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CE, cs-LG, cs.LG<br>Keyword Score: 50<br>Keywords: Convolution, Convolutional Neural Network, Convolutional Neural Network, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05073v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05073v1.pdf filename=2402.05073v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Topology optimization is a critical task in engineering design, where the goal is to optimally distribute material in a given space for maximum performance. We introduce Neural Implicit Topology Optimization (NITO), a novel approach to accelerate topology optimization problems using deep learning. NITO stands out as one of the first frameworks to offer a resolution-free and domain-agnostic solution in deep learning-based topology optimization. NITO synthesizes structures with up to seven times better structural efficiency compared to SOTA diffusion models and does so in a tenth of the time. In the NITO framework, we introduce a novel method, the Boundary Point Order-Invariant MLP (BPOM), to represent boundary conditions in a sparse and domain-agnostic manner, moving away from expensive <b>simulation-based</b> approaches. Crucially, NITO circumvents the domain and resolution limitations that restrict <b>Convolutional</b> <b>Neural</b> <b>Network</b> <b>(CNN)</b> models to a structured domain of fixed size &ndash; limitations that hinder the widespread adoption of <b>CNNs</b> in engineering applications. This generalizability allows a single NITO model to train and generate solutions in countless domains, eliminating the need for numerous domain-specific <b>CNNs</b> and their extensive datasets. Despite its generalizability, NITO outperforms SOTA models even in specialized tasks, is an order of magnitude smaller, and is practically trainable at high resolutions that would be restrictive for <b>CNNs.</b> This combination of versatility, efficiency, and performance underlines NITO&rsquo;s potential to transform the landscape of engineering design optimization problems through implicit fields.</p></p class="citation"></blockquote><h3 id=11236-apiq-finetuning-of-2-bit-quantized-large-language-model-baohao-liao-et-al-2024>(11/236) ApiQ: Finetuning of 2-Bit Quantized Large Language Model (Baohao Liao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Baohao Liao, Christof Monz. (2024)<br><strong>ApiQ: Finetuning of 2-Bit Quantized Large Language Model</strong><br><button class=copy-to-clipboard title="ApiQ: Finetuning of 2-Bit Quantized Large Language Model" index=11>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-11 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CL, cs-LG, cs.LG<br>Keyword Score: 50<br>Keywords: Fine-tuning, Quantization, Quantization, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05147v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05147v2.pdf filename=2402.05147v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Memory-efficient <b>finetuning</b> of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> has recently attracted huge attention with the increasing size of <b>LLMs,</b> primarily due to the constraints posed by GPU memory limitations and the comparable results of these methods with full <b>finetuning.</b> Despite the advancements, current strategies for memory-efficient <b>finetuning,</b> such as QLoRA, exhibit inconsistent performance across diverse bit-width <b>quantizations</b> and multifaceted tasks. This inconsistency largely stems from the detrimental impact of the <b>quantization</b> process on preserved knowledge, leading to catastrophic forgetting and undermining the utilization of pretrained models for <b>finetuning</b> purposes. In this work, we introduce a novel <b>quantization</b> framework named ApiQ, designed to restore the lost information from <b>quantization</b> by concurrently initializing LoRA components and quantizing the weights of <b>LLMs.</b> This approach ensures the maintenance of the original <b>LLM&rsquo;s</b> activation precision while mitigating the error propagation from shallower into deeper layers. Through comprehensive evaluations conducted on a spectrum of language tasks with various models, ApiQ demonstrably minimizes activation error during <b>quantization.</b> Consequently, it consistently achieves superior <b>finetuning</b> outcomes across various bit-widths of <b>quantization.</b></p></p class="citation"></blockquote><h3 id=12236-assessing-the-brittleness-of-safety-alignment-via-pruning-and-low-rank-modifications-boyi-wei-et-al-2024>(12/236) Assessing the Brittleness of Safety Alignment via Pruning and Low-Rank Modifications (Boyi Wei et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Boyi Wei, Kaixuan Huang, Yangsibo Huang, Tinghao Xie, Xiangyu Qi, Mengzhou Xia, Prateek Mittal, Mengdi Wang, Peter Henderson. (2024)<br><strong>Assessing the Brittleness of Safety Alignment via Pruning and Low-Rank Modifications</strong><br><button class=copy-to-clipboard title="Assessing the Brittleness of Safety Alignment via Pruning and Low-Rank Modifications" index=12>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-12 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CL, cs-LG, cs.LG<br>Keyword Score: 40<br>Keywords: Fine-tuning, Pruning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05162v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05162v1.pdf filename=2402.05162v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> show inherent brittleness in their safety mechanisms, as evidenced by their susceptibility to jailbreaking and even non-malicious <b>fine-tuning.</b> This study explores this brittleness of safety alignment by leveraging <b>pruning</b> and low-rank modifications. We develop methods to identify critical regions that are vital for safety guardrails, and that are disentangled from utility-relevant regions at both the neuron and rank levels. Surprisingly, the isolated regions we find are sparse, comprising about $3%$ at the parameter level and $2.5%$ at the rank level. Removing these regions compromises safety without significantly impacting utility, corroborating the inherent brittleness of the model&rsquo;s safety mechanisms. Moreover, we show that <b>LLMs</b> remain vulnerable to low-cost <b>fine-tuning</b> attacks even when modifications to the safety-critical regions are restricted. These findings underscore the urgent need for more robust safety strategies in <b>LLMs.</b></p></p class="citation"></blockquote><h3 id=13236-pac-learnability-under-explanation-preserving-graph-perturbations-xu-zheng-et-al-2024>(13/236) PAC Learnability under Explanation-Preserving Graph Perturbations (Xu Zheng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xu Zheng, Farhad Shirani, Tianchun Wang, Shouwei Gao, Wenqian Dong, Wei Cheng, Dongsheng Luo. (2024)<br><strong>PAC Learnability under Explanation-Preserving Graph Perturbations</strong><br><button class=copy-to-clipboard title="PAC Learnability under Explanation-Preserving Graph Perturbations" index=13>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-13 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 40<br>Keywords: Graph Neural Network, Graph Neural Network, Data Augmentation, Out-of-distribution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05039v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05039v1.pdf filename=2402.05039v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Graphical models capture relations between entities in a wide range of applications including social networks, biology, and natural language processing, among others. <b>Graph</b> <b>neural</b> <b>networks</b> <b>(GNN)</b> are neural models that operate over <b>graphs,</b> <b>enabling</b> <b>the</b> model to leverage the complex relationships and dependencies in <b>graph-structured</b> <b>data.</b> <b>A</b> <b>graph</b> <b>explanation</b> <b>is</b> a subgraph which is an `almost sufficient&rsquo; statistic of the input <b>graph</b> <b>with</b> <b>respect</b> to its classification label. Consequently, the classification label is invariant, with high probability, to perturbations of <b>graph</b> <b>edges</b> <b>not</b> belonging to its explanation subgraph. This work considers two methods for leveraging such perturbation invariances in the design and training of <b>GNNs.</b> First, explanation-assisted learning rules are considered. It is shown that the sample complexity of explanation-assisted learning can be arbitrarily smaller than explanation-agnostic learning. Next, explanation-assisted <b>data</b> <b>augmentation</b> is considered, where the training set is enlarged by artificially producing new training samples via perturbation of the non-explanation edges in the original training set. It is shown that such <b>data</b> <b>augmentation</b> methods may improve performance if the augmented <b>data</b> <b>is</b> in-distribution, however, it may also lead to worse sample complexity compared to explanation-agnostic learning rules if the augmented <b>data</b> <b>is</b> <b>out-of-distribution.</b> Extensive empirical evaluations are provided to verify the theoretical analysis.</p></p class="citation"></blockquote><h3 id=14236-a-sober-look-at-llms-for-material-discovery-are-they-actually-good-for-bayesian-optimization-over-molecules-agustinus-kristiadi-et-al-2024>(14/236) A Sober Look at LLMs for Material Discovery: Are They Actually Good for Bayesian Optimization Over Molecules? (Agustinus Kristiadi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Agustinus Kristiadi, Felix Strieth-Kalthoff, Marta Skreta, Pascal Poupart, Alán Aspuru-Guzik, Geoff Pleiss. (2024)<br><strong>A Sober Look at LLMs for Material Discovery: Are They Actually Good for Bayesian Optimization Over Molecules?</strong><br><button class=copy-to-clipboard title="A Sober Look at LLMs for Material Discovery: Are They Actually Good for Bayesian Optimization Over Molecules?" index=14>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-14 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 40<br>Keywords: Fine-tuning, Fine-tuning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05015v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05015v1.pdf filename=2402.05015v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Automation is one of the cornerstones of contemporary material discovery. Bayesian optimization (BO) is an essential part of such workflows, enabling scientists to leverage prior domain knowledge into efficient exploration of a <b>large</b> <b>molecular</b> <b>space.</b> While such prior knowledge can take many forms, there has been significant fanfare around the ancillary scientific knowledge encapsulated in <b>large</b> <b>language</b> <b>models</b> <b>(LLMs).</b> However, existing work thus far has only explored <b>LLMs</b> for heuristic materials searches. Indeed, recent work obtains the uncertainty estimate &ndash; an integral part of BO &ndash; from point-estimated, non-Bayesian <b>LLMs.</b> In this work, we study the question of whether <b>LLMs</b> are actually useful to accelerate principled Bayesian optimization in the molecular space. We take a sober, dispassionate stance in answering this question. This is done by carefully (i) viewing <b>LLMs</b> as fixed feature extractors for standard but principled BO surrogate models and by (ii) leveraging parameter-efficient <b>finetuning</b> methods and Bayesian neural networks to obtain the posterior of the <b>LLM</b> surrogate. Our extensive experiments with real-world chemistry problems show that <b>LLMs</b> can be useful for BO over molecules, but only if they have been pretrained or <b>finetuned</b> with domain-specific data.</p></p class="citation"></blockquote><h3 id=15236-example-based-explanations-for-random-forests-using-machine-unlearning-tanmay-surve-et-al-2024>(15/236) Example-based Explanations for Random Forests using Machine Unlearning (Tanmay Surve et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tanmay Surve, Romila Pradhan. (2024)<br><strong>Example-based Explanations for Random Forests using Machine Unlearning</strong><br><button class=copy-to-clipboard title="Example-based Explanations for Random Forests using Machine Unlearning" index=15>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-15 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 40<br>Keywords: Fairness, Machine Unlearning, Supervised Learning, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05007v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05007v1.pdf filename=2402.05007v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Tree-based <b>machine</b> <b>learning</b> models, such as decision trees and random forests, have been hugely successful in classification tasks primarily because of their predictive power in <b>supervised</b> <b>learning</b> tasks and ease of interpretation. Despite their popularity and power, these models have been found to produce unexpected or discriminatory outcomes. Given their overwhelming success for most tasks, it is of interest to identify sources of their unexpected and discriminatory behavior. However, there has not been much work on understanding and debugging tree-based classifiers in the context of <b>fairness.</b> We introduce FairDebugger, a system that utilizes recent advances in <b>machine</b> <b>unlearning</b> research to identify training data subsets responsible for instances of <b>fairness</b> violations in the outcomes of a random forest classifier. FairDebugger generates top-$k$ explanations (in the form of coherent training data subsets) for model unfairness. Toward this goal, FairDebugger first utilizes <b>machine</b> <b>unlearning</b> to estimate the change in the tree structures of the random forest when parts of the underlying training data are removed, and then leverages the Apriori algorithm from frequent itemset mining to reduce the subset search space. We empirically evaluate our approach on three real-world datasets, and demonstrate that the explanations generated by FairDebugger are consistent with insights from prior studies on these datasets.</p></p class="citation"></blockquote><h3 id=16236-multi-patch-prediction-adapting-llms-for-time-series-representation-learning-yuxuan-bian-et-al-2024>(16/236) Multi-Patch Prediction: Adapting LLMs for Time Series Representation Learning (Yuxuan Bian et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuxuan Bian, Xuan Ju, Jiangtong Li, Zhijian Xu, Dawei Cheng, Qiang Xu. (2024)<br><strong>Multi-Patch Prediction: Adapting LLMs for Time Series Representation Learning</strong><br><button class=copy-to-clipboard title="Multi-Patch Prediction: Adapting LLMs for Time Series Representation Learning" index=16>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-16 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 40<br>Keywords: Fine-tuning, Self-supervised Learning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04852v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04852v1.pdf filename=2402.04852v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this study, we present aLLM4TS, an innovative framework that adapts <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> for time-series representation learning. Central to our approach is that we reconceive time-series forecasting as a <b>self-supervised,</b> multi-patch prediction task, which, compared to traditional mask-and-reconstruction methods, captures temporal dynamics in patch representations more effectively. Our strategy encompasses two-stage training: (i). a causal continual pre-training phase on various time-series datasets, anchored on next patch prediction, effectively syncing <b>LLM</b> capabilities with the intricacies of time-series data; (ii). <b>fine-tuning</b> for multi-patch prediction in the targeted time-series context. A distinctive element of our framework is the patch-wise decoding layer, which departs from previous methods reliant on sequence-level decoding. Such a design directly transposes individual patches into temporal sequences, thereby significantly bolstering the model&rsquo;s proficiency in mastering temporal patch-based representations. aLLM4TS demonstrates superior performance in several downstream tasks, proving its effectiveness in deriving temporal representations with enhanced transferability and marking a pivotal advancement in the adaptation of <b>LLMs</b> for time-series analysis.</p></p class="citation"></blockquote><h3 id=17236-latent-plan-transformer-planning-as-latent-variable-inference-deqian-kong-et-al-2024>(17/236) Latent Plan Transformer: Planning as Latent Variable Inference (Deqian Kong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Deqian Kong, Dehong Xu, Minglu Zhao, Bo Pang, Jianwen Xie, Andrew Lizarraga, Yuhao Huang, Sirui Xie, Ying Nian Wu. (2024)<br><strong>Latent Plan Transformer: Planning as Latent Variable Inference</strong><br><button class=copy-to-clipboard title="Latent Plan Transformer: Planning as Latent Variable Inference" index=17>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-17 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 40<br>Keywords: Offline Reinforcement Learning, Reinforcement Learning, Transformer, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04647v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04647v1.pdf filename=2402.04647v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In tasks aiming for long-term returns, planning becomes necessary. We study generative modeling for planning with datasets repurposed from <b>offline</b> <b>reinforcement</b> <b>learning.</b> Specifically, we identify temporal consistency in the absence of step-wise rewards as one key technical challenge. We introduce the Latent Plan <b>Transformer</b> (LPT), a novel model that leverages a latent space to connect a <b>Transformer-based</b> trajectory generator and the final return. LPT can be learned with maximum likelihood estimation on trajectory-return pairs. In learning, posterior sampling of the latent variable naturally gathers sub-trajectories to form a consistent abstraction despite the finite context. During test time, the latent variable is inferred from an expected return before policy execution, realizing the idea of planning as inference. It then guides the autoregressive policy throughout the episode, functioning as a plan. Our experiments demonstrate that LPT can discover improved decisions from suboptimal trajectories. It achieves competitive performance across several benchmarks, including Gym-Mujoco, Maze2D, and Connect Four, exhibiting capabilities of nuanced credit assignments, trajectory stitching, and adaptation to environmental contingencies. These results validate that latent variable inference can be a strong alternative to step-wise reward <b>prompting.</b></p></p class="citation"></blockquote><h3 id=18236-levi-generalizable-fine-tuning-via-layer-wise-ensemble-of-different-views-yuji-roh-et-al-2024>(18/236) LEVI: Generalizable Fine-tuning via Layer-wise Ensemble of Different Views (Yuji Roh et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuji Roh, Qingyun Liu, Huan Gui, Zhe Yuan, Yujin Tang, Steven Euijong Whang, Liang Liu, Shuchao Bi, Lichan Hong, Ed H. Chi, Zhe Zhao. (2024)<br><strong>LEVI: Generalizable Fine-tuning via Layer-wise Ensemble of Different Views</strong><br><button class=copy-to-clipboard title="LEVI: Generalizable Fine-tuning via Layer-wise Ensemble of Different Views" index=18>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-18 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 40<br>Keywords: Fine-tuning, Fine-tuning, Foundation Model, Out-of-distribution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04644v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04644v1.pdf filename=2402.04644v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Fine-tuning</b> is becoming widely used for leveraging the power of pre-trained <b>foundation</b> <b>models</b> in new downstream tasks. While there are many successes of <b>fine-tuning</b> on various tasks, recent studies have observed challenges in the generalization of <b>fine-tuned</b> models to unseen distributions (i.e., <b>out-of-distribution;</b> OOD). To improve OOD generalization, some previous studies identify the limitations of <b>fine-tuning</b> data and regulate <b>fine-tuning</b> to preserve the general representation learned from pre-training data. However, potential limitations in the pre-training data and models are often ignored. In this paper, we contend that overly relying on the pre-trained representation may hinder <b>fine-tuning</b> from learning essential representations for downstream tasks and thus hurt its OOD generalization. It can be especially catastrophic when new tasks are from different (sub)domains compared to pre-training data. To address the issues in both pre-training and <b>fine-tuning</b> data, we propose a novel generalizable <b>fine-tuning</b> method LEVI, where the pre-trained model is adaptively ensembled layer-wise with a small task-specific model, while preserving training and inference efficiencies. By combining two complementing models, LEVI effectively suppresses problematic features in both the <b>fine-tuning</b> data and pre-trained model and preserves useful features for new tasks. Broad experiments with large language and vision models show that LEVI greatly improves <b>fine-tuning</b> generalization via emphasizing different views from <b>fine-tuning</b> data and pre-trained features.</p></p class="citation"></blockquote><h3 id=19236-feature-distribution-on-graph-topology-mediates-the-effect-of-graph-convolution-homophily-perspective-soo-yong-lee-et-al-2024>(19/236) Feature Distribution on Graph Topology Mediates the Effect of Graph Convolution: Homophily Perspective (Soo Yong Lee et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Soo Yong Lee, Sunwoo Kim, Fanchen Bu, Jaemin Yoo, Jiliang Tang, Kijung Shin. (2024)<br><strong>Feature Distribution on Graph Topology Mediates the Effect of Graph Convolution: Homophily Perspective</strong><br><button class=copy-to-clipboard title="Feature Distribution on Graph Topology Mediates the Effect of Graph Convolution: Homophily Perspective" index=19>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-19 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 40<br>Keywords: Node Classification, Graph Neural Network, Graph Neural Network, Convolution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04621v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04621v1.pdf filename=2402.04621v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>How would randomly shuffling feature vectors among <b>nodes</b> <b>from</b> the same class affect <b>graph</b> <b>neural</b> <b>networks</b> <b>(GNNs)?</b> The feature shuffle, intuitively, perturbs the dependence between <b>graph</b> <b>topology</b> <b>and</b> features (A-X dependence) for <b>GNNs</b> to learn from. Surprisingly, we observe a consistent and significant improvement in <b>GNN</b> performance following the feature shuffle. Having overlooked the impact of A-X dependence on <b>GNNs,</b> the prior literature does not provide a satisfactory understanding of the phenomenon. Thus, we raise two research questions. First, how should A-X dependence be measured, while controlling for potential confounds? Second, how does A-X dependence affect GNNs? In response, we (i) propose a principled measure for A-X dependence, (ii) design a random <b>graph</b> <b>model</b> <b>that</b> controls A-X dependence, (iii) establish a theory on how A-X dependence relates to <b>graph</b> <b>convolution,</b> <b>and</b> (iv) present empirical analysis on real-world <b>graphs</b> <b>that</b> <b>aligns</b> with the theory. We conclude that A-X dependence mediates the effect of <b>graph</b> <b>convolution,</b> <b>such</b> that smaller dependence improves <b>GNN-based</b> <b>node</b> <b>classification.</b></p></p class="citation"></blockquote><h3 id=20236-collective-counterfactual-explanations-via-optimal-transport-ahmad-reza-ehyaei-et-al-2024>(20/236) Collective Counterfactual Explanations via Optimal Transport (Ahmad-Reza Ehyaei et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ahmad-Reza Ehyaei, Ali Shirali, Samira Samadi. (2024)<br><strong>Collective Counterfactual Explanations via Optimal Transport</strong><br><button class=copy-to-clipboard title="Collective Counterfactual Explanations via Optimal Transport" index=20>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-20 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-ME<br>Keyword Score: 40<br>Keywords: Counter-factual, Recommendation, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04579v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04579v1.pdf filename=2402.04579v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Counterfactual</b> explanations provide individuals with cost-optimal actions that can alter their labels to desired classes. However, if substantial instances seek state modification, such individual-centric methods can lead to new competitions and unanticipated costs. Furthermore, these <b>recommendations,</b> disregarding the underlying data distribution, may suggest actions that users perceive as outliers. To address these issues, our work proposes a collective approach for formulating <b>counterfactual</b> explanations, with an emphasis on utilizing the current density of the individuals to inform the recommended actions. Our problem naturally casts as an optimal transport problem. Leveraging the extensive literature on optimal transport, we illustrate how this collective method improves upon the desiderata of classical <b>counterfactual</b> explanations. We support our proposal with numerical <b>simulations,</b> illustrating the effectiveness of the proposed approach and its relation to classic methods.</p></p class="citation"></blockquote><h3 id=21236-sumrec-a-framework-for-recommendation-using-open-domain-dialogue-ryutaro-asahara-et-al-2024>(21/236) SumRec: A Framework for Recommendation using Open-Domain Dialogue (Ryutaro Asahara et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ryutaro Asahara, Masaki Takahashi, Chiho Iwahashi, Michimasa Inaba. (2024)<br><strong>SumRec: A Framework for Recommendation using Open-Domain Dialogue</strong><br><button class=copy-to-clipboard title="SumRec: A Framework for Recommendation using Open-Domain Dialogue" index=21>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-21 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CL, cs-LG, cs.LG<br>Keyword Score: 40<br>Keywords: Recommendation, Open-Domain Dialogue, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04523v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04523v1.pdf filename=2402.04523v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Chat dialogues contain considerable useful information about a speaker&rsquo;s interests, preferences, and experiences.Thus, knowledge from <b>open-domain</b> <b>chat</b> dialogue can be used to personalize various systems and offer <b>recommendations</b> for advanced information.This study proposed a novel framework SumRec for recommending information from <b>open-domain</b> <b>chat</b> dialogue.The study also examined the framework using ChatRec, a newly constructed dataset for training and evaluation. To extract the speaker and item characteristics, the SumRec framework employs a <b>large</b> <b>language</b> <b>model</b> <b>(LLM)</b> to generate a summary of the speaker information from a dialogue and to recommend information about an item according to the type of user.The speaker and item information are then input into a score estimation model, generating a <b>recommendation</b> score.Experimental results show that the SumRec framework provides better <b>recommendations</b> than the baseline method of using dialogues and item descriptions in their original form. Our dataset and code is publicly available at <a href=https://github.com/Ryutaro-A/SumRec>https://github.com/Ryutaro-A/SumRec</a></p></p class="citation"></blockquote><h3 id=22236-sym-q-adaptive-symbolic-regression-via-sequential-decision-making-yuan-tian-et-al-2024>(22/236) Sym-Q: Adaptive Symbolic Regression via Sequential Decision-Making (Yuan Tian et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuan Tian, Wenqi Zhou, Hao Dong, David S. Kammer, Olga Fink. (2024)<br><strong>Sym-Q: Adaptive Symbolic Regression via Sequential Decision-Making</strong><br><button class=copy-to-clipboard title="Sym-Q: Adaptive Symbolic Regression via Sequential Decision-Making" index=22>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-22 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Reinforcement Learning, Supervised Learning, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05306v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05306v1.pdf filename=2402.05306v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Symbolic regression holds great potential for uncovering underlying mathematical and physical relationships from empirical data. While existing <b>transformer-based</b> models have recently achieved significant success in this domain, they face challenges in terms of generalizability and adaptability. Typically, in cases where the output expressions do not adequately fit experimental data, the models lack efficient mechanisms to adapt or modify the expression. This inflexibility hinders their application in real-world scenarios, particularly in discovering unknown physical or biological relationships. Inspired by how human experts refine and adapt expressions, we introduce Symbolic Q-network (Sym-Q), a novel <b>reinforcement</b> <b>learning-based</b> model that redefines symbolic regression as a sequential decision-making task. Sym-Q leverages <b>supervised</b> demonstrations and refines expressions based on reward signals indicating the quality of fitting precision. Its distinctive ability to manage the complexity of expression trees and perform precise step-wise updates significantly enhances flexibility and efficiency. Our results demonstrate that Sym-Q excels not only in recovering underlying mathematical structures but also uniquely learns to efficiently refine the output expression based on reward signals, thereby discovering underlying expressions. Sym-Q paves the way for more intuitive and impactful discoveries in physical science, marking a substantial advancement in the field of symbolic regression.</p></p class="citation"></blockquote><h3 id=23236-safety-filters-for-black-box-dynamical-systems-by-learning-discriminating-hyperplanes-will-lavanakul-et-al-2024>(23/236) Safety Filters for Black-Box Dynamical Systems by Learning Discriminating Hyperplanes (Will Lavanakul et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Will Lavanakul, Jason J. Choi, Koushil Sreenath, Claire J. Tomlin. (2024)<br><strong>Safety Filters for Black-Box Dynamical Systems by Learning Discriminating Hyperplanes</strong><br><button class=copy-to-clipboard title="Safety Filters for Black-Box Dynamical Systems by Learning Discriminating Hyperplanes" index=23>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-23 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Reinforcement Learning, Supervised Learning, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05279v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05279v1.pdf filename=2402.05279v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Learning-based approaches are emerging as an effective approach for safety filters for black-box dynamical systems. Existing methods have relied on certificate functions like Control Barrier Functions (CBFs) and Hamilton-Jacobi (HJ) reachability value functions. The primary motivation for our work is the recognition that ultimately, enforcing the safety constraint as a control input constraint at each state is what matters. By focusing on this constraint, we can eliminate dependence on any specific certificate function-based design. To achieve this, we define a discriminating hyperplane that shapes the half-space constraint on control input at each state, serving as a sufficient condition for safety. This concept not only generalizes over traditional safety methods but also simplifies safety filter design by eliminating dependence on specific certificate functions. We present two strategies to learn the discriminating hyperplane: (a) a <b>supervised</b> <b>learning</b> approach, using pre-verified control invariant sets for labeling, and (b) a <b>reinforcement</b> <b>learning</b> (RL) approach, which does not require such labels. The main advantage of our method, unlike conventional safe RL approaches, is the separation of performance and safety. This offers a reusable safety filter for learning new tasks, avoiding the need to retrain from scratch. As such, we believe that the new notion of the discriminating hyperplane offers a more generalizable direction towards designing safety filters, encompassing and extending existing certificate-function-based or safe RL methodologies.</p></p class="citation"></blockquote><h3 id=24236-towards-understanding-inductive-bias-in-transformers-a-view-from-infinity-itay-lavie-et-al-2024>(24/236) Towards Understanding Inductive Bias in Transformers: A View From Infinity (Itay Lavie et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Itay Lavie, Guy Gur-Ari, Zohar Ringel. (2024)<br><strong>Towards Understanding Inductive Bias in Transformers: A View From Infinity</strong><br><button class=copy-to-clipboard title="Towards Understanding Inductive Bias in Transformers: A View From Infinity" index=24>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-24 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cond-mat-dis-nn, cs-LG, cs.LG, stat-ML<br>Keyword Score: 30<br>Keywords: Gaussian Process, Transformer, Scaling Law<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05173v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05173v1.pdf filename=2402.05173v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We study inductive bias in <b>Transformers</b> in the infinitely over-parameterized <b>Gaussian</b> <b>process</b> limit and argue <b>transformers</b> tend to be biased towards more permutation symmetric functions in sequence space. We show that the representation theory of the symmetric group can be used to give quantitative analytical predictions when the dataset is symmetric to permutations between tokens. We present a simplified <b>transformer</b> block and solve the model at the limit, including accurate predictions for the learning curves and network outputs. We show that in common setups, one can derive tight bounds in the form of a <b>scaling</b> <b>law</b> for the learnability as a function of the context length. Finally, we argue WikiText dataset, does indeed possess a degree of permutation symmetry.</p></p class="citation"></blockquote><h3 id=25236-navigating-complexity-toward-lossless-graph-condensation-via-expanding-window-matching-yuchen-zhang-et-al-2024>(25/236) Navigating Complexity: Toward Lossless Graph Condensation via Expanding Window Matching (Yuchen Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuchen Zhang, Tianle Zhang, Kai Wang, Ziyao Guo, Yuxuan Liang, Xavier Bresson, Wei Jin, Yang You. (2024)<br><strong>Navigating Complexity: Toward Lossless Graph Condensation via Expanding Window Matching</strong><br><button class=copy-to-clipboard title="Navigating Complexity: Toward Lossless Graph Condensation via Expanding Window Matching" index=25>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-25 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Graph Neural Network, Graph Neural Network, Curriculum Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05011v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05011v1.pdf filename=2402.05011v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Graph</b> <b>condensation</b> <b>aims</b> to reduce the size of a large-scale <b>graph</b> <b>dataset</b> <b>by</b> synthesizing a compact counterpart without sacrificing the performance of <b>Graph</b> <b>Neural</b> <b>Networks</b> <b>(GNNs)</b> trained on it, which has shed light on reducing the computational cost for training <b>GNNs.</b> Nevertheless, existing methods often fall short of accurately replicating the original <b>graph</b> <b>for</b> <b>certain</b> datasets, thereby failing to achieve the objective of lossless condensation. To understand this phenomenon, we investigate the potential reasons and reveal that the previous state-of-the-art trajectory matching method provides biased and restricted supervision signals from the original <b>graph</b> <b>when</b> <b>optimizing</b> the condensed one. This significantly limits both the scale and efficacy of the condensed <b>graph.</b> <b>In</b> <b>this</b> paper, we make the first attempt toward \textit{lossless <b>graph</b> <b>condensation}</b> <b>by</b> bridging the previously neglected supervision signals. Specifically, we employ a <b>curriculum</b> <b>learning</b> strategy to train expert trajectories with more diverse supervision signals from the original <b>graph,</b> <b>and</b> <b>then</b> effectively transfer the information into the condensed <b>graph</b> <b>with</b> <b>expanding</b> window matching. Moreover, we design a loss function to further extract knowledge from the expert trajectories. Theoretical analysis justifies the design of our method and extensive experiments verify its superiority across different datasets. Code is released at <a href=https://github.com/NUS-HPC-AI-Lab/GEOM>https://github.com/NUS-HPC-AI-Lab/GEOM</a>.</p></p class="citation"></blockquote><h3 id=26236-a-bayesian-approach-to-online-learning-for-contextual-restless-bandits-with-applications-to-public-health-biyonka-liang-et-al-2024>(26/236) A Bayesian Approach to Online Learning for Contextual Restless Bandits with Applications to Public Health (Biyonka Liang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Biyonka Liang, Lily Xu, Aparna Taneja, Milind Tambe, Lucas Janson. (2024)<br><strong>A Bayesian Approach to Online Learning for Contextual Restless Bandits with Applications to Public Health</strong><br><button class=copy-to-clipboard title="A Bayesian Approach to Online Learning for Contextual Restless Bandits with Applications to Public Health" index=26>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-26 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-AP<br>Keyword Score: 30<br>Keywords: Bandit Algorithm, Online Reinforcement Learning, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04933v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04933v1.pdf filename=2402.04933v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Restless multi-armed <b>bandits</b> (RMABs) are used to model sequential resource allocation in public health intervention programs. In these settings, the underlying transition dynamics are often unknown a priori, requiring <b>online</b> <b>reinforcement</b> <b>learning</b> (RL). However, existing methods in <b>online</b> <b>RL</b> <b>for</b> RMABs cannot incorporate properties often present in real-world public health applications, such as contextual information and non-stationarity. We present Bayesian Learning for Contextual RMABs (BCoR), an <b>online</b> <b>RL</b> <b>approach</b> for RMABs that novelly combines techniques in Bayesian modeling with Thompson sampling to flexibly model a wide range of complex RMAB settings, such as contextual and non-stationary RMABs. A key contribution of our approach is its ability to leverage shared information within and between arms to learn unknown RMAB transition dynamics quickly in budget-constrained settings with relatively short time horizons. Empirically, we show that BCoR achieves substantially higher finite-sample performance than existing approaches over a range of experimental settings, including one constructed from a real-world public health campaign in India.</p></p class="citation"></blockquote><h3 id=27236-code-as-reward-empowering-reinforcement-learning-with-vlms-david-venuto-et-al-2024>(27/236) Code as Reward: Empowering Reinforcement Learning with VLMs (David Venuto et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>David Venuto, Sami Nur Islam, Martin Klissarov, Doina Precup, Sherry Yang, Ankit Anand. (2024)<br><strong>Code as Reward: Empowering Reinforcement Learning with VLMs</strong><br><button class=copy-to-clipboard title="Code as Reward: Empowering Reinforcement Learning with VLMs" index=27>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-27 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Reinforcement Learning, Code Generation, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04764v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04764v1.pdf filename=2402.04764v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Pre-trained <b>Vision-Language</b> Models (VLMs) are able to understand visual concepts, describe and decompose complex tasks into sub-tasks, and provide feedback on task completion. In this paper, we aim to leverage these capabilities to support the training of <b>reinforcement</b> <b>learning</b> (RL) agents. In principle, VLMs are well suited for this purpose, as they can naturally analyze image-based observations and provide feedback (reward) on learning progress. However, inference in VLMs is computationally expensive, so querying them frequently to compute rewards would significantly slowdown the training of an RL agent. To address this challenge, we propose a framework named <b>Code</b> <b>as</b> Reward (VLM-CaR). VLM-CaR produces dense reward functions from VLMs through <b>code</b> <b>generation,</b> thereby significantly reducing the computational burden of querying the VLM directly. We show that the dense rewards generated through our approach are very accurate across a diverse set of discrete and continuous environments, and can be more effective in training RL policies than the original sparse environment rewards.</p></p class="citation"></blockquote><h3 id=28236-oil-ad-an-anomaly-detection-framework-for-sequential-decision-sequences-chen-wang-et-al-2024>(28/236) OIL-AD: An Anomaly Detection Framework for Sequential Decision Sequences (Chen Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chen Wang, Sarah Erfani, Tansu Alpcan, Christopher Leckie. (2024)<br><strong>OIL-AD: An Anomaly Detection Framework for Sequential Decision Sequences</strong><br><button class=copy-to-clipboard title="OIL-AD: An Anomaly Detection Framework for Sequential Decision Sequences" index=28>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-28 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Reinforcement Learning, Unsupervised Learning, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04567v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04567v1.pdf filename=2402.04567v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Anomaly detection in decision-making sequences is a challenging problem due to the complexity of normality representation learning and the sequential nature of the task. Most existing methods based on <b>Reinforcement</b> <b>Learning</b> (RL) are difficult to implement in the real world due to unrealistic assumptions, such as having access to environment dynamics, reward signals, and online interactions with the environment. To address these limitations, we propose an <b>unsupervised</b> method named Offline Imitation Learning based Anomaly Detection (OIL-AD), which detects anomalies in decision-making sequences using two extracted behaviour features: action optimality and sequential association. Our offline learning model is an adaptation of behavioural cloning with a <b>transformer</b> policy network, where we modify the training process to learn a Q function and a state value function from normal trajectories. We propose that the Q function and the state value function can provide sufficient information about agents&rsquo; behavioural data, from which we derive two features for anomaly detection. The intuition behind our method is that the action optimality feature derived from the Q function can differentiate the optimal action from others at each local state, and the sequential association feature derived from the state value function has the potential to maintain the temporal correlations between decisions (state-action pairs). Our experiments show that OIL-AD can achieve outstanding online anomaly detection performance with up to 34.8% improvement in F1 score over comparable baselines.</p></p class="citation"></blockquote><h3 id=29236-examining-modality-incongruity-in-multimodal-federated-learning-for-medical-vision-and-language-based-disease-detection-pramit-saha-et-al-2024>(29/236) Examining Modality Incongruity in Multimodal Federated Learning for Medical Vision and Language-based Disease Detection (Pramit Saha et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Pramit Saha, Divyanshu Mishra, Felix Wagner, Konstantinos Kamnitsas, J. Alison Noble. (2024)<br><strong>Examining Modality Incongruity in Multimodal Federated Learning for Medical Vision and Language-based Disease Detection</strong><br><button class=copy-to-clipboard title="Examining Modality Incongruity in Multimodal Federated Learning for Medical Vision and Language-based Disease Detection" index=29>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-29 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CL, cs-CV, cs-LG, cs.LG<br>Keyword Score: 26<br>Keywords: Multi-modal, Multi-modal, Self-Attention, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05294v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05294v1.pdf filename=2402.05294v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Multimodal</b> Federated Learning (MMFL) utilizes multiple modalities in each client to build a more powerful Federated Learning (FL) model than its unimodal counterpart. However, the impact of missing modality in different clients, also called modality incongruity, has been greatly overlooked. This paper, for the first time, analyses the impact of modality incongruity and reveals its connection with data heterogeneity across participating clients. We particularly inspect whether incongruent MMFL with unimodal and <b>multimodal</b> clients is more beneficial than unimodal FL. Furthermore, we examine three potential routes of addressing this issue. Firstly, we study the effectiveness of various <b>self-attention</b> mechanisms towards incongruity-agnostic information fusion in MMFL. Secondly, we introduce a modality imputation network (MIN) pre-trained in a <b>multimodal</b> client for modality translation in unimodal clients and investigate its potential towards mitigating the missing modality problem. Thirdly, we assess the capability of client-level and server-level regularization techniques towards mitigating modality incongruity effects. Experiments are conducted under several MMFL settings on two publicly available real-world datasets, MIMIC-CXR and Open-I, with Chest X-Ray and radiology reports.</p></p class="citation"></blockquote><h3 id=30236-do-transformer-world-models-give-better-policy-gradients-michel-ma-et-al-2024>(30/236) Do Transformer World Models Give Better Policy Gradients? (Michel Ma et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Michel Ma, Tianwei Ni, Clement Gehring, Pierluca D&rsquo;Oro, Pierre-Luc Bacon. (2024)<br><strong>Do Transformer World Models Give Better Policy Gradients?</strong><br><button class=copy-to-clipboard title="Do Transformer World Models Give Better Policy Gradients?" index=30>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-30 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Reinforcement Learning, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05290v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05290v2.pdf filename=2402.05290v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A natural approach for <b>reinforcement</b> <b>learning</b> is to predict future rewards by unrolling a neural network world model, and to backpropagate through the resulting computational graph to learn a policy. However, this method often becomes impractical for long horizons since typical world models induce hard-to-optimize loss landscapes. <b>Transformers</b> are known to efficiently propagate gradients over long horizons: could they be the solution to this problem? Surprisingly, we show that commonly-used <b>transformer</b> world models produce circuitous gradient paths, which can be detrimental to long-range policy gradients. To tackle this challenge, we propose a class of world models called Actions World Models (AWMs), designed to provide more direct routes for gradient propagation. We integrate such AWMs into a policy gradient framework that underscores the relationship between network architectures and the policy gradient updates they inherently represent. We demonstrate that AWMs can generate optimization landscapes that are easier to navigate even when compared to those from the simulator itself. This property allows <b>transformer</b> AWMs to produce better policies than competitive baselines in realistic long-horizon tasks.</p></p class="citation"></blockquote><h3 id=31236-adabatchgrad-combining-adaptive-batch-size-and-adaptive-step-size-petr-ostroukhov-et-al-2024>(31/236) AdaBatchGrad: Combining Adaptive Batch Size and Adaptive Step Size (Petr Ostroukhov et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Petr Ostroukhov, Aigerim Zhumabayeva, Chulu Xiang, Alexander Gasnikov, Martin Takáč, Dmitry Kamzolov. (2024)<br><strong>AdaBatchGrad: Combining Adaptive Batch Size and Adaptive Step Size</strong><br><button class=copy-to-clipboard title="AdaBatchGrad: Combining Adaptive Batch Size and Adaptive Step Size" index=31>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-31 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, math-OC<br>Keyword Score: 20<br>Keywords: Stochastic Gradient Descent, Stochastic Gradient Descent<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05264v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05264v1.pdf filename=2402.05264v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents a novel adaptation of the <b>Stochastic</b> <b>Gradient</b> <b>Descent</b> <b>(SGD),</b> termed AdaBatchGrad. This modification seamlessly integrates an adaptive step size with an adjustable batch size. An increase in batch size and a decrease in step size are well-known techniques to tighten the area of convergence of <b>SGD</b> and decrease its variance. A range of studies by R. Byrd and J. Nocedal introduced various testing techniques to assess the quality of mini-batch gradient approximations and choose the appropriate batch sizes at every step. Methods that utilized exact tests were observed to converge within $O(LR^2/\varepsilon)$ iterations. Conversely, inexact test implementations sometimes resulted in non-convergence and erratic performance. To address these challenges, AdaBatchGrad incorporates both adaptive batch and step sizes, enhancing the method&rsquo;s robustness and stability. For exact tests, our approach converges in $O(LR^2/\varepsilon)$ iterations, analogous to standard gradient descent. For inexact tests, it achieves convergence in $O(\max\lbrace LR^2/\varepsilon, \sigma^2 R^2/\varepsilon^2 \rbrace )$ iterations. This makes AdaBatchGrad markedly more robust and computationally efficient relative to prevailing methods. To substantiate the efficacy of our method, we experimentally show, how the introduction of adaptive step size and adaptive batch size gradually improves the performance of regular <b>SGD.</b> The results imply that AdaBatchGrad surpasses alternative methods, especially when applied to inexact tests.</p></p class="citation"></blockquote><h3 id=32236-learning-fair-ranking-policies-via-differentiable-optimization-of-ordered-weighted-averages-my-h-dinh-et-al-2024>(32/236) Learning Fair Ranking Policies via Differentiable Optimization of Ordered Weighted Averages (My H. Dinh et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>My H. Dinh, James Kotary, Ferdinando Fioretto. (2024)<br><strong>Learning Fair Ranking Policies via Differentiable Optimization of Ordered Weighted Averages</strong><br><button class=copy-to-clipboard title="Learning Fair Ranking Policies via Differentiable Optimization of Ordered Weighted Averages" index=32>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-32 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CY, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Fairness, Information Retrieval<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05252v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05252v1.pdf filename=2402.05252v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Learning to Rank (LTR) is one of the most widely used machine learning applications. It is a key component in platforms with profound societal impacts, including job search, healthcare <b>information</b> <b>retrieval,</b> and social media content feeds. Conventional LTR models have been shown to produce biases results, stimulating a discourse on how to address the disparities introduced by ranking systems that solely prioritize user relevance. However, while several models of fair learning to rank have been proposed, they suffer from deficiencies either in accuracy or efficiency, thus limiting their applicability to real-world ranking platforms. This paper shows how efficiently-solvable fair ranking models, based on the optimization of Ordered Weighted Average (OWA) functions, can be integrated into the training loop of an LTR model to achieve favorable balances between <b>fairness,</b> user utility, and runtime efficiency. In particular, this paper is the first to show how to backpropagate through constrained optimizations of OWA objectives, enabling their use in integrated prediction and decision models.</p></p class="citation"></blockquote><h3 id=33236-on-diffusion-models-for-amortized-inference-benchmarking-and-improving-stochastic-control-and-sampling-marcin-sendera-et-al-2024>(33/236) On diffusion models for amortized inference: Benchmarking and improving stochastic control and sampling (Marcin Sendera et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Marcin Sendera, Minsu Kim, Sarthak Mittal, Pablo Lemos, Luca Scimeca, Jarrid Rector-Brooks, Alexandre Adam, Yoshua Bengio, Nikolay Malkin. (2024)<br><strong>On diffusion models for amortized inference: Benchmarking and improving stochastic control and sampling</strong><br><button class=copy-to-clipboard title="On diffusion models for amortized inference: Benchmarking and improving stochastic control and sampling" index=33>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-33 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-ML<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05098v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05098v2.pdf filename=2402.05098v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We study the problem of training diffusion models to sample from a distribution with a given unnormalized density or energy function. We benchmark several diffusion-structured inference methods, including <b>simulation-based</b> variational approaches and off-policy methods (continuous generative flow networks). Our results shed light on the relative advantages of existing algorithms while bringing into question some claims from past work. We also propose a novel exploration strategy for off-policy methods, based on local search in the target space with the use of a replay buffer, and show that it improves the quality of samples on a variety of target distributions. Our code for the sampling methods and benchmarks studied is made public at <a href=https://github.com/GFNOrg/gfn-diffusion>https://github.com/GFNOrg/gfn-diffusion</a> as a base for future work on diffusion models for amortized inference.</p></p class="citation"></blockquote><h3 id=34236-compression-of-structured-data-with-autoencoders-provable-benefit-of-nonlinearities-and-depth-kevin-kögler-et-al-2024>(34/236) Compression of Structured Data with Autoencoders: Provable Benefit of Nonlinearities and Depth (Kevin Kögler et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kevin Kögler, Alexander Shevchenko, Hamed Hassani, Marco Mondelli. (2024)<br><strong>Compression of Structured Data with Autoencoders: Provable Benefit of Nonlinearities and Depth</strong><br><button class=copy-to-clipboard title="Compression of Structured Data with Autoencoders: Provable Benefit of Nonlinearities and Depth" index=34>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-34 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-IT, cs-LG, cs.LG, math-IT, stat-ML<br>Keyword Score: 20<br>Keywords: Message-Passing, Autoencoder<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05013v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05013v1.pdf filename=2402.05013v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Autoencoders</b> are a prominent model in many empirical branches of machine learning and lossy data compression. However, basic theoretical questions remain unanswered even in a shallow two-layer setting. In particular, to what degree does a shallow <b>autoencoder</b> capture the structure of the underlying data distribution? For the prototypical case of the 1-bit compression of sparse Gaussian data, we prove that gradient descent converges to a solution that completely disregards the sparse structure of the input. Namely, the performance of the algorithm is the same as if it was compressing a Gaussian source - with no sparsity. For general data distributions, we give evidence of a phase transition phenomenon in the shape of the gradient descent minimizer, as a function of the data sparsity: below the critical sparsity level, the minimizer is a rotation taken uniformly at random (just like in the compression of non-sparse data); above the critical sparsity, the minimizer is the identity (up to a permutation). Finally, by exploiting a connection with approximate message passing algorithms, we show how to improve upon Gaussian performance for the compression of sparse data: adding a denoising function to a shallow architecture already reduces the loss provably, and a suitable multi-layer decoder leads to a further improvement. We validate our findings on image datasets, such as CIFAR-10 and MNIST.</p></p class="citation"></blockquote><h3 id=35236-a-masked-language-model-for-multi-source-ehr-trajectories-contextual-representation-learning-ali-amirahmadi-et-al-2024>(35/236) A Masked language model for multi-source EHR trajectories contextual representation learning (Ali Amirahmadi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ali Amirahmadi, Mattias Ohlsson, Kobra Etminani, Olle Melander, Jonas Björk. (2024)<br><strong>A Masked language model for multi-source EHR trajectories contextual representation learning</strong><br><button class=copy-to-clipboard title="A Masked language model for multi-source EHR trajectories contextual representation learning" index=35>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-35 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Transformer, Masked Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.06675v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.06675v1.pdf filename=2402.06675v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Using electronic health records data and machine learning to guide future decisions needs to address challenges, including 1) long/short-term dependencies and 2) interactions between diseases and interventions. Bidirectional <b>transformers</b> have effectively addressed the first challenge. Here we tackled the latter challenge by masking one source (e.g., ICD10 codes) and training the <b>transformer</b> to predict it using other sources (e.g., ATC codes).</p></p class="citation"></blockquote><h3 id=36236-on-provable-length-and-compositional-generalization-kartik-ahuja-et-al-2024>(36/236) On Provable Length and Compositional Generalization (Kartik Ahuja et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kartik Ahuja, Amin Mansouri. (2024)<br><strong>On Provable Length and Compositional Generalization</strong><br><button class=copy-to-clipboard title="On Provable Length and Compositional Generalization" index=36>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-36 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CL, cs-LG, cs.LG, stat-ML<br>Keyword Score: 20<br>Keywords: Out-of-distribution, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04875v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04875v1.pdf filename=2402.04875v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Length generalization &ndash; the ability to generalize to longer sequences than ones seen during training, and compositional generalization &ndash; the ability to generalize to token combinations not seen during training, are crucial forms of <b>out-of-distribution</b> generalization in sequence-to-sequence models. In this work, we take the first steps towards provable length and compositional generalization for a range of architectures, including deep sets, <b>transformers,</b> state space models, and simple recurrent neural nets. Depending on the architecture, we prove different degrees of representation identification, e.g., a linear or a permutation relation with ground truth representation, is necessary for length and compositional generalization.</p></p class="citation"></blockquote><h3 id=37236-learning-by-doing-an-online-causal-reinforcement-learning-framework-with-causal-aware-policy-ruichu-cai-et-al-2024>(37/236) Learning by Doing: An Online Causal Reinforcement Learning Framework with Causal-Aware Policy (Ruichu Cai et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ruichu Cai, Siyang Huang, Jie Qiao, Wei Chen, Yan Zeng, Keli Zhang, Fuchun Sun, Yang Yu, Zhifeng Hao. (2024)<br><strong>Learning by Doing: An Online Causal Reinforcement Learning Framework with Causal-Aware Policy</strong><br><button class=copy-to-clipboard title="Learning by Doing: An Online Causal Reinforcement Learning Framework with Causal-Aware Policy" index=37>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-37 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Reinforcement Learning, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04869v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04869v1.pdf filename=2402.04869v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>As a key component to intuitive cognition and <b>reasoning</b> solutions in human intelligence, causal knowledge provides great potential for <b>reinforcement</b> <b>learning</b> (RL) agents&rsquo; interpretability towards decision-making by helping reduce the searching space. However, there is still a considerable gap in discovering and incorporating causality into RL, which hinders the rapid development of causal RL. In this paper, we consider explicitly modeling the generation process of states with the causal graphical model, based on which we augment the policy. We formulate the causal structure updating into the RL interaction process with active intervention learning of the environment. To optimize the derived objective, we propose a framework with theoretical performance guarantees that alternates between two steps: using interventions for causal structure learning during exploration and using the learned causal structure for policy guidance during exploitation. Due to the lack of public benchmarks that allow direct intervention in the state space, we design the root cause localization task in our simulated fault alarm environment and then empirically show the effectiveness and robustness of the proposed method against state-of-the-art baselines. Theoretical analysis shows that our performance improvement attributes to the virtuous cycle of causal-guided policy learning and causal structure learning, which aligns with our experimental results.</p></p class="citation"></blockquote><h3 id=38236-closing-the-gap-between-sgp4-and-high-precision-propagation-via-differentiable-programming-giacomo-acciarini-et-al-2024>(38/236) Closing the Gap Between SGP4 and High-Precision Propagation via Differentiable Programming (Giacomo Acciarini et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Giacomo Acciarini, Atılım Güneş Baydin, Dario Izzo. (2024)<br><strong>Closing the Gap Between SGP4 and High-Precision Propagation via Differentiable Programming</strong><br><button class=copy-to-clipboard title="Closing the Gap Between SGP4 and High-Precision Propagation via Differentiable Programming" index=38>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-38 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: astro-ph-EP, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Fine-tuning, Stochastic Gradient Descent<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04830v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04830v1.pdf filename=2402.04830v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The Simplified General Perturbations 4 (SGP4) orbital propagation method is widely used for predicting the positions and velocities of Earth-orbiting objects rapidly and reliably. Despite continuous refinement, SGP models still lack the precision of numerical propagators, which offer significantly smaller errors. This study presents dSGP4, a novel differentiable version of SGP4 implemented using PyTorch. By making SGP4 differentiable, dSGP4 facilitates various space-related applications, including spacecraft orbit determination, state conversion, covariance transformation, state transition matrix computation, and covariance propagation. Additionally, dSGP4&rsquo;s PyTorch implementation allows for embarrassingly parallel orbital propagation across batches of Two-Line Element Sets (TLEs), leveraging the computational power of CPUs, GPUs, and advanced hardware for distributed prediction of satellite positions at future times. Furthermore, dSGP4&rsquo;s differentiability enables integration with modern machine learning techniques. Thus, we propose a novel orbital propagation paradigm, ML-dSGP4, where neural networks are integrated into the orbital propagator. Through <b>stochastic</b> <b>gradient</b> <b>descent,</b> this combined model&rsquo;s inputs, outputs, and parameters can be iteratively refined, surpassing SGP4&rsquo;s precision. Neural networks act as identity operators by default, adhering to SGP4&rsquo;s behavior. However, dSGP4&rsquo;s differentiability allows <b>fine-tuning</b> with ephemeris data, enhancing precision while maintaining computational speed. This empowers satellite operators and researchers to train the model using specific ephemeris or high-precision numerical propagation data, significantly advancing orbital prediction capabilities.</p></p class="citation"></blockquote><h3 id=39236-designing-deep-neural-networks-for-driver-intention-recognition-koen-vellenga-et-al-2024>(39/236) Designing deep neural networks for driver intention recognition (Koen Vellenga et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Koen Vellenga, H. Joe Steinhauer, Alexander Karlsson, Göran Falkman, Asli Rhodin, Ashok Koppisetty. (2024)<br><strong>Designing deep neural networks for driver intention recognition</strong><br><button class=copy-to-clipboard title="Designing deep neural networks for driver intention recognition" index=39>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-39 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs-NE, cs.LG<br>Keyword Score: 20<br>Keywords: Convolution, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05150v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05150v1.pdf filename=2402.05150v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Driver intention recognition studies increasingly rely on deep neural networks. Deep neural networks have achieved top performance for many different tasks, but it is not a common practice to explicitly analyse the complexity and performance of the network&rsquo;s architecture. Therefore, this paper applies neural architecture search to investigate the effects of the deep neural network architecture on a real-world safety critical application with limited computational capabilities. We explore a pre-defined search space for three deep neural network layer types that are capable to handle sequential data (a long-short term memory, temporal <b>convolution,</b> and a time-series <b>transformer</b> layer), and the influence of different data fusion strategies on the driver intention recognition performance. A set of eight search strategies are evaluated for two driver intention recognition datasets. For the two datasets, we observed that there is no search strategy clearly sampling better deep neural network architectures. However, performing an architecture search does improve the model performance compared to the original manually designed networks. Furthermore, we observe no relation between increased model complexity and higher driver intention recognition performance. The result indicate that multiple architectures yield similar performance, regardless of the deep neural network layer type or fusion strategy.</p></p class="citation"></blockquote><h3 id=40236-progressive-gradient-flow-for-robust-nm-sparsity-training-in-transformers-abhimanyu-rajeshkumar-bambhaniya-et-al-2024>(40/236) Progressive Gradient Flow for Robust N:M Sparsity Training in Transformers (Abhimanyu Rajeshkumar Bambhaniya et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Abhimanyu Rajeshkumar Bambhaniya, Amir Yazdanbakhsh, Suvinay Subramanian, Sheng-Chun Kao, Shivani Agrawal, Utku Evci, Tushar Krishna. (2024)<br><strong>Progressive Gradient Flow for Robust N:M Sparsity Training in Transformers</strong><br><button class=copy-to-clipboard title="Progressive Gradient Flow for Robust N:M Sparsity Training in Transformers" index=40>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-40 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AR, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Transformer, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04744v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04744v1.pdf filename=2402.04744v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>N:M Structured sparsity has garnered significant interest as a result of relatively modest overhead and improved efficiency. Additionally, this form of sparsity holds considerable appeal for reducing the memory footprint owing to their modest representation overhead. There have been efforts to develop training recipes for N:M structured sparsity, they primarily focus on low-sparsity regions ($\sim$50%). Nonetheless, performance of models trained using these approaches tends to decline when confronted with high-sparsity regions ($>$80%). In this work, we study the effectiveness of existing sparse training recipes at \textit{high-sparsity regions} and argue that these methods fail to sustain the model quality on par with low-sparsity regions. We demonstrate that the significant factor contributing to this disparity is the presence of elevated levels of induced noise in the gradient magnitudes. To mitigate this undesirable effect, we employ decay mechanisms to progressively restrict the flow of gradients towards pruned elements. Our approach improves the model quality by up to 2$%$ and 5$%$ in vision and language models at high sparsity regime, respectively. We also evaluate the trade-off between model accuracy and training compute cost in terms of FLOPs. At iso-training FLOPs, our method yields better performance compared to conventional sparse training recipes, exhibiting an accuracy improvement of up to 2$%$. The source code is available at <a href=https://github.com/abhibambhaniya/progressive_gradient_flow_nm_sparsity>https://github.com/abhibambhaniya/progressive_gradient_flow_nm_sparsity</a>.</p></p class="citation"></blockquote><h3 id=41236-incorporating-retrieval-based-causal-learning-with-information-bottlenecks-for-interpretable-graph-neural-networks-jiahua-rao-et-al-2024>(41/236) Incorporating Retrieval-based Causal Learning with Information Bottlenecks for Interpretable Graph Neural Networks (Jiahua Rao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiahua Rao, Jiancong Xie, Hanjing Lin, Shuangjia Zheng, Zhen Wang, Yuedong Yang. (2024)<br><strong>Incorporating Retrieval-based Causal Learning with Information Bottlenecks for Interpretable Graph Neural Networks</strong><br><button class=copy-to-clipboard title="Incorporating Retrieval-based Causal Learning with Information Bottlenecks for Interpretable Graph Neural Networks" index=41>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-41 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Graph Neural Network, Graph Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04710v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04710v1.pdf filename=2402.04710v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Graph</b> <b>Neural</b> <b>Networks</b> <b>(GNNs)</b> have gained considerable traction for their capability to effectively process topological data, yet their interpretability remains a critical concern. Current interpretation methods are dominated by post-hoc explanations to provide a transparent and intuitive understanding of <b>GNNs.</b> However, they have limited performance in interpreting complicated subgraphs and can&rsquo;t utilize the explanation to advance <b>GNN</b> predictions. On the other hand, transparent <b>GNN</b> models are proposed to capture critical subgraphs. While such methods could improve <b>GNN</b> predictions, they usually don&rsquo;t perform well on explanations. Thus, it is desired for a new strategy to better couple <b>GNN</b> explanation and prediction. In this study, we have developed a novel interpretable causal <b>GNN</b> framework that incorporates retrieval-based causal learning with <b>Graph</b> <b>Information</b> <b>Bottleneck</b> (GIB) theory. The framework could semi-parametrically retrieve crucial subgraphs detected by GIB and compress the explanatory subgraphs via a causal module. The framework was demonstrated to consistently outperform state-of-the-art methods, and to achieve 32.71% higher precision on real-world explanation scenarios with diverse explanation types. More importantly, the learned explanations were shown able to also improve <b>GNN</b> prediction performance.</p></p class="citation"></blockquote><h3 id=42236-group-distributionally-robust-dataset-distillation-with-risk-minimization-saeed-vahidian-et-al-2024>(42/236) Group Distributionally Robust Dataset Distillation with Risk Minimization (Saeed Vahidian et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Saeed Vahidian, Mingyu Wang, Jianyang Gu, Vyacheslav Kungurtsev, Wei Jiang, Yiran Chen. (2024)<br><strong>Group Distributionally Robust Dataset Distillation with Risk Minimization</strong><br><button class=copy-to-clipboard title="Group Distributionally Robust Dataset Distillation with Risk Minimization" index=42>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-42 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CV, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Knowledge Distillation, Transfer Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04676v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04676v1.pdf filename=2402.04676v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Dataset <b>distillation</b> (DD) has emerged as a widely adopted technique for crafting a synthetic dataset that captures the essential information of a training dataset, facilitating the training of accurate neural models. Its applications span various domains, including <b>transfer</b> <b>learning,</b> federated learning, and neural architecture search. The most popular methods for constructing the synthetic data rely on matching the convergence properties of training the model with the synthetic dataset and the training dataset. However, targeting the training dataset must be thought of as auxiliary in the same sense that the training set is an approximate substitute for the population distribution, and the latter is the data of interest. Yet despite its popularity, an aspect that remains unexplored is the relationship of DD to its generalization, particularly across uncommon subgroups. That is, how can we ensure that a model trained on the synthetic dataset performs well when faced with samples from regions with low population density? Here, the representativeness and coverage of the dataset become salient over the guaranteed training error at inference. Drawing inspiration from distributionally robust optimization, we introduce an algorithm that combines clustering with the minimization of a risk measure on the loss to conduct DD. We provide a theoretical rationale for our approach and demonstrate its effective generalization and robustness across subgroups through numerical experiments.</p></p class="citation"></blockquote><h3 id=43236-compressing-deep-reinforcement-learning-networks-with-a-dynamic-structured-pruning-method-for-autonomous-driving-wensheng-su-et-al-2024>(43/236) Compressing Deep Reinforcement Learning Networks with a Dynamic Structured Pruning Method for Autonomous Driving (Wensheng Su et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wensheng Su, Zhenni Li, Minrui Xu, Jiawen Kang, Dusit Niyato, Shengli Xie. (2024)<br><strong>Compressing Deep Reinforcement Learning Networks with a Dynamic Structured Pruning Method for Autonomous Driving</strong><br><button class=copy-to-clipboard title="Compressing Deep Reinforcement Learning Networks with a Dynamic Structured Pruning Method for Autonomous Driving" index=43>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-43 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs-RO, cs.LG<br>Keyword Score: 20<br>Keywords: Pruning, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05146v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05146v1.pdf filename=2402.05146v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep <b>reinforcement</b> <b>learning</b> (DRL) has shown remarkable success in complex autonomous driving scenarios. However, DRL models inevitably bring high memory consumption and computation, which hinders their wide deployment in resource-limited autonomous driving devices. Structured <b>Pruning</b> has been recognized as a useful method to compress and accelerate DRL models, but it is still challenging to estimate the contribution of a parameter (i.e., neuron) to DRL models. In this paper, we introduce a novel dynamic structured <b>pruning</b> approach that gradually removes a DRL model&rsquo;s unimportant neurons during the training stage. Our method consists of two steps, i.e. training DRL models with a group sparse regularizer and removing unimportant neurons with a dynamic <b>pruning</b> threshold. To efficiently train the DRL model with a small number of important neurons, we employ a neuron-importance group sparse regularizer. In contrast to conventional regularizers, this regularizer imposes a penalty on redundant groups of neurons that do not significantly influence the output of the DRL model. Furthermore, we design a novel structured <b>pruning</b> strategy to dynamically determine the <b>pruning</b> threshold and gradually remove unimportant neurons with a binary mask. Therefore, our method can remove not only redundant groups of neurons of the DRL model but also achieve high and robust performance. Experimental results show that the proposed method is competitive with existing DRL <b>pruning</b> methods on discrete control environments (i.e., CartPole-v1 and LunarLander-v2) and MuJoCo continuous environments (i.e., Hopper-v3 and Walker2D-v3). Specifically, our method effectively compresses $93%$ neurons and $96%$ weights of the DRL model in four challenging DRL environments with slight accuracy degradation.</p></p class="citation"></blockquote><h3 id=44236-online-learning-approach-for-survival-analysis-camila-fernandez-et-al-2024>(44/236) Online Learning Approach for Survival Analysis (Camila Fernandez et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Camila Fernandez, Pierre Gaillard, Joseph de Vilmarest, Olivier Wintenberger. (2024)<br><strong>Online Learning Approach for Survival Analysis</strong><br><button class=copy-to-clipboard title="Online Learning Approach for Survival Analysis" index=44>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-44 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, physics-data-an, stat-ML<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05145v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05145v1.pdf filename=2402.05145v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce an online mathematical framework for survival analysis, allowing real time adaptation to dynamic environments and censored data. This framework enables the estimation of event time distributions through an optimal second order online convex optimization algorithm-Online Newton Step (ONS). This approach, previously unexplored, presents substantial advantages, including explicit algorithms with non-asymptotic convergence guarantees. Moreover, we analyze the selection of ONS hyperparameters, which depends on the exp-concavity property and has a significant influence on the regret bound. We propose a stochastic approach that guarantees logarithmic stochastic regret for ONS. Additionally, we introduce an adaptive aggregation method that ensures robustness in hyperparameter selection while maintaining fast regret bounds. The findings of this paper can extend beyond the survival analysis field, and are relevant for any case characterized by poor exp-concavity and unstable ONS. Finally, these assertions are illustrated by <b>simulation</b> experiments.</p></p class="citation"></blockquote><h3 id=45236-curvature-informed-sgd-via-general-purpose-lie-group-preconditioners-omead-pooladzandi-et-al-2024>(45/236) Curvature-Informed SGD via General Purpose Lie-Group Preconditioners (Omead Pooladzandi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Omead Pooladzandi, Xi-Lin Li. (2024)<br><strong>Curvature-Informed SGD via General Purpose Lie-Group Preconditioners</strong><br><button class=copy-to-clipboard title="Curvature-Informed SGD via General Purpose Lie-Group Preconditioners" index=45>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-45 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Stochastic Gradient Descent, Stochastic Gradient Descent<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04553v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04553v1.pdf filename=2402.04553v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present a novel approach to accelerate <b>stochastic</b> <b>gradient</b> <b>descent</b> <b>(SGD)</b> by utilizing curvature information obtained from Hessian-vector products or finite differences of parameters and gradients, similar to the BFGS algorithm. Our approach involves two preconditioners: a matrix-free preconditioner and a low-rank approximation preconditioner. We update both preconditioners online using a criterion that is robust to <b>stochastic</b> <b>gradient</b> <b>noise</b> and does not require line search or damping. To preserve the corresponding symmetry or invariance, our preconditioners are constrained to certain connected Lie groups. The Lie group&rsquo;s equivariance property simplifies the preconditioner fitting process, while its invariance property eliminates the need for damping, which is commonly required in second-order optimizers. As a result, the learning rate for parameter updating and the step size for preconditioner fitting are naturally normalized, and their default values work well in most scenarios. Our proposed approach offers a promising direction for improving the convergence of <b>SGD</b> with low computational overhead. We demonstrate that Preconditioned <b>SGD</b> (PSGD) outperforms SoTA on Vision, NLP, and RL tasks across multiple modern deep-learning architectures. We have provided code for reproducing toy and large scale experiments in this paper.</p></p class="citation"></blockquote><h3 id=46236-triplet-interaction-improves-graph-transformers-accurate-molecular-graph-learning-with-triplet-graph-transformers-md-shamim-hussain-et-al-2024>(46/236) Triplet Interaction Improves Graph Transformers: Accurate Molecular Graph Learning with Triplet Graph Transformers (Md Shamim Hussain et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Md Shamim Hussain, Mohammed J. Zaki, Dharmashankar Subramanian. (2024)<br><strong>Triplet Interaction Improves Graph Transformers: Accurate Molecular Graph Learning with Triplet Graph Transformers</strong><br><button class=copy-to-clipboard title="Triplet Interaction Improves Graph Transformers: Accurate Molecular Graph Learning with Triplet Graph Transformers" index=46>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-46 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Transfer Learning, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04538v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04538v1.pdf filename=2402.04538v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Graph <b>transformers</b> typically lack direct pair-to-pair communication, instead forcing neighboring pairs to exchange information via a common node. We propose the Triplet Graph <b>Transformer</b> (TGT) that enables direct communication between two neighboring pairs in a graph via novel triplet attention and aggregation mechanisms. TGT is applied to molecular property prediction by first predicting interatomic distances from 2D graphs and then using these distances for downstream tasks. A novel three-stage training procedure and stochastic inference further improve training efficiency and model performance. Our model achieves new state-of-the-art (SOTA) results on open challenge benchmarks PCQM4Mv2 and OC20 IS2RE. We also obtain SOTA results on QM9, MOLPCBA, and LIT-PCBA molecular property prediction benchmarks via <b>transfer</b> <b>learning.</b> We also demonstrate the generality of TGT with SOTA results on the traveling salesman problem (TSP).</p></p class="citation"></blockquote><h3 id=47236-online-cascade-learning-for-efficient-inference-over-streams-lunyiu-nie-et-al-2024>(47/236) Online Cascade Learning for Efficient Inference over Streams (Lunyiu Nie et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lunyiu Nie, Zhimin Ding, Erdong Hu, Christopher Jermaine, Swarat Chaudhuri. (2024)<br><strong>Online Cascade Learning for Efficient Inference over Streams</strong><br><button class=copy-to-clipboard title="Online Cascade Learning for Efficient Inference over Streams" index=47>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-47 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CL, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04513v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04513v1.pdf filename=2402.04513v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have a natural role in answering complex queries about data streams, but the high computational cost of <b>LLM</b> inference makes them infeasible in many such tasks. We propose online cascade learning, the first approach to addressing this challenge. The objective here is to learn a &ldquo;cascade&rdquo; of models, starting with lower-capacity models (such as logistic regressors) and ending with a powerful <b>LLM,</b> along with a deferral policy that determines the model that is used on a given input. We formulate the task of learning cascades online as an imitation-learning problem and give a no-regret algorithm for the problem. Experimental results across four benchmarks show that our method parallels <b>LLMs</b> in accuracy while cutting down inference costs by as much as 90%, underscoring its efficacy and adaptability in stream processing.</p></p class="citation"></blockquote><h3 id=48236-the-fine-grained-complexity-of-gradient-computation-for-training-large-language-models-josh-alman-et-al-2024>(48/236) The Fine-Grained Complexity of Gradient Computation for Training Large Language Models (Josh Alman et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Josh Alman, Zhao Song. (2024)<br><strong>The Fine-Grained Complexity of Gradient Computation for Training Large Language Models</strong><br><button class=copy-to-clipboard title="The Fine-Grained Complexity of Gradient Computation for Training Large Language Models" index=48>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-48 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CC, cs-CL, cs-DS, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04497v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04497v1.pdf filename=2402.04497v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have made fundamental contributions over the last a few years. To train an <b>LLM,</b> one needs to alternatingly run <code>forward' computations and </code>backward&rsquo; computations. The forward computation can be viewed as attention function evaluation, and the backward computation can be viewed as a gradient computation. In previous work by [Alman and Song, NeurIPS 2023], it was proved that the forward step can be performed in almost-linear time in certain parameter regimes, but that there is no truly sub-quadratic time algorithm in the remaining parameter regimes unless the popular hypothesis SETH is false. In this work, we show nearly identical results for the harder-seeming problem of computing the gradient of loss function of one layer attention network, and thus for the entire process of <b>LLM</b> training. This completely characterizes the fine-grained complexity of every step of <b>LLM</b> training.</p></p class="citation"></blockquote><h3 id=49236-a-comparative-study-on-feature-selection-for-a-risk-prediction-model-for-colorectal-cancer-n-cueto-lópez-et-al-2024>(49/236) A comparative study on feature selection for a risk prediction model for colorectal cancer (N. Cueto-López et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>N. Cueto-López, M. T. García-Ordás, V. Dávila-Batista, V. Moreno, N. Aragonés, R. Alaiz-Rodríguez. (2024)<br><strong>A comparative study on feature selection for a risk prediction model for colorectal cancer</strong><br><button class=copy-to-clipboard title="A comparative study on feature selection for a risk prediction model for colorectal cancer" index=49>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-49 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Logistic Regression<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05293v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05293v1.pdf filename=2402.05293v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Background and objective Risk prediction models aim at identifying people at higher risk of developing a target disease. Feature selection is particularly important to improve the prediction model performance avoiding overfitting and to identify the leading cancer risk (and protective) factors. Assessing the stability of feature selection/ranking algorithms becomes an important issue when the aim is to analyze the features with more prediction power. Methods This work is focused on colorectal cancer, assessing several feature ranking algorithms in terms of performance for a set of risk prediction models (Neural Networks, Support Vector Machines (SVM), <b>Logistic</b> <b>Regression,</b> k-Nearest Neighbors and Boosted Trees). Additionally, their robustness is evaluated following a conventional approach with scalar stability metrics and a visual approach proposed in this work to study both similarity among feature ranking techniques as well as their individual stability. A comparative analysis is carried out between the most relevant features found out in this study and features provided by the experts according to the state-of-the-art knowledge. Results The two best performance results in terms of Area Under the ROC Curve (AUC) are achieved with a SVM classifier using the top-41 features selected by the SVM wrapper approach (AUC=0.693) and <b>Logistic</b> <b>Regression</b> with the top-40 features selected by the Pearson (AUC=0.689). Experiments showed that performing feature selection contributes to classification performance with a 3.9% and 1.9% improvement in AUC for the SVM and <b>Logistic</b> <b>Regression</b> classifier, respectively, with respect to the results using the full feature set. The visual approach proposed in this work allows to see that the Neural Network-based wrapper ranking is the most unstable while the Random Forest is the most stable.</p></p class="citation"></blockquote><h3 id=50236-analyzing-adversarial-inputs-in-deep-reinforcement-learning-davide-corsi-et-al-2024>(50/236) Analyzing Adversarial Inputs in Deep Reinforcement Learning (Davide Corsi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Davide Corsi, Guy Amir, Guy Katz, Alessandro Farinelli. (2024)<br><strong>Analyzing Adversarial Inputs in Deep Reinforcement Learning</strong><br><button class=copy-to-clipboard title="Analyzing Adversarial Inputs in Deep Reinforcement Learning" index=50>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-50 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05284v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05284v1.pdf filename=2402.05284v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In recent years, Deep <b>Reinforcement</b> <b>Learning</b> (DRL) has become a popular paradigm in machine learning due to its successful applications to real-world and complex systems. However, even the state-of-the-art DRL models have been shown to suffer from reliability concerns &ndash; for example, their susceptibility to adversarial inputs, i.e., small and abundant input perturbations that can fool the models into making unpredictable and potentially dangerous decisions. This drawback limits the deployment of DRL systems in safety-critical contexts, where even a small error cannot be tolerated. In this work, we present a comprehensive analysis of the characterization of adversarial inputs, through the lens of formal verification. Specifically, we introduce a novel metric, the Adversarial Rate, to classify models based on their susceptibility to such perturbations, and present a set of tools and algorithms for its computation. Our analysis empirically demonstrates how adversarial inputs can affect the safety of a given DRL system with respect to such perturbations. Moreover, we analyze the behavior of these configurations to suggest several useful practices and guidelines to help mitigate the vulnerability of trained DRL networks.</p></p class="citation"></blockquote><h3 id=51236-convergence-for-natural-policy-gradient-on-infinite-state-average-reward-markov-decision-processes-isaac-grosof-et-al-2024>(51/236) Convergence for Natural Policy Gradient on Infinite-State Average-Reward Markov Decision Processes (Isaac Grosof et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Isaac Grosof, Siva Theja Maguluri, R. Srikant. (2024)<br><strong>Convergence for Natural Policy Gradient on Infinite-State Average-Reward Markov Decision Processes</strong><br><button class=copy-to-clipboard title="Convergence for Natural Policy Gradient on Infinite-State Average-Reward Markov Decision Processes" index=51>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-51 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05274v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05274v1.pdf filename=2402.05274v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Infinite-state Markov Decision Processes (MDPs) are essential in modeling and optimizing a wide variety of engineering problems. In the <b>reinforcement</b> <b>learning</b> (RL) context, a variety of algorithms have been developed to learn and optimize these MDPs. At the heart of many popular policy-gradient based learning algorithms, such as natural actor-critic, TRPO, and PPO, lies the Natural Policy Gradient (NPG) algorithm. Convergence results for these RL algorithms rest on convergence results for the NPG algorithm. However, all existing results on the convergence of the NPG algorithm are limited to finite-state settings. We prove the first convergence rate bound for the NPG algorithm for infinite-state average-reward MDPs, proving a $O(1/\sqrt{T})$ convergence rate, if the NPG algorithm is initialized with a good initial policy. Moreover, we show that in the context of a large class of queueing MDPs, the MaxWeight policy suffices to satisfy our initial-policy requirement and achieve a $O(1/\sqrt{T})$ convergence rate. Key to our result are state-dependent bounds on the relative value function achieved by the iterate policies of the NPG algorithm.</p></p class="citation"></blockquote><h3 id=52236-qgfn-controllable-greediness-with-action-values-elaine-lau-et-al-2024>(52/236) QGFN: Controllable Greediness with Action Values (Elaine Lau et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Elaine Lau, Stephen Zhewen Lu, Ling Pan, Doina Precup, Emmanuel Bengio. (2024)<br><strong>QGFN: Controllable Greediness with Action Values</strong><br><button class=copy-to-clipboard title="QGFN: Controllable Greediness with Action Values" index=52>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-52 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05234v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05234v1.pdf filename=2402.05234v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Generative Flow Networks (GFlowNets; GFNs) are a family of reward/energy-based generative methods for combinatorial objects, capable of generating diverse and high-utility samples. However, biasing GFNs towards producing high-utility samples is non-trivial. In this work, we leverage connections between GFNs and <b>reinforcement</b> <b>learning</b> (RL) and propose to combine the GFN policy with an action-value estimate, $Q$, to create greedier sampling policies which can be controlled by a mixing parameter. We show that several variants of the proposed method, QGFN, are able to improve on the number of high-reward samples generated in a variety of tasks without sacrificing diversity.</p></p class="citation"></blockquote><h3 id=53236-hydra-sequentially-dependent-draft-heads-for-medusa-decoding-zachary-ankner-et-al-2024>(53/236) Hydra: Sequentially-Dependent Draft Heads for Medusa Decoding (Zachary Ankner et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zachary Ankner, Rishab Parthasarathy, Aniruddha Nrusimha, Christopher Rinard, Jonathan Ragan-Kelley, William Brandon. (2024)<br><strong>Hydra: Sequentially-Dependent Draft Heads for Medusa Decoding</strong><br><button class=copy-to-clipboard title="Hydra: Sequentially-Dependent Draft Heads for Medusa Decoding" index=53>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-53 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05109v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05109v1.pdf filename=2402.05109v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>To combat the memory bandwidth-bound nature of autoregressive <b>LLM</b> inference, previous research has proposed the speculative decoding framework. To perform speculative decoding, a small draft model proposes candidate continuations of the input sequence, that are then verified in parallel by the base model. One way to specify the draft model, as used in the recent Medusa decoding framework, is as a collection of light-weight heads, called draft heads, that operate on the base model&rsquo;s hidden states. To date, all existing draft heads have been sequentially independent, meaning that they speculate tokens in the candidate continuation independently of any preceding tokens in the candidate continuation. In this work, we propose Hydra heads, a sequentially dependent, drop-in replacement for standard draft heads that significantly improves speculation accuracy. Decoding with Hydra heads improves throughput compared to Medusa decoding with standard draft heads. We further explore the design space of Hydra head training objectives and architectures, and propose a carefully-tuned Hydra head recipe, which we call Hydra++, that improves decoding throughput by 1.31x and 2.71x compared to Medusa decoding and autoregressive decoding, respectively. Overall, Hydra heads are a simple intervention on standard draft heads that significantly improve the end-to-end speed of draft head based speculative decoding.</p></p class="citation"></blockquote><h3 id=54236-a-resource-model-for-neural-scaling-law-jinyeop-song-et-al-2024>(54/236) A Resource Model For Neural Scaling Law (Jinyeop Song et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jinyeop Song, Ziming Liu, Max Tegmark, Jeff Gore. (2024)<br><strong>A Resource Model For Neural Scaling Law</strong><br><button class=copy-to-clipboard title="A Resource Model For Neural Scaling Law" index=54>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-54 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs-NE, cs.LG<br>Keyword Score: 10<br>Keywords: Scaling Law<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05164v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05164v1.pdf filename=2402.05164v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Neural <b>scaling</b> <b>laws</b> characterize how model performance improves as the model size scales up. Inspired by empirical observations, we introduce a resource model of neural <b>scaling.</b> <b>A</b> task is usually composite hence can be decomposed into many subtasks, which compete for resources (measured by the number of neurons allocated to subtasks). On toy problems, we empirically find that: (1) The loss of a subtask is inversely proportional to its allocated neurons. (2) When multiple subtasks are present in a composite task, the resources acquired by each subtask uniformly grow as models get larger, keeping the ratios of acquired resources constants. We hypothesize these findings to be generally true and build a model to predict neural <b>scaling</b> <b>laws</b> for general composite tasks, which successfully replicates the neural <b>scaling</b> <b>law</b> of Chinchilla models reported in arXiv:2203.15556. We believe that the notion of resource used in this paper will be a useful tool for characterizing and diagnosing neural networks.</p></p class="citation"></blockquote><h3 id=55236-simulated-overparameterization-hanna-mazzawi-et-al-2024>(55/236) Simulated Overparameterization (Hanna Mazzawi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hanna Mazzawi, Pranjal Awasthi, Xavi Gonzalvo, Srikumar Ramalingam. (2024)<br><strong>Simulated Overparameterization</strong><br><button class=copy-to-clipboard title="Simulated Overparameterization" index=55>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-55 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05033v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05033v1.pdf filename=2402.05033v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this work, we introduce a novel paradigm called Simulated Overparametrization (SOP). SOP merges the computational efficiency of compact models with the advanced learning proficiencies of overparameterized models. SOP proposes a unique approach to model training and inference, where a model with a significantly larger number of parameters is trained in such a way that a smaller, efficient subset of these parameters is used for the actual computation during inference. Building upon this framework, we present a novel, architecture agnostic algorithm called &ldquo;majority kernels&rdquo;, which seamlessly integrates with predominant architectures, including <b>Transformer</b> models. Majority kernels enables the simulated training of overparameterized models, resulting in performance gains across architectures and tasks. Furthermore, our approach adds minimal overhead to the cost incurred (wall clock time) at training time. The proposed approach shows strong performance on a wide variety of datasets and models, even outperforming strong baselines such as combinatorial optimization methods based on submodular optimization.</p></p class="citation"></blockquote><h3 id=56236-moco-a-learnable-meta-optimizer-for-combinatorial-optimization-tim-dernedde-et-al-2024>(56/236) Moco: A Learnable Meta Optimizer for Combinatorial Optimization (Tim Dernedde et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tim Dernedde, Daniela Thyssens, Sören Dittrich, Maximilian Stubbemann, Lars Schmidt-Thieme. (2024)<br><strong>Moco: A Learnable Meta Optimizer for Combinatorial Optimization</strong><br><button class=copy-to-clipboard title="Moco: A Learnable Meta Optimizer for Combinatorial Optimization" index=56>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-56 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Graph Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04915v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04915v2.pdf filename=2402.04915v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Relevant combinatorial optimization problems (COPs) are often NP-hard. While they have been tackled mainly via handcrafted heuristics in the past, advances in neural networks have motivated the development of general methods to learn heuristics from data. Many approaches utilize a neural network to directly construct a solution, but are limited in further improving based on already constructed solutions at inference time. Our approach, Moco, learns a <b>graph</b> <b>neural</b> <b>network</b> that updates the solution construction procedure based on features extracted from the current search state. This meta training procedure targets the overall best solution found during the search procedure given information such as the search budget. This allows Moco to adapt to varying circumstances such as different computational budgets. Moco is a fully learnable meta optimizer that does not utilize any problem specific local search or decomposition. We test Moco on the Traveling Salesman Problem (TSP) and Maximum Independent Set (MIS) and show that it outperforms other approaches on MIS and is overall competitive on the TSP, especially outperforming related approaches, partially even if they use additional local search.</p></p class="citation"></blockquote><h3 id=57236-on-the-completeness-of-invariant-geometric-deep-learning-models-zian-li-et-al-2024>(57/236) On the Completeness of Invariant Geometric Deep Learning Models (Zian Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zian Li, Xiyuan Wang, Shijia Kang, Muhan Zhang. (2024)<br><strong>On the Completeness of Invariant Geometric Deep Learning Models</strong><br><button class=copy-to-clipboard title="On the Completeness of Invariant Geometric Deep Learning Models" index=57>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-57 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Message-Passing<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04836v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04836v1.pdf filename=2402.04836v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Invariant models, one important class of geometric deep learning models, are capable of generating meaningful geometric representations by leveraging informative geometric features. These models are characterized by their simplicity, good experimental results and computational efficiency. However, their theoretical expressive power still remains unclear, restricting a deeper understanding of the potential of such models. In this work, we concentrate on characterizing the theoretical expressiveness of invariant models. We first rigorously bound the expressiveness of the most classical invariant model, Vanilla DisGNN (message passing neural networks incorporating distance), restricting its unidentifiable cases to be only those highly symmetric geometric graphs. To break these corner cases&rsquo; symmetry, we introduce a simple yet E(3)-complete invariant design by nesting Vanilla DisGNN, named GeoNGNN. Leveraging GeoNGNN as a theoretical tool, we for the first time prove the E(3)-completeness of three well-established geometric models: DimeNet, GemNet and SphereNet. Our results fill the gap in the theoretical power of invariant models, contributing to a rigorous and comprehensive understanding of their capabilities. Experimentally, GeoNGNN exhibits good inductive bias in capturing local environments, and achieves competitive results w.r.t. complicated models relying on high-order invariant/equivariant representations while exhibiting significantly faster computational speed.</p></p class="citation"></blockquote><h3 id=58236-e3-equivariant-mesh-neural-networks-thuan-trang-et-al-2024>(58/236) E(3)-Equivariant Mesh Neural Networks (Thuan Trang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Thuan Trang, Nhat Khang Ngo, Daniel Levy, Thieu N. Vo, Siamak Ravanbakhsh, Truong Son Hy. (2024)<br><strong>E(3)-Equivariant Mesh Neural Networks</strong><br><button class=copy-to-clipboard title="E(3)-Equivariant Mesh Neural Networks" index=58>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-58 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Graph Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04821v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04821v1.pdf filename=2402.04821v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Triangular meshes are widely used to represent three-dimensional objects. As a result, many recent works have address the need for geometric deep learning on 3D mesh. However, we observe that the complexities in many of these architectures does not translate to practical performance, and simple deep models for geometric <b>graphs</b> <b>are</b> <b>competitive</b> in practice. Motivated by this observation, we minimally extend the update equations of E(n)-Equivariant <b>Graph</b> <b>Neural</b> <b>Networks</b> (EGNNs) (Satorras et al., 2021) to incorporate mesh face information, and further improve it to account for long-range interactions through hierarchy. The resulting architecture, Equivariant Mesh Neural Network (EMNN), outperforms other, more complicated equivariant methods on mesh tasks, with a fast run-time and no expensive pre-processing.</p></p class="citation"></blockquote><h3 id=59236-flowpg-action-constrained-policy-gradient-with-normalizing-flows-janaka-chathuranga-brahmanage-et-al-2024>(59/236) FlowPG: Action-constrained Policy Gradient with Normalizing Flows (Janaka Chathuranga Brahmanage et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Janaka Chathuranga Brahmanage, Jiajing Ling, Akshat Kumar. (2024)<br><strong>FlowPG: Action-constrained Policy Gradient with Normalizing Flows</strong><br><button class=copy-to-clipboard title="FlowPG: Action-constrained Policy Gradient with Normalizing Flows" index=59>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-59 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05149v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05149v1.pdf filename=2402.05149v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Action-constrained <b>reinforcement</b> <b>learning</b> (ACRL) is a popular approach for solving safety-critical and resource-allocation related decision making problems. A major challenge in ACRL is to ensure agent taking a valid action satisfying constraints in each RL step. Commonly used approach of using a projection layer on top of the policy network requires solving an optimization program which can result in longer training time, slow convergence, and zero gradient problem. To address this, first we use a normalizing flow model to learn an invertible, differentiable mapping between the feasible action space and the support of a simple distribution on a latent variable, such as Gaussian. Second, learning the flow model requires sampling from the feasible action space, which is also challenging. We develop multiple methods, based on Hamiltonian Monte-Carlo and probabilistic sentential decision diagrams for such action sampling for convex and non-convex constraints. Third, we integrate the learned normalizing flow with the DDPG algorithm. By design, a well-trained normalizing flow will transform policy output into a valid action without requiring an optimization solver. Empirically, our approach results in significantly fewer constraint violations (upto an order-of-magnitude for several instances) and is multiple times faster on a variety of continuous control tasks.</p></p class="citation"></blockquote><h3 id=60236-a-perspective-on-individualized-treatment-effects-estimation-from-time-series-health-data-ghadeer-o-ghosheh-et-al-2024>(60/236) A Perspective on Individualized Treatment Effects Estimation from Time-series Health Data (Ghadeer O. Ghosheh et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ghadeer O. Ghosheh, Moritz Gögl, Tingting Zhu. (2024)<br><strong>A Perspective on Individualized Treatment Effects Estimation from Time-series Health Data</strong><br><button class=copy-to-clipboard title="A Perspective on Individualized Treatment Effects Estimation from Time-series Health Data" index=60>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-60 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04668v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04668v1.pdf filename=2402.04668v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The burden of diseases is rising worldwide, with unequal treatment efficacy for patient populations that are underrepresented in clinical trials. Healthcare, however, is driven by the average population effect of medical treatments and, therefore, operates in a &ldquo;one-size-fits-all&rdquo; approach, not necessarily what best fits each patient. These facts suggest a pressing need for methodologies to study individualized treatment effects (ITE) to drive personalized treatment. Despite the increased interest in machine-learning-driven ITE estimation models, the vast majority focus on tabular data with limited review and understanding of methodologies proposed for time-series electronic health records (EHRs). To this end, this work provides an overview of ITE works for time-series data and insights into future research. The work <b>summarizes</b> the latest work in the literature and reviews it in light of theoretical assumptions, types of treatment settings, and computational frameworks. Furthermore, this work discusses challenges and future research directions for ITEs in a time-series setting. We hope this work opens new directions and serves as a resource for understanding one of the exciting yet under-studied research areas.</p></p class="citation"></blockquote><h3 id=61236-towards-improved-imbalance-robustness-in-continual-multi-label-learning-with-dual-output-spiking-architecture-dosa-sourav-mishra-et-al-2024>(61/236) Towards Improved Imbalance Robustness in Continual Multi-Label Learning with Dual Output Spiking Architecture (DOSA) (Sourav Mishra et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sourav Mishra, Shirin Dora, Suresh Sundaram. (2024)<br><strong>Towards Improved Imbalance Robustness in Continual Multi-Label Learning with Dual Output Spiking Architecture (DOSA)</strong><br><button class=copy-to-clipboard title="Towards Improved Imbalance Robustness in Continual Multi-Label Learning with Dual Output Spiking Architecture (DOSA)" index=61>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-61 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CV, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04596v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04596v1.pdf filename=2402.04596v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Algorithms designed for addressing typical <b>supervised</b> classification problems can only learn from a fixed set of samples and labels, making them unsuitable for the real world, where data arrives as a stream of samples often associated with multiple labels over time. This motivates the study of task-agnostic continual multi-label learning problems. While algorithms using deep learning approaches for continual multi-label learning have been proposed in the recent literature, they tend to be computationally heavy. Although spiking neural networks (SNNs) offer a computationally efficient alternative to artificial neural networks, existing literature has not used SNNs for continual multi-label learning. Also, accurately determining multiple labels with SNNs is still an open research problem. This work proposes a dual output spiking architecture (DOSA) to bridge these research gaps. A novel imbalance-aware loss function is also proposed, improving the multi-label classification performance of the model by making it more robust to data imbalance. A modified F1 score is presented to evaluate the effectiveness of the proposed loss function in handling imbalance. Experiments on several benchmark multi-label datasets show that DOSA trained with the proposed loss function shows improved robustness to data imbalance and obtains better continual multi-label learning performance than CIFDM, a previous state-of-the-art algorithm.</p></p class="citation"></blockquote><h3 id=62236-learning-diverse-policies-with-soft-self-generated-guidance-guojian-wang-et-al-2024>(62/236) Learning Diverse Policies with Soft Self-Generated Guidance (Guojian Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Guojian Wang, Faguo Wu, Xiao Zhang, Jianxiang Liu. (2024)<br><strong>Learning Diverse Policies with Soft Self-Generated Guidance</strong><br><button class=copy-to-clipboard title="Learning Diverse Policies with Soft Self-Generated Guidance" index=62>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-62 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04539v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04539v1.pdf filename=2402.04539v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Reinforcement</b> <b>learning</b> (RL) with sparse and deceptive rewards is challenging because non-zero rewards are rarely obtained. Hence, the gradient calculated by the agent can be stochastic and without valid information. Recent studies that utilize memory buffers of previous experiences can lead to a more efficient learning process. However, existing methods often require these experiences to be successful and may overly exploit them, which can cause the agent to adopt suboptimal behaviors. This paper develops an approach that uses diverse past trajectories for faster and more efficient online RL, even if these trajectories are suboptimal or not highly rewarded. The proposed algorithm combines a policy improvement step with an additional exploration step using offline demonstration data. The main contribution of this paper is that by regarding diverse past trajectories as guidance, instead of imitating them, our method directs its policy to follow and expand past trajectories while still being able to learn without rewards and approach optimality. Furthermore, a novel diversity measurement is introduced to maintain the team&rsquo;s diversity and regulate exploration. The proposed algorithm is evaluated on discrete and continuous control tasks with sparse and deceptive rewards. Compared with the existing RL methods, the experimental results indicate that our proposed algorithm is significantly better than the baseline methods regarding diverse exploration and avoiding local optima.</p></p class="citation"></blockquote><h3 id=63236-incentivized-truthful-communication-for-federated-bandits-zhepei-wei-et-al-2024>(63/236) Incentivized Truthful Communication for Federated Bandits (Zhepei Wei et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhepei Wei, Chuanhao Li, Tianze Ren, Haifeng Xu, Hongning Wang. (2024)<br><strong>Incentivized Truthful Communication for Federated Bandits</strong><br><button class=copy-to-clipboard title="Incentivized Truthful Communication for Federated Bandits" index=63>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-63 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-GT, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Bandit Algorithm<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04485v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04485v1.pdf filename=2402.04485v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>To enhance the efficiency and practicality of federated <b>bandit</b> learning, recent advances have introduced incentives to motivate communication among clients, where a client participates only when the incentive offered by the server outweighs its participation cost. However, existing incentive mechanisms naively assume the clients are truthful: they all report their true cost and thus the higher cost one participating client claims, the more the server has to pay. Therefore, such mechanisms are vulnerable to strategic clients aiming to optimize their own utility by misreporting. To address this issue, we propose an incentive compatible (i.e., truthful) communication protocol, named Truth-FedBan, where the incentive for each participant is independent of its self-reported cost, and reporting the true cost is the only way to achieve the best utility. More importantly, Truth-FedBan still guarantees the sub-linear regret and communication cost without any overheads. In other words, the core conceptual contribution of this paper is, for the first time, demonstrating the possibility of simultaneously achieving incentive compatibility and nearly optimal regret in federated <b>bandit</b> learning. Extensive numerical studies further validate the effectiveness of our proposed solution.</p></p class="citation"></blockquote><h3 id=64236-learning-on-multimodal-graphs-a-survey-ciyuan-peng-et-al-2024>(64/236) Learning on Multimodal Graphs: A Survey (Ciyuan Peng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ciyuan Peng, Jiayuan He, Feng Xia. (2024)<br><strong>Learning on Multimodal Graphs: A Survey</strong><br><button class=copy-to-clipboard title="Learning on Multimodal Graphs: A Survey" index=64>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-64 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-GR, cs-LG, cs-SI, cs.LG<br>Keyword Score: 6<br>Keywords: Multi-modal, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05322v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05322v1.pdf filename=2402.05322v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Multimodal</b> data pervades various domains, including healthcare, social media, and transportation, where <b>multimodal</b> graphs play a pivotal role. Machine learning on <b>multimodal</b> graphs, referred to as <b>multimodal</b> graph learning (MGL), is essential for successful artificial intelligence (AI) applications. The burgeoning research in this field encompasses diverse graph data types and modalities, learning techniques, and application scenarios. This survey paper conducts a comparative analysis of existing works in <b>multimodal</b> graph learning, elucidating how <b>multimodal</b> learning is achieved across different graph types and exploring the characteristics of prevalent learning techniques. Additionally, we delineate significant applications of <b>multimodal</b> graph learning and offer insights into future directions in this domain. Consequently, this paper serves as a foundational resource for researchers seeking to comprehend existing MGL techniques and their applicability across diverse scenarios.</p></p class="citation"></blockquote><h3 id=65236-crashformer-a-multimodal-architecture-to-predict-the-risk-of-crash-amin-karimi-monsefi-et-al-2024>(65/236) CrashFormer: A Multimodal Architecture to Predict the Risk of Crash (Amin Karimi Monsefi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Amin Karimi Monsefi, Pouya Shiri, Ahmad Mohammadshirazi, Nastaran Karimi Monsefi, Ron Davies, Sobhan Moosavi, Rajiv Ramnath. (2024)<br><strong>CrashFormer: A Multimodal Architecture to Predict the Risk of Crash</strong><br><button class=copy-to-clipboard title="CrashFormer: A Multimodal Architecture to Predict the Risk of Crash" index=65>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-65 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 6<br>Keywords: Multi-modal, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05151v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05151v1.pdf filename=2402.05151v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Reducing traffic accidents is a crucial global public safety concern. Accident prediction is key to improving traffic safety, enabling proactive measures to be taken before a crash occurs, and informing safety policies, regulations, and targeted interventions. Despite numerous studies on accident prediction over the past decades, many have limitations in terms of generalizability, reproducibility, or feasibility for practical use due to input data or problem formulation. To address existing shortcomings, we propose CrashFormer, a <b>multi-modal</b> architecture that utilizes comprehensive (but relatively easy to obtain) inputs such as the history of accidents, weather information, map images, and demographic information. The model predicts the future risk of accidents on a reasonably acceptable cadence (i.e., every six hours) for a geographical location of 5.161 square kilometers. CrashFormer is composed of five components: a sequential encoder to utilize historical accidents and weather data, an image encoder to use map imagery data, a raw data encoder to utilize demographic information, a feature fusion module for aggregating the encoded features, and a classifier that accepts the aggregated data and makes predictions accordingly. Results from extensive real-world experiments in 10 major US cities show that CrashFormer outperforms state-of-the-art sequential and non-sequential models by 1.8% in F1-score on average when using ``sparse&rsquo;&rsquo; input data.</p></p class="citation"></blockquote><h2 id=cscl-29>cs.CL (29)</h2><h3 id=66236-transllama-llm-based-simultaneous-translation-system-roman-koshkin-et-al-2024>(66/236) TransLLaMa: LLM-based Simultaneous Translation System (Roman Koshkin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Roman Koshkin, Katsuhito Sudoh, Satoshi Nakamura. (2024)<br><strong>TransLLaMa: LLM-based Simultaneous Translation System</strong><br><button class=copy-to-clipboard title="TransLLaMa: LLM-based Simultaneous Translation System" index=66>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-66 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 110<br>Keywords: Fine-tuning, Zero-shot, GPT, GPT-4, Transformer, Neural Machine Translation, Reasoning, Text Generation, BLEU, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04636v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04636v1.pdf filename=2402.04636v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Decoder-only <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have recently demonstrated impressive capabilities in <b>text</b> <b>generation</b> and <b>reasoning.</b> Nonetheless, they have limited applications in simultaneous <b>machine</b> <b>translation</b> (SiMT), currently dominated by encoder-decoder <b>transformers.</b> This study demonstrates that, after <b>fine-tuning</b> on a small dataset comprising causally aligned source and target sentence pairs, a pre-trained open-source <b>LLM</b> can control input segmentation directly by generating a special &ldquo;wait&rdquo; token. This obviates the need for a separate policy and enables the <b>LLM</b> to perform English-German and English-Russian SiMT tasks with <b>BLEU</b> scores that are comparable to those of specific state-of-the-art baselines. We also evaluated closed-source models such as <b>GPT-4,</b> which displayed encouraging results in performing the SiMT task without prior training <b>(zero-shot),</b> indicating a promising avenue for enhancing future SiMT systems.</p></p class="citation"></blockquote><h3 id=67236-long-is-more-for-alignment-a-simple-but-tough-to-beat-baseline-for-instruction-fine-tuning-hao-zhao-et-al-2024>(67/236) Long Is More for Alignment: A Simple but Tough-to-Beat Baseline for Instruction Fine-Tuning (Hao Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hao Zhao, Maksym Andriushchenko, Francesco Croce, Nicolas Flammarion. (2024)<br><strong>Long Is More for Alignment: A Simple but Tough-to-Beat Baseline for Instruction Fine-Tuning</strong><br><button class=copy-to-clipboard title="Long Is More for Alignment: A Simple but Tough-to-Beat Baseline for Instruction Fine-Tuning" index=67>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-67 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 100<br>Keywords: Fine-tuning, Fine-tuning, Alpaca, GPT, GPT-3, GPT-3.5, GPT-4, LLaMA, PaLM, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04833v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04833v1.pdf filename=2402.04833v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>There is a consensus that instruction <b>fine-tuning</b> of <b>LLMs</b> requires high-quality data, but what are they? LIMA (NeurIPS 2023) and AlpaGasus (ICLR 2024) are state-of-the-art methods for selecting such high-quality examples, either via manual curation or using <b>GPT-3.5-Turbo</b> as a quality scorer. We show that the extremely simple baseline of selecting the 1,000 instructions with longest responses from standard datasets can consistently outperform these sophisticated methods according to <b>GPT-4</b> and <b>PaLM-2</b> as judges, while remaining competitive on the OpenLLM benchmarks that test factual knowledge. We demonstrate this for several state-of-the-art <b>LLMs</b> <b>(Llama-2-7B,</b> <b>Llama-2-13B,</b> and Mistral-7B) and datasets <b>(Alpaca-52k</b> and Evol-Instruct-70k). In addition, a lightweight refinement of such long instructions can further improve the abilities of the <b>fine-tuned</b> <b>LLMs,</b> and allows us to obtain the 2nd highest-ranked <b>Llama-2-7B-based</b> model on AlpacaEval 2.0 while training on only 1,000 examples and no extra preference data. We also conduct a thorough analysis of our models to ensure that their enhanced performance is not simply due to <b>GPT-4&rsquo;s</b> preference for longer responses, thus ruling out any artificial improvement. In conclusion, our findings suggest that <b>fine-tuning</b> on the longest instructions should be the default baseline for any research on instruction <b>fine-tuning.</b></p></p class="citation"></blockquote><h3 id=68236-improving-cross-domain-low-resource-text-generation-through-llm-post-editing-a-programmer-interpreter-approach-zhuang-li-et-al-2024>(68/236) Improving Cross-Domain Low-Resource Text Generation through LLM Post-Editing: A Programmer-Interpreter Approach (Zhuang Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhuang Li, Levon Haroutunian, Raj Tumuluri, Philip Cohen, Gholamreza Haffari. (2024)<br><strong>Improving Cross-Domain Low-Resource Text Generation through LLM Post-Editing: A Programmer-Interpreter Approach</strong><br><button class=copy-to-clipboard title="Improving Cross-Domain Low-Resource Text Generation through LLM Post-Editing: A Programmer-Interpreter Approach" index=68>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-68 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 90<br>Keywords: Low-Resource, GPT, GPT-3, GPT-3.5, GPT-4, Neural Machine Translation, Text Generation, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04609v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04609v1.pdf filename=2402.04609v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Post-editing has proven effective in improving the quality of <b>text</b> <b>generated</b> by <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> such as <b>GPT-3.5</b> or <b>GPT-4,</b> particularly when direct updating of their parameters to enhance <b>text</b> <b>quality</b> is infeasible or expensive. However, relying solely on smaller language models for post-editing can limit the <b>LLMs&rsquo;</b> ability to generalize across domains. Moreover, the editing strategies in these methods are not optimally designed for <b>text-generation</b> <b>tasks.</b> To address these limitations, we propose a neural programmer-interpreter approach that preserves the domain generalization ability of <b>LLMs</b> when editing their output. The editing actions in this framework are specifically devised for <b>text</b> <b>generation.</b> Extensive experiments demonstrate that the programmer-interpreter significantly enhances <b>GPT-3.5&rsquo;s</b> performance in logical form-to-text conversion and <b>low-resource</b> <b>machine</b> <b>translation,</b> surpassing other state-of-the-art (SOTA) <b>LLM</b> post-editing methods in cross-domain settings.</p></p class="citation"></blockquote><h3 id=69236-prompting-implicit-discourse-relation-annotation-frances-yung-et-al-2024>(69/236) Prompting Implicit Discourse Relation Annotation (Frances Yung et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Frances Yung, Mansoor Ahmad, Merel Scholman, Vera Demberg. (2024)<br><strong>Prompting Implicit Discourse Relation Annotation</strong><br><button class=copy-to-clipboard title="Prompting Implicit Discourse Relation Annotation" index=69>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-69 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 70<br>Keywords: Few-shot, Supervised Learning, Zero-shot, ChatGPT, Reasoning, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04918v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04918v1.pdf filename=2402.04918v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Pre-trained <b>large</b> <b>language</b> <b>models,</b> such as <b>ChatGPT,</b> archive outstanding performance in various <b>reasoning</b> tasks without <b>supervised</b> training and were found to have outperformed crowdsourcing workers. Nonetheless, <b>ChatGPT&rsquo;s</b> performance in the task of implicit discourse relation classification, <b>prompted</b> by a standard multiple-choice question, is still far from satisfactory and considerably inferior to state-of-the-art <b>supervised</b> approaches. This work investigates several proven <b>prompting</b> techniques to improve <b>ChatGPT&rsquo;s</b> recognition of discourse relations. In particular, we experimented with breaking down the classification task that involves numerous abstract labels into smaller subtasks. Nonetheless, experiment results show that the inference accuracy hardly changes even with sophisticated <b>prompt</b> engineering, suggesting that implicit discourse relation classification is not yet resolvable under <b>zero-shot</b> or <b>few-shot</b> settings.</p></p class="citation"></blockquote><h3 id=70236-aspect-based-sentiment-analysis-for-open-ended-hr-survey-responses-lois-rink-et-al-2024>(70/236) Aspect-Based Sentiment Analysis for Open-Ended HR Survey Responses (Lois Rink et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lois Rink, Job Meijdam, David Graus. (2024)<br><strong>Aspect-Based Sentiment Analysis for Open-Ended HR Survey Responses</strong><br><button class=copy-to-clipboard title="Aspect-Based Sentiment Analysis for Open-Ended HR Survey Responses" index=70>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-70 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 70<br>Keywords: Few-shot, Zero-shot, BERT, Bag-of-Words, Aspect-based Sentiment Analysis, Sentiment Analysis, Pre-trained Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04812v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04812v1.pdf filename=2402.04812v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Understanding preferences, opinions, and <b>sentiment</b> <b>of</b> the workforce is paramount for effective employee lifecycle management. Open-ended survey responses serve as a valuable source of information. This paper proposes a machine learning approach for <b>aspect-based</b> <b>sentiment</b> <b>analysis</b> (ABSA) of Dutch open-ended responses in employee satisfaction surveys. Our approach aims to overcome the inherent noise and variability in these responses, enabling a comprehensive analysis of <b>sentiments</b> <b>that</b> can support employee lifecycle management. Through response clustering we identify six key aspects (salary, schedule, contact, communication, personal attention, agreements), which we validate by domain experts. We compile a dataset of 1,458 Dutch survey responses, revealing label imbalance in aspects and <b>sentiments.</b> <b>We</b> propose <b>few-shot</b> approaches for ABSA based on Dutch <b>BERT</b> models, and compare them against <b>bag-of-words</b> and <b>zero-shot</b> baselines. Our work significantly contributes to the field of ABSA by demonstrating the first successful application of Dutch <b>pre-trained</b> <b>language</b> <b>models</b> to <b>aspect-based</b> <b>sentiment</b> <b>analysis</b> in the domain of human resources (HR).</p></p class="citation"></blockquote><h3 id=71236-tinyllm-learning-a-small-student-from-multiple-large-language-models-yijun-tian-et-al-2024>(71/236) TinyLLM: Learning a Small Student from Multiple Large Language Models (Yijun Tian et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yijun Tian, Yikun Han, Xiusi Chen, Wei Wang, Nitesh V. Chawla. (2024)<br><strong>TinyLLM: Learning a Small Student from Multiple Large Language Models</strong><br><button class=copy-to-clipboard title="TinyLLM: Learning a Small Student from Multiple Large Language Models" index=71>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-71 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 60<br>Keywords: Knowledge Distillation, Knowledge Distillation, Reasoning, In-context Learning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04616v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04616v1.pdf filename=2402.04616v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Transferring the <b>reasoning</b> capability from stronger <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> to smaller ones has been quite appealing, as smaller <b>LLMs</b> are more flexible to deploy with less expense. Among the existing solutions, <b>knowledge</b> <b>distillation</b> stands out due to its outstanding efficiency and generalization. However, existing methods suffer from several drawbacks, including limited <b>knowledge</b> <b>diversity</b> and the lack of rich contextual information. To solve the problems and facilitate the learning of compact language models, we propose TinyLLM, a novel <b>knowledge</b> <b>distillation</b> paradigm to learn a small student <b>LLM</b> from multiple <b>large</b> <b>teacher</b> <b>LLMs.</b> In particular, we encourage the student <b>LLM</b> to not only generate the correct answers but also understand the rationales behind these answers. Given that different <b>LLMs</b> possess diverse <b>reasoning</b> skills, we guide the student model to assimilate <b>knowledge</b> <b>from</b> various teacher <b>LLMs.</b> We further introduce an <b>in-context</b> example generator and a teacher-forcing Chain-of-Thought strategy to ensure that the rationales are accurate and grounded in contextually appropriate scenarios. Extensive experiments on six datasets across two <b>reasoning</b> tasks demonstrate the superiority of our method. Results show that TinyLLM can outperform <b>large</b> <b>teacher</b> <b>LLMs</b> significantly, despite having a considerably smaller model size.</p></p class="citation"></blockquote><h3 id=72236-pedagogical-alignment-of-large-language-models-shashank-sonkar-et-al-2024>(72/236) Pedagogical Alignment of Large Language Models (Shashank Sonkar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shashank Sonkar, Kangqi Ni, Sapana Chaudhary, Richard G. Baraniuk. (2024)<br><strong>Pedagogical Alignment of Large Language Models</strong><br><button class=copy-to-clipboard title="Pedagogical Alignment of Large Language Models" index=72>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-72 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: Fine-tuning, Reinforcement Learning, Supervised Learning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05000v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05000v1.pdf filename=2402.05000v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we introduce the novel concept of pedagogically aligned <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> that signifies a transformative shift in the application of <b>LLMs</b> within educational contexts. Rather than providing direct responses to user queries, pedagogically-aligned <b>LLMs</b> function as scaffolding tools, breaking complex problems into manageable subproblems and guiding students towards the final answer through constructive feedback and hints. The objective is to equip learners with problem-solving strategies that deepen their understanding and internalization of the subject matter. Previous research in this field has primarily applied the <b>supervised</b> <b>finetuning</b> approach without framing the objective as an alignment problem, hence not employing <b>reinforcement</b> <b>learning</b> through human feedback (RLHF) methods. This study reinterprets the narrative by viewing the task through the lens of alignment and demonstrates how RLHF methods emerge naturally as a superior alternative for aligning <b>LLM</b> behaviour. Building on this perspective, we propose a novel approach for constructing a reward dataset specifically designed for the pedagogical alignment of <b>LLMs.</b> We apply three state-of-the-art RLHF algorithms and find that they outperform SFT significantly. Our qualitative analyses across model differences and hyperparameter sensitivity further validate the superiority of RLHF over SFT. Also, our study sheds light on the potential of online feedback for enhancing the performance of pedagogically-aligned <b>LLMs,</b> thus providing valuable insights for the advancement of these models in educational settings.</p></p class="citation"></blockquote><h3 id=73236-an-enhanced-prompt-based-llm-reasoning-scheme-via-knowledge-graph-integrated-collaboration-yihao-li-et-al-2024>(73/236) An Enhanced Prompt-Based LLM Reasoning Scheme via Knowledge Graph-Integrated Collaboration (Yihao Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yihao Li, Ru Zhang, Jianyi Liu, Gongshen Liu. (2024)<br><strong>An Enhanced Prompt-Based LLM Reasoning Scheme via Knowledge Graph-Integrated Collaboration</strong><br><button class=copy-to-clipboard title="An Enhanced Prompt-Based LLM Reasoning Scheme via Knowledge Graph-Integrated Collaboration" index=73>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-73 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: Fine-tuning, Reasoning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04978v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04978v1.pdf filename=2402.04978v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>While <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> demonstrate exceptional performance in a multitude of Natural Language Processing (NLP) tasks, they encounter challenges in practical applications, including issues with hallucinations, inadequate knowledge updating, and limited transparency in the <b>reasoning</b> process. To overcome these limitations, this study innovatively proposes a collaborative training-free <b>reasoning</b> scheme involving tight cooperation between Knowledge Graph (KG) and <b>LLMs.</b> This scheme first involves using <b>LLMs</b> to iteratively explore KG, selectively retrieving a task-relevant knowledge subgraph to support <b>reasoning.</b> The <b>LLMs</b> are then guided to further combine inherent implicit knowledge to reason on the subgraph while explicitly elucidating the <b>reasoning</b> process. Through such a cooperative approach, our scheme achieves more reliable knowledge-based <b>reasoning</b> and facilitates the tracing of the <b>reasoning</b> results. Experimental results show that our scheme significantly progressed across multiple datasets, notably achieving over a 10% improvement on the QALD10 dataset compared to the best baseline and the <b>fine-tuned</b> state-of-the-art (SOTA) work. Building on this success, this study hopes to offer a valuable reference for future research in the fusion of KG and <b>LLMs,</b> thereby enhancing <b>LLMs&rsquo;</b> proficiency in solving complex issues.</p></p class="citation"></blockquote><h3 id=74236-a-hypothesis-driven-framework-for-the-analysis-of-self-rationalising-models-marc-braun-et-al-2024>(74/236) A Hypothesis-Driven Framework for the Analysis of Self-Rationalising Models (Marc Braun et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Marc Braun, Jenny Kunz. (2024)<br><strong>A Hypothesis-Driven Framework for the Analysis of Self-Rationalising Models</strong><br><button class=copy-to-clipboard title="A Hypothesis-Driven Framework for the Analysis of Self-Rationalising Models" index=74>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-74 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: GPT, GPT-3, GPT-3.5, Natural Language Inference, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04787v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04787v1.pdf filename=2402.04787v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The self-rationalising capabilities of <b>LLMs</b> are appealing because the generated explanations can give insights into the plausibility of the predictions. However, how faithful the explanations are to the predictions is questionable, raising the need to explore the patterns behind them further. To this end, we propose a hypothesis-driven statistical framework. We use a Bayesian network to implement a hypothesis about how a task (in our example, <b>natural</b> <b>language</b> <b>inference)</b> is solved, and its internal states are translated into <b>natural</b> <b>language</b> <b>with</b> templates. Those explanations are then compared to <b>LLM-generated</b> free-text explanations using automatic and human evaluations. This allows us to judge how similar the <b>LLM&rsquo;s</b> and the Bayesian network&rsquo;s decision processes are. We demonstrate the usage of our framework with an example hypothesis and two realisations in Bayesian networks. The resulting models do not exhibit a strong similarity to <b>GPT-3.5.</b> We discuss the implications of this as well as the framework&rsquo;s potential to approximate <b>LLM</b> decisions better in future work.</p></p class="citation"></blockquote><h3 id=75236-the-future-of-cognitive-strategy-enhanced-persuasive-dialogue-agents-new-perspectives-and-trends-mengqi-chen-et-al-2024>(75/236) The Future of Cognitive Strategy-enhanced Persuasive Dialogue Agents: New Perspectives and Trends (Mengqi Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mengqi Chen, Bin Guo, Hao Wang, Haoyu Li, Qian Zhao, Jingqi Liu, Yasan Ding, Yan Pan, Zhiwen Yu. (2024)<br><strong>The Future of Cognitive Strategy-enhanced Persuasive Dialogue Agents: New Perspectives and Trends</strong><br><button class=copy-to-clipboard title="The Future of Cognitive Strategy-enhanced Persuasive Dialogue Agents: New Perspectives and Trends" index=75>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-75 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: Dialogue System, In-context Learning, Large Language Model, Large Language Model, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04631v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04631v1.pdf filename=2402.04631v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Persuasion, as one of the crucial abilities in human communication, has garnered extensive attention from researchers within the field of intelligent <b>dialogue</b> <b>systems.</b> We humans tend to persuade others to change their viewpoints, attitudes or behaviors through conversations in various scenarios (e.g., persuasion for social good, arguing in online platforms). Developing <b>dialogue</b> <b>agents</b> that can persuade others to accept certain standpoints is essential to achieving truly intelligent and anthropomorphic <b>dialogue</b> <b>system.</b> Benefiting from the substantial progress of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs),</b> <b>dialogue</b> <b>agents</b> have acquired an exceptional capability in context understanding and response generation. However, as a typical and complicated cognitive psychological system, persuasive <b>dialogue</b> <b>agents</b> also require knowledge from the domain of cognitive psychology to attain a level of human-like persuasion. Consequently, the cognitive strategy-enhanced persuasive <b>dialogue</b> <b>agent</b> (defined as CogAgent), which incorporates cognitive strategies to achieve persuasive targets through conversation, has become a predominant research paradigm. To depict the research trends of CogAgent, in this paper, we first present several fundamental cognitive psychology theories and give the formalized definition of three typical cognitive strategies, including the persuasion strategy, the topic path planning strategy, and the argument structure prediction strategy. Then we propose a new system architecture by incorporating the formalized definition to lay the foundation of CogAgent. Representative works are detailed and investigated according to the combined cognitive strategy, followed by the summary of authoritative benchmarks and evaluation metrics. Finally, we <b>summarize</b> our insights on open issues and future directions of CogAgent for upcoming researchers.</p></p class="citation"></blockquote><h3 id=76236-ultralink-an-open-source-knowledge-enhanced-multilingual-supervised-fine-tuning-dataset-haoyu-wang-et-al-2024>(76/236) UltraLink: An Open-Source Knowledge-Enhanced Multilingual Supervised Fine-tuning Dataset (Haoyu Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haoyu Wang, Shuo Wang, Yukun Yan, Xujia Wang, Zhiyu Yang, Yuzhuang Xu, Zhenghao Liu, Ning Ding, Xu Han, Zhiyuan Liu, Maosong Sun. (2024)<br><strong>UltraLink: An Open-Source Knowledge-Enhanced Multilingual Supervised Fine-tuning Dataset</strong><br><button class=copy-to-clipboard title="UltraLink: An Open-Source Knowledge-Enhanced Multilingual Supervised Fine-tuning Dataset" index=76>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-76 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: Data Augmentation, Fine-tuning, Supervised Learning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04588v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04588v1.pdf filename=2402.04588v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Open-source <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have gained significant strength across diverse fields. Nevertheless, the majority of studies primarily concentrate on English, with only limited exploration into the realm of multilingual <b>supervised</b> <b>fine-tuning.</b> In this work, we therefore construct an open-source multilingual <b>supervised</b> <b>fine-tuning</b> dataset. Different from previous works that simply translate English instructions, we consider both the language-specific and language-agnostic abilities of <b>LLMs.</b> For language-specific abilities, we introduce a knowledge-grounded <b>data</b> <b>augmentation</b> approach to elicit more culture-specific knowledge of <b>LLMs,</b> improving their ability to serve users from different countries. For language-agnostic abilities, we find through experiments that modern <b>LLMs</b> exhibit strong cross-lingual transfer capabilities, thus repeatedly learning identical content in various languages is not necessary. Consequently, we can substantially prune the language-agnostic SFT <b>data</b> <b>without</b> any performance degradation, making the SFT process more efficient. The resulting UltraLink dataset comprises approximately 1 million samples across five languages, and the proposed <b>data</b> <b>construction</b> method can also be easily extended to other languages. UltraLink-LM, which is trained on UltraLink, outperforms several representative baselines across many tasks.</p></p class="citation"></blockquote><h3 id=77236-mllm-as-a-judge-assessing-multimodal-llm-as-a-judge-with-vision-language-benchmark-dongping-chen-et-al-2024>(77/236) MLLM-as-a-Judge: Assessing Multimodal LLM-as-a-Judge with Vision-Language Benchmark (Dongping Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dongping Chen, Ruoxi Chen, Shilin Zhang, Yinuo Liu, Yaochen Wang, Huichi Zhou, Qihui Zhang, Pan Zhou, Yao Wan, Lichao Sun. (2024)<br><strong>MLLM-as-a-Judge: Assessing Multimodal LLM-as-a-Judge with Vision-Language Benchmark</strong><br><button class=copy-to-clipboard title="MLLM-as-a-Judge: Assessing Multimodal LLM-as-a-Judge with Vision-Language Benchmark" index=77>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-77 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-CV, cs.CL<br>Keyword Score: 46<br>Keywords: Multi-modal, Multi-modal, GPT, Large Language Model, Large Language Model, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04788v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04788v1.pdf filename=2402.04788v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Multimodal</b> <b>Large</b> <b>Language</b> <b>Models</b> (MLLMs) have gained significant attention recently, showing remarkable potential in artificial general intelligence. However, assessing the utility of MLLMs presents considerable challenges, primarily due to the absence <b>multimodal</b> benchmarks that align with human preferences. Inspired by <b>LLM-as-a-Judge</b> in <b>LLMs,</b> this paper introduces a novel benchmark, termed MLLM-as-a-Judge, to assess the ability of MLLMs in assisting judges including three distinct tasks: Scoring Evaluation, Pair Comparison, and Batch Ranking. Our study reveals that, while MLLMs demonstrate remarkable human-like discernment in Pair Comparisons, there is a significant divergence from human preferences in Scoring Evaluation and Batch Ranking tasks. Furthermore, MLLMs still face challenges in judgment, including diverse biases, hallucinatory responses, and inconsistencies, even for advanced models such as <b>GPT-4V.</b> These findings emphasize the pressing need for enhancements and further research efforts regarding MLLMs as fully reliable evaluators. Code and dataset are available at <a href=https://github.com/Dongping-Chen/MLLM-as-a-Judge>https://github.com/Dongping-Chen/MLLM-as-a-Judge</a>.</p></p class="citation"></blockquote><h3 id=78236-reconfidencing-llms-from-the-grouping-loss-perspective-lihu-chen-et-al-2024>(78/236) Reconfidencing LLMs from the Grouping Loss Perspective (Lihu Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lihu Chen, Alexandre Perez-Lebel, Fabian M. Suchanek, Gaël Varoquaux. (2024)<br><strong>Reconfidencing LLMs from the Grouping Loss Perspective</strong><br><button class=copy-to-clipboard title="Reconfidencing LLMs from the Grouping Loss Perspective" index=78>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-78 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: ChatGPT, LLaMA, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04957v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04957v1.pdf filename=2402.04957v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs),</b> including <b>ChatGPT</b> and <b>LLaMA,</b> are susceptible to generating hallucinated answers in a confident tone. While efforts to elicit and calibrate confidence scores have proven useful, recent findings show that controlling uncertainty must go beyond calibration: predicted scores may deviate significantly from the actual posterior probabilities due to the impact of grouping loss. In this work, we construct a new evaluation dataset derived from a knowledge base to assess confidence scores given to answers of Mistral and <b>LLaMA.</b> Experiments show that they tend to be overconfident. Further, we show that they are more overconfident on some answers than others, \emph{eg} depending on the nationality of the person in the query. In uncertainty-quantification theory, this is grouping loss. To address this, we propose a solution to reconfidence <b>LLMs,</b> canceling not only calibration but also grouping loss. The <b>LLMs,</b> after the reconfidencing process, indicate improved confidence alignment with the accuracy of their responses.</p></p class="citation"></blockquote><h3 id=79236-padellm-ner-parallel-decoding-in-large-language-models-for-named-entity-recognition-jinghui-lu-et-al-2024>(79/236) PaDeLLM-NER: Parallel Decoding in Large Language Models for Named Entity Recognition (Jinghui Lu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jinghui Lu, Ziwei Yang, Yanjie Wang, Xuejing Liu, Can Huang. (2024)<br><strong>PaDeLLM-NER: Parallel Decoding in Large Language Models for Named Entity Recognition</strong><br><button class=copy-to-clipboard title="PaDeLLM-NER: Parallel Decoding in Large Language Models for Named Entity Recognition" index=79>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-79 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Named Entity Recognition, Named Entity Recognition, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04838v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04838v2.pdf filename=2402.04838v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this study, we aim to reduce generation latency for <b>Named</b> <b>Entity</b> <b>Recognition</b> <b>(NER)</b> with <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs).</b> The main cause of high latency in <b>LLMs</b> is the sequential decoding process, which autoregressively generates all labels and mentions for <b>NER,</b> significantly increase the sequence length. To this end, we introduce Parallel Decoding in <b>LLM</b> for NE} (PaDeLLM-NER), a approach that integrates seamlessly into existing generative model frameworks without necessitating additional modules or architectural modifications. PaDeLLM-NER allows for the simultaneous decoding of all mentions, thereby reducing generation latency. Experiments reveal that PaDeLLM-NER significantly increases inference speed that is 1.76 to 10.22 times faster than the autoregressive approach for both English and Chinese. Simultaneously it maintains the quality of predictions as evidenced by the performance that is on par with the state-of-the-art across various datasets.</p></p class="citation"></blockquote><h3 id=80236-large-language-models-as-faithful-explainers-yu-neng-chuang-et-al-2024>(80/236) Large Language Models As Faithful Explainers (Yu-Neng Chuang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yu-Neng Chuang, Guanchu Wang, Chia-Yuan Chang, Ruixiang Tang, Fan Yang, Mengnan Du, Xuanting Cai, Xia Hu. (2024)<br><strong>Large Language Models As Faithful Explainers</strong><br><button class=copy-to-clipboard title="Large Language Models As Faithful Explainers" index=80>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-80 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 40<br>Keywords: Natural Language Explanation, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04678v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04678v1.pdf filename=2402.04678v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have recently become proficient in addressing complex tasks by utilizing their rich internal knowledge and <b>reasoning</b> ability. Consequently, this complexity hinders traditional input-focused explanation algorithms for explaining the complex decision-making processes of <b>LLMs.</b> Recent advancements have thus emerged for self-explaining their predictions through a single feed-forward inference in a <b>natural</b> <b>language</b> <b>format.</b> However, <b>natural</b> <b>language</b> <b>explanations</b> are often criticized for lack of faithfulness since these explanations may not accurately reflect the decision-making behaviors of the <b>LLMs.</b> In this work, we introduce a generative explanation framework, xLLM, to improve the faithfulness of the explanations provided in <b>natural</b> <b>language</b> <b>formats</b> for <b>LLMs.</b> Specifically, we propose an evaluator to quantify the faithfulness of <b>natural</b> <b>language</b> <b>explanation</b> and enhance the faithfulness by an iterative optimization process of xLLM, with the goal of maximizing the faithfulness scores. Experiments conducted on three NLU datasets demonstrate that xLLM can significantly improve the faithfulness of generated explanations, which are in alignment with the behaviors of <b>LLMs.</b></p></p class="citation"></blockquote><h3 id=81236-veras-verify-then-assess-stem-lab-reports-berk-atil-et-al-2024>(81/236) VerAs: Verify then Assess STEM Lab Reports (Berk Atil et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Berk Atil, Mahsa Sheikhi Karizaki, Rebecca J. Passonneau. (2024)<br><strong>VerAs: Verify then Assess STEM Lab Reports</strong><br><button class=copy-to-clipboard title="VerAs: Verify then Assess STEM Lab Reports" index=81>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-81 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 30<br>Keywords: Essay Scoring, Open-Domain Question Answering, Question Answering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05224v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05224v1.pdf filename=2402.05224v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With an increasing focus in STEM education on critical thinking skills, science writing plays an ever more important role in curricula that stress inquiry skills. A recently published dataset of two sets of college level lab reports from an inquiry-based physics curriculum relies on analytic assessment rubrics that utilize multiple dimensions, specifying subject matter knowledge and general components of good explanations. Each analytic dimension is assessed on a 6-point scale, to provide detailed feedback to students that can help them improve their science writing skills. Manual assessment can be slow, and difficult to calibrate for consistency across all students in large classes. While much work exists on automated assessment of open-ended <b>questions</b> <b>in</b> STEM subjects, there has been far less work on long-form writing such as lab reports. We present an end-to-end neural architecture that has separate verifier and assessment modules, inspired by approaches to Open Domain <b>Question</b> <b>Answering</b> (OpenQA). VerAs first verifies whether a report contains any content relevant to a given rubric dimension, and if so, assesses the relevant sentences. On the lab reports, VerAs outperforms multiple baselines based on OpenQA systems or Automated <b>Essay</b> <b>Scoring</b> (AES). VerAs also performs well on an analytic rubric for middle school physics essays.</p></p class="citation"></blockquote><h3 id=82236-the-effect-of-sampling-temperature-on-problem-solving-in-large-language-models-matthew-renze-et-al-2024>(82/236) The Effect of Sampling Temperature on Problem Solving in Large Language Models (Matthew Renze et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Matthew Renze, Erhan Guven. (2024)<br><strong>The Effect of Sampling Temperature on Problem Solving in Large Language Models</strong><br><button class=copy-to-clipboard title="The Effect of Sampling Temperature on Problem Solving in Large Language Models" index=82>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-82 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05201v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05201v1.pdf filename=2402.05201v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this research study, we empirically investigate the effect of sampling temperature on the performance of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> on various problem-solving tasks. We created a multiple-choice question-and-answer (MCQA) exam by randomly sampling problems from standard <b>LLM</b> benchmarks. Then, we used four popular <b>LLMs</b> with five <b>prompt-engineering</b> techniques to solve the MCQA problems while increasing the sampling temperature from 0.0 to 1.0. Despite anecdotal reports to the contrary, our empirical results indicate that changes in temperature in the range 0.0 to 1.0 do not have a statistically significant impact on <b>LLM</b> performance for problem-solving tasks. In addition, these results appear to hold regardless of the <b>LLM,</b> the <b>prompt-engineering</b> technique, or the problem domain. All code, data, and supplemental materials are available on GitHub at: <a href=https://github.com/matthewrenze/jhu-llm-temperature>https://github.com/matthewrenze/jhu-llm-temperature</a>.</p></p class="citation"></blockquote><h3 id=83236-salad-bench-a-hierarchical-and-comprehensive-safety-benchmark-for-large-language-models-lijun-li-et-al-2024>(83/236) SALAD-Bench: A Hierarchical and Comprehensive Safety Benchmark for Large Language Models (Lijun Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lijun Li, Bowen Dong, Ruohui Wang, Xuhao Hu, Wangmeng Zuo, Dahua Lin, Yu Qiao, Jing Shao. (2024)<br><strong>SALAD-Bench: A Hierarchical and Comprehensive Safety Benchmark for Large Language Models</strong><br><button class=copy-to-clipboard title="SALAD-Bench: A Hierarchical and Comprehensive Safety Benchmark for Large Language Models" index=83>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-83 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-CR, cs-LG, cs.CL<br>Keyword Score: 30<br>Keywords: Question Answering, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05044v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05044v2.pdf filename=2402.05044v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the rapidly evolving landscape of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs),</b> ensuring robust safety measures is paramount. To meet this crucial need, we propose \emph{SALAD-Bench}, a safety benchmark specifically designed for evaluating <b>LLMs,</b> attack, and defense methods. Distinguished by its breadth, SALAD-Bench transcends conventional benchmarks through its <b>large</b> <b>scale,</b> <b>rich</b> diversity, intricate taxonomy spanning three levels, and versatile functionalities.SALAD-Bench is crafted with a meticulous array of questions, from standard queries to complex ones enriched with attack, defense modifications and multiple-choice. To effectively manage the inherent complexity, we introduce an innovative evaluators: the <b>LLM-based</b> MD-Judge for <b>QA</b> pairs with a particular focus on attack-enhanced queries, ensuring a seamless, and reliable evaluation. Above components extend SALAD-Bench from standard <b>LLM</b> safety evaluation to both <b>LLM</b> attack and defense methods evaluation, ensuring the joint-purpose utility. Our extensive experiments shed light on the resilience of <b>LLMs</b> against emerging threats and the efficacy of contemporary defense tactics. Data and evaluator are released under <a href=https://github.com/OpenSafetyLab/SALAD-BENCH>https://github.com/OpenSafetyLab/SALAD-BENCH</a>.</p></p class="citation"></blockquote><h3 id=84236-memoryllm-towards-self-updatable-large-language-models-yu-wang-et-al-2024>(84/236) MEMORYLLM: Towards Self-Updatable Large Language Models (Yu Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yu Wang, Xiusi Chen, Jingbo Shang, Julian McAuley. (2024)<br><strong>MEMORYLLM: Towards Self-Updatable Large Language Models</strong><br><button class=copy-to-clipboard title="MEMORYLLM: Towards Self-Updatable Large Language Models" index=84>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-84 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Transformer, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04624v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04624v1.pdf filename=2402.04624v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Existing <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> usually remain static after deployment, which might make it hard to inject new knowledge into the model. We aim to build models containing a considerable portion of self-updatable parameters, enabling the model to integrate new knowledge effectively and efficiently. To this end, we introduce MEMORYLLM, a model that comprises a <b>transformer</b> and a fixed-size memory pool within the latent space of the <b>transformer.</b> MEMORYLLM can self-update with text knowledge and memorize the knowledge injected earlier. Our evaluations demonstrate the ability of MEMORYLLM to effectively incorporate new knowledge, as evidenced by its performance on model editing benchmarks. Meanwhile, the model exhibits long-term information retention capacity, which is validated through our custom-designed evaluations and long-context benchmarks. MEMORYLLM also shows operational integrity without any sign of performance degradation even after nearly a million memory updates.</p></p class="citation"></blockquote><h3 id=85236-infllm-unveiling-the-intrinsic-capacity-of-llms-for-understanding-extremely-long-sequences-with-training-free-memory-chaojun-xiao-et-al-2024>(85/236) InfLLM: Unveiling the Intrinsic Capacity of LLMs for Understanding Extremely Long Sequences with Training-Free Memory (Chaojun Xiao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chaojun Xiao, Pengle Zhang, Xu Han, Guangxuan Xiao, Yankai Lin, Zhengyan Zhang, Zhiyuan Liu, Song Han, Maosong Sun. (2024)<br><strong>InfLLM: Unveiling the Intrinsic Capacity of LLMs for Understanding Extremely Long Sequences with Training-Free Memory</strong><br><button class=copy-to-clipboard title="InfLLM: Unveiling the Intrinsic Capacity of LLMs for Understanding Extremely Long Sequences with Training-Free Memory" index=85>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-85 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 30<br>Keywords: Out-of-domain, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04617v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04617v1.pdf filename=2402.04617v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have emerged as a cornerstone in real-world applications with lengthy streaming inputs, such as <b>LLM-driven</b> agents. However, existing <b>LLMs,</b> pre-trained on sequences with restricted maximum length, cannot generalize to longer sequences due to the <b>out-of-domain</b> and distraction issues. To alleviate these issues, existing efforts employ sliding attention windows and discard distant tokens to achieve the processing of extremely long sequences. Unfortunately, these approaches inevitably fail to capture long-distance dependencies within sequences to deeply understand semantics. This paper introduces a training-free memory-based method, InfLLM, to unveil the intrinsic ability of <b>LLMs</b> to process streaming long sequences. Specifically, InfLLM stores distant contexts into additional memory units and employs an efficient mechanism to lookup token-relevant units for attention computation. Thereby, InfLLM allows <b>LLMs</b> to efficiently process long sequences while maintaining the ability to capture long-distance dependencies. Without any training, InfLLM enables <b>LLMs</b> pre-trained on sequences of a few thousand tokens to achieve superior performance than competitive baselines continually training these <b>LLMs</b> on long sequences. Even when the sequence length is scaled to $1,024$K, InfLLM still effectively captures long-distance dependencies.</p></p class="citation"></blockquote><h3 id=86236-faithfulness-vs-plausibility-on-the-unreliability-of-explanations-from-large-language-models-chirag-agarwal-et-al-2024>(86/236) Faithfulness vs. Plausibility: On the (Un)Reliability of Explanations from Large Language Models (Chirag Agarwal et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chirag Agarwal, Sree Harsha Tanneru, Himabindu Lakkaraju. (2024)<br><strong>Faithfulness vs. Plausibility: On the (Un)Reliability of Explanations from Large Language Models</strong><br><button class=copy-to-clipboard title="Faithfulness vs. Plausibility: On the (Un)Reliability of Explanations from Large Language Models" index=86>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-86 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04614v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04614v2.pdf filename=2402.04614v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> are deployed as powerful tools for several natural language processing (NLP) applications. Recent works show that modern <b>LLMs</b> can generate self-explanations (SEs), which elicit their intermediate <b>reasoning</b> steps for explaining their behavior. Self-explanations have seen widespread adoption owing to their conversational and plausible nature. However, there is little to no understanding of their faithfulness. In this work, we discuss the dichotomy between faithfulness and plausibility in SEs generated by <b>LLMs.</b> We argue that while <b>LLMs</b> are adept at generating plausible explanations &ndash; seemingly logical and coherent to human users &ndash; these explanations do not necessarily align with the <b>reasoning</b> processes of the <b>LLMs,</b> raising concerns about their faithfulness. We highlight that the current trend towards increasing the plausibility of explanations, primarily driven by the demand for user-friendly interfaces, may come at the cost of diminishing their faithfulness. We assert that the faithfulness of explanations is critical in <b>LLMs</b> employed for high-stakes decision-making. Moreover, we urge the community to identify the faithfulness requirements of real-world applications and ensure explanations meet those needs. Finally, we propose some directions for future work, emphasizing the need for novel methodologies and frameworks that can enhance the faithfulness of self-explanations without compromising their plausibility, essential for the transparent deployment of <b>LLMs</b> in diverse high-stakes domains.</p></p class="citation"></blockquote><h3 id=87236-alirector-alignment-enhanced-chinese-grammatical-error-corrector-haihui-yang-et-al-2024>(87/236) Alirector: Alignment-Enhanced Chinese Grammatical Error Corrector (Haihui Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haihui Yang, Xiaojun Quan. (2024)<br><strong>Alirector: Alignment-Enhanced Chinese Grammatical Error Corrector</strong><br><button class=copy-to-clipboard title="Alirector: Alignment-Enhanced Chinese Grammatical Error Corrector" index=87>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-87 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Grammatical Error Correction, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04601v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04601v1.pdf filename=2402.04601v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Chinese <b>grammatical</b> <b>error</b> <b>correction</b> (CGEC) faces serious overcorrection challenges when employing autoregressive generative models such as sequence-to-sequence (Seq2Seq) models and decoder-only <b>large</b> <b>language</b> <b>models</b> <b>(LLMs).</b> While previous methods aim to address overcorrection in Seq2Seq models, they are difficult to adapt to decoder-only <b>LLMs.</b> In this paper, we propose an alignment-enhanced corrector for the overcorrection problem that applies to both Seq2Seq models and decoder-only <b>LLMs.</b> Our method first trains a correction model to generate an initial correction of the source sentence. Then, we combine the source sentence with the initial correction and feed it through an alignment model for another round of correction, aiming to enforce the alignment model to focus on potential overcorrection. Moreover, to enhance the model&rsquo;s ability to identify nuances, we further explore the reverse alignment of the source sentence and the initial correction. Finally, we transfer the alignment knowledge from two alignment models to the correction model, instructing it on how to avoid overcorrection. Experimental results on three CGEC datasets demonstrate the effectiveness of our approach in alleviating overcorrection and improving overall performance.</p></p class="citation"></blockquote><h3 id=88236-personalized-text-generation-with-fine-grained-linguistic-control-bashar-alhafni-et-al-2024>(88/236) Personalized Text Generation with Fine-Grained Linguistic Control (Bashar Alhafni et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bashar Alhafni, Vivek Kulkarni, Dhruv Kumar, Vipul Raheja. (2024)<br><strong>Personalized Text Generation with Fine-Grained Linguistic Control</strong><br><button class=copy-to-clipboard title="Personalized Text Generation with Fine-Grained Linguistic Control" index=88>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-88 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Text Generation, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04914v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04914v1.pdf filename=2402.04914v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>As the <b>text</b> <b>generation</b> capabilities of <b>large</b> <b>language</b> <b>models</b> become increasingly prominent, recent studies have focused on controlling particular aspects of the generated <b>text</b> <b>to</b> make it more personalized. However, most research on controllable <b>text</b> <b>generation</b> focuses on controlling the content or modeling specific high-level/coarse-grained attributes that reflect authors&rsquo; writing styles, such as formality, domain, or sentiment. In this paper, we focus on controlling fine-grained attributes spanning multiple linguistic dimensions, such as lexical and syntactic attributes. We introduce a novel benchmark to train generative models and evaluate their ability to generate personalized <b>text</b> <b>based</b> on multiple fine-grained linguistic attributes. We systematically investigate the performance of various <b>large</b> <b>language</b> <b>models</b> on our benchmark and draw insights from the factors that impact their performance. We make our code, data, and pretrained models publicly available.</p></p class="citation"></blockquote><h3 id=89236-learning-communication-policies-for-different-follower-behaviors-in-a-collaborative-reference-game-philipp-sadler-et-al-2024>(89/236) Learning Communication Policies for Different Follower Behaviors in a Collaborative Reference Game (Philipp Sadler et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Philipp Sadler, Sherzod Hakimov, David Schlangen. (2024)<br><strong>Learning Communication Policies for Different Follower Behaviors in a Collaborative Reference Game</strong><br><button class=copy-to-clipboard title="Learning Communication Policies for Different Follower Behaviors in a Collaborative Reference Game" index=89>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-89 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Reinforcement Learning, Grounding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04824v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04824v1.pdf filename=2402.04824v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Albrecht and Stone (2018) state that modeling of changing behaviors remains an open problem &ldquo;due to the essentially unconstrained nature of what other agents may do&rdquo;. In this work we evaluate the adaptability of neural artificial agents towards assumed partner behaviors in a collaborative reference game. In this game success is achieved when a knowledgeable Guide can verbally lead a Follower to the selection of a specific puzzle piece among several distractors. We frame this language <b>grounding</b> and coordination task as a <b>reinforcement</b> <b>learning</b> problem and measure to which extent a common <b>reinforcement</b> <b>training</b> algorithm (PPO) is able to produce neural agents (the Guides) that perform well with various heuristic Follower behaviors that vary along the dimensions of confidence and autonomy. We experiment with a learning signal that in addition to the goal condition also respects an assumed communicative effort. Our results indicate that this novel ingredient leads to communicative strategies that are less verbose (staying silent in some of the steps) and that with respect to that the Guide&rsquo;s strategies indeed adapt to the partner&rsquo;s level of confidence and autonomy.</p></p class="citation"></blockquote><h3 id=90236-source-identification-in-abstractive-summarization-yoshi-suhara-et-al-2024>(90/236) Source Identification in Abstractive Summarization (Yoshi Suhara et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yoshi Suhara, Dimitris Alikaniotis. (2024)<br><strong>Source Identification in Abstractive Summarization</strong><br><button class=copy-to-clipboard title="Source Identification in Abstractive Summarization" index=90>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-90 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Perplexity, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04677v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04677v1.pdf filename=2402.04677v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Neural abstractive <b>summarization</b> models make summaries in an end-to-end manner, and little is known about how the source information is actually converted into summaries. In this paper, we define input sentences that contain essential information in the generated summary as $\textit{source sentences}$ and study how abstractive summaries are made by analyzing the source sentences. To this end, we annotate source sentences for reference summaries and system summaries generated by PEGASUS on document-summary pairs sampled from the CNN/DailyMail and XSum datasets. We also formulate automatic source sentence detection and compare multiple methods to establish a strong baseline for the task. Experimental results show that the <b>perplexity-based</b> method performs well in highly abstractive settings, while similarity-based methods perform robustly in relatively extractive settings. Our code and data are available at <a href=https://github.com/suhara/sourcesum>https://github.com/suhara/sourcesum</a>.</p></p class="citation"></blockquote><h3 id=91236-text-or-image-what-is-more-important-in-cross-domain-generalization-capabilities-of-hate-meme-detection-models-piush-aggarwal-et-al-2024>(91/236) Text or Image? What is More Important in Cross-Domain Generalization Capabilities of Hate Meme Detection Models? (Piush Aggarwal et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Piush Aggarwal, Jawar Mehrabanian, Weigang Huang, Özge Alacam, Torsten Zesch. (2024)<br><strong>Text or Image? What is More Important in Cross-Domain Generalization Capabilities of Hate Meme Detection Models?</strong><br><button class=copy-to-clipboard title="Text or Image? What is More Important in Cross-Domain Generalization Capabilities of Hate Meme Detection Models?" index=91>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-91 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-CV, cs.CL<br>Keyword Score: 16<br>Keywords: Multi-modal, Multi-modal, Zero-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04967v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04967v1.pdf filename=2402.04967v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper delves into the formidable challenge of cross-domain generalization in <b>multimodal</b> hate meme detection, presenting compelling findings. We provide enough pieces of evidence supporting the hypothesis that only the textual component of hateful memes enables the existing <b>multimodal</b> classifier to generalize across different domains, while the image component proves highly sensitive to a specific training dataset. The evidence includes demonstrations showing that hate-text classifiers perform similarly to hate-meme classifiers in a <b>zero-shot</b> setting. Simultaneously, the introduction of captions generated from images of memes to the hate-meme classifier worsens performance by an average F1 of 0.02. Through blackbox explanations, we identify a substantial contribution of the text modality (average of 83%), which diminishes with the introduction of meme&rsquo;s image captions (52%). Additionally, our evaluation on a newly created confounder dataset reveals higher performance on text confounders as compared to image confounders with an average $\Delta$F1 of 0.18.</p></p class="citation"></blockquote><h3 id=92236-how-bert-speaks-shakespearean-english-evaluating-historical-bias-in-contextual-language-models-miriam-cuscito-et-al-2024>(92/236) How BERT Speaks Shakespearean English? Evaluating Historical Bias in Contextual Language Models (Miriam Cuscito et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Miriam Cuscito, Alfio Ferrara, Martin Ruskov. (2024)<br><strong>How BERT Speaks Shakespearean English? Evaluating Historical Bias in Contextual Language Models</strong><br><button class=copy-to-clipboard title="How BERT Speaks Shakespearean English? Evaluating Historical Bias in Contextual Language Models" index=92>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-92 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: I-2-7; J-5, cs-CL, cs-CY, cs.CL<br>Keyword Score: 10<br>Keywords: BERT<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05034v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05034v1.pdf filename=2402.05034v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we explore the idea of analysing the historical bias of contextual language models based on <b>BERT</b> by measuring their adequacy with respect to Early Modern (EME) and Modern (ME) English. In our preliminary experiments, we perform fill-in-the-blank tests with 60 masked sentences (20 EME-specific, 20 ME-specific and 20 generic) and three different models (i.e., <b>BERT</b> Base, MacBERTh, English HLM). We then rate the model predictions according to a 5-point bipolar scale between the two language varieties and derive a weighted score to measure the adequacy of each model to EME and ME varieties of English.</p></p class="citation"></blockquote><h3 id=93236-stablemask-refining-causal-masking-in-decoder-only-transformer-qingyu-yin-et-al-2024>(93/236) StableMask: Refining Causal Masking in Decoder-only Transformer (Qingyu Yin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qingyu Yin, Xuzheng He, Xiang Zhuang, Yu Zhao, Jianhua Yao, Xiaoyu Shen, Qiang Zhang. (2024)<br><strong>StableMask: Refining Causal Masking in Decoder-only Transformer</strong><br><button class=copy-to-clipboard title="StableMask: Refining Causal Masking in Decoder-only Transformer" index=93>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-93 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 10<br>Keywords: Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04779v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04779v1.pdf filename=2402.04779v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The decoder-only <b>Transformer</b> architecture with causal masking and relative position encoding (RPE) has become the de facto choice in language modeling. Despite its exceptional performance across various tasks, we have identified two limitations: First, it requires all attention scores to be non-zero and sum up to 1, even if the current embedding has sufficient self-contained information. This compels the model to assign disproportional excessive attention to specific tokens. Second, RPE-based <b>Transformers</b> are not universal approximators due to their limited capacity at encoding absolute positional information, which limits their application in position-critical tasks. In this work, we propose StableMask: a parameter-free method to address both limitations by refining the causal mask. It introduces pseudo-attention values to balance attention distributions and encodes absolute positional information via a progressively decreasing mask ratio. StableMask&rsquo;s effectiveness is validated both theoretically and empirically, showing significant enhancements in language models with parameter sizes ranging from 71M to 1.4B across diverse datasets and encoding methods. We further show that it naturally supports (1) efficient extrapolation without special tricks such as StreamingLLM and (2) easy integration with existing attention optimization techniques.</p></p class="citation"></blockquote><h3 id=94236-developments-in-sheaf-theoretic-models-of-natural-language-ambiguities-kin-ian-lo-et-al-2024>(94/236) Developments in Sheaf-Theoretic Models of Natural Language Ambiguities (Kin Ian Lo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kin Ian Lo, Mehrnoosh Sadrzadeh, Shane Mansfield. (2024)<br><strong>Developments in Sheaf-Theoretic Models of Natural Language Ambiguities</strong><br><button class=copy-to-clipboard title="Developments in Sheaf-Theoretic Models of Natural Language Ambiguities" index=94>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-94 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL, quant-ph<br>Keyword Score: 10<br>Keywords: Disambiguation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04505v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04505v1.pdf filename=2402.04505v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Sheaves are mathematical objects consisting of a base which constitutes a topological space and the data associated with each open set thereof, e.g. continuous functions defined on the open sets. Sheaves have originally been used in algebraic topology and logic. Recently, they have also modelled events such as physical experiments and natural language <b>disambiguation</b> processes. We extend the latter models from lexical ambiguities to discourse ambiguities arising from anaphora. To begin, we calculated a new measure of contextuality for a dataset of basic anaphoric discourses, resulting in a higher proportion of contextual models&ndash;82.9%&ndash;compared to previous work which only yielded 3.17% contextual models. Then, we show how an extension of the natural language processing challenge, known as the Winograd Schema, which involves anaphoric ambiguities can be modelled on the Bell-CHSH scenario with a contextual fraction of 0.096.</p></p class="citation"></blockquote><h2 id=csse-8>cs.SE (8)</h2><h3 id=95236-automated-smart-contract-summarization-via-llms-yingjie-mao-et-al-2024>(95/236) Automated Smart Contract Summarization via LLMs (Yingjie Mao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yingjie Mao, Xiaoqi Li, Zongwei Li, Wenkai Li. (2024)<br><strong>Automated Smart Contract Summarization via LLMs</strong><br><button class=copy-to-clipboard title="Automated Smart Contract Summarization via LLMs" index=95>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-95 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-SE, cs.SE<br>Keyword Score: 76<br>Keywords: Multi-modal, Multi-modal, BLEU, Large Language Model, Large Language Model, Prompt, Rouge, Rouge-L, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04863v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04863v2.pdf filename=2402.04863v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Automatic code <b>Summarization</b> generation technology is widely used in the development and maintenance of smart contracts. In recent years, with the advent of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs),</b> Gemini has received a lot of attention as the first <b>Large</b> <b>Multimodal</b> <b>models</b> (LMMs) to support <b>multimodal</b> input. However, it is unclear how LMMs can generate contract code <b>summarization</b> from <b>multimodal</b> inputs. In this paper, we focus on evaluating Gemini on real-world smart contracts, comparing it to the MMTrans, and exploring how to combine <b>multimodal</b> <b>prompts</b> to generate a contract code <b>summarization.</b> We used several widely used metrics <b>(BLEU,</b> METEOR, and <b>ROUGE-L)</b> to measure the quality of the generated <b>summarization.</b> Our experiments show that METEOR and <b>ROUGEL</b> metrics, Gemini-Pro-Vision achieves 21.17% and 21.05% scores for code comments generated by three-shot <b>prompts.</b> These scores are better than those generated by one-shot and five-shot <b>prompts.</b></p></p class="citation"></blockquote><h3 id=96236-you-can-rest-now-automated-specification-inference-and-black-box-testing-of-restful-apis-with-large-language-models-alix-decrop-et-al-2024>(96/236) You Can REST Now: Automated Specification Inference and Black-Box Testing of RESTful APIs with Large Language Models (Alix Decrop et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Alix Decrop, Gilles Perrouin, Mike Papadakis, Xavier Devroey, Pierre-Yves Schobbens. (2024)<br><strong>You Can REST Now: Automated Specification Inference and Black-Box Testing of RESTful APIs with Large Language Models</strong><br><button class=copy-to-clipboard title="You Can REST Now: Automated Specification Inference and Black-Box Testing of RESTful APIs with Large Language Models" index=96>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-96 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-SE, cs.SE<br>Keyword Score: 50<br>Keywords: Fine-tuning, In-context Learning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05102v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05102v1.pdf filename=2402.05102v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>RESTful APIs are popular web services, requiring documentation to ease their comprehension, reusability and testing practices. The OpenAPI Specification (OAS) is a widely adopted and machine-readable format used to document such APIs. However, manually documenting RESTful APIs is a time-consuming and error-prone task, resulting in unavailable, incomplete, or imprecise documentation. As RESTful API testing tools require an OpenAPI specification as input, insufficient or informal documentation hampers testing quality. Recently, <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have demonstrated exceptional abilities to automate tasks based on their colossal training data. Accordingly, such capabilities could be utilized to assist the documentation and testing process of RESTful APIs. In this paper, we present RESTSpecIT, the first automated RESTful API specification inference and black-box testing approach leveraging <b>LLMs.</b> The approach requires minimal user input compared to state-of-the-art RESTful API inference and testing tools; Given an API name and an <b>LLM</b> key, HTTP requests are generated and mutated with data returned by the <b>LLM.</b> By sending the requests to the API endpoint, HTTP responses can be analyzed for inference and testing purposes. RESTSpecIT utilizes an <b>in-context</b> <b>prompt</b> masking strategy, requiring no model <b>fine-tuning.</b> Our evaluation demonstrates that RESTSpecIT is capable of: (1) inferring specifications with 85.05% of GET routes and 81.05% of query parameters found on average, (2) discovering undocumented and valid routes and parameters, and (3) uncovering server errors in RESTful APIs. Inferred specifications can also be used as testing tool inputs.</p></p class="citation"></blockquote><h3 id=97236-an-investigation-of-patch-porting-practices-of-the-linux-kernel-ecosystem-xingyu-li-et-al-2024>(97/236) An Investigation of Patch Porting Practices of the Linux Kernel Ecosystem (Xingyu Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xingyu Li, Zheng Zhang, Zhiyun Qian, Trent Jaeger, Chengyu Song. (2024)<br><strong>An Investigation of Patch Porting Practices of the Linux Kernel Ecosystem</strong><br><button class=copy-to-clipboard title="An Investigation of Patch Porting Practices of the Linux Kernel Ecosystem" index=97>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-97 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-CR, cs-SE, cs.SE<br>Keyword Score: 20<br>Keywords: Recommendation, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05212v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05212v1.pdf filename=2402.05212v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Open-source software is increasingly reused, complicating the process of patching to repair bugs. In the case of Linux, a distinct ecosystem has formed, with Linux mainline serving as the upstream, stable or long-term-support (LTS) systems forked from mainline, and Linux distributions, such as Ubuntu and Android, as downstreams forked from stable or LTS systems for end-user use. Ideally, when a patch is committed in the Linux upstream, it should not introduce new bugs and be ported to all the applicable downstream branches in a timely fashion. However, several concerns have been expressed in prior work about the responsiveness of patch porting in this Linux ecosystem. In this paper, we mine the software repositories to investigate a range of Linux distributions in combination with Linux stable and LTS, and find diverse patch porting strategies and competence levels that help explain the phenomenon. Furthermore, we show concretely using three metrics, i.e., patch delay, patch rate, and bug inheritance ratio, that different porting strategies have different tradeoffs. We find that hinting tags(e.g., Cc stable tags and fixes tags) are significantly important to the <b>prompt</b> patch porting, but it is noteworthy that a substantial portion of patches remain devoid of these indicative tags. Finally, we offer <b>recommendations</b> based on our analysis of the general patch flow, e.g., interactions among various stakeholders in the ecosystem and automatic generation of hinting tags, as well as tailored suggestions for specific porting strategies.</p></p class="citation"></blockquote><h3 id=98236-can-we-identify-stack-overflow-questions-requiring-code-snippets-investigating-the-cause--effect-of-missing-code-snippets-saikat-mondal-et-al-2024>(98/236) Can We Identify Stack Overflow Questions Requiring Code Snippets? Investigating the Cause & Effect of Missing Code Snippets (Saikat Mondal et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Saikat Mondal, Mohammad Masudur Rahman, Chanchal K. Roy. (2024)<br><strong>Can We Identify Stack Overflow Questions Requiring Code Snippets? Investigating the Cause & Effect of Missing Code Snippets</strong><br><button class=copy-to-clipboard title="Can We Identify Stack Overflow Questions Requiring Code Snippets? Investigating the Cause & Effect of Missing Code Snippets" index=98>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-98 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-SE, cs.SE<br>Keyword Score: 20<br>Keywords: Question Answering, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04575v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04575v1.pdf filename=2402.04575v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>On the Stack Overflow (SO) Q&amp;A site, users often request solutions to their code-related problems (e.g., errors, unexpected behavior). Unfortunately, they often miss required code snippets during their <b>question</b> <b>submission,</b> which could prevent their <b>questions</b> <b>from</b> getting <b>prompt</b> and appropriate answers. In this study, we conduct an empirical study investigating the cause & effect of missing code snippets in SO <b>questions</b> <b>whenever</b> required. Here, our contributions are threefold. First, we analyze how the presence or absence of required code snippets affects the correlation between <b>question</b> <b>types</b> (missed code, included code after requests & had code snippets during submission) and corresponding answer meta-data (e.g., presence of an accepted answer). According to our analysis, the chance of getting accepted answers is three times higher for <b>questions</b> <b>that</b> include required code snippets during their <b>question</b> <b>submission</b> than those that missed the code. We also investigate whether the confounding factors (e.g., user reputation) affect <b>questions</b> <b>receiving</b> answers besides the presence or absence of required code snippets. We found that such factors do not hurt the correlation between the presence or absence of required code snippets and answer meta-data. Second, we surveyed 64 practitioners to understand why users miss necessary code snippets. About 60% of them agree that users are unaware of whether their <b>questions</b> <b>require</b> any code snippets. Third, we thus extract four text-based features (e.g., keywords) and build six ML models to identify the <b>questions</b> <b>that</b> need code snippets. Our models can predict the target <b>questions</b> <b>with</b> 86.5% precision, 90.8% recall, 85.3% F1-score, and 85.2% overall accuracy. Our work has the potential to save significant time in programming <b>question-answering</b> <b>and</b> improve the quality of the valuable knowledge base by decreasing unanswered and unresolved questions.</p></p class="citation"></blockquote><h3 id=99236-enhancing-user-interaction-in-chatgpt-characterizing-and-consolidating-multiple-prompts-for-issue-resolution-saikat-mondal-et-al-2024>(99/236) Enhancing User Interaction in ChatGPT: Characterizing and Consolidating Multiple Prompts for Issue Resolution (Saikat Mondal et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Saikat Mondal, Suborno Deb Bappon, Chanchal K. Roy. (2024)<br><strong>Enhancing User Interaction in ChatGPT: Characterizing and Consolidating Multiple Prompts for Issue Resolution</strong><br><button class=copy-to-clipboard title="Enhancing User Interaction in ChatGPT: Characterizing and Consolidating Multiple Prompts for Issue Resolution" index=99>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-99 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-SE, cs.SE<br>Keyword Score: 20<br>Keywords: ChatGPT, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04568v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04568v1.pdf filename=2402.04568v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Prompt</b> design plays a crucial role in shaping the efficacy of <b>ChatGPT,</b> influencing the model&rsquo;s ability to extract contextually accurate responses. Thus, optimal <b>prompt</b> construction is essential for maximizing the utility and performance of <b>ChatGPT.</b> However, sub-optimal <b>prompt</b> design may necessitate iterative refinement, as imprecise or ambiguous instructions can lead to undesired responses from <b>ChatGPT.</b> Existing studies explore several <b>prompt</b> patterns and strategies to improve the relevance of responses generated by <b>ChatGPT.</b> However, the exploration of constraints that necessitate the submission of multiple <b>prompts</b> is still an unmet attempt. In this study, our contributions are twofold. First, we attempt to uncover gaps in <b>prompt</b> design that demand multiple iterations. In particular, we manually analyze 686 <b>prompts</b> that were submitted to resolve issues related to Java and Python programming languages and identify eleven <b>prompt</b> design gaps (e.g., missing specifications). Such gap exploration can enhance the efficacy of single <b>prompts</b> in <b>ChatGPT.</b> Second, we attempt to reproduce the <b>ChatGPT</b> response by consolidating multiple <b>prompts</b> into a single one. We can completely consolidate <b>prompts</b> with four gaps (e.g., missing context) and partially consolidate <b>prompts</b> with three gaps (e.g., additional functionality). Such an effort provides concrete evidence to users to design more optimal <b>prompts</b> mitigating these gaps. Our study findings and evidence can - (a) save users time, (b) reduce costs, and (c) increase user satisfaction.</p></p class="citation"></blockquote><h3 id=100236-the-foundations-of-computational-management-a-systematic-approach-to-task-automation-for-the-integration-of-artificial-intelligence-into-existing-workflows-tamen-jadad-garcia-et-al-2024>(100/236) The Foundations of Computational Management: A Systematic Approach to Task Automation for the Integration of Artificial Intelligence into Existing Workflows (Tamen Jadad-Garcia et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tamen Jadad-Garcia, Alejandro R. Jadad. (2024)<br><strong>The Foundations of Computational Management: A Systematic Approach to Task Automation for the Integration of Artificial Intelligence into Existing Workflows</strong><br><button class=copy-to-clipboard title="The Foundations of Computational Management: A Systematic Approach to Task Automation for the Integration of Artificial Intelligence into Existing Workflows" index=100>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-100 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-AI, cs-SE, cs.SE<br>Keyword Score: 20<br>Keywords: Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05142v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05142v1.pdf filename=2402.05142v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Driven by the rapid ascent of artificial intelligence (AI), organizations are at the epicenter of a seismic shift, facing a crucial question: How can AI be successfully integrated into existing operations? To help answer it, manage expectations and mitigate frustration, this article introduces Computational Management, a systematic approach to task automation for enhancing the ability of organizations to harness AI&rsquo;s potential within existing workflows. Computational Management acts as a bridge between the strategic insights of management science with the analytical rigor of computational thinking. The article offers three easy step-by-step procedures to begin the process of implementing AI within a workflow. Such procedures focus on task (re)formulation, on the assessment of the automation potential of tasks, on the completion of task specification templates for AI selection and adaptation. Included in the article there are manual and automated methods, with <b>prompt</b> suggestions for publicly available <b>LLMs,</b> to complete these three procedures. The first procedure, task (re)formulation, focuses on breaking down work activities into basic units, so they can be completed by one agent, involve a single well-defined action, and produce a distinct outcome. The second, allows the assessment of the granular task and its suitability for automation, using the Task Automation Index to rank tasks based on whether they have standardized input, well-defined rules, repetitiveness, data dependency, and objective outputs. The third, focuses on a task specification template which details information on 16 critical components of tasks, and can be used as a checklist to select or adapt the most suitable AI solution for integration into existing workflows. Computational Management provides a roadmap and a toolkit for humans and AI to thrive together, while enhancing organizational efficiency and innovation.</p></p class="citation"></blockquote><h3 id=101236-on-the-standardization-of-behavioral-use-clauses-and-their-adoption-for-responsible-licensing-of-ai-daniel-mcduff-et-al-2024>(101/236) On the Standardization of Behavioral Use Clauses and Their Adoption for Responsible Licensing of AI (Daniel McDuff et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Daniel McDuff, Tim Korjakow, Scott Cambo, Jesse Josua Benjamin, Jenny Lee, Yacine Jernite, Carlos Muñoz Ferrandis, Aaron Gokaslan, Alek Tarkowski, Joseph Lindley, A. Feder Cooper, Danish Contractor. (2024)<br><strong>On the Standardization of Behavioral Use Clauses and Their Adoption for Responsible Licensing of AI</strong><br><button class=copy-to-clipboard title="On the Standardization of Behavioral Use Clauses and Their Adoption for Responsible Licensing of AI" index=101>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-101 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-AI, cs-SE, cs.SE<br>Keyword Score: 10<br>Keywords: BLOOM<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05979v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05979v1.pdf filename=2402.05979v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Growing concerns over negligent or malicious uses of AI have increased the appetite for tools that help manage the risks of the technology. In 2018, licenses with behaviorial-use clauses (commonly referred to as Responsible AI Licenses) were proposed to give developers a framework for releasing AI assets while specifying their users to mitigate negative applications. As of the end of 2023, on the order of 40,000 software and model repositories have adopted responsible AI licenses licenses. Notable models licensed with behavioral use clauses include <b>BLOOM</b> (language) and LLaMA2 (language), Stable Diffusion (image), and GRID (robotics). This paper explores why and how these licenses have been adopted, and why and how they have been adapted to fit particular use cases. We use a mixed-methods methodology of qualitative interviews, clustering of license clauses, and quantitative analysis of license adoption. Based on this evidence we take the position that responsible AI licenses need standardization to avoid confusing users or diluting their impact. At the same time, customization of behavioral restrictions is also appropriate in some contexts (e.g., medical domains). We advocate for ``standardized customization&rsquo;&rsquo; that can meet users&rsquo; needs and can be supported via tooling.</p></p class="citation"></blockquote><h3 id=102236-irfuzzer-specialized-fuzzing-for-llvm-backend-code-generation-yuyang-rong-et-al-2024>(102/236) IRFuzzer: Specialized Fuzzing for LLVM Backend Code Generation (Yuyang Rong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuyang Rong, Zhanghan Yu, Zhenkai Weng, Stephen Neuendorffer, Hao Chen. (2024)<br><strong>IRFuzzer: Specialized Fuzzing for LLVM Backend Code Generation</strong><br><button class=copy-to-clipboard title="IRFuzzer: Specialized Fuzzing for LLVM Backend Code Generation" index=102>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-102 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-SE, cs.SE<br>Keyword Score: 10<br>Keywords: Code Generation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05256v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05256v1.pdf filename=2402.05256v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Modern compilers, such as LLVM, are complex pieces of software. Due to their complexity, manual testing is unlikely to suffice, yet formal verification is difficult to scale. End-to-end fuzzing can be used, but it has difficulties in achieving high coverage of some components of LLVM. In this paper, we implement IRFuzzer to investigate the effectiveness of specialized fuzzing of the LLVM compiler backend. We focus on two approaches to improve the fuzzer: guaranteed input validity using constrained mutations and improved feedback quality. The mutator in IRFuzzer is capable of generating a wide range of LLVM IR inputs, including structured control flow, vector types, and function definitions. The system instruments coding patterns in the compiler to monitor the execution status of instruction selection. The instrumentation not only provides a new coverage feedback called matcher table coverage, but also provides an architecture specific guidance to the mutator. We show that IRFuzzer is more effective than existing fuzzers by fuzzing on 29 mature LLVM backend targets. In the process, we reported 74 confirmed new bugs in LLVM upstream, out of which 49 have been fixed, five have been back ported to LLVM 15, showing that specialized fuzzing provides useful and actionable insights to LLVM developers.</p></p class="citation"></blockquote><h2 id=cscv-38>cs.CV (38)</h2><h3 id=103236-llms-meet-vlms-boost-open-vocabulary-object-detection-with-fine-grained-descriptors-sheng-jin-et-al-2024>(103/236) LLMs Meet VLMs: Boost Open Vocabulary Object Detection with Fine-grained Descriptors (Sheng Jin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sheng Jin, Xueying Jiang, Jiaxing Huang, Lewei Lu, Shijian Lu. (2024)<br><strong>LLMs Meet VLMs: Boost Open Vocabulary Object Detection with Fine-grained Descriptors</strong><br><button class=copy-to-clipboard title="LLMs Meet VLMs: Boost Open Vocabulary Object Detection with Fine-grained Descriptors" index=103>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-103 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 70<br>Keywords: Object Detection, Knowledge Distillation, Zero-shot, Large Language Model, Large Language Model, Prompt, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04630v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04630v1.pdf filename=2402.04630v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Inspired by the outstanding <b>zero-shot</b> capability of vision language models (VLMs) in image classification tasks, open-vocabulary <b>object</b> <b>detection</b> has attracted increasing interest by <b>distilling</b> the broad VLM knowledge into detector training. However, most existing open-vocabulary detectors learn by aligning region embeddings with categorical labels (e.g., bicycle) only, disregarding the capability of VLMs on aligning visual embeddings with fine-grained text description of <b>object</b> <b>parts</b> (e.g., pedals and bells). This paper presents DVDet, a Descriptor-Enhanced Open Vocabulary Detector that introduces conditional context <b>prompts</b> and hierarchical textual descriptors that enable precise region-text alignment as well as open-vocabulary detection training in general. Specifically, the conditional context <b>prompt</b> transforms regional embeddings into image-like representations that can be directly integrated into general open vocabulary detection training. In addition, we introduce <b>large</b> <b>language</b> <b>models</b> as an interactive and implicit knowledge repository which enables iterative mining and refining visually oriented textual descriptors for precise region-text alignment. Extensive experiments over multiple <b>large-scale</b> <b>benchmarks</b> <b>show</b> that DVDet outperforms the state-of-the-art consistently by large margins.</p></p class="citation"></blockquote><h3 id=104236-colorswap-a-color-and-word-order-dataset-for-multimodal-evaluation-jirayu-burapacheep-et-al-2024>(104/236) ColorSwap: A Color and Word Order Dataset for Multimodal Evaluation (Jirayu Burapacheep et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jirayu Burapacheep, Ishan Gaur, Agam Bhatia, Tristan Thrush. (2024)<br><strong>ColorSwap: A Color and Word Order Dataset for Multimodal Evaluation</strong><br><button class=copy-to-clipboard title="ColorSwap: A Color and Word Order Dataset for Multimodal Evaluation" index=104>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-104 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CL, cs-CV, cs.CV<br>Keyword Score: 56<br>Keywords: Fine-tuning, Multi-modal, Multi-modal, Out-of-distribution, GPT, Image2text, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04492v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04492v1.pdf filename=2402.04492v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper introduces the ColorSwap dataset, designed to assess and improve the proficiency of <b>multimodal</b> models in matching objects with their colors. The dataset is comprised of 2,000 unique image-caption pairs, grouped into 1,000 examples. Each example includes a caption-image pair, along with a ``color-swapped&rsquo;&rsquo; pair. We follow the Winoground schema: the two captions in an example have the same words, but the color words have been rearranged to modify different objects. The dataset was created through a novel blend of automated caption and image generation with humans in the loop. We evaluate <b>image-text</b> matching (ITM) and visual language models (VLMs) and find that even the latest ones are still not robust at this task. <b>GPT-4V</b> and LLaVA score 72% and 42% on our main VLM metric, although they may improve with more advanced <b>prompting</b> techniques. On the main ITM metric, contrastive models such as CLIP and SigLIP perform close to chance (at 12% and 30%, respectively), although the non-contrastive BLIP ITM model is stronger (87%). We also find that <b>finetuning</b> on fewer than 2,000 examples yields significant performance gains on this <b>out-of-distribution</b> word-order understanding task. The dataset is here: <a href=https://github.com/Top34051/colorswap>https://github.com/Top34051/colorswap</a>.</p></p class="citation"></blockquote><h3 id=105236-source-free-domain-adaptation-with-diffusion-guided-source-data-generation-shivang-chopra-et-al-2024>(105/236) Source-Free Domain Adaptation with Diffusion-Guided Source Data Generation (Shivang Chopra et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shivang Chopra, Suraj Kothawade, Houda Aynaou, Aman Chadha. (2024)<br><strong>Source-Free Domain Adaptation with Diffusion-Guided Source Data Generation</strong><br><button class=copy-to-clipboard title="Source-Free Domain Adaptation with Diffusion-Guided Source Data Generation" index=105>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-105 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-LG, cs.CV<br>Keyword Score: 50<br>Keywords: Fine-tuning, Fine-tuning, Unsupervised Learning, Text2image, Domain Adaptation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04929v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04929v1.pdf filename=2402.04929v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper introduces a novel approach to leverage the generalizability capability of Diffusion Models for Source-Free <b>Domain</b> <b>Adaptation</b> (DM-SFDA). Our proposed DM-SFDA method involves <b>fine-tuning</b> a pre-trained <b>text-to-image</b> diffusion model to generate source <b>domain</b> <b>images</b> using features from the target images to guide the diffusion process. Specifically, the pre-trained diffusion model is <b>fine-tuned</b> to generate source samples that minimize entropy and maximize confidence for the pre-trained source model. We then apply established <b>unsupervised</b> <b>domain</b> <b>adaptation</b> techniques to align the generated source images with target <b>domain</b> <b>data.</b> We validate our approach through comprehensive experiments across a range of datasets, including Office-31, Office-Home, and VisDA. The results highlight significant improvements in SFDA performance, showcasing the potential of diffusion models in generating contextually relevant, <b>domain-specific</b> <b>images.</b></p></p class="citation"></blockquote><h3 id=106236-screenai-a-vision-language-model-for-ui-and-infographics-understanding-gilles-baechler-et-al-2024>(106/236) ScreenAI: A Vision-Language Model for UI and Infographics Understanding (Gilles Baechler et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Gilles Baechler, Srinivas Sunkara, Maria Wang, Fedir Zubach, Hassan Mansoor, Vincent Etter, Victor Cărbune, Jason Lin, Jindong Chen, Abhanshu Sharma. (2024)<br><strong>ScreenAI: A Vision-Language Model for UI and Infographics Understanding</strong><br><button class=copy-to-clipboard title="ScreenAI: A Vision-Language Model for UI and Infographics Understanding" index=106>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-106 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 50<br>Keywords: Question Answering, Question Answering, Large Language Model, Summarization, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04615v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04615v1.pdf filename=2402.04615v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Screen user interfaces (UIs) and infographics, sharing similar visual language and design principles, play important roles in human communication and human-machine interaction. We introduce ScreenAI, a <b>vision-language</b> model that specializes in UI and infographics understanding. Our model improves upon the PaLI architecture with the flexible patching strategy of pix2struct and is trained on a unique mixture of datasets. At the heart of this mixture is a novel screen annotation task in which the model has to identify the type and location of UI elements. We use these text annotations to describe screens to <b>Large</b> <b>Language</b> <b>Models</b> and automatically generate <b>question-answering</b> <b>(QA),</b> UI navigation, and <b>summarization</b> training datasets at scale. We run ablation studies to demonstrate the impact of these design choices. At only 5B parameters, ScreenAI achieves new state-of-the-artresults on UI- and infographics-based tasks (Multi-page DocVQA, WebSRC, MoTIF and Widget Captioning), and new best-in-class performance on others (Chart <b>QA,</b> DocVQA, and InfographicVQA) compared to models of similar size. Finally, we release three new datasets: one focused on the screen annotation task and two others focused on <b>question</b> <b>answering.</b></p></p class="citation"></blockquote><h3 id=107236-fm-fusion-instance-aware-semantic-mapping-boosted-by-vision-language-foundation-models-chuhao-liu-et-al-2024>(107/236) FM-Fusion: Instance-aware Semantic Mapping Boosted by Vision-Language Foundation Models (Chuhao Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chuhao Liu, Ke Wang, Jieqi Shi, Zhijian Qiao, Shaojie Shen. (2024)<br><strong>FM-Fusion: Instance-aware Semantic Mapping Boosted by Vision-Language Foundation Models</strong><br><button class=copy-to-clipboard title="FM-Fusion: Instance-aware Semantic Mapping Boosted by Vision-Language Foundation Models" index=107>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-107 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-RO, cs.CV<br>Keyword Score: 50<br>Keywords: Object Detection, Foundation Model, Supervised Learning, Zero-shot, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04555v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04555v1.pdf filename=2402.04555v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Semantic mapping based on the <b>supervised</b> <b>object</b> <b>detectors</b> is sensitive to image distribution. In real-world environments, the <b>object</b> <b>detection</b> and segmentation performance can lead to a major drop, preventing the use of semantic mapping in a wider domain. On the other hand, the development of <b>vision-language</b> <b>foundation</b> <b>models</b> demonstrates a strong <b>zero-shot</b> transferability across data distribution. It provides an opportunity to construct generalizable instance-aware semantic maps. Hence, this work explores how to boost instance-aware semantic mapping from <b>object</b> <b>detection</b> generated from <b>foundation</b> <b>models.</b> We propose a probabilistic label fusion method to predict close-set semantic classes from open-set label measurements. An instance refinement module merges the over-segmented instances caused by inconsistent segmentation. We integrate all the modules into a unified semantic mapping system. Reading a sequence of RGB-D input, our work incrementally reconstructs an instance-aware semantic map. We evaluate the <b>zero-shot</b> performance of our method in ScanNet and SceneNN datasets. Our method achieves 40.3 mean average precision (mAP) on the ScanNet semantic instance segmentation task. It outperforms the traditional semantic mapping method significantly.</p></p class="citation"></blockquote><h3 id=108236-knowledge-distillation-for-road-detection-based-on-cross-model-semi-supervised-learning-wanli-ma-et-al-2024>(108/236) Knowledge Distillation for Road Detection based on cross-model Semi-Supervised Learning (Wanli Ma et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wanli Ma, Oktay Karakus, Paul L. Rosin. (2024)<br><strong>Knowledge Distillation for Road Detection based on cross-model Semi-Supervised Learning</strong><br><button class=copy-to-clipboard title="Knowledge Distillation for Road Detection based on cross-model Semi-Supervised Learning" index=108>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-108 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Knowledge Distillation, Knowledge Distillation, Knowledge Distillation, Semi-Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05305v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05305v1.pdf filename=2402.05305v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The advancement of <b>knowledge</b> <b>distillation</b> has played a crucial role in enabling the transfer of <b>knowledge</b> <b>from</b> larger teacher models to smaller and more efficient student models, and is particularly beneficial for online and resource-constrained applications. The effectiveness of the student model heavily relies on the quality of the <b>distilled</b> <b>knowledge</b> <b>received</b> from the teacher. Given the accessibility of unlabelled remote sensing data, <b>semi-supervised</b> <b>learning</b> has become a prevalent strategy for enhancing model performance. However, relying solely on <b>semi-supervised</b> <b>learning</b> with smaller models may be insufficient due to their limited capacity for feature extraction. This limitation restricts their ability to exploit training data. To address this issue, we propose an integrated approach that combines <b>knowledge</b> <b>distillation</b> and <b>semi-supervised</b> <b>learning</b> methods. This hybrid approach leverages the robust capabilities of large models to effectively utilise large unlabelled data whilst subsequently providing the small student model with rich and informative features for enhancement. The proposed <b>semi-supervised</b> <b>learning-based</b> <b>knowledge</b> <b>distillation</b> (SSLKD) approach demonstrates a notable improvement in the performance of the student model, in the application of road segmentation, surpassing the effectiveness of traditional <b>semi-supervised</b> <b>learning</b> methods.</p></p class="citation"></blockquote><h3 id=109236-convlora-and-adabn-based-domain-adaptation-via-self-training-sidra-aleem-et-al-2024>(109/236) ConvLoRA and AdaBN based Domain Adaptation via Self-Training (Sidra Aleem et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sidra Aleem, Julia Dietlmeier, Eric Arazo, Suzanne Little. (2024)<br><strong>ConvLoRA and AdaBN based Domain Adaptation via Self-Training</strong><br><button class=copy-to-clipboard title="ConvLoRA and AdaBN based Domain Adaptation via Self-Training" index=109>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-109 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Convolution, Fine-tuning, Fine-tuning, Domain Adaptation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04964v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04964v1.pdf filename=2402.04964v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Existing <b>domain</b> <b>adaptation</b> (DA) methods often involve pre-training on the source <b>domain</b> <b>and</b> <b>fine-tuning</b> on the target <b>domain.</b> <b>For</b> multi-target <b>domain</b> <b>adaptation,</b> having a dedicated/separate <b>fine-tuned</b> network for each target <b>domain,</b> <b>that</b> retain all the pre-trained model parameters, is prohibitively expensive. To address this limitation, we propose <b>Convolutional</b> Low-Rank Adaptation (ConvLoRA). ConvLoRA freezes pre-trained model weights, adds trainable low-rank decomposition matrices to <b>convolutional</b> layers, and backpropagates the gradient through these matrices thus greatly reducing the number of trainable parameters. To further boost adaptation, we utilize Adaptive Batch Normalization (AdaBN) which computes target-specific running statistics and use it along with ConvLoRA. Our method has fewer trainable parameters and performs better or on-par with large independent <b>fine-tuned</b> networks (with less than 0.9% trainable parameters of the total base model) when tested on the segmentation of Calgary-Campinas dataset containing brain MRI images. Our approach is simple, yet effective and can be applied to any deep learning-based architecture which uses <b>convolutional</b> and batch normalization layers. Code is available at: <a href=https://github.com/aleemsidra/ConvLoRA>https://github.com/aleemsidra/ConvLoRA</a>.</p></p class="citation"></blockquote><h3 id=110236-sari-simplistic-average-and-robust-identification-based-noisy-partial-label-learning-darshana-saravanan-et-al-2024>(110/236) SARI: Simplistic Average and Robust Identification based Noisy Partial Label Learning (Darshana Saravanan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Darshana Saravanan, Naresh Manwani, Vineet Gandhi. (2024)<br><strong>SARI: Simplistic Average and Robust Identification based Noisy Partial Label Learning</strong><br><button class=copy-to-clipboard title="SARI: Simplistic Average and Robust Identification based Noisy Partial Label Learning" index=110>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-110 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 40<br>Keywords: Label Smoothing, Weakly-supervised Learning, Weakly-supervised Learning, Weakly Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04835v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04835v1.pdf filename=2402.04835v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Partial <b>label</b> <b>learning</b> (PLL) is a <b>weakly-supervised</b> <b>learning</b> <b>paradigm</b> where each training instance is paired with a set of candidate <b>labels</b> <b>(partial</b> <b>label),</b> <b>one</b> of which is the true <b>label.</b> <b>Noisy</b> PLL (NPLL) relaxes this constraint by allowing some partial <b>labels</b> <b>to</b> not contain the true <b>label,</b> <b>enhancing</b> the practicality of the problem. Our work centers on NPLL and presents a minimalistic framework called SARI that initially assigns pseudo-labels to images by exploiting the noisy partial <b>labels</b> <b>through</b> a weighted nearest neighbour algorithm. These pseudo-label and image pairs are then used to train a deep neural network classifier with <b>label</b> <b>smoothing</b> and standard regularization techniques. The classifier&rsquo;s features and predictions are subsequently employed to refine and enhance the accuracy of pseudo-labels. SARI combines the strengths of Average Based Strategies (in pseudo labelling) and Identification Based Strategies (in classifier training) from the literature. We perform thorough experiments on seven datasets and compare SARI against nine NPLL and PLL methods from the prior art. SARI achieves state-of-the-art results in almost all studied settings, obtaining substantial gains in fine-grained classification and extreme noise settings.</p></p class="citation"></blockquote><h3 id=111236-meet-jeanie-a-similarity-measure-for-3d-skeleton-sequences-via-temporal-viewpoint-alignment-lei-wang-et-al-2024>(111/236) Meet JEANIE: a Similarity Measure for 3D Skeleton Sequences via Temporal-Viewpoint Alignment (Lei Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lei Wang, Jun Liu, Liang Zheng, Tom Gedeon, Piotr Koniusz. (2024)<br><strong>Meet JEANIE: a Similarity Measure for 3D Skeleton Sequences via Temporal-Viewpoint Alignment</strong><br><button class=copy-to-clipboard title="Meet JEANIE: a Similarity Measure for 3D Skeleton Sequences via Temporal-Viewpoint Alignment" index=111>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-111 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-LG, cs.CV<br>Keyword Score: 40<br>Keywords: Few-shot, Meta Learning, Supervised Learning, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04599v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04599v1.pdf filename=2402.04599v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Video sequences exhibit significant nuisance variations (undesired effects) of speed of actions, temporal locations, and subjects&rsquo; poses, leading to temporal-viewpoint misalignment when comparing two sets of frames or evaluating the similarity of two sequences. Thus, we propose Joint tEmporal and cAmera viewpoiNt alIgnmEnt (JEANIE) for sequence pairs. In particular, we focus on 3D skeleton sequences whose camera and subjects&rsquo; poses can be easily manipulated in 3D. We evaluate JEANIE on skeletal <b>Few-shot</b> Action Recognition (FSAR), where matching well temporal blocks (temporal chunks that make up a sequence) of support-query sequence pairs (by factoring out nuisance variations) is essential due to limited samples of novel classes. Given a query sequence, we create its several views by simulating several camera locations. For a support sequence, we match it with view-simulated query sequences, as in the popular Dynamic Time Warping (DTW). Specifically, each support temporal block can be matched to the query temporal block with the same or adjacent (next) temporal index, and adjacent camera views to achieve joint local temporal-viewpoint warping. JEANIE selects the smallest distance among matching paths with different temporal-viewpoint warping patterns, an advantage over DTW which only performs temporal alignment. We also propose an <b>unsupervised</b> FSAR akin to clustering of sequences with JEANIE as a distance measure. JEANIE achieves state-of-the-art results on NTU-60, NTU-120, Kinetics-skeleton and UWA3D Multiview Activity II on <b>supervised</b> and <b>unsupervised</b> FSAR, and their <b>meta-learning</b> <b>inspired</b> fusion.</p></p class="citation"></blockquote><h3 id=112236-sparse-anatomical-prompt-semi-supervised-learning-with-masked-image-modeling-for-cbct-tooth-segmentation-pengyu-dai-et-al-2024>(112/236) Sparse Anatomical Prompt Semi-Supervised Learning with Masked Image Modeling for CBCT Tooth Segmentation (Pengyu Dai et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Pengyu Dai, Yafei Ou, Yang Liu, Yue Zhao. (2024)<br><strong>Sparse Anatomical Prompt Semi-Supervised Learning with Masked Image Modeling for CBCT Tooth Segmentation</strong><br><button class=copy-to-clipboard title="Sparse Anatomical Prompt Semi-Supervised Learning with Masked Image Modeling for CBCT Tooth Segmentation" index=112>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-112 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: I-4-6, cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Self-supervised Learning, Self-supervised Pre-training, Semi-Supervised Learning, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04587v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04587v1.pdf filename=2402.04587v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Accurate tooth identification and segmentation in Cone Beam Computed Tomography (CBCT) dental images can significantly enhance the efficiency and precision of manual diagnoses performed by dentists. However, existing segmentation methods are mainly developed based on large data volumes training, on which their annotations are extremely time-consuming. Meanwhile, the teeth of each class in CBCT dental images being closely positioned, coupled with subtle inter-class differences, gives rise to the challenge of indistinct boundaries when training model with limited data. To address these challenges, this study aims to propose a tasked-oriented Masked Auto-Encoder paradigm to effectively utilize large amounts of unlabeled data to achieve accurate tooth segmentation with limited labeled data. Specifically, we first construct a <b>self-supervised</b> <b>pre-training</b> framework of masked auto encoder to efficiently utilize unlabeled data to enhance the network performance. Subsequently, we introduce a sparse masked <b>prompt</b> mechanism based on graph attention to incorporate boundary information of the teeth, aiding the network in learning the anatomical structural features of teeth. To the best of our knowledge, we are pioneering the integration of the mask pre-training paradigm into the CBCT tooth segmentation task. Extensive experiments demonstrate both the feasibility of our proposed method and the potential of the boundary <b>prompt</b> mechanism.</p></p class="citation"></blockquote><h3 id=113236-attention-guided-cam-visual-explanations-of-vision-transformer-guided-by-self-attention-saebom-leem-et-al-2024>(113/236) Attention Guided CAM: Visual Explanations of Vision Transformer Guided by Self-Attention (Saebom Leem et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Saebom Leem, Hyunseok Seo. (2024)<br><strong>Attention Guided CAM: Visual Explanations of Vision Transformer Guided by Self-Attention</strong><br><button class=copy-to-clipboard title="Attention Guided CAM: Visual Explanations of Vision Transformer Guided by Self-Attention" index=113>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-113 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Convolutional Neural Network, Weakly-supervised Learning, Transformer, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04563v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04563v1.pdf filename=2402.04563v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Vision Transformer(ViT) is one of the most widely used models in the computer vision field with its great performance on various tasks. In order to fully utilize the ViT-based architecture in various applications, proper visualization methods with a decent localization performance are necessary, but these methods employed in <b>CNN-based</b> models are still not available in ViT due to its unique structure. In this work, we propose an attention-guided visualization method applied to ViT that provides a high-level semantic explanation for its decision. Our method selectively aggregates the gradients directly propagated from the classification output to each <b>self-attention,</b> collecting the contribution of image features extracted from each location of the input image. These gradients are additionally guided by the normalized <b>self-attention</b> scores, which are the pairwise patch correlation scores. They are used to supplement the gradients on the patch-level context information efficiently detected by the <b>self-attention</b> mechanism. This approach of our method provides elaborate high-level semantic explanations with great localization performance only with the class labels. As a result, our method outperforms the previous leading explainability methods of ViT in the <b>weakly-supervised</b> localization task and presents great capability in capturing the full instances of the target class object. Meanwhile, our method provides a visualization that faithfully explains the model, which is demonstrated in the perturbation comparison test.</p></p class="citation"></blockquote><h3 id=114236-spad--spatially-aware-multiview-diffusers-yash-kant-et-al-2024>(114/236) SPAD : Spatially Aware Multiview Diffusers (Yash Kant et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yash Kant, Ziyi Wu, Michael Vasilkovsky, Guocheng Qian, Jian Ren, Riza Alp Guler, Bernard Ghanem, Sergey Tulyakov, Igor Gilitschenski, Aliaksandr Siarohin. (2024)<br><strong>SPAD : Spatially Aware Multiview Diffusers</strong><br><button class=copy-to-clipboard title="SPAD : Spatially Aware Multiview Diffusers" index=114>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-114 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Fine-tuning, Prompt, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05235v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05235v1.pdf filename=2402.05235v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present SPAD, a novel approach for creating consistent multi-view images from text <b>prompts</b> or single images. To enable multi-view generation, we repurpose a pretrained 2D diffusion model by extending its <b>self-attention</b> layers with cross-view interactions, and <b>fine-tune</b> it on a high quality subset of Objaverse. We find that a naive extension of the <b>self-attention</b> proposed in prior work (e.g. MVDream) leads to content copying between views. Therefore, we explicitly constrain the cross-view attention based on epipolar geometry. To further enhance 3D consistency, we utilize Plucker coordinates derived from camera rays and inject them as positional encoding. This enables SPAD to reason over spatial proximity in 3D well. In contrast to recent works that can only generate views at fixed azimuth and elevation, SPAD offers full camera control and achieves state-of-the-art results in novel view synthesis on unseen objects from the Objaverse and Google Scanned Objects datasets. Finally, we demonstrate that text-to-3D generation using SPAD prevents the multi-face Janus issue. See more details at our webpage: <a href=https://yashkant.github.io/spad>https://yashkant.github.io/spad</a></p></p class="citation"></blockquote><h3 id=115236-efficientvit-sam-accelerated-segment-anything-model-without-performance-loss-zhuoyang-zhang-et-al-2024>(115/236) EfficientViT-SAM: Accelerated Segment Anything Model Without Performance Loss (Zhuoyang Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhuoyang Zhang, Han Cai, Song Han. (2024)<br><strong>EfficientViT-SAM: Accelerated Segment Anything Model Without Performance Loss</strong><br><button class=copy-to-clipboard title="EfficientViT-SAM: Accelerated Segment Anything Model Without Performance Loss" index=115>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-115 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-LG, cs.CV<br>Keyword Score: 30<br>Keywords: Knowledge Distillation, Knowledge Distillation, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05008v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05008v1.pdf filename=2402.05008v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present EfficientViT-SAM, a new family of accelerated segment anything models. We retain SAM&rsquo;s lightweight <b>prompt</b> encoder and mask decoder while replacing the heavy image encoder with EfficientViT. For the training, we begin with the <b>knowledge</b> <b>distillation</b> from the SAM-ViT-H image encoder to EfficientViT. Subsequently, we conduct end-to-end training on the SA-1B dataset. Benefiting from EfficientViT&rsquo;s efficiency and capacity, EfficientViT-SAM delivers 48.9x measured TensorRT speedup on A100 GPU over SAM-ViT-H without sacrificing performance. Our code and pre-trained models are released at <a href=https://github.com/mit-han-lab/efficientvit>https://github.com/mit-han-lab/efficientvit</a>.</p></p class="citation"></blockquote><h3 id=116236-toward-accurate-camera-based-3d-object-detection-via-cascade-depth-estimation-and-calibration-chaoqun-wang-et-al-2024>(116/236) Toward Accurate Camera-based 3D Object Detection via Cascade Depth Estimation and Calibration (Chaoqun Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chaoqun Wang, Yiran Qin, Zijian Kang, Ningning Ma, Ruimao Zhang. (2024)<br><strong>Toward Accurate Camera-based 3D Object Detection via Cascade Depth Estimation and Calibration</strong><br><button class=copy-to-clipboard title="Toward Accurate Camera-based 3D Object Detection via Cascade Depth Estimation and Calibration" index=116>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-116 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Object Detection, Supervised Learning, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04883v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04883v1.pdf filename=2402.04883v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent camera-based 3D <b>object</b> <b>detection</b> is limited by the precision of transforming from image to 3D feature spaces, as well as the accuracy of <b>object</b> <b>localization</b> within the 3D space. This paper aims to address such a fundamental problem of camera-based 3D <b>object</b> <b>detection:</b> How to effectively learn depth information for accurate feature lifting and <b>object</b> <b>localization.</b> Different from previous methods which directly predict depth distributions by using a <b>supervised</b> estimation model, we propose a cascade framework consisting of two depth-aware learning paradigms. First, a depth estimation (DE) scheme leverages relative depth information to realize the effective feature lifting from 2D to 3D spaces. Furthermore, a depth calibration (DC) scheme introduces depth reconstruction to further adjust the 3D <b>object</b> <b>localization</b> perturbation along the depth axis. In practice, the DE is explicitly realized by using both the absolute and relative depth optimization loss to promote the precision of depth prediction, while the capability of DC is implicitly embedded into the detection <b>Transformer</b> through a depth denoising mechanism in the training phase. The entire model training is accomplished through an end-to-end manner. We propose a baseline detector and evaluate the effectiveness of our proposal with +2.2%/+2.7% NDS/mAP improvements on NuScenes benchmark, and gain a comparable performance with 55.9%/45.7% NDS/mAP. Furthermore, we conduct extensive experiments to demonstrate its generality based on various detectors with about +2% NDS improvements.</p></p class="citation"></blockquote><h3 id=117236-advancing-anomaly-detection-an-adaptation-model-and-a-new-dataset-liyun-zhu-et-al-2024>(117/236) Advancing Anomaly Detection: An Adaptation Model and a New Dataset (Liyun Zhu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Liyun Zhu, Arjun Raj, Lei Wang. (2024)<br><strong>Advancing Anomaly Detection: An Adaptation Model and a New Dataset</strong><br><button class=copy-to-clipboard title="Advancing Anomaly Detection: An Adaptation Model and a New Dataset" index=117>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-117 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Few-shot, Few-shot Learning, Fine-tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04857v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04857v1.pdf filename=2402.04857v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Industry surveillance is widely applicable in sectors like retail, manufacturing, education, and smart cities, each presenting unique anomalies requiring specialized detection. However, adapting anomaly detection models to novel viewpoints within the same scenario poses challenges. Extending these models to entirely new scenarios necessitates retraining or <b>fine-tuning,</b> a process that can be time consuming. To address these challenges, we propose the Scenario-Adaptive Anomaly Detection (SA2D) method, leveraging the <b>few-shot</b> <b>learning</b> framework for faster adaptation of pre-trained models to new concepts. Despite this approach, a significant challenge emerges from the absence of a comprehensive dataset with diverse scenarios and camera views. In response, we introduce the Multi-Scenario Anomaly Detection (MSAD) dataset, encompassing 14 distinct scenarios captured from various camera views. This real-world dataset is the first high-resolution anomaly detection dataset, offering a solid foundation for training superior models. MSAD includes diverse normal motion patterns, incorporating challenging variations like different lighting and weather conditions. Through experimentation, we validate the efficacy of SA2D, particularly when trained on the MSAD dataset. Our results show that SA2D not only excels under novel viewpoints within the same scenario but also demonstrates competitive performance when faced with entirely new scenarios. This highlights our method&rsquo;s potential in addressing challenges in detecting anomalies across diverse and evolving surveillance scenarios.</p></p class="citation"></blockquote><h3 id=118236-color-recognition-in-challenging-lighting-environments-cnn-approach-nizamuddin-maitlo-et-al-2024>(118/236) Color Recognition in Challenging Lighting Environments: CNN Approach (Nizamuddin Maitlo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nizamuddin Maitlo, Nooruddin Noonari, Sajid Ahmed Ghanghro, Sathishkumar Duraisamy, Fayaz Ahmed. (2024)<br><strong>Color Recognition in Challenging Lighting Environments: CNN Approach</strong><br><button class=copy-to-clipboard title="Color Recognition in Challenging Lighting Environments: CNN Approach" index=118>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-118 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 30<br>Keywords: Convolution, Convolutional Neural Network, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04762v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04762v1.pdf filename=2402.04762v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Light plays a vital role in vision either human or machine vision, the perceived color is always based on the lighting conditions of the surroundings. Researchers are working to enhance the color detection techniques for the application of computer vision. They have implemented proposed several methods using different color detection approaches but still, there is a gap that can be filled. To address this issue, a color detection method, which is based on a <b>Convolutional</b> <b>Neural</b> <b>Network</b> <b>(CNN),</b> is proposed. Firstly, image segmentation is performed using the edge detection segmentation technique to specify the object and then the segmented object is fed to the <b>Convolutional</b> <b>Neural</b> <b>Network</b> trained to detect the color of an object in different lighting conditions. It is experimentally verified that our method can substantially enhance the robustness of color detection in different lighting conditions, and our method performed better results than existing methods.</p></p class="citation"></blockquote><h3 id=119236-progressive-conservative-adaptation-for-evolving-target-domains-gangming-zhao-et-al-2024>(119/236) Progressive Conservative Adaptation for Evolving Target Domains (Gangming Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Gangming Zhao, Chaoqi Chen, Wenhao He, Chengwei Pan, Chaowei Fang, Jinpeng Li, Xilin Chen, Yizhou Yu. (2024)<br><strong>Progressive Conservative Adaptation for Evolving Target Domains</strong><br><button class=copy-to-clipboard title="Progressive Conservative Adaptation for Evolving Target Domains" index=119>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-119 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Fine-tuning, Meta Learning, Domain Adaptation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04573v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04573v1.pdf filename=2402.04573v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Conventional <b>domain</b> <b>adaptation</b> typically transfers knowledge from a source <b>domain</b> <b>to</b> a stationary target <b>domain.</b> <b>However,</b> in many real-world cases, target data usually emerge sequentially and have continuously evolving distributions. Restoring and adapting to such target data results in escalating computational and resource consumption over time. Hence, it is vital to devise algorithms to address the evolving <b>domain</b> <b>adaptation</b> (EDA) problem, \emph{i.e.,} adapting models to evolving target <b>domains</b> <b>without</b> access to historic target <b>domains.</b> <b>To</b> achieve this goal, we propose a simple yet effective approach, termed progressive conservative adaptation (PCAda). To manage new target data that diverges from previous distributions, we <b>fine-tune</b> the classifier head based on the progressively updated class prototypes. Moreover, as adjusting to the most recent target <b>domain</b> <b>can</b> interfere with the features learned from previous target <b>domains,</b> <b>we</b> develop a conservative sparse attention mechanism. This mechanism restricts feature adaptation within essential dimensions, thus easing the inference related to historical knowledge. The proposed PCAda is implemented with a <b>meta-learning</b> <b>framework,</b> which achieves the fast adaptation of the classifier with the help of the progressively updated class prototypes in the inner loop and learns a generalized feature without severely interfering with the historic knowledge via the conservative sparse attention in the outer loop. Experiments on Rotated MNIST, Caltran, and Portraits datasets demonstrate the effectiveness of our method.</p></p class="citation"></blockquote><h3 id=120236-physics-informed-and-data-driven-simulation-of-underwater-images-via-residual-learning-tanmoy-mondal-et-al-2024>(120/236) Physics Informed and Data Driven Simulation of Underwater Images via Residual Learning (Tanmoy Mondal et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tanmoy Mondal, Ricardo Mendoza, Lucas Drumetz. (2024)<br><strong>Physics Informed and Data Driven Simulation of Underwater Images via Residual Learning</strong><br><button class=copy-to-clipboard title="Physics Informed and Data Driven Simulation of Underwater Images via Residual Learning" index=120>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-120 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV, eess-IV<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05281v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05281v1.pdf filename=2402.05281v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In general, underwater images suffer from color distortion and low contrast, because light is attenuated and backscattered as it propagates through water (differently depending on wavelength and on the properties of the water body). An existing simple degradation model (similar to atmospheric image &ldquo;hazing&rdquo; effects), though helpful, is not sufficient to properly represent the underwater image degradation because there are unaccounted for and non-measurable factors e.g. scattering of light due to turbidity of water, reflective characteristics of turbid medium etc. We propose a deep learning-based architecture to automatically simulate the underwater effects where only a dehazing-like image formation equation is known to the network, and the additional degradation due to the other unknown factors if inferred in a data-driven way. We only use RGB images (because in real-time scenario depth image is not available) to estimate the depth image. For testing, we have proposed (due to the lack of real underwater image datasets) a complex image formation model/equation to manually generate images that resemble real underwater images (used as ground truth). However, only the classical image formation equation (the one used for image dehazing) is informed to the network. This mimics the fact that in a real scenario, the physics are never completely known and only simplified models are known. Thanks to the ground truth, generated by a complex image formation equation, we could successfully perform a qualitative and quantitative evaluation of proposed technique, compared to other purely data driven approaches</p></p class="citation"></blockquote><h3 id=121236-λ-eclipse-multi-concept-personalized-text-to-image-diffusion-models-by-leveraging-clip-latent-space-maitreya-patel-et-al-2024>(121/236) $λ$-ECLIPSE: Multi-Concept Personalized Text-to-Image Diffusion Models by Leveraging CLIP Latent Space (Maitreya Patel et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Maitreya Patel, Sangmin Jung, Chitta Baral, Yezhou Yang. (2024)<br><strong>$λ$-ECLIPSE: Multi-Concept Personalized Text-to-Image Diffusion Models by Leveraging CLIP Latent Space</strong><br><button class=copy-to-clipboard title="$λ$-ECLIPSE: Multi-Concept Personalized Text-to-Image Diffusion Models by Leveraging CLIP Latent Space" index=121>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-121 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CL, cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Image2text, Text2image<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05195v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05195v1.pdf filename=2402.05195v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Despite the recent advances in personalized <b>text-to-image</b> (P-T2I) generative models, subject-driven T2I remains challenging. The primary bottlenecks include 1) Intensive training resource requirements, 2) Hyper-parameter sensitivity leading to inconsistent outputs, and 3) Balancing the intricacies of novel visual concept and composition alignment. We start by re-iterating the core philosophy of T2I diffusion models to address the above limitations. Predominantly, contemporary subject-driven T2I approaches hinge on Latent Diffusion Models (LDMs), which facilitate T2I mapping through cross-attention layers. While LDMs offer distinct advantages, P-T2I methods&rsquo; reliance on the latent space of these diffusion models significantly escalates resource demands, leading to inconsistent results and necessitating numerous iterations for a single desired image. Recently, ECLIPSE has demonstrated a more resource-efficient pathway for training UnCLIP-based T2I models, circumventing the need for diffusion <b>text-to-image</b> priors. Building on this, we introduce $\lambda$-ECLIPSE. Our method illustrates that effective P-T2I does not necessarily depend on the latent space of diffusion models. $\lambda$-ECLIPSE achieves single, multi-subject, and edge-guided T2I personalization with just 34M parameters and is trained on a mere 74 GPU hours using 1.6M <b>image-text</b> interleaved data. Through extensive experiments, we also establish that $\lambda$-ECLIPSE surpasses existing baselines in composition alignment while preserving concept alignment performance, even with significantly lower resource utilization.</p></p class="citation"></blockquote><h3 id=122236-star-shape-focused-texture-agnostic-representations-for-improved-object-detection-and-6d-pose-estimation-peter-hönig-et-al-2024>(122/236) STAR: Shape-focused Texture Agnostic Representations for Improved Object Detection and 6D Pose Estimation (Peter Hönig et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Peter Hönig, Stefan Thalhammer, Jean-Baptiste Weibel, Matthias Hirschmanner, Markus Vincze. (2024)<br><strong>STAR: Shape-focused Texture Agnostic Representations for Improved Object Detection and 6D Pose Estimation</strong><br><button class=copy-to-clipboard title="STAR: Shape-focused Texture Agnostic Representations for Improved Object Detection and 6D Pose Estimation" index=122>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-122 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Object Detection, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04878v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04878v1.pdf filename=2402.04878v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advances in machine learning have greatly benefited <b>object</b> <b>detection</b> and 6D pose estimation for robotic grasping. However, textureless and metallic <b>objects</b> <b>still</b> pose a significant challenge due to fewer visual cues and the texture bias of <b>CNNs.</b> To address this issue, we propose a texture-agnostic approach that focuses on learning from CAD models and emphasizes <b>object</b> <b>shape</b> features. To achieve a focus on learning shape features, the textures are randomized during the rendering of the training data. By treating the texture as noise, the need for real-world <b>object</b> <b>instances</b> or their final appearance during training data generation is eliminated. The TLESS and ITODD datasets, specifically created for industrial settings in robotics and featuring textureless and metallic <b>objects,</b> <b>were</b> used for evaluation. Texture agnosticity also increases the robustness against image perturbations such as imaging noise, motion blur, and brightness changes, which are common in robotics applications. Code and datasets are publicly available at github.com/hoenigpeter/randomized_texturing.</p></p class="citation"></blockquote><h3 id=123236-dual-path-coupled-image-deraining-network-via-spatial-frequency-interaction-yuhong-he-et-al-2024>(123/236) Dual-Path Coupled Image Deraining Network via Spatial-Frequency Interaction (Yuhong He et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuhong He, Aiwen Jiang, Lingfang Jiang, Zhifeng Wang, Lu Wang. (2024)<br><strong>Dual-Path Coupled Image Deraining Network via Spatial-Frequency Interaction</strong><br><button class=copy-to-clipboard title="Dual-Path Coupled Image Deraining Network via Spatial-Frequency Interaction" index=123>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-123 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Transformer, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04855v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04855v1.pdf filename=2402.04855v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Transformers</b> have recently emerged as a significant force in the field of image deraining. Existing image deraining methods utilize extensive research on <b>self-attention.</b> Though showcasing impressive results, they tend to neglect critical frequency information, as <b>self-attention</b> is generally less adept at capturing high-frequency details. To overcome this shortcoming, we have developed an innovative Dual-Path Coupled Deraining Network (DPCNet) that integrates information from both spatial and frequency domains through Spatial Feature Extraction Block (SFEBlock) and Frequency Feature Extraction Block (FFEBlock). We have further introduced an effective Adaptive Fusion Module (AFM) for the dual-path feature aggregation. Extensive experiments on six public deraining benchmarks and downstream vision tasks have demonstrated that our proposed method not only outperforms the existing state-of-the-art deraining method but also achieves visually pleasuring results with excellent robustness on downstream vision tasks.</p></p class="citation"></blockquote><h3 id=124236-nerf-as-non-distant-environment-emitter-in-physics-based-inverse-rendering-jingwang-ling-et-al-2024>(124/236) NeRF as Non-Distant Environment Emitter in Physics-based Inverse Rendering (Jingwang Ling et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jingwang Ling, Ruihan Yu, Feng Xu, Chun Du, Shuang Zhao. (2024)<br><strong>NeRF as Non-Distant Environment Emitter in Physics-based Inverse Rendering</strong><br><button class=copy-to-clipboard title="NeRF as Non-Distant Environment Emitter in Physics-based Inverse Rendering" index=124>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-124 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-GR, cs.CV<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04829v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04829v1.pdf filename=2402.04829v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Physics-based inverse rendering aims to jointly optimize shape, materials, and lighting from captured 2D images. Here lighting is an important part of achieving faithful light transport <b>simulation.</b> While the environment map is commonly used as the lighting model in inverse rendering, we show that its distant lighting assumption leads to spatial invariant lighting, which can be an inaccurate approximation in real-world inverse rendering. We propose to use NeRF as a spatially varying environment lighting model and build an inverse rendering pipeline using NeRF as the non-distant environment emitter. By comparing our method with the environment map on real and synthetic datasets, we show that our NeRF-based emitter models the scene lighting more accurately and leads to more accurate inverse rendering. Project page and video: <a href=https://nerfemitterpbir.github.io/>https://nerfemitterpbir.github.io/</a>.</p></p class="citation"></blockquote><h3 id=125236-spiking-physformer-camera-based-remote-photoplethysmography-with-parallel-spike-driven-transformer-mingxuan-liu-et-al-2024>(125/236) Spiking-PhysFormer: Camera-Based Remote Photoplethysmography with Parallel Spike-driven Transformer (Mingxuan Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mingxuan Liu, Jiankai Tang, Haoxiang Li, Jiahao Qi, Siwei Li, Kegang Wang, Yuntao Wang, Hong Chen. (2024)<br><strong>Spiking-PhysFormer: Camera-Based Remote Photoplethysmography with Parallel Spike-driven Transformer</strong><br><button class=copy-to-clipboard title="Spiking-PhysFormer: Camera-Based Remote Photoplethysmography with Parallel Spike-driven Transformer" index=125>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-125 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Transformer, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04798v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04798v2.pdf filename=2402.04798v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Artificial neural networks (ANNs) can help camera-based remote photoplethysmography (rPPG) in measuring cardiac activity and physiological signals from facial videos, such as pulse wave, heart rate and respiration rate with better accuracy. However, most existing ANN-based methods require substantial computing resources, which poses challenges for effective deployment on mobile devices. Spiking neural networks (SNNs), on the other hand, hold immense potential for energy-efficient deep learning owing to their binary and event-driven architecture. To the best of our knowledge, we are the first to introduce SNNs into the realm of rPPG, proposing a hybrid neural network (HNN) model, the Spiking-PhysFormer, aimed at reducing power consumption. Specifically, the proposed Spiking-PhyFormer consists of an ANN-based patch embedding block, SNN-based <b>transformer</b> blocks, and an ANN-based predictor head. First, to simplify the <b>transformer</b> block while preserving its capacity to aggregate local and global spatio-temporal features, we design a parallel spike <b>transformer</b> block to replace sequential sub-blocks. Additionally, we propose a simplified spiking <b>self-attention</b> mechanism that omits the value parameter without compromising the model&rsquo;s performance. Experiments conducted on four datasets-PURE, UBFC-rPPG, UBFC-Phys, and MMPD demonstrate that the proposed model achieves a 12.4% reduction in power consumption compared to PhysFormer. Additionally, the power consumption of the <b>transformer</b> block is reduced by a factor of 12.2, while maintaining decent performance as PhysFormer and other ANN-based models.</p></p class="citation"></blockquote><h3 id=126236-ov-nerf-open-vocabulary-neural-radiance-fields-with-vision-and-language-foundation-models-for-3d-semantic-understanding-guibiao-liao-et-al-2024>(126/236) OV-NeRF: Open-vocabulary Neural Radiance Fields with Vision and Language Foundation Models for 3D Semantic Understanding (Guibiao Liao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Guibiao Liao, Kaichen Zhou, Zhenyu Bao, Kanglin Liu, Qing Li. (2024)<br><strong>OV-NeRF: Open-vocabulary Neural Radiance Fields with Vision and Language Foundation Models for 3D Semantic Understanding</strong><br><button class=copy-to-clipboard title="OV-NeRF: Open-vocabulary Neural Radiance Fields with Vision and Language Foundation Models for 3D Semantic Understanding" index=126>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-126 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Foundation Model, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04648v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04648v1.pdf filename=2402.04648v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The development of Neural Radiance Fields (NeRFs) has provided a potent representation for encapsulating the geometric and appearance characteristics of 3D scenes. Enhancing the capabilities of NeRFs in open-vocabulary 3D semantic perception tasks has been a recent focus. However, current methods that extract semantics directly from Contrastive Language-Image Pretraining (CLIP) for semantic field learning encounter difficulties due to noisy and view-inconsistent semantics provided by CLIP. To tackle these limitations, we propose OV-NeRF, which exploits the potential of pre-trained vision and language <b>foundation</b> <b>models</b> to enhance semantic field learning through proposed single-view and cross-view strategies. First, from the single-view perspective, we introduce Region Semantic Ranking (RSR) regularization by leveraging 2D mask proposals derived from SAM to rectify the noisy semantics of each training view, facilitating accurate semantic field learning. Second, from the cross-view perspective, we propose a Cross-view Self-enhancement (CSE) strategy to address the challenge raised by view-inconsistent semantics. Rather than invariably utilizing the 2D inconsistent semantics from CLIP, CSE leverages the 3D consistent semantics generated from the well-trained semantic field itself for semantic field training, aiming to reduce ambiguity and enhance overall semantic consistency across different views. Extensive experiments validate our OV-NeRF outperforms current state-of-the-art methods, achieving a significant improvement of 20.31% and 18.42% in mIoU metric on Replica and Scannet, respectively. Furthermore, our approach exhibits consistent superior results across various CLIP configurations, further verifying its robustness.</p></p class="citation"></blockquote><h3 id=127236-dmat-a-dynamic-mask-aware-transformer-for-human-de-occlusion-guoqiang-liang-et-al-2024>(127/236) DMAT: A Dynamic Mask-Aware Transformer for Human De-occlusion (Guoqiang Liang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Guoqiang Liang, Jiahao Hu, Qingyue Wang, Shizhou Zhang. (2024)<br><strong>DMAT: A Dynamic Mask-Aware Transformer for Human De-occlusion</strong><br><button class=copy-to-clipboard title="DMAT: A Dynamic Mask-Aware Transformer for Human De-occlusion" index=127>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-127 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Convolution, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04558v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04558v1.pdf filename=2402.04558v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Human de-occlusion, which aims to infer the appearance of invisible human parts from an occluded image, has great value in many human-related tasks, such as person re-id, and intention inference. To address this task, this paper proposes a dynamic mask-aware <b>transformer</b> (DMAT), which dynamically augments information from human regions and weakens that from occlusion. First, to enhance token representation, we design an expanded <b>convolution</b> head with enlarged kernels, which captures more local valid context and mitigates the influence of surrounding occlusion. To concentrate on the visible human parts, we propose a novel dynamic multi-head human-mask guided attention mechanism through integrating multiple masks, which can prevent the de-occluded regions from assimilating to the background. Besides, a region upsampling strategy is utilized to alleviate the impact of occlusion on interpolated images. During model learning, an amodal loss is developed to further emphasize the recovery effect of human regions, which also refines the model&rsquo;s convergence. Extensive experiments on the AHP dataset demonstrate its superior performance compared to recent state-of-the-art methods.</p></p class="citation"></blockquote><h3 id=128236-instructscene-instruction-driven-3d-indoor-scene-synthesis-with-semantic-graph-prior-chenguo-lin-et-al-2024>(128/236) InstructScene: Instruction-Driven 3D Indoor Scene Synthesis with Semantic Graph Prior (Chenguo Lin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chenguo Lin, Yadong Mu. (2024)<br><strong>InstructScene: Instruction-Driven 3D Indoor Scene Synthesis with Semantic Graph Prior</strong><br><button class=copy-to-clipboard title="InstructScene: Instruction-Driven 3D Indoor Scene Synthesis with Semantic Graph Prior" index=128>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-128 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 16<br>Keywords: Multi-modal, Multi-modal, Zero-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04717v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04717v1.pdf filename=2402.04717v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Comprehending natural language instructions is a charming property for 3D indoor scene synthesis systems. Existing methods directly model object joint distributions and express object relations implicitly within a scene, thereby hindering the controllability of generation. We introduce InstructScene, a novel generative framework that integrates a semantic graph prior and a layout decoder to improve controllability and fidelity for 3D scene synthesis. The proposed semantic graph prior jointly learns scene appearances and layout distributions, exhibiting versatility across various downstream tasks in a <b>zero-shot</b> manner. To facilitate the benchmarking for text-driven 3D scene synthesis, we curate a high-quality dataset of scene-instruction pairs with large language and <b>multimodal</b> models. Extensive experimental results reveal that the proposed method surpasses existing state-of-the-art approaches by a large margin. Thorough ablation studies confirm the efficacy of crucial design components. Project page: <a href=https://chenguolin.github.io/projects/InstructScene>https://chenguolin.github.io/projects/InstructScene</a>.</p></p class="citation"></blockquote><h3 id=129236-image-captioning-for-brazilian-portuguese-using-grit-model-rafael-silva-de-alencar-et-al-2024>(129/236) Image captioning for Brazilian Portuguese using GRIT model (Rafael Silva de Alencar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rafael Silva de Alencar, William Alberto Cruz Castañeda, Marcellus Amadeus. (2024)<br><strong>Image captioning for Brazilian Portuguese using GRIT model</strong><br><button class=copy-to-clipboard title="Image captioning for Brazilian Portuguese using GRIT model" index=129>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-129 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CL, cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05106v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05106v1.pdf filename=2402.05106v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This work presents the early development of a model of image captioning for the Brazilian Portuguese language. We used the GRIT (Grid - and Region-based Image captioning <b>Transformer)</b> model to accomplish this work. GRIT is a <b>Transformer-only</b> neural architecture that effectively utilizes two visual features to generate better captions. The GRIT method emerged as a proposal to be a more efficient way to generate image captioning. In this work, we adapt the GRIT model to be trained in a Brazilian Portuguese dataset to have an image captioning method for the Brazilian Portuguese Language.</p></p class="citation"></blockquote><h3 id=130236-enhancement-of-bengali-ocr-by-specialized-models-and-advanced-techniques-for-diverse-document-types-akm-shahariar-azad-rabby-et-al-2024>(130/236) Enhancement of Bengali OCR by Specialized Models and Advanced Techniques for Diverse Document Types (AKM Shahariar Azad Rabby et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>AKM Shahariar Azad Rabby, Hasmot Ali, Md. Majedul Islam, Sheikh Abujar, Fuad Rahman. (2024)<br><strong>Enhancement of Bengali OCR by Specialized Models and Advanced Techniques for Diverse Document Types</strong><br><button class=copy-to-clipboard title="Enhancement of Bengali OCR by Specialized Models and Advanced Techniques for Diverse Document Types" index=130>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-130 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-LG, cs.CV<br>Keyword Score: 10<br>Keywords: Optical Character Recognition<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05158v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05158v1.pdf filename=2402.05158v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This research paper presents a unique Bengali <b>OCR</b> system with some capabilities. The system excels in reconstructing document layouts while preserving structure, alignment, and images. It incorporates advanced image and signature detection for accurate extraction. Specialized models for word segmentation cater to diverse document types, including computer-composed, letterpress, typewriter, and handwritten documents. The system handles static and dynamic handwritten inputs, recognizing various writing styles. Furthermore, it has the ability to recognize compound characters in Bengali. Extensive data collection efforts provide a diverse corpus, while advanced technical components optimize character and word recognition. Additional contributions include image, logo, signature and table recognition, perspective correction, layout reconstruction, and a queuing module for efficient and scalable processing. The system demonstrates outstanding performance in efficient and accurate text extraction and analysis.</p></p class="citation"></blockquote><h3 id=131236-lgm-large-multi-view-gaussian-model-for-high-resolution-3d-content-creation-jiaxiang-tang-et-al-2024>(131/236) LGM: Large Multi-View Gaussian Model for High-Resolution 3D Content Creation (Jiaxiang Tang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiaxiang Tang, Zhaoxi Chen, Xiaokang Chen, Tengfei Wang, Gang Zeng, Ziwei Liu. (2024)<br><strong>LGM: Large Multi-View Gaussian Model for High-Resolution 3D Content Creation</strong><br><button class=copy-to-clipboard title="LGM: Large Multi-View Gaussian Model for High-Resolution 3D Content Creation" index=131>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-131 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05054v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05054v1.pdf filename=2402.05054v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>3D content creation has achieved significant progress in terms of both quality and speed. Although current feed-forward models can produce 3D objects in seconds, their resolution is constrained by the intensive computation required during training. In this paper, we introduce Large Multi-View Gaussian Model (LGM), a novel framework designed to generate high-resolution 3D models from text <b>prompts</b> or single-view images. Our key insights are two-fold: 1) 3D Representation: We propose multi-view Gaussian features as an efficient yet powerful representation, which can then be fused together for differentiable rendering. 2) 3D Backbone: We present an asymmetric U-Net as a high-throughput backbone operating on multi-view images, which can be produced from text or single-view image input by leveraging multi-view diffusion models. Extensive experiments demonstrate the high fidelity and efficiency of our approach. Notably, we maintain the fast speed to generate 3D objects within 5 seconds while boosting the training resolution to 512, thereby achieving high-resolution 3D content generation.</p></p class="citation"></blockquote><h3 id=132236-a-survey-on-domain-generalization-for-medical-image-analysis-ziwei-niu-et-al-2024>(132/236) A Survey on Domain Generalization for Medical Image Analysis (Ziwei Niu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ziwei Niu, Shuyi Ouyang, Shiao Xie, Yen-wei Chen, Lanfen Lin. (2024)<br><strong>A Survey on Domain Generalization for Medical Image Analysis</strong><br><button class=copy-to-clipboard title="A Survey on Domain Generalization for Medical Image Analysis" index=132>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-132 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05035v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05035v2.pdf filename=2402.05035v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Medical Image Analysis (MedIA) has emerged as a crucial tool in computer-aided diagnosis systems, particularly with the advancement of deep learning (DL) in recent years. However, well-trained deep models often experience significant performance degradation when deployed in different medical sites, modalities, and sequences, known as a domain shift issue. In light of this, Domain Generalization (DG) for MedIA aims to address the domain shift challenge by generalizing effectively and performing robustly across unknown data distributions. This paper presents the a comprehensive review of substantial developments in this area. First, we provide a formal definition of domain shift and domain generalization in medical field, and discuss several related settings. Subsequently, we <b>summarize</b> the recent methods from three viewpoints: data manipulation level, feature representation level, and model training level, and present some algorithms in detail for each viewpoints. Furthermore, we introduce the commonly used datasets. Finally, we <b>summarize</b> existing literature and present some potential research topics for the future. For this survey, we also created a GitHub project by collecting the supporting resources, at the link: <a href=https://github.com/Ziwei-Niu/DG_for_MedIA>https://github.com/Ziwei-Niu/DG_for_MedIA</a></p></p class="citation"></blockquote><h3 id=133236-data-efficient-large-vision-models-through-sequential-autoregression-jianyuan-guo-et-al-2024>(133/236) Data-efficient Large Vision Models through Sequential Autoregression (Jianyuan Guo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jianyuan Guo, Zhiwei Hao, Chengcheng Wang, Yehui Tang, Han Wu, Han Hu, Kai Han, Chang Xu. (2024)<br><strong>Data-efficient Large Vision Models through Sequential Autoregression</strong><br><button class=copy-to-clipboard title="Data-efficient Large Vision Models through Sequential Autoregression" index=133>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-133 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Out-of-domain<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04841v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04841v1.pdf filename=2402.04841v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Training general-purpose vision models on purely sequential visual data, eschewing linguistic inputs, has heralded a new frontier in visual understanding. These models are intended to not only comprehend but also seamlessly transit to <b>out-of-domain</b> tasks. However, current endeavors are hamstrung by an over-reliance on colossal models, exemplified by models with upwards of 3B parameters, and the necessity for an extensive corpus of visual data, often comprising a staggering 400B tokens. In this paper, we delve into the development of an efficient, autoregression-based vision model, innovatively architected to operate on a limited dataset. We meticulously demonstrate how this model achieves proficiency in a spectrum of visual tasks spanning both high-level and low-level semantic understanding during the testing phase. Our empirical evaluations underscore the model&rsquo;s agility in adapting to various tasks, heralding a significant reduction in the parameter footprint, and a marked decrease in training data requirements, thereby paving the way for more sustainable and accessible advancements in the field of generalist vision models. The code is available at <a href=https://github.com/ggjy/DeLVM>https://github.com/ggjy/DeLVM</a>.</p></p class="citation"></blockquote><h3 id=134236-boundary-aware-contrastive-learning-for-semi-supervised-nuclei-instance-segmentation-ye-zhang-et-al-2024>(134/236) Boundary-aware Contrastive Learning for Semi-supervised Nuclei Instance Segmentation (Ye Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ye Zhang, Ziyue Wang, Yifeng Wang, Hao Bian, Linghan Cai, Hengrui Li, Lingbo Zhang, Yongbing Zhang. (2024)<br><strong>Boundary-aware Contrastive Learning for Semi-supervised Nuclei Instance Segmentation</strong><br><button class=copy-to-clipboard title="Boundary-aware Contrastive Learning for Semi-supervised Nuclei Instance Segmentation" index=134>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-134 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Contrastive Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04756v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04756v1.pdf filename=2402.04756v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Semi-supervised segmentation methods have demonstrated promising results in natural scenarios, providing a solution to reduce dependency on manual annotation. However, these methods face significant challenges when directly applied to pathological images due to the subtle color differences between nuclei and tissues, as well as the significant morphological variations among nuclei. Consequently, the generated pseudo-labels often contain much noise, especially at the nuclei boundaries. To address the above problem, this paper proposes a boundary-aware <b>contrastive</b> <b>learning</b> network to denoise the boundary noise in a semi-supervised nuclei segmentation task. The model has two key designs: a low-resolution denoising (LRD) module and a cross-RoI <b>contrastive</b> <b>learning</b> (CRC) module. The LRD improves the smoothness of the nuclei boundary by pseudo-labels denoising, and the CRC enhances the discrimination between foreground and background by boundary feature <b>contrastive</b> <b>learning.</b> We conduct extensive experiments to demonstrate the superiority of our proposed method over existing semi-supervised instance segmentation methods.</p></p class="citation"></blockquote><h3 id=135236-towards-aligned-layout-generation-via-diffusion-model-with-aesthetic-constraints-jian-chen-et-al-2024>(135/236) Towards Aligned Layout Generation via Diffusion Model with Aesthetic Constraints (Jian Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jian Chen, Ruiyi Zhang, Yufan Zhou, Changyou Chen. (2024)<br><strong>Towards Aligned Layout Generation via Diffusion Model with Aesthetic Constraints</strong><br><button class=copy-to-clipboard title="Towards Aligned Layout Generation via Diffusion Model with Aesthetic Constraints" index=135>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-135 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 10<br>Keywords: Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04754v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04754v1.pdf filename=2402.04754v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Controllable layout generation refers to the process of creating a plausible visual arrangement of elements within a graphic design (e.g., document and web designs) with constraints representing design intentions. Although recent diffusion-based models have achieved state-of-the-art FID scores, they tend to exhibit more pronounced misalignment compared to earlier <b>transformer-based</b> models. In this work, we propose the $\textbf{LA}$yout $\textbf{C}$onstraint diffusion mod$\textbf{E}$l (LACE), a unified model to handle a broad range of layout generation tasks, such as arranging elements with specified attributes and refining or completing a coarse layout design. The model is based on continuous diffusion models. Compared with existing methods that use discrete diffusion models, continuous state-space design can enable the incorporation of differentiable aesthetic constraint functions in training. For conditional generation, we introduce conditions via masked input. Extensive experiment results show that LACE produces high-quality layouts and outperforms existing state-of-the-art baselines.</p></p class="citation"></blockquote><h3 id=136236-g-nas-generalizable-neural-architecture-search-for-single-domain-generalization-object-detection-fan-wu-et-al-2024>(136/236) G-NAS: Generalizable Neural Architecture Search for Single Domain Generalization Object Detection (Fan Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fan Wu, Jinling Gao, Lanqing Hong, Xinbing Wang, Chenghu Zhou, Nanyang Ye. (2024)<br><strong>G-NAS: Generalizable Neural Architecture Search for Single Domain Generalization Object Detection</strong><br><button class=copy-to-clipboard title="G-NAS: Generalizable Neural Architecture Search for Single Domain Generalization Object Detection" index=136>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-136 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Object Detection<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04672v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04672v1.pdf filename=2402.04672v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we focus on a realistic yet challenging task, Single Domain Generalization <b>Object</b> <b>Detection</b> (S-DGOD), where only one source domain&rsquo;s data can be used for training <b>object</b> <b>detectors,</b> but have to generalize multiple distinct target domains. In S-DGOD, both high-capacity fitting and generalization abilities are needed due to the task&rsquo;s complexity. Differentiable Neural Architecture Search (NAS) is known for its high capacity for complex data fitting and we propose to leverage Differentiable NAS to solve S-DGOD. However, it may confront severe over-fitting issues due to the feature imbalance phenomenon, where parameters optimized by gradient descent are biased to learn from the easy-to-learn features, which are usually non-causal and spuriously correlated to ground truth labels, such as the features of background in <b>object</b> <b>detection</b> data. Consequently, this leads to serious performance degradation, especially in generalizing to unseen target domains with huge domain gaps between the source domain and target domains. To address this issue, we propose the Generalizable loss (G-loss), which is an OoD-aware objective, preventing NAS from over-fitting by using gradient descent to optimize parameters not only on a subset of easy-to-learn features but also the remaining predictive features for generalization, and the overall framework is named G-NAS. Experimental results on the S-DGOD urban-scene datasets demonstrate that the proposed G-NAS achieves SOTA performance compared to baseline methods. Codes are available at <a href=https://github.com/wufan-cse/G-NAS>https://github.com/wufan-cse/G-NAS</a>.</p></p class="citation"></blockquote><h3 id=137236-gsn-generalisable-segmentation-in-neural-radiance-field-vinayak-gupta-et-al-2024>(137/236) GSN: Generalisable Segmentation in Neural Radiance Field (Vinayak Gupta et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Vinayak Gupta, Rahul Goel, Sirikonda Dhawal, P. J. Narayanan. (2024)<br><strong>GSN: Generalisable Segmentation in Neural Radiance Field</strong><br><button class=copy-to-clipboard title="GSN: Generalisable Segmentation in Neural Radiance Field" index=137>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-137 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-GR, cs.CV<br>Keyword Score: 10<br>Keywords: Knowledge Distillation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04632v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04632v1.pdf filename=2402.04632v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Traditional Radiance Field (RF) representations capture details of a specific scene and must be trained afresh on each scene. Semantic feature fields have been added to RFs to facilitate several segmentation tasks. Generalised RF representations learn the principles of view interpolation. A generalised RF can render new views of an unknown and untrained scene, given a few views. We present a way to distil feature fields into the generalised GNT representation. Our GSN representation generates new views of unseen scenes on the fly along with consistent, per-pixel semantic features. This enables multi-view segmentation of arbitrary new scenes. We show different semantic features being <b>distilled</b> into generalised RFs. Our multi-view segmentation results are on par with methods that use traditional RFs. GSN closes the gap between standard and generalisable RF methods significantly. Project Page: <a href=https://vinayak-vg.github.io/GSN/>https://vinayak-vg.github.io/GSN/</a></p></p class="citation"></blockquote><h3 id=138236-text2street-controllable-text-to-image-generation-for-street-views-jinming-su-et-al-2024>(138/236) Text2Street: Controllable Text-to-image Generation for Street Views (Jinming Su et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jinming Su, Songen Gu, Yiting Duan, Xingyue Chen, Junfeng Luo. (2024)<br><strong>Text2Street: Controllable Text-to-image Generation for Street Views</strong><br><button class=copy-to-clipboard title="Text2Street: Controllable Text-to-image Generation for Street Views" index=138>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-138 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Text2image<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04504v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04504v1.pdf filename=2402.04504v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Text-to-image</b> generation has made remarkable progress with the emergence of diffusion models. However, it is still a difficult task to generate images for street views based on text, mainly because the road topology of street scenes is complex, the traffic status is diverse and the weather condition is various, which makes conventional <b>text-to-image</b> models difficult to deal with. To address these challenges, we propose a novel controllable <b>text-to-image</b> framework, named \textbf{Text2Street}. In the framework, we first introduce the lane-aware road topology generator, which achieves text-to-map generation with the accurate road structure and lane lines armed with the counting adapter, realizing the controllable road topology generation. Then, the position-based object layout generator is proposed to obtain text-to-layout generation through an object-level bounding box diffusion strategy, realizing the controllable traffic object layout generation. Finally, the multiple control image generator is designed to integrate the road topology, object layout and weather description to realize controllable street-view image generation. Extensive experiments show that the proposed approach achieves controllable street-view <b>text-to-image</b> generation and validates the effectiveness of the Text2Street framework for street views.</p></p class="citation"></blockquote><h3 id=139236-biked-a-multimodal-dataset-of-14-million-bicycle-image-and-parametric-cad-designs-lyle-regenwetter-et-al-2024>(139/236) BIKED++: A Multimodal Dataset of 1.4 Million Bicycle Image and Parametric CAD Designs (Lyle Regenwetter et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lyle Regenwetter, Yazan Abu Obaideh, Amin Heyrani Nobari, Faez Ahmed. (2024)<br><strong>BIKED++: A Multimodal Dataset of 1.4 Million Bicycle Image and Parametric CAD Designs</strong><br><button class=copy-to-clipboard title="BIKED++: A Multimodal Dataset of 1.4 Million Bicycle Image and Parametric CAD Designs" index=139>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-139 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-LG, cs.CV<br>Keyword Score: 6<br>Keywords: Multi-modal, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05301v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05301v2.pdf filename=2402.05301v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper introduces a public dataset of 1.4 million procedurally-generated bicycle designs represented parametrically, as JSON files, and as rasterized images. The dataset is created through the use of a rendering engine which harnesses the BikeCAD software to generate vector graphics from parametric designs. This rendering engine is discussed in the paper and also released publicly alongside the dataset. Though this dataset has numerous applications, a principal motivation is the need to train cross-modal predictive models between parametric and image-based design representations. For example, we demonstrate that a predictive model can be trained to accurately estimate Contrastive Language-Image Pretraining (CLIP) embeddings from a parametric representation directly. This allows similarity relations to be established between parametric bicycle designs and text strings or reference images. Trained predictive models are also made public. The dataset joins the BIKED dataset family which includes thousands of mixed-representation human-designed bicycle models and several datasets quantifying design performance. The code and dataset can be found at: <a href=https://github.com/Lyleregenwetter/BIKED_multimodal/tree/main>https://github.com/Lyleregenwetter/BIKED_multimodal/tree/main</a></p></p class="citation"></blockquote><h3 id=140236-efficient-multi-resolution-fusion-for-remote-sensing-data-with-label-uncertainty-hersh-vakharia-et-al-2024>(140/236) Efficient Multi-Resolution Fusion for Remote Sensing Data with Label Uncertainty (Hersh Vakharia et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hersh Vakharia, Xiaoxiao Du. (2024)<br><strong>Efficient Multi-Resolution Fusion for Remote Sensing Data with Label Uncertainty</strong><br><button class=copy-to-clipboard title="Efficient Multi-Resolution Fusion for Remote Sensing Data with Label Uncertainty" index=140>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-140 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 3<br>Keywords: Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05045v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05045v1.pdf filename=2402.05045v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Multi-modal</b> sensor data fusion takes advantage of complementary or reinforcing information from each sensor and can boost overall performance in applications such as scene classification and target detection. This paper presents a new method for fusing <b>multi-modal</b> and multi-resolution remote sensor data without requiring pixel-level training labels, which can be difficult to obtain. Previously, we developed a Multiple Instance Multi-Resolution Fusion (MIMRF) framework that addresses label uncertainty for fusion, but it can be slow to train due to the large search space for the fuzzy measures used to integrate sensor data sources. We propose a new method based on binary fuzzy measures, which reduces the search space and significantly improves the efficiency of the MIMRF framework. We present experimental results on synthetic data and a real-world remote sensing detection task and show that the proposed MIMRF-BFM algorithm can effectively and efficiently perform multi-resolution fusion given remote sensing data with uncertainty.</p></p class="citation"></blockquote><h2 id=csro-13>cs.RO (13)</h2><h3 id=141236-exploration-without-maps-via-zero-shot-out-of-distribution-deep-reinforcement-learning-shathushan-sivashangaran-et-al-2024>(141/236) Exploration Without Maps via Zero-Shot Out-of-Distribution Deep Reinforcement Learning (Shathushan Sivashangaran et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shathushan Sivashangaran, Apoorva Khairnar, Azim Eskandarian. (2024)<br><strong>Exploration Without Maps via Zero-Shot Out-of-Distribution Deep Reinforcement Learning</strong><br><button class=copy-to-clipboard title="Exploration Without Maps via Zero-Shot Out-of-Distribution Deep Reinforcement Learning" index=141>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-141 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 60<br>Keywords: Out-of-distribution, Reinforcement Learning, Simulation, Simulator, Supervised Learning, Zero-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05066v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05066v1.pdf filename=2402.05066v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Operation of Autonomous Mobile Robots (AMRs) of all forms that include wheeled ground vehicles, quadrupeds and humanoids in dynamically changing GPS denied environments without a-priori maps, exclusively using onboard sensors, is an unsolved problem that has potential to transform the economy, and vastly improve humanity&rsquo;s capabilities with improvements to agriculture, manufacturing, disaster response, military and space exploration. Conventional AMR automation approaches are modularized into perception, motion planning and control which is computationally inefficient, and requires explicit feature extraction and engineering, that inhibits generalization, and deployment at scale. Few works have focused on real-world end-to-end approaches that directly map sensor inputs to control outputs due to the large amount of well curated training data required for <b>supervised</b> Deep Learning (DL) which is time consuming and labor intensive to collect and label, and sample inefficiency and challenges to bridging the <b>simulation</b> to reality gap using Deep <b>Reinforcement</b> <b>Learning</b> (DRL). This paper presents a novel method to efficiently train DRL for robust end-to-end AMR exploration, in a constrained environment at physical limits in <b>simulation,</b> transferred <b>zero-shot</b> to the real-world. The representation learned in a compact parameter space with 2 fully connected layers with 64 nodes each is demonstrated to exhibit emergent behavior for <b>out-of-distribution</b> generalization to navigation in new environments that include unstructured terrain without maps, and dynamic obstacle avoidance. The learned policy outperforms conventional navigation algorithms while consuming a fraction of the computation resources, enabling execution on a range of AMR forms with varying embedded computer payloads.</p></p class="citation"></blockquote><h3 id=142236-incoro-in-context-learning-for-robotics-control-with-feedback-loops-jiaqiang-ye-zhu-et-al-2024>(142/236) InCoRo: In-Context Learning for Robotics Control with Feedback Loops (Jiaqiang Ye Zhu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiaqiang Ye Zhu, Carla Gomez Cano, David Vazquez Bermudez, Michal Drozdzal. (2024)<br><strong>InCoRo: In-Context Learning for Robotics Control with Feedback Loops</strong><br><button class=copy-to-clipboard title="InCoRo: In-Context Learning for Robotics Control with Feedback Loops" index=142>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-142 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-CL, cs-RO, cs.RO<br>Keyword Score: 50<br>Keywords: Zero-shot, Reasoning, In-context Learning, In-context Learning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05188v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05188v1.pdf filename=2402.05188v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>One of the challenges in robotics is to enable robotic units with the <b>reasoning</b> capability that would be robust enough to execute complex tasks in dynamic environments. Recent advances in <b>LLMs</b> have positioned them as go-to tools for simple <b>reasoning</b> tasks, motivating the pioneering work of Liang et al. [35] that uses an <b>LLM</b> to translate natural language commands into low-level static execution plans for robotic units. Using <b>LLMs</b> inside robotics systems brings their generalization to a new level, enabling <b>zero-shot</b> generalization to new tasks. This paper extends this prior work to dynamic environments. We propose InCoRo, a system that uses a classical robotic feedback loop composed of an <b>LLM</b> controller, a scene understanding unit, and a robot. Our system continuously analyzes the state of the environment and provides adapted execution commands, enabling the robot to adjust to changing environmental conditions and correcting for controller errors. Our system does not require any iterative optimization to learn to accomplish a task as it leverages <b>in-context</b> <b>learning</b> with an off-the-shelf <b>LLM</b> model. Through an extensive validation process involving two standardized industrial robotic units &ndash; SCARA and DELTA types &ndash; we contribute knowledge about these robots, not popular in the community, thereby enriching it. We highlight the generalization capabilities of our system and show that (1) <b>in-context</b> <b>learning</b> in combination with the current state-of-the-art <b>LLMs</b> is an effective way to implement a robotic controller; (2) in static environments, InCoRo surpasses the prior art in terms of the success rate; (3) in dynamic environments, we establish new state-of-the-art for the SCARA and DELTA units, respectively. This research paves the way towards building reliable, efficient, intelligent autonomous systems that adapt to dynamic environments.</p></p class="citation"></blockquote><h3 id=143236-real-time-line-based-room-segmentation-and-continuous-euclidean-distance-fields-erik-warberg-et-al-2024>(143/236) Real-Time Line-Based Room Segmentation and Continuous Euclidean Distance Fields (Erik Warberg et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Erik Warberg, Adam Miksits, Fernando S. Barbosa. (2024)<br><strong>Real-Time Line-Based Room Segmentation and Continuous Euclidean Distance Fields</strong><br><button class=copy-to-clipboard title="Real-Time Line-Based Room Segmentation and Continuous Euclidean Distance Fields" index=143>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-143 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 30<br>Keywords: Gaussian Process, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05236v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05236v1.pdf filename=2402.05236v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Continuous maps representations, as opposed to traditional discrete ones such as grid maps, have been gaining traction in the research community. However, current approaches still suffer from high computation costs, making them unable to be used in large environments without sacrificing precision. In this paper, a scalable method building upon <b>Gaussian</b> <b>Process-based</b> Euclidean Distance Fields (GP-EDFs) is proposed. By leveraging structure inherent to indoor environments, namely walls and rooms, we achieve an accurate continuous map representation that is fast enough to be updated and used in real-time. This is possible thanks to a novel line-based room segmentation algorithm, enabling the creation of smaller local GP-EDFs for each room, which in turn also use line segments as its shape priors, thus representing the map more efficiently with fewer data points. We evaluate this method in <b>simulation</b> experiments, and make the code available open-source.</p></p class="citation"></blockquote><h3 id=144236-a-comprehensive-survey-of-cross-domain-policy-transfer-for-embodied-agents-haoyi-niu-et-al-2024>(144/236) A Comprehensive Survey of Cross-Domain Policy Transfer for Embodied Agents (Haoyi Niu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haoyi Niu, Jianming Hu, Guyue Zhou, Xianyuan Zhan. (2024)<br><strong>A Comprehensive Survey of Cross-Domain Policy Transfer for Embodied Agents</strong><br><button class=copy-to-clipboard title="A Comprehensive Survey of Cross-Domain Policy Transfer for Embodied Agents" index=144>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-144 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-LG, cs-RO, cs.RO<br>Keyword Score: 30<br>Keywords: Simulation, Simulator, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04580v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04580v1.pdf filename=2402.04580v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The burgeoning fields of robot learning and embodied AI have triggered an increasing demand for large quantities of data. However, collecting sufficient unbiased data from the target domain remains a challenge due to costly data collection processes and stringent safety requirements. Consequently, researchers often resort to data from easily accessible source domains, such as <b>simulation</b> and laboratory environments, for cost-effective data acquisition and rapid model iteration. Nevertheless, the environments and embodiments of these source domains can be quite different from their target domain counterparts, underscoring the need for effective cross-domain policy transfer approaches. In this paper, we conduct a systematic review of existing cross-domain policy transfer methods. Through a nuanced categorization of domain gaps, we encapsulate the overarching insights and design considerations of each problem setting. We also provide a high-level discussion about the key methodologies used in cross-domain policy transfer problems. Lastly, we <b>summarize</b> the open challenges that lie beyond the capabilities of current paradigms and discuss potential future directions in this field.</p></p class="citation"></blockquote><h3 id=145236-tactile-based-object-retrieval-from-granular-media-jingxi-xu-et-al-2024>(145/236) Tactile-based Object Retrieval From Granular Media (Jingxi Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jingxi Xu, Yinsen Jia, Dongxiao Yang, Patrick Meng, Xinyue Zhu, Zihan Guo, Shuran Song, Matei Ciocarlie. (2024)<br><strong>Tactile-based Object Retrieval From Granular Media</strong><br><button class=copy-to-clipboard title="Tactile-based Object Retrieval From Granular Media" index=145>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-145 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-LG, cs-RO, cs.RO<br>Keyword Score: 30<br>Keywords: Simulation, Simulator, Zero-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04536v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04536v1.pdf filename=2402.04536v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce GEOTACT, a robotic manipulation method capable of retrieving objects buried in granular media. This is a challenging task due to the need to interact with granular media, and doing so based exclusively on tactile feedback, since a buried object can be completely hidden from vision. Tactile feedback is in itself challenging in this context, due to ubiquitous contact with the surrounding media, and the inherent noise level induced by the tactile readings. To address these challenges, we use a learning method trained end-to-end with simulated sensor noise. We show that our problem formulation leads to the natural emergence of learned pushing behaviors that the manipulator uses to reduce uncertainty and funnel the object to a stable grasp despite spurious and noisy tactile readings. We also introduce a training curriculum that enables learning these behaviors in <b>simulation,</b> followed by <b>zero-shot</b> transfer to real hardware. To the best of our knowledge, GEOTACT is the first method to reliably retrieve a number of different objects from a granular environment, doing so on real hardware and with integrated tactile sensing. Videos and additional information can be found at <a href=https://jxu.ai/geotact>https://jxu.ai/geotact</a>.</p></p class="citation"></blockquote><h3 id=146236-language-based-augmentation-to-address-shortcut-learning-in-object-goal-navigation-dennis-hoftijzer-et-al-2024>(146/236) Language-Based Augmentation to Address Shortcut Learning in Object Goal Navigation (Dennis Hoftijzer et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dennis Hoftijzer, Gertjan Burghouts, Luuk Spreeuwers. (2024)<br><strong>Language-Based Augmentation to Address Shortcut Learning in Object Goal Navigation</strong><br><button class=copy-to-clipboard title="Language-Based Augmentation to Address Shortcut Learning in Object Goal Navigation" index=146>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-146 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-CV, cs-RO, cs.RO<br>Keyword Score: 26<br>Keywords: Multi-modal, Multi-modal, Reinforcement Learning, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05090v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05090v1.pdf filename=2402.05090v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep <b>Reinforcement</b> <b>Learning</b> (DRL) has shown great potential in enabling robots to find certain objects (e.g., `find a fridge&rsquo;) in environments like homes or schools. This task is known as Object-Goal Navigation (ObjectNav). DRL methods are predominantly trained and evaluated using environment simulators. Although DRL has shown impressive results, the simulators may be biased or limited. This creates a risk of shortcut learning, i.e., learning a policy tailored to specific visual details of training environments. We aim to deepen our understanding of shortcut learning in ObjectNav, its implications and propose a solution. We design an experiment for inserting a shortcut bias in the appearance of training environments. As a proof-of-concept, we associate room types to specific wall colors (e.g., bedrooms with green walls), and observe poor generalization of a state-of-the-art (SOTA) ObjectNav method to environments where this is not the case (e.g., bedrooms with blue walls). We find that shortcut learning is the root cause: the agent learns to navigate to target objects, by simply searching for the associated wall color of the target object&rsquo;s room. To solve this, we propose Language-Based (L-B) augmentation. Our key insight is that we can leverage the <b>multimodal</b> feature space of a <b>Vision-Language</b> Model (VLM) to augment visual representations directly at the feature-level, requiring no changes to the simulator, and only an addition of one layer to the model. Where the SOTA ObjectNav method&rsquo;s success rate drops 69%, our proposal has only a drop of 23%.</p></p class="citation"></blockquote><h3 id=147236-tactile-ergodic-control-using-diffusion-and-geometric-algebra-cem-bilaloglu-et-al-2024>(147/236) Tactile Ergodic Control Using Diffusion and Geometric Algebra (Cem Bilaloglu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Cem Bilaloglu, Tobias Löw, Sylvain Calinon. (2024)<br><strong>Tactile Ergodic Control Using Diffusion and Geometric Algebra</strong><br><button class=copy-to-clipboard title="Tactile Ergodic Control Using Diffusion and Geometric Algebra" index=147>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-147 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04862v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04862v1.pdf filename=2402.04862v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Continuous physical interaction between robots and their environment is a requirement in many industrial and household tasks, such as sanding and cleaning. Due to the complex tactile information, these tasks are notoriously difficult to model and to sense. In this article, we introduce a closed-loop control method that is constrained to surfaces. The applications that we target have in common that they can be represented by probability distributions on the surface that correlate to the time the robot should spend in a region. These surfaces can easily be captured jointly with the target distributions using coloured point clouds. We present the extension of an ergodic control approach that can be used with point clouds, based on heat equation-driven area coverage (HEDAC). Our method enables closed-loop exploration by measuring the actual coverage using vision. Unlike existing approaches, we approximate the potential field from non-stationary diffusion using spectral acceleration, which does not require complex preprocessing steps and achieves real-time closed-loop control frequencies. We exploit geometric algebra to stay in contact with the target surface by tracking a line while simultaneously exerting a desired force along that line. Our approach is suitable for fully autonomous and human-robot interaction settings where the robot can either directly measure the coverage of the target with its sensors or by being guided online by markings or annotations of a human expert. We tested the performance of the approach in kinematic <b>simulation</b> using point clouds, ranging from the Stanford bunny to a variety of kitchen utensils. Our real-world experiments demonstrate that the proposed approach can successfully be used to wash kitchenware with curved surfaces, by cleaning the dirt detected by vision in an online manner. Website: <a href=https://geometric-algebra.tobiloew.ch/tactile_ergodic_control>https://geometric-algebra.tobiloew.ch/tactile_ergodic_control</a></p></p class="citation"></blockquote><h3 id=148236-offline-deep-model-predictive-control-mpc-for-visual-navigation-taha-bouzid-et-al-2024>(148/236) Offline Deep Model Predictive Control (MPC) for Visual Navigation (Taha Bouzid et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Taha Bouzid, Youssef Alj. (2024)<br><strong>Offline Deep Model Predictive Control (MPC) for Visual Navigation</strong><br><button class=copy-to-clipboard title="Offline Deep Model Predictive Control (MPC) for Visual Navigation" index=148>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-148 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04797v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04797v1.pdf filename=2402.04797v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we propose a new visual navigation method based on a single RGB perspective camera. Using the Visual Teach & Repeat (VT&amp;R) methodology, the robot acquires a visual trajectory consisting of multiple subgoal images in the teaching step. In the repeat step, we propose two network architectures, namely ViewNet and VelocityNet. The combination of the two networks allows the robot to follow the visual trajectory. ViewNet is trained to generate a future image based on the current view and the velocity command. The generated future image is combined with the subgoal image for training VelocityNet. We develop an offline Model Predictive Control (MPC) policy within VelocityNet with the dual goals of (1) reducing the difference between current and subgoal images and (2) ensuring smooth trajectories by mitigating velocity discontinuities. Offline training conserves computational resources, making it a more suitable option for scenarios with limited computational capabilities, such as embedded systems. We validate our experiments in a <b>simulation</b> environment, demonstrating that our model can effectively minimize the metric error between real and played trajectories.</p></p class="citation"></blockquote><h3 id=149236-investigating-driving-interactions-a-robust-multi-agent-simulation-framework-for-autonomous-vehicles-marc-kaufeld-et-al-2024>(149/236) Investigating Driving Interactions: A Robust Multi-Agent Simulation Framework for Autonomous Vehicles (Marc Kaufeld et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Marc Kaufeld, Rainer Trauth, Johannes Betz. (2024)<br><strong>Investigating Driving Interactions: A Robust Multi-Agent Simulation Framework for Autonomous Vehicles</strong><br><button class=copy-to-clipboard title="Investigating Driving Interactions: A Robust Multi-Agent Simulation Framework for Autonomous Vehicles" index=149>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-149 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04720v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04720v1.pdf filename=2402.04720v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Current validation methods often rely on recorded data and basic functional checks, which may not be sufficient to encompass the scenarios an autonomous vehicle might encounter. In addition, there is a growing need for complex scenarios with changing vehicle interactions for comprehensive validation. This work introduces a novel synchronous multi-agent <b>simulation</b> framework for autonomous vehicles in interactive scenarios. Our approach creates an interactive scenario and incorporates publicly available edge-case scenarios wherein simulated vehicles are replaced by agents navigating to predefined destinations. We provide a platform that enables the integration of different autonomous driving planning methodologies and includes a set of evaluation metrics to assess autonomous driving behavior. Our study explores different planning setups and adjusts <b>simulation</b> complexity to test the framework&rsquo;s adaptability and performance. Results highlight the critical role of simulating vehicle interactions to enhance autonomous driving systems. Our setup offers unique insights for developing advanced algorithms for complex driving tasks to accelerate future investigations and developments in this field. The multi-agent <b>simulation</b> framework is available as open-source software: <a href=https://github.com/TUM-AVS/Frenetix-Motion-Planner>https://github.com/TUM-AVS/Frenetix-Motion-Planner</a></p></p class="citation"></blockquote><h3 id=150236-lidar-forest-dataset-lidar-point-cloud-simulation-dataset-for-forestry-application-yawen-lu-et-al-2024>(150/236) LiDAR-Forest Dataset: LiDAR Point Cloud Simulation Dataset for Forestry Application (Yawen Lu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yawen Lu, Zhuoyang Sun, Jinyuan Shao, Qianyu Guo, Yunhan Huang, Songlin Fei, Victor Chen. (2024)<br><strong>LiDAR-Forest Dataset: LiDAR Point Cloud Simulation Dataset for Forestry Application</strong><br><button class=copy-to-clipboard title="LiDAR-Forest Dataset: LiDAR Point Cloud Simulation Dataset for Forestry Application" index=150>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-150 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04546v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04546v1.pdf filename=2402.04546v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The popularity of LiDAR devices and sensor technology has gradually empowered users from autonomous driving to forest monitoring, and research on 3D LiDAR has made remarkable progress over the years. Unlike 2D images, whose focused area is visible and rich in texture information, understanding the point distribution can help companies and researchers find better ways to develop point-based 3D applications. In this work, we contribute an unreal-based LiDAR <b>simulation</b> tool and a 3D <b>simulation</b> dataset named LiDAR-Forest, which can be used by various studies to evaluate forest reconstruction, tree DBH estimation, and point cloud compression for easy visualization. The <b>simulation</b> is customizable in tree species, LiDAR types and scene generation, with low cost and high efficiency.</p></p class="citation"></blockquote><h3 id=151236-deep-reinforcement-learning-with-dynamic-graphs-for-adaptive-informative-path-planning-apoorva-vashisth-et-al-2024>(151/236) Deep Reinforcement Learning with Dynamic Graphs for Adaptive Informative Path Planning (Apoorva Vashisth et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Apoorva Vashisth, Julius Rückin, Federico Magistri, Cyrill Stachniss, Marija Popović. (2024)<br><strong>Deep Reinforcement Learning with Dynamic Graphs for Adaptive Informative Path Planning</strong><br><button class=copy-to-clipboard title="Deep Reinforcement Learning with Dynamic Graphs for Adaptive Informative Path Planning" index=151>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-151 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-LG, cs-RO, cs.RO<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04894v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04894v1.pdf filename=2402.04894v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Autonomous robots are often employed for data collection due to their efficiency and low labour costs. A key task in robotic data acquisition is planning paths through an initially unknown environment to collect observations given platform-specific resource constraints, such as limited battery life. Adaptive online path planning in 3D environments is challenging due to the large set of valid actions and the presence of unknown occlusions. To address these issues, we propose a novel deep <b>reinforcement</b> <b>learning</b> approach for adaptively replanning robot paths to map targets of interest in unknown 3D environments. A key aspect of our approach is a dynamically constructed graph that restricts planning actions local to the robot, allowing us to quickly react to newly discovered obstacles and targets of interest. For replanning, we propose a new reward function that balances between exploring the unknown environment and exploiting online-collected data about the targets of interest. Our experiments show that our method enables more efficient target detection compared to state-of-the-art learning and non-learning baselines. We also show the applicability of our approach for orchard monitoring using an unmanned aerial vehicle in a photorealistic simulator.</p></p class="citation"></blockquote><h3 id=152236-robot-interaction-behavior-generation-based-on-social-motion-forecasting-for-human-robot-interaction-esteve-valls-mascaro-et-al-2024>(152/236) Robot Interaction Behavior Generation based on Social Motion Forecasting for Human-Robot Interaction (Esteve Valls Mascaro et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Esteve Valls Mascaro, Yashuai Yan, Dongheui Lee. (2024)<br><strong>Robot Interaction Behavior Generation based on Social Motion Forecasting for Human-Robot Interaction</strong><br><button class=copy-to-clipboard title="Robot Interaction Behavior Generation based on Social Motion Forecasting for Human-Robot Interaction" index=152>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-152 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-CV, cs-HC, cs-RO, cs.RO<br>Keyword Score: 10<br>Keywords: Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04768v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04768v1.pdf filename=2402.04768v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Integrating robots into populated environments is a complex challenge that requires an understanding of human social dynamics. In this work, we propose to model social motion forecasting in a shared human-robot representation space, which facilitates us to synthesize robot motions that interact with humans in social scenarios despite not observing any robot in the motion training. We develop a <b>transformer-based</b> architecture called ECHO, which operates in the aforementioned shared space to predict the future motions of the agents encountered in social scenarios. Contrary to prior works, we reformulate the social motion problem as the refinement of the predicted individual motions based on the surrounding agents, which facilitates the training while allowing for single-motion forecasting when only one human is in the scene. We evaluate our model in multi-person and human-robot motion forecasting tasks and obtain state-of-the-art performance by a large margin while being efficient and performing in real-time. Additionally, our qualitative results showcase the effectiveness of our approach in generating human-robot interaction behaviors that can be controlled via text commands.</p></p class="citation"></blockquote><h3 id=153236-boosting-reinforcement-learning-algorithms-in-continuous-robotic-reaching-tasks-using-adaptive-potential-functions-yifei-chen-et-al-2024>(153/236) Boosting Reinforcement Learning Algorithms in Continuous Robotic Reaching Tasks using Adaptive Potential Functions (Yifei Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yifei Chen, Lambert Schomaker, Francisco Cruz. (2024)<br><strong>Boosting Reinforcement Learning Algorithms in Continuous Robotic Reaching Tasks using Adaptive Potential Functions</strong><br><button class=copy-to-clipboard title="Boosting Reinforcement Learning Algorithms in Continuous Robotic Reaching Tasks using Adaptive Potential Functions" index=153>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-153 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04581v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04581v1.pdf filename=2402.04581v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In <b>reinforcement</b> <b>learning,</b> reward shaping is an efficient way to guide the learning process of an agent, as the reward can indicate the optimal policy of the task. The potential-based reward shaping framework was proposed to guarantee policy invariance after reward shaping, where a potential function is used to calculate the shaping reward. In former work, we proposed a novel adaptive potential function (APF) method to learn the potential function concurrently with training the agent based on information collected by the agent during the training process, and examined the APF method in discrete action space scenarios. This paper investigates the feasibility of using APF in solving continuous-reaching tasks in a real-world robotic scenario with continuous action space. We combine the Deep Deterministic Policy Gradient (DDPG) algorithm and our proposed method to form a new algorithm called APF-DDPG. To compare APF-DDPG with DDPG, we designed a task where the agent learns to control Baxter&rsquo;s right arm to reach a goal position. The experimental results show that the APF-DDPG algorithm outperforms the DDPG algorithm on both learning speed and robustness.</p></p class="citation"></blockquote><h2 id=mathoc-3>math.OC (3)</h2><h3 id=154236-non-convergence-to-global-minimizers-for-adam-and-stochastic-gradient-descent-optimization-and-constructions-of-local-minimizers-in-the-training-of-artificial-neural-networks-arnulf-jentzen-et-al-2024>(154/236) Non-convergence to global minimizers for Adam and stochastic gradient descent optimization and constructions of local minimizers in the training of artificial neural networks (Arnulf Jentzen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Arnulf Jentzen, Adrian Riekert. (2024)<br><strong>Non-convergence to global minimizers for Adam and stochastic gradient descent optimization and constructions of local minimizers in the training of artificial neural networks</strong><br><button class=copy-to-clipboard title="Non-convergence to global minimizers for Adam and stochastic gradient descent optimization and constructions of local minimizers in the training of artificial neural networks" index=154>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-154 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.OC<br>Categories: cs-LG, math-OC, math.OC<br>Keyword Score: 60<br>Keywords: Simulation, Simulator, Stochastic Gradient Descent, Stochastic Gradient Descent, Supervised Learning, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05155v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05155v1.pdf filename=2402.05155v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Stochastic</b> <b>gradient</b> <b>descent</b> <b>(SGD)</b> optimization methods such as the plain vanilla <b>SGD</b> method and the popular Adam optimizer are nowadays the method of choice in the training of artificial neural networks (ANNs). Despite the remarkable success of <b>SGD</b> methods in the ANN training in numerical <b>simulations,</b> it remains in essentially all practical relevant scenarios an open problem to rigorously explain why <b>SGD</b> methods seem to succeed to train ANNs. In particular, in most practically relevant <b>supervised</b> <b>learning</b> problems, it seems that <b>SGD</b> methods do with high probability not converge to global minimizers in the optimization landscape of the ANN training problem. Nevertheless, it remains an open problem of research to disprove the convergence of <b>SGD</b> methods to global minimizers. In this work we solve this research problem in the situation of shallow ANNs with the rectified linear unit (ReLU) and related activations with the standard mean square error loss by disproving in the training of such ANNs that <b>SGD</b> methods (such as the plain vanilla <b>SGD,</b> the momentum <b>SGD,</b> the AdaGrad, the RMSprop, and the Adam optimizers) can find a global minimizer with high probability. Even stronger, we reveal in the training of such ANNs that <b>SGD</b> methods do with high probability fail to converge to global minimizers in the optimization landscape. The findings of this work do, however, not disprove that <b>SGD</b> methods succeed to train ANNs since they do not exclude the possibility that <b>SGD</b> methods find good local minimizers whose risk values are close to the risk values of the global minimizers. In this context, another key contribution of this work is to establish the existence of a hierarchical structure of local minimizers with distinct risk values in the optimization landscape of ANN training problems with ReLU and related activations.</p></p class="citation"></blockquote><h3 id=155236-extending-the-reach-of-first-order-algorithms-for-nonconvex-min-max-problems-with-cohypomonotonicity-ahmet-alacaoglu-et-al-2024>(155/236) Extending the Reach of First-Order Algorithms for Nonconvex Min-Max Problems with Cohypomonotonicity (Ahmet Alacaoglu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ahmet Alacaoglu, Donghwan Kim, Stephen J. Wright. (2024)<br><strong>Extending the Reach of First-Order Algorithms for Nonconvex Min-Max Problems with Cohypomonotonicity</strong><br><button class=copy-to-clipboard title="Extending the Reach of First-Order Algorithms for Nonconvex Min-Max Problems with Cohypomonotonicity" index=155>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-155 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.OC<br>Categories: cs-LG, math-OC, math.OC, stat-ML<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05071v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05071v1.pdf filename=2402.05071v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We focus on constrained, $L$-smooth, nonconvex-nonconcave min-max problems either satisfying $\rho$-cohypomonotonicity or admitting a solution to the $\rho$-weakly Minty Variational Inequality (MVI), where larger values of the parameter $\rho>0$ correspond to a greater degree of nonconvexity. These problem classes include examples in two player <b>reinforcement</b> <b>learning,</b> interaction dominant min-max problems, and certain synthetic test problems on which classical min-max algorithms fail. It has been conjectured that first-order methods can tolerate value of $\rho$ no larger than $\frac{1}{L}$, but existing results in the literature have stagnated at the tighter requirement $\rho &lt; \frac{1}{2L}$. With a simple argument, we obtain optimal or best-known complexity guarantees with cohypomonotonicity or weak MVI conditions for $\rho &lt; \frac{1}{L}$. The algorithms we analyze are inexact variants of Halpern and Krasnosel&rsquo;ski\u{\i}-Mann (KM) iterations. We also provide algorithms and complexity guarantees in the stochastic case with the same range on $\rho$. Our main insight for the improvements in the convergence analyses is to harness the recently proposed &ldquo;conic nonexpansiveness&rdquo; property of operators. As byproducts, we provide a refined analysis for inexact Halpern iteration and propose a stochastic KM iteration with a multilevel Monte Carlo estimator.</p></p class="citation"></blockquote><h3 id=156236-shadowheart-sgd-distributed-asynchronous-sgd-with-optimal-time-complexity-under-arbitrary-computation-and-communication-heterogeneity-alexander-tyurin-et-al-2024>(156/236) Shadowheart SGD: Distributed Asynchronous SGD with Optimal Time Complexity Under Arbitrary Computation and Communication Heterogeneity (Alexander Tyurin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Alexander Tyurin, Marta Pozzi, Ivan Ilin, Peter Richtárik. (2024)<br><strong>Shadowheart SGD: Distributed Asynchronous SGD with Optimal Time Complexity Under Arbitrary Computation and Communication Heterogeneity</strong><br><button class=copy-to-clipboard title="Shadowheart SGD: Distributed Asynchronous SGD with Optimal Time Complexity Under Arbitrary Computation and Communication Heterogeneity" index=156>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-156 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.OC<br>Categories: cs-LG, math-OC, math.OC<br>Keyword Score: 10<br>Keywords: Stochastic Gradient Descent<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04785v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04785v1.pdf filename=2402.04785v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We consider nonconvex stochastic optimization problems in the asynchronous centralized distributed setup where the communication times from workers to a server can not be ignored, and the computation and communication times are potentially different for all workers. Using an unbiassed compression technique, we develop a new method-Shadowheart <b>SGD-that</b> provably improves the time complexities of all previous centralized methods. Moreover, we show that the time complexity of Shadowheart <b>SGD</b> is optimal in the family of centralized methods with compressed communication. We also consider the bidirectional setup, where broadcasting from the server to the workers is non-negligible, and develop a corresponding method.</p></p class="citation"></blockquote><h2 id=cssi-1>cs.SI (1)</h2><h3 id=157236-adaptive-hypergraph-network-for-trust-prediction-rongwei-xu-et-al-2024>(157/236) Adaptive Hypergraph Network for Trust Prediction (Rongwei Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rongwei Xu, Guanfeng Liu, Yan Wang, Xuyun Zhang, Kai Zheng, Xiaofang Zhou. (2024)<br><strong>Adaptive Hypergraph Network for Trust Prediction</strong><br><button class=copy-to-clipboard title="Adaptive Hypergraph Network for Trust Prediction" index=157>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-157 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SI<br>Categories: cs-AI, cs-SI, cs.SI<br>Keyword Score: 60<br>Keywords: Graph Convolutional Network, Graph Convolutional Network, Contrastive Learning, Convolution, Convolutional Neural Network, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05154v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05154v1.pdf filename=2402.05154v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Trust plays an essential role in an individual&rsquo;s decision-making. Traditional trust prediction models rely on pairwise correlations to infer potential relationships between users. However, in the real world, interactions between users are usually complicated rather than pairwise only. Hypergraphs offer a flexible approach to modeling these complex high-order correlations (not just pairwise connections), since hypergraphs can leverage hyperedeges to link more than two nodes. However, most hypergraph-based methods are generic and cannot be well applied to the trust prediction task. In this paper, we propose an Adaptive Hypergraph Network for Trust Prediction (AHNTP), a novel approach that improves trust prediction accuracy by using higher-order correlations. AHNTP utilizes Motif-based PageRank to capture high-order social influence information. In addition, it constructs hypergroups from both node-level and structure-level attributes to incorporate complex correlation information. Furthermore, AHNTP leverages adaptive hypergraph <b>Graph</b> <b>Convolutional</b> <b>Network</b> <b>(GCN)</b> layers and multilayer perceptrons (MLPs) to generate comprehensive user embeddings, facilitating trust relationship prediction. To enhance model generalization and robustness, we introduce a novel <b>supervised</b> <b>contrastive</b> <b>learning</b> loss for optimization. Extensive experiments demonstrate the superiority of our model over the state-of-the-art approaches in terms of trust prediction accuracy. The source code of this work can be accessed via <a href=https://github.com/Sherry-XU1995/AHNTP>https://github.com/Sherry-XU1995/AHNTP</a>.</p></p class="citation"></blockquote><h2 id=csir-7>cs.IR (7)</h2><h3 id=158236-navigating-the-knowledge-sea-planet-scale-answer-retrieval-using-llms-dipankar-sarkar-2024>(158/236) Navigating the Knowledge Sea: Planet-scale answer retrieval using LLMs (Dipankar Sarkar, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dipankar Sarkar. (2024)<br><strong>Navigating the Knowledge Sea: Planet-scale answer retrieval using LLMs</strong><br><button class=copy-to-clipboard title="Navigating the Knowledge Sea: Planet-scale answer retrieval using LLMs" index=158>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-158 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-CL, cs-IR, cs-LG, cs.IR<br>Keyword Score: 50<br>Keywords: GPT, GPT-4, Information Retrieval, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05318v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05318v1.pdf filename=2402.05318v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Information</b> <b>retrieval</b> is a rapidly evolving field of <b>information</b> <b>retrieval,</b> which is characterized by a continuous refinement of techniques and technologies, from basic hyperlink-based navigation to sophisticated algorithm-driven search engines. This paper aims to provide a comprehensive overview of the evolution of <b>Information</b> <b>Retrieval</b> Technology, with a particular focus on the role of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> in bridging the gap between traditional search methods and the emerging paradigm of answer retrieval. The integration of <b>LLMs</b> in the realms of response retrieval and indexing signifies a paradigm shift in how users interact with <b>information</b> <b>systems.</b> This paradigm shift is driven by the integration of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> like <b>GPT-4,</b> which are capable of understanding and generating human-like text, thus enabling them to provide more direct and contextually relevant answers to user queries. Through this exploration, we seek to illuminate the technological milestones that have shaped this journey and the potential future directions in this rapidly changing field.</p></p class="citation"></blockquote><h3 id=159236-multimodal-query-suggestion-with-multi-agent-reinforcement-learning-from-human-feedback-zheng-wang-et-al-2024>(159/236) Multimodal Query Suggestion with Multi-Agent Reinforcement Learning from Human Feedback (Zheng Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zheng Wang, Bingzheng Gan, Wei Shi. (2024)<br><strong>Multimodal Query Suggestion with Multi-Agent Reinforcement Learning from Human Feedback</strong><br><button class=copy-to-clipboard title="Multimodal Query Suggestion with Multi-Agent Reinforcement Learning from Human Feedback" index=159>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-159 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 46<br>Keywords: Multi-modal, Multi-modal, Reinforcement Learning, Information Retrieval, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04867v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04867v2.pdf filename=2402.04867v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the rapidly evolving landscape of <b>information</b> <b>retrieval,</b> search engines strive to provide more personalized and relevant results to users. Query suggestion systems play a crucial role in achieving this goal by assisting users in formulating effective queries. However, existing query suggestion systems mainly rely on textual inputs, potentially limiting user search experiences for querying images. In this paper, we introduce a novel <b>Multimodal</b> Query Suggestion (MMQS) task, which aims to generate query suggestions based on user query images to improve the intentionality and diversity of search results. We present the RL4Sugg framework, leveraging the power of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> with Multi-Agent <b>Reinforcement</b> <b>Learning</b> from Human Feedback to optimize the generation process. Through comprehensive experiments, we validate the effectiveness of RL4Sugg, demonstrating a 18% improvement compared to the best existing approach. Moreover, the MMQS has been transferred into real-world search engine products, which yield enhanced user engagement. Our research advances query suggestion systems and provides a new perspective on <b>multimodal</b> <b>information</b> <b>retrieval.</b></p></p class="citation"></blockquote><h3 id=160236-detecting-generated-native-ads-in-conversational-search-sebastian-schmidt-et-al-2024>(160/236) Detecting Generated Native Ads in Conversational Search (Sebastian Schmidt et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sebastian Schmidt, Ines Zelch, Janek Bevendorff, Benno Stein, Matthias Hagen, Martin Potthast. (2024)<br><strong>Detecting Generated Native Ads in Conversational Search</strong><br><button class=copy-to-clipboard title="Detecting Generated Native Ads in Conversational Search" index=160>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-160 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-CL, cs-IR, cs.IR<br>Keyword Score: 40<br>Keywords: Fine-tuning, Transformer, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04889v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04889v1.pdf filename=2402.04889v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Conversational search engines such as YouChat and Microsoft Copilot use <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> to generate answers to queries. It is only a small step to also use this technology to generate and integrate advertising within these answers - instead of placing ads separately from the organic search results. This type of advertising is reminiscent of native advertising and product placement, both of which are very effective forms of subtle and manipulative advertising. It is likely that information seekers will be confronted with such use of <b>LLM</b> technology in the near future, especially when considering the high computational costs associated with <b>LLMs,</b> for which providers need to develop sustainable business models. This paper investigates whether <b>LLMs</b> can also be used as a countermeasure against generated native ads, i.e., to block them. For this purpose we compile a <b>large</b> <b>dataset</b> <b>of</b> ad-prone queries and of generated answers with automatically integrated ads to experiment with <b>fine-tuned</b> sentence <b>transformers</b> and state-of-the-art <b>LLMs</b> on the task of recognizing the ads. In our experiments sentence <b>transformers</b> achieve detection precision and recall values above 0.9, while the investigated <b>LLMs</b> struggle with the task.</p></p class="citation"></blockquote><h3 id=161236-leveraging-llms-for-unsupervised-dense-retriever-ranking-ekaterina-khramtsova-et-al-2024>(161/236) Leveraging LLMs for Unsupervised Dense Retriever Ranking (Ekaterina Khramtsova et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ekaterina Khramtsova, Shengyao Zhuang, Mahsa Baktashmotlagh, Guido Zuccon. (2024)<br><strong>Leveraging LLMs for Unsupervised Dense Retriever Ranking</strong><br><button class=copy-to-clipboard title="Leveraging LLMs for Unsupervised Dense Retriever Ranking" index=161>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-161 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 40<br>Keywords: Unsupervised Learning, Zero-shot, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04853v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04853v1.pdf filename=2402.04853v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper introduces a novel <b>unsupervised</b> technique that utilizes <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> to determine the most suitable dense retriever for a specific test(target) corpus. Selecting the appropriate dense retriever is vital for numerous IR applications that employ these retrievers, trained on public datasets, to encode or conduct searches within a new private target corpus. The effectiveness of a dense retriever can significantly diminish when applied to a target corpus that diverges in domain or task from the original training set. The problem becomes more pronounced in cases where the target corpus is unlabeled, e.g. in <b>zero-shot</b> scenarios, rendering direct evaluation of the model&rsquo;s effectiveness on the target corpus unattainable. Therefore, the <b>unsupervised</b> selection of an optimally pre-trained dense retriever, especially under conditions of domain shift, emerges as a critical challenge. Existing methodologies for ranking dense retrievers fall short in addressing these domain shift scenarios. To tackle this, our method capitalizes on <b>LLMs</b> to create pseudo-relevant queries, labels, and reference lists by analyzing a subset of documents from the target corpus. This allows for the ranking of dense retrievers based on their performance with these pseudo-relevant signals. Significantly, this strategy is the first to depend exclusively on the target corpus data, removing the necessity for training data and test labels. We assessed the effectiveness of our approach by compiling a comprehensive pool of cutting-edge dense retrievers and comparing our method against traditional dense retriever selection benchmarks. The findings reveal that our proposed solution surpasses the existing benchmarks in both the selection and ranking of dense retrievers.</p></p class="citation"></blockquote><h3 id=162236-ra-rec-an-efficient-id-representation-alignment-framework-for-llm-based-recommendation-xiaohan-yu-et-al-2024>(162/236) RA-Rec: An Efficient ID Representation Alignment Framework for LLM-based Recommendation (Xiaohan Yu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiaohan Yu, Li Zhang, Xin Zhao, Yue Wang, Zhongrui Ma. (2024)<br><strong>RA-Rec: An Efficient ID Representation Alignment Framework for LLM-based Recommendation</strong><br><button class=copy-to-clipboard title="RA-Rec: An Efficient ID Representation Alignment Framework for LLM-based Recommendation" index=162>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-162 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-AI, cs-IR, cs.IR<br>Keyword Score: 40<br>Keywords: Recommendation, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04527v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04527v1.pdf filename=2402.04527v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLM)</b> have recently emerged as a powerful tool for a variety of natural language processing tasks, bringing a new surge of combining <b>LLM</b> with <b>recommendation</b> systems, termed as <b>LLM-based</b> RS. Current approaches generally fall into two main paradigms, the ID direct usage paradigm and the ID translation paradigm, noting their core weakness stems from lacking <b>recommendation</b> knowledge and uniqueness. To address this limitation, we propose a new paradigm, ID representation, which incorporates pre-trained ID embeddings into <b>LLMs</b> in a complementary manner. In this work, we present RA-Rec, an efficient ID representation alignment framework for <b>LLM-based</b> <b>recommendation,</b> which is compatible with multiple ID-based methods and <b>LLM</b> architectures. Specifically, we treat ID embeddings as soft <b>prompts</b> and design an innovative alignment module and an efficient tuning method with tailored data construction for alignment. Extensive experiments demonstrate RA-Rec substantially outperforms current state-of-the-art methods, achieving up to 3.0% absolute HitRate@100 improvements while utilizing less than 10x training data.</p></p class="citation"></blockquote><h3 id=163236-normy-non-uniform-history-modeling-for-open-retrieval-conversational-question-answering-muhammad-shihab-rashid-et-al-2024>(163/236) NORMY: Non-Uniform History Modeling for Open Retrieval Conversational Question Answering (Muhammad Shihab Rashid et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Muhammad Shihab Rashid, Jannat Ara Meem, Vagelis Hristidis. (2024)<br><strong>NORMY: Non-Uniform History Modeling for Open Retrieval Conversational Question Answering</strong><br><button class=copy-to-clipboard title="NORMY: Non-Uniform History Modeling for Open Retrieval Conversational Question Answering" index=163>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-163 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 30<br>Keywords: Rerank, Unsupervised Learning, Question Answering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04548v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04548v1.pdf filename=2402.04548v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Open Retrieval Conversational <b>Question</b> <b>Answering</b> (OrConvQA) answers a <b>question</b> <b>given</b> a conversation as context and a document collection. A typical OrConvQA pipeline consists of three modules: a Retriever to retrieve relevant documents from the collection, a Reranker to <b>rerank</b> them given the <b>question</b> <b>and</b> the context, and a Reader to extract an answer span. The conversational turns can provide valuable context to answer the final query. State-of-the-art OrConvQA systems use the same history modeling for all three modules of the pipeline. We hypothesize this as suboptimal. Specifically, we argue that a broader context is needed in the first modules of the pipeline to not miss relevant documents, while a narrower context is needed in the last modules to identify the exact answer span. We propose NORMY, the first <b>unsupervised</b> non-uniform history modeling pipeline which generates the best conversational history for each module. We further propose a novel Retriever for NORMY, which employs keyphrase extraction on the conversation history, and leverages passages retrieved in previous turns as additional context. We also created a new dataset for OrConvQA, by expanding the doc2dial dataset. We implemented various state-of-the-art history modeling techniques and comprehensively evaluated them separately for each module of the pipeline on three datasets: OR-QUAC, our doc2dial extension, and ConvMix. Our extensive experiments show that NORMY outperforms the state-of-the-art in the individual modules and in the end-to-end system.</p></p class="citation"></blockquote><h3 id=164236-theoretical-and-empirical-analysis-of-adaptive-entry-point-selection-for-graph-based-approximate-nearest-neighbor-search-yutaro-oguri-et-al-2024>(164/236) Theoretical and Empirical Analysis of Adaptive Entry Point Selection for Graph-based Approximate Nearest Neighbor Search (Yutaro Oguri et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yutaro Oguri, Yusuke Matsui. (2024)<br><strong>Theoretical and Empirical Analysis of Adaptive Entry Point Selection for Graph-based Approximate Nearest Neighbor Search</strong><br><button class=copy-to-clipboard title="Theoretical and Empirical Analysis of Adaptive Entry Point Selection for Graph-based Approximate Nearest Neighbor Search" index=164>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-164 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-DB, cs-IR, cs-LG, cs.IR<br>Keyword Score: 10<br>Keywords: Out-of-distribution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04713v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04713v1.pdf filename=2402.04713v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present a theoretical and empirical analysis of the adaptive entry point selection for graph-based approximate nearest neighbor search (ANNS). We introduce novel concepts: $b\textit{-monotonic path}$ and $B\textit{-MSNET}$, which better capture an actual graph in practical algorithms than existing concepts like MSNET. We prove that adaptive entry point selection offers better performance upper bound than the fixed central entry point under more general conditions than previous work. Empirically, we validate the method&rsquo;s effectiveness in accuracy, speed, and memory usage across various datasets, especially in challenging scenarios with <b>out-of-distribution</b> data and hard instances. Our comprehensive study provides deeper insights into optimizing entry points for graph-based ANNS for real-world high-dimensional data applications.</p></p class="citation"></blockquote><h2 id=eessiv-7>eess.IV (7)</h2><h3 id=165236-mamba-unet-unet-like-pure-visual-mamba-for-medical-image-segmentation-ziyang-wang-et-al-2024>(165/236) Mamba-UNet: UNet-Like Pure Visual Mamba for Medical Image Segmentation (Ziyang Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ziyang Wang, Jian-Qing Zheng, Yichi Zhang, Ge Cui, Lei Li. (2024)<br><strong>Mamba-UNet: UNet-Like Pure Visual Mamba for Medical Image Segmentation</strong><br><button class=copy-to-clipboard title="Mamba-UNet: UNet-Like Pure Visual Mamba for Medical Image Segmentation" index=165>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-165 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 50<br>Keywords: Convolution, Convolutional Neural Network, Convolutional Neural Network, Transformer, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05079v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05079v1.pdf filename=2402.05079v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In recent advancements in medical image analysis, <b>Convolutional</b> <b>Neural</b> <b>Networks</b> <b>(CNN)</b> and Vision <b>Transformers</b> (ViT) have set significant benchmarks. While the former excels in capturing local features through its <b>convolution</b> operations, the latter achieves remarkable global context understanding by leveraging <b>self-attention</b> mechanisms. However, both architectures exhibit limitations in efficiently modeling long-range dependencies within medical images, which is a critical aspect for precise segmentation. Inspired by the Mamba architecture, known for its proficiency in handling long sequences and global contextual information with enhanced computational efficiency as a State Space Model (SSM), we propose Mamba-UNet, a novel architecture that synergizes the U-Net in medical image segmentation with Mamba&rsquo;s capability. Mamba-UNet adopts a pure Visual Mamba (VMamba)-based encoder-decoder structure, infused with skip connections to preserve spatial information across different scales of the network. This design facilitates a comprehensive feature learning process, capturing intricate details and broader semantic contexts within medical images. We introduce a novel integration mechanism within the VMamba blocks to ensure seamless connectivity and information flow between the encoder and decoder paths, enhancing the segmentation performance. We conducted experiments on publicly available MRI cardiac multi-structures segmentation dataset. The results show that Mamba-UNet outperforms UNet, Swin-UNet in medical image segmentation under the same hyper-parameter setting. The source code and baseline implementations are available.</p></p class="citation"></blockquote><h3 id=166236-troublemaker-learning-for-low-light-image-enhancement-yinghao-song-et-al-2024>(166/236) Troublemaker Learning for Low-Light Image Enhancement (Yinghao Song et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yinghao Song, Zhiyuan Cao, Wanhong Xiang, Sifan Long, Bo Yang, Hongwei Ge, Yanchun Liang, Chunguo Wu. (2024)<br><strong>Troublemaker Learning for Low-Light Image Enhancement</strong><br><button class=copy-to-clipboard title="Troublemaker Learning for Low-Light Image Enhancement" index=166>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-166 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 50<br>Keywords: Convolution, Convolutional Neural Network, Supervised Learning, Unsupervised Learning, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04584v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04584v1.pdf filename=2402.04584v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Low-light image enhancement (LLIE) restores the color and brightness of underexposed images. <b>Supervised</b> methods suffer from high costs in collecting low/normal-light image pairs. <b>Unsupervised</b> methods invest substantial effort in crafting complex loss functions. We address these two challenges through the proposed TroubleMaker Learning (TML) strategy, which employs normal-light images as inputs for training. TML is simple: we first dim the input and then increase its brightness. TML is based on two core components. First, the troublemaker model (TM) constructs pseudo low-light images from normal images to relieve the cost of pairwise data. Second, the predicting model (PM) enhances the brightness of pseudo low-light images. Additionally, we incorporate an enhancing model (EM) to further improve the visual performance of PM outputs. Moreover, in LLIE tasks, characterizing global element correlations is important because more information on the same object can be captured. <b>CNN</b> cannot achieve this well, and <b>self-attention</b> has high time complexity. Accordingly, we propose Global Dynamic <b>Convolution</b> (GDC) with O(n) time complexity, which essentially imitates the partial calculation process of <b>self-attention</b> to formulate elementwise correlations. Based on the GDC module, we build the UGDC model. Extensive quantitative and qualitative experiments demonstrate that UGDC trained with TML can achieve competitive performance against state-of-the-art approaches on public datasets. The code is available at <a href=https://github.com/Rainbowman0/TML_LLIE>https://github.com/Rainbowman0/TML_LLIE</a>.</p></p class="citation"></blockquote><h3 id=167236-triplet-constraint-transformer-with-multi-scale-refinement-for-dose-prediction-in-radiotherapy-lu-wen-et-al-2024>(167/236) Triplet-constraint Transformer with Multi-scale Refinement for Dose Prediction in Radiotherapy (Lu Wen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lu Wen, Qihun Zhang, Zhenghao Feng, Yuanyuan Xu, Xiao Chen, Jiliu Zhou, Yan Wang. (2024)<br><strong>Triplet-constraint Transformer with Multi-scale Refinement for Dose Prediction in Radiotherapy</strong><br><button class=copy-to-clipboard title="Triplet-constraint Transformer with Multi-scale Refinement for Dose Prediction in Radiotherapy" index=167>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-167 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 40<br>Keywords: Convolution, Convolutional Neural Network, Convolutional Neural Network, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04566v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04566v1.pdf filename=2402.04566v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Radiotherapy is a primary treatment for cancers with the aim of applying sufficient radiation dose to the planning target volume (PTV) while minimizing dose hazards to the organs at risk (OARs). <b>Convolutional</b> <b>neural</b> <b>networks</b> <b>(CNNs)</b> have automated the radiotherapy plan-making by predicting the dose maps. However, current <b>CNN-based</b> methods ignore the remarkable dose difference in the dose map, i.e., high dose value in the interior PTV while low value in the exterior PTV, leading to a suboptimal prediction. In this paper, we propose a triplet-constraint <b>transformer</b> (TCtrans) with multi-scale refinement to predict the high-quality dose distribution. Concretely, a novel PTV-guided triplet constraint is designed to refine dose feature representations in the interior and exterior PTV by utilizing the explicit geometry of PTV. Furthermore, we introduce a multi-scale refinement (MSR) module to effectively fulfill the triplet constraint in different decoding layers with multiple scales. Besides, a <b>transformer</b> encoder is devised to learn the important global dosimetric knowledge. Experiments on a clinical cervical cancer dataset demonstrate the superiority of our method.</p></p class="citation"></blockquote><h3 id=168236-mirt-a-simultaneous-reconstruction-and-affine-motion-compensation-technique-for-four-dimensional-computed-tomography-4dct-anh-tuan-nguyen-et-al-2024>(168/236) MIRT: a simultaneous reconstruction and affine motion compensation technique for four dimensional computed tomography (4DCT) (Anh-Tuan Nguyen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Anh-Tuan Nguyen, Jens Renders, Domenico Iuso, Yves Maris, Jeroen Soete, Martine Wevers, Jan Sijbers, Jan De Beenhouwer. (2024)<br><strong>MIRT: a simultaneous reconstruction and affine motion compensation technique for four dimensional computed tomography (4DCT)</strong><br><button class=copy-to-clipboard title="MIRT: a simultaneous reconstruction and affine motion compensation technique for four dimensional computed tomography (4DCT)" index=168>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-168 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: 65K10, 68U10, 68W01, 92C55, 94A08, cs-CV, eess-IV, eess.IV, math-OC<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04480v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04480v1.pdf filename=2402.04480v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In four-dimensional computed tomography (4DCT), 3D images of moving or deforming samples are reconstructed from a set of 2D projection images. Recent techniques for iterative motion-compensated reconstruction either necessitate a reference acquisition or alternate image reconstruction and motion estimation steps. In these methods, the motion estimation step involves the estimation of either complete deformation vector fields (DVFs) or a limited set of parameters corresponding to the affine motion, including rigid motion or scaling. The majority of these approaches rely on nested iterations, incurring significant computational expenses. Notably, despite the direct benefits of an analytical formulation and a substantial reduction in computational complexity, there has been no exploration into parameterizing DVFs for general affine motion in CT imaging. In this work, we propose the Motion-compensated Iterative Reconstruction Technique (MIRT)- an efficient iterative reconstruction scheme that combines image reconstruction and affine motion estimation in a single update step, based on the analytical gradients of the motion towards both the reconstruction and the affine motion parameters. When most of the state-of-the-art 4DCT methods have not attempted to be tested on real data, results from <b>simulation</b> and real experiments show that our method outperforms the state-of-the-art CT reconstruction with affine motion correction methods in computational feasibility and projection distance. In particular, this allows accurate reconstruction for a proper microscale diamond in the appearance of motion from the practically acquired projection radiographs, which leads to a novel application of 4DCT.</p></p class="citation"></blockquote><h3 id=169236-self-calibrated-convolution-towards-glioma-segmentation-felipe-c-r-salvagnini-et-al-2024>(169/236) Self-calibrated convolution towards glioma segmentation (Felipe C. R. Salvagnini et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Felipe C. R. Salvagnini, Gerson O. Barbosa, Alexandre X. Falcao, Cid A. N. Santos. (2024)<br><strong>Self-calibrated convolution towards glioma segmentation</strong><br><button class=copy-to-clipboard title="Self-calibrated convolution towards glioma segmentation" index=169>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-169 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, cs-LG, eess-IV, eess.IV<br>Keyword Score: 10<br>Keywords: Convolution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05218v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05218v1.pdf filename=2402.05218v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Accurate brain tumor segmentation in the early stages of the disease is crucial for the treatment&rsquo;s effectiveness, avoiding exhaustive visual inspection of a qualified specialist on 3D MR brain images of multiple protocols (e.g., T1, T2, T2-FLAIR, T1-Gd). Several networks exist for Glioma segmentation, being nnU-Net one of the best. In this work, we evaluate self-calibrated <b>convolutions</b> in different parts of the nnU-Net network to demonstrate that self-calibrated modules in skip connections can significantly improve the enhanced-tumor and tumor-core segmentation accuracy while preserving the wholetumor segmentation accuracy.</p></p class="citation"></blockquote><h3 id=170236-anatomically-controllable-medical-image-generation-with-segmentation-guided-diffusion-models-nicholas-konz-et-al-2024>(170/236) Anatomically-Controllable Medical Image Generation with Segmentation-Guided Diffusion Models (Nicholas Konz et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nicholas Konz, Yuwen Chen, Haoyu Dong, Maciej A. Mazurowski. (2024)<br><strong>Anatomically-Controllable Medical Image Generation with Segmentation-Guided Diffusion Models</strong><br><button class=copy-to-clipboard title="Anatomically-Controllable Medical Image Generation with Segmentation-Guided Diffusion Models" index=170>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-170 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, cs-LG, eess-IV, eess.IV, stat-ML<br>Keyword Score: 10<br>Keywords: Counter-factual<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05210v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05210v2.pdf filename=2402.05210v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Diffusion models have enabled remarkably high-quality medical image generation, which can help mitigate the expenses of acquiring and annotating new images by supplementing small or imbalanced datasets, along with other applications. However, these are hampered by the challenge of enforcing global anatomical realism in generated images. To this end, we propose a diffusion model for anatomically-controlled medical image generation. Our model follows a multi-class anatomical segmentation mask at each sampling step and incorporates a \textit{random mask ablation} training algorithm, to enable conditioning on a selected combination of anatomical constraints while allowing flexibility in other anatomical areas. This also improves the network&rsquo;s learning of anatomical realism for the completely unconditional (unconstrained generation) case. Comparative evaluation on breast MRI and abdominal/neck-to-pelvis CT datasets demonstrates superior anatomical realism and input mask faithfulness over state-of-the-art models. We also offer an accessible codebase and release a dataset of generated paired breast MRIs. Our approach facilitates diverse applications, including pre-registered image generation, <b>counterfactual</b> scenarios, and others.</p></p class="citation"></blockquote><h3 id=171236-cortical-surface-diffusion-generative-models-zhenshan-xie-et-al-2024>(171/236) Cortical Surface Diffusion Generative Models (Zhenshan Xie et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhenshan Xie, Simon Dahan, Logan Z. J. Williams, M. Jorge Cardoso, Emma C. Robinson. (2024)<br><strong>Cortical Surface Diffusion Generative Models</strong><br><button class=copy-to-clipboard title="Cortical Surface Diffusion Generative Models" index=171>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-171 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 10<br>Keywords: Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04753v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04753v1.pdf filename=2402.04753v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Cortical surface analysis has gained increased prominence, given its potential implications for neurological and developmental disorders. Traditional vision diffusion models, while effective in generating natural images, present limitations in capturing intricate development patterns in neuroimaging due to limited datasets. This is particularly true for generating cortical surfaces where individual variability in cortical morphology is high, leading to an urgent need for better methods to model brain development and diverse variability inherent across different individuals. In this work, we proposed a novel diffusion model for the generation of cortical surface metrics, using modified surface vision <b>transformers</b> as the principal architecture. We validate our method in the developing Human Connectome Project (dHCP), the results suggest our model demonstrates superior performance in capturing the intricate details of evolving cortical surfaces. Furthermore, our model can generate high-quality realistic samples of cortical surfaces conditioned on postmenstrual age(PMA) at scan.</p></p class="citation"></blockquote><h2 id=cshc-6>cs.HC (6)</h2><h3 id=172236-chatbots-in-knowledge-intensive-contexts-comparing-intent-and-llm-based-systems-samuel-kernan-freire-et-al-2024>(172/236) Chatbots in Knowledge-Intensive Contexts: Comparing Intent and LLM-Based Systems (Samuel Kernan Freire et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Samuel Kernan Freire, Chaofan Wang, Evangelos Niforatos. (2024)<br><strong>Chatbots in Knowledge-Intensive Contexts: Comparing Intent and LLM-Based Systems</strong><br><button class=copy-to-clipboard title="Chatbots in Knowledge-Intensive Contexts: Comparing Intent and LLM-Based Systems" index=172>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-172 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-AI, cs-HC, cs.HC<br>Keyword Score: 50<br>Keywords: GPT, GPT-4, Chatbot, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04955v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04955v1.pdf filename=2402.04955v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Cognitive assistants (CA) are <b>chatbots</b> that provide context-aware support to human workers in knowledge-intensive tasks. Traditionally, cognitive assistants respond in specific ways to predefined user intents and conversation patterns. However, this rigidness does not handle the diversity of natural language well. Recent advances in natural language processing (NLP), powering <b>large</b> <b>language</b> <b>models</b> <b>(LLM)</b> such as <b>GPT-4,</b> Llama2, and Gemini, could enable CAs to converse in a more flexible, human-like manner. However, the additional degrees of freedom may have unforeseen consequences, especially in knowledge-intensive contexts where accuracy is crucial. As a preliminary step to assessing the potential of using <b>LLMs</b> in these contexts, we conducted a user study comparing an <b>LLM-based</b> CA to an intent-based system regarding interaction efficiency, user experience, workload, and usability. This revealed that <b>LLM-based</b> CAs exhibited better user experience, task completion rate, usability, and perceived performance than intent-based systems, suggesting that switching NLP techniques should be investigated further.</p></p class="citation"></blockquote><h3 id=173236-cataractbot-an-llm-powered-expert-in-the-loop-chatbot-for-cataract-patients-pragnya-ramjee-et-al-2024>(173/236) CataractBot: An LLM-Powered Expert-in-the-Loop Chatbot for Cataract Patients (Pragnya Ramjee et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Pragnya Ramjee, Bhuvan Sachdeva, Satvik Golechha, Shreyas Kulkarni, Geeta Fulari, Kaushik Murali, Mohit Jain. (2024)<br><strong>CataractBot: An LLM-Powered Expert-in-the-Loop Chatbot for Cataract Patients</strong><br><button class=copy-to-clipboard title="CataractBot: An LLM-Powered Expert-in-the-Loop Chatbot for Cataract Patients" index=173>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-173 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-HC, cs-LG, cs.HC<br>Keyword Score: 36<br>Keywords: Multi-modal, Multi-modal, Chatbot, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04620v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04620v1.pdf filename=2402.04620v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The healthcare landscape is evolving, with patients seeking more reliable information about their health conditions, treatment options, and potential risks. Despite the abundance of information sources, the digital age overwhelms individuals with excess, often inaccurate information. Patients primarily trust doctors and hospital staff, highlighting the need for expert-endorsed health information. However, the pressure on experts has led to reduced communication time, impacting information sharing. To address this gap, we propose CataractBot, an experts-in-the-loop <b>chatbot</b> powered by <b>large</b> <b>language</b> <b>models</b> <b>(LLMs).</b> Developed in collaboration with a tertiary eye hospital in India, CataractBot answers cataract surgery related questions instantly by querying a curated knowledge base, and provides expert-verified responses asynchronously. CataractBot features <b>multimodal</b> support and multilingual capabilities. In an in-the-wild deployment study with 49 participants, CataractBot proved valuable, providing anytime accessibility, saving time, and accommodating diverse literacy levels. Trust was established through expert verification. Broadly, our results could inform future work on designing expert-mediated <b>LLM</b> bots.</p></p class="citation"></blockquote><h3 id=174236-large-language-user-interfaces-voice-interactive-user-interfaces-powered-by-llms-syed-mekael-wasti-et-al-2024>(174/236) Large Language User Interfaces: Voice Interactive User Interfaces powered by LLMs (Syed Mekael Wasti et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Syed Mekael Wasti, Ken Q. Pu, Ali Neshati. (2024)<br><strong>Large Language User Interfaces: Voice Interactive User Interfaces powered by LLMs</strong><br><button class=copy-to-clipboard title="Large Language User Interfaces: Voice Interactive User Interfaces powered by LLMs" index=174>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-174 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: I-2-7; I-2-1, cs-AI, cs-CL, cs-HC, cs-LG, cs.HC<br>Keyword Score: 30<br>Keywords: Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07938v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07938v1.pdf filename=2402.07938v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The recent meteoric advancements in <b>large</b> <b>language</b> <b>models</b> have showcased a remarkable capacity for logical <b>reasoning</b> and comprehension. These newfound capabilities have opened the door to a new generation of software, as has been made obvious through the innumerable ways they are being applied in the industry. This research focuses on harnessing and guiding the upgraded power of <b>LLMs</b> to construct a framework that can serve as an intermediary between a user and their user interface. By comprehending a user&rsquo;s needs through a thorough analysis of natural textual inputs, an effectively crafted <b>LLM</b> engine can classify the most likely available application, identify the desired UI component and subsequently execute the user&rsquo;s expected actions. This integration can evolve static UI systems into highly dynamic and adaptable solutions, introducing a new frontier of intelligent and responsive user experiences. Such a framework can fundamentally shift how users accomplish daily tasks, skyrocket efficiency, and greatly reduce cognitive load.</p></p class="citation"></blockquote><h3 id=175236-chatscratch-an-ai-augmented-system-toward-autonomous-visual-programming-learning-for-children-aged-6-12-liuqing-chen-et-al-2024>(175/236) ChatScratch: An AI-Augmented System Toward Autonomous Visual Programming Learning for Children Aged 6-12 (Liuqing Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Liuqing Chen, Shuhong Xiao, Yunnong Chen, Ruoyu Wu, Yaxuan Song, Lingyun Sun. (2024)<br><strong>ChatScratch: An AI-Augmented System Toward Autonomous Visual Programming Learning for Children Aged 6-12</strong><br><button class=copy-to-clipboard title="ChatScratch: An AI-Augmented System Toward Autonomous Visual Programming Learning for Children Aged 6-12" index=175>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-175 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-AI, cs-HC, cs-PL, cs.HC<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04975v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04975v1.pdf filename=2402.04975v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>As Computational Thinking (CT) continues to permeate younger age groups in K-12 education, established CT platforms such as Scratch face challenges in catering to these younger learners, particularly those in the elementary school (ages 6-12). Through formative investigation with Scratch experts, we uncover three key obstacles to children&rsquo;s autonomous Scratch learning: artist&rsquo;s block in project planning, bounded creativity in asset creation, and inadequate coding guidance during implementation. To address these barriers, we introduce ChatScratch, an AI-augmented system to facilitate autonomous programming learning for young children. ChatScratch employs structured interactive storyboards and visual cues to overcome artist&rsquo;s block, integrates digital drawing and advanced image generation technologies to elevate creativity, and leverages Scratch-specialized <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> for professional coding guidance. Our study shows that, compared to Scratch, ChatScratch efficiently fosters autonomous programming learning, and contributes to the creation of high-quality, personally meaningful Scratch projects for children.</p></p class="citation"></blockquote><h3 id=176236-giving-robots-a-voice-human-in-the-loop-voice-creation-and-open-ended-labeling-pol-van-rijn-et-al-2024>(176/236) Giving Robots a Voice: Human-in-the-Loop Voice Creation and open-ended Labeling (Pol van Rijn et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Pol van Rijn, Silvan Mertes, Kathrin Janowski, Katharina Weitz, Nori Jacoby, Elisabeth André. (2024)<br><strong>Giving Robots a Voice: Human-in-the-Loop Voice Creation and open-ended Labeling</strong><br><button class=copy-to-clipboard title="Giving Robots a Voice: Human-in-the-Loop Voice Creation and open-ended Labeling" index=176>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-176 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-HC, cs-RO, cs.HC<br>Keyword Score: 10<br>Keywords: human-in-the-loop<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05206v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05206v1.pdf filename=2402.05206v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Speech is a natural interface for humans to interact with robots. Yet, aligning a robot&rsquo;s voice to its appearance is challenging due to the rich vocabulary of both modalities. Previous research has explored a few labels to describe robots and tested them on a limited number of robots and existing voices. Here, we develop a robot-voice creation tool followed by large-scale behavioral human experiments (N=2,505). First, participants collectively tune robotic voices to match 175 robot images using an adaptive <b>human-in-the-loop</b> pipeline. Then, participants describe their impression of the robot or their matched voice using another <b>human-in-the-loop</b> paradigm for open-ended labeling. The elicited taxonomy is then used to rate robot attributes and to predict the best voice for an unseen robot. We offer a web interface to aid engineers in customizing robot voices, demonstrating the synergy between cognitive science and machine learning for engineering tools.</p></p class="citation"></blockquote><h3 id=177236-charting-the-covid-long-haul-experience----a-longitudinal-exploration-of-symptoms-activity-and-clinical-adherence-jessica-pater-et-al-2024>(177/236) Charting the COVID Long Haul Experience &ndash; A Longitudinal Exploration of Symptoms, Activity, and Clinical Adherence (Jessica Pater et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jessica Pater, Shaan Chopra, Juliette Zaccour, Jeanne Carroll, Fayika Farhat Nova, Tammy Toscos, Shion Guha, Fen Lei Chang. (2024)<br><strong>Charting the COVID Long Haul Experience &ndash; A Longitudinal Exploration of Symptoms, Activity, and Clinical Adherence</strong><br><button class=copy-to-clipboard title="Charting the COVID Long Haul Experience -- A Longitudinal Exploration of Symptoms, Activity, and Clinical Adherence" index=177>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-177 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: K-4, cs-CY, cs-HC, cs.HC<br>Keyword Score: 10<br>Keywords: Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04937v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04937v1.pdf filename=2402.04937v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>COVID Long Haul (CLH) is an emerging chronic illness with varied patient experiences. Our understanding of CLH is often limited to data from electronic health records (EHRs), such as diagnoses or problem lists, which do not capture the volatility and severity of symptoms or their impact. To better understand the unique presentation of CLH, we conducted a 3-month long cohort study with 14 CLH patients, collecting objective (EHR, daily Fitbit logs) and subjective (weekly surveys, interviews) data. Our findings reveal a complex presentation of symptoms, associated uncertainty, and the ensuing impact CLH has on patients&rsquo; personal and professional lives. We identify patient needs, practices, and challenges around adhering to clinical <b>recommendations,</b> engaging with health data, and establishing &ldquo;new normals&rdquo; post COVID. We reflect on the potential found at the intersection of these various data streams and the persuasive heuristics possible when designing for this new population and their specific needs.</p></p class="citation"></blockquote><h2 id=csai-10>cs.AI (10)</h2><h3 id=178236-sparql-generation-an-analysis-on-fine-tuning-openllama-for-question-answering-over-a-life-science-knowledge-graph-julio-c-rangel-et-al-2024>(178/236) SPARQL Generation: an analysis on fine-tuning OpenLLaMA for Question Answering over a Life Science Knowledge Graph (Julio C. Rangel et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Julio C. Rangel, Tarcisio Mendes de Farias, Ana Claudia Sima, Norio Kobayashi. (2024)<br><strong>SPARQL Generation: an analysis on fine-tuning OpenLLaMA for Question Answering over a Life Science Knowledge Graph</strong><br><button class=copy-to-clipboard title="SPARQL Generation: an analysis on fine-tuning OpenLLaMA for Question Answering over a Life Science Knowledge Graph" index=178>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-178 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CL, cs-DB, cs-IR, cs.AI<br>Keyword Score: 50<br>Keywords: Data Augmentation, Fine-tuning, Question Answering, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04627v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04627v1.pdf filename=2402.04627v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The recent success of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLM)</b> in a wide range of Natural Language Processing applications opens the path towards novel <b>Question</b> <b>Answering</b> Systems over Knowledge Graphs leveraging <b>LLMs.</b> However, one of the main obstacles preventing their implementation is the scarcity of training <b>data</b> <b>for</b> the task of translating <b>questions</b> <b>into</b> corresponding SPARQL queries, particularly in the case of domain-specific KGs. To overcome this challenge, in this study, we evaluate several strategies for <b>fine-tuning</b> the OpenLlama <b>LLM</b> for <b>question</b> <b>answering</b> over life science knowledge graphs. In particular, we propose an end-to-end <b>data</b> <b>augmentation</b> approach for extending a set of existing queries over a given knowledge graph towards a larger dataset of semantically enriched <b>question-to-SPARQL</b> <b>query</b> pairs, enabling <b>fine-tuning</b> even for datasets where these pairs are scarce. In this context, we also investigate the role of semantic &ldquo;clues&rdquo; in the queries, such as meaningful variable names and inline comments. Finally, we evaluate our approach over the real-world Bgee gene expression knowledge graph and we show that semantic clues can improve model performance by up to 33% compared to a baseline with random variable names and no comments included.</p></p class="citation"></blockquote><h3 id=179236-can-large-language-model-agents-simulate-human-trust-behaviors-chengxing-xie-et-al-2024>(179/236) Can Large Language Model Agents Simulate Human Trust Behaviors? (Chengxing Xie et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chengxing Xie, Canyu Chen, Feiran Jia, Ziyu Ye, Kai Shu, Adel Bibi, Ziniu Hu, Philip Torr, Bernard Ghanem, Guohao Li. (2024)<br><strong>Can Large Language Model Agents Simulate Human Trust Behaviors?</strong><br><button class=copy-to-clipboard title="Can Large Language Model Agents Simulate Human Trust Behaviors?" index=179>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-179 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CL, cs-HC, cs.AI<br>Keyword Score: 50<br>Keywords: Simulation, Simulator, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04559v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04559v1.pdf filename=2402.04559v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Model</b> <b>(LLM)</b> agents have been increasingly adopted as <b>simulation</b> tools to model humans in applications such as social science. However, one fundamental question remains: can <b>LLM</b> agents really simulate human behaviors? In this paper, we focus on one of the most critical behaviors in human interactions, trust, and aim to investigate whether or not <b>LLM</b> agents can simulate human trust behaviors. We first find that <b>LLM</b> agents generally exhibit trust behaviors, referred to as agent trust, under the framework of Trust Games, which are widely recognized in behavioral economics. Then, we discover that <b>LLM</b> agents can have high behavioral alignment with humans regarding trust behaviors, indicating the feasibility to simulate human trust behaviors with <b>LLM</b> agents. In addition, we probe into the biases in agent trust and the differences in agent trust towards agents and humans. We also explore the intrinsic properties of agent trust under conditions including advanced <b>reasoning</b> strategies and external manipulations. We further offer important implications for various scenarios where trust is paramount. Our study represents a significant step in understanding the behaviors of <b>LLM</b> agents and the <b>LLM-human</b> analogy.</p></p class="citation"></blockquote><h3 id=180236-three-pathways-to-neurosymbolic-reinforcement-learning-with-interpretable-model-and-policy-networks-peter-graf-et-al-2024>(180/236) Three Pathways to Neurosymbolic Reinforcement Learning with Interpretable Model and Policy Networks (Peter Graf et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Peter Graf, Patrick Emami. (2024)<br><strong>Three Pathways to Neurosymbolic Reinforcement Learning with Interpretable Model and Policy Networks</strong><br><button class=copy-to-clipboard title="Three Pathways to Neurosymbolic Reinforcement Learning with Interpretable Model and Policy Networks" index=180>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-180 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-LG, cs.AI<br>Keyword Score: 40<br>Keywords: Reinforcement Learning, Simulation, Simulator, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05307v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05307v1.pdf filename=2402.05307v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Neurosymbolic AI combines the interpretability, parsimony, and explicit <b>reasoning</b> of classical symbolic approaches with the statistical learning of data-driven neural approaches. Models and policies that are simultaneously differentiable and interpretable may be key enablers of this marriage. This paper demonstrates three pathways to implementing such models and policies in a real-world <b>reinforcement</b> <b>learning</b> setting. Specifically, we study a broad class of neural networks that build interpretable semantics directly into their architecture. We reveal and highlight both the potential and the essential difficulties of combining logic, <b>simulation,</b> and learning. One lesson is that learning benefits from continuity and differentiability, but classical logic is discrete and non-differentiable. The relaxation to real-valued, differentiable representations presents a trade-off; the more learnable, the less interpretable. Another lesson is that using logic in the context of a numerical <b>simulation</b> involves a non-trivial mapping from raw (e.g., real-valued time series) <b>simulation</b> data to logical predicates. Some open questions this note exposes include: What are the limits of rule-based controllers, and how learnable are they? Do the differentiable interpretable approaches discussed here scale to large, complex, uncertain systems? Can we truly achieve interpretability? We highlight these and other themes across the three approaches.</p></p class="citation"></blockquote><h3 id=181236-codeit-self-improving-language-models-with-prioritized-hindsight-replay-natasha-butt-et-al-2024>(181/236) CodeIt: Self-Improving Language Models with Prioritized Hindsight Replay (Natasha Butt et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Natasha Butt, Blazej Manczak, Auke Wiggers, Corrado Rainone, David Zhang, Michaël Defferrard, Taco Cohen. (2024)<br><strong>CodeIt: Self-Improving Language Models with Prioritized Hindsight Replay</strong><br><button class=copy-to-clipboard title="CodeIt: Self-Improving Language Models with Prioritized Hindsight Replay" index=181>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-181 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CL, cs-LG, cs.AI<br>Keyword Score: 30<br>Keywords: Data Augmentation, Reasoning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04858v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04858v1.pdf filename=2402.04858v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> are increasingly solving tasks that are commonly believed to require human-level <b>reasoning</b> ability. However, these models still perform very poorly on benchmarks of general intelligence such as the Abstraction and <b>Reasoning</b> Corpus (ARC). In this paper, we approach ARC as a programming-by-examples problem, and introduce a novel and scalable method for language model self-improvement called Code Iteration (CodeIt). Our method iterates between 1) program sampling and hindsight relabeling, and 2) learning from prioritized experience replay. By relabeling the goal of an episode (i.e., the target program output given input) to the realized output produced by the sampled program, our method effectively deals with the extreme sparsity of rewards in program synthesis. Applying CodeIt to the ARC dataset, we demonstrate that prioritized hindsight replay, along with pre-training and <b>data-augmentation,</b> <b>leads</b> to successful inter-task generalization. CodeIt is the first neuro-symbolic approach that scales to the full ARC evaluation dataset. Our method solves 15% of ARC evaluation tasks, achieving state-of-the-art performance and outperforming existing neural and symbolic baselines.</p></p class="citation"></blockquote><h3 id=182236-explaining-learned-reward-functions-with-counterfactual-trajectories-jan-wehner-et-al-2024>(182/236) Explaining Learned Reward Functions with Counterfactual Trajectories (Jan Wehner et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jan Wehner, Frans Oliehoek, Luciano Cavalcante Siebert. (2024)<br><strong>Explaining Learned Reward Functions with Counterfactual Trajectories</strong><br><button class=copy-to-clipboard title="Explaining Learned Reward Functions with Counterfactual Trajectories" index=182>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-182 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-LG, cs.AI<br>Keyword Score: 30<br>Keywords: Counter-factual, Out-of-distribution, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04856v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04856v1.pdf filename=2402.04856v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Learning rewards from human behaviour or feedback is a promising approach to aligning AI systems with human values but fails to consistently extract correct reward functions. Interpretability tools could enable users to understand and evaluate possible flaws in learned reward functions. We propose <b>Counterfactual</b> Trajectory Explanations (CTEs) to interpret reward functions in <b>reinforcement</b> <b>learning</b> by contrasting an original with a <b>counterfactual</b> partial trajectory and the rewards they each receive. We derive six quality criteria for CTEs and propose a novel Monte-Carlo-based algorithm for generating CTEs that optimises these quality criteria. Finally, we measure how informative the generated explanations are to a proxy-human model by training it on CTEs. CTEs are demonstrably informative for the proxy-human model, increasing the similarity between its predictions and the reward function on unseen trajectories. Further, it learns to accurately judge differences in rewards between trajectories and generalises to <b>out-of-distribution</b> examples. Although CTEs do not lead to a perfect understanding of the reward, our method, and more generally the adaptation of XAI methods, are presented as a fruitful approach for interpreting learned reward functions.</p></p class="citation"></blockquote><h3 id=183236-direct-language-model-alignment-from-online-ai-feedback-shangmin-guo-et-al-2024>(183/236) Direct Language Model Alignment from Online AI Feedback (Shangmin Guo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shangmin Guo, Biao Zhang, Tianlin Liu, Tianqi Liu, Misha Khalman, Felipe Llinares, Alexandre Rame, Thomas Mesnard, Yao Zhao, Bilal Piot, Johan Ferret, Mathieu Blondel. (2024)<br><strong>Direct Language Model Alignment from Online AI Feedback</strong><br><button class=copy-to-clipboard title="Direct Language Model Alignment from Online AI Feedback" index=183>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-183 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CL, cs-HC, cs.AI<br>Keyword Score: 30<br>Keywords: Reinforcement Learning, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04792v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04792v1.pdf filename=2402.04792v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Direct alignment from preferences (DAP) methods, such as DPO, have recently emerged as efficient alternatives to <b>reinforcement</b> <b>learning</b> from human feedback (RLHF), that do not require a separate reward model. However, the preference datasets used in DAP methods are usually collected ahead of training and never updated, thus the feedback is purely offline. Moreover, responses in these datasets are often sampled from a language model distinct from the one being aligned, and since the model evolves over training, the alignment phase is inevitably off-policy. In this study, we posit that online feedback is key and improves DAP methods. Our method, online AI feedback (OAIF), uses an <b>LLM</b> as annotator: on each training iteration, we sample two responses from the current model and <b>prompt</b> the <b>LLM</b> annotator to choose which one is preferred, thus providing online feedback. Despite its simplicity, we demonstrate via human evaluation in several tasks that OAIF outperforms both offline DAP and RLHF methods. We further show that the feedback leveraged in OAIF is easily controllable, via instruction <b>prompts</b> to the <b>LLM</b> annotator.</p></p class="citation"></blockquote><h3 id=184236-s-agents-self-organizing-agents-in-open-ended-environment-jiaqi-chen-et-al-2024>(184/236) S-Agents: self-organizing agents in open-ended environment (Jiaqi Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiaqi Chen, Yuxian Jiang, Jiachen Lu, Li Zhang. (2024)<br><strong>S-Agents: self-organizing agents in open-ended environment</strong><br><button class=copy-to-clipboard title="S-Agents: self-organizing agents in open-ended environment" index=184>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-184 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-MA, cs.AI<br>Keyword Score: 30<br>Keywords: Human Intervention, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04578v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04578v2.pdf filename=2402.04578v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Leveraging <b>large</b> <b>language</b> <b>models</b> <b>(LLMs),</b> autonomous agents have significantly improved, gaining the ability to handle a variety of tasks. In open-ended settings, optimizing collaboration for efficiency and effectiveness demands flexible adjustments. Despite this, current research mainly emphasizes fixed, task-oriented workflows and overlooks agent-centric organizational structures. Drawing inspiration from <b>human</b> <b>organizational</b> behavior, we introduce a self-organizing agent system (S-Agents) with a &ldquo;tree of agents&rdquo; structure for dynamic workflow, an &ldquo;hourglass agent architecture&rdquo; for balancing information priorities, and a &ldquo;non-obstructive collaboration&rdquo; method to allow asynchronous task execution among agents. This structure can autonomously coordinate a group of agents, efficiently addressing the challenges of an open and dynamic environment without <b>human</b> <b>intervention.</b> Our experiments demonstrate that S-Agents proficiently execute collaborative building tasks and resource collection in the Minecraft environment, validating their effectiveness.</p></p class="citation"></blockquote><h3 id=185236-the-strain-of-success-a-predictive-model-for-injury-risk-mitigation-and-team-success-in-soccer-gregory-everett-et-al-2024>(185/236) The Strain of Success: A Predictive Model for Injury Risk Mitigation and Team Success in Soccer (Gregory Everett et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Gregory Everett, Ryan Beal, Tim Matthews, Timothy J. Norman, Sarvapali D. Ramchurn. (2024)<br><strong>The Strain of Success: A Predictive Model for Injury Risk Mitigation and Team Success in Soccer</strong><br><button class=copy-to-clipboard title="The Strain of Success: A Predictive Model for Injury Risk Mitigation and Team Success in Soccer" index=185>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-185 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-LG, cs.AI<br>Keyword Score: 10<br>Keywords: Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04898v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04898v1.pdf filename=2402.04898v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we present a novel sequential team selection model in soccer. Specifically, we model the stochastic process of player injury and unavailability using player-specific information learned from real-world soccer data. Monte-Carlo Tree Search is used to select teams for games that optimise long-term team performance across a soccer season by <b>reasoning</b> over player injury probability. We validate our approach compared to benchmark solutions for the 2018/19 English Premier League season. Our model achieves similar season expected points to the benchmark whilst reducing first-team injuries by ~13% and the money inefficiently spent on injured players by ~11% - demonstrating the potential to reduce costs and improve player welfare in real-world soccer teams.</p></p class="citation"></blockquote><h3 id=186236-a-unified-framework-for-probabilistic-verification-of-ai-systems-via-weighted-model-integration-paolo-morettin-et-al-2024>(186/236) A Unified Framework for Probabilistic Verification of AI Systems via Weighted Model Integration (Paolo Morettin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Paolo Morettin, Andrea Passerini, Roberto Sebastiani. (2024)<br><strong>A Unified Framework for Probabilistic Verification of AI Systems via Weighted Model Integration</strong><br><button class=copy-to-clipboard title="A Unified Framework for Probabilistic Verification of AI Systems via Weighted Model Integration" index=186>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-186 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-LG, cs.AI<br>Keyword Score: 10<br>Keywords: Fairness<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04892v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04892v1.pdf filename=2402.04892v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The probabilistic formal verification (PFV) of AI systems is in its infancy. So far, approaches have been limited to ad-hoc algorithms for specific classes of models and/or properties. We propose a unifying framework for the PFV of AI systems based onWeighted Model Integration (WMI), which allows to frame the problem in very general terms. Crucially, this reduction enables the verification of many properties of interest, like <b>fairness,</b> robustness or monotonicity, over a wide range of machine learning models, without making strong distributional assumptions. We support the generality of the approach by solving multiple verification tasks with a single, off-the-shelf WMI solver, then discuss the scalability challenges and research directions related to this promising framework.</p></p class="citation"></blockquote><h3 id=187236-advancing-explainable-ai-toward-human-like-intelligence-forging-the-path-to-artificial-brain-yongchen-zhou-et-al-2024>(187/236) Advancing Explainable AI Toward Human-Like Intelligence: Forging the Path to Artificial Brain (Yongchen Zhou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yongchen Zhou, Richard Jiang. (2024)<br><strong>Advancing Explainable AI Toward Human-Like Intelligence: Forging the Path to Artificial Brain</strong><br><button class=copy-to-clipboard title="Advancing Explainable AI Toward Human-Like Intelligence: Forging the Path to Artificial Brain" index=187>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-187 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 10<br>Keywords: Explainable AI<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.06673v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.06673v1.pdf filename=2402.06673v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The intersection of Artificial Intelligence (AI) and neuroscience in <b>Explainable</b> <b>AI</b> (XAI) is pivotal for enhancing transparency and interpretability in complex decision-making processes. This paper explores the evolution of XAI methodologies, ranging from feature-based to human-centric approaches, and delves into their applications in diverse domains, including healthcare and finance. The challenges in achieving explainability in generative models, ensuring responsible AI practices, and addressing ethical implications are discussed. The paper further investigates the potential convergence of XAI with cognitive sciences, the development of emotionally intelligent AI, and the quest for Human-Like Intelligence (HLI) in AI systems. As AI progresses towards Artificial General Intelligence (AGI), considerations of consciousness, ethics, and societal impact become paramount. The ongoing pursuit of deciphering the mysteries of the brain with AI and the quest for HLI represent transformative endeavors, bridging technical advancements with multidisciplinary explorations of human cognition.</p></p class="citation"></blockquote><h2 id=cond-matmtrl-sci-1>cond-mat.mtrl-sci (1)</h2><h3 id=188236-are-llms-ready-for-real-world-materials-discovery-santiago-miret-et-al-2024>(188/236) Are LLMs Ready for Real-World Materials Discovery? (Santiago Miret et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Santiago Miret, N M Anoop Krishnan. (2024)<br><strong>Are LLMs Ready for Real-World Materials Discovery?</strong><br><button class=copy-to-clipboard title="Are LLMs Ready for Real-World Materials Discovery?" index=188>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-188 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cond-mat.mtrl-sci<br>Categories: cond-mat-mtrl-sci, cond-mat.mtrl-sci, cs-AI, cs-CL, cs-LG<br>Keyword Score: 43<br>Keywords: Multi-modal, Information Retrieval, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05200v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05200v1.pdf filename=2402.05200v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> create exciting possibilities for powerful language processing tools to accelerate research in materials science. While <b>LLMs</b> have great potential to accelerate materials understanding and discovery, they currently fall short in being practical materials science tools. In this position paper, we show relevant failure cases of <b>LLMs</b> in materials science that reveal current limitations of <b>LLMs</b> related to comprehending and <b>reasoning</b> over complex, interconnected materials science knowledge. Given those shortcomings, we outline a framework for developing Materials Science <b>LLMs</b> (MatSci-LLMs) that are grounded in materials science knowledge and hypothesis generation followed by hypothesis testing. The path to attaining performant MatSci-LLMs rests in <b>large</b> <b>part</b> <b>on</b> building high-quality, <b>multi-modal</b> datasets sourced from scientific literature where various <b>information</b> <b>extraction</b> challenges persist. As such, we describe key materials science <b>information</b> <b>extraction</b> challenges which need to be overcome in order to build <b>large-scale,</b> <b>multi-modal</b> <b>datasets</b> that capture valuable materials science knowledge. Finally, we outline a roadmap for applying future MatSci-LLMs for real-world materials discovery via: 1. Automated Knowledge Base Generation; 2. Automated In-Silico Material Design; and 3. MatSci-LLM Integrated Self-Driving Materials Laboratories.</p></p class="citation"></blockquote><h2 id=csgr-1>cs.GR (1)</h2><h3 id=189236-m2fnet-multi-modal-forest-monitoring-network-on-large-scale-virtual-dataset-yawen-lu-et-al-2024>(189/236) M2fNet: Multi-modal Forest Monitoring Network on Large-scale Virtual Dataset (Yawen Lu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yawen Lu, Yunhan Huang, Su Sun, Tansi Zhang, Xuewen Zhang, Songlin Fei, Victor Chen. (2024)<br><strong>M2fNet: Multi-modal Forest Monitoring Network on Large-scale Virtual Dataset</strong><br><button class=copy-to-clipboard title="M2fNet: Multi-modal Forest Monitoring Network on Large-scale Virtual Dataset" index=189>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-189 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.GR<br>Categories: cs-GR, cs.GR<br>Keyword Score: 36<br>Keywords: Multi-modal, Multi-modal, Simulation, Simulator, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04534v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04534v1.pdf filename=2402.04534v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Forest monitoring and education are key to forest protection, education and management, which is an effective way to measure the progress of a country&rsquo;s forest and climate commitments. Due to the lack of a large-scale wild forest monitoring benchmark, the common practice is to train the model on a common outdoor benchmark (e.g., KITTI) and evaluate it on real forest datasets (e.g., CanaTree100). However, there is a large domain gap in this setting, which makes the evaluation and deployment difficult. In this paper, we propose a new photorealistic virtual forest dataset and a <b>multimodal</b> <b>transformer-based</b> algorithm for tree detection and instance segmentation. To the best of our knowledge, it is the first time that a <b>multimodal</b> detection and segmentation algorithm is applied to large-scale forest scenes. We believe that the proposed dataset and method will inspire the <b>simulation,</b> computer vision, education, and forestry communities towards a more comprehensive <b>multi-modal</b> understanding.</p></p class="citation"></blockquote><h2 id=q-biobm-2>q-bio.BM (2)</h2><h3 id=190236-alphafold-meets-flow-matching-for-generating-protein-ensembles-bowen-jing-et-al-2024>(190/236) AlphaFold Meets Flow Matching for Generating Protein Ensembles (Bowen Jing et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bowen Jing, Bonnie Berger, Tommi Jaakkola. (2024)<br><strong>AlphaFold Meets Flow Matching for Generating Protein Ensembles</strong><br><button class=copy-to-clipboard title="AlphaFold Meets Flow Matching for Generating Protein Ensembles" index=190>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-190 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: q-bio.BM<br>Categories: cs-LG, q-bio-BM, q-bio.BM<br>Keyword Score: 30<br>Keywords: Fine-tuning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04845v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04845v1.pdf filename=2402.04845v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The biological functions of proteins often depend on dynamic structural ensembles. In this work, we develop a flow-based generative modeling approach for learning and sampling the conformational landscapes of proteins. We repurpose highly accurate single-state predictors such as AlphaFold and ESMFold and <b>fine-tune</b> them under a custom flow matching framework to obtain sequence-conditoned generative models of protein structure called AlphaFlow and ESMFlow. When trained and evaluated on the PDB, our method provides a superior combination of precision and diversity compared to AlphaFold with MSA subsampling. When further trained on ensembles from all-atom MD, our method accurately captures conformational flexibility, positional distributions, and higher-order ensemble observables for unseen proteins. Moreover, our method can diversify a static PDB structure with faster wall-clock convergence to certain equilibrium properties than replicate MD trajectories, demonstrating its potential as a proxy for expensive physics-based <b>simulations.</b> Code is available at <a href=https://github.com/bjing2016/alphaflow>https://github.com/bjing2016/alphaflow</a>.</p></p class="citation"></blockquote><h3 id=191236-structure-informed-protein-language-model-zuobai-zhang-et-al-2024>(191/236) Structure-Informed Protein Language Model (Zuobai Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zuobai Zhang, Jiarui Lu, Vijil Chenthamarakshan, Aurélie Lozano, Payel Das, Jian Tang. (2024)<br><strong>Structure-Informed Protein Language Model</strong><br><button class=copy-to-clipboard title="Structure-Informed Protein Language Model" index=191>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-191 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: q-bio.BM<br>Categories: cs-LG, q-bio-BM, q-bio.BM<br>Keyword Score: 10<br>Keywords: Knowledge Distillation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05856v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05856v1.pdf filename=2402.05856v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Protein language models are a powerful tool for learning protein representations through pre-training on vast protein sequence datasets. However, traditional protein language models lack explicit structural supervision, despite its relevance to protein function. To address this issue, we introduce the integration of remote homology detection to <b>distill</b> structural information into protein language models without requiring explicit protein structures as input. We evaluate the impact of this structure-informed training on downstream protein function prediction tasks. Experimental results reveal consistent improvements in function annotation accuracy for EC number and GO term prediction. Performance on mutant datasets, however, varies based on the relationship between targeted properties and protein structures. This underscores the importance of considering this relationship when applying structure-aware training to protein function prediction tasks. Code and model weights are available at <a href=https://github.com/DeepGraphLearning/esm-s>https://github.com/DeepGraphLearning/esm-s</a>.</p></p class="citation"></blockquote><h2 id=cssd-2>cs.SD (2)</h2><h3 id=192236-fast-timing-conditioned-latent-audio-diffusion-zach-evans-et-al-2024>(192/236) Fast Timing-Conditioned Latent Audio Diffusion (Zach Evans et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zach Evans, CJ Carr, Josiah Taylor, Scott H. Hawley, Jordi Pons. (2024)<br><strong>Fast Timing-Conditioned Latent Audio Diffusion</strong><br><button class=copy-to-clipboard title="Fast Timing-Conditioned Latent Audio Diffusion" index=192>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-192 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SD<br>Categories: cs-LG, cs-SD, cs.SD, eess-AS<br>Keyword Score: 30<br>Keywords: Autoencoder, Variational Autoencoder, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04825v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04825v2.pdf filename=2402.04825v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Generating long-form 44.1kHz stereo audio from text <b>prompts</b> can be computationally demanding. Further, most previous works do not tackle that music and sound effects naturally vary in their duration. Our research focuses on the efficient generation of long-form, variable-length stereo music and sounds at 44.1kHz using text <b>prompts</b> with a generative model. Stable Audio is based on latent diffusion, with its latent defined by a fully-convolutional <b>variational</b> <b>autoencoder.</b> It is conditioned on text <b>prompts</b> as well as timing embeddings, allowing for fine control over both the content and length of the generated music and sounds. Stable Audio is capable of rendering stereo signals of up to 95 sec at 44.1kHz in 8 sec on an A100 GPU. Despite its compute efficiency and fast inference, it is one of the best in two public text-to-music and -audio benchmarks and, differently from state-of-the-art models, can generate music with structure and stereo sounds.</p></p class="citation"></blockquote><h3 id=193236-review-of-cetaceans-click-detection-algorithms-mak-gracic-et-al-2024>(193/236) Review of Cetacean&rsquo;s click detection algorithms (Mak Gracic et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mak Gracic, Guy Gubnisky, Roee Diamant. (2024)<br><strong>Review of Cetacean&rsquo;s click detection algorithms</strong><br><button class=copy-to-clipboard title="Review of Cetacean's click detection algorithms" index=193>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-193 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SD<br>Categories: 53-02, J-7; A-1, cs-SD, cs.SD, eess-AS, q-bio-QM<br>Keyword Score: 20<br>Keywords: Supervised Learning, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04735v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04735v1.pdf filename=2402.04735v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The detection of echolocation clicks is key in understanding the intricate behaviors of cetaceans and monitoring their populations. Cetacean species relying on clicks for navigation, foraging and even communications are sperm whales (Physeter macrocephalus) and a variety of dolphin groups. Echolocation clicks are wideband signals of short duration that are often emitted in sequences of varying inter-click-intervals. While datasets and models for clicks exist, the detection and classification of clicks present a significant challenge, mostly due to the diversity of clicks&rsquo; structures, overlapping signals from simultaneously emitting animals, and the abundance of noise transients from, for example, snapping shrimps and shipping cavitation noise. This paper provides a survey of the many detection and classification methodologies of clicks, ranging from 2002 to 2023. We divide the surveyed techniques into categories by their methodology. Specifically, feature analysis (e.g., phase, ICI and duration), frequency content, energy based detection, <b>supervised</b> and <b>unsupervised</b> machine learning, template matching and adaptive detection approaches. Also surveyed are open access platforms for click detections, and databases openly available for testing. Details of the method applied for each paper are given along with advantages and limitations, and for each category we analyze the remaining challenges. The paper also includes a performance comparison for several schemes over a shared database. Finally, we provide tables summarizing the existing detection schemes in terms of challenges address, methods, detection and classification tools applied, features used and applications.</p></p class="citation"></blockquote><h2 id=csni-3>cs.NI (3)</h2><h3 id=194236-an-advanced-scheme-for-queue-management-intcpip-networks-abderrahmane-boudi-et-al-2024>(194/236) An advanced scheme for queue management inTCP/IP networks (Abderrahmane Boudi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Abderrahmane Boudi, Malik Loudini. (2024)<br><strong>An advanced scheme for queue management inTCP/IP networks</strong><br><button class=copy-to-clipboard title="An advanced scheme for queue management inTCP/IP networks" index=194>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-194 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NI<br>Categories: cs-NI, cs-SY, cs.NI, eess-SY<br>Keyword Score: 30<br>Keywords: Fairness, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04818v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04818v1.pdf filename=2402.04818v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Active Queue Management (AQM) is a key congestion control scheme that aims to find a balance between keeping high link utilization, minimizing queuing delays, and ensuring a fair share of the bandwidth between the competing flows. Traditional AQM mechanisms use only information that is present at the intermediate nodes (routers). They do not take into account the particularities of the flows composing the traffic. In this paper, we make use of a mechanism, called Explicit RTT Notification (ERN), that shares with routers information about the Round Trip Times (RTTs) of the flows. We propose a new fuzzy logic based AQM controller that relies on the RTTs of the flows to improve <b>fairness</b> between them. The performances of the new proposed method, FuzzyRTT, is examined and compared to existing schemes via <b>simulation</b> experiments.</p></p class="citation"></blockquote><h3 id=195236-a-deep-reinforcement-learning-approach-for-adaptive-traffic-routing-in-next-gen-networks-akshita-abrol-et-al-2024>(195/236) A Deep Reinforcement Learning Approach for Adaptive Traffic Routing in Next-gen Networks (Akshita Abrol et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Akshita Abrol, Purnima Murali Mohan, Tram Truong-Huu. (2024)<br><strong>A Deep Reinforcement Learning Approach for Adaptive Traffic Routing in Next-gen Networks</strong><br><button class=copy-to-clipboard title="A Deep Reinforcement Learning Approach for Adaptive Traffic Routing in Next-gen Networks" index=195>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-195 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NI<br>Categories: cs-AI, cs-NI, cs.NI<br>Keyword Score: 30<br>Keywords: Convolution, Convolutional Neural Network, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04515v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04515v1.pdf filename=2402.04515v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Next-gen networks require significant evolution of management to enable automation and adaptively adjust network configuration based on traffic dynamics. The advent of software-defined networking (SDN) and programmable switches enables flexibility and programmability. However, traditional techniques that decide traffic policies are usually based on hand-crafted programming optimization and heuristic algorithms. These techniques make non-realistic assumptions, e.g., considering static network load and topology, to obtain tractable solutions, which are inadequate for next-gen networks. In this paper, we design and develop a deep <b>reinforcement</b> <b>learning</b> (DRL) approach for adaptive traffic routing. We design a deep graph <b>convolutional</b> <b>neural</b> <b>network</b> (DGCNN) integrated into the DRL framework to learn the traffic behavior from not only the network topology but also link and node attributes. We adopt the Deep Q-Learning technique to train the DGCNN model in the DRL framework without the need for a labeled training dataset, enabling the framework to quickly adapt to traffic dynamics. The model leverages q-value estimates to select the routing path for every traffic flow request, balancing exploration and exploitation. We perform extensive experiments with various traffic patterns and compare the performance of the proposed approach with the Open Shortest Path First (OSPF) protocol. The experimental results show the effectiveness and adaptiveness of the proposed framework by increasing the network throughput by up to 7.8% and reducing the traffic delay by up to 16.1% compared to OSPF.</p></p class="citation"></blockquote><h3 id=196236-splitsim-large-scale-simulations-for-evaluating-network-systems-research-hejing-li-et-al-2024>(196/236) SplitSim: Large-Scale Simulations for Evaluating Network Systems Research (Hejing Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hejing Li, Praneeth Balasubramanian, Marvin Meiers, Jialin Li, Antoine Kaufmann. (2024)<br><strong>SplitSim: Large-Scale Simulations for Evaluating Network Systems Research</strong><br><button class=copy-to-clipboard title="SplitSim: Large-Scale Simulations for Evaluating Network Systems Research" index=196>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-196 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NI<br>Categories: cs-DC, cs-NI, cs.NI<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05312v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05312v1.pdf filename=2402.05312v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>When physical testbeds are out of reach for evaluating a networked system, we frequently turn to <b>simulation.</b> In today&rsquo;s datacenter networks, bottlenecks are rarely at the network protocol level, but instead in end-host software or hardware components, thus current protocol-level <b>simulations</b> are inadequate means of evaluation. End-to-end <b>simulations</b> covering these components on the other hand, simply cannot achieve the required scale with feasible <b>simulation</b> performance and computational resources. In this paper, we address this with SplitSim, a <b>simulation</b> framework for end-to-end evaluation for large-scale network and distributed systems. To this end, SplitSim builds on prior work on modular end-to-end <b>simulations</b> and combines this with key elements to achieve scalability. First, mixed fidelity <b>simulations</b> judiciously reduce detail in <b>simulation</b> of parts of the system where this can be tolerated, while retaining the necessary detail elsewhere. SplitSim then parallelizes bottleneck simulators by decomposing them into multiple parallel but synchronized processes. Next, SplitSim provides a profiler to help users understand <b>simulation</b> performance and where the bottlenecks are, so users can adjust the configuration. Finally SplitSim provides abstractions to make it easy for users to build complex large-scale <b>simulations.</b> Our evaluation demonstrates SplitSim in multiple large-scale case studies.</p></p class="citation"></blockquote><h2 id=csit-4>cs.IT (4)</h2><h3 id=197236-detection-schemes-with-low-resolution-adcs-and-spatial-oversampling-for-transmission-with-higher-order-constellations-in-the-terahertz-band-christian-forsch-et-al-2024>(197/236) Detection Schemes with Low-Resolution ADCs and Spatial Oversampling for Transmission with Higher-Order Constellations in the Terahertz Band (Christian Forsch et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Christian Forsch, Peter Zillmann, Osama Alrabadi, Stefan Brueck, Wolfgang Gerstacker. (2024)<br><strong>Detection Schemes with Low-Resolution ADCs and Spatial Oversampling for Transmission with Higher-Order Constellations in the Terahertz Band</strong><br><button class=copy-to-clipboard title="Detection Schemes with Low-Resolution ADCs and Spatial Oversampling for Transmission with Higher-Order Constellations in the Terahertz Band" index=197>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-197 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, eess-SP, math-IT<br>Keyword Score: 30<br>Keywords: Quantization, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04728v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04728v1.pdf filename=2402.04728v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this work, we consider Terahertz (THz) communications with low-resolution uniform <b>quantization</b> and spatial oversampling at the receiver side. We compare different analog-to-digital converter (ADC) parametrizations in a fair manner by keeping the ADC power consumption constant. Here, 1-, 2-, and 3-bit <b>quantization</b> is investigated with different oversampling factors. We analytically compute the statistics of the detection variable, and we propose the optimal as well as several suboptimal detection schemes for arbitrary <b>quantization</b> resolutions. Then, we evaluate the symbol error rate (SER) of the different detectors for a 16- and a 64-ary quadrature amplitude modulation (QAM) constellation. The results indicate that there is a noticeable performance degradation of the suboptimal detection schemes compared to the optimal scheme when the constellation size is larger than the number of <b>quantization</b> levels. Furthermore, at low signal-to-noise ratios (SNRs), 1-bit <b>quantization</b> outperforms 2- and 3-bit <b>quantization,</b> respectively, even when employing higher-order constellations. We confirm our analytical results by Monte Carlo <b>simulations.</b> Both a pure line-of-sight (LoS) and a more realistically modeled indoor THz channel are considered. Then, we optimize the input signal constellation with respect to SER for 1-bit <b>quantization.</b> The results show that the minimum SER can be lowered significantly for 16-QAM by increasing the distance between the inner and outer points of the input constellation. For larger constellations, however, the achievable reduction of the minimum SER is much smaller compared to 16-QAM.</p></p class="citation"></blockquote><h3 id=198236-near-optimal-generalized-decoding-of-polar-like-codes-peihong-yuan-et-al-2024>(198/236) Near-Optimal Generalized Decoding of Polar-like Codes (Peihong Yuan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Peihong Yuan, Ken R. Duffy, Muriel Médard. (2024)<br><strong>Near-Optimal Generalized Decoding of Polar-like Codes</strong><br><button class=copy-to-clipboard title="Near-Optimal Generalized Decoding of Polar-like Codes" index=198>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-198 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, math-IT<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05004v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05004v1.pdf filename=2402.05004v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this work, we present a framework that explores the tradeoff between the undetected error rate (UER) and block error rate (BLER) of polar-like codes. It relies on a novel approximation for what we call codebook probability, which assumes an auxiliary distribution mimicking the dynamics of decoding algorithms with successive cancellation (SC) decoding schedule. <b>Simulation</b> results demonstrates that, in the case of SC list decoding, the proposed framework outperforms the state-of-art approximations of Forney&rsquo;s generalized decoding rule for polar-like codes with dynamic frozen bits. In addition, the proposed generalized decoding outperforms the CRC-concatenated polar codes significantly in both BLER and UER. Finally, we briefly discuss two potential applications of the approximated codebook probability: coded pilot-free channel estimation and bitwise soft-output decoding.</p></p class="citation"></blockquote><h3 id=199236-fast-beam-training-for-near-field-communication-systems-yuan-xu-et-al-2024>(199/236) Fast Beam Training for Near-Field Communication Systems (Yuan Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuan Xu, Chongwen Huang, Wei Li, Zhaohui Yang, Ahmed Al Hammadi, Jun Yang, Zhaoyang Zhang, Chau Yuen, Merouane Debbah. (2024)<br><strong>Fast Beam Training for Near-Field Communication Systems</strong><br><button class=copy-to-clipboard title="Fast Beam Training for Near-Field Communication Systems" index=199>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-199 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, eess-SP, math-IT<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04913v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04913v1.pdf filename=2402.04913v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In millimeter-wave communications, large-scale antenna arrays are commonly employed to mitigate obstacle occlusion and path loss. However, these large-scale arrays generate pencil-shaped beams, which necessitate a higher number of training beams to cover the desired space. This results in the heavy beam training overhead. Furthermore, as the antenna aperture increases, users are more likely to be situated in the near-field region of the base station (BS) antenna array. This motivates our investigation into the beam training problem in the near-field region to achieve efficient beam alignment. To address the high complexity and low identification accuracy of existing beam training techniques, we propose an efficient hashing multi-arm beam (HMB) training scheme for the near-field scenario. Specifically, we first design a set of sparse bases based on the polar domain sparsity of the near-field channel and construct a near-field single-beam training codebook. Then, the hash functions are chosen to construct the near-field multi-arm beam training codebook. Each multi-arm beam training codeword is used in a time slot until the predefined codebook is traversed. Finally, the soft decision and voting methods are applied to distinguish the signal from different BS and obtain the correctly aligned beams. In addition, we provide the logically rigorous proof of computational complexity. <b>Simulation</b> results show that our proposed near-field HMB training method can achieve 96.4% identification accuracy of the exhaustive beam training method and greatly reduce the training overhead to the logarithmic level. Furthermore, we verify its applicability under the far-field scenario as well.</p></p class="citation"></blockquote><h3 id=200236-outer-code-designs-for-augmented-and-local-global-polar-code-architectures-ziyuan-zhu-et-al-2024>(200/236) Outer Code Designs for Augmented and Local-Global Polar Code Architectures (Ziyuan Zhu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ziyuan Zhu, Paul H. Siegel. (2024)<br><strong>Outer Code Designs for Augmented and Local-Global Polar Code Architectures</strong><br><button class=copy-to-clipboard title="Outer Code Designs for Augmented and Local-Global Polar Code Architectures" index=200>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-200 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, math-IT<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04486v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04486v1.pdf filename=2402.04486v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we introduce two novel methods to design outer polar codes for two previously proposed concatenated polar code architectures: augmented polar codes and local-global polar codes. These methods include a stopping set (SS) construction and a nonstationary density evolution (NDE) construction. <b>Simulation</b> results demonstrate the advantage of these methods over previously proposed constructions based on density evolution (DE) and LLR evolution.</p></p class="citation"></blockquote><h2 id=eesssy-5>eess.SY (5)</h2><h3 id=201236-gaussian-process-based-nonlinear-moving-horizon-estimation-tobias-m-wolff-et-al-2024>(201/236) Gaussian Process-Based Nonlinear Moving Horizon Estimation (Tobias M. Wolff et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tobias M. Wolff, Victor G. Lopez, Matthias A. Müller. (2024)<br><strong>Gaussian Process-Based Nonlinear Moving Horizon Estimation</strong><br><button class=copy-to-clipboard title="Gaussian Process-Based Nonlinear Moving Horizon Estimation" index=201>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-201 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 30<br>Keywords: Gaussian Process, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04665v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04665v1.pdf filename=2402.04665v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we propose a novel <b>Gaussian</b> <b>process-based</b> moving horizon estimation (MHE) framework for unknown nonlinear systems. In the proposed scheme, we take advantage of the properties of <b>Gaussian</b> <b>processes.</b> On the one hand, we approximate the system dynamics by the posterior means of the learned <b>Gaussian</b> <b>processes</b> (GPs). On the other hand, we exploit the posterior variances of the <b>Gaussian</b> <b>processes</b> to design the weighting matrices in the MHE cost function and account for the uncertainty in the learned system dynamics. The data collection and the tuning of the hyperparameters are done offline. We prove robust stability of the GP-based MHE scheme using a Lyapunov-based proof technique. Furthermore, as additional contribution, we analyze under which conditions incremental input/output-to-state stability (a nonlinear detectability notion) is preserved when approximating the system dynamics using, e.g., machine learning techniques. Finally, we illustrate the performance of the GP-based MHE scheme in a <b>simulation</b> case study and show how the chosen weighting matrices can lead to an improved performance compared to standard cost functions.</p></p class="citation"></blockquote><h3 id=202236-control-of-ac-ac-interlinking-converters-for-multi-grids-jeremy-watson-et-al-2024>(202/236) Control of AC-AC interlinking converters for multi-grids (Jeremy Watson et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jeremy Watson, Ioannis Lestas. (2024)<br><strong>Control of AC-AC interlinking converters for multi-grids</strong><br><button class=copy-to-clipboard title="Control of AC-AC interlinking converters for multi-grids" index=202>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-202 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY, math-OC<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05303v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05303v1.pdf filename=2402.05303v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper considers the control of AC-AC inter-linking converters (ILCs) in a multi-grid network. We overview the control schemes in the literature and propose a passivity framework for the stabilization of multi-grid networks, considering both AC grid-following and AC grid-forming behavior for the ILC connections. We then analyze a range of AC/AC interlinking converter control methods derived from the literature and propose suitable controllers for this purpose including both AC grid-forming and grid-following behavior. The controller we propose is partially grid-forming; in particular, it is based on a combination of a grid-following and a grid-forming converter to improve the stability properties of the network. <b>Simulation</b> results and theoretical analysis confirm that the proposed ILC control designs are appropriate for the multi-grid network.</p></p class="citation"></blockquote><h3 id=203236-ascent-a-context-aware-spectrum-coexistence-design-and-implementation-toolset-for-policymakers-in-satellite-bands-ta-seen-reaz-niloy-et-al-2024>(203/236) ASCENT: A Context-Aware Spectrum Coexistence Design and Implementation Toolset for Policymakers in Satellite Bands (Ta-seen Reaz Niloy et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ta-seen Reaz Niloy, Saurav Kumar, Aniruddha Hore, Zoheb Hassan, Carl Dietrich, Eric W. Burger, Jeffrey H. Reed, Vijay K. Shah. (2024)<br><strong>ASCENT: A Context-Aware Spectrum Coexistence Design and Implementation Toolset for Policymakers in Satellite Bands</strong><br><button class=copy-to-clipboard title="ASCENT: A Context-Aware Spectrum Coexistence Design and Implementation Toolset for Policymakers in Satellite Bands" index=203>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-203 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05273v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05273v1.pdf filename=2402.05273v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper introduces ASCENT (context Aware Spectrum Coexistence Design and Implementation) toolset, an advanced context-aware terrestrial satellite spectrum sharing toolset designed for researchers, policymakers, and regulators. It serves two essential purposes (a) evaluating the potential for harmful interference to primary users in satellite bands and (b) facilitating the analysis, design, and implementation of diverse regulatory policies on spectrum usage and sharing. Notably, ASCENT implements a closed-loop feedback system that allows dynamic adaptation of policies according to a wide range of contextual factors (e.g., weather, buildings, summer/winter foliage, etc.) and feedback on the impact of these policies through realistic <b>simulation.</b> Specifically, ASCENT comprises the following components (i) interference evaluation tool for evaluating interference at the incumbents in a spectrum-sharing environment while taking the underlying contexts, (ii) dynamic spectrum access (DSA) framework for providing context-aware instructions to adapt networking parameters and control secondary terrestrial network&rsquo;s access to the shared spectrum band according to context aware prioritization, (iii) Context broker to acquire essential and relevant contexts from external context information providers; and (iv) DSA Database to store dynamic and static contexts and the regulator&rsquo;s policy information. The closed-loop feedback system of ASCENT is implemented by integrating these components in a modular software architecture. A case study of sharing the lower 12 GHz Ku band (12.2-12.7 GHz) with the 5G terrestrial cellular network is considered, and the usability of ASCENT is demonstrated by dynamically changing exclusion zone&rsquo;s radius in different weather conditions.</p></p class="citation"></blockquote><h3 id=204236-reconfigurable-intelligent-surface-for-industrial-automation-mmwave-propagation-measurement-simulation-and-control-algorithm-requirements-hamed-radpour-et-al-2024>(204/236) Reconfigurable Intelligent Surface for Industrial Automation: mmWave Propagation Measurement, Simulation, and Control Algorithm Requirements (Hamed Radpour et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hamed Radpour, Markus Hofer, David Loschenbrand, Lukas Walter Mayer, Andreas Hofmann, Martin Schiefer, Thomas Zemen. (2024)<br><strong>Reconfigurable Intelligent Surface for Industrial Automation: mmWave Propagation Measurement, Simulation, and Control Algorithm Requirements</strong><br><button class=copy-to-clipboard title="Reconfigurable Intelligent Surface for Industrial Automation: mmWave Propagation Measurement, Simulation, and Control Algorithm Requirements" index=204>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-204 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04844v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04844v2.pdf filename=2402.04844v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Reconfigurable intelligent surfaces (RISs) enable reliable low-latency millimeter wave (mmWave) communication links in cases of a blocked line-of-sight (LoS) between the base station (BS) and the user equipment (UE), i.e. a RIS mounted on a wall or the ceiling provides a bypass for the radio communication link. We present an active RIS with 127 patch antenna elements arranged in a hexagonal grid for a center frequency of 23.8 GHz. Each RIS element uses an orthogonal polarization transformation to enable amplification using a field-effect transistor (FET). The source and drain voltages of each FET is controlled using two bits. We assume that the coordinates of the UE in an industrial control scenario are known to the RIS. We measure the received power on a 2D grid of 60 cm by 100 cm with the RIS working in reflective and active mode. The results show that the RIS can successfully focus the radio signal at the desired target points. The half-power beam width is characterized in axial and radial directions with respect to the RIS position, obtaining a practical RIS configuration update criterion for a mobile UE. These results clearly show that RISs are prominent solutions for enabling reliable wireless communication in indoor industrial scenarios.</p></p class="citation"></blockquote><h3 id=205236-adaptive-smooth-control-via-nonsingular-fast-terminal-sliding-mode-for-distributed-space-telescope-demonstration-mission-by-cubesat-formation-flying-soobin-jeon-et-al-2024>(205/236) Adaptive Smooth Control via Nonsingular Fast Terminal Sliding Mode for Distributed Space Telescope Demonstration Mission by CubeSat Formation Flying (Soobin Jeon et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Soobin Jeon, Hancheol Cho, Sang-Young Park. (2024)<br><strong>Adaptive Smooth Control via Nonsingular Fast Terminal Sliding Mode for Distributed Space Telescope Demonstration Mission by CubeSat Formation Flying</strong><br><button class=copy-to-clipboard title="Adaptive Smooth Control via Nonsingular Fast Terminal Sliding Mode for Distributed Space Telescope Demonstration Mission by CubeSat Formation Flying" index=205>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-205 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04718v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04718v1.pdf filename=2402.04718v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper investigates the efficiency of nonsingular fast terminal sliding mode and adaptive smooth control method for the distributed space telescope demonstration mission. The distributed space telescope has a flexible focal length that corresponds to the relative position of the formation flying concept. The precise formation flying technology by CubeSats enhances the utility of distributed space systems with low costs. The propulsion systems for CubeSats usually have restricted degrees of freedom. Since the scientific mission requires continuous orbit control, the attitude and orbit control system mutually affect the control performance. The nonsingular fast terminal sliding mode has the advantage of a fast convergence rate and is able to improve the control performance. The adaptive smooth controller designed for the SISO system is expanded and applied to the attitude and orbit control system. The <b>simulation</b> results verify the efficiency of the adaptive smooth controller based on the nonsingular fast terminal sliding mode.</p></p class="citation"></blockquote><h2 id=mathna-2>math.NA (2)</h2><h3 id=206236-early-stopping-of-untrained-convolutional-neural-networks-tim-jahn-et-al-2024>(206/236) Early Stopping of Untrained Convolutional Neural Networks (Tim Jahn et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tim Jahn, Bangti Jin. (2024)<br><strong>Early Stopping of Untrained Convolutional Neural Networks</strong><br><button class=copy-to-clipboard title="Early Stopping of Untrained Convolutional Neural Networks" index=206>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-206 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: cs-NA, math-NA, math.NA<br>Keyword Score: 30<br>Keywords: Convolution, Convolutional Neural Network, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04610v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04610v1.pdf filename=2402.04610v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In recent years, new regularization methods based on (deep) neural networks have shown very promising empirical performance for the numerical solution of ill-posed problems, such as in medical imaging and imaging science. Due to the nonlinearity of neural networks, these methods often lack satisfactory theoretical justification. In this work, we rigorously discuss the convergence of a successful <b>unsupervised</b> approach that utilizes untrained <b>convolutional</b> <b>neural</b> <b>networks</b> to represent solutions to linear ill-posed problems. Untrained neural networks have particular appeal for many applications because they do not require paired training data. The regularization property of the approach relies solely on the architecture of the neural network instead. Due to the vast over-parameterization of the employed neural network, suitable early stopping is essential for the success of the method. We establish that the classical discrepancy principle is an adequate method for early stopping of two-layer untrained <b>convolutional</b> <b>neural</b> <b>networks</b> learned by gradient descent, and furthermore, it yields an approximation with minimax optimal convergence rates. Numerical results are also presented to illustrate the theoretical findings.</p></p class="citation"></blockquote><h3 id=207236-an-efficient-unconditional-energy-stable-scheme-for-the-simulation-of-droplet-formation-jinpeng-zhang-et-al-2024>(207/236) An efficient unconditional energy stable scheme for the simulation of droplet formation (Jinpeng Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jinpeng Zhang, Changjuan Zhang, Xiaoping Wang. (2024)<br><strong>An efficient unconditional energy stable scheme for the simulation of droplet formation</strong><br><button class=copy-to-clipboard title="An efficient unconditional energy stable scheme for the simulation of droplet formation" index=207>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-207 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: cs-NA, math-NA, math.NA, physics-flu-dyn<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04638v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04638v1.pdf filename=2402.04638v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We have developed an efficient and unconditionally energy-stable method for simulating droplet formation dynamics. Our approach involves a novel time-marching scheme based on the scalar auxiliary variable technique, specifically designed for solving the Cahn-Hilliard-Navier-Stokes phase field model with variable density and viscosity. We have successfully applied this method to simulate droplet formation in scenarios where a Newtonian fluid is injected through a vertical tube into another immiscible Newtonian fluid. To tackle the challenges posed by nonhomogeneous Dirichlet boundary conditions at the tube entrance, we have introduced additional nonlocal auxiliary variables and associated ordinary differential equations. These additions effectively eliminate the influence of boundary terms. Moreover, we have incorporated stabilization terms into the scheme to enhance its numerical effectiveness. Notably, our resulting scheme is fully decoupled, requiring the solution of only linear systems at each time step. We have also demonstrated the energy decaying property of the scheme, with suitable modifications. To assess the accuracy and stability of our algorithm, we have conducted extensive numerical <b>simulations.</b> Additionally, we have examined the dynamics of droplet formation and explored the impact of dimensionless parameters on the process. Overall, our work presents a refined method for simulating droplet formation dynamics, offering improved efficiency, energy stability, and accuracy.</p></p class="citation"></blockquote><h2 id=physicschem-ph-1>physics.chem-ph (1)</h2><h3 id=208236-an-artificial-intelligence-ai-workflow-for-catalyst-design-and-optimization-nung-siong-lai-et-al-2024>(208/236) An Artificial Intelligence (AI) workflow for catalyst design and optimization (Nung Siong Lai et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nung Siong Lai, Yi Shen Tew, Xialin Zhong, Jun Yin, Jiali Li, Binhang Yan, Xiaonan Wang. (2024)<br><strong>An Artificial Intelligence (AI) workflow for catalyst design and optimization</strong><br><button class=copy-to-clipboard title="An Artificial Intelligence (AI) workflow for catalyst design and optimization" index=208>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-208 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: physics.chem-ph<br>Categories: cs-LG, physics-chem-ph, physics.chem-ph<br>Keyword Score: 30<br>Keywords: Active Learning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04557v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04557v1.pdf filename=2402.04557v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the pursuit of novel catalyst development to address pressing environmental concerns and energy demand, conventional design and optimization methods often fall short due to the complexity and vastness of the catalyst parameter space. The advent of Machine Learning (ML) has ushered in a new era in the field of catalyst optimization, offering potential solutions to the shortcomings of traditional techniques. However, existing methods fail to effectively harness the wealth of information contained within the burgeoning body of scientific literature on catalyst synthesis. To address this gap, this study proposes an innovative Artificial Intelligence (AI) workflow that integrates <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs),</b> Bayesian optimization, and an <b>active</b> <b>learning</b> loop to expedite and enhance catalyst optimization. Our methodology combines advanced language understanding with robust optimization strategies, effectively translating knowledge extracted from diverse literature into actionable parameters for practical experimentation and optimization. In this article, we demonstrate the application of this AI workflow in the optimization of catalyst synthesis for ammonia production. The results underscore the workflow&rsquo;s ability to streamline the catalyst development process, offering a swift, resource-efficient, and high-precision alternative to conventional methods.</p></p class="citation"></blockquote><h2 id=mathst-1>math.ST (1)</h2><h3 id=209236-online-quantile-regression-yinan-shen-et-al-2024>(209/236) Online Quantile Regression (Yinan Shen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yinan Shen, Dong Xia, Wen-Xin Zhou. (2024)<br><strong>Online Quantile Regression</strong><br><button class=copy-to-clipboard title="Online Quantile Regression" index=209>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-209 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.ST<br>Categories: cs-IT, math-IT, math-ST, math.ST, stat-ME, stat-TH<br>Keyword Score: 25<br>Keywords: Simulation, Simulator, Square Loss<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04602v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04602v1.pdf filename=2402.04602v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper tackles the challenge of integrating sequentially arriving data within the quantile regression framework, where the number of covariates is allowed to grow with the number of observations, the horizon is unknown, and memory is limited. We employ stochastic sub-gradient descent to minimize the empirical check loss and study its statistical properties and regret performance. In our analysis, we unveil the delicate interplay between updating iterates based on individual observations versus batches of observations, revealing distinct regularity properties in each scenario. Our method ensures long-term optimal estimation irrespective of the chosen update strategy. Importantly, our contributions go beyond prior works by achieving exponential-type concentration inequalities and attaining optimal regret and error rates that exhibit only short-term sensitivity to initial errors. A key insight from our study is the delicate statistical analyses and the revelation that appropriate stepsize schemes significantly mitigate the impact of initial errors on subsequent errors and regrets. This underscores the robustness of stochastic sub-gradient descent in handling initial uncertainties, emphasizing its efficacy in scenarios where the sequential arrival of data introduces uncertainties regarding both the horizon and the total number of observations. Additionally, when the initial error rate is well controlled, there is a trade-off between short-term error rate and long-term optimality. Due to the lack of delicate statistical analysis for <b>square</b> <b>loss,</b> we also briefly discuss its properties and proper schemes. Extensive <b>simulations</b> support our theoretical findings.</p></p class="citation"></blockquote><h2 id=statml-7>stat.ML (7)</h2><h3 id=210236-gradient-descent-induces-alignment-between-weights-and-the-empirical-ntk-for-deep-non-linear-networks-daniel-beaglehole-et-al-2024>(210/236) Gradient descent induces alignment between weights and the empirical NTK for deep non-linear networks (Daniel Beaglehole et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Daniel Beaglehole, Ioannis Mitliagkas, Atish Agarwala. (2024)<br><strong>Gradient descent induces alignment between weights and the empirical NTK for deep non-linear networks</strong><br><button class=copy-to-clipboard title="Gradient descent induces alignment between weights and the empirical NTK for deep non-linear networks" index=210>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-210 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-AI, cs-LG, stat-ML, stat.ML<br>Keyword Score: 20<br>Keywords: Supervised Learning, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05271v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05271v1.pdf filename=2402.05271v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Understanding the mechanisms through which neural networks extract statistics from input-label pairs is one of the most important unsolved problems in <b>supervised</b> <b>learning.</b> Prior works have identified that the gram matrices of the weights in trained neural networks of general architectures are proportional to the average gradient outer product of the model, in a statement known as the Neural Feature Ansatz (NFA). However, the reason these quantities become correlated during training is poorly understood. In this work, we explain the emergence of this correlation. We identify that the NFA is equivalent to alignment between the left singular structure of the weight matrices and a significant component of the empirical neural tangent kernels associated with those weights. We establish that the NFA introduced in prior works is driven by a centered NFA that isolates this alignment. We show that the speed of NFA development can be predicted analytically at early training times in terms of simple statistics of the inputs and labels. Finally, we introduce a simple intervention to increase NFA correlation at any given layer, which dramatically improves the quality of features learned.</p></p class="citation"></blockquote><h3 id=211236-meta-learning-the-mirror-map-in-policy-mirror-descent-carlo-alfano-et-al-2024>(211/236) Meta-learning the mirror map in policy mirror descent (Carlo Alfano et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Carlo Alfano, Sebastian Towers, Silvia Sapora, Chris Lu, Patrick Rebeschini. (2024)<br><strong>Meta-learning the mirror map in policy mirror descent</strong><br><button class=copy-to-clipboard title="Meta-learning the mirror map in policy mirror descent" index=211>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-211 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-LG, math-OC, stat-ML, stat.ML<br>Keyword Score: 20<br>Keywords: Meta Learning, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05187v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05187v1.pdf filename=2402.05187v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Policy Mirror Descent (PMD) is a popular framework in <b>reinforcement</b> <b>learning,</b> serving as a unifying perspective that encompasses numerous algorithms. These algorithms are derived through the selection of a mirror map and enjoy finite-time convergence guarantees. Despite its popularity, the exploration of PMD&rsquo;s full potential is limited, with the majority of research focusing on a particular mirror map &ndash; namely, the negative entropy &ndash; which gives rise to the renowned Natural Policy Gradient (NPG) method. It remains uncertain from existing theoretical studies whether the choice of mirror map significantly influences PMD&rsquo;s efficacy. In our work, we conduct empirical investigations to show that the conventional mirror map choice (NPG) often yields less-than-optimal outcomes across several standard benchmark environments. By applying a <b>meta-learning</b> <b>approach,</b> we identify more efficient mirror maps that enhance performance, both on average and in terms of best performance achieved along the training trajectory. We analyze the characteristics of these learned mirror maps and reveal shared traits among certain settings. Our results suggest that mirror maps have the potential to be adaptable across various environments, raising questions about how to best match a mirror map to an environment&rsquo;s structure and characteristics.</p></p class="citation"></blockquote><h3 id=212236-learning-operators-with-stochastic-gradient-descent-in-general-hilbert-spaces-lei-shi-et-al-2024>(212/236) Learning Operators with Stochastic Gradient Descent in General Hilbert Spaces (Lei Shi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lei Shi, Jia-Qi Yang. (2024)<br><strong>Learning Operators with Stochastic Gradient Descent in General Hilbert Spaces</strong><br><button class=copy-to-clipboard title="Learning Operators with Stochastic Gradient Descent in General Hilbert Spaces" index=212>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-212 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-LG, math-FA, math-ST, stat-ML, stat-TH, stat.ML<br>Keyword Score: 20<br>Keywords: Stochastic Gradient Descent, Stochastic Gradient Descent<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04691v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04691v2.pdf filename=2402.04691v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This study investigates leveraging <b>stochastic</b> <b>gradient</b> <b>descent</b> <b>(SGD)</b> to learn operators between general Hilbert spaces. We propose weak and strong regularity conditions for the target operator to depict its intrinsic structure and complexity. Under these conditions, we establish upper bounds for convergence rates of the <b>SGD</b> algorithm and conduct a minimax lower bound analysis, further illustrating that our convergence analysis and regularity conditions quantitatively characterize the tractability of solving operator learning problems using the <b>SGD</b> algorithm. It is crucial to highlight that our convergence analysis is still valid for nonlinear operator learning. We show that the <b>SGD</b> estimator will converge to the best linear approximation of the nonlinear target operator. Moreover, applying our analysis to operator learning problems based on vector-valued and real-valued reproducing kernel Hilbert spaces yields new convergence results, thereby refining the conclusions of existing literature.</p></p class="citation"></blockquote><h3 id=213236-riemann-lebesgue-forest-for-regression-tian-qin-et-al-2024>(213/236) Riemann-Lebesgue Forest for Regression (Tian Qin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tian Qin, Wei-Min Huang. (2024)<br><strong>Riemann-Lebesgue Forest for Regression</strong><br><button class=copy-to-clipboard title="Riemann-Lebesgue Forest for Regression" index=213>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-213 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-LG, stat-ML, stat.ML<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04550v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04550v1.pdf filename=2402.04550v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We propose a novel ensemble method called Riemann-Lebesgue Forest (RLF) for regression. The core idea of RLF is to mimic the way how a measurable function can be approximated by partitioning its range into a few intervals. With this idea in mind, we develop a new tree learner named Riemann-Lebesgue Tree which has a chance to split the node from response $Y$ or a direction in feature space $\mathbf{X}$ at each non-terminal node. We generalize the asymptotic performance of RLF under different parameter settings mainly through Hoeffding decomposition \cite{Vaart} and Stein&rsquo;s method \cite{Chen2010NormalAB}. When the underlying function $Y=f(\mathbf{X})$ follows an additive regression model, RLF is consistent with the argument from \cite{Scornet2014ConsistencyOR}. The competitive performance of RLF against original random forest \cite{Breiman2001RandomF} is demonstrated by experiments in <b>simulation</b> data and real world datasets.</p></p class="citation"></blockquote><h3 id=214236-a-primal-dual-algorithm-for-offline-constrained-reinforcement-learning-with-low-rank-mdps-kihyuk-hong-et-al-2024>(214/236) A Primal-Dual Algorithm for Offline Constrained Reinforcement Learning with Low-Rank MDPs (Kihyuk Hong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kihyuk Hong, Ambuj Tewari. (2024)<br><strong>A Primal-Dual Algorithm for Offline Constrained Reinforcement Learning with Low-Rank MDPs</strong><br><button class=copy-to-clipboard title="A Primal-Dual Algorithm for Offline Constrained Reinforcement Learning with Low-Rank MDPs" index=214>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-214 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-LG, stat-ML, stat.ML<br>Keyword Score: 20<br>Keywords: Offline Reinforcement Learning, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04493v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04493v1.pdf filename=2402.04493v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Offline</b> <b>reinforcement</b> <b>learning</b> (RL) aims to learn a policy that maximizes the expected cumulative reward using a pre-collected dataset. <b>Offline</b> <b>RL</b> <b>with</b> low-rank MDPs or general function approximation has been widely studied recently, but existing algorithms with sample complexity $O(\epsilon^{-2})$ for finding an $\epsilon$-optimal policy either require a uniform data coverage assumptions or are computationally inefficient. In this paper, we propose a primal dual algorithm for <b>offline</b> <b>RL</b> <b>with</b> low-rank MDPs in the discounted infinite-horizon setting. Our algorithm is the first computationally efficient algorithm in this setting that achieves sample complexity of $O(\epsilon^{-2})$ with partial data coverage assumption. This improves upon a recent work that requires $O(\epsilon^{-4})$ samples. Moreover, our algorithm extends the previous work to the <b>offline</b> <b>constrained</b> <b>RL</b> setting by supporting constraints on additional reward signals.</p></p class="citation"></blockquote><h3 id=215236-generalized-sobolev-transport-for-probability-measures-on-a-graph-tam-le-et-al-2024>(215/236) Generalized Sobolev Transport for Probability Measures on a Graph (Tam Le et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tam Le, Truyen Nguyen, Kenji Fukumizu. (2024)<br><strong>Generalized Sobolev Transport for Probability Measures on a Graph</strong><br><button class=copy-to-clipboard title="Generalized Sobolev Transport for Probability Measures on a Graph" index=215>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-215 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-LG, stat-ML, stat.ML<br>Keyword Score: 10<br>Keywords: Document Classification<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04516v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04516v1.pdf filename=2402.04516v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We study the optimal transport (OT) problem for measures supported on a graph metric space. Recently, Le et al. (2022) leverage the graph structure and propose a variant of OT, namely Sobolev transport (ST), which yields a closed-form expression for a fast computation. However, ST is essentially coupled with the $L^p$ geometric structure within its definition which makes it nontrivial to utilize ST for other prior structures. In contrast, the classic OT has the flexibility to adapt to various geometric structures by modifying the underlying cost function. An important instance is the Orlicz-Wasserstein (OW) which moves beyond the $L^p$ structure by leveraging the \emph{Orlicz geometric structure}. Comparing to the usage of standard $p$-order Wasserstein, OW remarkably helps to advance certain machine learning approaches. Nevertheless, OW brings up a new challenge on its computation due to its two-level optimization formulation. In this work, we leverage a specific class of convex functions for Orlicz structure to propose the generalized Sobolev transport (GST). GST encompasses the ST as its special case, and can be utilized for prior structures beyond the $L^p$ geometry. In connection with the OW, we show that one only needs to simply solve a univariate optimization problem to compute the GST, unlike the complex two-level optimization problem in OW. We empirically illustrate that GST is several-order faster than the OW. Moreover, we provide preliminary evidences on the advantages of GST for <b>document</b> <b>classification</b> and for several tasks in topological data analysis.</p></p class="citation"></blockquote><h3 id=216236-generative-flows-on-discrete-state-spaces-enabling-multimodal-flows-with-applications-to-protein-co-design-andrew-campbell-et-al-2024>(216/236) Generative Flows on Discrete State-Spaces: Enabling Multimodal Flows with Applications to Protein Co-Design (Andrew Campbell et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Andrew Campbell, Jason Yim, Regina Barzilay, Tom Rainforth, Tommi Jaakkola. (2024)<br><strong>Generative Flows on Discrete State-Spaces: Enabling Multimodal Flows with Applications to Protein Co-Design</strong><br><button class=copy-to-clipboard title="Generative Flows on Discrete State-Spaces: Enabling Multimodal Flows with Applications to Protein Co-Design" index=216>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-216 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-LG, q-bio-QM, stat-ML, stat.ML<br>Keyword Score: 6<br>Keywords: Multi-modal, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04997v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04997v1.pdf filename=2402.04997v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Combining discrete and continuous data is an important capability for generative models. We present Discrete Flow Models (DFMs), a new flow-based model of discrete data that provides the missing link in enabling flow-based generative models to be applied to <b>multimodal</b> continuous and discrete data problems. Our key insight is that the discrete equivalent of continuous space flow matching can be realized using Continuous Time Markov Chains. DFMs benefit from a simple derivation that includes discrete diffusion models as a specific instance while allowing improved performance over existing diffusion-based approaches. We utilize our DFMs method to build a <b>multimodal</b> flow-based modeling framework. We apply this capability to the task of protein co-design, wherein we learn a model for jointly generating protein structure and sequence. Our approach achieves state-of-the-art co-design performance while allowing the same <b>multimodal</b> model to be used for flexible generation of the sequence or structure.</p></p class="citation"></blockquote><h2 id=csdc-2>cs.DC (2)</h2><h3 id=217236-criu----checkpoint-restore-in-userspace-for-computational-simulations-and-scientific-applications-fabio-andrijauskas-et-al-2024>(217/236) CRIU &ndash; Checkpoint Restore in Userspace for computational simulations and scientific applications (Fabio Andrijauskas et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fabio Andrijauskas, Igor Sfiligoi, Diego Davila, Aashay Arora, Jonathan Guiang, Brian Bockelman, Greg Thain, Frank Wurthwein. (2024)<br><strong>CRIU &ndash; Checkpoint Restore in Userspace for computational simulations and scientific applications</strong><br><button class=copy-to-clipboard title="CRIU -- Checkpoint Restore in Userspace for computational simulations and scientific applications" index=217>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-217 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DC<br>Categories: cs-DC, cs.DC<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05244v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05244v1.pdf filename=2402.05244v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Creating new materials, discovering new drugs, and simulating systems are essential processes for research and innovation and require substantial computational power. While many applications can be split into many smaller independent tasks, some cannot and may take hours or weeks to run to completion. To better manage those longer-running jobs, it would be desirable to stop them at any arbitrary point in time and later continue their computation on another compute resource; this is usually referred to as checkpointing. While some applications can manage checkpointing programmatically, it would be preferable if the batch scheduling system could do that independently. This paper evaluates the feasibility of using CRIU (Checkpoint Restore in Userspace), an open-source tool for the GNU/Linux environments, emphasizing the OSG&rsquo;s OSPool HTCondor setup. CRIU allows checkpointing the process state into a disk image and can deal with both open files and established network connections seamlessly. Furthermore, it can checkpoint traditional Linux processes and containerized workloads. The functionality seems adequate for many scenarios supported in the OSPool. However, some limitations prevent it from being usable in all circumstances.</p></p class="citation"></blockquote><h3 id=218236-leveraging-knowledge-as-a-service-kaas-for-qos-aware-resource-management-in-multi-user-video-transcoding-luis-costero-et-al-2024>(218/236) Leveraging knowledge-as-a-service (KaaS) for QoS-aware resource management in multi-user video transcoding (Luis Costero et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Luis Costero, Francisco D. Igual, Katzalin Olcoz, Francisco Tirado. (2024)<br><strong>Leveraging knowledge-as-a-service (KaaS) for QoS-aware resource management in multi-user video transcoding</strong><br><button class=copy-to-clipboard title="Leveraging knowledge-as-a-service (KaaS) for QoS-aware resource management in multi-user video transcoding" index=218>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-218 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DC<br>Categories: cs-DC, cs.DC<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04891v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04891v1.pdf filename=2402.04891v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The coexistence of parallel applications in shared computing nodes, each one featuring different Quality of Service (QoS) requirements, carries out new challenges to improve resource occupation while keeping acceptable rates in terms of QoS. As more application-specific and system-wide metrics are included as QoS dimensions, or under situations in which resource-usage limits are strict, building and serving the most appropriate set of actions (application control knobs and system resource assignment) to concurrent applications in an automatic and optimal fashion becomes mandatory. In this paper, we propose strategies to build and serve this type of knowledge to concurrent applications by leveraging <b>Reinforcement</b> <b>Learning</b> techniques. Taking multi-user video transcoding as a driving example, our experimental results reveal an excellent adaptation of resource and knob management to heterogeneous QoS requests, and increases in the amount of concurrently served users up to 1.24x compared with alternative approaches considering homogeneous QoS requests.</p></p class="citation"></blockquote><h2 id=statme-2>stat.ME (2)</h2><h3 id=219236-stochastic-modeling-of-random-access-memories-reset-transitions-m-carmen-aguilera-morillo-et-al-2024>(219/236) Stochastic modeling of Random Access Memories reset transitions (M Carmen Aguilera-Morillo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>M Carmen Aguilera-Morillo, Ana M Aguilera, Francisco Jiménez-Molinos, Juan B Roldán. (2024)<br><strong>Stochastic modeling of Random Access Memories reset transitions</strong><br><button class=copy-to-clipboard title="Stochastic modeling of Random Access Memories reset transitions" index=219>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-219 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ME<br>Categories: cs-ET, math-ST, stat-ME, stat-TH, stat.ME<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05209v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05209v1.pdf filename=2402.05209v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Resistive Random Access Memories (RRAMs) are being studied by the industry and academia because it is widely accepted that they are promising candidates for the next generation of high density nonvolatile memories. Taking into account the stochastic nature of mechanisms behind resistive switching, a new technique based on the use of functional data analysis has been developed to accurately model resistive memory device characteristics. Functional principal component analysis (FPCA) based on Karhunen-Loeve expansion is applied to obtain an orthogonal decomposition of the reset process in terms of uncorrelated scalar random variables. Then, the device current has been accurately described making use of just one variable presenting a modeling approach that can be very attractive from the circuit <b>simulation</b> viewpoint. The new method allows a comprehensive description of the stochastic variability of these devices by introducing a probability distribution that allows the <b>simulation</b> of the main parameter that is employed for the model implementation. A rigorous description of the mathematical theory behind the technique is given and its application for a broad set of experimental measurements is explained.</p></p class="citation"></blockquote><h3 id=220236-data-driven-bayesian-estimation-of-monod-kinetics-kévin-colin-et-al-2024>(220/236) Data-driven Bayesian estimation of Monod kinetics (Kévin Colin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kévin Colin, Håkan Hjalmarsson, Véronique Chotteau. (2024)<br><strong>Data-driven Bayesian estimation of Monod kinetics</strong><br><button class=copy-to-clipboard title="Data-driven Bayesian estimation of Monod kinetics" index=220>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-220 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ME<br>Categories: cs-SY, eess-SY, stat-ME, stat.ME<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04727v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04727v1.pdf filename=2402.04727v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we consider the well known problem of non-linear identification of the rates of the reactions involved in cells with Monod functions. In bioprocesses, generating data is very expensive and long and so it is important to incorporate prior knowledge on the Monod kinetic parameters. Bayesian estimation is an elegant estimation technique which deals with parameter estimation with prior knowledge modeled as probability density functions. However, we might not have an accurate knowledge of the kinetic parameters such as interval bounds, especially for newly developed cell lines. Hence, we consider the case when there is no accurate prior information on the kinetic parameters except qualitative knowledge such that their non-negativity. A log-Gaussian prior distribution is considered for the parameters and the mean and variances of these distribution are tuned using the Expectation Maximization algorithm. The algorithm requires to use Metropolis Hastings within Gibbs sampling which can be computationally expensive. We develop a novel variant of the Metropolis-Hastings within Gibbs sampling sampling scheme in order to accelerate and improve on the hyperparameter tuning. We show that it can give better modeling performances on a relatively large-scale <b>simulation</b> example compared to available methods in the literature.</p></p class="citation"></blockquote><h2 id=physicsflu-dyn-2>physics.flu-dyn (2)</h2><h3 id=221236-jax-fluids-20-towards-hpc-for-differentiable-cfd-of-compressible-two-phase-flows-deniz-a-bezgin-et-al-2024>(221/236) JAX-Fluids 2.0: Towards HPC for Differentiable CFD of Compressible Two-phase Flows (Deniz A. Bezgin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Deniz A. Bezgin, Aaron B. Buhendwa, Nikolaus A. Adams. (2024)<br><strong>JAX-Fluids 2.0: Towards HPC for Differentiable CFD of Compressible Two-phase Flows</strong><br><button class=copy-to-clipboard title="JAX-Fluids 2.0: Towards HPC for Differentiable CFD of Compressible Two-phase Flows" index=221>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-221 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: physics.flu-dyn<br>Categories: cs-CE, cs-LG, physics-flu-dyn, physics.flu-dyn<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05193v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05193v1.pdf filename=2402.05193v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In our effort to facilitate machine learning-assisted computational fluid dynamics (CFD), we introduce the second iteration of JAX-Fluids. JAX-Fluids is a Python-based fully-differentiable CFD solver designed for compressible single- and two-phase flows. In this work, the first version is extended to incorporate high-performance computing (HPC) capabilities. We introduce a parallelization strategy utilizing JAX primitive operations that scales efficiently on GPU (up to 512 NVIDIA A100 graphics cards) and TPU (up to 1024 TPU v3 cores) HPC systems. We further demonstrate the stable parallel computation of automatic differentiation gradients across extended integration trajectories. The new code version offers enhanced two-phase flow modeling capabilities. In particular, a five-equation diffuse-interface model is incorporated which complements the level-set sharp-interface model. Additional algorithmic improvements include positivity-preserving limiters for increased robustness, support for stretched Cartesian meshes, refactored I/O handling, comprehensive post-processing routines, and an updated list of state-of-the-art high-order numerical discretization schemes. We verify newly added numerical models by showcasing <b>simulation</b> results for single- and two-phase flows, including turbulent boundary layer and channel flows, air-helium shock bubble interactions, and air-water shock drop interactions.</p></p class="citation"></blockquote><h3 id=222236-multiscale-modelling-with-physics-informed-neural-network-from-large-scale-dynamics-to-small-scale-predictions-in-complex-systems-jing-wang-et-al-2024>(222/236) Multiscale Modelling with Physics-informed Neural Network: from Large-scale Dynamics to Small-scale Predictions in Complex Systems (Jing Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jing Wang, Zheng Li, Pengyu Lai, Rui Wang, Di Yang, Dewu Yang, Hui Xu. (2024)<br><strong>Multiscale Modelling with Physics-informed Neural Network: from Large-scale Dynamics to Small-scale Predictions in Complex Systems</strong><br><button class=copy-to-clipboard title="Multiscale Modelling with Physics-informed Neural Network: from Large-scale Dynamics to Small-scale Predictions in Complex Systems" index=222>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-222 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: physics.flu-dyn<br>Categories: cs-LG, physics-comp-ph, physics-flu-dyn, physics.flu-dyn<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05067v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05067v2.pdf filename=2402.05067v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Multiscale phenomena manifest across various scientific domains, presenting a ubiquitous challenge in accurately and effectively predicting multiscale dynamics in complex systems. In this paper, a novel decoupling solving mode is proposed through modelling large-scale dynamics independently and treating small-scale dynamics as a slaved system. A Spectral Physics-informed Neural Network (PINN) is developed to characterize the small-scale system in an efficient and accurate way. The effectiveness of the method is demonstrated through extensive numerical experiments, including one-dimensional Kuramot-Sivashinsky equation, two- and three-dimensional Navier-Stokes equations, showcasing its versatility in addressing problems of fluid dynamics. Furthermore, we also delve into the application of the proposed approach to more complex problems, including non-uniform meshes, complex geometries, large-scale data with noise, and high-dimensional small-scale dynamics. The discussions about these scenarios contribute to a comprehensive understanding of the method&rsquo;s capabilities and limitations. This paper presents a valuable and promising approach to enhance the computational <b>simulations</b> of multiscale spatiotemporal systems, which enables the acquisition of large-scale data with minimal computational demands, followed by Spectral PINN to capture small-scale dynamics with improved efficiency and accuracy.</p></p class="citation"></blockquote><h2 id=cscr-4>cs.CR (4)</h2><h3 id=223236-threats-and-limitations-of-terrestrial-broadcast-attacks-benjamin-michele-et-al-2024>(223/236) Threats and Limitations of Terrestrial Broadcast Attacks (Benjamin Michele et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Benjamin Michele, Ivan Pena, Pablo Angueira. (2024)<br><strong>Threats and Limitations of Terrestrial Broadcast Attacks</strong><br><button class=copy-to-clipboard title="Threats and Limitations of Terrestrial Broadcast Attacks" index=223>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-223 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs.CR, eess-SP<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05159v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05159v1.pdf filename=2402.05159v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The DVB standard does not mandate the use of authentication and integrity protection for transport streams. This allows malicious third parties to replace legitimate broadcasts by overpowering terrestrial transmissions. The rogue signal can then deliver a malicious broadcast stream to exploit security vulnerabilities on Smart TVs (STVs) in range. We implemented a proof-of-concept attack based on a malicious Hybrid Broadcast Broadband TV app, able to acquire permanent system-level access to an STV over the air, in less than 10 s. These attacks, however, are severely limited in range due to required co-channel protection ratios (CCPRs), which is in direct contradiction to previous publications. We present evidence for these limitations in form of laboratory experiments, extensive <b>simulations,</b> and field measurements. To this end, we developed an automated, low-cost method for CCPR determination, as well as a method for non-disruptive attack range measurements based on a gap filler and the resulting channel impulse response.</p></p class="citation"></blockquote><h3 id=224236-understanding-practical-membership-privacy-of-deep-learning-marlon-tobaben-et-al-2024>(224/236) Understanding Practical Membership Privacy of Deep Learning (Marlon Tobaben et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Marlon Tobaben, Gauri Pradhan, Yuan He, Joonas Jälkö, Antti Honkela. (2024)<br><strong>Understanding Practical Membership Privacy of Deep Learning</strong><br><button class=copy-to-clipboard title="Understanding Practical Membership Privacy of Deep Learning" index=224>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-224 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs-LG, cs.CR<br>Keyword Score: 10<br>Keywords: Fine-tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.06674v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.06674v1.pdf filename=2402.06674v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We apply a state-of-the-art membership inference attack (MIA) to systematically test the practical privacy vulnerability of <b>fine-tuning</b> large image classification models.We focus on understanding the properties of data sets and samples that make them vulnerable to membership inference. In terms of data set properties, we find a strong power law dependence between the number of examples per class in the data and the MIA vulnerability, as measured by true positive rate of the attack at a low false positive rate. For an individual sample, large gradients at the end of training are strongly correlated with MIA vulnerability.</p></p class="citation"></blockquote><h3 id=225236-adversarial-robustness-through-artifact-design-tsufit-shua-et-al-2024>(225/236) Adversarial Robustness Through Artifact Design (Tsufit Shua et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tsufit Shua, Mahmood Sharif. (2024)<br><strong>Adversarial Robustness Through Artifact Design</strong><br><button class=copy-to-clipboard title="Adversarial Robustness Through Artifact Design" index=225>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-225 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-AI, cs-CR, cs-CV, cs-LG, cs.CR<br>Keyword Score: 10<br>Keywords: Adversarial Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04660v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04660v1.pdf filename=2402.04660v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Adversarial</b> <b>examples</b> arose as a challenge for machine learning. To hinder them, most defenses alter how models are trained (e.g., <b>adversarial</b> <b>training)</b> or inference is made (e.g., randomized smoothing). Still, while these approaches markedly improve models&rsquo; <b>adversarial</b> <b>robustness,</b> models remain highly susceptible to <b>adversarial</b> <b>examples.</b> Identifying that, in certain domains such as traffic-sign recognition, objects are implemented per standards specifying how artifacts (e.g., signs) should be designed, we propose a novel approach for improving <b>adversarial</b> <b>robustness.</b> Specifically, we offer a method to redefine standards, making minor changes to existing ones, to defend against <b>adversarial</b> <b>examples.</b> We formulate the problem of artifact design as a robust optimization problem, and propose gradient-based and greedy search methods to solve it. We evaluated our approach in the domain of traffic-sign recognition, allowing it to alter traffic-sign pictograms (i.e., symbols within the signs) and their colors. We found that, combined with <b>adversarial</b> <b>training,</b> our approach led to up to 25.18% higher robust accuracy compared to state-of-the-art methods against two adversary types, while further increasing accuracy on benign inputs.</p></p class="citation"></blockquote><h3 id=226236-ransomware-detection-dynamics-insights-and-implications-mike-nkongolo-2024>(226/236) Ransomware Detection Dynamics: Insights and Implications (Mike Nkongolo, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mike Nkongolo. (2024)<br><strong>Ransomware Detection Dynamics: Insights and Implications</strong><br><button class=copy-to-clipboard title="Ransomware Detection Dynamics: Insights and Implications" index=226>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-226 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs-CY, cs.CR<br>Keyword Score: 10<br>Keywords: Mutual Information<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04594v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04594v1.pdf filename=2402.04594v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The rise of ransomware attacks has necessitated the development of effective strategies for identifying and mitigating these threats. This research investigates the utilization of a feature selection algorithm for distinguishing ransomware-related and benign transactions in both Bitcoin (BTC) and United States Dollar (USD). Leveraging the UGRansome dataset, a comprehensive repository of ransomware related BTC and USD transactions, we propose a set of novel features designed to capture the distinct characteristics of ransomware activity within the cryptocurrency ecosystem. These features encompass transaction metadata, ransom analysis, and behavioral patterns, offering a multifaceted view of ransomware-related financial transactions. Through rigorous experimentation and evaluation, we demonstrate the effectiveness of our feature set in accurately extracting BTC and USD transactions, thereby aiding in the early detection and prevention of ransomware-related financial flows. We introduce a Ransomware Feature Selection Algorithm (RFSA) based on Gini Impurity and <b>Mutual</b> <b>Information</b> (MI) for selecting crucial ransomware features from the UGRansome dataset. Insights from the visualization highlight the potential of Gini Impurity and MI-based feature selection to enhance ransomware detection systems by effectively discriminating between ransomware classes. The analysis reveals that approximately 68% of ransomware incidents involve BTC transactions within the range of 1.46 to 2.56, with an average of 2.01 BTC transactions per attack. The findings emphasize the dynamic and adaptable nature of ransomware demands, suggesting that there is no fixed amount for specific cyberattacks, highlighting the evolving landscape of ransomware threats.</p></p class="citation"></blockquote><h2 id=csma-2>cs.MA (2)</h2><h3 id=227236-quantifying-population-exposure-to-long-term-pm10-a-city-wide-agent-based-assessment-hyesop-shin-2024>(227/236) Quantifying Population Exposure to Long-term PM10: A City-wide Agent-based Assessment (Hyesop Shin, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hyesop Shin. (2024)<br><strong>Quantifying Population Exposure to Long-term PM10: A City-wide Agent-based Assessment</strong><br><button class=copy-to-clipboard title="Quantifying Population Exposure to Long-term PM10: A City-wide Agent-based Assessment" index=227>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-227 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.MA<br>Categories: cs-MA, cs.MA<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05029v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05029v1.pdf filename=2402.05029v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This study evaluates the health effects of long-term exposure to PM10 in Seoul. Building on the preliminary model Shin and Bithell (2019), an in-silico agent-based model (ABM) is used to simulate the travel patterns of individuals according to their origins and destinations. During the <b>simulation,</b> each person, with their inherent socio-economic attributes and allocated origin and destination location, is assumed to commute to and from the same places for 10 consecutive years. A nominal measure of their health is set to decrease whenever the concentration of PM10 exceeds the national standard. Sensitivity analysis on calibrated parameters reveals increased vulnerability among certain demographic groups, particularly those aged over 65 and under 15, with a significant health decline associated with road proximity. The study reveals a substantial health disparity after 7,000 <b>simulation</b> ticks (equivalent to 10 years), especially under scenarios of a 3% annual increase in pollution levels. Long-term exposure to PM10 has a significant impact on health vulnerabilities, despite initial resilience being minimal. The study emphasises the importance of future research that takes into account different pollution thresholds as well as more detailed models of population dynamics and pollution generation in order to better understand and mitigate the health effects of air pollution on diverse urban populations.</p></p class="citation"></blockquote><h3 id=228236-towards-generalizability-of-multi-agent-reinforcement-learning-in-graphs-with-recurrent-message-passing-jannis-weil-et-al-2024>(228/236) Towards Generalizability of Multi-Agent Reinforcement Learning in Graphs with Recurrent Message Passing (Jannis Weil et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jannis Weil, Zhenghua Bao, Osama Abboud, Tobias Meuser. (2024)<br><strong>Towards Generalizability of Multi-Agent Reinforcement Learning in Graphs with Recurrent Message Passing</strong><br><button class=copy-to-clipboard title="Towards Generalizability of Multi-Agent Reinforcement Learning in Graphs with Recurrent Message Passing" index=228>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-228 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.MA<br>Categories: cs-AI, cs-MA, cs.MA<br>Keyword Score: 20<br>Keywords: Message-Passing, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05027v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05027v2.pdf filename=2402.05027v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Graph-based environments pose unique challenges to multi-agent <b>reinforcement</b> <b>learning.</b> In decentralized approaches, agents operate within a given graph and make decisions based on partial or outdated observations. The size of the observed neighborhood limits the generalizability to different graphs and affects the reactivity of agents, the quality of the selected actions, and the communication overhead. This work focuses on generalizability and resolves the trade-off in observed neighborhood size with a continuous information flow in the whole graph. We propose a recurrent <b>message-passing</b> model that iterates with the environment&rsquo;s steps and allows nodes to create a global representation of the graph by exchanging messages with their neighbors. Agents receive the resulting learned graph observations based on their location in the graph. Our approach can be used in a decentralized manner at runtime and in combination with a <b>reinforcement</b> <b>learning</b> algorithm of choice. We evaluate our method across 1000 diverse graphs in the context of routing in communication networks and find that it enables agents to generalize and adapt to changes in the graph.</p></p class="citation"></blockquote><h2 id=csne-2>cs.NE (2)</h2><h3 id=229236-a-computational-approach-to-visual-ecology-with-deep-reinforcement-learning-sacha-sokoloski-et-al-2024>(229/236) A computational approach to visual ecology with deep reinforcement learning (Sacha Sokoloski et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sacha Sokoloski, Jure Majnik, Philipp Berens. (2024)<br><strong>A computational approach to visual ecology with deep reinforcement learning</strong><br><button class=copy-to-clipboard title="A computational approach to visual ecology with deep reinforcement learning" index=229>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-229 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NE<br>Categories: cs-NE, cs.NE<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05266v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05266v1.pdf filename=2402.05266v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Animal vision is thought to optimize various objectives from metabolic efficiency to discrimination performance, yet its ultimate objective is to facilitate the survival of the animal within its ecological niche. However, modeling animal behavior in complex environments has been challenging. To study how environments shape and constrain visual processing, we developed a deep <b>reinforcement</b> <b>learning</b> framework in which an agent moves through a 3-d environment that it perceives through a vision model, where its only goal is to survive. Within this framework we developed a foraging task where the agent must gather food that sustains it, and avoid food that harms it. We first established that the complexity of the vision model required for survival on this task scaled with the variety and visual complexity of the food in the environment. Moreover, we showed that a recurrent network architecture was necessary to fully exploit complex vision models on the most visually demanding tasks. Finally, we showed how different network architectures learned distinct representations of the environment and task, and lead the agent to exhibit distinct behavioural strategies. In summary, this paper lays the foundation for a computational approach to visual ecology, provides extensive benchmarks for future work, and demonstrates how representations and behaviour emerge from an agent&rsquo;s drive for survival.</p></p class="citation"></blockquote><h3 id=230236-a-bandit-approach-with-evolutionary-operators-for-model-selection-margaux-brégère-et-al-2024>(230/236) A Bandit Approach with Evolutionary Operators for Model Selection (Margaux Brégère et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Margaux Brégère, Julie Keisler. (2024)<br><strong>A Bandit Approach with Evolutionary Operators for Model Selection</strong><br><button class=copy-to-clipboard title="A Bandit Approach with Evolutionary Operators for Model Selection" index=230>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-230 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NE<br>Categories: cs-AI, cs-LG, cs-NE, cs.NE, math-OC<br>Keyword Score: 10<br>Keywords: Bandit Algorithm<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05144v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05144v1.pdf filename=2402.05144v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper formulates model selection as an infinite-armed <b>bandit</b> problem. The models are arms, and picking an arm corresponds to a partial training of the model (resource allocation). The reward is the accuracy of the selected model after its partial training. In this best arm identification problem, regret is the gap between the expected accuracy of the optimal model and that of the model finally chosen. We first consider a straightforward generalization of UCB-E to the stochastic infinite-armed <b>bandit</b> problem and show that, under basic assumptions, the expected regret order is $T^{-\alpha}$ for some $\alpha \in (0,1/5)$ and $T$ the number of resources to allocate. From this vanilla algorithm, we introduce the algorithm Mutant-UCB that incorporates operators from evolutionary algorithms. Tests carried out on three open source image classification data sets attest to the relevance of this novel combining approach, which outperforms the state-of-the-art for a fixed budget.</p></p class="citation"></blockquote><h2 id=csfl-1>cs.FL (1)</h2><h3 id=231236-distributed-fair-assignment-and-rebalancing-for-mobility-on-demand-systems-via-an-auction-based-method-kaier-liang-et-al-2024>(231/236) Distributed Fair Assignment and Rebalancing for Mobility-on-Demand Systems via an Auction-based Method (Kaier Liang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kaier Liang, Cristian-Ioan Vasile. (2024)<br><strong>Distributed Fair Assignment and Rebalancing for Mobility-on-Demand Systems via an Auction-based Method</strong><br><button class=copy-to-clipboard title="Distributed Fair Assignment and Rebalancing for Mobility-on-Demand Systems via an Auction-based Method" index=231>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-231 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.FL<br>Categories: cs-FL, cs-GT, cs.FL<br>Keyword Score: 10<br>Keywords: Fairness<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04972v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04972v1.pdf filename=2402.04972v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we consider fair assignment of complex requests for Mobility-On-Demand systems. We model the transportation requests as temporal logic formulas that must be satisfied by a fleet of vehicles. We require that the assignment of requests to vehicles is performed in a distributed manner based only on communication between vehicles while ensuring fair allocation. Our approach to the vehicle-request assignment problem is based on a distributed auction scheme with no centralized bidding that leverages utility history correction of bids to improve <b>fairness.</b> Complementarily, we propose a rebalancing scheme that employs rerouting vehicles to more rewarding areas to increase the potential future utility and ensure a fairer utility distribution. We adopt the max-min and deviation of utility as the two criteria for <b>fairness.</b> We demonstrate the methods in the mid-Manhattan map with a large number of requests generated in different probability settings. We show that we increase the <b>fairness</b> between vehicles based on the <b>fairness</b> criteria without degenerating the servicing quality.</p></p class="citation"></blockquote><h2 id=csdl-1>cs.DL (1)</h2><h3 id=232236-hierarchical-tree-structured-knowledge-graph-for-academic-insight-survey-jinghong-li-et-al-2024>(232/236) Hierarchical Tree-structured Knowledge Graph For Academic Insight Survey (Jinghong Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jinghong Li, Huy Phan, Wen Gu, Koichi Ota, Shinobu Hasegawa. (2024)<br><strong>Hierarchical Tree-structured Knowledge Graph For Academic Insight Survey</strong><br><button class=copy-to-clipboard title="Hierarchical Tree-structured Knowledge Graph For Academic Insight Survey" index=232>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-232 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DL<br>Categories: cs-CL, cs-DL, cs-LG, cs.DL<br>Keyword Score: 10<br>Keywords: Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04854v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04854v1.pdf filename=2402.04854v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Research surveys have always posed a challenge for beginner researchers who lack of research training. These researchers struggle to understand the directions within their research topic, and the discovery of new research findings within a short time. One way to provide intuitive assistance to beginner researchers is by offering relevant knowledge graphs(KG) and recommending related academic papers. However, existing navigation knowledge graphs primarily rely on keywords in the research field and often fail to present the logical hierarchy among multiple related papers clearly. Moreover, most <b>recommendation</b> systems for academic papers simply rely on high text similarity, which can leave researchers confused as to why a particular article is being recommended. They may lack of grasp important information about the insight connection between &ldquo;Issue resolved&rdquo; and &ldquo;Issue finding&rdquo; that they hope to obtain. To address these issues, this study aims to support research insight surveys for beginner researchers by establishing a hierarchical tree-structured knowledge graph that reflects the inheritance insight of research topics and the relevance insight among the academic papers.</p></p class="citation"></blockquote><h2 id=mathfa-1>math.FA (1)</h2><h3 id=233236-stochastic-data-driven-bouligand-landweber-method-for-solving-non-smooth-inverse-problems-harshit-bajpai-et-al-2024>(233/236) Stochastic Data-Driven Bouligand Landweber Method for Solving Non-smooth Inverse Problems (Harshit Bajpai et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Harshit Bajpai, Gaurav Mittal, Ankik Kumar Giri. (2024)<br><strong>Stochastic Data-Driven Bouligand Landweber Method for Solving Non-smooth Inverse Problems</strong><br><button class=copy-to-clipboard title="Stochastic Data-Driven Bouligand Landweber Method for Solving Non-smooth Inverse Problems" index=233>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-233 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.FA<br>Categories: 47H17, 65J15, 65J20, cs-NA, math-CA, math-FA, math-NA, math-OC, math.FA<br>Keyword Score: 10<br>Keywords: Stochastic Gradient Descent<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04772v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04772v1.pdf filename=2402.04772v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this study, we present and analyze a novel variant of the <b>stochastic</b> <b>gradient</b> <b>descent</b> method, referred as <b>Stochastic</b> <b>data-driven</b> <b>Bouligand</b> Landweber iteration tailored for addressing the system of non-smooth ill-posed inverse problems. Our method incorporates the utilization of training data, using a bounded linear operator, which guides the iterative procedure. At each iteration step, the method randomly chooses one equation from the nonlinear system with data-driven term. When dealing with the precise or exact data, it has been established that mean square iteration error converges to zero. However, when confronted with the noisy data, we employ our approach in conjunction with a predefined stopping criterion, which we refer to as an \textit{a-priori} stopping rule. We provide a comprehensive theoretical foundation, establishing convergence and stability for this scheme within the realm of infinite-dimensional Hilbert spaces. These theoretical underpinnings are further bolstered by discussing an example that fulfills assumptions of the paper.</p></p class="citation"></blockquote><h2 id=quant-ph-2>quant-ph (2)</h2><h3 id=234236-continuous-variable-qkd-with-key-rates-far-above-devetak-winter-arpan-akash-ray-et-al-2024>(234/236) Continuous-Variable QKD with key rates far above Devetak-Winter (Arpan Akash Ray et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Arpan Akash Ray, Boris Skoric. (2024)<br><strong>Continuous-Variable QKD with key rates far above Devetak-Winter</strong><br><button class=copy-to-clipboard title="Continuous-Variable QKD with key rates far above Devetak-Winter" index=234>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-234 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: quant-ph<br>Categories: cs-CR, quant-ph, quant-ph<br>Keyword Score: 10<br>Keywords: Mutual Information<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04770v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04770v1.pdf filename=2402.04770v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Continuous-Variable Quantum Key Distribution (CVQKD) at large distances has such high noise levels that the employed error-correcting codes must have very low rate. In this regime it becomes feasible to implement random-codebook error correction, which is known to perform close to capacity. We propose a random-codebook reverse reconciliation scheme for CVQKD that is inspired by spread-spectrum watermarking. Our scheme has a novel way of achieving statistical decoupling between the publicly sent reconciliation data and the secret key. We provide a theoretical analysis of the secret key rate and we present numerical results. The best performance is obtained when the message size exceeds the <b>mutual</b> <b>information</b> I(X;Y) between Alice and Bob&rsquo;s measurements. This somewhat counter-intuitive result is understood from a tradeoff between code rate and frame rejection rate, combined with the fact that error correction for QKD needs to reconcile only random data. We obtain secret key lengths that lie far above the Devetak-Winter value I(X;Y)-I(E;Y).</p></p class="citation"></blockquote><h3 id=235236-gaussian-process-regression-based-method-for-the-localization-of-exceptional-points-in-complex-resonance-spectra-patrick-egenlauf-et-al-2024>(235/236) Gaussian-process-regression-based method for the localization of exceptional points in complex resonance spectra (Patrick Egenlauf et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Patrick Egenlauf, Patric Rommel, Jörg Main. (2024)<br><strong>Gaussian-process-regression-based method for the localization of exceptional points in complex resonance spectra</strong><br><button class=copy-to-clipboard title="Gaussian-process-regression-based method for the localization of exceptional points in complex resonance spectra" index=235>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-235 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: quant-ph<br>Categories: cond-mat-mes-hall, cs-LG, quant-ph, quant-ph<br>Keyword Score: 10<br>Keywords: Gaussian Process<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05972v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05972v1.pdf filename=2402.05972v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Resonances in open quantum systems depending on at least two controllable parameters can show the phenomenon of exceptional points (EPs), where not only the eigenvalues but also the eigenvectors of two or more resonances coalesce. Their exact localization in the parameter space is challenging, in particular in systems, where the computation of the quantum spectra and resonances is numerically very expensive. We introduce an efficient machine learning algorithm to find exceptional points based on <b>Gaussian</b> <b>process</b> regression (GPR). The GPR-model is trained with an initial set of eigenvalue pairs belonging to an EP and used for a first estimation of the EP position via a numerically cheap root search. The estimate is then improved iteratively by adding selected exact eigenvalue pairs as training points to the GPR-model. The GPR-based method is developed and tested on a simple low-dimensional matrix model and then applied to a challenging real physical system, viz., the localization of EPs in the resonance spectra of excitons in cuprous oxide in external electric and magnetic fields. The precise computation of EPs, by taking into account the complete valence band structure and central-cell corrections of the crystal, can be the basis for the experimental observation of EPs in this system.</p></p class="citation"></blockquote><h2 id=csgt-1>cs.GT (1)</h2><h3 id=236236-no-transaction-fees-no-problem-achieving-fairness-in-transaction-fee-mechanism-design-sankarshan-damle-et-al-2024>(236/236) No Transaction Fees? No Problem! Achieving Fairness in Transaction Fee Mechanism Design (Sankarshan Damle et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sankarshan Damle, Varul Srivastava, Sujit Gujar. (2024)<br><strong>No Transaction Fees? No Problem! Achieving Fairness in Transaction Fee Mechanism Design</strong><br><button class=copy-to-clipboard title="No Transaction Fees? No Problem! Achieving Fairness in Transaction Fee Mechanism Design" index=236>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-236 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.GT<br>Categories: cs-GT, cs.GT<br>Keyword Score: 10<br>Keywords: Fairness<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.04634v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.04634v1.pdf filename=2402.04634v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The recently proposed Transaction Fee Mechanism (TFM) literature studies the strategic interaction between the miner of a block and the transaction creators (or users) in a blockchain. In a TFM, the miner includes transactions that maximize its utility while users submit fees for a slot in the block. The existing TFM literature focuses on satisfying standard incentive properties &ndash; which may limit widespread adoption. We argue that a TFM is &ldquo;fair&rdquo; to the transaction creators if it satisfies specific notions, namely Zero-fee Transaction Inclusion and Monotonicity. First, we prove that one generally cannot ensure both these properties and prevent a miner&rsquo;s strategic manipulation. We also show that existing TFMs either do not satisfy these notions or do so at a high cost to the miners&rsquo; utility. As such, we introduce a novel TFM using on-chain randomness &ndash; rTFM. We prove that rTFM guarantees incentive compatibility for miners and users while satisfying our novel <b>fairness</b> constraints.</p></p class="citation"></blockquote></div><div class="row pl-3 pr-3"><div class="col-md-6 share-buttons"></div></div><hr><div class="row next-prev-navigator"><div class="col-md-6 previous-article"><a href=/akitenkrad-blog/posts/arxiv/202402/20240208000000/ title="arXiv @ 2024.02.08" class="btn btn-outline-info"><div><i class="fas fa-chevron-circle-left"></i> Prev</div><div class=next-prev-text>arXiv @ 2024.02.08</div></a></div><div class="col-md-6 next-article"><a href=/akitenkrad-blog/posts/arxiv/202402/20240210000000/ title="arXiv @ 2024.02.10" class="btn btn-outline-info"><div>Next <i class="fas fa-chevron-circle-right"></i></div><div class=next-prev-text>arXiv @ 2024.02.10</div></a></div></div><hr></div></div></div><a id=scroll-to-top class=btn><i class="fas fa-chevron-circle-up"></i></a></section><section class=toc-section id=toc-section><div class=toc-holder><h5 class="text-center pl-3">Table of Contents</h5><hr><div class=toc><nav id=TableOfContents><ul><li><a href=#primary-categories>Primary Categories</a></li><li><a href=#keywords>Keywords</a></li><li><a href=#cslg-65>cs.LG (65)</a><ul><li><a href=#1236-graph-neural-networks-as-fast-and-high-fidelity-emulators-for-finite-element-ice-sheet-modeling-maryam-rahnemoonfar-et-al-2024>(1/236) Graph Neural Networks as Fast and High-fidelity Emulators for Finite-Element Ice Sheet Modeling (Maryam Rahnemoonfar et al., 2024)</a></li><li><a href=#2236-l4q-parameter-efficient-quantization-aware-training-on-large-language-models-via-lora-wise-lsq-hyesung-jeon-et-al-2024>(2/236) L4Q: Parameter Efficient Quantization-Aware Training on Large Language Models via LoRA-wise LSQ (Hyesung Jeon et al., 2024)</a></li><li><a href=#3236-de-amplifying-bias-from-differential-privacy-in-language-model-fine-tuning-sanjari-srivastava-et-al-2024>(3/236) De-amplifying Bias from Differential Privacy in Language Model Fine-tuning (Sanjari Srivastava et al., 2024)</a></li><li><a href=#4236-hydragen-high-throughput-llm-inference-with-shared-prefixes-jordan-juravsky-et-al-2024>(4/236) Hydragen: High-Throughput LLM Inference with Shared Prefixes (Jordan Juravsky et al., 2024)</a></li><li><a href=#5236-open-vocabulary-calibration-for-vision-language-models-shuoyuan-wang-et-al-2024>(5/236) Open-Vocabulary Calibration for Vision-Language Models (Shuoyuan Wang et al., 2024)</a></li><li><a href=#6236-grandmaster-level-chess-without-search-anian-ruoss-et-al-2024>(6/236) Grandmaster-Level Chess Without Search (Anian Ruoss et al., 2024)</a></li><li><a href=#7236-ranksum-an-unsupervised-extractive-text-summarization-based-on-rank-fusion-a-joshi-et-al-2024>(7/236) RankSum An unsupervised extractive text summarization based on rank fusion (A. Joshi et al., 2024)</a></li><li><a href=#8236-classifying-spam-emails-using-agglomerative-hierarchical-clustering-and-a-topic-based-approach-f-janez-martino-et-al-2024>(8/236) Classifying spam emails using agglomerative hierarchical clustering and a topic-based approach (F. Janez-Martino et al., 2024)</a></li><li><a href=#9236-opening-the-ai-black-box-program-synthesis-via-mechanistic-interpretability-eric-j-michaud-et-al-2024>(9/236) Opening the AI black box: program synthesis via mechanistic interpretability (Eric J. Michaud et al., 2024)</a></li><li><a href=#10236-nito-neural-implicit-fields-for-resolution-free-topology-optimization-amin-heyrani-nobari-et-al-2024>(10/236) NITO: Neural Implicit Fields for Resolution-free Topology Optimization (Amin Heyrani Nobari et al., 2024)</a></li><li><a href=#11236-apiq-finetuning-of-2-bit-quantized-large-language-model-baohao-liao-et-al-2024>(11/236) ApiQ: Finetuning of 2-Bit Quantized Large Language Model (Baohao Liao et al., 2024)</a></li><li><a href=#12236-assessing-the-brittleness-of-safety-alignment-via-pruning-and-low-rank-modifications-boyi-wei-et-al-2024>(12/236) Assessing the Brittleness of Safety Alignment via Pruning and Low-Rank Modifications (Boyi Wei et al., 2024)</a></li><li><a href=#13236-pac-learnability-under-explanation-preserving-graph-perturbations-xu-zheng-et-al-2024>(13/236) PAC Learnability under Explanation-Preserving Graph Perturbations (Xu Zheng et al., 2024)</a></li><li><a href=#14236-a-sober-look-at-llms-for-material-discovery-are-they-actually-good-for-bayesian-optimization-over-molecules-agustinus-kristiadi-et-al-2024>(14/236) A Sober Look at LLMs for Material Discovery: Are They Actually Good for Bayesian Optimization Over Molecules? (Agustinus Kristiadi et al., 2024)</a></li><li><a href=#15236-example-based-explanations-for-random-forests-using-machine-unlearning-tanmay-surve-et-al-2024>(15/236) Example-based Explanations for Random Forests using Machine Unlearning (Tanmay Surve et al., 2024)</a></li><li><a href=#16236-multi-patch-prediction-adapting-llms-for-time-series-representation-learning-yuxuan-bian-et-al-2024>(16/236) Multi-Patch Prediction: Adapting LLMs for Time Series Representation Learning (Yuxuan Bian et al., 2024)</a></li><li><a href=#17236-latent-plan-transformer-planning-as-latent-variable-inference-deqian-kong-et-al-2024>(17/236) Latent Plan Transformer: Planning as Latent Variable Inference (Deqian Kong et al., 2024)</a></li><li><a href=#18236-levi-generalizable-fine-tuning-via-layer-wise-ensemble-of-different-views-yuji-roh-et-al-2024>(18/236) LEVI: Generalizable Fine-tuning via Layer-wise Ensemble of Different Views (Yuji Roh et al., 2024)</a></li><li><a href=#19236-feature-distribution-on-graph-topology-mediates-the-effect-of-graph-convolution-homophily-perspective-soo-yong-lee-et-al-2024>(19/236) Feature Distribution on Graph Topology Mediates the Effect of Graph Convolution: Homophily Perspective (Soo Yong Lee et al., 2024)</a></li><li><a href=#20236-collective-counterfactual-explanations-via-optimal-transport-ahmad-reza-ehyaei-et-al-2024>(20/236) Collective Counterfactual Explanations via Optimal Transport (Ahmad-Reza Ehyaei et al., 2024)</a></li><li><a href=#21236-sumrec-a-framework-for-recommendation-using-open-domain-dialogue-ryutaro-asahara-et-al-2024>(21/236) SumRec: A Framework for Recommendation using Open-Domain Dialogue (Ryutaro Asahara et al., 2024)</a></li><li><a href=#22236-sym-q-adaptive-symbolic-regression-via-sequential-decision-making-yuan-tian-et-al-2024>(22/236) Sym-Q: Adaptive Symbolic Regression via Sequential Decision-Making (Yuan Tian et al., 2024)</a></li><li><a href=#23236-safety-filters-for-black-box-dynamical-systems-by-learning-discriminating-hyperplanes-will-lavanakul-et-al-2024>(23/236) Safety Filters for Black-Box Dynamical Systems by Learning Discriminating Hyperplanes (Will Lavanakul et al., 2024)</a></li><li><a href=#24236-towards-understanding-inductive-bias-in-transformers-a-view-from-infinity-itay-lavie-et-al-2024>(24/236) Towards Understanding Inductive Bias in Transformers: A View From Infinity (Itay Lavie et al., 2024)</a></li><li><a href=#25236-navigating-complexity-toward-lossless-graph-condensation-via-expanding-window-matching-yuchen-zhang-et-al-2024>(25/236) Navigating Complexity: Toward Lossless Graph Condensation via Expanding Window Matching (Yuchen Zhang et al., 2024)</a></li><li><a href=#26236-a-bayesian-approach-to-online-learning-for-contextual-restless-bandits-with-applications-to-public-health-biyonka-liang-et-al-2024>(26/236) A Bayesian Approach to Online Learning for Contextual Restless Bandits with Applications to Public Health (Biyonka Liang et al., 2024)</a></li><li><a href=#27236-code-as-reward-empowering-reinforcement-learning-with-vlms-david-venuto-et-al-2024>(27/236) Code as Reward: Empowering Reinforcement Learning with VLMs (David Venuto et al., 2024)</a></li><li><a href=#28236-oil-ad-an-anomaly-detection-framework-for-sequential-decision-sequences-chen-wang-et-al-2024>(28/236) OIL-AD: An Anomaly Detection Framework for Sequential Decision Sequences (Chen Wang et al., 2024)</a></li><li><a href=#29236-examining-modality-incongruity-in-multimodal-federated-learning-for-medical-vision-and-language-based-disease-detection-pramit-saha-et-al-2024>(29/236) Examining Modality Incongruity in Multimodal Federated Learning for Medical Vision and Language-based Disease Detection (Pramit Saha et al., 2024)</a></li><li><a href=#30236-do-transformer-world-models-give-better-policy-gradients-michel-ma-et-al-2024>(30/236) Do Transformer World Models Give Better Policy Gradients? (Michel Ma et al., 2024)</a></li><li><a href=#31236-adabatchgrad-combining-adaptive-batch-size-and-adaptive-step-size-petr-ostroukhov-et-al-2024>(31/236) AdaBatchGrad: Combining Adaptive Batch Size and Adaptive Step Size (Petr Ostroukhov et al., 2024)</a></li><li><a href=#32236-learning-fair-ranking-policies-via-differentiable-optimization-of-ordered-weighted-averages-my-h-dinh-et-al-2024>(32/236) Learning Fair Ranking Policies via Differentiable Optimization of Ordered Weighted Averages (My H. Dinh et al., 2024)</a></li><li><a href=#33236-on-diffusion-models-for-amortized-inference-benchmarking-and-improving-stochastic-control-and-sampling-marcin-sendera-et-al-2024>(33/236) On diffusion models for amortized inference: Benchmarking and improving stochastic control and sampling (Marcin Sendera et al., 2024)</a></li><li><a href=#34236-compression-of-structured-data-with-autoencoders-provable-benefit-of-nonlinearities-and-depth-kevin-kögler-et-al-2024>(34/236) Compression of Structured Data with Autoencoders: Provable Benefit of Nonlinearities and Depth (Kevin Kögler et al., 2024)</a></li><li><a href=#35236-a-masked-language-model-for-multi-source-ehr-trajectories-contextual-representation-learning-ali-amirahmadi-et-al-2024>(35/236) A Masked language model for multi-source EHR trajectories contextual representation learning (Ali Amirahmadi et al., 2024)</a></li><li><a href=#36236-on-provable-length-and-compositional-generalization-kartik-ahuja-et-al-2024>(36/236) On Provable Length and Compositional Generalization (Kartik Ahuja et al., 2024)</a></li><li><a href=#37236-learning-by-doing-an-online-causal-reinforcement-learning-framework-with-causal-aware-policy-ruichu-cai-et-al-2024>(37/236) Learning by Doing: An Online Causal Reinforcement Learning Framework with Causal-Aware Policy (Ruichu Cai et al., 2024)</a></li><li><a href=#38236-closing-the-gap-between-sgp4-and-high-precision-propagation-via-differentiable-programming-giacomo-acciarini-et-al-2024>(38/236) Closing the Gap Between SGP4 and High-Precision Propagation via Differentiable Programming (Giacomo Acciarini et al., 2024)</a></li><li><a href=#39236-designing-deep-neural-networks-for-driver-intention-recognition-koen-vellenga-et-al-2024>(39/236) Designing deep neural networks for driver intention recognition (Koen Vellenga et al., 2024)</a></li><li><a href=#40236-progressive-gradient-flow-for-robust-nm-sparsity-training-in-transformers-abhimanyu-rajeshkumar-bambhaniya-et-al-2024>(40/236) Progressive Gradient Flow for Robust N:M Sparsity Training in Transformers (Abhimanyu Rajeshkumar Bambhaniya et al., 2024)</a></li><li><a href=#41236-incorporating-retrieval-based-causal-learning-with-information-bottlenecks-for-interpretable-graph-neural-networks-jiahua-rao-et-al-2024>(41/236) Incorporating Retrieval-based Causal Learning with Information Bottlenecks for Interpretable Graph Neural Networks (Jiahua Rao et al., 2024)</a></li><li><a href=#42236-group-distributionally-robust-dataset-distillation-with-risk-minimization-saeed-vahidian-et-al-2024>(42/236) Group Distributionally Robust Dataset Distillation with Risk Minimization (Saeed Vahidian et al., 2024)</a></li><li><a href=#43236-compressing-deep-reinforcement-learning-networks-with-a-dynamic-structured-pruning-method-for-autonomous-driving-wensheng-su-et-al-2024>(43/236) Compressing Deep Reinforcement Learning Networks with a Dynamic Structured Pruning Method for Autonomous Driving (Wensheng Su et al., 2024)</a></li><li><a href=#44236-online-learning-approach-for-survival-analysis-camila-fernandez-et-al-2024>(44/236) Online Learning Approach for Survival Analysis (Camila Fernandez et al., 2024)</a></li><li><a href=#45236-curvature-informed-sgd-via-general-purpose-lie-group-preconditioners-omead-pooladzandi-et-al-2024>(45/236) Curvature-Informed SGD via General Purpose Lie-Group Preconditioners (Omead Pooladzandi et al., 2024)</a></li><li><a href=#46236-triplet-interaction-improves-graph-transformers-accurate-molecular-graph-learning-with-triplet-graph-transformers-md-shamim-hussain-et-al-2024>(46/236) Triplet Interaction Improves Graph Transformers: Accurate Molecular Graph Learning with Triplet Graph Transformers (Md Shamim Hussain et al., 2024)</a></li><li><a href=#47236-online-cascade-learning-for-efficient-inference-over-streams-lunyiu-nie-et-al-2024>(47/236) Online Cascade Learning for Efficient Inference over Streams (Lunyiu Nie et al., 2024)</a></li><li><a href=#48236-the-fine-grained-complexity-of-gradient-computation-for-training-large-language-models-josh-alman-et-al-2024>(48/236) The Fine-Grained Complexity of Gradient Computation for Training Large Language Models (Josh Alman et al., 2024)</a></li><li><a href=#49236-a-comparative-study-on-feature-selection-for-a-risk-prediction-model-for-colorectal-cancer-n-cueto-lópez-et-al-2024>(49/236) A comparative study on feature selection for a risk prediction model for colorectal cancer (N. Cueto-López et al., 2024)</a></li><li><a href=#50236-analyzing-adversarial-inputs-in-deep-reinforcement-learning-davide-corsi-et-al-2024>(50/236) Analyzing Adversarial Inputs in Deep Reinforcement Learning (Davide Corsi et al., 2024)</a></li><li><a href=#51236-convergence-for-natural-policy-gradient-on-infinite-state-average-reward-markov-decision-processes-isaac-grosof-et-al-2024>(51/236) Convergence for Natural Policy Gradient on Infinite-State Average-Reward Markov Decision Processes (Isaac Grosof et al., 2024)</a></li><li><a href=#52236-qgfn-controllable-greediness-with-action-values-elaine-lau-et-al-2024>(52/236) QGFN: Controllable Greediness with Action Values (Elaine Lau et al., 2024)</a></li><li><a href=#53236-hydra-sequentially-dependent-draft-heads-for-medusa-decoding-zachary-ankner-et-al-2024>(53/236) Hydra: Sequentially-Dependent Draft Heads for Medusa Decoding (Zachary Ankner et al., 2024)</a></li><li><a href=#54236-a-resource-model-for-neural-scaling-law-jinyeop-song-et-al-2024>(54/236) A Resource Model For Neural Scaling Law (Jinyeop Song et al., 2024)</a></li><li><a href=#55236-simulated-overparameterization-hanna-mazzawi-et-al-2024>(55/236) Simulated Overparameterization (Hanna Mazzawi et al., 2024)</a></li><li><a href=#56236-moco-a-learnable-meta-optimizer-for-combinatorial-optimization-tim-dernedde-et-al-2024>(56/236) Moco: A Learnable Meta Optimizer for Combinatorial Optimization (Tim Dernedde et al., 2024)</a></li><li><a href=#57236-on-the-completeness-of-invariant-geometric-deep-learning-models-zian-li-et-al-2024>(57/236) On the Completeness of Invariant Geometric Deep Learning Models (Zian Li et al., 2024)</a></li><li><a href=#58236-e3-equivariant-mesh-neural-networks-thuan-trang-et-al-2024>(58/236) E(3)-Equivariant Mesh Neural Networks (Thuan Trang et al., 2024)</a></li><li><a href=#59236-flowpg-action-constrained-policy-gradient-with-normalizing-flows-janaka-chathuranga-brahmanage-et-al-2024>(59/236) FlowPG: Action-constrained Policy Gradient with Normalizing Flows (Janaka Chathuranga Brahmanage et al., 2024)</a></li><li><a href=#60236-a-perspective-on-individualized-treatment-effects-estimation-from-time-series-health-data-ghadeer-o-ghosheh-et-al-2024>(60/236) A Perspective on Individualized Treatment Effects Estimation from Time-series Health Data (Ghadeer O. Ghosheh et al., 2024)</a></li><li><a href=#61236-towards-improved-imbalance-robustness-in-continual-multi-label-learning-with-dual-output-spiking-architecture-dosa-sourav-mishra-et-al-2024>(61/236) Towards Improved Imbalance Robustness in Continual Multi-Label Learning with Dual Output Spiking Architecture (DOSA) (Sourav Mishra et al., 2024)</a></li><li><a href=#62236-learning-diverse-policies-with-soft-self-generated-guidance-guojian-wang-et-al-2024>(62/236) Learning Diverse Policies with Soft Self-Generated Guidance (Guojian Wang et al., 2024)</a></li><li><a href=#63236-incentivized-truthful-communication-for-federated-bandits-zhepei-wei-et-al-2024>(63/236) Incentivized Truthful Communication for Federated Bandits (Zhepei Wei et al., 2024)</a></li><li><a href=#64236-learning-on-multimodal-graphs-a-survey-ciyuan-peng-et-al-2024>(64/236) Learning on Multimodal Graphs: A Survey (Ciyuan Peng et al., 2024)</a></li><li><a href=#65236-crashformer-a-multimodal-architecture-to-predict-the-risk-of-crash-amin-karimi-monsefi-et-al-2024>(65/236) CrashFormer: A Multimodal Architecture to Predict the Risk of Crash (Amin Karimi Monsefi et al., 2024)</a></li></ul></li><li><a href=#cscl-29>cs.CL (29)</a><ul><li><a href=#66236-transllama-llm-based-simultaneous-translation-system-roman-koshkin-et-al-2024>(66/236) TransLLaMa: LLM-based Simultaneous Translation System (Roman Koshkin et al., 2024)</a></li><li><a href=#67236-long-is-more-for-alignment-a-simple-but-tough-to-beat-baseline-for-instruction-fine-tuning-hao-zhao-et-al-2024>(67/236) Long Is More for Alignment: A Simple but Tough-to-Beat Baseline for Instruction Fine-Tuning (Hao Zhao et al., 2024)</a></li><li><a href=#68236-improving-cross-domain-low-resource-text-generation-through-llm-post-editing-a-programmer-interpreter-approach-zhuang-li-et-al-2024>(68/236) Improving Cross-Domain Low-Resource Text Generation through LLM Post-Editing: A Programmer-Interpreter Approach (Zhuang Li et al., 2024)</a></li><li><a href=#69236-prompting-implicit-discourse-relation-annotation-frances-yung-et-al-2024>(69/236) Prompting Implicit Discourse Relation Annotation (Frances Yung et al., 2024)</a></li><li><a href=#70236-aspect-based-sentiment-analysis-for-open-ended-hr-survey-responses-lois-rink-et-al-2024>(70/236) Aspect-Based Sentiment Analysis for Open-Ended HR Survey Responses (Lois Rink et al., 2024)</a></li><li><a href=#71236-tinyllm-learning-a-small-student-from-multiple-large-language-models-yijun-tian-et-al-2024>(71/236) TinyLLM: Learning a Small Student from Multiple Large Language Models (Yijun Tian et al., 2024)</a></li><li><a href=#72236-pedagogical-alignment-of-large-language-models-shashank-sonkar-et-al-2024>(72/236) Pedagogical Alignment of Large Language Models (Shashank Sonkar et al., 2024)</a></li><li><a href=#73236-an-enhanced-prompt-based-llm-reasoning-scheme-via-knowledge-graph-integrated-collaboration-yihao-li-et-al-2024>(73/236) An Enhanced Prompt-Based LLM Reasoning Scheme via Knowledge Graph-Integrated Collaboration (Yihao Li et al., 2024)</a></li><li><a href=#74236-a-hypothesis-driven-framework-for-the-analysis-of-self-rationalising-models-marc-braun-et-al-2024>(74/236) A Hypothesis-Driven Framework for the Analysis of Self-Rationalising Models (Marc Braun et al., 2024)</a></li><li><a href=#75236-the-future-of-cognitive-strategy-enhanced-persuasive-dialogue-agents-new-perspectives-and-trends-mengqi-chen-et-al-2024>(75/236) The Future of Cognitive Strategy-enhanced Persuasive Dialogue Agents: New Perspectives and Trends (Mengqi Chen et al., 2024)</a></li><li><a href=#76236-ultralink-an-open-source-knowledge-enhanced-multilingual-supervised-fine-tuning-dataset-haoyu-wang-et-al-2024>(76/236) UltraLink: An Open-Source Knowledge-Enhanced Multilingual Supervised Fine-tuning Dataset (Haoyu Wang et al., 2024)</a></li><li><a href=#77236-mllm-as-a-judge-assessing-multimodal-llm-as-a-judge-with-vision-language-benchmark-dongping-chen-et-al-2024>(77/236) MLLM-as-a-Judge: Assessing Multimodal LLM-as-a-Judge with Vision-Language Benchmark (Dongping Chen et al., 2024)</a></li><li><a href=#78236-reconfidencing-llms-from-the-grouping-loss-perspective-lihu-chen-et-al-2024>(78/236) Reconfidencing LLMs from the Grouping Loss Perspective (Lihu Chen et al., 2024)</a></li><li><a href=#79236-padellm-ner-parallel-decoding-in-large-language-models-for-named-entity-recognition-jinghui-lu-et-al-2024>(79/236) PaDeLLM-NER: Parallel Decoding in Large Language Models for Named Entity Recognition (Jinghui Lu et al., 2024)</a></li><li><a href=#80236-large-language-models-as-faithful-explainers-yu-neng-chuang-et-al-2024>(80/236) Large Language Models As Faithful Explainers (Yu-Neng Chuang et al., 2024)</a></li><li><a href=#81236-veras-verify-then-assess-stem-lab-reports-berk-atil-et-al-2024>(81/236) VerAs: Verify then Assess STEM Lab Reports (Berk Atil et al., 2024)</a></li><li><a href=#82236-the-effect-of-sampling-temperature-on-problem-solving-in-large-language-models-matthew-renze-et-al-2024>(82/236) The Effect of Sampling Temperature on Problem Solving in Large Language Models (Matthew Renze et al., 2024)</a></li><li><a href=#83236-salad-bench-a-hierarchical-and-comprehensive-safety-benchmark-for-large-language-models-lijun-li-et-al-2024>(83/236) SALAD-Bench: A Hierarchical and Comprehensive Safety Benchmark for Large Language Models (Lijun Li et al., 2024)</a></li><li><a href=#84236-memoryllm-towards-self-updatable-large-language-models-yu-wang-et-al-2024>(84/236) MEMORYLLM: Towards Self-Updatable Large Language Models (Yu Wang et al., 2024)</a></li><li><a href=#85236-infllm-unveiling-the-intrinsic-capacity-of-llms-for-understanding-extremely-long-sequences-with-training-free-memory-chaojun-xiao-et-al-2024>(85/236) InfLLM: Unveiling the Intrinsic Capacity of LLMs for Understanding Extremely Long Sequences with Training-Free Memory (Chaojun Xiao et al., 2024)</a></li><li><a href=#86236-faithfulness-vs-plausibility-on-the-unreliability-of-explanations-from-large-language-models-chirag-agarwal-et-al-2024>(86/236) Faithfulness vs. Plausibility: On the (Un)Reliability of Explanations from Large Language Models (Chirag Agarwal et al., 2024)</a></li><li><a href=#87236-alirector-alignment-enhanced-chinese-grammatical-error-corrector-haihui-yang-et-al-2024>(87/236) Alirector: Alignment-Enhanced Chinese Grammatical Error Corrector (Haihui Yang et al., 2024)</a></li><li><a href=#88236-personalized-text-generation-with-fine-grained-linguistic-control-bashar-alhafni-et-al-2024>(88/236) Personalized Text Generation with Fine-Grained Linguistic Control (Bashar Alhafni et al., 2024)</a></li><li><a href=#89236-learning-communication-policies-for-different-follower-behaviors-in-a-collaborative-reference-game-philipp-sadler-et-al-2024>(89/236) Learning Communication Policies for Different Follower Behaviors in a Collaborative Reference Game (Philipp Sadler et al., 2024)</a></li><li><a href=#90236-source-identification-in-abstractive-summarization-yoshi-suhara-et-al-2024>(90/236) Source Identification in Abstractive Summarization (Yoshi Suhara et al., 2024)</a></li><li><a href=#91236-text-or-image-what-is-more-important-in-cross-domain-generalization-capabilities-of-hate-meme-detection-models-piush-aggarwal-et-al-2024>(91/236) Text or Image? What is More Important in Cross-Domain Generalization Capabilities of Hate Meme Detection Models? (Piush Aggarwal et al., 2024)</a></li><li><a href=#92236-how-bert-speaks-shakespearean-english-evaluating-historical-bias-in-contextual-language-models-miriam-cuscito-et-al-2024>(92/236) How BERT Speaks Shakespearean English? Evaluating Historical Bias in Contextual Language Models (Miriam Cuscito et al., 2024)</a></li><li><a href=#93236-stablemask-refining-causal-masking-in-decoder-only-transformer-qingyu-yin-et-al-2024>(93/236) StableMask: Refining Causal Masking in Decoder-only Transformer (Qingyu Yin et al., 2024)</a></li><li><a href=#94236-developments-in-sheaf-theoretic-models-of-natural-language-ambiguities-kin-ian-lo-et-al-2024>(94/236) Developments in Sheaf-Theoretic Models of Natural Language Ambiguities (Kin Ian Lo et al., 2024)</a></li></ul></li><li><a href=#csse-8>cs.SE (8)</a><ul><li><a href=#95236-automated-smart-contract-summarization-via-llms-yingjie-mao-et-al-2024>(95/236) Automated Smart Contract Summarization via LLMs (Yingjie Mao et al., 2024)</a></li><li><a href=#96236-you-can-rest-now-automated-specification-inference-and-black-box-testing-of-restful-apis-with-large-language-models-alix-decrop-et-al-2024>(96/236) You Can REST Now: Automated Specification Inference and Black-Box Testing of RESTful APIs with Large Language Models (Alix Decrop et al., 2024)</a></li><li><a href=#97236-an-investigation-of-patch-porting-practices-of-the-linux-kernel-ecosystem-xingyu-li-et-al-2024>(97/236) An Investigation of Patch Porting Practices of the Linux Kernel Ecosystem (Xingyu Li et al., 2024)</a></li><li><a href=#98236-can-we-identify-stack-overflow-questions-requiring-code-snippets-investigating-the-cause--effect-of-missing-code-snippets-saikat-mondal-et-al-2024>(98/236) Can We Identify Stack Overflow Questions Requiring Code Snippets? Investigating the Cause & Effect of Missing Code Snippets (Saikat Mondal et al., 2024)</a></li><li><a href=#99236-enhancing-user-interaction-in-chatgpt-characterizing-and-consolidating-multiple-prompts-for-issue-resolution-saikat-mondal-et-al-2024>(99/236) Enhancing User Interaction in ChatGPT: Characterizing and Consolidating Multiple Prompts for Issue Resolution (Saikat Mondal et al., 2024)</a></li><li><a href=#100236-the-foundations-of-computational-management-a-systematic-approach-to-task-automation-for-the-integration-of-artificial-intelligence-into-existing-workflows-tamen-jadad-garcia-et-al-2024>(100/236) The Foundations of Computational Management: A Systematic Approach to Task Automation for the Integration of Artificial Intelligence into Existing Workflows (Tamen Jadad-Garcia et al., 2024)</a></li><li><a href=#101236-on-the-standardization-of-behavioral-use-clauses-and-their-adoption-for-responsible-licensing-of-ai-daniel-mcduff-et-al-2024>(101/236) On the Standardization of Behavioral Use Clauses and Their Adoption for Responsible Licensing of AI (Daniel McDuff et al., 2024)</a></li><li><a href=#102236-irfuzzer-specialized-fuzzing-for-llvm-backend-code-generation-yuyang-rong-et-al-2024>(102/236) IRFuzzer: Specialized Fuzzing for LLVM Backend Code Generation (Yuyang Rong et al., 2024)</a></li></ul></li><li><a href=#cscv-38>cs.CV (38)</a><ul><li><a href=#103236-llms-meet-vlms-boost-open-vocabulary-object-detection-with-fine-grained-descriptors-sheng-jin-et-al-2024>(103/236) LLMs Meet VLMs: Boost Open Vocabulary Object Detection with Fine-grained Descriptors (Sheng Jin et al., 2024)</a></li><li><a href=#104236-colorswap-a-color-and-word-order-dataset-for-multimodal-evaluation-jirayu-burapacheep-et-al-2024>(104/236) ColorSwap: A Color and Word Order Dataset for Multimodal Evaluation (Jirayu Burapacheep et al., 2024)</a></li><li><a href=#105236-source-free-domain-adaptation-with-diffusion-guided-source-data-generation-shivang-chopra-et-al-2024>(105/236) Source-Free Domain Adaptation with Diffusion-Guided Source Data Generation (Shivang Chopra et al., 2024)</a></li><li><a href=#106236-screenai-a-vision-language-model-for-ui-and-infographics-understanding-gilles-baechler-et-al-2024>(106/236) ScreenAI: A Vision-Language Model for UI and Infographics Understanding (Gilles Baechler et al., 2024)</a></li><li><a href=#107236-fm-fusion-instance-aware-semantic-mapping-boosted-by-vision-language-foundation-models-chuhao-liu-et-al-2024>(107/236) FM-Fusion: Instance-aware Semantic Mapping Boosted by Vision-Language Foundation Models (Chuhao Liu et al., 2024)</a></li><li><a href=#108236-knowledge-distillation-for-road-detection-based-on-cross-model-semi-supervised-learning-wanli-ma-et-al-2024>(108/236) Knowledge Distillation for Road Detection based on cross-model Semi-Supervised Learning (Wanli Ma et al., 2024)</a></li><li><a href=#109236-convlora-and-adabn-based-domain-adaptation-via-self-training-sidra-aleem-et-al-2024>(109/236) ConvLoRA and AdaBN based Domain Adaptation via Self-Training (Sidra Aleem et al., 2024)</a></li><li><a href=#110236-sari-simplistic-average-and-robust-identification-based-noisy-partial-label-learning-darshana-saravanan-et-al-2024>(110/236) SARI: Simplistic Average and Robust Identification based Noisy Partial Label Learning (Darshana Saravanan et al., 2024)</a></li><li><a href=#111236-meet-jeanie-a-similarity-measure-for-3d-skeleton-sequences-via-temporal-viewpoint-alignment-lei-wang-et-al-2024>(111/236) Meet JEANIE: a Similarity Measure for 3D Skeleton Sequences via Temporal-Viewpoint Alignment (Lei Wang et al., 2024)</a></li><li><a href=#112236-sparse-anatomical-prompt-semi-supervised-learning-with-masked-image-modeling-for-cbct-tooth-segmentation-pengyu-dai-et-al-2024>(112/236) Sparse Anatomical Prompt Semi-Supervised Learning with Masked Image Modeling for CBCT Tooth Segmentation (Pengyu Dai et al., 2024)</a></li><li><a href=#113236-attention-guided-cam-visual-explanations-of-vision-transformer-guided-by-self-attention-saebom-leem-et-al-2024>(113/236) Attention Guided CAM: Visual Explanations of Vision Transformer Guided by Self-Attention (Saebom Leem et al., 2024)</a></li><li><a href=#114236-spad--spatially-aware-multiview-diffusers-yash-kant-et-al-2024>(114/236) SPAD : Spatially Aware Multiview Diffusers (Yash Kant et al., 2024)</a></li><li><a href=#115236-efficientvit-sam-accelerated-segment-anything-model-without-performance-loss-zhuoyang-zhang-et-al-2024>(115/236) EfficientViT-SAM: Accelerated Segment Anything Model Without Performance Loss (Zhuoyang Zhang et al., 2024)</a></li><li><a href=#116236-toward-accurate-camera-based-3d-object-detection-via-cascade-depth-estimation-and-calibration-chaoqun-wang-et-al-2024>(116/236) Toward Accurate Camera-based 3D Object Detection via Cascade Depth Estimation and Calibration (Chaoqun Wang et al., 2024)</a></li><li><a href=#117236-advancing-anomaly-detection-an-adaptation-model-and-a-new-dataset-liyun-zhu-et-al-2024>(117/236) Advancing Anomaly Detection: An Adaptation Model and a New Dataset (Liyun Zhu et al., 2024)</a></li><li><a href=#118236-color-recognition-in-challenging-lighting-environments-cnn-approach-nizamuddin-maitlo-et-al-2024>(118/236) Color Recognition in Challenging Lighting Environments: CNN Approach (Nizamuddin Maitlo et al., 2024)</a></li><li><a href=#119236-progressive-conservative-adaptation-for-evolving-target-domains-gangming-zhao-et-al-2024>(119/236) Progressive Conservative Adaptation for Evolving Target Domains (Gangming Zhao et al., 2024)</a></li><li><a href=#120236-physics-informed-and-data-driven-simulation-of-underwater-images-via-residual-learning-tanmoy-mondal-et-al-2024>(120/236) Physics Informed and Data Driven Simulation of Underwater Images via Residual Learning (Tanmoy Mondal et al., 2024)</a></li><li><a href=#121236-λ-eclipse-multi-concept-personalized-text-to-image-diffusion-models-by-leveraging-clip-latent-space-maitreya-patel-et-al-2024>(121/236) $λ$-ECLIPSE: Multi-Concept Personalized Text-to-Image Diffusion Models by Leveraging CLIP Latent Space (Maitreya Patel et al., 2024)</a></li><li><a href=#122236-star-shape-focused-texture-agnostic-representations-for-improved-object-detection-and-6d-pose-estimation-peter-hönig-et-al-2024>(122/236) STAR: Shape-focused Texture Agnostic Representations for Improved Object Detection and 6D Pose Estimation (Peter Hönig et al., 2024)</a></li><li><a href=#123236-dual-path-coupled-image-deraining-network-via-spatial-frequency-interaction-yuhong-he-et-al-2024>(123/236) Dual-Path Coupled Image Deraining Network via Spatial-Frequency Interaction (Yuhong He et al., 2024)</a></li><li><a href=#124236-nerf-as-non-distant-environment-emitter-in-physics-based-inverse-rendering-jingwang-ling-et-al-2024>(124/236) NeRF as Non-Distant Environment Emitter in Physics-based Inverse Rendering (Jingwang Ling et al., 2024)</a></li><li><a href=#125236-spiking-physformer-camera-based-remote-photoplethysmography-with-parallel-spike-driven-transformer-mingxuan-liu-et-al-2024>(125/236) Spiking-PhysFormer: Camera-Based Remote Photoplethysmography with Parallel Spike-driven Transformer (Mingxuan Liu et al., 2024)</a></li><li><a href=#126236-ov-nerf-open-vocabulary-neural-radiance-fields-with-vision-and-language-foundation-models-for-3d-semantic-understanding-guibiao-liao-et-al-2024>(126/236) OV-NeRF: Open-vocabulary Neural Radiance Fields with Vision and Language Foundation Models for 3D Semantic Understanding (Guibiao Liao et al., 2024)</a></li><li><a href=#127236-dmat-a-dynamic-mask-aware-transformer-for-human-de-occlusion-guoqiang-liang-et-al-2024>(127/236) DMAT: A Dynamic Mask-Aware Transformer for Human De-occlusion (Guoqiang Liang et al., 2024)</a></li><li><a href=#128236-instructscene-instruction-driven-3d-indoor-scene-synthesis-with-semantic-graph-prior-chenguo-lin-et-al-2024>(128/236) InstructScene: Instruction-Driven 3D Indoor Scene Synthesis with Semantic Graph Prior (Chenguo Lin et al., 2024)</a></li><li><a href=#129236-image-captioning-for-brazilian-portuguese-using-grit-model-rafael-silva-de-alencar-et-al-2024>(129/236) Image captioning for Brazilian Portuguese using GRIT model (Rafael Silva de Alencar et al., 2024)</a></li><li><a href=#130236-enhancement-of-bengali-ocr-by-specialized-models-and-advanced-techniques-for-diverse-document-types-akm-shahariar-azad-rabby-et-al-2024>(130/236) Enhancement of Bengali OCR by Specialized Models and Advanced Techniques for Diverse Document Types (AKM Shahariar Azad Rabby et al., 2024)</a></li><li><a href=#131236-lgm-large-multi-view-gaussian-model-for-high-resolution-3d-content-creation-jiaxiang-tang-et-al-2024>(131/236) LGM: Large Multi-View Gaussian Model for High-Resolution 3D Content Creation (Jiaxiang Tang et al., 2024)</a></li><li><a href=#132236-a-survey-on-domain-generalization-for-medical-image-analysis-ziwei-niu-et-al-2024>(132/236) A Survey on Domain Generalization for Medical Image Analysis (Ziwei Niu et al., 2024)</a></li><li><a href=#133236-data-efficient-large-vision-models-through-sequential-autoregression-jianyuan-guo-et-al-2024>(133/236) Data-efficient Large Vision Models through Sequential Autoregression (Jianyuan Guo et al., 2024)</a></li><li><a href=#134236-boundary-aware-contrastive-learning-for-semi-supervised-nuclei-instance-segmentation-ye-zhang-et-al-2024>(134/236) Boundary-aware Contrastive Learning for Semi-supervised Nuclei Instance Segmentation (Ye Zhang et al., 2024)</a></li><li><a href=#135236-towards-aligned-layout-generation-via-diffusion-model-with-aesthetic-constraints-jian-chen-et-al-2024>(135/236) Towards Aligned Layout Generation via Diffusion Model with Aesthetic Constraints (Jian Chen et al., 2024)</a></li><li><a href=#136236-g-nas-generalizable-neural-architecture-search-for-single-domain-generalization-object-detection-fan-wu-et-al-2024>(136/236) G-NAS: Generalizable Neural Architecture Search for Single Domain Generalization Object Detection (Fan Wu et al., 2024)</a></li><li><a href=#137236-gsn-generalisable-segmentation-in-neural-radiance-field-vinayak-gupta-et-al-2024>(137/236) GSN: Generalisable Segmentation in Neural Radiance Field (Vinayak Gupta et al., 2024)</a></li><li><a href=#138236-text2street-controllable-text-to-image-generation-for-street-views-jinming-su-et-al-2024>(138/236) Text2Street: Controllable Text-to-image Generation for Street Views (Jinming Su et al., 2024)</a></li><li><a href=#139236-biked-a-multimodal-dataset-of-14-million-bicycle-image-and-parametric-cad-designs-lyle-regenwetter-et-al-2024>(139/236) BIKED++: A Multimodal Dataset of 1.4 Million Bicycle Image and Parametric CAD Designs (Lyle Regenwetter et al., 2024)</a></li><li><a href=#140236-efficient-multi-resolution-fusion-for-remote-sensing-data-with-label-uncertainty-hersh-vakharia-et-al-2024>(140/236) Efficient Multi-Resolution Fusion for Remote Sensing Data with Label Uncertainty (Hersh Vakharia et al., 2024)</a></li></ul></li><li><a href=#csro-13>cs.RO (13)</a><ul><li><a href=#141236-exploration-without-maps-via-zero-shot-out-of-distribution-deep-reinforcement-learning-shathushan-sivashangaran-et-al-2024>(141/236) Exploration Without Maps via Zero-Shot Out-of-Distribution Deep Reinforcement Learning (Shathushan Sivashangaran et al., 2024)</a></li><li><a href=#142236-incoro-in-context-learning-for-robotics-control-with-feedback-loops-jiaqiang-ye-zhu-et-al-2024>(142/236) InCoRo: In-Context Learning for Robotics Control with Feedback Loops (Jiaqiang Ye Zhu et al., 2024)</a></li><li><a href=#143236-real-time-line-based-room-segmentation-and-continuous-euclidean-distance-fields-erik-warberg-et-al-2024>(143/236) Real-Time Line-Based Room Segmentation and Continuous Euclidean Distance Fields (Erik Warberg et al., 2024)</a></li><li><a href=#144236-a-comprehensive-survey-of-cross-domain-policy-transfer-for-embodied-agents-haoyi-niu-et-al-2024>(144/236) A Comprehensive Survey of Cross-Domain Policy Transfer for Embodied Agents (Haoyi Niu et al., 2024)</a></li><li><a href=#145236-tactile-based-object-retrieval-from-granular-media-jingxi-xu-et-al-2024>(145/236) Tactile-based Object Retrieval From Granular Media (Jingxi Xu et al., 2024)</a></li><li><a href=#146236-language-based-augmentation-to-address-shortcut-learning-in-object-goal-navigation-dennis-hoftijzer-et-al-2024>(146/236) Language-Based Augmentation to Address Shortcut Learning in Object Goal Navigation (Dennis Hoftijzer et al., 2024)</a></li><li><a href=#147236-tactile-ergodic-control-using-diffusion-and-geometric-algebra-cem-bilaloglu-et-al-2024>(147/236) Tactile Ergodic Control Using Diffusion and Geometric Algebra (Cem Bilaloglu et al., 2024)</a></li><li><a href=#148236-offline-deep-model-predictive-control-mpc-for-visual-navigation-taha-bouzid-et-al-2024>(148/236) Offline Deep Model Predictive Control (MPC) for Visual Navigation (Taha Bouzid et al., 2024)</a></li><li><a href=#149236-investigating-driving-interactions-a-robust-multi-agent-simulation-framework-for-autonomous-vehicles-marc-kaufeld-et-al-2024>(149/236) Investigating Driving Interactions: A Robust Multi-Agent Simulation Framework for Autonomous Vehicles (Marc Kaufeld et al., 2024)</a></li><li><a href=#150236-lidar-forest-dataset-lidar-point-cloud-simulation-dataset-for-forestry-application-yawen-lu-et-al-2024>(150/236) LiDAR-Forest Dataset: LiDAR Point Cloud Simulation Dataset for Forestry Application (Yawen Lu et al., 2024)</a></li><li><a href=#151236-deep-reinforcement-learning-with-dynamic-graphs-for-adaptive-informative-path-planning-apoorva-vashisth-et-al-2024>(151/236) Deep Reinforcement Learning with Dynamic Graphs for Adaptive Informative Path Planning (Apoorva Vashisth et al., 2024)</a></li><li><a href=#152236-robot-interaction-behavior-generation-based-on-social-motion-forecasting-for-human-robot-interaction-esteve-valls-mascaro-et-al-2024>(152/236) Robot Interaction Behavior Generation based on Social Motion Forecasting for Human-Robot Interaction (Esteve Valls Mascaro et al., 2024)</a></li><li><a href=#153236-boosting-reinforcement-learning-algorithms-in-continuous-robotic-reaching-tasks-using-adaptive-potential-functions-yifei-chen-et-al-2024>(153/236) Boosting Reinforcement Learning Algorithms in Continuous Robotic Reaching Tasks using Adaptive Potential Functions (Yifei Chen et al., 2024)</a></li></ul></li><li><a href=#mathoc-3>math.OC (3)</a><ul><li><a href=#154236-non-convergence-to-global-minimizers-for-adam-and-stochastic-gradient-descent-optimization-and-constructions-of-local-minimizers-in-the-training-of-artificial-neural-networks-arnulf-jentzen-et-al-2024>(154/236) Non-convergence to global minimizers for Adam and stochastic gradient descent optimization and constructions of local minimizers in the training of artificial neural networks (Arnulf Jentzen et al., 2024)</a></li><li><a href=#155236-extending-the-reach-of-first-order-algorithms-for-nonconvex-min-max-problems-with-cohypomonotonicity-ahmet-alacaoglu-et-al-2024>(155/236) Extending the Reach of First-Order Algorithms for Nonconvex Min-Max Problems with Cohypomonotonicity (Ahmet Alacaoglu et al., 2024)</a></li><li><a href=#156236-shadowheart-sgd-distributed-asynchronous-sgd-with-optimal-time-complexity-under-arbitrary-computation-and-communication-heterogeneity-alexander-tyurin-et-al-2024>(156/236) Shadowheart SGD: Distributed Asynchronous SGD with Optimal Time Complexity Under Arbitrary Computation and Communication Heterogeneity (Alexander Tyurin et al., 2024)</a></li></ul></li><li><a href=#cssi-1>cs.SI (1)</a><ul><li><a href=#157236-adaptive-hypergraph-network-for-trust-prediction-rongwei-xu-et-al-2024>(157/236) Adaptive Hypergraph Network for Trust Prediction (Rongwei Xu et al., 2024)</a></li></ul></li><li><a href=#csir-7>cs.IR (7)</a><ul><li><a href=#158236-navigating-the-knowledge-sea-planet-scale-answer-retrieval-using-llms-dipankar-sarkar-2024>(158/236) Navigating the Knowledge Sea: Planet-scale answer retrieval using LLMs (Dipankar Sarkar, 2024)</a></li><li><a href=#159236-multimodal-query-suggestion-with-multi-agent-reinforcement-learning-from-human-feedback-zheng-wang-et-al-2024>(159/236) Multimodal Query Suggestion with Multi-Agent Reinforcement Learning from Human Feedback (Zheng Wang et al., 2024)</a></li><li><a href=#160236-detecting-generated-native-ads-in-conversational-search-sebastian-schmidt-et-al-2024>(160/236) Detecting Generated Native Ads in Conversational Search (Sebastian Schmidt et al., 2024)</a></li><li><a href=#161236-leveraging-llms-for-unsupervised-dense-retriever-ranking-ekaterina-khramtsova-et-al-2024>(161/236) Leveraging LLMs for Unsupervised Dense Retriever Ranking (Ekaterina Khramtsova et al., 2024)</a></li><li><a href=#162236-ra-rec-an-efficient-id-representation-alignment-framework-for-llm-based-recommendation-xiaohan-yu-et-al-2024>(162/236) RA-Rec: An Efficient ID Representation Alignment Framework for LLM-based Recommendation (Xiaohan Yu et al., 2024)</a></li><li><a href=#163236-normy-non-uniform-history-modeling-for-open-retrieval-conversational-question-answering-muhammad-shihab-rashid-et-al-2024>(163/236) NORMY: Non-Uniform History Modeling for Open Retrieval Conversational Question Answering (Muhammad Shihab Rashid et al., 2024)</a></li><li><a href=#164236-theoretical-and-empirical-analysis-of-adaptive-entry-point-selection-for-graph-based-approximate-nearest-neighbor-search-yutaro-oguri-et-al-2024>(164/236) Theoretical and Empirical Analysis of Adaptive Entry Point Selection for Graph-based Approximate Nearest Neighbor Search (Yutaro Oguri et al., 2024)</a></li></ul></li><li><a href=#eessiv-7>eess.IV (7)</a><ul><li><a href=#165236-mamba-unet-unet-like-pure-visual-mamba-for-medical-image-segmentation-ziyang-wang-et-al-2024>(165/236) Mamba-UNet: UNet-Like Pure Visual Mamba for Medical Image Segmentation (Ziyang Wang et al., 2024)</a></li><li><a href=#166236-troublemaker-learning-for-low-light-image-enhancement-yinghao-song-et-al-2024>(166/236) Troublemaker Learning for Low-Light Image Enhancement (Yinghao Song et al., 2024)</a></li><li><a href=#167236-triplet-constraint-transformer-with-multi-scale-refinement-for-dose-prediction-in-radiotherapy-lu-wen-et-al-2024>(167/236) Triplet-constraint Transformer with Multi-scale Refinement for Dose Prediction in Radiotherapy (Lu Wen et al., 2024)</a></li><li><a href=#168236-mirt-a-simultaneous-reconstruction-and-affine-motion-compensation-technique-for-four-dimensional-computed-tomography-4dct-anh-tuan-nguyen-et-al-2024>(168/236) MIRT: a simultaneous reconstruction and affine motion compensation technique for four dimensional computed tomography (4DCT) (Anh-Tuan Nguyen et al., 2024)</a></li><li><a href=#169236-self-calibrated-convolution-towards-glioma-segmentation-felipe-c-r-salvagnini-et-al-2024>(169/236) Self-calibrated convolution towards glioma segmentation (Felipe C. R. Salvagnini et al., 2024)</a></li><li><a href=#170236-anatomically-controllable-medical-image-generation-with-segmentation-guided-diffusion-models-nicholas-konz-et-al-2024>(170/236) Anatomically-Controllable Medical Image Generation with Segmentation-Guided Diffusion Models (Nicholas Konz et al., 2024)</a></li><li><a href=#171236-cortical-surface-diffusion-generative-models-zhenshan-xie-et-al-2024>(171/236) Cortical Surface Diffusion Generative Models (Zhenshan Xie et al., 2024)</a></li></ul></li><li><a href=#cshc-6>cs.HC (6)</a><ul><li><a href=#172236-chatbots-in-knowledge-intensive-contexts-comparing-intent-and-llm-based-systems-samuel-kernan-freire-et-al-2024>(172/236) Chatbots in Knowledge-Intensive Contexts: Comparing Intent and LLM-Based Systems (Samuel Kernan Freire et al., 2024)</a></li><li><a href=#173236-cataractbot-an-llm-powered-expert-in-the-loop-chatbot-for-cataract-patients-pragnya-ramjee-et-al-2024>(173/236) CataractBot: An LLM-Powered Expert-in-the-Loop Chatbot for Cataract Patients (Pragnya Ramjee et al., 2024)</a></li><li><a href=#174236-large-language-user-interfaces-voice-interactive-user-interfaces-powered-by-llms-syed-mekael-wasti-et-al-2024>(174/236) Large Language User Interfaces: Voice Interactive User Interfaces powered by LLMs (Syed Mekael Wasti et al., 2024)</a></li><li><a href=#175236-chatscratch-an-ai-augmented-system-toward-autonomous-visual-programming-learning-for-children-aged-6-12-liuqing-chen-et-al-2024>(175/236) ChatScratch: An AI-Augmented System Toward Autonomous Visual Programming Learning for Children Aged 6-12 (Liuqing Chen et al., 2024)</a></li><li><a href=#176236-giving-robots-a-voice-human-in-the-loop-voice-creation-and-open-ended-labeling-pol-van-rijn-et-al-2024>(176/236) Giving Robots a Voice: Human-in-the-Loop Voice Creation and open-ended Labeling (Pol van Rijn et al., 2024)</a></li><li><a href=#177236-charting-the-covid-long-haul-experience----a-longitudinal-exploration-of-symptoms-activity-and-clinical-adherence-jessica-pater-et-al-2024>(177/236) Charting the COVID Long Haul Experience &ndash; A Longitudinal Exploration of Symptoms, Activity, and Clinical Adherence (Jessica Pater et al., 2024)</a></li></ul></li><li><a href=#csai-10>cs.AI (10)</a><ul><li><a href=#178236-sparql-generation-an-analysis-on-fine-tuning-openllama-for-question-answering-over-a-life-science-knowledge-graph-julio-c-rangel-et-al-2024>(178/236) SPARQL Generation: an analysis on fine-tuning OpenLLaMA for Question Answering over a Life Science Knowledge Graph (Julio C. Rangel et al., 2024)</a></li><li><a href=#179236-can-large-language-model-agents-simulate-human-trust-behaviors-chengxing-xie-et-al-2024>(179/236) Can Large Language Model Agents Simulate Human Trust Behaviors? (Chengxing Xie et al., 2024)</a></li><li><a href=#180236-three-pathways-to-neurosymbolic-reinforcement-learning-with-interpretable-model-and-policy-networks-peter-graf-et-al-2024>(180/236) Three Pathways to Neurosymbolic Reinforcement Learning with Interpretable Model and Policy Networks (Peter Graf et al., 2024)</a></li><li><a href=#181236-codeit-self-improving-language-models-with-prioritized-hindsight-replay-natasha-butt-et-al-2024>(181/236) CodeIt: Self-Improving Language Models with Prioritized Hindsight Replay (Natasha Butt et al., 2024)</a></li><li><a href=#182236-explaining-learned-reward-functions-with-counterfactual-trajectories-jan-wehner-et-al-2024>(182/236) Explaining Learned Reward Functions with Counterfactual Trajectories (Jan Wehner et al., 2024)</a></li><li><a href=#183236-direct-language-model-alignment-from-online-ai-feedback-shangmin-guo-et-al-2024>(183/236) Direct Language Model Alignment from Online AI Feedback (Shangmin Guo et al., 2024)</a></li><li><a href=#184236-s-agents-self-organizing-agents-in-open-ended-environment-jiaqi-chen-et-al-2024>(184/236) S-Agents: self-organizing agents in open-ended environment (Jiaqi Chen et al., 2024)</a></li><li><a href=#185236-the-strain-of-success-a-predictive-model-for-injury-risk-mitigation-and-team-success-in-soccer-gregory-everett-et-al-2024>(185/236) The Strain of Success: A Predictive Model for Injury Risk Mitigation and Team Success in Soccer (Gregory Everett et al., 2024)</a></li><li><a href=#186236-a-unified-framework-for-probabilistic-verification-of-ai-systems-via-weighted-model-integration-paolo-morettin-et-al-2024>(186/236) A Unified Framework for Probabilistic Verification of AI Systems via Weighted Model Integration (Paolo Morettin et al., 2024)</a></li><li><a href=#187236-advancing-explainable-ai-toward-human-like-intelligence-forging-the-path-to-artificial-brain-yongchen-zhou-et-al-2024>(187/236) Advancing Explainable AI Toward Human-Like Intelligence: Forging the Path to Artificial Brain (Yongchen Zhou et al., 2024)</a></li></ul></li><li><a href=#cond-matmtrl-sci-1>cond-mat.mtrl-sci (1)</a><ul><li><a href=#188236-are-llms-ready-for-real-world-materials-discovery-santiago-miret-et-al-2024>(188/236) Are LLMs Ready for Real-World Materials Discovery? (Santiago Miret et al., 2024)</a></li></ul></li><li><a href=#csgr-1>cs.GR (1)</a><ul><li><a href=#189236-m2fnet-multi-modal-forest-monitoring-network-on-large-scale-virtual-dataset-yawen-lu-et-al-2024>(189/236) M2fNet: Multi-modal Forest Monitoring Network on Large-scale Virtual Dataset (Yawen Lu et al., 2024)</a></li></ul></li><li><a href=#q-biobm-2>q-bio.BM (2)</a><ul><li><a href=#190236-alphafold-meets-flow-matching-for-generating-protein-ensembles-bowen-jing-et-al-2024>(190/236) AlphaFold Meets Flow Matching for Generating Protein Ensembles (Bowen Jing et al., 2024)</a></li><li><a href=#191236-structure-informed-protein-language-model-zuobai-zhang-et-al-2024>(191/236) Structure-Informed Protein Language Model (Zuobai Zhang et al., 2024)</a></li></ul></li><li><a href=#cssd-2>cs.SD (2)</a><ul><li><a href=#192236-fast-timing-conditioned-latent-audio-diffusion-zach-evans-et-al-2024>(192/236) Fast Timing-Conditioned Latent Audio Diffusion (Zach Evans et al., 2024)</a></li><li><a href=#193236-review-of-cetaceans-click-detection-algorithms-mak-gracic-et-al-2024>(193/236) Review of Cetacean&rsquo;s click detection algorithms (Mak Gracic et al., 2024)</a></li></ul></li><li><a href=#csni-3>cs.NI (3)</a><ul><li><a href=#194236-an-advanced-scheme-for-queue-management-intcpip-networks-abderrahmane-boudi-et-al-2024>(194/236) An advanced scheme for queue management inTCP/IP networks (Abderrahmane Boudi et al., 2024)</a></li><li><a href=#195236-a-deep-reinforcement-learning-approach-for-adaptive-traffic-routing-in-next-gen-networks-akshita-abrol-et-al-2024>(195/236) A Deep Reinforcement Learning Approach for Adaptive Traffic Routing in Next-gen Networks (Akshita Abrol et al., 2024)</a></li><li><a href=#196236-splitsim-large-scale-simulations-for-evaluating-network-systems-research-hejing-li-et-al-2024>(196/236) SplitSim: Large-Scale Simulations for Evaluating Network Systems Research (Hejing Li et al., 2024)</a></li></ul></li><li><a href=#csit-4>cs.IT (4)</a><ul><li><a href=#197236-detection-schemes-with-low-resolution-adcs-and-spatial-oversampling-for-transmission-with-higher-order-constellations-in-the-terahertz-band-christian-forsch-et-al-2024>(197/236) Detection Schemes with Low-Resolution ADCs and Spatial Oversampling for Transmission with Higher-Order Constellations in the Terahertz Band (Christian Forsch et al., 2024)</a></li><li><a href=#198236-near-optimal-generalized-decoding-of-polar-like-codes-peihong-yuan-et-al-2024>(198/236) Near-Optimal Generalized Decoding of Polar-like Codes (Peihong Yuan et al., 2024)</a></li><li><a href=#199236-fast-beam-training-for-near-field-communication-systems-yuan-xu-et-al-2024>(199/236) Fast Beam Training for Near-Field Communication Systems (Yuan Xu et al., 2024)</a></li><li><a href=#200236-outer-code-designs-for-augmented-and-local-global-polar-code-architectures-ziyuan-zhu-et-al-2024>(200/236) Outer Code Designs for Augmented and Local-Global Polar Code Architectures (Ziyuan Zhu et al., 2024)</a></li></ul></li><li><a href=#eesssy-5>eess.SY (5)</a><ul><li><a href=#201236-gaussian-process-based-nonlinear-moving-horizon-estimation-tobias-m-wolff-et-al-2024>(201/236) Gaussian Process-Based Nonlinear Moving Horizon Estimation (Tobias M. Wolff et al., 2024)</a></li><li><a href=#202236-control-of-ac-ac-interlinking-converters-for-multi-grids-jeremy-watson-et-al-2024>(202/236) Control of AC-AC interlinking converters for multi-grids (Jeremy Watson et al., 2024)</a></li><li><a href=#203236-ascent-a-context-aware-spectrum-coexistence-design-and-implementation-toolset-for-policymakers-in-satellite-bands-ta-seen-reaz-niloy-et-al-2024>(203/236) ASCENT: A Context-Aware Spectrum Coexistence Design and Implementation Toolset for Policymakers in Satellite Bands (Ta-seen Reaz Niloy et al., 2024)</a></li><li><a href=#204236-reconfigurable-intelligent-surface-for-industrial-automation-mmwave-propagation-measurement-simulation-and-control-algorithm-requirements-hamed-radpour-et-al-2024>(204/236) Reconfigurable Intelligent Surface for Industrial Automation: mmWave Propagation Measurement, Simulation, and Control Algorithm Requirements (Hamed Radpour et al., 2024)</a></li><li><a href=#205236-adaptive-smooth-control-via-nonsingular-fast-terminal-sliding-mode-for-distributed-space-telescope-demonstration-mission-by-cubesat-formation-flying-soobin-jeon-et-al-2024>(205/236) Adaptive Smooth Control via Nonsingular Fast Terminal Sliding Mode for Distributed Space Telescope Demonstration Mission by CubeSat Formation Flying (Soobin Jeon et al., 2024)</a></li></ul></li><li><a href=#mathna-2>math.NA (2)</a><ul><li><a href=#206236-early-stopping-of-untrained-convolutional-neural-networks-tim-jahn-et-al-2024>(206/236) Early Stopping of Untrained Convolutional Neural Networks (Tim Jahn et al., 2024)</a></li><li><a href=#207236-an-efficient-unconditional-energy-stable-scheme-for-the-simulation-of-droplet-formation-jinpeng-zhang-et-al-2024>(207/236) An efficient unconditional energy stable scheme for the simulation of droplet formation (Jinpeng Zhang et al., 2024)</a></li></ul></li><li><a href=#physicschem-ph-1>physics.chem-ph (1)</a><ul><li><a href=#208236-an-artificial-intelligence-ai-workflow-for-catalyst-design-and-optimization-nung-siong-lai-et-al-2024>(208/236) An Artificial Intelligence (AI) workflow for catalyst design and optimization (Nung Siong Lai et al., 2024)</a></li></ul></li><li><a href=#mathst-1>math.ST (1)</a><ul><li><a href=#209236-online-quantile-regression-yinan-shen-et-al-2024>(209/236) Online Quantile Regression (Yinan Shen et al., 2024)</a></li></ul></li><li><a href=#statml-7>stat.ML (7)</a><ul><li><a href=#210236-gradient-descent-induces-alignment-between-weights-and-the-empirical-ntk-for-deep-non-linear-networks-daniel-beaglehole-et-al-2024>(210/236) Gradient descent induces alignment between weights and the empirical NTK for deep non-linear networks (Daniel Beaglehole et al., 2024)</a></li><li><a href=#211236-meta-learning-the-mirror-map-in-policy-mirror-descent-carlo-alfano-et-al-2024>(211/236) Meta-learning the mirror map in policy mirror descent (Carlo Alfano et al., 2024)</a></li><li><a href=#212236-learning-operators-with-stochastic-gradient-descent-in-general-hilbert-spaces-lei-shi-et-al-2024>(212/236) Learning Operators with Stochastic Gradient Descent in General Hilbert Spaces (Lei Shi et al., 2024)</a></li><li><a href=#213236-riemann-lebesgue-forest-for-regression-tian-qin-et-al-2024>(213/236) Riemann-Lebesgue Forest for Regression (Tian Qin et al., 2024)</a></li><li><a href=#214236-a-primal-dual-algorithm-for-offline-constrained-reinforcement-learning-with-low-rank-mdps-kihyuk-hong-et-al-2024>(214/236) A Primal-Dual Algorithm for Offline Constrained Reinforcement Learning with Low-Rank MDPs (Kihyuk Hong et al., 2024)</a></li><li><a href=#215236-generalized-sobolev-transport-for-probability-measures-on-a-graph-tam-le-et-al-2024>(215/236) Generalized Sobolev Transport for Probability Measures on a Graph (Tam Le et al., 2024)</a></li><li><a href=#216236-generative-flows-on-discrete-state-spaces-enabling-multimodal-flows-with-applications-to-protein-co-design-andrew-campbell-et-al-2024>(216/236) Generative Flows on Discrete State-Spaces: Enabling Multimodal Flows with Applications to Protein Co-Design (Andrew Campbell et al., 2024)</a></li></ul></li><li><a href=#csdc-2>cs.DC (2)</a><ul><li><a href=#217236-criu----checkpoint-restore-in-userspace-for-computational-simulations-and-scientific-applications-fabio-andrijauskas-et-al-2024>(217/236) CRIU &ndash; Checkpoint Restore in Userspace for computational simulations and scientific applications (Fabio Andrijauskas et al., 2024)</a></li><li><a href=#218236-leveraging-knowledge-as-a-service-kaas-for-qos-aware-resource-management-in-multi-user-video-transcoding-luis-costero-et-al-2024>(218/236) Leveraging knowledge-as-a-service (KaaS) for QoS-aware resource management in multi-user video transcoding (Luis Costero et al., 2024)</a></li></ul></li><li><a href=#statme-2>stat.ME (2)</a><ul><li><a href=#219236-stochastic-modeling-of-random-access-memories-reset-transitions-m-carmen-aguilera-morillo-et-al-2024>(219/236) Stochastic modeling of Random Access Memories reset transitions (M Carmen Aguilera-Morillo et al., 2024)</a></li><li><a href=#220236-data-driven-bayesian-estimation-of-monod-kinetics-kévin-colin-et-al-2024>(220/236) Data-driven Bayesian estimation of Monod kinetics (Kévin Colin et al., 2024)</a></li></ul></li><li><a href=#physicsflu-dyn-2>physics.flu-dyn (2)</a><ul><li><a href=#221236-jax-fluids-20-towards-hpc-for-differentiable-cfd-of-compressible-two-phase-flows-deniz-a-bezgin-et-al-2024>(221/236) JAX-Fluids 2.0: Towards HPC for Differentiable CFD of Compressible Two-phase Flows (Deniz A. Bezgin et al., 2024)</a></li><li><a href=#222236-multiscale-modelling-with-physics-informed-neural-network-from-large-scale-dynamics-to-small-scale-predictions-in-complex-systems-jing-wang-et-al-2024>(222/236) Multiscale Modelling with Physics-informed Neural Network: from Large-scale Dynamics to Small-scale Predictions in Complex Systems (Jing Wang et al., 2024)</a></li></ul></li><li><a href=#cscr-4>cs.CR (4)</a><ul><li><a href=#223236-threats-and-limitations-of-terrestrial-broadcast-attacks-benjamin-michele-et-al-2024>(223/236) Threats and Limitations of Terrestrial Broadcast Attacks (Benjamin Michele et al., 2024)</a></li><li><a href=#224236-understanding-practical-membership-privacy-of-deep-learning-marlon-tobaben-et-al-2024>(224/236) Understanding Practical Membership Privacy of Deep Learning (Marlon Tobaben et al., 2024)</a></li><li><a href=#225236-adversarial-robustness-through-artifact-design-tsufit-shua-et-al-2024>(225/236) Adversarial Robustness Through Artifact Design (Tsufit Shua et al., 2024)</a></li><li><a href=#226236-ransomware-detection-dynamics-insights-and-implications-mike-nkongolo-2024>(226/236) Ransomware Detection Dynamics: Insights and Implications (Mike Nkongolo, 2024)</a></li></ul></li><li><a href=#csma-2>cs.MA (2)</a><ul><li><a href=#227236-quantifying-population-exposure-to-long-term-pm10-a-city-wide-agent-based-assessment-hyesop-shin-2024>(227/236) Quantifying Population Exposure to Long-term PM10: A City-wide Agent-based Assessment (Hyesop Shin, 2024)</a></li><li><a href=#228236-towards-generalizability-of-multi-agent-reinforcement-learning-in-graphs-with-recurrent-message-passing-jannis-weil-et-al-2024>(228/236) Towards Generalizability of Multi-Agent Reinforcement Learning in Graphs with Recurrent Message Passing (Jannis Weil et al., 2024)</a></li></ul></li><li><a href=#csne-2>cs.NE (2)</a><ul><li><a href=#229236-a-computational-approach-to-visual-ecology-with-deep-reinforcement-learning-sacha-sokoloski-et-al-2024>(229/236) A computational approach to visual ecology with deep reinforcement learning (Sacha Sokoloski et al., 2024)</a></li><li><a href=#230236-a-bandit-approach-with-evolutionary-operators-for-model-selection-margaux-brégère-et-al-2024>(230/236) A Bandit Approach with Evolutionary Operators for Model Selection (Margaux Brégère et al., 2024)</a></li></ul></li><li><a href=#csfl-1>cs.FL (1)</a><ul><li><a href=#231236-distributed-fair-assignment-and-rebalancing-for-mobility-on-demand-systems-via-an-auction-based-method-kaier-liang-et-al-2024>(231/236) Distributed Fair Assignment and Rebalancing for Mobility-on-Demand Systems via an Auction-based Method (Kaier Liang et al., 2024)</a></li></ul></li><li><a href=#csdl-1>cs.DL (1)</a><ul><li><a href=#232236-hierarchical-tree-structured-knowledge-graph-for-academic-insight-survey-jinghong-li-et-al-2024>(232/236) Hierarchical Tree-structured Knowledge Graph For Academic Insight Survey (Jinghong Li et al., 2024)</a></li></ul></li><li><a href=#mathfa-1>math.FA (1)</a><ul><li><a href=#233236-stochastic-data-driven-bouligand-landweber-method-for-solving-non-smooth-inverse-problems-harshit-bajpai-et-al-2024>(233/236) Stochastic Data-Driven Bouligand Landweber Method for Solving Non-smooth Inverse Problems (Harshit Bajpai et al., 2024)</a></li></ul></li><li><a href=#quant-ph-2>quant-ph (2)</a><ul><li><a href=#234236-continuous-variable-qkd-with-key-rates-far-above-devetak-winter-arpan-akash-ray-et-al-2024>(234/236) Continuous-Variable QKD with key rates far above Devetak-Winter (Arpan Akash Ray et al., 2024)</a></li><li><a href=#235236-gaussian-process-regression-based-method-for-the-localization-of-exceptional-points-in-complex-resonance-spectra-patrick-egenlauf-et-al-2024>(235/236) Gaussian-process-regression-based method for the localization of exceptional points in complex resonance spectra (Patrick Egenlauf et al., 2024)</a></li></ul></li><li><a href=#csgt-1>cs.GT (1)</a><ul><li><a href=#236236-no-transaction-fees-no-problem-achieving-fairness-in-transaction-fee-mechanism-design-sankarshan-damle-et-al-2024>(236/236) No Transaction Fees? No Problem! Achieving Fairness in Transaction Fee Mechanism Design (Sankarshan Damle et al., 2024)</a></li></ul></li></ul></nav></div></div></section></div><footer id=footer class="container-fluid text-center align-content-center footer pb-2"><div class="container pt-5"><div class="row text-left"><div class="col-md-4 col-sm-12"><h5>Navigation</h5><ul><li class=nav-item><a class=smooth-scroll href=https://akitenkrad.github.io/akitenkrad-blog/#about>About</a></li><li class=nav-item><a class=smooth-scroll href=https://akitenkrad.github.io/akitenkrad-blog/#recent-posts>Recent Posts</a></li></ul></div><div class="col-md-4 col-sm-12"><h5>Contact me:</h5><ul><li><a href=mailto:contact.to.akitenkrad@gmail.com target=_blank rel=noopener><span><i class="fas fa-envelope"></i></span> <span>contact.to.akitenkrad@gmail.com</span></a></li></ul></div></div></div><hr><div class=container><div class="row text-left"><div class=col-md-4><a id=theme href=https://github.com/hossainemruz/toha target=_blank rel=noopener><img src=/akitenkrad-blog/images/theme-logo_hu8376fd15465fef26ffe66b6bcf0ca686_13669_32x0_resize_box_3.png alt="Toha Theme Logo">
Toha</a></div><div class="col-md-4 text-center">© 2020 Akitenkrad.</div><div class="col-md-4 text-right"><a id=hugo href=https://gohugo.io/ target=_blank rel=noopener>Powered by
<img src=/akitenkrad-blog/images/hugo-logo.svg alt="Hugo Logo" height=18></a></div></div></div></footer><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/jquery-3.4.1.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/popper.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/bootstrap.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/navbar.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/plyr.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/main.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js></script><script src=https://akitenkrad.github.io/akitenkrad-blog/js/single.js></script><script>hljs.initHighlightingOnLoad()</script><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/katex/katex.min.css><script type=text/javascript defer src=https://akitenkrad.github.io/akitenkrad-blog/katex/katex.min.js></script><script type=text/javascript defer src=https://akitenkrad.github.io/akitenkrad-blog/katex/auto-render.min.js></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\[",right:"\\]",display:!0},{left:"\\(",right:"\\)",display:!1}],throwOnError:!0})})</script></body></html>