<!doctype html><html><head><title>arXiv @ 2024.02.16</title>
<meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/bootstrap.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/layouts/main.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/navigators/navbar.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/plyr.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/flag-icon.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/custom/style.css><link href=https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css rel=stylesheet integrity=sha384-9ndCyUaIbzAi2FUVXJi0CjmCapSmO7SnpJef0486qhLnuZ2cdeRhO02iuK6FUUVM crossorigin=anonymous><script async src=https://cdn.jsdelivr.net/npm/es-module-shims@1/dist/es-module-shims.min.js crossorigin=anonymous></script><script type=importmap>
{
  "imports": {
    "@popperjs/core": "https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.8/dist/esm/popper.min.js",
    "bootstrap": "https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.esm.min.js"
  }
}
</script><script src=https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js integrity=sha384-geWF76RCwLtnZ8qwWowPQNguL3RmwHVBC9FhGdlKrxdiJJigb/j/68SIy3Te4Bkz crossorigin=anonymous></script><script type=module src=https://akitenkrad.github.io/akitenkrad-blog/js/custom/custom.js></script><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Muli:wght@300;400;500;600"><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/fontawesome/css/all.min.css><link rel=icon type=image/png href=https://akitenkrad.github.io/akitenkrad-blog/images/favicons/favicon-96x96_huf1ee13f0caf27d1547f91fb46207d708_13005_42x0_resize_box_3.png><meta property="og:title" content="arXiv @ 2024.02.16"><meta property="og:description" content="Primary Categories astro-ph.SR (1) cs.AI (27) cs.AR (1) cs.CE (1) cs.CL (31) cs.CR (7) cs.CV (36) cs.CY (1) cs.DB (1) cs.DC (5) cs.DM (2) cs.DS (2) cs.FL (1) cs.GT (1) cs.HC (6) cs.IR (5) cs.IT (2) cs.LG (57) cs.LO (5) cs.MA (2) cs.MM (1) cs.RO (10) cs.SD (3) cs.SE (8) cs.SI (4) econ.EM (1) econ.TH (1) eess.AS (4) eess.IV (5) eess.SP (3) eess.SY (5) hep-ph (1) math.CO (1) math.NA (8) physics."><meta property="og:type" content="article"><meta property="og:url" content="https://akitenkrad.github.io/akitenkrad-blog/posts/arxiv/202402/20240216000000/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-02-16T00:00:00+00:00"><meta property="article:modified_time" content="2024-02-16T00:00:00+00:00"><meta name=description content="arXiv @ 2024.02.16"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/atom-one-dark.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/layouts/single.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/navigators/sidebar.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/style.css><script type=text/javascript src=//ajax.googleapis.com/ajax/libs/jquery/3.1.1/jquery.min.js></script><script language=JavaScript>$(document).ready(function(){$("a[href^='http']:not([href*='"+location.hostname+"'])").attr("target","_blank")})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-1MYYZQG0WE"></script><script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-1MYYZQG0WE",{anonymize_ip:!1})}</script></head><body data-spy=scroll data-target=#TableOfContents data-offset=80><div class="container-fluid bg-dimmed wrapper"><nav class="navbar navbar-expand-xl top-navbar final-navbar shadow" style=position:fixed><div class=container><button class="navbar-toggler navbar-light navbar-for-sidebar" id=sidebar-toggler type=button onclick=toggleSidebar()>
<span class=navbar-toggler-icon></span>
</button>
<button class="navbar-toggler navbar-light" id=toc-toggler type=button onclick=toggleTOC()>
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse lang-selector" id=top-nav-items><ul class="navbar-nav mr-auto"><li class=nav-item><a class=navbar-brand href=/akitenkrad-blog><img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png alt=Logo>
Akitenkrad's Blog</a></li></ul><ul class="navbar-nav ml-auto"><li class=nav-item><a class=nav-link href=/akitenkrad-blog#home>Home</a></li><li class=nav-item><a class=nav-link href=/akitenkrad-blog#about>About</a></li><li class=nav-item><a class=nav-link href=/akitenkrad-blog#recent-posts>Recent Posts</a></li><li class=nav-item><a class=nav-link id=blog-link href=https://akitenkrad.github.io/akitenkrad-blog/posts>Posts</a></li><li class=nav-item><a class=nav-link id=tag-link href=https://akitenkrad.github.io/akitenkrad-blog/tags>Tags</a></li></ul></div></div><img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png class=d-none id=main-logo alt=Logo>
<img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png class=d-none id=inverted-logo alt="Inverted Logo"></nav><section class=sidebar-section id=sidebar-section><div class=sidebar-holder><div class=sidebar id=sidebar><form class=mx-auto method=get action=/akitenkrad-blog/search><input type=text name=keyword placeholder=Search data-search id=search-box></form><div class=sidebar-tree><ul class=tree id=tree><li id=list-heading><a href=/posts data-filter=all>Posts</a></li><div class=subtree><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/>Papers</a><ul><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202205/>2022.05</a><ul><li><a href=/akitenkrad-blog/posts/papers/202205/20220518224923/ title="A Context-Aware Citation Recommendation Model with BERT and Graph Convolutional Networks">A Context-Aware Citation Recommendation Model with BERT and Graph Convolutional Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220508162318/ title="A Deep Cascade Model for Multi-Document Reading Comprehension">A Deep Cascade Model for Multi-Document Reading Comprehension</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220506021208/ title="A Primer in BERTology: What We Know About How BERT Works">A Primer in BERTology: What We Know About How BERT Works</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220529131339/ title="Attention Is All You Need">Attention Is All You Need</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220503010000/ title="DeBERTa: Decoding-Enhanced BERT with Disentangled Attention">DeBERTa: Decoding-Enhanced BERT with Disentangled Attention</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220505222900/ title="Dense Passage Retrieval for Open-Domain Question Answering">Dense Passage Retrieval for Open-Domain Question Answering</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220511010217/ title="Multi-Style Generative Reading Comprehension">Multi-Style Generative Reading Comprehension</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220530102936/ title="Neural Machine Translation of Rare Words with Subword Units">Neural Machine Translation of Rare Words with Subword Units</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220523223206/ title="RoBERTa: A Robustly Optimized BERT Pretraining Approach">RoBERTa: A Robustly Optimized BERT Pretraining Approach</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220520124748/ title="Semi-Supervised Classification with Graph Convolutional Networks">Semi-Supervised Classification with Graph Convolutional Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220509110738/ title="Survey on graph embeddings and their applications to machine learning problems on graphs">Survey on graph embeddings and their applications to machine learning problems on graphs</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220514151839/ title="UnitedQA: A Hybrid Approach for Open Domain Question Answering">UnitedQA: A Hybrid Approach for Open Domain Question Answering</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202206/>2022.06</a><ul><li><a href=/akitenkrad-blog/posts/papers/202206/20220612105422/ title="Attributed Network Embedding for Learning in a Dynamic Environment">Attributed Network Embedding for Learning in a Dynamic Environment</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220608085622/ title="CodeBERT: A Pre-Trained Model for Programming and Natural Languages">CodeBERT: A Pre-Trained Model for Programming and Natural Languages</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220618223844/ title="High-order Proximity Preserved Embedding for Dynamic Networks">High-order Proximity Preserved Embedding for Dynamic Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220602171700/ title="S-Net: From Answer Extraction to Answer Generation for Machine Reading Comprehension">S-Net: From Answer Extraction to Answer Generation for Machine Reading Comprehension</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202207/>2022.07</a><ul><li><a href=/akitenkrad-blog/posts/papers/202207/20220727145036/ title="Dynamic Heterogeneous Graph Embedding Using Hierarchical Attentions">Dynamic Heterogeneous Graph Embedding Using Hierarchical Attentions</a></li><li><a href=/akitenkrad-blog/posts/papers/202207/20220726163444/ title="Dynamic Network Embedding Survey">Dynamic Network Embedding Survey</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202208/>2022.08</a><ul><li><a href=/akitenkrad-blog/posts/papers/202208/20220802103319/ title="Modeling Dynamic Heterogeneous Network for Link Prediction using Hierarchical Attention with Temporal RNN">Modeling Dynamic Heterogeneous Network for Link Prediction using Hierarchical Attention with Temporal RNN</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202209/>2022.09</a><ul><li><a href=/akitenkrad-blog/posts/papers/202209/20220909180827/ title="Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting">Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202305/>2023.05</a><ul><li><a href=/akitenkrad-blog/posts/papers/202305/20230513094804/ title="Improving Language Understanding by Generative Pre-Training">Improving Language Understanding by Generative Pre-Training</a></li><li><a href=/akitenkrad-blog/posts/papers/202305/20230514200241/ title="Language Models are Unsupervised Multitask Learners">Language Models are Unsupervised Multitask Learners</a></li></ul></li><li><a href=/akitenkrad-blog/posts/papers/202306/ title=2023.06>2023.06</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/akitenkrad-blog/posts/arxiv/>arXiv</a><ul class=active><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202307/>2023.07</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230701000000/ title="arXiv @ 2023.07.01">arXiv @ 2023.07.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230702000000/ title="arXiv @ 2023.07.02">arXiv @ 2023.07.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230703000000/ title="arXiv @ 2023.07.03">arXiv @ 2023.07.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230704000000/ title="arXiv @ 2023.07.04">arXiv @ 2023.07.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230705000000/ title="arXiv @ 2023.07.05">arXiv @ 2023.07.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230706000000/ title="arXiv @ 2023.07.06">arXiv @ 2023.07.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230707000000/ title="arXiv @ 2023.07.07">arXiv @ 2023.07.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230708000000/ title="arXiv @ 2023.07.08">arXiv @ 2023.07.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230709000000/ title="arXiv @ 2023.07.09">arXiv @ 2023.07.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230710000000/ title="arXiv @ 2023.07.10">arXiv @ 2023.07.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230711000000/ title="arXiv @ 2023.07.11">arXiv @ 2023.07.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230712000000/ title="arXiv @ 2023.07.12">arXiv @ 2023.07.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230713000000/ title="arXiv @ 2023.07.13">arXiv @ 2023.07.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230714000000/ title="arXiv @ 2023.07.14">arXiv @ 2023.07.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230715000000/ title="arXiv @ 2023.07.15">arXiv @ 2023.07.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230716000000/ title="arXiv @ 2023.07.16">arXiv @ 2023.07.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230717000000/ title="arXiv @ 2023.07.17">arXiv @ 2023.07.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230718000000/ title="arXiv @ 2023.07.18">arXiv @ 2023.07.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230719000000/ title="arXiv @ 2023.07.19">arXiv @ 2023.07.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230720000000/ title="arXiv @ 2023.07.20">arXiv @ 2023.07.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230721000000/ title="arXiv @ 2023.07.21">arXiv @ 2023.07.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230722000000/ title="arXiv @ 2023.07.22">arXiv @ 2023.07.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230723000000/ title="arXiv @ 2023.07.23">arXiv @ 2023.07.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230724000000/ title="arXiv @ 2023.07.24">arXiv @ 2023.07.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230725000000/ title="arXiv @ 2023.07.25">arXiv @ 2023.07.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230726000000/ title="arXiv @ 2023.07.26">arXiv @ 2023.07.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230727000000/ title="arXiv @ 2023.07.27">arXiv @ 2023.07.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230728000000/ title="arXiv @ 2023.07.28">arXiv @ 2023.07.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230729000000/ title="arXiv @ 2023.07.29">arXiv @ 2023.07.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230730000000/ title="arXiv @ 2023.07.30">arXiv @ 2023.07.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230731000000/ title="arXiv @ 2023.07.31">arXiv @ 2023.07.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202308/>2023.08</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230801000000/ title="arXiv @ 2023.08.01">arXiv @ 2023.08.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230802000000/ title="arXiv @ 2023.08.02">arXiv @ 2023.08.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230803000000/ title="arXiv @ 2023.08.03">arXiv @ 2023.08.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230804000000/ title="arXiv @ 2023.08.04">arXiv @ 2023.08.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230805000000/ title="arXiv @ 2023.08.05">arXiv @ 2023.08.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230806000000/ title="arXiv @ 2023.08.06">arXiv @ 2023.08.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230807000000/ title="arXiv @ 2023.08.07">arXiv @ 2023.08.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230808000000/ title="arXiv @ 2023.08.08">arXiv @ 2023.08.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230809000000/ title="arXiv @ 2023.08.09">arXiv @ 2023.08.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230810000000/ title="arXiv @ 2023.08.10">arXiv @ 2023.08.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230811000000/ title="arXiv @ 2023.08.11">arXiv @ 2023.08.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230812000000/ title="arXiv @ 2023.08.12">arXiv @ 2023.08.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230813000000/ title="arXiv @ 2023.08.13">arXiv @ 2023.08.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230814000000/ title="arXiv @ 2023.08.14">arXiv @ 2023.08.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230815000000/ title="arXiv @ 2023.08.15">arXiv @ 2023.08.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230816000000/ title="arXiv @ 2023.08.16">arXiv @ 2023.08.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230817000000/ title="arXiv @ 2023.08.17">arXiv @ 2023.08.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230818000000/ title="arXiv @ 2023.08.18">arXiv @ 2023.08.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230819000000/ title="arXiv @ 2023.08.19">arXiv @ 2023.08.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230820000000/ title="arXiv @ 2023.08.20">arXiv @ 2023.08.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230821000000/ title="arXiv @ 2023.08.21">arXiv @ 2023.08.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230822000000/ title="arXiv @ 2023.08.22">arXiv @ 2023.08.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230823000000/ title="arXiv @ 2023.08.23">arXiv @ 2023.08.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230824000000/ title="arXiv @ 2023.08.24">arXiv @ 2023.08.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230825000000/ title="arXiv @ 2023.08.25">arXiv @ 2023.08.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230826000000/ title="arXiv @ 2023.08.26">arXiv @ 2023.08.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230827000000/ title="arXiv @ 2023.08.27">arXiv @ 2023.08.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230828000000/ title="arXiv @ 2023.08.28">arXiv @ 2023.08.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230829000000/ title="arXiv @ 2023.08.29">arXiv @ 2023.08.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230830000000/ title="arXiv @ 2023.08.30">arXiv @ 2023.08.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230831000000/ title="arXiv @ 2023.08.31">arXiv @ 2023.08.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202309/>2023.09</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230901000000/ title="arXiv @ 2023.09.01">arXiv @ 2023.09.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230902000000/ title="arXiv @ 2023.09.02">arXiv @ 2023.09.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230903000000/ title="arXiv @ 2023.09.03">arXiv @ 2023.09.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230904000000/ title="arXiv @ 2023.09.04">arXiv @ 2023.09.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230905000000/ title="arXiv @ 2023.09.05">arXiv @ 2023.09.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230906000000/ title="arXiv @ 2023.09.06">arXiv @ 2023.09.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230907000000/ title="arXiv @ 2023.09.07">arXiv @ 2023.09.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230908000000/ title="arXiv @ 2023.09.08">arXiv @ 2023.09.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230909000000/ title="arXiv @ 2023.09.09">arXiv @ 2023.09.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230910000000/ title="arXiv @ 2023.09.10">arXiv @ 2023.09.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230911000000/ title="arXiv @ 2023.09.11">arXiv @ 2023.09.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230912000000/ title="arXiv @ 2023.09.12">arXiv @ 2023.09.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230913000000/ title="arXiv @ 2023.09.13">arXiv @ 2023.09.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230914000000/ title="arXiv @ 2023.09.14">arXiv @ 2023.09.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230915000000/ title="arXiv @ 2023.09.15">arXiv @ 2023.09.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230916000000/ title="arXiv @ 2023.09.16">arXiv @ 2023.09.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230917000000/ title="arXiv @ 2023.09.17">arXiv @ 2023.09.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230918000000/ title="arXiv @ 2023.09.18">arXiv @ 2023.09.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230919000000/ title="arXiv @ 2023.09.19">arXiv @ 2023.09.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230920000000/ title="arXiv @ 2023.09.20">arXiv @ 2023.09.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230921000000/ title="arXiv @ 2023.09.21">arXiv @ 2023.09.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230922000000/ title="arXiv @ 2023.09.22">arXiv @ 2023.09.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230923000000/ title="arXiv @ 2023.09.23">arXiv @ 2023.09.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230924000000/ title="arXiv @ 2023.09.24">arXiv @ 2023.09.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230925000000/ title="arXiv @ 2023.09.25">arXiv @ 2023.09.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230926000000/ title="arXiv @ 2023.09.26">arXiv @ 2023.09.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230927000000/ title="arXiv @ 2023.09.27">arXiv @ 2023.09.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230928000000/ title="arXiv @ 2023.09.28">arXiv @ 2023.09.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230929000000/ title="arXiv @ 2023.09.29">arXiv @ 2023.09.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230930000000/ title="arXiv @ 2023.09.30">arXiv @ 2023.09.30</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202310/>2023.1</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231001000000/ title="arXiv @ 2023.10.01">arXiv @ 2023.10.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231002000000/ title="arXiv @ 2023.10.02">arXiv @ 2023.10.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231003000000/ title="arXiv @ 2023.10.03">arXiv @ 2023.10.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231004000000/ title="arXiv @ 2023.10.04">arXiv @ 2023.10.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231005000000/ title="arXiv @ 2023.10.05">arXiv @ 2023.10.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231006000000/ title="arXiv @ 2023.10.06">arXiv @ 2023.10.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231007000000/ title="arXiv @ 2023.10.07">arXiv @ 2023.10.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231008000000/ title="arXiv @ 2023.10.08">arXiv @ 2023.10.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231009000000/ title="arXiv @ 2023.10.09">arXiv @ 2023.10.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231010000000/ title="arXiv @ 2023.10.10">arXiv @ 2023.10.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231011000000/ title="arXiv @ 2023.10.11">arXiv @ 2023.10.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231012000000/ title="arXiv @ 2023.10.12">arXiv @ 2023.10.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231013000000/ title="arXiv @ 2023.10.13">arXiv @ 2023.10.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231014000000/ title="arXiv @ 2023.10.14">arXiv @ 2023.10.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231015000000/ title="arXiv @ 2023.10.15">arXiv @ 2023.10.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231016000000/ title="arXiv @ 2023.10.16">arXiv @ 2023.10.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231017000000/ title="arXiv @ 2023.10.17">arXiv @ 2023.10.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231018000000/ title="arXiv @ 2023.10.18">arXiv @ 2023.10.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231019000000/ title="arXiv @ 2023.10.19">arXiv @ 2023.10.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231020000000/ title="arXiv @ 2023.10.20">arXiv @ 2023.10.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231021000000/ title="arXiv @ 2023.10.21">arXiv @ 2023.10.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231022000000/ title="arXiv @ 2023.10.22">arXiv @ 2023.10.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231023000000/ title="arXiv @ 2023.10.23">arXiv @ 2023.10.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231024000000/ title="arXiv @ 2023.10.24">arXiv @ 2023.10.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231025000000/ title="arXiv @ 2023.10.25">arXiv @ 2023.10.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231026000000/ title="arXiv @ 2023.10.26">arXiv @ 2023.10.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231027000000/ title="arXiv @ 2023.10.27">arXiv @ 2023.10.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231028000000/ title="arXiv @ 2023.10.28">arXiv @ 2023.10.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231029000000/ title="arXiv @ 2023.10.29">arXiv @ 2023.10.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231030000000/ title="arXiv @ 2023.10.30">arXiv @ 2023.10.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231031000000/ title="arXiv @ 2023.10.31">arXiv @ 2023.10.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202311/>2023.11</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231101000000/ title="arXiv @ 2023.11.01">arXiv @ 2023.11.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231102000000/ title="arXiv @ 2023.11.02">arXiv @ 2023.11.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231103000000/ title="arXiv @ 2023.11.03">arXiv @ 2023.11.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231104000000/ title="arXiv @ 2023.11.04">arXiv @ 2023.11.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231105000000/ title="arXiv @ 2023.11.05">arXiv @ 2023.11.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231106000000/ title="arXiv @ 2023.11.06">arXiv @ 2023.11.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231107000000/ title="arXiv @ 2023.11.07">arXiv @ 2023.11.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231108000000/ title="arXiv @ 2023.11.08">arXiv @ 2023.11.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231109000000/ title="arXiv @ 2023.11.09">arXiv @ 2023.11.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231110000000/ title="arXiv @ 2023.11.10">arXiv @ 2023.11.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231111000000/ title="arXiv @ 2023.11.11">arXiv @ 2023.11.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231112000000/ title="arXiv @ 2023.11.12">arXiv @ 2023.11.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231113000000/ title="arXiv @ 2023.11.13">arXiv @ 2023.11.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231114000000/ title="arXiv @ 2023.11.14">arXiv @ 2023.11.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231115000000/ title="arXiv @ 2023.11.15">arXiv @ 2023.11.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231116000000/ title="arXiv @ 2023.11.16">arXiv @ 2023.11.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231117000000/ title="arXiv @ 2023.11.17">arXiv @ 2023.11.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231118000000/ title="arXiv @ 2023.11.18">arXiv @ 2023.11.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231119000000/ title="arXiv @ 2023.11.19">arXiv @ 2023.11.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231120000000/ title="arXiv @ 2023.11.20">arXiv @ 2023.11.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231121000000/ title="arXiv @ 2023.11.21">arXiv @ 2023.11.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231122000000/ title="arXiv @ 2023.11.22">arXiv @ 2023.11.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231123000000/ title="arXiv @ 2023.11.23">arXiv @ 2023.11.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231124000000/ title="arXiv @ 2023.11.24">arXiv @ 2023.11.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231125000000/ title="arXiv @ 2023.11.25">arXiv @ 2023.11.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231126000000/ title="arXiv @ 2023.11.26">arXiv @ 2023.11.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231127000000/ title="arXiv @ 2023.11.27">arXiv @ 2023.11.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231128000000/ title="arXiv @ 2023.11.28">arXiv @ 2023.11.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231129000000/ title="arXiv @ 2023.11.29">arXiv @ 2023.11.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231130000000/ title="arXiv @ 2023.11.30">arXiv @ 2023.11.30</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202312/>2023.12</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231201000000/ title="arXiv @ 2023.12.01">arXiv @ 2023.12.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231202000000/ title="arXiv @ 2023.12.02">arXiv @ 2023.12.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231203000000/ title="arXiv @ 2023.12.03">arXiv @ 2023.12.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231204000000/ title="arXiv @ 2023.12.04">arXiv @ 2023.12.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231205000000/ title="arXiv @ 2023.12.05">arXiv @ 2023.12.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231206000000/ title="arXiv @ 2023.12.06">arXiv @ 2023.12.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231207000000/ title="arXiv @ 2023.12.07">arXiv @ 2023.12.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231208000000/ title="arXiv @ 2023.12.08">arXiv @ 2023.12.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231209000000/ title="arXiv @ 2023.12.09">arXiv @ 2023.12.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231210000000/ title="arXiv @ 2023.12.10">arXiv @ 2023.12.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231211000000/ title="arXiv @ 2023.12.11">arXiv @ 2023.12.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231212000000/ title="arXiv @ 2023.12.12">arXiv @ 2023.12.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231213000000/ title="arXiv @ 2023.12.13">arXiv @ 2023.12.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231214000000/ title="arXiv @ 2023.12.14">arXiv @ 2023.12.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231215000000/ title="arXiv @ 2023.12.15">arXiv @ 2023.12.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231216000000/ title="arXiv @ 2023.12.16">arXiv @ 2023.12.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231217000000/ title="arXiv @ 2023.12.17">arXiv @ 2023.12.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231218000000/ title="arXiv @ 2023.12.18">arXiv @ 2023.12.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231219000000/ title="arXiv @ 2023.12.19">arXiv @ 2023.12.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231220000000/ title="arXiv @ 2023.12.20">arXiv @ 2023.12.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231221000000/ title="arXiv @ 2023.12.21">arXiv @ 2023.12.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231222000000/ title="arXiv @ 2023.12.22">arXiv @ 2023.12.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231223000000/ title="arXiv @ 2023.12.23">arXiv @ 2023.12.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231224000000/ title="arXiv @ 2023.12.24">arXiv @ 2023.12.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231225000000/ title="arXiv @ 2023.12.25">arXiv @ 2023.12.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231226000000/ title="arXiv @ 2023.12.26">arXiv @ 2023.12.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231227000000/ title="arXiv @ 2023.12.27">arXiv @ 2023.12.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231228000000/ title="arXiv @ 2023.12.28">arXiv @ 2023.12.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231229000000/ title="arXiv @ 2023.12.29">arXiv @ 2023.12.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231230000000/ title="arXiv @ 2023.12.30">arXiv @ 2023.12.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231231000000/ title="arXiv @ 2023.12.31">arXiv @ 2023.12.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202401/>2024.01</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240101000000/ title="arXiv @ 2024.01.01">arXiv @ 2024.01.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240102000000/ title="arXiv @ 2024.01.02">arXiv @ 2024.01.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240103000000/ title="arXiv @ 2024.01.03">arXiv @ 2024.01.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240104000000/ title="arXiv @ 2024.01.04">arXiv @ 2024.01.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240105000000/ title="arXiv @ 2024.01.05">arXiv @ 2024.01.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240106000000/ title="arXiv @ 2024.01.06">arXiv @ 2024.01.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240107000000/ title="arXiv @ 2024.01.07">arXiv @ 2024.01.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240108000000/ title="arXiv @ 2024.01.08">arXiv @ 2024.01.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240109000000/ title="arXiv @ 2024.01.09">arXiv @ 2024.01.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240110000000/ title="arXiv @ 2024.01.10">arXiv @ 2024.01.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240111000000/ title="arXiv @ 2024.01.11">arXiv @ 2024.01.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240112000000/ title="arXiv @ 2024.01.12">arXiv @ 2024.01.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240113000000/ title="arXiv @ 2024.01.13">arXiv @ 2024.01.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240114000000/ title="arXiv @ 2024.01.14">arXiv @ 2024.01.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240115000000/ title="arXiv @ 2024.01.15">arXiv @ 2024.01.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240116000000/ title="arXiv @ 2024.01.16">arXiv @ 2024.01.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240117000000/ title="arXiv @ 2024.01.17">arXiv @ 2024.01.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240118000000/ title="arXiv @ 2024.01.18">arXiv @ 2024.01.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240119000000/ title="arXiv @ 2024.01.19">arXiv @ 2024.01.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240120000000/ title="arXiv @ 2024.01.20">arXiv @ 2024.01.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240121000000/ title="arXiv @ 2024.01.21">arXiv @ 2024.01.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240122000000/ title="arXiv @ 2024.01.22">arXiv @ 2024.01.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240123000000/ title="arXiv @ 2024.01.23">arXiv @ 2024.01.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240124000000/ title="arXiv @ 2024.01.24">arXiv @ 2024.01.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240125000000/ title="arXiv @ 2024.01.25">arXiv @ 2024.01.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240126000000/ title="arXiv @ 2024.01.26">arXiv @ 2024.01.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240127000000/ title="arXiv @ 2024.01.27">arXiv @ 2024.01.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240128000000/ title="arXiv @ 2024.01.28">arXiv @ 2024.01.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240129000000/ title="arXiv @ 2024.01.29">arXiv @ 2024.01.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240130000000/ title="arXiv @ 2024.01.30">arXiv @ 2024.01.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240131000000/ title="arXiv @ 2024.01.31">arXiv @ 2024.01.31</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/akitenkrad-blog/posts/arxiv/202402/>2024.02</a><ul class=active><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240201000000/ title="arXiv @ 2024.02.01">arXiv @ 2024.02.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240202000000/ title="arXiv @ 2024.02.02">arXiv @ 2024.02.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240203000000/ title="arXiv @ 2024.02.03">arXiv @ 2024.02.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240204000000/ title="arXiv @ 2024.02.04">arXiv @ 2024.02.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240205000000/ title="arXiv @ 2024.02.05">arXiv @ 2024.02.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240206000000/ title="arXiv @ 2024.02.06">arXiv @ 2024.02.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240207000000/ title="arXiv @ 2024.02.07">arXiv @ 2024.02.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240208000000/ title="arXiv @ 2024.02.08">arXiv @ 2024.02.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240209000000/ title="arXiv @ 2024.02.09">arXiv @ 2024.02.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240210000000/ title="arXiv @ 2024.02.10">arXiv @ 2024.02.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240211000000/ title="arXiv @ 2024.02.11">arXiv @ 2024.02.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240212000000/ title="arXiv @ 2024.02.12">arXiv @ 2024.02.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240213000000/ title="arXiv @ 2024.02.13">arXiv @ 2024.02.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240214000000/ title="arXiv @ 2024.02.14">arXiv @ 2024.02.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240215000000/ title="arXiv @ 2024.02.15">arXiv @ 2024.02.15</a></li><li><a class=active href=/akitenkrad-blog/posts/arxiv/202402/20240216000000/ title="arXiv @ 2024.02.16">arXiv @ 2024.02.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240217000000/ title="arXiv @ 2024.02.17">arXiv @ 2024.02.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240218000000/ title="arXiv @ 2024.02.18">arXiv @ 2024.02.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240219000000/ title="arXiv @ 2024.02.19">arXiv @ 2024.02.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240220000000/ title="arXiv @ 2024.02.20">arXiv @ 2024.02.20</a></li></ul></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/algorithms/>Algorithms</a><ul><li><a href=/akitenkrad-blog/posts/algorithms/bandit-algorithm-basic/ title="Bandit Algorithm Basic">Bandit Algorithm Basic</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/conference/>Conference</a><ul><li><a href=/akitenkrad-blog/posts/conference/acl/ title="Annual Meeting of the Association for Computational Linguistics">Annual Meeting of the Association for Computational Linguistics</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/>Figures</a><ul><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/202208/>2022.08</a><ul><li><a href=/akitenkrad-blog/posts/figures/202208/20220822092748/ title=1ヶ月当たりの実労働時間の推移>1ヶ月当たりの実労働時間の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220813115200/ title=コンビニエンスストアの店舗数の推移>コンビニエンスストアの店舗数の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220821113322/ title=地域別1世帯当たり1ヶ月間の支出額の推移>地域別1世帯当たり1ヶ月間の支出額の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220828111836/ title="夫婦別子供の有無別生活時間 (2016)">夫婦別子供の有無別生活時間 (2016)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220827103615/ title=家族類型の変遷>家族類型の変遷</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220815131016/ title="年齢別大学院入学者数 (2021)">年齢別大学院入学者数 (2021)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220818122628/ title=情報サービス業における企業特殊的人的資本（名目）の推移>情報サービス業における企業特殊的人的資本（名目）の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220824095127/ title=有業者の社会生活における活動の変化>有業者の社会生活における活動の変化</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220814175022/ title=東京23区の乗用車保有台数>東京23区の乗用車保有台数</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220817121117/ title="業種別企業特殊的人的資本 (2018)">業種別企業特殊的人的資本 (2018)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220825094111/ title=無業者の社会生活における活動の変化>無業者の社会生活における活動の変化</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220823104100/ title=睡眠時間の推移>睡眠時間の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220826102652/ title=総人口の推移>総人口の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220820225528/ title=都道府県別国公立別学校数・学生数>都道府県別国公立別学校数・学生数</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220816120845/ title="都道府県別消費者物価指数/教育（全国平均=100）">都道府県別消費者物価指数/教育（全国平均=100）</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220812173814/ title=金沢市の夏の月平均気温の遷移>金沢市の夏の月平均気温の遷移</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/202209/>2022.09</a><ul><li><a href=/akitenkrad-blog/posts/figures/202209/20220907122906/ title="ACL-2022 WordCloud">ACL-2022 WordCloud</a></li></ul></li></ul></li><li><a href=/akitenkrad-blog/posts/latex/ title="Latex Mathematics Syntax Guide">Latex Mathematics Syntax Guide</a></li><li><a href=/akitenkrad-blog/posts/markdown/ title="Markdown Sample">Markdown Sample</a></li></div></ul></div></div></div></section><section class=content-section id=content-section><div class=content><div class="container p-0 read-area"><div class="hero-area col-sm-12" id=hero-area style=background-image:url(/akitenkrad-blog/posts/arxiv/202402/20240216000000/hero.png)></div><div class=page-content><div class="author-profile ml-auto align-self-lg-center"><img class=rounded-circle src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_120x120_fit_box_3.png alt="Author Image"><h5 class=author-name></h5><p>Friday, Feb 16, 2024</p></div><div class=title><h1>arXiv @ 2024.02.16</h1></div><div class=taxonomy-terms><ul><li class=rounded><a href=/akitenkrad-blog/tags/arxiv class="btn, btn-sm">arXiv</a></li><li class=rounded><a href=/akitenkrad-blog/tags/published2024 class="btn, btn-sm">2024</a></li></ul></div><div class=post-content id=post-content><figure style=border:none;width:100%;display:flex;justify-content:center><iframe src=pie.html width=900 height=620 style=border:none></iframe></figure><h2 id=primary-categories>Primary Categories</h2><ul><li><a href=/akitenkrad-blog/#astro-phsr-1>astro-ph.SR (1)</a></li><li><a href=/akitenkrad-blog/#csai-27>cs.AI (27)</a></li><li><a href=/akitenkrad-blog/#csar-1>cs.AR (1)</a></li><li><a href=/akitenkrad-blog/#csce-1>cs.CE (1)</a></li><li><a href=/akitenkrad-blog/#cscl-31>cs.CL (31)</a></li><li><a href=/akitenkrad-blog/#cscr-7>cs.CR (7)</a></li><li><a href=/akitenkrad-blog/#cscv-36>cs.CV (36)</a></li><li><a href=/akitenkrad-blog/#cscy-1>cs.CY (1)</a></li><li><a href=/akitenkrad-blog/#csdb-1>cs.DB (1)</a></li><li><a href=/akitenkrad-blog/#csdc-5>cs.DC (5)</a></li><li><a href=/akitenkrad-blog/#csdm-2>cs.DM (2)</a></li><li><a href=/akitenkrad-blog/#csds-2>cs.DS (2)</a></li><li><a href=/akitenkrad-blog/#csfl-1>cs.FL (1)</a></li><li><a href=/akitenkrad-blog/#csgt-1>cs.GT (1)</a></li><li><a href=/akitenkrad-blog/#cshc-6>cs.HC (6)</a></li><li><a href=/akitenkrad-blog/#csir-5>cs.IR (5)</a></li><li><a href=/akitenkrad-blog/#csit-2>cs.IT (2)</a></li><li><a href=/akitenkrad-blog/#cslg-57>cs.LG (57)</a></li><li><a href=/akitenkrad-blog/#cslo-5>cs.LO (5)</a></li><li><a href=/akitenkrad-blog/#csma-2>cs.MA (2)</a></li><li><a href=/akitenkrad-blog/#csmm-1>cs.MM (1)</a></li><li><a href=/akitenkrad-blog/#csro-10>cs.RO (10)</a></li><li><a href=/akitenkrad-blog/#cssd-3>cs.SD (3)</a></li><li><a href=/akitenkrad-blog/#csse-8>cs.SE (8)</a></li><li><a href=/akitenkrad-blog/#cssi-4>cs.SI (4)</a></li><li><a href=/akitenkrad-blog/#econem-1>econ.EM (1)</a></li><li><a href=/akitenkrad-blog/#econth-1>econ.TH (1)</a></li><li><a href=/akitenkrad-blog/#eessas-4>eess.AS (4)</a></li><li><a href=/akitenkrad-blog/#eessiv-5>eess.IV (5)</a></li><li><a href=/akitenkrad-blog/#eesssp-3>eess.SP (3)</a></li><li><a href=/akitenkrad-blog/#eesssy-5>eess.SY (5)</a></li><li><a href=/akitenkrad-blog/#hep-ph-1>hep-ph (1)</a></li><li><a href=/akitenkrad-blog/#mathco-1>math.CO (1)</a></li><li><a href=/akitenkrad-blog/#mathna-8>math.NA (8)</a></li><li><a href=/akitenkrad-blog/#physicscomp-ph-1>physics.comp-ph (1)</a></li><li><a href=/akitenkrad-blog/#physicsflu-dyn-1>physics.flu-dyn (1)</a></li><li><a href=/akitenkrad-blog/#physicsplasm-ph-1>physics.plasm-ph (1)</a></li><li><a href=/akitenkrad-blog/#physicssoc-ph-1>physics.soc-ph (1)</a></li><li><a href=/akitenkrad-blog/#q-biobm-1>q-bio.BM (1)</a></li><li><a href=/akitenkrad-blog/#quant-ph-2>quant-ph (2)</a></li><li><a href=/akitenkrad-blog/#statml-5>stat.ML (5)</a></li></ul><h2 id=keywords>Keywords</h2><table border=1 class=dataframe><thead><tr style=text-align:right><th>keyword</th><th>cs.AI</th><th>cs.CL</th><th>cs.CV</th><th>cs.LG</th><th>cs.RO</th></tr></thead><tbody><tr><td>AI-generated Text Detection</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Active Learning</td><td>1</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Adversarial Attack</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Adversarial Learning</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Anomaly Detection</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Autoencoder</td><td></td><td></td><td></td><td></td><td>1</td></tr><tr><td>BART</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>BERT</td><td>1</td><td>1</td><td></td><td></td><td></td></tr><tr><td>BLEU</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>BLOOM</td><td>1</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Bandit Algorithm</td><td></td><td></td><td></td><td>4</td><td></td></tr><tr><td>Benchmarking</td><td>4</td><td>10</td><td>8</td><td>8</td><td>2</td></tr><tr><td>Black Box</td><td></td><td>2</td><td>1</td><td>4</td><td>1</td></tr><tr><td>ChatGPT</td><td>2</td><td></td><td></td><td></td><td></td></tr><tr><td>Chatbot</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Clustering</td><td></td><td></td><td></td><td>2</td><td></td></tr><tr><td>Code Generation</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Common-sense Reasoning</td><td></td><td>2</td><td></td><td></td><td></td></tr><tr><td>Constrained Learning</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Continual Learning</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Contrastive Learning</td><td>1</td><td></td><td></td><td>1</td><td></td></tr><tr><td>Convolution</td><td>2</td><td></td><td>7</td><td>3</td><td></td></tr><tr><td>Convolutional Neural Network</td><td>1</td><td></td><td>10</td><td>3</td><td></td></tr><tr><td>Counter-factual</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Curriculum Learning</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Data Augmentation</td><td></td><td></td><td>3</td><td>1</td><td></td></tr><tr><td>Emotion Recognition</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Event Detection</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Face Recognition</td><td></td><td></td><td>2</td><td></td><td></td></tr><tr><td>Fairness</td><td></td><td>1</td><td></td><td>1</td><td></td></tr><tr><td>Fake News Detection</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Federated Learning</td><td></td><td></td><td></td><td>3</td><td></td></tr><tr><td>Few-shot</td><td></td><td>1</td><td>2</td><td></td><td>1</td></tr><tr><td>Few-shot Learning</td><td></td><td></td><td></td><td></td><td>1</td></tr><tr><td>Fine-tuning</td><td>7</td><td>10</td><td>8</td><td>4</td><td>1</td></tr><tr><td>Foundation Model</td><td></td><td></td><td>1</td><td>2</td><td></td></tr><tr><td>GPT</td><td>3</td><td>5</td><td></td><td></td><td></td></tr><tr><td>GPT-2</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>GPT-3</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>GPT-3.5</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>GPT-4</td><td>2</td><td>4</td><td></td><td></td><td></td></tr><tr><td>Gaussian Process</td><td></td><td></td><td></td><td>1</td><td>1</td></tr><tr><td>Gemini</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Generative AI</td><td>1</td><td></td><td>1</td><td>1</td><td></td></tr><tr><td>Generative Adversarial Network</td><td></td><td></td><td>2</td><td></td><td></td></tr><tr><td>Geometry</td><td>2</td><td></td><td>1</td><td>2</td><td></td></tr><tr><td>Graph</td><td>4</td><td></td><td>2</td><td>12</td><td>2</td></tr><tr><td>Graph Attention Networks</td><td></td><td></td><td>1</td><td>1</td><td></td></tr><tr><td>Graph Contrastive Learning</td><td></td><td></td><td></td><td>2</td><td></td></tr><tr><td>Graph Neural Network</td><td>2</td><td></td><td></td><td>8</td><td>2</td></tr><tr><td>Grounding</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Hate Speech Detection</td><td>1</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Image2text</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>In-context Learning</td><td>6</td><td>3</td><td>1</td><td></td><td></td></tr><tr><td>Information Retrieval</td><td></td><td>2</td><td>1</td><td></td><td></td></tr><tr><td>Instruction Following</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Instruction Tuning</td><td>1</td><td>2</td><td></td><td></td><td></td></tr><tr><td>Karush-Kuhn-Tucker</td><td></td><td></td><td></td><td>2</td><td></td></tr><tr><td>Knowledge Distillation</td><td>2</td><td>3</td><td></td><td>6</td><td></td></tr><tr><td>Knowledge Graph</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Knowledge Transfer</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>LLaMA</td><td>1</td><td>3</td><td></td><td></td><td></td></tr><tr><td>LSTM</td><td></td><td></td><td></td><td>3</td><td></td></tr><tr><td>Language Generation</td><td>1</td><td>2</td><td></td><td></td><td></td></tr><tr><td>Large Language Model</td><td>34</td><td>40</td><td></td><td>12</td><td>2</td></tr><tr><td>Logistic Regression</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>MNIST</td><td>1</td><td></td><td>1</td><td>1</td><td></td></tr><tr><td>Markov Decision Process</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Mathematical Reasoning</td><td>1</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Message-Passing</td><td></td><td></td><td></td><td>1</td><td>1</td></tr><tr><td>Mistral</td><td>1</td><td>1</td><td></td><td></td><td></td></tr><tr><td>Multi-modal</td><td></td><td></td><td>6</td><td>2</td><td>1</td></tr><tr><td>Mutual Information</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Named Entity Recognition</td><td></td><td>3</td><td></td><td></td><td></td></tr><tr><td>Natural Language Generation</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Neural Machine Translation</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Node Classification</td><td>1</td><td></td><td></td><td>2</td><td></td></tr><tr><td>Node Embedding</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Object Detection</td><td></td><td></td><td>3</td><td>1</td><td></td></tr><tr><td>Online Clustering</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Out-of-distribution</td><td>1</td><td>1</td><td>1</td><td>1</td><td></td></tr><tr><td>Perplexity</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Pre-trained Language Model</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Probabilistic Reasoning</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Prompt</td><td>5</td><td>6</td><td>2</td><td>2</td><td>1</td></tr><tr><td>Pruning</td><td>1</td><td>1</td><td></td><td>2</td><td></td></tr><tr><td>Quantization</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Question Answering</td><td></td><td>3</td><td>1</td><td></td><td></td></tr><tr><td>Reasoning</td><td>8</td><td>8</td><td></td><td>2</td><td>1</td></tr><tr><td>Recommendation</td><td>2</td><td>1</td><td></td><td></td><td>1</td></tr><tr><td>Recurrent Neural Network</td><td></td><td></td><td></td><td>3</td><td></td></tr><tr><td>Reinforcement Learning</td><td>1</td><td>1</td><td></td><td>7</td><td>1</td></tr><tr><td>Reinforcement Learning from Human Feedback</td><td></td><td>2</td><td></td><td>4</td><td></td></tr><tr><td>Representation Learning</td><td>1</td><td></td><td>1</td><td>3</td><td></td></tr><tr><td>Rerank</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Sample Size</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Self-Attention</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Self-supervised Learning</td><td></td><td>1</td><td>3</td><td>10</td><td></td></tr><tr><td>Self-supervised Pre-training</td><td></td><td>1</td><td></td><td>1</td><td></td></tr><tr><td>Sentiment Analysis</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Simulation</td><td>2</td><td></td><td></td><td>2</td><td>2</td></tr><tr><td>Simulator</td><td>2</td><td></td><td></td><td>2</td><td>2</td></tr><tr><td>Square Loss</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Stochastic Gradient Descent</td><td></td><td></td><td></td><td>2</td><td></td></tr><tr><td>Sub-Quadratic Transformer</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Summarization</td><td></td><td>1</td><td></td><td></td><td>1</td></tr><tr><td>Supervised Learning</td><td></td><td>2</td><td>7</td><td>5</td><td></td></tr><tr><td>T5</td><td>2</td><td></td><td></td><td></td><td></td></tr><tr><td>Text Analysis</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Text Augmentation</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Text Embedding</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Text Generation</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Text2image</td><td></td><td></td><td>3</td><td></td><td></td></tr><tr><td>Transfer Learning</td><td></td><td></td><td>2</td><td>2</td><td></td></tr><tr><td>Transformer</td><td>3</td><td>1</td><td>6</td><td>8</td><td></td></tr><tr><td>Unsupervised Learning</td><td></td><td></td><td>1</td><td>5</td><td></td></tr><tr><td>Variational Autoencoder</td><td></td><td></td><td></td><td></td><td>1</td></tr><tr><td>Video-and-Language</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Vision Transformer</td><td></td><td></td><td>2</td><td></td><td></td></tr><tr><td>Vision-and-Language</td><td></td><td></td><td>3</td><td>1</td><td></td></tr><tr><td>Visual Question Answering</td><td></td><td></td><td>2</td><td></td><td></td></tr><tr><td>Weakly-supervised Learning</td><td></td><td></td><td>2</td><td></td><td></td></tr><tr><td>Yolo</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Zero-shot</td><td></td><td>2</td><td>2</td><td></td><td>1</td></tr><tr><td>human-in-the-loop</td><td>1</td><td></td><td></td><td></td><td></td></tr></tbody></table><script>$(function(){$("table").addClass("keyword-table table-bordered border-success"),$("table thead").addClass("sticky-top"),$("table tbody td").css("text-align","")})</script><h2 id=cscl-31>cs.CL (31)</h2><h3 id=131--1261-leveraging-large-language-models-for-enhanced-nlp-task-performance-through-knowledge-distillation-and-optimized-training-strategies-yining-huang-2024>(1/31 | 1/261) Leveraging Large Language Models for Enhanced NLP Task Performance through Knowledge Distillation and Optimized Training Strategies (Yining Huang, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yining Huang. (2024)<br><strong>Leveraging Large Language Models for Enhanced NLP Task Performance through Knowledge Distillation and Optimized Training Strategies</strong><br><button class=copy-to-clipboard title="Leveraging Large Language Models for Enhanced NLP Task Performance through Knowledge Distillation and Optimized Training Strategies" index=1>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-1 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 110<br>Keywords: Knowledge Distillation, Knowledge Distillation, Knowledge Distillation, BERT, GPT, GPT-4, Named Entity Recognition, Named Entity Recognition, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09282v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09282v1.pdf filename=2402.09282v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The integration of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> like <b>GPT-4</b> into traditional Natural Language Processing (NLP) tasks has opened new avenues for enhancing model performance while reducing the reliance on extensive human annotations. This paper presents a novel approach that leverages the Chain of Thought (CoT) <b>prompting</b> technique to <b>distill</b> <b>knowledge</b> <b>from</b> <b>GPT-4,</b> subsequently applying it to improve the efficiency and effectiveness of a smaller model, <b>BERT,</b> on <b>Named</b> <b>Entity</b> <b>Recognition</b> <b>(NER)</b> tasks. Our method involves a two-phase training process: initially employing <b>GPT-4</b> annotated data for pre-training and then refining the model with a combination of <b>distilled</b> and original human-annotated data. The results demonstrate that our mixed-training strategy significantly outperforms models trained solely on human annotations, achieving superior F1-scores and showcasing a cost-effective solution for resource-limited or closed-network settings. The study also discusses the challenges encountered, such as <b>LLM</b> output variability and the tendency towards hallucinations, proposing future work directions to enhance <b>prompt</b> design and annotation selection. Our findings indicate a promising synergy between <b>LLM</b> insights and traditional NLP techniques, paving the way for more accessible and robust NLP applications.</p></p class="citation"></blockquote><h3 id=231--2261-personalized-large-language-models-stanisław-woźniak-et-al-2024>(2/31 | 2/261) Personalized Large Language Models (Stanisław Woźniak et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Stanisław Woźniak, Bartłomiej Koptyra, Arkadiusz Janz, Przemysław Kazienko, Jan Kocoń. (2024)<br><strong>Personalized Large Language Models</strong><br><button class=copy-to-clipboard title="Personalized Large Language Models" index=2>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-2 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 90<br>Keywords: Fine-tuning, Recommendation, Zero-shot, Chatbot, Emotion Recognition, Hate Speech Detection, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09269v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09269v1.pdf filename=2402.09269v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have significantly advanced Natural Language Processing (NLP) tasks in recent years. However, their universal nature poses limitations in scenarios requiring personalized responses, such as <b>recommendation</b> systems and <b>chatbots.</b> This paper investigates methods to personalize <b>LLMs,</b> comparing <b>fine-tuning</b> and <b>zero-shot</b> <b>reasoning</b> approaches on subjective tasks. Results demonstrate that personalized <b>fine-tuning</b> improves model <b>reasoning</b> compared to non-personalized models. Experiments on datasets for <b>emotion</b> <b>recognition</b> and <b>hate</b> <b>speech</b> <b>detection</b> show consistent performance gains with personalized methods across different <b>LLM</b> architectures. These findings underscore the importance of personalization for enhancing <b>LLM</b> capabilities in subjective text perception tasks.</p></p class="citation"></blockquote><h3 id=331--3261-multi-query-focused-disaster-summarization-via-instruction-based-prompting-philipp-seeberger-et-al-2024>(3/31 | 3/261) Multi-Query Focused Disaster Summarization via Instruction-Based Prompting (Philipp Seeberger et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Philipp Seeberger, Korbinian Riedhammer. (2024)<br><strong>Multi-Query Focused Disaster Summarization via Instruction-Based Prompting</strong><br><button class=copy-to-clipboard title="Multi-Query Focused Disaster Summarization via Instruction-Based Prompting" index=3>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-3 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 90<br>Keywords: Rerank, LLaMA, Instruction Following, Question Answering, Question Answering, Large Language Model, Large Language Model, Prompt, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09008v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09008v1.pdf filename=2402.09008v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Automatic <b>summarization</b> of mass-emergency events plays a critical role in disaster management. The second edition of CrisisFACTS aims to advance disaster <b>summarization</b> based on multi-stream fact-finding with a focus on web sources such as Twitter, Reddit, Facebook, and Webnews. Here, participants are asked to develop systems that can extract key facts from several disaster-related events, which ultimately serve as a summary. This paper describes our method to tackle this challenging task. We follow previous work and propose to use a combination of retrieval, <b>reranking,</b> and an embarrassingly simple <b>instruction-following</b> <b>summarization.</b> The two-stage retrieval pipeline relies on BM25 and MonoT5, while the summarizer module is based on the open-source <b>Large</b> <b>Language</b> <b>Model</b> <b>(LLM)</b> <b>LLaMA-13b.</b> For <b>summarization,</b> we explore a <b>Question</b> <b>Answering</b> <b>(QA)-motivated</b> <b>prompting</b> approach and find the evidence useful for extracting query-relevant facts. The automatic metrics and human evaluation show strong results but also highlight the gap between open-source and proprietary systems.</p></p class="citation"></blockquote><h3 id=431--4261-aqa-bench-an-interactive-benchmark-for-evaluating-llms-sequential-reasoning-ability-siwei-yang-et-al-2024>(4/31 | 4/261) AQA-Bench: An Interactive Benchmark for Evaluating LLMs&rsquo; Sequential Reasoning Ability (Siwei Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Siwei Yang, Bingchen Zhao, Cihang Xie. (2024)<br><strong>AQA-Bench: An Interactive Benchmark for Evaluating LLMs&rsquo; Sequential Reasoning Ability</strong><br><button class=copy-to-clipboard title="AQA-Bench: An Interactive Benchmark for Evaluating LLMs' Sequential Reasoning Ability" index=4>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-4 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 73<br>Keywords: Benchmarking, Few-shot, GPT, GPT-4, Gemini, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09404v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09404v1.pdf filename=2402.09404v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper introduces AQA-Bench, a novel <b>benchmark</b> to assess the sequential <b>reasoning</b> capabilities of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> in algorithmic contexts, such as depth-first search (DFS). The key feature of our evaluation <b>benchmark</b> lies in its interactive evaluation protocol &ndash; for example, in DFS, the availability of each node&rsquo;s connected edge is contingent upon the model&rsquo;s traversal to that node, thereby necessitating the <b>LLM&rsquo;s</b> ability to effectively remember visited nodes and strategize subsequent moves. We comprehensively build AQA-Bench with three different algorithms, namely binary search, depth-first search, and breadth-first search, and to evaluate the sequential <b>reasoning</b> ability of 12 different <b>LLMs.</b> Our investigations reveal several interesting findings: (1) Closed-source models like <b>GPT-4</b> and <b>Gemini</b> generally show strong sequential <b>reasoning</b> ability, significantly outperforming open-source <b>LLMs.</b> (2) Naively providing interactive examples may inadvertently hurt <b>few-shot</b> performance. (3) A very limited number of predecessor steps following the optimal policy can substantially boost small models&rsquo; performance. (4) The scaling correlation between performance and model size is not always significant, sometimes even showcasing an inverse trend. We hope our study can catalyze future work on advancing the understanding and enhancement of <b>LLMs&rsquo;</b> capabilities in sequential <b>reasoning.</b> The code is available at <a href=https://github.com/UCSC-VLAA/AQA-Bench>https://github.com/UCSC-VLAA/AQA-Bench</a>.</p></p class="citation"></blockquote><h3 id=531--5261-api-pack-a-massive-multilingual-dataset-for-api-call-generation-zhen-guo-et-al-2024>(5/31 | 5/261) API Pack: A Massive Multilingual Dataset for API Call Generation (Zhen Guo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhen Guo, Adriana Meza Soria, Wei Sun, Yikang Shen, Rameswar Panda. (2024)<br><strong>API Pack: A Massive Multilingual Dataset for API Call Generation</strong><br><button class=copy-to-clipboard title="API Pack: A Massive Multilingual Dataset for API Call Generation" index=5>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-5 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 70<br>Keywords: Fine-tuning, Fine-tuning, GPT, GPT-3, GPT-3.5, GPT-4, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09615v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09615v2.pdf filename=2402.09615v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce API Pack, a multilingual dataset featuring over one million instruction-API call pairs aimed at advancing <b>large</b> <b>language</b> <b>models&rsquo;</b> API call generation capabilities. Through experiments, we demonstrate API Pack&rsquo;s efficacy in enhancing models for this specialized task while maintaining their overall proficiency at general coding. <b>Fine-tuning</b> CodeLlama-13B on just 20,000 Python instances yields over 10% and 5% higher accuracy than <b>GPT-3.5</b> and <b>GPT-4</b> respectively in generating unseen API calls. Scaling to 100k examples improves generalization to new APIs not seen during training. In addition, cross-lingual API call generation is achieved without needing extensive data per language. The dataset, <b>fine-tuned</b> models, and overall code base are publicly available at <a href=https://github.com/zguo0525/API-Pack>https://github.com/zguo0525/API-Pack</a>.</p></p class="citation"></blockquote><h3 id=631--6261-towards-privacy-aware-sign-language-translation-at-scale-phillip-rust-et-al-2024>(6/31 | 6/261) Towards Privacy-Aware Sign Language Translation at Scale (Phillip Rust et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Phillip Rust, Bowen Shi, Skyler Wang, Necati Cihan Camgöz, Jean Maillard. (2024)<br><strong>Towards Privacy-Aware Sign Language Translation at Scale</strong><br><button class=copy-to-clipboard title="Towards Privacy-Aware Sign Language Translation at Scale" index=6>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-6 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-CV, cs-LG, cs.CL<br>Keyword Score: 70<br>Keywords: Fine-tuning, Fine-tuning, Self-supervised Learning, Self-supervised Pre-training, Supervised Learning, Zero-shot, BLEU<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09611v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09611v1.pdf filename=2402.09611v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A major impediment to the advancement of sign language translation (SLT) is data scarcity. Much of the sign language data currently available on the web cannot be used for training <b>supervised</b> models due to the lack of aligned captions. Furthermore, scaling SLT using large-scale web-scraped datasets bears privacy risks due to the presence of biometric information, which the responsible development of SLT technologies should account for. In this work, we propose a two-stage framework for privacy-aware SLT at scale that addresses both of these issues. We introduce SSVP-SLT, which leverages <b>self-supervised</b> <b>video</b> pretraining on anonymized and unannotated videos, followed by <b>supervised</b> SLT <b>finetuning</b> on a curated parallel dataset. SSVP-SLT achieves state-of-the-art <b>finetuned</b> and <b>zero-shot</b> gloss-free SLT performance on the How2Sign dataset, outperforming the strongest respective baselines by over 3 <b>BLEU-4.</b> Based on controlled experiments, we further discuss the advantages and limitations of <b>self-supervised</b> <b>pretraining</b> and anonymization via facial obfuscation for SLT.</p></p class="citation"></blockquote><h3 id=731--7261-maxmin-rlhf-towards-equitable-alignment-of-large-language-models-with-diverse-human-preferences-souradip-chakraborty-et-al-2024>(7/31 | 7/261) MaxMin-RLHF: Towards Equitable Alignment of Large Language Models with Diverse Human Preferences (Souradip Chakraborty et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Souradip Chakraborty, Jiahao Qiu, Hui Yuan, Alec Koppel, Furong Huang, Dinesh Manocha, Amrit Singh Bedi, Mengdi Wang. (2024)<br><strong>MaxMin-RLHF: Towards Equitable Alignment of Large Language Models with Diverse Human Preferences</strong><br><button class=copy-to-clipboard title="MaxMin-RLHF: Towards Equitable Alignment of Large Language Models with Diverse Human Preferences" index=7>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-7 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs-RO, cs.CL<br>Keyword Score: 70<br>Keywords: Fairness, Reinforcement Learning, Reinforcement Learning from Human Feedback, Reinforcement Learning from Human Feedback, GPT, GPT-2, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08925v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08925v1.pdf filename=2402.08925v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Reinforcement</b> <b>Learning</b> <b>from</b> <b>Human</b> <b>Feedback</b> <b>(RLHF)</b> aligns language models to human preferences by employing a singular reward model derived from preference data. However, such an approach overlooks the rich diversity of human preferences inherent in data collected from multiple users. In this work, we first derive an impossibility result of alignment with single reward <b>RLHF,</b> thereby highlighting its insufficiency in representing diverse human preferences. To provide an equitable solution to the problem, we learn a mixture of preference distributions via an expectation-maximization algorithm and propose a MaxMin alignment objective for policy learning inspired by the Egalitarian principle in social choice theory to better represent diverse human preferences. We elucidate the connection of our proposed approach to distributionally robust optimization and general utility RL, thereby highlighting the generality and robustness of our proposed solution. We present comprehensive experimental results on small-scale <b>(GPT-2)</b> and <b>large-scale</b> <b>language</b> <b>models</b> (with Tulu2-7B) and show the efficacy of the proposed approach in the presence of diversity among human preferences. Our algorithm achieves an average improvement of more than 16% in win-rates over conventional <b>RLHF</b> algorithms and improves the win-rate (accuracy) for minority groups by over 33% without compromising the performance of majority groups, showcasing the robustness and <b>fairness</b> of our approach. We remark that our findings in this work are not only limited to language models but also extend to <b>reinforcement</b> <b>learning</b> in general.</p></p class="citation"></blockquote><h3 id=831--8261-probabilistic-reasoning-in-generative-large-language-models-aliakbar-nafar-et-al-2024>(8/31 | 8/261) Probabilistic Reasoning in Generative Large Language Models (Aliakbar Nafar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Aliakbar Nafar, Kristen Brent Venable, Parisa Kordjamshidi. (2024)<br><strong>Probabilistic Reasoning in Generative Large Language Models</strong><br><button class=copy-to-clipboard title="Probabilistic Reasoning in Generative Large Language Models" index=8>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-8 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: I-2-7, cs-AI, cs-CL, cs.CL<br>Keyword Score: 60<br>Keywords: Mathematical Reasoning, Probabilistic Reasoning, Question Answering, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09614v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09614v1.pdf filename=2402.09614v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper considers the challenges that <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> face when <b>reasoning</b> over text that includes information involving uncertainty explicitly quantified via probability values. This type of <b>reasoning</b> is relevant to a variety of contexts ranging from everyday conversations to medical decision-making. Despite improvements in the <b>mathematical</b> <b>reasoning</b> capabilities of <b>LLMs,</b> they still exhibit significant difficulties when it comes to <b>probabilistic</b> <b>reasoning.</b> To deal with this problem, we first introduce the Bayesian Linguistic Inference Dataset (BLInD), a new dataset specifically designed to test the <b>probabilistic</b> <b>reasoning</b> capabilities of <b>LLMs.</b> We then leverage this new dataset to thoroughly illustrate the specific limitations of <b>LLMs</b> for tasks involving <b>probabilistic</b> <b>reasoning</b> and present several strategies that map the problem to different formal representations, including Python code, <b>probabilistic</b> <b>inference</b> algorithms, and <b>probabilistic</b> <b>logical</b> programming. We conclude by providing an evaluation of our methods on BLInD and on an adaptation of a causal <b>reasoning</b> <b>question-answering</b> <b>dataset,</b> which further shows their practical effectiveness.</p></p class="citation"></blockquote><h3 id=931--9261-icdpo-effectively-borrowing-alignment-capability-of-others-via-in-context-direct-preference-optimization-feifan-song-et-al-2024>(9/31 | 9/261) ICDPO: Effectively Borrowing Alignment Capability of Others via In-context Direct Preference Optimization (Feifan Song et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Feifan Song, Yuxuan Fan, Xin Zhang, Peiyi Wang, Houfeng Wang. (2024)<br><strong>ICDPO: Effectively Borrowing Alignment Capability of Others via In-context Direct Preference Optimization</strong><br><button class=copy-to-clipboard title="ICDPO: Effectively Borrowing Alignment Capability of Others via In-context Direct Preference Optimization" index=9>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-9 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 60<br>Keywords: Fine-tuning, In-context Learning, In-context Learning, In-context Learning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09320v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09320v1.pdf filename=2402.09320v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> rely on Human Preference Alignment (HPA) to ensure the generation of safe content. Due to the heavy cost associated with <b>fine-tuning,</b> <b>fine-tuning-free</b> methods have emerged, typically modifying <b>LLM</b> decoding with external auxiliary methods. However, these methods do not essentially enhance the <b>LLM</b> itself. In this paper, we rethink the derivation procedures of DPO, based on which we conversely build an instant scorer using the states of the <b>LLM</b> before and after <b>In-context</b> <b>Learning</b> <b>(ICL).</b> Accordingly, we propose a novel approach called <b>In-Context</b> <b>Direct</b> Preference Optimization (ICDPO). It enables <b>LLMs</b> to borrow the HPA capabilities from superior <b>LLMs</b> with <b>ICL,</b> generating well-aligned responses as estimated by the aforementioned instant scorer, thereby enhancing the final performance. ICDPO can be further enhanced with a two-stage retriever and an upgraded scorer, both offering benefits. Extensive experiments show its effectiveness, particularly in outperforming two <b>fine-tuning-free</b> baselines, and it exhibits competitiveness with SFT + LoRA. We also conduct detailed analyses to offer comprehensive insights into ICDPO.</p></p class="citation"></blockquote><h3 id=1031--10261-ten-words-only-still-help-improving-black-box-ai-generated-text-detection-via-proxy-guided-efficient-re-sampling-yuhui-shi-et-al-2024>(10/31 | 10/261) Ten Words Only Still Help: Improving Black-Box AI-Generated Text Detection via Proxy-Guided Efficient Re-Sampling (Yuhui Shi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuhui Shi, Qiang Sheng, Juan Cao, Hao Mi, Beizhe Hu, Danding Wang. (2024)<br><strong>Ten Words Only Still Help: Improving Black-Box AI-Generated Text Detection via Proxy-Guided Efficient Re-Sampling</strong><br><button class=copy-to-clipboard title="Ten Words Only Still Help: Improving Black-Box AI-Generated Text Detection via Proxy-Guided Efficient Re-Sampling" index=10>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-10 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 55<br>Keywords: Black Box, Out-of-distribution, AI-generated Text Detection, Fake News Detection, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09199v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09199v1.pdf filename=2402.09199v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the rapidly increasing application of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs),</b> their abuse has caused many undesirable societal problems such as <b>fake</b> <b>news,</b> academic dishonesty, and information pollution. This makes <b>AI-generated</b> <b>text</b> <b>(AIGT)</b> detection of great importance. Among existing methods, white-box methods are generally superior to <b>black-box</b> <b>methods</b> in terms of performance and generalizability, but they require access to <b>LLMs&rsquo;</b> internal states and are not applicable to <b>black-box</b> <b>settings.</b> In this paper, we propose to estimate word generation probabilities as pseudo white-box features via multiple re-sampling to help improve AIGT detection under the <b>black-box</b> <b>setting.</b> Specifically, we design POGER, a proxy-guided efficient re-sampling method, which selects a small subset of representative words (e.g., 10 words) for performing multiple re-sampling in <b>black-box</b> <b>AIGT</b> detection. Experiments on datasets containing texts from humans and seven <b>LLMs</b> show that POGER outperforms all baselines in macro F1 under <b>black-box,</b> <b>partial</b> white-box, and <b>out-of-distribution</b> settings and maintains lower re-sampling costs than its existing counterparts.</p></p class="citation"></blockquote><h3 id=1131--11261-dolphcoder-echo-locating-code-large-language-models-with-diverse-and-multi-objective-instruction-tuning-yejie-wang-et-al-2024>(11/31 | 11/261) DolphCoder: Echo-Locating Code Large Language Models with Diverse and Multi-Objective Instruction Tuning (Yejie Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yejie Wang, Keqing He, Guanting Dong, Pei Wang, Weihao Zeng, Muxi Diao, Yutao Mou, Mengdi Zhang, Jingang Wang, Xunliang Cai, Weiran Xu. (2024)<br><strong>DolphCoder: Echo-Locating Code Large Language Models with Diverse and Multi-Objective Instruction Tuning</strong><br><button class=copy-to-clipboard title="DolphCoder: Echo-Locating Code Large Language Models with Diverse and Multi-Objective Instruction Tuning" index=11>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-11 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 53<br>Keywords: Benchmarking, Code Generation, Reasoning, Instruction Tuning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09136v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09136v1.pdf filename=2402.09136v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Code</b> <b>Large</b> <b>Language</b> <b>Models</b> <b>(Code</b> <b>LLMs)</b> have demonstrated outstanding performance in <b>code-related</b> <b>tasks.</b> Several <b>instruction</b> <b>tuning</b> approaches have been proposed to boost the <b>code</b> <b>generation</b> performance of pre-trained <b>Code</b> <b>LLMs.</b> In this paper, we introduce a diverse <b>instruction</b> <b>model</b> (DolphCoder) with self-evaluating for <b>code</b> <b>generation.</b> It learns diverse <b>instruction</b> <b>targets</b> and combines a <b>code</b> <b>evaluation</b> objective to enhance its <b>code</b> <b>generation</b> ability. Our model achieves superior performance on the HumanEval and MBPP <b>benchmarks,</b> demonstrating new insights for future <b>code</b> <b>instruction</b> <b>tuning</b> work. Our key findings are: (1) Augmenting more diverse responses with distinct <b>reasoning</b> paths increases the <b>code</b> <b>capability</b> of <b>LLMs.</b> (2) Improving one&rsquo;s ability to evaluate the correctness of <b>code</b> <b>solutions</b> also enhances their ability to create it.</p></p class="citation"></blockquote><h3 id=1231--12261-dora-weight-decomposed-low-rank-adaptation-shih-yang-liu-et-al-2024>(12/31 | 12/261) DoRA: Weight-Decomposed Low-Rank Adaptation (Shih-Yang Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shih-Yang Liu, Chien-Yi Wang, Hongxu Yin, Pavlo Molchanov, Yu-Chiang Frank Wang, Kwang-Ting Cheng, Min-Hung Chen. (2024)<br><strong>DoRA: Weight-Decomposed Low-Rank Adaptation</strong><br><button class=copy-to-clipboard title="DoRA: Weight-Decomposed Low-Rank Adaptation" index=12>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-12 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-CV, cs.CL<br>Keyword Score: 50<br>Keywords: Fine-tuning, LLaMA, Common-sense Reasoning, Reasoning, Instruction Tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09353v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09353v1.pdf filename=2402.09353v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Among the widely used parameter-efficient <b>finetuning</b> (PEFT) methods, LoRA and its variants have gained considerable popularity because of avoiding additional inference costs. However, there still often exists an accuracy gap between these methods and full <b>fine-tuning</b> (FT). In this work, we first introduce a novel weight decomposition analysis to investigate the inherent differences between FT and LoRA. Aiming to resemble the learning capacity of FT from the findings, we propose Weight-Decomposed LowRank Adaptation (DoRA). DoRA decomposes the pre-trained weight into two components, magnitude and direction, for <b>fine-tuning,</b> specifically employing LoRA for directional updates to efficiently minimize the number of trainable parameters. By employing DoRA, we enhance both the learning capacity and training stability of LoRA while avoiding any additional inference overhead. DoRA consistently outperforms LoRA on <b>fine-tuning</b> <b>LLaMA,</b> LLaVA, and VL-BART on various downstream tasks, such as <b>commonsense</b> <b>reasoning,</b> visual <b>instruction</b> <b>tuning,</b> and image/video-text understanding.</p></p class="citation"></blockquote><h3 id=1331--13261-self-alignment-for-factuality-mitigating-hallucinations-in-llms-via-self-evaluation-xiaoying-zhang-et-al-2024>(13/31 | 13/261) Self-Alignment for Factuality: Mitigating Hallucinations in LLMs via Self-Evaluation (Xiaoying Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiaoying Zhang, Baolin Peng, Ye Tian, Jingyan Zhou, Lifeng Jin, Linfeng Song, Haitao Mi, Helen Meng. (2024)<br><strong>Self-Alignment for Factuality: Mitigating Hallucinations in LLMs via Self-Evaluation</strong><br><button class=copy-to-clipboard title="Self-Alignment for Factuality: Mitigating Hallucinations in LLMs via Self-Evaluation" index=13>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-13 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: Fine-tuning, LLaMA, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09267v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09267v1.pdf filename=2402.09267v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Despite showing increasingly human-like abilities, <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> often struggle with factual inaccuracies, i.e. &ldquo;hallucinations&rdquo;, even when they hold relevant knowledge. To address these hallucinations, current approaches typically necessitate high-quality human factuality annotations. In this work, we explore Self-Alignment for Factuality, where we leverage the self-evaluation capability of an <b>LLM</b> to provide training signals that steer the model towards factuality. Specifically, we incorporate Self-Eval, a self-evaluation component, to <b>prompt</b> an <b>LLM</b> to validate the factuality of its own generated responses solely based on its internal knowledge. Additionally, we design Self-Knowledge Tuning (SK-Tuning) to augment the <b>LLM&rsquo;s</b> self-evaluation ability by improving the model&rsquo;s confidence estimation and calibration. We then utilize these self-annotated responses to <b>fine-tune</b> the model via Direct Preference Optimization algorithm. We show that the proposed self-alignment approach substantially enhances factual accuracy over <b>Llama</b> family models across three key knowledge-intensive tasks on TruthfulQA and BioGEN.</p></p class="citation"></blockquote><h3 id=1431--14261-rationality-report-cards-assessing-the-economic-rationality-of-large-language-models-narun-raman-et-al-2024>(14/31 | 14/261) Rationality Report Cards: Assessing the Economic Rationality of Large Language Models (Narun Raman et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Narun Raman, Taylor Lundy, Samuel Amouyal, Yoav Levine, Kevin Leyton-Brown, Moshe Tennenholtz. (2024)<br><strong>Rationality Report Cards: Assessing the Economic Rationality of Large Language Models</strong><br><button class=copy-to-clipboard title="Rationality Report Cards: Assessing the Economic Rationality of Large Language Models" index=14>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-14 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL, econ-GN, q-fin-EC<br>Keyword Score: 43<br>Keywords: Benchmarking, Reasoning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09552v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09552v1.pdf filename=2402.09552v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>There is increasing interest in using <b>LLMs</b> as decision-making &ldquo;agents.&rdquo; Doing so includes many degrees of freedom: which model should be used; how should it be <b>prompted;</b> should it be asked to introspect, conduct chain-of-thought <b>reasoning,</b> etc? Settling these questions &ndash; and more broadly, determining whether an <b>LLM</b> agent is reliable enough to be trusted &ndash; requires a methodology for assessing such an agent&rsquo;s economic rationality. In this paper, we provide one. We begin by surveying the economic literature on rational decision making, taxonomizing a <b>large</b> <b>set</b> <b>of</b> fine-grained &ldquo;elements&rdquo; that an agent should exhibit, along with dependencies between them. We then propose a <b>benchmark</b> distribution that quantitatively scores an <b>LLMs</b> performance on these elements and, combined with a user-provided rubric, produces a &ldquo;rationality report card.&rdquo; Finally, we describe the results of a <b>large-scale</b> <b>empirical</b> <b>experiment</b> with 14 different <b>LLMs,</b> characterizing the both current state of the art and the impact of different model sizes on models&rsquo; ability to exhibit rational behavior.</p></p class="citation"></blockquote><h3 id=1531--15261-generalization-in-healthcare-ai-evaluation-of-a-clinical-large-language-model-salman-rahman-et-al-2024>(15/31 | 15/261) Generalization in Healthcare AI: Evaluation of a Clinical Large Language Model (Salman Rahman et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Salman Rahman, Lavender Yao Jiang, Saadia Gabriel, Yindalon Aphinyanaphongs, Eric Karl Oermann, Rumi Chunara. (2024)<br><strong>Generalization in Healthcare AI: Evaluation of a Clinical Large Language Model</strong><br><button class=copy-to-clipboard title="Generalization in Healthcare AI: Evaluation of a Clinical Large Language Model" index=15>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-15 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-CY, cs-LG, cs.CL<br>Keyword Score: 43<br>Keywords: Fine-tuning, Sample Size, Supervised Learning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10965v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10965v1.pdf filename=2402.10965v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Advances in <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> provide new opportunities in healthcare for improved patient care, clinical decision-making, and enhancement of physician and administrator workflows. However, the potential of these models importantly depends on their ability to generalize effectively across clinical environments and populations, a challenge often underestimated in early development. To better understand reasons for these challenges and inform mitigation approaches, we evaluated ClinicLLM, an <b>LLM</b> trained on [HOSPITAL]&rsquo;s clinical notes, analyzing its performance on 30-day all-cause readmission prediction focusing on variability across hospitals and patient characteristics. We found poorer generalization particularly in hospitals with fewer <b>samples,</b> <b>among</b> patients with government and unspecified insurance, the elderly, and those with high comorbidities. To understand reasons for lack of generalization, we investigated <b>sample</b> <b>sizes</b> for <b>fine-tuning,</b> note content (number of words per note), patient characteristics (comorbidity level, age, insurance type, borough), and health system aspects (hospital, all-cause 30-day readmission, and mortality rates). We used descriptive statistics and <b>supervised</b> classification to identify features. We found that, along with <b>sample</b> <b>size,</b> patient age, number of comorbidities, and the number of words in notes are all important factors related to generalization. Finally, we compared local <b>fine-tuning</b> (hospital specific), instance-based augmented <b>fine-tuning</b> and cluster-based <b>fine-tuning</b> for improving generalization. Among these, local <b>fine-tuning</b> proved most effective, increasing AUC by 0.25% to 11.74% (most helpful in settings with limited data). Overall, this study provides new insights for enhancing the deployment of <b>large</b> <b>language</b> <b>models</b> in the societally important domain of healthcare, and improving their performance for broader populations.</p></p class="citation"></blockquote><h3 id=1631--16261-scaling-the-authoring-of-autotutors-with-large-language-models-sankalan-pal-chowdhury-et-al-2024>(16/31 | 16/261) Scaling the Authoring of AutoTutors with Large Language Models (Sankalan Pal Chowdhury et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sankalan Pal Chowdhury, Vilém Zouhar, Mrinmaya Sachan. (2024)<br><strong>Scaling the Authoring of AutoTutors with Large Language Models</strong><br><button class=copy-to-clipboard title="Scaling the Authoring of AutoTutors with Large Language Models" index=16>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-16 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-HC, cs.CL<br>Keyword Score: 40<br>Keywords: GPT, GPT-4, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09216v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09216v1.pdf filename=2402.09216v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have found several use cases in education, ranging from automatic question generation to essay evaluation. In this paper, we explore the potential of using <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> to author Intelligent Tutoring Systems. A common pitfall of <b>LLMs</b> is their straying from desired pedagogical strategies such as leaking the answer to the student, and in general, providing no guarantees. We posit that while <b>LLMs</b> with certain guardrails can take the place of subject experts, the overall pedagogical design still needs to be handcrafted for the best learning results. Based on this principle, we create a sample end-to-end tutoring system named MWPTutor, which uses <b>LLMs</b> to fill in the state space of a pre-defined finite state transducer. This approach retains the structure and the pedagogy of traditional tutoring systems that has been developed over the years by learning scientists but brings in additional flexibility of <b>LLM-based</b> approaches. Through a human evaluation study on two datasets based on math word problems, we show that our hybrid approach achieves a better overall tutoring score than an instructed, but otherwise free-form, <b>GPT-4.</b> MWPTutor is completely modular and opens up the scope for the community to improve its performance by improving individual modules or using different teaching strategies that it can follow</p></p class="citation"></blockquote><h3 id=1731--17261-sleb-streamlining-llms-through-redundancy-verification-and-elimination-of-transformer-blocks-jiwon-song-et-al-2024>(17/31 | 17/261) SLEB: Streamlining LLMs through Redundancy Verification and Elimination of Transformer Blocks (Jiwon Song et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiwon Song, Kyungseok Oh, Taesu Kim, Hyungjun Kim, Yulhwa Kim, Jae-Joon Kim. (2024)<br><strong>SLEB: Streamlining LLMs through Redundancy Verification and Elimination of Transformer Blocks</strong><br><button class=copy-to-clipboard title="SLEB: Streamlining LLMs through Redundancy Verification and Elimination of Transformer Blocks" index=17>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-17 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keyword Score: 40<br>Keywords: Pruning, Transformer, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09025v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09025v1.pdf filename=2402.09025v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have proven to be highly effective across various natural language processing tasks. However, their <b>large</b> <b>number</b> <b>of</b> parameters poses significant challenges for practical deployment. <b>Pruning,</b> a technique aimed at reducing the size and complexity of <b>LLMs,</b> offers a potential solution by removing redundant components from the network. Despite the promise of <b>pruning,</b> existing methods often struggle to achieve substantial end-to-end <b>LLM</b> inference speedup. In this paper, we introduce SLEB, a novel approach designed to streamline <b>LLMs</b> by eliminating redundant <b>transformer</b> blocks. We choose the <b>transformer</b> block as the fundamental unit for <b>pruning,</b> because <b>LLMs</b> exhibit block-level redundancy with high similarity between the outputs of neighboring blocks. This choice allows us to effectively enhance the processing speed of <b>LLMs.</b> Our experimental results demonstrate that SLEB successfully accelerates <b>LLM</b> inference without compromising the linguistic capabilities of these models, making it a promising technique for optimizing the efficiency of <b>LLMs.</b> The code is available at: <a href=https://github.com/leapingjagg-dev/SLEB>https://github.com/leapingjagg-dev/SLEB</a></p></p class="citation"></blockquote><h3 id=1831--18261-structured-language-generation-model-for-robust-structure-prediction-minho-lee-et-al-2024>(18/31 | 18/261) Structured Language Generation Model for Robust Structure Prediction (Minho Lee et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Minho Lee, Junghyun Min, Woochul Lee, Yeonsoo Lee. (2024)<br><strong>Structured Language Generation Model for Robust Structure Prediction</strong><br><button class=copy-to-clipboard title="Structured Language Generation Model for Robust Structure Prediction" index=18>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-18 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Fine-tuning, Information Retrieval, Language Generation, Named Entity Recognition<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08971v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08971v2.pdf filename=2402.08971v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Previous work in structured prediction (e.g. <b>NER,</b> <b>information</b> <b>extraction)</b> using single model make use of explicit dataset <b>information,</b> <b>which</b> helps boost in-distribution performance but is orthogonal to robust generalization in real-world situations. To overcome this limitation, we propose the Structured <b>Language</b> <b>Generation</b> Model (SLGM), a framework that reduces sequence-to-sequence problems to classification problems via methodologies in loss calibration and decoding method. Our experimental results show that SLGM is able to maintain performance without explicit dataset <b>information,</b> <b>follow</b> and potentially replace dataset-specific <b>fine-tuning.</b></p></p class="citation"></blockquote><h3 id=1931--19261-copyright-traps-for-large-language-models-matthieu-meeus-et-al-2024>(19/31 | 19/261) Copyright Traps for Large Language Models (Matthieu Meeus et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Matthieu Meeus, Igor Shilov, Manuel Faysse, Yves-Alexandre de Montjoye. (2024)<br><strong>Copyright Traps for Large Language Models</strong><br><button class=copy-to-clipboard title="Copyright Traps for Large Language Models" index=19>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-19 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-CR, cs.CL<br>Keyword Score: 35<br>Keywords: Black Box, Large Language Model, Large Language Model, Perplexity<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09363v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09363v1.pdf filename=2402.09363v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Questions of fair use of copyright-protected content to train <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> are being very actively debated. Document-level inference has been proposed as a new task: inferring from <b>black-box</b> <b>access</b> to the trained model whether a piece of content has been seen during training. SOTA methods however rely on naturally occurring memorization of (part of) the content. While very effective against models that memorize a lot, we hypothesize&ndash;and later confirm&ndash;that they will not work against models that do not naturally memorize, e.g. medium-size 1B models. We here propose to use copyright traps, the inclusion of fictitious entries in original content, to detect the use of copyrighted materials in <b>LLMs</b> with a focus on models where memorization does not naturally occur. We carefully design an experimental setup, randomly inserting traps into original content (books) and train a 1.3B <b>LLM.</b> We first validate that the use of content in our target model would be undetectable using existing methods. We then show, contrary to intuition, that even medium-length trap sentences repeated a significant number of times (100) are not detectable using existing methods. However, we show that longer sequences repeated a <b>large</b> <b>number</b> <b>of</b> times can be reliably detected (AUC=0.75) and used as copyright traps. We further improve these results by studying how the number of times a sequence is seen improves detectability, how sequences with higher <b>perplexity</b> tend to be memorized more, and how taking context into account further improves detectability.</p></p class="citation"></blockquote><h3 id=2031--20261-long-form-evaluation-of-model-editing-domenic-rosati-et-al-2024>(20/31 | 20/261) Long-form evaluation of model editing (Domenic Rosati et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Domenic Rosati, Robie Gonzales, Jinkun Chen, Xuemin Yu, Melis Erkan, Yahya Kayani, Satya Deepika Chavatapalli, Frank Rudzicz, Hassan Sajjad. (2024)<br><strong>Long-form evaluation of model editing</strong><br><button class=copy-to-clipboard title="Long-form evaluation of model editing" index=20>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-20 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 33<br>Keywords: Benchmarking, Language Generation, Natural Language Generation, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09394v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09394v1.pdf filename=2402.09394v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Evaluations of model editing currently only use the `next few token&rsquo; completions after a <b>prompt.</b> As a result, the impact of these methods on longer <b>natural</b> <b>language</b> <b>generation</b> is largely unknown. We introduce long-form evaluation of model editing (\textbf{\textit{LEME}}) a novel evaluation protocol that measures the efficacy and impact of model editing in long-form generative settings. Our protocol consists of a machine-rated survey and a classifier which correlates well with human ratings. Importantly, we find that our protocol has very little relationship with previous short-form metrics (despite being designed to extend efficacy, generalization, locality, and portability into a long-form setting), indicating that our method introduces a novel set of dimensions for understanding model editing methods. Using this protocol, we <b>benchmark</b> a number of model editing techniques and present several findings including that, while some methods (ROME and MEMIT) perform well in making consistent edits within a limited scope, they suffer much more from factual drift than other methods. Finally, we present a qualitative analysis that illustrates common failure modes in long-form generative settings including internal consistency, lexical cohesion, and locality issues.</p></p class="citation"></blockquote><h3 id=2131--21261-tree-based-hard-attention-with-self-motivation-for-large-language-models-chenxi-lin-et-al-2024>(21/31 | 21/261) Tree-Based Hard Attention with Self-Motivation for Large Language Models (Chenxi Lin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chenxi Lin, Jiayu Ren, Guoxiu He, Zhuoren Jiang, Haiyan Yu, Xiaomin Zhu. (2024)<br><strong>Tree-Based Hard Attention with Self-Motivation for Large Language Models</strong><br><button class=copy-to-clipboard title="Tree-Based Hard Attention with Self-Motivation for Large Language Models" index=21>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-21 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 33<br>Keywords: Benchmarking, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08874v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08874v1.pdf filename=2402.08874v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>While <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> excel at understanding and generating plain text, they are not specifically tailored to handle hierarchical text structures. Extracting the task-desired property from their natural language responses typically necessitates additional processing steps. In fact, selectively comprehending the hierarchical structure of <b>large-scale</b> <b>text</b> <b>is</b> pivotal to understanding its substance. Aligning <b>LLMs</b> more closely with the classification or regression values of specific task through <b>prompting</b> also remains challenging. To this end, we propose a novel framework called Tree-Based Hard Attention with Self-Motivation for <b>Large</b> <b>Language</b> <b>Models</b> (TEAROOM). TEAROOM incorporates a tree-based hard attention mechanism for <b>LLMs</b> to process hierarchically structured text inputs. By leveraging <b>prompting,</b> it enables a frozen <b>LLM</b> to selectively focus on relevant leaves in relation to the root, generating a tailored symbolic representation of their relationship. Moreover, TEAROOM comprises a self-motivation strategy for another <b>LLM</b> equipped with a trainable adapter and a linear layer. The selected symbolic outcomes are integrated into another <b>prompt,</b> along with the predictive value of the task. We iteratively feed output values back into the <b>prompt,</b> enabling the trainable <b>LLM</b> to progressively approximate the golden truth. TEAROOM outperforms existing state-of-the-art methods in experimental evaluations across three <b>benchmark</b> datasets, showing its effectiveness in estimating task-specific properties. Through comprehensive experiments and analysis, we have validated the ability of TEAROOM to gradually approach the underlying golden truth through multiple inferences.</p></p class="citation"></blockquote><h3 id=2231--22261-irrationality-and-cognitive-biases-in-large-language-models-olivia-macmillan-scott-et-al-2024>(22/31 | 22/261) (Ir)rationality and Cognitive Biases in Large Language Models (Olivia Macmillan-Scott et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Olivia Macmillan-Scott, Mirco Musolesi. (2024)<br><strong>(Ir)rationality and Cognitive Biases in Large Language Models</strong><br><button class=copy-to-clipboard title="(Ir)rationality and Cognitive Biases in Large Language Models" index=22>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-22 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-HC, cs.CL<br>Keyword Score: 30<br>Keywords: Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09193v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09193v2.pdf filename=2402.09193v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Do <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> display rational reasoning? <b>LLMs</b> have been shown to contain human biases due to the data they have been trained on; whether this is reflected in rational <b>reasoning</b> remains less clear. In this paper, we answer this question by evaluating seven language models using tasks from the cognitive psychology literature. We find that, like humans, <b>LLMs</b> display irrationality in these tasks. However, the way this irrationality is displayed does not reflect that shown by humans. When incorrect answers are given by <b>LLMs</b> to these tasks, they are often incorrect in ways that differ from human-like biases. On top of this, the <b>LLMs</b> reveal an additional layer of irrationality in the significant inconsistency of the responses. Aside from the experimental results, this paper seeks to make a methodological contribution by showing how we can assess and compare different capabilities of these types of models, in this case with respect to rational <b>reasoning.</b></p></p class="citation"></blockquote><h3 id=2331--23261-advancing-nlp-models-with-strategic-text-augmentation-a-comprehensive-study-of-augmentation-methods-and-curriculum-strategies-himmet-toprak-kesgin-et-al-2024>(23/31 | 23/261) Advancing NLP Models with Strategic Text Augmentation: A Comprehensive Study of Augmentation Methods and Curriculum Strategies (Himmet Toprak Kesgin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Himmet Toprak Kesgin, Mehmet Fatih Amasyali. (2024)<br><strong>Advancing NLP Models with Strategic Text Augmentation: A Comprehensive Study of Augmentation Methods and Curriculum Strategies</strong><br><button class=copy-to-clipboard title="Advancing NLP Models with Strategic Text Augmentation: A Comprehensive Study of Augmentation Methods and Curriculum Strategies" index=23>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-23 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Curriculum Learning, Sentiment Analysis, Text Augmentation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09141v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09141v1.pdf filename=2402.09141v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This study conducts a thorough evaluation of <b>text</b> <b>augmentation</b> techniques across a variety of datasets and natural language processing (NLP) tasks to address the lack of reliable, generalized evidence for these methods. It examines the effectiveness of these techniques in augmenting training sets to improve performance in tasks such as topic classification, <b>sentiment</b> <b>analysis,</b> and offensive language detection. The research emphasizes not only the augmentation methods, but also the strategic order in which real and augmented instances are introduced during training. A major contribution is the development and evaluation of Modified Cyclical <b>Curriculum</b> <b>Learning</b> (MCCL) for augmented datasets, which represents a novel approach in the field. Results show that specific augmentation methods, especially when integrated with MCCL, significantly outperform traditional training approaches in NLP model performance. These results underscore the need for careful selection of augmentation techniques and sequencing strategies to optimize the balance between speed and quality improvement in various NLP tasks. The study concludes that the use of augmentation methods, especially in conjunction with MCCL, leads to improved results in various classification tasks, providing a foundation for future advances in <b>text</b> <b>augmentation</b> strategies in NLP.</p></p class="citation"></blockquote><h3 id=2431--24261-massively-multi-cultural-knowledge-acquisition--lm-benchmarking-yi-fung-et-al-2024>(24/31 | 24/261) Massively Multi-Cultural Knowledge Acquisition & LM Benchmarking (Yi Fung et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yi Fung, Ruining Zhao, Jae Doo, Chenkai Sun, Heng Ji. (2024)<br><strong>Massively Multi-Cultural Knowledge Acquisition & LM Benchmarking</strong><br><button class=copy-to-clipboard title="Massively Multi-Cultural Knowledge Acquisition & LM Benchmarking" index=24>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-24 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 26<br>Keywords: Benchmarking, Benchmarking, Information Retrieval, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09369v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09369v1.pdf filename=2402.09369v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Pretrained <b>large</b> <b>language</b> <b>models</b> have revolutionized many applications but still face challenges related to cultural bias and a lack of cultural commonsense knowledge crucial for guiding cross-culture communication and interactions. Recognizing the shortcomings of existing methods in capturing the diverse and rich cultures across the world, this paper introduces a novel approach for massively multicultural knowledge acquisition. Specifically, our method strategically navigates from densely informative Wikipedia documents on cultural topics to an extensive network of linked pages. Leveraging this valuable source of data collection, we construct the CultureAtlas dataset, which covers a wide range of sub-country level geographical regions and ethnolinguistic groups, with data cleaning and preprocessing to ensure textual assertion sentence self-containment, as well as fine-grained cultural profile <b>information</b> <b>extraction.</b> Our dataset not only facilitates the evaluation of language model performance in culturally diverse contexts but also serves as a foundational tool for the development of culturally sensitive and aware language models. Our work marks an important step towards deeper understanding and bridging the gaps of cultural disparities in AI, to promote a more inclusive and balanced representation of global cultures in the digital domain.</p></p class="citation"></blockquote><h3 id=2531--25261-logicprpbank-a-corpus-for-logical-implication-and-equivalence-zhexiong-liu-et-al-2024>(25/31 | 25/261) LogicPrpBank: A Corpus for Logical Implication and Equivalence (Zhexiong Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhexiong Liu, Jing Zhang, Jiaying Lu, Wenjing Ma, Joyce C Ho. (2024)<br><strong>LogicPrpBank: A Corpus for Logical Implication and Equivalence</strong><br><button class=copy-to-clipboard title="LogicPrpBank: A Corpus for Logical Implication and Equivalence" index=25>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-25 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 23<br>Keywords: Benchmarking, Common-sense Reasoning, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09609v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09609v1.pdf filename=2402.09609v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Logic <b>reasoning</b> has been critically needed in problem-solving and decision-making. Although Language Models (LMs) have demonstrated capabilities of handling multiple <b>reasoning</b> tasks (e.g., <b>commonsense</b> <b>reasoning),</b> their ability to reason complex mathematical problems, specifically propositional logic, remains largely underexplored. This lack of exploration can be attributed to the limited availability of annotated corpora. Here, we present a well-labeled propositional logic corpus, LogicPrpBank, containing 7093 Propositional Logic Statements (PLSs) across six mathematical subjects, to study a brand-new task of <b>reasoning</b> logical implication and equivalence. We <b>benchmark</b> LogicPrpBank with widely-used LMs to show that our corpus offers a useful resource for this challenging task and there is ample room for model improvement.</p></p class="citation"></blockquote><h3 id=2631--26261-chinese-mentalbert-domain-adaptive-pre-training-on-social-media-for-chinese-mental-health-text-analysis-wei-zhai-et-al-2024>(26/31 | 26/261) Chinese MentalBERT: Domain-Adaptive Pre-training on Social Media for Chinese Mental Health Text Analysis (Wei Zhai et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wei Zhai, Hongzhi Qi, Qing Zhao, Jianqiang Li, Ziqi Wang, Han Wang, Bing Xiang Yang, Guanghui Fu. (2024)<br><strong>Chinese MentalBERT: Domain-Adaptive Pre-training on Social Media for Chinese Mental Health Text Analysis</strong><br><button class=copy-to-clipboard title="Chinese MentalBERT: Domain-Adaptive Pre-training on Social Media for Chinese Mental Health Text Analysis" index=26>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-26 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keyword Score: 23<br>Keywords: Benchmarking, Text Analysis, Pre-trained Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09151v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09151v1.pdf filename=2402.09151v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the current environment, psychological issues are prevalent and widespread, with social media serving as a key outlet for individuals to share their feelings. This results in the generation of vast quantities of data daily, where negative emotions have the potential to precipitate crisis situations. There is a recognized need for models capable of efficient analysis. While <b>pre-trained</b> <b>language</b> <b>models</b> have demonstrated their effectiveness broadly, there&rsquo;s a noticeable gap in <b>pre-trained</b> <b>models</b> <b>tailored</b> for specialized domains like psychology. To address this, we have collected a huge dataset from Chinese social media platforms and enriched it with publicly available datasets to create a comprehensive database encompassing 3.36 million <b>text</b> <b>entries.</b> To enhance the model&rsquo;s applicability to psychological <b>text</b> <b>analysis,</b> we integrated psychological lexicons into the pre-training masking mechanism. Building on an existing Chinese language model, we performed adaptive training to develop a model specialized for the psychological domain. We assessed our model&rsquo;s effectiveness across four public <b>benchmarks,</b> where it not only surpassed the performance of standard <b>pre-trained</b> <b>models</b> <b>but</b> also showed a inclination for making psychologically relevant predictions. Due to concerns regarding data privacy, the dataset will not be made publicly available. However, we have made the <b>pre-trained</b> <b>models</b> <b>and</b> codes publicly accessible to the community via: <a href=https://github.com/zwzzzQAQ/Chinese-MentalBERT>https://github.com/zwzzzQAQ/Chinese-MentalBERT</a>.</p></p class="citation"></blockquote><h3 id=2731--27261-attacks-defenses-and-evaluations-for-llm-conversation-safety-a-survey-zhichen-dong-et-al-2024>(27/31 | 27/261) Attacks, Defenses and Evaluations for LLM Conversation Safety: A Survey (Zhichen Dong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhichen Dong, Zhanhui Zhou, Chao Yang, Jing Shao, Yu Qiao. (2024)<br><strong>Attacks, Defenses and Evaluations for LLM Conversation Safety: A Survey</strong><br><button class=copy-to-clipboard title="Attacks, Defenses and Evaluations for LLM Conversation Safety: A Survey" index=27>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-27 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09283v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09283v1.pdf filename=2402.09283v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> are now commonplace in conversation applications. However, their risks of misuse for generating harmful responses have raised serious societal concerns and spurred recent research on <b>LLM</b> conversation safety. Therefore, in this survey, we provide a comprehensive overview of recent studies, covering three critical aspects of <b>LLM</b> conversation safety: attacks, defenses, and evaluations. Our goal is to provide a structured summary that enhances understanding of <b>LLM</b> conversation safety and encourages further investigation into this important subject. For easy reference, we have categorized all the studies mentioned in this survey according to our taxonomy, available at: <a href=https://github.com/niconi19/LLM-conversation-safety>https://github.com/niconi19/LLM-conversation-safety</a>.</p></p class="citation"></blockquote><h3 id=2831--28261-syntaxshap-syntax-aware-explainability-method-for-text-generation-kenza-amara-et-al-2024>(28/31 | 28/261) SyntaxShap: Syntax-aware Explainability Method for Text Generation (Kenza Amara et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kenza Amara, Rita Sevastjanova, Mennatallah El-Assady. (2024)<br><strong>SyntaxShap: Syntax-aware Explainability Method for Text Generation</strong><br><button class=copy-to-clipboard title="SyntaxShap: Syntax-aware Explainability Method for Text Generation" index=28>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-28 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Text Generation, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09259v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09259v1.pdf filename=2402.09259v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>To harness the power of <b>large</b> <b>language</b> <b>models</b> in safety-critical domains we need to ensure the explainability of their predictions. However, despite the significant attention to model interpretability, there remains an unexplored domain in explaining sequence-to-sequence tasks using methods tailored for textual data. This paper introduces SyntaxShap, a local, model-agnostic explainability method for <b>text</b> <b>generation</b> that takes into consideration the syntax in the <b>text</b> <b>data.</b> The presented work extends Shapley values to account for parsing-based syntactic dependencies. Taking a game theoric approach, SyntaxShap only considers coalitions constraint by the dependency tree. We adopt a model-based evaluation to compare SyntaxShap and its weighted form to state-of-the-art explainability methods adapted to <b>text</b> <b>generation</b> tasks, using diverse metrics including faithfulness, complexity, coherency, and semantic alignment of the explanations to the model. We show that our syntax-aware method produces explanations that help build more faithful, coherent, and interpretable explanations for predictions by autoregressive models.</p></p class="citation"></blockquote><h3 id=2931--29261-towards-better-human-agent-alignment-assessing-task-utility-in-llm-powered-applications-negar-arabzadeh-et-al-2024>(29/31 | 29/261) Towards better Human-Agent Alignment: Assessing Task Utility in LLM-Powered Applications (Negar Arabzadeh et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Negar Arabzadeh, Julia Kiseleva, Qingyun Wu, Chi Wang, Ahmed Awadallah, Victor Dibia, Adam Fourney, Charles Clarke. (2024)<br><strong>Towards better Human-Agent Alignment: Assessing Task Utility in LLM-Powered Applications</strong><br><button class=copy-to-clipboard title="Towards better Human-Agent Alignment: Assessing Task Utility in LLM-Powered Applications" index=29>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-29 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09015v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09015v2.pdf filename=2402.09015v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The rapid development in the field of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> has led to a surge in applications that facilitate collaboration among multiple agents to assist humans in their daily tasks. However, a significant gap remains in assessing whether <b>LLM-powered</b> applications genuinely enhance user experience and task execution efficiency. This highlights the pressing need for methods to verify utility of <b>LLM-powered</b> applications, particularly by ensuring alignment between the application&rsquo;s functionality and end-user needs. We introduce AgentEval provides an implementation for the math problems}, a novel framework designed to simplify the utility verification process by automatically proposing a set of criteria tailored to the unique purpose of any given application. This allows for a comprehensive assessment, quantifying the utility of an application against the suggested criteria. We present a comprehensive analysis of the robustness of quantifier&rsquo;s work.</p></p class="citation"></blockquote><h3 id=3031--30261-tell-me-more-towards-implicit-user-intention-understanding-of-language-model-driven-agents-cheng-qian-et-al-2024>(30/31 | 30/261) Tell Me More! Towards Implicit User Intention Understanding of Language Model Driven Agents (Cheng Qian et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Cheng Qian, Bingxiang He, Zhong Zhuang, Jia Deng, Yujia Qin, Xin Cong, Zhong Zhang, Jie Zhou, Yankai Lin, Zhiyuan Liu, Maosong Sun. (2024)<br><strong>Tell Me More! Towards Implicit User Intention Understanding of Language Model Driven Agents</strong><br><button class=copy-to-clipboard title="Tell Me More! Towards Implicit User Intention Understanding of Language Model Driven Agents" index=30>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-30 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-HC, cs.CL<br>Keyword Score: 13<br>Keywords: Benchmarking, Mistral<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09205v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09205v2.pdf filename=2402.09205v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Current language model-driven agents often lack mechanisms for effective user participation, which is crucial given the vagueness commonly found in user instructions. Although adept at devising strategies and performing tasks, these agents struggle with seeking clarification and grasping precise user intentions. To bridge this gap, we introduce Intention-in-Interaction (IN3), a novel <b>benchmark</b> designed to inspect users&rsquo; implicit intentions through explicit queries. Next, we propose the incorporation of model experts as the upstream in agent designs to enhance user-agent interaction. Employing IN3, we empirically train <b>Mistral-Interact,</b> a powerful model that proactively assesses task vagueness, inquires user intentions, and refines them into actionable goals before starting downstream agent task execution. Integrating it into the XAgent framework, we comprehensively evaluate the enhanced agent system regarding user instruction understanding and execution, revealing that our approach notably excels at identifying vague user tasks, recovering and summarizing critical missing information, setting precise and necessary agent execution goals, and minimizing redundant tool usage, thus boosting overall efficiency. All the data and codes are released.</p></p class="citation"></blockquote><h3 id=3131--31261-generating-diverse-translation-with-perturbed-knn-mt-yuto-nishida-et-al-2024>(31/31 | 31/261) Generating Diverse Translation with Perturbed kNN-MT (Yuto Nishida et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuto Nishida, Makoto Morishita, Hidetaka Kamigaito, Taro Watanabe. (2024)<br><strong>Generating Diverse Translation with Perturbed kNN-MT</strong><br><button class=copy-to-clipboard title="Generating Diverse Translation with Perturbed kNN-MT" index=31>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-31 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 10<br>Keywords: Neural Machine Translation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09344v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09344v1.pdf filename=2402.09344v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Generating multiple translation candidates would enable users to choose the one that satisfies their needs. Although there has been work on diversified generation, there exists room for improving the diversity mainly because the previous methods do not address the overcorrection problem &ndash; the model underestimates a prediction that is largely different from the training data, even if that prediction is likely. This paper proposes methods that generate more diverse translations by introducing perturbed k-nearest neighbor <b>machine</b> <b>translation</b> (kNN-MT). Our methods expand the search space of kNN-MT and help incorporate diverse words into candidates by addressing the overcorrection problem. Our experiments show that the proposed methods drastically improve candidate diversity and control the degree of diversity by tuning the perturbation&rsquo;s magnitude.</p></p class="citation"></blockquote><h2 id=csai-27>cs.AI (27)</h2><h3 id=127--32261-mustard-mastering-uniform-synthesis-of-theorem-and-proof-data-yinya-huang-et-al-2024>(1/27 | 32/261) MUSTARD: Mastering Uniform Synthesis of Theorem and Proof Data (Yinya Huang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yinya Huang, Xiaohan Lin, Zhengying Liu, Qingxing Cao, Huajian Xin, Haiming Wang, Zhenguo Li, Linqi Song, Xiaodan Liang. (2024)<br><strong>MUSTARD: Mastering Uniform Synthesis of Theorem and Proof Data</strong><br><button class=copy-to-clipboard title="MUSTARD: Mastering Uniform Synthesis of Theorem and Proof Data" index=32>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-32 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CL, cs-FL, cs-LG, cs-PL, cs.AI<br>Keyword Score: 83<br>Keywords: Benchmarking, Fine-tuning, Fine-tuning, LLaMA, Mathematical Reasoning, Reasoning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08957v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08957v1.pdf filename=2402.08957v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have witnessed significant advancement in various tasks, including <b>mathematical</b> <b>reasoning</b> and theorem proving. As these two tasks require strict and formal multi-step inference, they are appealing domains for exploring the <b>reasoning</b> ability of <b>LLMs</b> but still face important challenges. Previous studies such as Chain-of-Thought (CoT) have revealed the effectiveness of intermediate steps guidance. However, such step-wise annotation requires heavy labor, leading to insufficient training steps for current <b>benchmarks.</b> To fill this gap, this work introduces MUSTARD, a data generation framework that masters uniform synthesis of theorem and proof data of high quality and diversity. MUSTARD synthesizes data in three stages: (1) It samples a few <b>mathematical</b> <b>concept</b> seeds as the problem category. (2) Then, it <b>prompts</b> a generative language model with the sampled concepts to obtain both the problems and their step-wise formal solutions. (3) Lastly, the framework utilizes a proof assistant (e.g., Lean Prover) to filter the valid proofs. With the proposed MUSTARD, we present a theorem-and-proof <b>benchmark</b> MUSTARDSAUCE with 5,866 valid data points. Each data point contains an informal statement, an informal proof, and a translated formal proof that passes the prover validation. We perform extensive analysis and demonstrate that MUSTARD generates validated high-quality step-by-step data. We further apply the MUSTARDSAUCE for <b>fine-tuning</b> smaller language models. The <b>fine-tuned</b> <b>Llama</b> 2-7B achieves a 15.41% average relative performance gain in automated theorem proving, and 8.18% in math word problems. Codes and data are available at <a href=https://github.com/Eleanor-H/MUSTARD>https://github.com/Eleanor-H/MUSTARD</a>.</p></p class="citation"></blockquote><h3 id=227--33261-l3go-language-agents-with-chain-of-3d-thoughts-for-generating-unconventional-objects-yutaro-yamada-et-al-2024>(2/27 | 33/261) L3GO: Language Agents with Chain-of-3D-Thoughts for Generating Unconventional Objects (Yutaro Yamada et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yutaro Yamada, Khyathi Chandu, Yuchen Lin, Jack Hessel, Ilker Yildirim, Yejin Choi. (2024)<br><strong>L3GO: Language Agents with Chain-of-3D-Thoughts for Generating Unconventional Objects</strong><br><button class=copy-to-clipboard title="L3GO: Language Agents with Chain-of-3D-Thoughts for Generating Unconventional Objects" index=33>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-33 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 73<br>Keywords: Benchmarking, Out-of-distribution, Simulation, Simulator, GPT, GPT-4, Reasoning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09052v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09052v1.pdf filename=2402.09052v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Diffusion-based image generation models such as DALL-E 3 and Stable Diffusion-XL demonstrate remarkable capabilities in generating images with realistic and unique compositions. Yet, these models are not robust in precisely <b>reasoning</b> about physical and spatial configurations of objects, especially when instructed with unconventional, thereby <b>out-of-distribution</b> descriptions, such as &ldquo;a chair with five legs&rdquo;. In this paper, we propose a language agent with chain-of-3D-thoughts (L3GO), an inference-time approach that can reason about part-based 3D mesh generation of unconventional objects that current data-driven diffusion models struggle with. More concretely, we use <b>large</b> <b>language</b> <b>models</b> as agents to compose a desired object via trial-and-error within the 3D <b>simulation</b> environment. To facilitate our investigation, we develop a new <b>benchmark,</b> Unconventionally Feasible Objects (UFO), as well as SimpleBlenv, a wrapper environment built on top of Blender where language agents can build and compose atomic building blocks via API calls. Human and automatic <b>GPT-4V</b> evaluations show that our approach surpasses the standard <b>GPT-4</b> and other language agents (e.g., ReAct and Reflexion) for 3D mesh generation on ShapeNet. Moreover, when tested on our UFO <b>benchmark,</b> our approach outperforms other state-of-the-art text-to-2D image and text-to-3D models based on human evaluation.</p></p class="citation"></blockquote><h3 id=327--34261-llasmol-advancing-large-language-models-for-chemistry-with-a-large-scale-comprehensive-high-quality-instruction-tuning-dataset-botao-yu-et-al-2024>(3/27 | 34/261) LlaSMol: Advancing Large Language Models for Chemistry with a Large-Scale, Comprehensive, High-Quality Instruction Tuning Dataset (Botao Yu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Botao Yu, Frazier N. Baker, Ziqi Chen, Xia Ning, Huan Sun. (2024)<br><strong>LlaSMol: Advancing Large Language Models for Chemistry with a Large-Scale, Comprehensive, High-Quality Instruction Tuning Dataset</strong><br><button class=copy-to-clipboard title="LlaSMol: Advancing Large Language Models for Chemistry with a Large-Scale, Comprehensive, High-Quality Instruction Tuning Dataset" index=34>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-34 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CE, cs-CL, cs.AI<br>Keyword Score: 70<br>Keywords: Fine-tuning, GPT, GPT-4, Mistral, Instruction Tuning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09391v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09391v2.pdf filename=2402.09391v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Chemistry plays a crucial role in many domains, such as drug discovery and material science. While <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> such as <b>GPT-4</b> exhibit remarkable capabilities on natural language processing tasks, existing work shows their performance on chemistry tasks is discouragingly low. In this paper, however, we demonstrate that our developed <b>LLMs</b> can achieve very strong results on a comprehensive set of chemistry tasks, outperforming the most advanced <b>GPT-4</b> across all the tasks by a substantial margin and approaching the SoTA task-specific models. The key to our success is a <b>large-scale,</b> <b>comprehensive,</b> <b>high-quality</b> dataset for <b>instruction</b> <b>tuning</b> named SMolInstruct. It contains 14 meticulously selected chemistry tasks and over three million high-quality samples, laying a solid foundation for training and evaluating <b>LLMs</b> for chemistry. Based on SMolInstruct, we <b>fine-tune</b> a set of open-source <b>LLMs,</b> among which, we find that <b>Mistral</b> serves as the best base model for chemistry tasks. We further conduct analysis on the impact of trainable parameters, providing insights for future research.</p></p class="citation"></blockquote><h3 id=427--35261-llm-enhanced-user-item-interactions-leveraging-edge-information-for-optimized-recommendations-xinyuan-wang-et-al-2024>(4/27 | 35/261) LLM-Enhanced User-Item Interactions: Leveraging Edge Information for Optimized Recommendations (Xinyuan Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xinyuan Wang, Liang Wu, Liangjie Hong, Hao Liu, Yanjie Fu. (2024)<br><strong>LLM-Enhanced User-Item Interactions: Leveraging Edge Information for Optimized Recommendations</strong><br><button class=copy-to-clipboard title="LLM-Enhanced User-Item Interactions: Leveraging Edge Information for Optimized Recommendations" index=35>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-35 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-IR, cs.AI<br>Keyword Score: 63<br>Keywords: Graph, Graph Neural Network, Graph Neural Network, Recommendation, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09617v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09617v1.pdf filename=2402.09617v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The extraordinary performance of <b>large</b> <b>language</b> <b>models</b> has not only reshaped the research landscape in the field of NLP but has also demonstrated its exceptional applicative potential in various domains. However, the potential of these models in mining relationships from <b>graph</b> <b>data</b> <b>remains</b> under-explored. <b>Graph</b> <b>neural</b> <b>networks,</b> as a popular research area in recent years, have numerous studies on relationship mining. Yet, current cutting-edge research in <b>graph</b> <b>neural</b> <b>networks</b> has not been effectively integrated with <b>large</b> <b>language</b> <b>models,</b> leading to limited efficiency and capability in <b>graph</b> <b>relationship</b> <b>mining</b> tasks. A primary challenge is the inability of <b>LLMs</b> to deeply exploit the edge information in <b>graphs,</b> <b>which</b> <b>is</b> critical for understanding complex node relationships. This gap limits the potential of <b>LLMs</b> to extract meaningful insights from <b>graph</b> <b>structures,</b> <b>limiting</b> their applicability in more complex <b>graph-based</b> <b>analysis.</b> <b>We</b> focus on how to utilize existing <b>LLMs</b> for mining and understanding relationships in <b>graph</b> <b>data,</b> <b>applying</b> these techniques to <b>recommendation</b> tasks. We propose an innovative framework that combines the strong contextual representation capabilities of <b>LLMs</b> with the relationship extraction and analysis functions of <b>GNNs</b> for mining relationships in <b>graph</b> <b>data.</b> <b>Specifically,</b> we design a new <b>prompt</b> construction framework that integrates relational information of <b>graph</b> <b>data</b> <b>into</b> natural language expressions, aiding <b>LLMs</b> in more intuitively grasping the connectivity information within <b>graph</b> <b>data.</b> <b>Additionally,</b> we introduce <b>graph</b> <b>relationship</b> <b>understanding</b> and analysis functions into <b>LLMs</b> to enhance their focus on connectivity information in <b>graph</b> <b>data.</b> <b>Our</b> evaluation on real-world datasets demonstrates the framework&rsquo;s ability to understand connectivity information in <b>graph</b> data.</p></p class="citation"></blockquote><h3 id=527--36261-large-language-model-with-graph-convolution-for-recommendation-yingpeng-du-et-al-2024>(5/27 | 36/261) Large Language Model with Graph Convolution for Recommendation (Yingpeng Du et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yingpeng Du, Ziyan Wang, Zhu Sun, Haoyan Chua, Hongzhi Liu, Zhonghai Wu, Yining Ma, Jie Zhang, Youchen Sun. (2024)<br><strong>Large Language Model with Graph Convolution for Recommendation</strong><br><button class=copy-to-clipboard title="Large Language Model with Graph Convolution for Recommendation" index=36>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-36 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 63<br>Keywords: Graph, Convolution, Recommendation, Reasoning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08859v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08859v1.pdf filename=2402.08859v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In recent years, efforts have been made to use text information for better user profiling and item characterization in <b>recommendations.</b> However, text information can sometimes be of low quality, hindering its effectiveness for real-world applications. With knowledge and <b>reasoning</b> capabilities capsuled in <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs),</b> utilizing <b>LLMs</b> emerges as a promising way for description improvement. However, existing ways of <b>prompting</b> <b>LLMs</b> with raw texts ignore structured knowledge of user-item interactions, which may lead to hallucination problems like inconsistent description generation. To this end, we propose a <b>Graph-aware</b> <b>Convolutional</b> <b>LLM</b> method to elicit <b>LLMs</b> to capture high-order relations in the user-item <b>graph.</b> To adapt text-based <b>LLMs</b> with structured <b>graphs,</b> We use the <b>LLM</b> as an aggregator in <b>graph</b> processing, allowing it to understand <b>graph-based</b> information step by step. Specifically, the <b>LLM</b> is required for description enhancement by exploring multi-hop neighbors layer by layer, thereby propagating information progressively in the <b>graph.</b> To enable <b>LLMs</b> to capture <b>large-scale</b> <b>graph</b> <b>information,</b> we break down the description task into smaller parts, which drastically reduces the context length of the token input with each step. Extensive experiments on three real-world datasets show that our method consistently outperforms state-of-the-art methods.</p></p class="citation"></blockquote><h3 id=627--37261-role-playing-simulation-games-using-chatgpt-rita-stampfl-et-al-2024>(6/27 | 37/261) Role-Playing Simulation Games using ChatGPT (Rita Stampfl et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rita Stampfl, Igor Ivkić, Barbara Geyer. (2024)<br><strong>Role-Playing Simulation Games using ChatGPT</strong><br><button class=copy-to-clipboard title="Role-Playing Simulation Games using ChatGPT" index=37>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-37 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-HC, cs.AI<br>Keyword Score: 60<br>Keywords: Active Learning, Simulation, Simulator, ChatGPT, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09161v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09161v1.pdf filename=2402.09161v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Since the COVID-19 pandemic, educational institutions have embarked on digital transformation projects. The success of these projects depends on integrating new technologies and understanding the needs of digitally literate students. The &ldquo;learning by doing&rdquo; approach suggests that real success in learning new skills is achieved when students can try out and practise these skills. In this article, we demonstrate how <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> can enhance the quality of teaching by using <b>ChatGPT</b> in a role-playing <b>simulation</b> game scenario to promote <b>active</b> <b>learning.</b> Moreover, we discuss how <b>LLMs</b> can boost students&rsquo; interest in learning by allowing them to practice real-life scenarios using <b>ChatGPT.</b></p></p class="citation"></blockquote><h3 id=727--38261-groundial-human-norm-grounded-safe-dialog-response-generation-siwon-kim-et-al-2024>(7/27 | 38/261) GrounDial: Human-norm Grounded Safe Dialog Response Generation (Siwon Kim et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Siwon Kim, Shuyang Dai, Mohammad Kachuee, Shayan Ray, Tara Taghavi, Sungroh Yoon. (2024)<br><strong>GrounDial: Human-norm Grounded Safe Dialog Response Generation</strong><br><button class=copy-to-clipboard title="GrounDial: Human-norm Grounded Safe Dialog Response Generation" index=38>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-38 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 60<br>Keywords: Fine-tuning, Grounding, In-context Learning, In-context Learning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08968v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08968v1.pdf filename=2402.08968v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Current conversational AI systems based on <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> are known to generate unsafe responses, agreeing to offensive user input or including toxic content. Previous research aimed to alleviate the toxicity, by <b>fine-tuning</b> <b>LLM</b> with manually annotated safe dialogue histories. However, the dependency on additional tuning requires substantial costs. To remove the dependency, we propose GrounDial, where response safety is achieved by <b>grounding</b> responses to commonsense social rules without requiring <b>fine-tuning.</b> A hybrid approach of <b>in-context</b> <b>learning</b> and human-norm-guided decoding of GrounDial enables the response to be quantitatively and qualitatively safer even without additional data or tuning.</p></p class="citation"></blockquote><h3 id=827--39261-fgeo-tp-a-language-model-enhanced-solver-for-geometry-problems-yiming-he-et-al-2024>(8/27 | 39/261) FGeo-TP: A Language Model-Enhanced Solver for Geometry Problems (Yiming He et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yiming He, Jia Zou, Xiaokai Zhang, Na Zhu, Tuo Leng. (2024)<br><strong>FGeo-TP: A Language Model-Enhanced Solver for Geometry Problems</strong><br><button class=copy-to-clipboard title="FGeo-TP: A Language Model-Enhanced Solver for Geometry Problems" index=39>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-39 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 55<br>Keywords: Geometry, Pruning, BART, T5, Transformer, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09047v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09047v1.pdf filename=2402.09047v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The application of contemporary artificial intelligence techniques to address geometric problems and automated deductive proof has always been a grand challenge to the interdiscipline field of mathematics and artificial Intelligence. This is the fourth article in a series of our works, in our previous work, we established of a geometric formalized system known as FormalGeo. Moreover we annotated approximately 7000 geometric problems, forming the FormalGeo7k dataset. Despite the FGPS (Formal <b>Geometry</b> Problem Solver) can achieve interpretable algebraic equation solving and human-like deductive <b>reasoning,</b> it often experiences timeouts due to the complexity of the search strategy. In this paper, we introduced FGeo-TP (Theorem Predictor), which utilizes the language model to predict theorem sequences for solving <b>geometry</b> problems. We compared the effectiveness of various <b>Transformer</b> architectures, such as <b>BART</b> or <b>T5,</b> in theorem prediction, implementing <b>pruning</b> in the search process of FGPS, thereby improving its performance in solving <b>geometry</b> problems. Our results demonstrate a significant increase in the problem-solving rate of the language model-enhanced FGeo-TP on the FormalGeo7k dataset, rising from 39.7% to 80.86%. Furthermore, FGeo-TP exhibits notable reductions in solving time and search steps across problems of varying difficulty levels.</p></p class="citation"></blockquote><h3 id=927--40261-using-counterfactual-tasks-to-evaluate-the-generality-of-analogical-reasoning-in-large-language-models-martha-lewis-et-al-2024>(9/27 | 40/261) Using Counterfactual Tasks to Evaluate the Generality of Analogical Reasoning in Large Language Models (Martha Lewis et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Martha Lewis, Melanie Mitchell. (2024)<br><strong>Using Counterfactual Tasks to Evaluate the Generality of Analogical Reasoning in Large Language Models</strong><br><button class=copy-to-clipboard title="Using Counterfactual Tasks to Evaluate the Generality of Analogical Reasoning in Large Language Models" index=40>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-40 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CL, cs.AI<br>Keyword Score: 53<br>Keywords: Benchmarking, Counter-factual, GPT, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08955v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08955v1.pdf filename=2402.08955v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have performed well on several <b>reasoning</b> <b>benchmarks,</b> including ones that test analogical <b>reasoning</b> abilities. However, it has been debated whether they are actually performing humanlike abstract <b>reasoning</b> or instead employing less general processes that rely on similarity to what has been seen in their training data. Here we investigate the generality of analogy-making abilities previously claimed for <b>LLMs</b> (Webb, Holyoak, & Lu, 2023). We take one set of analogy problems used to evaluate <b>LLMs</b> and create a set of <b>&ldquo;counterfactual&rdquo;</b> variants-versions that test the same abstract <b>reasoning</b> abilities but that are likely dissimilar from any pre-training data. We test humans and three <b>GPT</b> models on both the original and <b>counterfactual</b> problems, and show that, while the performance of humans remains high for all the problems, the <b>GPT</b> models&rsquo; performance declines sharply on the <b>counterfactual</b> set. This work provides evidence that, despite previously reported successes of <b>LLMs</b> on analogical <b>reasoning,</b> these models lack the robustness and generality of human analogy-making.</p></p class="citation"></blockquote><h3 id=1027--41261-hgot-hierarchical-graph-of-thoughts-for-retrieval-augmented-in-context-learning-in-factuality-evaluation-yihao-fang-et-al-2024>(10/27 | 41/261) HGOT: Hierarchical Graph of Thoughts for Retrieval-Augmented In-Context Learning in Factuality Evaluation (Yihao Fang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yihao Fang, Stephen W. Thomas, Xiaodan Zhu. (2024)<br><strong>HGOT: Hierarchical Graph of Thoughts for Retrieval-Augmented In-Context Learning in Factuality Evaluation</strong><br><button class=copy-to-clipboard title="HGOT: Hierarchical Graph of Thoughts for Retrieval-Augmented In-Context Learning in Factuality Evaluation" index=41>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-41 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CL, cs.AI<br>Keyword Score: 43<br>Keywords: Graph, In-context Learning, In-context Learning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09390v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09390v1.pdf filename=2402.09390v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the widespread adoption of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> in numerous applications, the challenge of factuality and the propensity for hallucinations raises significant concerns. To address this issue, particularly in retrieval-augmented <b>in-context</b> <b>learning,</b> we introduce the hierarchical <b>graph</b> of thoughts (HGOT), a structured, multi-layered <b>graph</b> approach designed to enhance the retrieval of pertinent passages during <b>in-context</b> <b>learning.</b> The framework utilizes the emergent planning capabilities of <b>LLMs,</b> employing the divide-and-conquer strategy to break down complex queries into manageable sub-queries. It refines self-consistency majority voting for answer selection, which incorporates the recently proposed citation recall and precision metrics to assess the quality of thoughts, linking an answer&rsquo;s credibility intrinsically to the thought&rsquo;s quality. This methodology introduces a weighted system in majority voting, prioritizing answers based on the citation quality of their thoughts. Additionally, we propose a scoring mechanism for evaluating retrieved passages, considering factors such as citation frequency and quality, self-consistency confidence, and the retrieval module&rsquo;s ranking. Experiments reveal that HGOT outperforms other retrieval-augmented <b>in-context</b> <b>learning</b> methods, including Demonstrate-Search-Predict (DSP), ReAct, Self-Ask, and Retrieve-then-Read on different datasets by as much as $7%$, demonstrating its efficacy in enhancing the factuality of <b>LLMs.</b></p></p class="citation"></blockquote><h3 id=1127--42261-premise-order-matters-in-reasoning-with-large-language-models-xinyun-chen-et-al-2024>(11/27 | 42/261) Premise Order Matters in Reasoning with Large Language Models (Xinyun Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xinyun Chen, Ryan A. Chi, Xuezhi Wang, Denny Zhou. (2024)<br><strong>Premise Order Matters in Reasoning with Large Language Models</strong><br><button class=copy-to-clipboard title="Premise Order Matters in Reasoning with Large Language Models" index=42>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-42 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CL, cs.AI<br>Keyword Score: 43<br>Keywords: Benchmarking, Reasoning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08939v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08939v1.pdf filename=2402.08939v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have accomplished remarkable <b>reasoning</b> performance in various domains. However, in the domain of <b>reasoning</b> tasks, we discover a frailty: <b>LLMs</b> are surprisingly brittle to the ordering of the premises, despite the fact that such ordering does not alter the underlying task. In particular, we observe that <b>LLMs</b> achieve the best performance when the premise order aligns with the context required in intermediate <b>reasoning</b> steps. For example, in deductive <b>reasoning</b> tasks, presenting the premises in the same order as the ground truth proof in the <b>prompt</b> (as opposed to random ordering) drastically increases the model&rsquo;s accuracy. We first examine the effect of premise ordering on deductive <b>reasoning</b> on a variety of <b>LLMs,</b> and our evaluation shows that permuting the premise order can cause a performance drop of over 30%. In addition, we release the <b>benchmark</b> R-GSM, based on GSM8K, to examine the ordering effect for mathematical problem-solving, and we again observe a significant drop in accuracy, relative to the original GSM8K <b>benchmark.</b></p></p class="citation"></blockquote><h3 id=1227--43261-emerging-opportunities-of-using-large-language-models-for-translation-between-drug-molecules-and-indications-david-oniani-et-al-2024>(12/27 | 43/261) Emerging Opportunities of Using Large Language Models for Translation Between Drug Molecules and Indications (David Oniani et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>David Oniani, Jordan Hilsman, Chengxi Zang, Junmei Wang, Lianjin Cai, Jan Zawala, Yanshan Wang. (2024)<br><strong>Emerging Opportunities of Using Large Language Models for Translation Between Drug Molecules and Indications</strong><br><button class=copy-to-clipboard title="Emerging Opportunities of Using Large Language Models for Translation Between Drug Molecules and Indications" index=43>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-43 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CL, cs.AI<br>Keyword Score: 40<br>Keywords: Generative AI, T5, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09588v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09588v2.pdf filename=2402.09588v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A drug molecule is a substance that changes the organism&rsquo;s mental or physical state. Every approved drug has an indication, which refers to the therapeutic use of that drug for treating a particular medical condition. While the <b>Large</b> <b>Language</b> <b>Model</b> <b>(LLM),</b> a <b>generative</b> <b>Artificial</b> Intelligence (AI) technique, has recently demonstrated effectiveness in translating between molecules and their textual descriptions, there remains a gap in research regarding their application in facilitating the translation between drug molecules and indications, or vice versa, which could greatly benefit the drug discovery process. The capability of generating a drug from a given indication would allow for the discovery of drugs targeting specific diseases or targets and ultimately provide patients with better treatments. In this paper, we first propose a new task, which is the translation between drug molecules and corresponding indications, and then test existing <b>LLMs</b> on this new task. Specifically, we consider nine variations of the <b>T5</b> <b>LLM</b> and evaluate them on two public datasets obtained from ChEMBL and DrugBank. Our experiments show the early results of using <b>LLMs</b> for this task and provide a perspective on the state-of-the-art. We also emphasize the current limitations and discuss future work that has the potential to improve the performance on this task. The creation of molecules from indications, or vice versa, will allow for more efficient targeting of diseases and significantly reduce the cost of drug discovery, with the potential to revolutionize the field of drug discovery in the era of <b>generative</b> <b>AI.</b></p></p class="citation"></blockquote><h3 id=1327--44261-large-language-model-based-interpretable-machine-learning-control-in-building-energy-systems-liang-zhang-et-al-2024>(13/27 | 44/261) Large Language Model-Based Interpretable Machine Learning Control in Building Energy Systems (Liang Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Liang Zhang, Zhelun Chen. (2024)<br><strong>Large Language Model-Based Interpretable Machine Learning Control in Building Energy Systems</strong><br><button class=copy-to-clipboard title="Large Language Model-Based Interpretable Machine Learning Control in Building Energy Systems" index=44>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-44 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-HC, cs.AI<br>Keyword Score: 40<br>Keywords: In-context Learning, In-context Learning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09584v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09584v1.pdf filename=2402.09584v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The potential of Machine Learning Control (MLC) in HVAC systems is hindered by its opaque nature and inference mechanisms, which is challenging for users and modelers to fully comprehend, ultimately leading to a lack of trust in MLC-based decision-making. To address this challenge, this paper investigates and explores Interpretable Machine Learning (IML), a branch of Machine Learning (ML) that enhances transparency and understanding of models and their inferences, to improve the credibility of MLC and its industrial application in HVAC systems. Specifically, we developed an innovative framework that combines the principles of Shapley values and the <b>in-context</b> <b>learning</b> feature of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs).</b> While the Shapley values are instrumental in dissecting the contributions of various features in ML models, <b>LLM</b> provides an in-depth understanding of rule-based parts in MLC; combining them, <b>LLM</b> further packages these insights into a coherent, human-understandable narrative. The paper presents a case study to demonstrate the feasibility of the developed IML framework for model predictive control-based precooling under demand response events in a virtual testbed. The results indicate that the developed framework generates and explains the control signals in accordance with the rule-based rationale.</p></p class="citation"></blockquote><h3 id=1427--45261-integrating-chatgpt-into-secure-hospital-networks-a-case-study-on-improving-radiology-report-analysis-kyungsu-kim-et-al-2024>(14/27 | 45/261) Integrating ChatGPT into Secure Hospital Networks: A Case Study on Improving Radiology Report Analysis (Kyungsu Kim et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kyungsu Kim, Junhyun Park, Saul Langarica, Adham Mahmoud Alkhadrawi, Synho Do. (2024)<br><strong>Integrating ChatGPT into Secure Hospital Networks: A Case Study on Improving Radiology Report Analysis</strong><br><button class=copy-to-clipboard title="Integrating ChatGPT into Secure Hospital Networks: A Case Study on Improving Radiology Report Analysis" index=45>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-45 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-LG, cs.AI<br>Keyword Score: 40<br>Keywords: Contrastive Learning, Knowledge Distillation, Knowledge Distillation, ChatGPT<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09358v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09358v1.pdf filename=2402.09358v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This study demonstrates the first in-hospital adaptation of a cloud-based AI, similar to <b>ChatGPT,</b> into a secure model for analyzing radiology reports, prioritizing patient data privacy. By employing a unique sentence-level <b>knowledge</b> <b>distillation</b> method through <b>contrastive</b> <b>learning,</b> we achieve over 95% accuracy in detecting anomalies. The model also accurately flags uncertainties in its predictions, enhancing its reliability and interpretability for physicians with certainty indicators. These advancements represent significant progress in developing secure and efficient AI tools for healthcare, suggesting a promising future for in-hospital AI applications with minimal supervision.</p></p class="citation"></blockquote><h3 id=1527--46261-developing-a-framework-for-auditing-large-language-models-using-human-in-the-loop-maryam-amirizaniani-et-al-2024>(15/27 | 46/261) Developing a Framework for Auditing Large Language Models Using Human-in-the-Loop (Maryam Amirizaniani et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Maryam Amirizaniani, Jihan Yao, Adrian Lavergne, Elizabeth Snell Okada, Aman Chadha, Tanya Roosta, Chirag Shah. (2024)<br><strong>Developing a Framework for Auditing Large Language Models Using Human-in-the-Loop</strong><br><button class=copy-to-clipboard title="Developing a Framework for Auditing Large Language Models Using Human-in-the-Loop" index=46>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-46 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 40<br>Keywords: human-in-the-loop, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09346v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09346v2.pdf filename=2402.09346v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>As <b>LLMs</b> become more pervasive across various users and scenarios, identifying potential issues when using these models becomes essential. Examples include bias, inconsistencies, and hallucination. Although auditing the <b>LLM</b> for these problems is desirable, it is far from being easy or solved. An effective method is to probe the <b>LLM</b> using different versions of the same question. This could expose inconsistencies in its knowledge or operation, indicating potential for bias or hallucination. However, to operationalize this auditing method at scale, we need an approach to create those probes reliably and automatically. In this paper we propose an automatic and scalable solution, where one uses a different <b>LLM</b> along with <b>human-in-the-loop.</b> This approach offers verifiability and transparency, while avoiding circular reliance on the same <b>LLMs,</b> and increasing scientific rigor and generalizability. Specifically, we present a novel methodology with two phases of verification using humans: standardized evaluation criteria to verify responses, and a structured <b>prompt</b> template to generate desired probes. Experiments on a set of questions from TruthfulQA dataset show that we can generate a reliable set of probes from one <b>LLM</b> that can be used to audit inconsistencies in a different <b>LLM.</b> The criteria for generating and applying auditing probes is generalizable to various <b>LLMs</b> regardless of the underlying structure or training mechanism.</p></p class="citation"></blockquote><h3 id=1627--47261-exploring-the-adversarial-capabilities-of-large-language-models-lukas-struppek-et-al-2024>(16/27 | 47/261) Exploring the Adversarial Capabilities of Large Language Models (Lukas Struppek et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lukas Struppek, Minh Hieu Le, Dominik Hintersdorf, Kristian Kersting. (2024)<br><strong>Exploring the Adversarial Capabilities of Large Language Models</strong><br><button class=copy-to-clipboard title="Exploring the Adversarial Capabilities of Large Language Models" index=47>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-47 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-LG, cs.AI<br>Keyword Score: 40<br>Keywords: Hate Speech Detection, Language Generation, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09132v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09132v2.pdf filename=2402.09132v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The proliferation of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> has sparked widespread and general interest due to their strong <b>language</b> <b>generation</b> capabilities, offering great potential for both industry and research. While previous research delved into the security and privacy issues of <b>LLMs,</b> the extent to which these models can exhibit adversarial behavior remains largely unexplored. Addressing this gap, we investigate whether common publicly available <b>LLMs</b> have inherent capabilities to perturb text samples to fool safety measures, so-called adversarial examples resp.~attacks. More specifically, we investigate whether <b>LLMs</b> are inherently able to craft adversarial examples out of benign samples to fool existing safe rails. Our experiments, which focus on <b>hate</b> <b>speech</b> <b>detection,</b> reveal that <b>LLMs</b> succeed in finding adversarial perturbations, effectively undermining <b>hate</b> <b>speech</b> <b>detection</b> systems. Our findings carry significant implications for (semi-)autonomous systems relying on <b>LLMs,</b> highlighting potential challenges in their interaction with existing systems and safety measures.</p></p class="citation"></blockquote><h3 id=1727--48261-fgeo-drl-deductive-reasoning-for-geometric-problems-through-deep-reinforcement-learning-jia-zou-et-al-2024>(17/27 | 48/261) FGeo-DRL: Deductive Reasoning for Geometric Problems through Deep Reinforcement Learning (Jia Zou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jia Zou, Xiaokai Zhang, Yiming He, Na Zhu, Tuo Leng. (2024)<br><strong>FGeo-DRL: Deductive Reasoning for Geometric Problems through Deep Reinforcement Learning</strong><br><button class=copy-to-clipboard title="FGeo-DRL: Deductive Reasoning for Geometric Problems through Deep Reinforcement Learning" index=48>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-48 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 35<br>Keywords: Geometry, Markov Decision Process, Reinforcement Learning, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09051v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09051v2.pdf filename=2402.09051v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The human-like automatic deductive <b>reasoning</b> has always been one of the most challenging open problems in the interdiscipline of mathematics and artificial intelligence. This paper is the third in a series of our works. We built a neural-symbolic system, called FGeoDRL, to automatically perform human-like geometric deductive <b>reasoning.</b> The neural part is an AI agent based on <b>reinforcement</b> <b>learning,</b> capable of autonomously learning problem-solving methods from the feedback of a formalized environment, without the need for human supervision. It leverages a pre-trained natural language model to establish a policy network for theorem selection and employ Monte Carlo Tree Search for heuristic exploration. The symbolic part is a <b>reinforcement</b> <b>learning</b> environment based on <b>geometry</b> formalization theory and FormalGeo, which models GPS as a <b>Markov</b> <b>Decision</b> <b>Process.</b> In this formal symbolic system, the known conditions and objectives of the problem form the state space, while the set of theorems forms the action space. Leveraging FGeoDRL, we have achieved readable and verifiable automated solutions to geometric problems. Experiments conducted on the formalgeo7k dataset have achieved a problem-solving success rate of 86.40%. The project is available at <a href=https://github.com/PersonNoName/FGeoDRL>https://github.com/PersonNoName/FGeoDRL</a>.</p></p class="citation"></blockquote><h3 id=1827--49261-into-the-unknown-self-learning-large-language-models-teddy-ferdinan-et-al-2024>(18/27 | 49/261) Into the Unknown: Self-Learning Large Language Models (Teddy Ferdinan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Teddy Ferdinan, Jan Kocoń, Przemysław Kazienko. (2024)<br><strong>Into the Unknown: Self-Learning Large Language Models</strong><br><button class=copy-to-clipboard title="Into the Unknown: Self-Learning Large Language Models" index=49>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-49 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 30<br>Keywords: Fine-tuning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09147v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09147v1.pdf filename=2402.09147v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We address the main problem of self-learning <b>LLM:</b> the question of what to learn. We propose a self-learning <b>LLM</b> framework that enables an <b>LLM</b> to independently learn previously unknown knowledge through self-assessment of their own hallucinations. Using the hallucination score, we introduce a new concept of Points in The Unknown (PiUs), along with one extrinsic and three intrinsic methods for automatic PiUs identification. It facilitates the creation of a self-learning loop that focuses exclusively on the knowledge gap in Points in The Unknown, resulting in a reduced hallucination score. We also developed evaluation metrics for gauging an <b>LLM&rsquo;s</b> self-learning capability. Our experiments revealed that 7B-Mistral models that have been <b>finetuned</b> or aligned are capable of self-learning considerably well. Our self-learning concept allows more efficient <b>LLM</b> updates and opens new perspectives for knowledge exchange. It may also increase public trust in AI.</p></p class="citation"></blockquote><h3 id=1927--50261-bidirectional-generative-pre-training-for-improving-time-series-representation-learning-ziyang-song-et-al-2024>(19/27 | 50/261) Bidirectional Generative Pre-training for Improving Time Series Representation Learning (Ziyang Song et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ziyang Song, Qincheng Lu, He Zhu, Yue Li. (2024)<br><strong>Bidirectional Generative Pre-training for Improving Time Series Representation Learning</strong><br><button class=copy-to-clipboard title="Bidirectional Generative Pre-training for Improving Time Series Representation Learning" index=50>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-50 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-LG, cs.AI<br>Keyword Score: 25<br>Keywords: Fine-tuning, Representation Learning, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09558v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09558v1.pdf filename=2402.09558v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Learning time-series <b>representations</b> <b>for</b> discriminative tasks has been a long-standing challenge. Current pre-training methods are limited in either unidirectional next-token prediction or randomly masked token prediction. We propose a novel architecture called Bidirectional Timely Generative Pre-trained <b>Transformer</b> (BiTimelyGPT), which pre-trains on time-series data by both next-token and previous-token predictions in alternating <b>transformer</b> layers. This pre-training task preserves original distribution and data shapes of the time-series. Additionally, the full-rank forward and backward attention matrices exhibit more expressive <b>representation</b> <b>capabilities.</b> Using biosignal data, BiTimelyGPT demonstrates superior performance in predicting neurological functionality, disease diagnosis, and physiological signs. By visualizing the attention heatmap, we observe that the pre-trained BiTimelyGPT can identify discriminative segments from time-series sequences, even more so after <b>fine-tuning</b> on the task.</p></p class="citation"></blockquote><h3 id=2027--51261-auditllm-a-tool-for-auditing-large-language-models-using-multiprobe-approach-maryam-amirizaniani-et-al-2024>(20/27 | 51/261) AuditLLM: A Tool for Auditing Large Language Models Using Multiprobe Approach (Maryam Amirizaniani et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Maryam Amirizaniani, Tanya Roosta, Aman Chadha, Chirag Shah. (2024)<br><strong>AuditLLM: A Tool for Auditing Large Language Models Using Multiprobe Approach</strong><br><button class=copy-to-clipboard title="AuditLLM: A Tool for Auditing Large Language Models Using Multiprobe Approach" index=51>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-51 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09334v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09334v1.pdf filename=2402.09334v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>As <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> gain wider adoption in various contexts, it becomes crucial to ensure they are reasonably safe, consistent, and reliable for an application at hand. This may require probing or auditing them. Probing <b>LLMs</b> with varied iterations of a single question could reveal potential inconsistencies in their knowledge or functionality. However, a tool for performing such audits with simple workflow and low technical threshold is lacking. In this demo, we introduce &ldquo;AuditLLM,&rdquo; a novel tool designed to evaluate the performance of various <b>LLMs</b> in a methodical way. AuditLLM&rsquo;s core functionality lies in its ability to test a given <b>LLM</b> by auditing it using multiple probes generated from a single question, thereby identifying any inconsistencies in the model&rsquo;s understanding or operation. A reasonably robust, reliable, and consistent <b>LLM</b> should output semantically similar responses for a question asked differently or by different people. Based on this assumption, AuditLLM produces easily interpretable results regarding the <b>LLM&rsquo;s</b> consistencies from a single question that the user enters. A certain level of inconsistency has been shown to be an indicator of potential bias, hallucinations, and other issues. One could then use the output of AuditLLM to further investigate issues with the aforementioned <b>LLM.</b> To facilitate demonstration and practical uses, AuditLLM offers two key modes: (1) Live mode which allows instant auditing of <b>LLMs</b> by analyzing responses to real-time queries; (2) Batch mode which facilitates comprehensive <b>LLM</b> auditing by processing multiple queries at once for in-depth analysis. This tool is beneficial for both researchers and general users, as it enhances our understanding of <b>LLMs&rsquo;</b> capabilities in generating responses, using a standardized auditing platform.</p></p class="citation"></blockquote><h3 id=2127--52261-spectral-filters-dark-signals-and-attention-sinks-nicola-cancedda-2024>(21/27 | 52/261) Spectral Filters, Dark Signals, and Attention Sinks (Nicola Cancedda, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nicola Cancedda. (2024)<br><strong>Spectral Filters, Dark Signals, and Attention Sinks</strong><br><button class=copy-to-clipboard title="Spectral Filters, Dark Signals, and Attention Sinks" index=52>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-52 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CL, cs.AI<br>Keyword Score: 20<br>Keywords: Transformer, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09221v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09221v1.pdf filename=2402.09221v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Projecting intermediate representations onto the vocabulary is an increasingly popular interpretation tool for <b>transformer-based</b> <b>LLMs,</b> also known as the logit lens. We propose a quantitative extension to this approach and define spectral filters on intermediate representations based on partitioning the singular vectors of the vocabulary embedding and unembedding matrices into bands. We find that the signals exchanged in the tail end of the spectrum are responsible for attention sinking (Xiao et al. 2023), of which we provide an explanation. We find that the loss of pretrained models can be kept low despite suppressing sizable parts of the embedding spectrum in a layer-dependent way, as long as attention sinking is preserved. Finally, we discover that the representation of tokens that draw attention from many tokens have large projections on the tail end of the spectrum.</p></p class="citation"></blockquote><h3 id=2227--53261-exploring-neuron-interactions-and-emergence-in-llms-from-the-multifractal-analysis-perspective-xiongye-xiao-et-al-2024>(22/27 | 53/261) Exploring Neuron Interactions and Emergence in LLMs: From the Multifractal Analysis Perspective (Xiongye Xiao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiongye Xiao, Chenyu Zhou, Heng Ping, Defu Cao, Yaxing Li, Yizhuo Zhou, Shixuan Li, Paul Bogdan. (2024)<br><strong>Exploring Neuron Interactions and Emergence in LLMs: From the Multifractal Analysis Perspective</strong><br><button class=copy-to-clipboard title="Exploring Neuron Interactions and Emergence in LLMs: From the Multifractal Analysis Perspective" index=53>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-53 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09099v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09099v1.pdf filename=2402.09099v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Prior studies on the emergence in <b>large</b> <b>models</b> <b>have</b> primarily focused on how the functional capabilities of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> scale with model size. Our research, however, transcends this traditional paradigm, aiming to deepen our understanding of the emergence within <b>LLMs</b> by placing a special emphasis not just on the model size but more significantly on the complex behavior of neuron interactions during the training process. By introducing the concepts of &ldquo;self-organization&rdquo; and &ldquo;multifractal analysis,&rdquo; we explore how neuron interactions dynamically evolve during training, leading to &ldquo;emergence,&rdquo; mirroring the phenomenon in natural systems where simple micro-level interactions give rise to complex macro-level behaviors. To quantitatively analyze the continuously evolving interactions among neurons in <b>large</b> <b>models</b> <b>during</b> training, we propose the Neuron-based Multifractal Analysis (NeuroMFA). Utilizing NeuroMFA, we conduct a comprehensive examination of the emergent behavior in <b>LLMs</b> through the lens of both model size and training process, paving new avenues for research into the emergence in large models.</p></p class="citation"></blockquote><h3 id=2327--54261-inference-of-abstraction-for-a-unified-account-of-reasoning-and-learning-hiroyuki-kido-2024>(23/27 | 54/261) Inference of Abstraction for a Unified Account of Reasoning and Learning (Hiroyuki Kido, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hiroyuki Kido. (2024)<br><strong>Inference of Abstraction for a Unified Account of Reasoning and Learning</strong><br><button class=copy-to-clipboard title="Inference of Abstraction for a Unified Account of Reasoning and Learning" index=54>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-54 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-LG, cs-LO, cs.AI<br>Keyword Score: 20<br>Keywords: MNIST, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09046v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09046v1.pdf filename=2402.09046v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Inspired by Bayesian approaches to brain function in neuroscience, we give a simple theory of probabilistic inference for a unified account of <b>reasoning</b> and learning. We simply model how data cause symbolic knowledge in terms of its satisfiability in formal logic. The underlying idea is that <b>reasoning</b> is a process of deriving symbolic knowledge from data via abstraction, i.e., selective ignorance. The logical consequence relation is discussed for its proof-based theoretical correctness. The <b>MNIST</b> dataset is discussed for its experiment-based empirical correctness.</p></p class="citation"></blockquote><h3 id=2427--55261-hycube-efficient-knowledge-hypergraph-3d-circular-convolutional-embedding-zhao-li-et-al-2024>(24/27 | 55/261) HyCubE: Efficient Knowledge Hypergraph 3D Circular Convolutional Embedding (Zhao Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhao Li, Xin Wang, Jianxin Li, Wenbin Guo, Jun Zhao. (2024)<br><strong>HyCubE: Efficient Knowledge Hypergraph 3D Circular Convolutional Embedding</strong><br><button class=copy-to-clipboard title="HyCubE: Efficient Knowledge Hypergraph 3D Circular Convolutional Embedding" index=55>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-55 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 20<br>Keywords: Convolution, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08961v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08961v1.pdf filename=2402.08961v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Existing knowledge hypergraph embedding methods mainly focused on improving model performance, but their model structures are becoming more complex and redundant. Furthermore, due to the inherent complex semantic knowledge, the computation of knowledge hypergraph embedding models is often very expensive, leading to low efficiency. In this paper, we propose a feature interaction and extraction-enhanced 3D circular <b>convolutional</b> <b>embedding</b> <b>model,</b> HyCubE, which designs a novel 3D circular <b>convolutional</b> <b>neural</b> <b>network</b> and introduces the alternate mask stack strategy to achieve efficient n-ary knowledge hypergraph embedding. By adaptively adjusting the 3D circular <b>convolution</b> kernel size and uniformly embedding the entity position information, HyCubE improves the model performance with fewer parameters and reaches a better trade-off between model performance and efficiency. In addition, we use 1-N multilinear scoring based on the entity mask mechanism to further accelerate the model training efficiency. Finally, extensive experimental results on all datasets demonstrate that HyCubE consistently outperforms state-of-the-art baselines, with an average improvement of 4.08%-10.77% and a maximum improvement of 21.16% across all metrics. Commendably, HyCubE speeds up by an average of 7.55x and reduces memory usage by an average of 77.02% compared to the latest state-of-the-art baselines.</p></p class="citation"></blockquote><h3 id=2527--56261-scamspot-fighting-financial-fraud-in-instagram-comments-stefan-erben-et-al-2024>(25/27 | 56/261) ScamSpot: Fighting Financial Fraud in Instagram Comments (Stefan Erben et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Stefan Erben, Andreas Waldis. (2024)<br><strong>ScamSpot: Fighting Financial Fraud in Instagram Comments</strong><br><button class=copy-to-clipboard title="ScamSpot: Fighting Financial Fraud in Instagram Comments" index=56>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-56 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 20<br>Keywords: Fine-tuning, BERT<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08869v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08869v1.pdf filename=2402.08869v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The long-standing problem of spam and fraudulent messages in the comment sections of Instagram pages in the financial sector claims new victims every day. Instagram&rsquo;s current spam filter proves inadequate, and existing research approaches are primarily confined to theoretical concepts. Practical implementations with evaluated results are missing. To solve this problem, we propose ScamSpot, a comprehensive system that includes a browser extension, a <b>fine-tuned</b> <b>BERT</b> model and a REST API. This approach ensures public accessibility of our results for Instagram users using the Chrome browser. Furthermore, we conduct a data annotation study, shedding light on the reasons and causes of the problem and evaluate the system through user feedback and comparison with existing models. ScamSpot is an open-source project and is publicly available at <a href=https://scamspot.github.io/>https://scamspot.github.io/</a>.</p></p class="citation"></blockquote><h3 id=2627--57261-graph-skeleton-1-nodes-are-sufficient-to-represent-billion-scale-graph-linfeng-cao-et-al-2024>(26/27 | 57/261) Graph-Skeleton: ~1% Nodes are Sufficient to Represent Billion-Scale Graph (Linfeng Cao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Linfeng Cao, Haoran Deng, Chunping Wang, Lei Chen, Yang Yang. (2024)<br><strong>Graph-Skeleton: ~1% Nodes are Sufficient to Represent Billion-Scale Graph</strong><br><button class=copy-to-clipboard title="Graph-Skeleton: ~1% Nodes are Sufficient to Represent Billion-Scale Graph" index=57>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-57 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 13<br>Keywords: Node Classification, Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09565v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09565v1.pdf filename=2402.09565v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Due to the ubiquity of <b>graph</b> data on the web, web <b>graph</b> mining has become a hot research spot. Nonetheless, the prevalence of large-scale web <b>graphs</b> in real applications poses significant challenges to storage, computational capacity and <b>graph</b> model design. Despite numerous studies to enhance the scalability of <b>graph</b> models, a noticeable gap remains between academic research and practical web <b>graph</b> mining applications. One major cause is that in most industrial scenarios, only a small part of <b>nodes</b> <b>in</b> a web <b>graph</b> are actually required to be analyzed, where we term these <b>nodes</b> <b>as</b> target <b>nodes,</b> <b>while</b> others as background <b>nodes.</b> <b>In</b> this paper, we argue that properly fetching and condensing the background <b>nodes</b> <b>from</b> massive web <b>graph</b> data might be a more economical shortcut to tackle the obstacles fundamentally. To this end, we make the first attempt to study the problem of massive background <b>nodes</b> <b>compression</b> for target <b>nodes</b> <b>classification.</b> Through extensive experiments, we reveal two critical roles played by the background <b>nodes</b> <b>in</b> target <b>node</b> <b>classification:</b> enhancing structural connectivity between target <b>nodes,</b> <b>and</b> feature correlation with target <b>nodes.</b> <b>Followingthis,</b> we propose a novel <b>Graph-Skeleton1</b> model, which properly fetches the background <b>nodes,</b> <b>and</b> further condenses the semantic and topological information of background <b>nodes</b> <b>within</b> similar target-background local structures. Extensive experiments on various web <b>graph</b> datasets demonstrate the effectiveness and efficiency of the proposed method. In particular, for MAG240M dataset with 0.24 billion <b>nodes,</b> <b>our</b> generated skeleton <b>graph</b> achieves highly comparable performance while only containing 1.8% <b>nodes</b> <b>of</b> the original <b>graph.</b></p></p class="citation"></blockquote><h3 id=2727--58261-machine-learning-in-management-of-precautionary-closures-caused-by-lipophilic-biotoxins-andres-molares-ulloa-et-al-2024>(27/27 | 58/261) Machine Learning in management of precautionary closures caused by lipophilic biotoxins (Andres Molares-Ulloa et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Andres Molares-Ulloa, Enrique Fernandez-Blanco, Alejandro Pazos, Daniel Rivero. (2024)<br><strong>Machine Learning in management of precautionary closures caused by lipophilic biotoxins</strong><br><button class=copy-to-clipboard title="Machine Learning in management of precautionary closures caused by lipophilic biotoxins" index=58>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-58 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 10<br>Keywords: BLOOM<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09266v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09266v1.pdf filename=2402.09266v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Mussel farming is one of the most important aquaculture industries. The main risk to mussel farming is harmful algal <b>blooms</b> (HABs), which pose a risk to human consumption. In Galicia, the Spanish main producer of cultivated mussels, the opening and closing of the production areas is controlled by a monitoring program. In addition to the closures resulting from the presence of toxicity exceeding the legal threshold, in the absence of a confirmatory sampling and the existence of risk factors, precautionary closures may be applied. These decisions are made by experts without the support or formalisation of the experience on which they are based. Therefore, this work proposes a predictive model capable of supporting the application of precautionary closures. Achieving sensitivity, accuracy and kappa index values of 97.34%, 91.83% and 0.75 respectively, the kNN algorithm has provided the best results. This allows the creation of a system capable of helping in complex situations where forecast errors are more common.</p></p class="citation"></blockquote><h2 id=cslg-57>cs.LG (57)</h2><h3 id=157--59261-reinforcement-learning-from-human-feedback-with-active-queries-kaixuan-ji-et-al-2024>(1/57 | 59/261) Reinforcement Learning from Human Feedback with Active Queries (Kaixuan Ji et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kaixuan Ji, Jiafan He, Quanquan Gu. (2024)<br><strong>Reinforcement Learning from Human Feedback with Active Queries</strong><br><button class=copy-to-clipboard title="Reinforcement Learning from Human Feedback with Active Queries" index=59>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-59 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CL, cs-LG, cs.LG, math-OC, stat-ML<br>Keyword Score: 80<br>Keywords: Active Learning, Bandit Algorithm, Fine-tuning, Reinforcement Learning, Reinforcement Learning from Human Feedback, Reinforcement Learning from Human Feedback, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09401v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09401v1.pdf filename=2402.09401v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Aligning <b>large</b> <b>language</b> <b>models</b> <b>(LLM)</b> with human preference plays a key role in building modern generative models and can be achieved by <b>reinforcement</b> <b>learning</b> <b>from</b> <b>human</b> <b>feedback</b> <b>(RLHF).</b> Despite their superior performance, current <b>RLHF</b> approaches often require a <b>large</b> <b>amount</b> <b>of</b> human-labelled preference data, which is expensive to collect. In this paper, inspired by the success of <b>active</b> <b>learning,</b> we address this problem by proposing query-efficient <b>RLHF</b> methods. We first formalize the alignment problem as a contextual dueling <b>bandit</b> problem and design an <b>active-query-based</b> <b>proximal</b> policy optimization (APPO) algorithm with an $\tilde{O}(d^2/\Delta)$ regret bound and an $\tilde{O}(d^2/\Delta^2)$ query complexity, where $d$ is the dimension of feature space and $\Delta$ is the sub-optimality gap over all the contexts. We then propose ADPO, a practical version of our algorithm based on direct preference optimization (DPO) and apply it to <b>fine-tuning</b> <b>LLMs.</b> Our experiments show that ADPO, while only making about half of queries for human preference, matches the performance of the state-of-the-art DPO method.</p></p class="citation"></blockquote><h3 id=257--60261-graph-inference-acceleration-by-learning-mlps-on-graphs-without-supervision-zehong-wang-et-al-2024>(2/57 | 60/261) Graph Inference Acceleration by Learning MLPs on Graphs without Supervision (Zehong Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zehong Wang, Zheyuan Zhang, Chuxu Zhang, Yanfang Ye. (2024)<br><strong>Graph Inference Acceleration by Learning MLPs on Graphs without Supervision</strong><br><button class=copy-to-clipboard title="Graph Inference Acceleration by Learning MLPs on Graphs without Supervision" index=60>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-60 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs-SI, cs.LG<br>Keyword Score: 73<br>Keywords: Message-Passing, Graph, Graph Neural Network, Graph Neural Network, Knowledge Distillation, Knowledge Distillation, Self-supervised Learning, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08918v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08918v1.pdf filename=2402.08918v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Graph</b> <b>Neural</b> <b>Networks</b> <b>(GNNs)</b> have demonstrated effectiveness in various <b>graph</b> <b>learning</b> <b>tasks,</b> yet their reliance on <b>message-passing</b> constraints their deployment in latency-sensitive applications such as financial fraud detection. Recent works have explored <b>distilling</b> knowledge from <b>GNNs</b> to Multi-Layer Perceptrons (MLPs) to accelerate inference. However, this task-specific <b>supervised</b> <b>distillation</b> limits generalization to unseen nodes, which are prevalent in latency-sensitive applications. To this end, we present \textbf{\textsc{SimMLP}}, a \textbf{\textsc{Sim}}ple yet effective framework for learning \textbf{\textsc{MLP}}s on <b>graphs</b> <b>without</b> <b>supervision,</b> to enhance generalization. \textsc{SimMLP} employs <b>self-supervised</b> alignment between <b>GNNs</b> and MLPs to capture the fine-grained and generalizable correlation between node features and <b>graph</b> <b>structures,</b> <b>and</b> proposes two strategies to alleviate the risk of trivial solutions. Theoretically, we comprehensively analyze \textsc{SimMLP} to demonstrate its equivalence to <b>GNNs</b> in the optimal case and its generalization capability. Empirically, \textsc{SimMLP} outperforms state-of-the-art baselines, especially in settings with unseen nodes. In particular, it obtains significant performance gains {\bf (7$\sim$26%)} over MLPs and inference acceleration over <b>GNNs</b> {\bf (90$\sim$126$\times$)} on large-scale <b>graph</b> <b>datasets.</b> <b>Our</b> codes are available at: \url{https://github.com/Zehong-Wang/SimMLP}.</p></p class="citation"></blockquote><h3 id=357--61261-low-rank-graph-contrastive-learning-for-node-classification-yancheng-wang-et-al-2024>(3/57 | 61/261) Low-Rank Graph Contrastive Learning for Node Classification (Yancheng Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yancheng Wang, Yingzhen Yang. (2024)<br><strong>Low-Rank Graph Contrastive Learning for Node Classification</strong><br><button class=copy-to-clipboard title="Low-Rank Graph Contrastive Learning for Node Classification" index=61>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-61 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs-SI, cs.LG, stat-ML<br>Keyword Score: 66<br>Keywords: Node Classification, Graph, Graph Contrastive Learning, Graph Contrastive Learning, Graph Neural Network, Graph Neural Network, Benchmarking, Contrastive Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09600v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09600v1.pdf filename=2402.09600v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Graph</b> <b>Neural</b> <b>Networks</b> <b>(GNNs)</b> have been widely used to learn <b>node</b> <b>representations</b> and with outstanding performance on various tasks such as <b>node</b> <b>classification.</b> However, noise, which inevitably exists in real-world <b>graph</b> <b>data,</b> <b>would</b> considerably degrade the performance of <b>GNNs</b> revealed by recent studies. In this work, we propose a novel and robust <b>GNN</b> encoder, Low-Rank <b>Graph</b> <b>Contrastive</b> <b>Learning</b> (LR-GCL). Our method performs transductive <b>node</b> <b>classification</b> in two steps. First, a low-rank <b>GCL</b> encoder named LR-GCL is trained by prototypical <b>contrastive</b> <b>learning</b> with low-rank regularization. Next, using the features produced by LR-GCL, a linear transductive classification algorithm is used to classify the unlabeled <b>nodes</b> <b>in</b> the <b>graph.</b> <b>Our</b> <b>LR-GCL</b> is inspired by the low frequency property of the <b>graph</b> <b>data</b> <b>and</b> its labels, and it is also theoretically motivated by our sharp generalization bound for transductive learning. To the best of our knowledge, our theoretical result is among the first to theoretically demonstrate the advantage of low-rank learning in <b>graph</b> <b>contrastive</b> <b>learning</b> supported by strong empirical performance. Extensive experiments on public <b>benchmarks</b> demonstrate the superior performance of LR-GCL and the robustness of the learned <b>node</b> <b>representations.</b> The code of LR-GCL is available at \url{https://anonymous.4open.science/r/Low-Rank_Graph_Contrastive_Learning-64A6/}.</p></p class="citation"></blockquote><h3 id=457--62261-learning-interpretable-policies-in-hindsight-observable-pomdps-through-partially-supervised-reinforcement-learning-michael-lanier-et-al-2024>(4/57 | 62/261) Learning Interpretable Policies in Hindsight-Observable POMDPs through Partially Supervised Reinforcement Learning (Michael Lanier et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Michael Lanier, Ying Xu, Nathan Jacobs, Chongjie Zhang, Yevgeniy Vorobeychik. (2024)<br><strong>Learning Interpretable Policies in Hindsight-Observable POMDPs through Partially Supervised Reinforcement Learning</strong><br><button class=copy-to-clipboard title="Learning Interpretable Policies in Hindsight-Observable POMDPs through Partially Supervised Reinforcement Learning" index=62>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-62 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 63<br>Keywords: Benchmarking, Knowledge Distillation, Reinforcement Learning, Supervised Learning, Unsupervised Learning, Unsupervised Learning, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09290v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09290v1.pdf filename=2402.09290v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep <b>reinforcement</b> <b>learning</b> has demonstrated remarkable achievements across diverse domains such as video games, robotic control, autonomous driving, and drug discovery. Common methodologies in partially-observable domains largely lean on end-to-end learning from high-dimensional observations, such as images, without explicitly <b>reasoning</b> about true state. We suggest an alternative direction, introducing the Partially <b>Supervised</b> <b>Reinforcement</b> <b>Learning</b> (PSRL) framework. At the heart of PSRL is the fusion of both <b>supervised</b> and <b>unsupervised</b> <b>learning.</b> The approach leverages a state estimator to <b>distill</b> <b>supervised</b> semantic state information from high-dimensional observations which are often fully observable at training time. This yields more interpretable policies that compose state predictions with control. In parallel, it captures an <b>unsupervised</b> <b>latent</b> representation. These two-the semantic state and the latent state-are then fused and utilized as inputs to a policy network. This juxtaposition offers practitioners a flexible and dynamic spectrum: from emphasizing <b>supervised</b> state information to integrating richer, latent insights. Extensive experimental results indicate that by merging these dual representations, PSRL offers a potent balance, enhancing model interpretability while preserving, and often significantly outperforming, the performance <b>benchmarks</b> set by traditional methods in terms of reward and convergence speed.</p></p class="citation"></blockquote><h3 id=557--63261-enhancing-sequential-model-performance-with-squared-sigmoid-tanh-sst-activation-under-data-constraints-barathi-subramanian-et-al-2024>(5/57 | 63/261) Enhancing Sequential Model Performance with Squared Sigmoid TanH (SST) Activation Under Data Constraints (Barathi Subramanian et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Barathi Subramanian, Rathinaraja Jeyaraj, Rakhmonov Akhrorjon Akhmadjon Ugli, Jeonghong Kim. (2024)<br><strong>Enhancing Sequential Model Performance with Squared Sigmoid TanH (SST) Activation Under Data Constraints</strong><br><button class=copy-to-clipboard title="Enhancing Sequential Model Performance with Squared Sigmoid TanH (SST) Activation Under Data Constraints" index=63>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-63 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 60<br>Keywords: Graph Attention Networks, LSTM, LSTM, LSTM, Recurrent Neural Network, Recurrent Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09034v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09034v1.pdf filename=2402.09034v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Activation functions enable neural networks to learn complex representations by introducing non-linearities. While feedforward models commonly use rectified linear units, sequential models like <b>recurrent</b> <b>neural</b> <b>networks,</b> <b>long</b> <b>short-term</b> <b>memory</b> <b>(LSTMs)</b> and <b>gated</b> <b>recurrent</b> <b>units</b> <b>(GRUs)</b> still rely on Sigmoid and TanH activation functions. However, these classical activation functions often struggle to model sparse patterns when trained on small sequential datasets to effectively capture temporal dependencies. To address this limitation, we propose squared Sigmoid TanH (SST) activation specifically tailored to enhance the learning capability of sequential models under data constraints. SST applies mathematical squaring to amplify differences between strong and weak activations as signals propagate over time, facilitating improved gradient flow and information filtering. We evaluate SST-powered <b>LSTMs</b> and GRUs for diverse applications, such as sign language recognition, regression, and time-series classification tasks, where the dataset is limited. Our experiments demonstrate that SST models consistently outperform <b>RNN-based</b> models with baseline activations, exhibiting improved test accuracy.</p></p class="citation"></blockquote><h3 id=657--64261-grassrep-graph-based-self-supervised-learning-for-repeat-detection-in-metagenomic-assembly-ali-azizpour-et-al-2024>(6/57 | 64/261) GraSSRep: Graph-Based Self-Supervised Learning for Repeat Detection in Metagenomic Assembly (Ali Azizpour et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ali Azizpour, Advait Balaji, Todd J. Treangen, Santiago Segarra. (2024)<br><strong>GraSSRep: Graph-Based Self-Supervised Learning for Repeat Detection in Metagenomic Assembly</strong><br><button class=copy-to-clipboard title="GraSSRep: Graph-Based Self-Supervised Learning for Repeat Detection in Metagenomic Assembly" index=64>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-64 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 53<br>Keywords: Node Classification, Graph, Graph Neural Network, Graph Neural Network, Self-supervised Learning, Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09381v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09381v1.pdf filename=2402.09381v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Repetitive DNA (repeats) poses significant challenges for accurate and efficient genome assembly and sequence alignment. This is particularly true for metagenomic data, where genome dynamics such as horizontal gene transfer, gene duplication, and gene loss/gain complicate accurate genome assembly from metagenomic communities. Detecting repeats is a crucial first step in overcoming these challenges. To address this issue, we propose GraSSRep, a novel approach that leverages the assembly <b>graph&rsquo;s</b> <b>structure</b> <b>through</b> <b>graph</b> <b>neural</b> <b>networks</b> <b>(GNNs)</b> within a <b>self-supervised</b> <b>learning</b> framework to classify DNA sequences into repetitive and non-repetitive categories. Specifically, we frame this problem as a <b>node</b> <b>classification</b> task within a metagenomic assembly <b>graph.</b> <b>In</b> <b>a</b> <b>self-supervised</b> <b>fashion,</b> we rely on a high-precision (but low-recall) heuristic to generate pseudo-labels for a small proportion of the <b>nodes.</b> <b>We</b> then use those pseudo-labels to train a <b>GNN</b> embedding and a random forest classifier to propagate the labels to the remaining <b>nodes.</b> <b>In</b> this way, GraSSRep combines sequencing features with pre-defined and learned <b>graph</b> <b>features</b> <b>to</b> achieve state-of-the-art performance in repeat detection. We evaluate our method using simulated and synthetic metagenomic datasets. The results on the simulated data highlight our GraSSRep&rsquo;s robustness to repeat attributes, demonstrating its effectiveness in handling the complexity of repeated sequences. Additionally, our experiments with synthetic metagenomic datasets reveal that incorporating the <b>graph</b> <b>structure</b> <b>and</b> the <b>GNN</b> enhances our detection performance. Finally, in comparative analyses, GraSSRep outperforms existing repeat detection tools with respect to precision and recall.</p></p class="citation"></blockquote><h3 id=757--65261-multi-hierarchical-surrogate-learning-for-structural-dynamical-crash-simulations-using-graph-convolutional-neural-networks-jonas-kneifl-et-al-2024>(7/57 | 65/261) Multi-Hierarchical Surrogate Learning for Structural Dynamical Crash Simulations Using Graph Convolutional Neural Networks (Jonas Kneifl et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jonas Kneifl, Jörg Fehr, Steven L. Brunton, J. Nathan Kutz. (2024)<br><strong>Multi-Hierarchical Surrogate Learning for Structural Dynamical Crash Simulations Using Graph Convolutional Neural Networks</strong><br><button class=copy-to-clipboard title="Multi-Hierarchical Surrogate Learning for Structural Dynamical Crash Simulations Using Graph Convolutional Neural Networks" index=65>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-65 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, math-DS<br>Keyword Score: 53<br>Keywords: Graph, Convolution, Convolutional Neural Network, Simulation, Simulator, Transfer Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09234v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09234v2.pdf filename=2402.09234v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Crash <b>simulations</b> play an essential role in improving vehicle safety, design optimization, and injury risk estimation. Unfortunately, numerical solutions of such problems using state-of-the-art high-fidelity models require significant computational effort. Conventional data-driven surrogate modeling approaches create low-dimensional embeddings for evolving the dynamics in order to circumvent this computational effort. Most approaches directly operate on high-resolution data obtained from numerical discretization, which is both costly and complicated for mapping the flow of information over large spatial distances. Furthermore, working with a fixed resolution prevents the adaptation of surrogate models to environments with variable computing capacities, different visualization resolutions, and different accuracy requirements. We thus propose a multi-hierarchical framework for structurally creating a series of surrogate models for a kart frame, which is a good proxy for industrial-relevant crash <b>simulations,</b> at different levels of resolution. For multiscale phenomena, macroscale features are captured on a coarse surrogate, whereas microscale effects are resolved by finer ones. The learned behavior of the individual surrogates is passed from coarse to finer levels through <b>transfer</b> <b>learning.</b> In detail, we perform a mesh simplification on the kart model to obtain multi-resolution representations of it. We then train a <b>graph-convolutional</b> neural network-based surrogate that learns parameter-dependent low-dimensional latent dynamics on the coarsest representation. Subsequently, another, similarly structured surrogate is trained on the residual of the first surrogate using a finer resolution. This step can be repeated multiple times. By doing so, we construct multiple surrogates for the same system with varying hardware requirements and increasing accuracy.</p></p class="citation"></blockquote><h3 id=857--66261-parametric-learning-of-time-advancement-operators-for-unstable-flame-evolution-rixin-yu-et-al-2024>(8/57 | 66/261) Parametric Learning of Time-Advancement Operators for Unstable Flame Evolution (Rixin Yu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rixin Yu, Erdzan Hodzic. (2024)<br><strong>Parametric Learning of Time-Advancement Operators for Unstable Flame Evolution</strong><br><button class=copy-to-clipboard title="Parametric Learning of Time-Advancement Operators for Unstable Flame Evolution" index=66>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-66 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: 65Z05, G-1-8, cs-LG, cs.LG<br>Keyword Score: 50<br>Keywords: Convolution, Convolutional Neural Network, Convolutional Neural Network, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10238v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10238v1.pdf filename=2402.10238v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This study investigates the application of machine learning, specifically Fourier Neural Operator (FNO) and <b>Convolutional</b> <b>Neural</b> <b>Network</b> <b>(CNN),</b> to learn time-advancement operators for parametric partial differential equations (PDEs). Our focus is on extending existing operator learning methods to handle additional inputs representing PDE parameters. The goal is to create a unified learning approach that accurately predicts short-term solutions and provides robust long-term statistics under diverse parameter conditions, facilitating computational cost savings and accelerating development in engineering <b>simulations.</b> We develop and compare parametric learning methods based on FNO and <b>CNN,</b> evaluating their effectiveness in learning parametric-dependent solution time-advancement operators for one-dimensional PDEs and realistic flame front evolution data obtained from direct numerical <b>simulations</b> of the Navier-Stokes equations.</p></p class="citation"></blockquote><h3 id=957--67261-fedsikd-clients-similarity-and-knowledge-distillation-addressing-non-iid-and-constraints-in-federated-learning-yousef-alsenani-et-al-2024>(9/57 | 67/261) FedSiKD: Clients Similarity and Knowledge Distillation: Addressing Non-i.i.d. and Constraints in Federated Learning (Yousef Alsenani et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yousef Alsenani, Rahul Mishra, Khaled R. Ahmed, Atta Ur Rahman. (2024)<br><strong>FedSiKD: Clients Similarity and Knowledge Distillation: Addressing Non-i.i.d. and Constraints in Federated Learning</strong><br><button class=copy-to-clipboard title="FedSiKD: Clients Similarity and Knowledge Distillation: Addressing Non-i.i.d. and Constraints in Federated Learning" index=67>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-67 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CR, cs-LG, cs.LG<br>Keyword Score: 50<br>Keywords: MNIST, Federated Learning, Knowledge Distillation, Knowledge Distillation, Knowledge Distillation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09095v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09095v1.pdf filename=2402.09095v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In recent years, <b>federated</b> <b>learning</b> (FL) has emerged as a promising technique for training machine learning models in a decentralized manner while also preserving data privacy. The non-independent and identically distributed (non-i.i.d.) nature of client data, coupled with constraints on client or edge devices, presents significant challenges in FL. Furthermore, learning across a high number of communication rounds can be risky and potentially unsafe for model exploitation. Traditional FL approaches may suffer from these challenges. Therefore, we introduce FedSiKD, which incorporates <b>knowledge</b> <b>distillation</b> <b>(KD)</b> within a similarity-based <b>federated</b> <b>learning</b> framework. As clients join the system, they securely share relevant statistics about their data distribution, promoting intra-cluster homogeneity. This enhances optimization efficiency and accelerates the learning process, effectively transferring <b>knowledge</b> <b>between</b> teacher and student models and addressing device constraints. FedSiKD outperforms state-of-the-art algorithms by achieving higher accuracy, exceeding by 25% and 18% for highly skewed data at $\alpha = {0.1,0.5}$ on the HAR and <b>MNIST</b> datasets, respectively. Its faster convergence is illustrated by a 17% and 20% increase in accuracy within the first five rounds on the HAR and <b>MNIST</b> datasets, respectively, highlighting its early-stage learning proficiency. Code is publicly available and hosted on GitHub (<a href=https://github.com/SimuEnv/FedSiKD>https://github.com/SimuEnv/FedSiKD</a>)</p></p class="citation"></blockquote><h3 id=1057--68261-robust-training-of-temporal-gnns-using-nearest-neighbours-based-hard-negatives-shubham-gupta-et-al-2024>(10/57 | 68/261) Robust Training of Temporal GNNs using Nearest Neighbours based Hard Negatives (Shubham Gupta et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shubham Gupta, Srikanta Bedathur. (2024)<br><strong>Robust Training of Temporal GNNs using Nearest Neighbours based Hard Negatives</strong><br><button class=copy-to-clipboard title="Robust Training of Temporal GNNs using Nearest Neighbours based Hard Negatives" index=68>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-68 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-IR, cs-LG, cs.LG<br>Keyword Score: 43<br>Keywords: Graph, Graph Neural Network, Graph Neural Network, Unsupervised Learning, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09239v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09239v1.pdf filename=2402.09239v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Temporal <b>graph</b> <b>neural</b> <b>networks</b> Tgnn have exhibited state-of-art performance in future-link prediction tasks. Training of these TGNNs is enumerated by uniform random sampling based <b>unsupervised</b> <b>loss.</b> During training, in the context of a positive example, the loss is computed over uninformative negatives, which introduces redundancy and sub-optimal performance. In this paper, we propose modified <b>unsupervised</b> <b>learning</b> of Tgnn, by replacing the uniform negative sampling with importance-based negative sampling. We theoretically motivate and define the dynamically computed distribution for a sampling of negative examples. Finally, using empirical evaluations over three real-world datasets, we show that Tgnn trained using loss based on proposed negative sampling provides consistent superior performance.</p></p class="citation"></blockquote><h3 id=1157--69261-switch-ema-a-free-lunch-for-better-flatness-and-sharpness-siyuan-li-et-al-2024>(11/57 | 69/261) Switch EMA: A Free Lunch for Better Flatness and Sharpness (Siyuan Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Siyuan Li, Zicheng Liu, Juanxi Tian, Ge Wang, Zedong Wang, Weiyang Jin, Di Wu, Cheng Tan, Tao Lin, Yang Liu, Baigui Sun, Stan Z. Li. (2024)<br><strong>Switch EMA: A Free Lunch for Better Flatness and Sharpness</strong><br><button class=copy-to-clipboard title="Switch EMA: A Free Lunch for Better Flatness and Sharpness" index=69>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-69 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CV, cs-LG, cs.LG<br>Keyword Score: 40<br>Keywords: Object Detection, Self-supervised Learning, Self-supervised Learning, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09240v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09240v1.pdf filename=2402.09240v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Exponential Moving Average (EMA) is a widely used weight averaging (WA) regularization to learn flat optima for better generalizations without extra cost in deep neural network (DNN) optimization. Despite achieving better flatness, existing WA methods might fall into worse final performances or require extra test-time computations. This work unveils the full potential of EMA with a single line of modification, i.e., switching the EMA parameters to the original model after each epoch, dubbed as Switch EMA (SEMA). From both theoretical and empirical aspects, we demonstrate that SEMA can help DNNs to reach generalization optima that better trade-off between flatness and sharpness. To verify the effectiveness of SEMA, we conduct comparison experiments with discriminative, generative, and regression tasks on vision and language datasets, including image classification, <b>self-supervised</b> <b>learning,</b> <b>object</b> <b>detection</b> and segmentation, image generation, video prediction, attribute regression, and language modeling. Comprehensive results with popular optimizers and networks show that SEMA is a free lunch for DNN training by improving performances and boosting convergence speeds.</p></p class="citation"></blockquote><h3 id=1257--70261-attacking-large-language-models-with-projected-gradient-descent-simon-geisler-et-al-2024>(12/57 | 70/261) Attacking Large Language Models with Projected Gradient Descent (Simon Geisler et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Simon Geisler, Tom Wollschläger, M. H. I. Abdalla, Johannes Gasteiger, Stephan Günnemann. (2024)<br><strong>Attacking Large Language Models with Projected Gradient Descent</strong><br><button class=copy-to-clipboard title="Attacking Large Language Models with Projected Gradient Descent" index=70>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-70 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 40<br>Keywords: Adversarial Learning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09154v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09154v1.pdf filename=2402.09154v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Current <b>LLM</b> alignment methods are readily broken through specifically crafted <b>adversarial</b> <b>prompts.</b> While crafting <b>adversarial</b> <b>prompts</b> using discrete optimization is highly effective, such attacks typically use more than 100,000 <b>LLM</b> calls. This high computational cost makes them unsuitable for, e.g., quantitative analyses and <b>adversarial</b> <b>training.</b> To remedy this, we revisit Projected Gradient Descent (PGD) on the continuously relaxed input <b>prompt.</b> Although previous attempts with ordinary gradient-based attacks largely failed, we show that carefully controlling the error introduced by the continuous relaxation tremendously boosts their efficacy. Our PGD for <b>LLMs</b> is up to one order of magnitude faster than state-of-the-art discrete optimization to achieve the same devastating attack results.</p></p class="citation"></blockquote><h3 id=1357--71261-measuring-exploration-in-reinforcement-learning-via-optimal-transport-in-policy-space-reabetswe-m-nkhumise-et-al-2024>(13/57 | 71/261) Measuring Exploration in Reinforcement Learning via Optimal Transport in Policy Space (Reabetswe M. Nkhumise et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Reabetswe M. Nkhumise, Debabrota Basu, Tony J. Prescott, Aditya Gilra. (2024)<br><strong>Measuring Exploration in Reinforcement Learning via Optimal Transport in Policy Space</strong><br><button class=copy-to-clipboard title="Measuring Exploration in Reinforcement Learning via Optimal Transport in Policy Space" index=71>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-71 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 40<br>Keywords: Knowledge Transfer, Reinforcement Learning, Supervised Learning, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09113v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09113v1.pdf filename=2402.09113v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Exploration is the key ingredient of <b>reinforcement</b> <b>learning</b> (RL) that determines the speed and success of learning. Here, we quantify and compare the amount of exploration and learning accomplished by a <b>Reinforcement</b> <b>Learning</b> (RL) algorithm. Specifically, we propose a novel measure, named Exploration Index, that quantifies the relative effort of <b>knowledge</b> <b>transfer</b> (transferability) by an RL algorithm in comparison to <b>supervised</b> <b>learning</b> (SL) that transforms the initial data distribution of RL to the corresponding final data distribution. The comparison is established by formulating learning in RL as a sequence of SL tasks, and using optimal transport based metrics to compare the total path traversed by the RL and SL algorithms in the data distribution space. We perform extensive empirical analysis on various environments and with multiple algorithms to demonstrate that the exploration index yields insights about the exploration behaviour of any RL algorithm, and also allows us to compare the exploratory behaviours of different RL algorithms.</p></p class="citation"></blockquote><h3 id=1457--72261-werank-towards-rank-degradation-prevention-for-self-supervised-learning-using-weight-regularization-ali-saheb-pasand-et-al-2024>(14/57 | 72/261) WERank: Towards Rank Degradation Prevention for Self-Supervised Learning Using Weight Regularization (Ali Saheb Pasand et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ali Saheb Pasand, Reza Moravej, Mahdi Biparva, Ali Ghodsi. (2024)<br><strong>WERank: Towards Rank Degradation Prevention for Self-Supervised Learning Using Weight Regularization</strong><br><button class=copy-to-clipboard title="WERank: Towards Rank Degradation Prevention for Self-Supervised Learning Using Weight Regularization" index=72>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-72 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 33<br>Keywords: Graph, Data Augmentation, Self-supervised Learning, Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09586v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09586v1.pdf filename=2402.09586v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A common phenomena confining the representation quality in <b>Self-Supervised</b> <b>Learning</b> (SSL) is dimensional collapse (also known as rank degeneration), where the learned representations are mapped to a low dimensional subspace of the representation space. The State-of-the-Art SSL methods have shown to suffer from dimensional collapse and fall behind maintaining full rank. Recent approaches to prevent this problem have proposed using contrastive losses, regularization techniques, or architectural tricks. We propose WERank, a new regularizer on the weight parameters of the network to prevent rank degeneration at different layers of the network. We provide empirical evidence and mathematical justification to demonstrate the effectiveness of the proposed regularization method in preventing dimensional collapse. We verify the impact of WERank on <b>graph</b> SSL where dimensional collapse is more pronounced due to the lack of proper <b>data</b> <b>augmentation.</b> We empirically demonstrate that WERank is effective in helping BYOL to achieve higher rank during SSL pre-training and consequently downstream accuracy during evaluation probing. Ablation studies and experimental analysis shed lights on the underlying factors behind the performance gains of the proposed approach.</p></p class="citation"></blockquote><h3 id=1557--73261-get-more-with-less-synthesizing-recurrence-with-kv-cache-compression-for-efficient-llm-inference-harry-dong-et-al-2024>(15/57 | 73/261) Get More with LESS: Synthesizing Recurrence with KV Cache Compression for Efficient LLM Inference (Harry Dong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Harry Dong, Xinyu Yang, Zhenyu Zhang, Zhangyang Wang, Yuejie Chi, Beidi Chen. (2024)<br><strong>Get More with LESS: Synthesizing Recurrence with KV Cache Compression for Efficient LLM Inference</strong><br><button class=copy-to-clipboard title="Get More with LESS: Synthesizing Recurrence with KV Cache Compression for Efficient LLM Inference" index=73>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-73 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Pruning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09398v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09398v1.pdf filename=2402.09398v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Many computational factors limit broader deployment of <b>large</b> <b>language</b> <b>models.</b> In this paper, we focus on a memory bottleneck imposed by the key-value (KV) cache, a computational shortcut that requires storing previous KV pairs during decoding. While existing KV cache methods approach this problem by <b>pruning</b> or evicting <b>large</b> <b>swaths</b> <b>of</b> relatively less important KV pairs to dramatically reduce the memory footprint of the cache, they can have limited success in tasks that require recollecting a majority of previous tokens. To alleviate this issue, we propose LESS, a simple integration of a (nearly free) constant sized cache with eviction-based cache methods, such that all tokens can be queried at later decoding steps. Its ability to retain information throughout time shows merit on a variety of tasks where we demonstrate LESS can help reduce the performance gap from caching everything, sometimes even matching it, all while being efficient.</p></p class="citation"></blockquote><h3 id=1657--74261-mitigating-reward-hacking-via-information-theoretic-reward-modeling-yuchun-miao-et-al-2024>(16/57 | 74/261) Mitigating Reward Hacking via Information-Theoretic Reward Modeling (Yuchun Miao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuchun Miao, Sen Zhang, Liang Ding, Rong Bao, Lefei Zhang, Dacheng Tao. (2024)<br><strong>Mitigating Reward Hacking via Information-Theoretic Reward Modeling</strong><br><button class=copy-to-clipboard title="Mitigating Reward Hacking via Information-Theoretic Reward Modeling" index=74>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-74 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Reinforcement Learning, Reinforcement Learning from Human Feedback, Reinforcement Learning from Human Feedback<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09345v3 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09345v3.pdf filename=2402.09345v3.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Despite the success of <b>reinforcement</b> <b>learning</b> <b>from</b> <b>human</b> <b>feedback</b> <b>(RLHF)</b> in aligning language models with human values, reward hacking, also termed reward overoptimization, remains a critical challenge, which primarily stems from limitations in reward modeling, i.e., generalizability of the reward model and inconsistency in the preference dataset. In this work, we tackle this problem from an information theoretic-perspective, and propose a generalizable and robust framework for reward modeling, namely InfoRM, by introducing a variational information bottleneck objective to filter out irrelevant information and developing a mechanism for model complexity modulation. Notably, we further identify a correlation between overoptimization and outliers in the latent space, establishing InfoRM as a promising tool for detecting reward overoptimization. Inspired by this finding, we propose the Integrated Cluster Deviation Score (ICDS), which quantifies deviations in the latent space, as an indicator of reward overoptimization to facilitate the development of online mitigation strategies. Extensive experiments on a wide range of settings and model scales (70M, 440M, 1.4B, and 7B) support the effectiveness of InfoRM. Further analyses reveal that InfoRM&rsquo;s overoptimization detection mechanism is effective, potentially signifying a notable advancement in the field of <b>RLHF.</b> Code will be released upon acceptance.</p></p class="citation"></blockquote><h3 id=1757--75261-transformers-parallel-computation-and-logarithmic-depth-clayton-sanford-et-al-2024>(17/57 | 75/261) Transformers, parallel computation, and logarithmic depth (Clayton Sanford et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Clayton Sanford, Daniel Hsu, Matus Telgarsky. (2024)<br><strong>Transformers, parallel computation, and logarithmic depth</strong><br><button class=copy-to-clipboard title="Transformers, parallel computation, and logarithmic depth" index=75>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-75 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Sub-Quadratic Transformer, Transformer, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09268v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09268v1.pdf filename=2402.09268v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We show that a constant number of <b>self-attention</b> layers can efficiently simulate, and be simulated by, a constant number of communication rounds of Massively Parallel Computation. As a consequence, we show that logarithmic depth is sufficient for <b>transformers</b> to solve basic computational tasks that cannot be efficiently solved by several other neural sequence models and <b>sub-quadratic</b> <b>transformer</b> approximations. We thus establish parallelism as a key distinguishing property of <b>transformers.</b></p></p class="citation"></blockquote><h3 id=1857--76261-soft-prompt-threats-attacking-safety-alignment-and-unlearning-in-open-source-llms-through-the-embedding-space-leo-schwinn-et-al-2024>(18/57 | 76/261) Soft Prompt Threats: Attacking Safety Alignment and Unlearning in Open-Source LLMs through the Embedding Space (Leo Schwinn et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Leo Schwinn, David Dobre, Sophie Xhonneux, Gauthier Gidel, Stephan Gunnemann. (2024)<br><strong>Soft Prompt Threats: Attacking Safety Alignment and Unlearning in Open-Source LLMs through the Embedding Space</strong><br><button class=copy-to-clipboard title="Soft Prompt Threats: Attacking Safety Alignment and Unlearning in Open-Source LLMs through the Embedding Space" index=76>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-76 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Fine-tuning, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09063v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09063v1.pdf filename=2402.09063v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Current research in adversarial robustness of <b>LLMs</b> focuses on discrete input manipulations in the natural language space, which can be directly transferred to closed-source models. However, this approach neglects the steady progression of open-source models. As open-source models advance in capability, ensuring their safety also becomes increasingly imperative. Yet, attacks tailored to open-source <b>LLMs</b> that exploit full model access remain largely unexplored. We address this research gap and propose the embedding space attack, which directly attacks the continuous embedding representation of input tokens. We find that embedding space attacks circumvent model alignments and trigger harmful behaviors more efficiently than discrete attacks or model <b>fine-tuning.</b> Furthermore, we present a novel threat model in the context of unlearning and show that embedding space attacks can extract supposedly deleted information from unlearned <b>LLMs</b> across multiple datasets and models. Our findings highlight embedding space attacks as an important threat model in open-source <b>LLMs.</b> Trigger Warning: the appendix contains <b>LLM-generated</b> text with violence and harassment.</p></p class="citation"></blockquote><h3 id=1957--77261-i-cant-see-it-but-i-can-fine-tune-it-on-encrypted-fine-tuning-of-transformers-using-fully-homomorphic-encryption-prajwal-panzade-et-al-2024>(19/57 | 77/261) I can&rsquo;t see it but I can Fine-tune it: On Encrypted Fine-tuning of Transformers using Fully Homomorphic Encryption (Prajwal Panzade et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Prajwal Panzade, Daniel Takabi, Zhipeng Cai. (2024)<br><strong>I can&rsquo;t see it but I can Fine-tune it: On Encrypted Fine-tuning of Transformers using Fully Homomorphic Encryption</strong><br><button class=copy-to-clipboard title="I can't see it but I can Fine-tune it: On Encrypted Fine-tuning of Transformers using Fully Homomorphic Encryption" index=77>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-77 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CR, cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Fine-tuning, Fine-tuning, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09059v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09059v1.pdf filename=2402.09059v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In today&rsquo;s machine learning landscape, <b>fine-tuning</b> pretrained <b>transformer</b> models has emerged as an essential technique, particularly in scenarios where access to task-aligned training data is limited. However, challenges surface when data sharing encounters obstacles due to stringent privacy regulations or user apprehension regarding personal information disclosure. Earlier works based on secure multiparty computation (SMC) and fully homomorphic encryption (FHE) for privacy-preserving machine learning (PPML) focused more on privacy-preserving inference than privacy-preserving training. In response, we introduce BlindTuner, a privacy-preserving <b>fine-tuning</b> system that enables <b>transformer</b> training exclusively on homomorphically encrypted data for image classification. Our extensive experimentation validates BlindTuner&rsquo;s effectiveness by demonstrating comparable accuracy to non-encrypted models. Notably, our findings highlight a substantial speed enhancement of 1.5x to 600x over previous work in this domain.</p></p class="citation"></blockquote><h3 id=2057--78261-towards-next-level-post-training-quantization-of-hyper-scale-transformers-junhan-kim-et-al-2024>(20/57 | 78/261) Towards Next-Level Post-Training Quantization of Hyper-Scale Transformers (Junhan Kim et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Junhan Kim, Kyungphil Park, Chungman Lee, Ho-young Kim, Joonyoung Kim, Yongkweon Jeon. (2024)<br><strong>Towards Next-Level Post-Training Quantization of Hyper-Scale Transformers</strong><br><button class=copy-to-clipboard title="Towards Next-Level Post-Training Quantization of Hyper-Scale Transformers" index=78>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-78 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Generative AI, Quantization, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08958v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08958v1.pdf filename=2402.08958v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the increasing complexity of <b>generative</b> <b>AI</b> models, post-training <b>quantization</b> (PTQ) has emerged as a promising solution for deploying hyper-scale models on edge devices such as mobile devices and TVs. Existing PTQ schemes, however, consume considerable time and resources, which could be a bottleneck in real situations where frequent model updates and multiple hyper-parameter tunings are required. As a cost-effective alternative, one-shot PTQ schemes have been proposed. Still, the performance is somewhat limited because they cannot consider the inter-layer dependency within the attention module, which is a very important feature of <b>Transformers.</b> In this paper, we thus propose a novel PTQ algorithm that balances accuracy and efficiency. The key idea of the proposed algorithm called aespa is to perform <b>quantization</b> layer-wise for efficiency while considering cross-layer dependency to preserve the attention score. Through extensive experiments on various language models and complexity analysis, we demonstrate that aespa is accurate and efficient in quantizing <b>Transformer</b> models.</p></p class="citation"></blockquote><h3 id=2157--79261-second-order-methods-for-bandit-optimization-and-control-arun-suggala-et-al-2024>(21/57 | 79/261) Second Order Methods for Bandit Optimization and Control (Arun Suggala et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Arun Suggala, Y. Jennifer Sun, Praneeth Netrapalli, Elad Hazan. (2024)<br><strong>Second Order Methods for Bandit Optimization and Control</strong><br><button class=copy-to-clipboard title="Second Order Methods for Bandit Optimization and Control" index=79>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-79 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-ML<br>Keyword Score: 30<br>Keywords: Bandit Algorithm, Bandit Algorithm, Logistic Regression<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08929v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08929v1.pdf filename=2402.08929v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Bandit</b> <b>convex</b> optimization (BCO) is a general framework for online decision making under uncertainty. While tight regret bounds for general convex losses have been established, existing algorithms achieving these bounds have prohibitive computational costs for high dimensional data. In this paper, we propose a simple and practical BCO algorithm inspired by the online Newton step algorithm. We show that our algorithm achieves optimal (in terms of horizon) regret bounds for a large class of convex functions that we call $\kappa$-convex. This class contains a wide range of practically relevant loss functions including linear, quadratic, and generalized linear models. In addition to optimal regret, this method is the most efficient known algorithm for several well-studied applications including <b>bandit</b> <b>logistic</b> <b>regression.</b> Furthermore, we investigate the adaptation of our second-order <b>bandit</b> <b>algorithm</b> to online convex optimization with memory. We show that for loss functions with a certain affine structure, the extended algorithm attains optimal regret. This leads to an algorithm with optimal regret for <b>bandit</b> <b>LQR/LQG</b> problems under a fully adversarial noise model, thereby resolving an open question posed in \citep{gradu2020non} and \citep{sun2023optimal}. Finally, we show that the more general problem of BCO with (non-affine) memory is harder. We derive a $\tilde{\Omega}(T^{2/3})$ regret lower bound, even under the assumption of smooth and quadratic losses.</p></p class="citation"></blockquote><h3 id=2257--80261-embracing-the-black-box-heading-towards-foundation-models-for-causal-discovery-from-time-series-data-gideon-stein-et-al-2024>(22/57 | 80/261) Embracing the black box: Heading towards foundation models for causal discovery from time series data (Gideon Stein et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Gideon Stein, Maha Shadaydeh, Joachim Denzler. (2024)<br><strong>Embracing the black box: Heading towards foundation models for causal discovery from time series data</strong><br><button class=copy-to-clipboard title="Embracing the black box: Heading towards foundation models for causal discovery from time series data" index=80>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-80 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: 68T07, cs-AI, cs-LG, cs.LG<br>Keyword Score: 28<br>Keywords: Graph, Black Box, Foundation Model, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09305v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09305v1.pdf filename=2402.09305v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Causal discovery from time series data encompasses many existing solutions, including those based on deep learning techniques. However, these methods typically do not endorse one of the most prevalent paradigms in deep learning: End-to-end learning. To address this gap, we explore what we call Causal Pretraining. A methodology that aims to learn a direct mapping from multivariate time series to the underlying causal <b>graphs</b> in a <b>supervised</b> manner. Our empirical findings suggest that causal discovery in a <b>supervised</b> manner is possible, assuming that the training and test time series samples share most of their dynamics. More importantly, we found evidence that the performance of Causal Pretraining can increase with data and model size, even if the additional data do not share the same dynamics. Further, we provide examples where causal discovery for real-world data with causally pretrained neural networks is possible within limits. We argue that this hints at the possibility of a <b>foundation</b> <b>model</b> for causal discovery.</p></p class="citation"></blockquote><h3 id=2357--81261-evolving-restricted-boltzmann-machine-kohonen-network-for-online-clustering-j-senthilnath-et-al-2024>(23/57 | 81/261) Evolving Restricted Boltzmann Machine-Kohonen Network for Online Clustering (J. Senthilnath et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>J. Senthilnath, Adithya Bhattiprolu, Ankur Singh, Bangjian Zhou, Min Wu, Jón Atli Benediktsson, Xiaoli Li. (2024)<br><strong>Evolving Restricted Boltzmann Machine-Kohonen Network for Online Clustering</strong><br><button class=copy-to-clipboard title="Evolving Restricted Boltzmann Machine-Kohonen Network for Online Clustering" index=81>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-81 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 26<br>Keywords: Benchmarking, Clustering, Online Clustering, Pruning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09167v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09167v1.pdf filename=2402.09167v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A novel <b>online</b> <b>clustering</b> algorithm is presented where an Evolving Restricted Boltzmann Machine (ERBM) is embedded with a Kohonen Network called ERBM-KNet. The proposed ERBM-KNet efficiently handles streaming data in a single-pass mode using the ERBM, employing a bias-variance strategy for neuron growing and <b>pruning,</b> as well as <b>online</b> <b>clustering</b> based on a cluster update strategy for cluster prediction and cluster center update using KNet. Initially, ERBM evolves its architecture while processing unlabeled image data, effectively disentangling the data distribution in the latent space. Subsequently, the KNet utilizes the feature extracted from ERBM to predict the number of clusters and updates the cluster centers. By overcoming the common challenges associated with <b>clustering</b> algorithms, such as prior initialization of the number of clusters and subpar <b>clustering</b> accuracy, the proposed ERBM-KNet offers significant improvements. Extensive experimental evaluations on four <b>benchmarks</b> and one industry dataset demonstrate the superiority of ERBM-KNet compared to state-of-the-art approaches.</p></p class="citation"></blockquote><h3 id=2457--82261-learning-interpretable-concepts-unifying-causal-representation-learning-and-foundation-models-goutham-rajendran-et-al-2024>(24/57 | 82/261) Learning Interpretable Concepts: Unifying Causal Representation Learning and Foundation Models (Goutham Rajendran et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Goutham Rajendran, Simon Buchholz, Bryon Aragam, Bernhard Schölkopf, Pradeep Ravikumar. (2024)<br><strong>Learning Interpretable Concepts: Unifying Causal Representation Learning and Foundation Models</strong><br><button class=copy-to-clipboard title="Learning Interpretable Concepts: Unifying Causal Representation Learning and Foundation Models" index=82>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-82 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG, math-ST, stat-ML, stat-TH<br>Keyword Score: 25<br>Keywords: Foundation Model, Representation Learning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09236v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09236v1.pdf filename=2402.09236v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>To build intelligent machine learning systems, there are two broad approaches. One approach is to build inherently interpretable models, as endeavored by the growing field of causal <b>representation</b> <b>learning.</b> The other approach is to build highly-performant <b>foundation</b> <b>models</b> and then invest efforts into understanding how they work. In this work, we relate these two approaches and study how to learn human-interpretable concepts from data. Weaving together ideas from both fields, we formally define a notion of concepts and show that they can be provably recovered from diverse data. Experiments on synthetic data and <b>large</b> <b>language</b> <b>models</b> show the utility of our unified approach.</p></p class="citation"></blockquote><h3 id=2557--83261-directional-convergence-near-small-initializations-and-saddles-in-two-homogeneous-neural-networks-akshay-kumar-et-al-2024>(25/57 | 83/261) Directional Convergence Near Small Initializations and Saddles in Two-Homogeneous Neural Networks (Akshay Kumar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Akshay Kumar, Jarvis Haupt. (2024)<br><strong>Directional Convergence Near Small Initializations and Saddles in Two-Homogeneous Neural Networks</strong><br><button class=copy-to-clipboard title="Directional Convergence Near Small Initializations and Saddles in Two-Homogeneous Neural Networks" index=83>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-83 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, math-OC, stat-ML<br>Keyword Score: 25<br>Keywords: Karush-Kuhn-Tucker, Karush-Kuhn-Tucker, Square Loss<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09226v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09226v1.pdf filename=2402.09226v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper examines gradient flow dynamics of two-homogeneous neural networks for small initializations, where all weights are initialized near the origin. For both <b>square</b> <b>and</b> logistic losses, it is shown that for sufficiently small initializations, the gradient flow dynamics spend sufficient time in the neighborhood of the origin to allow the weights of the neural network to approximately converge in direction to the <b>Karush-Kuhn-Tucker</b> <b>(KKT)</b> points of a neural correlation function that quantifies the correlation between the output of the neural network and corresponding labels in the training data set. For <b>square</b> <b>loss,</b> it has been observed that neural networks undergo saddle-to-saddle dynamics when initialized close to the origin. Motivated by this, this paper also shows a similar directional convergence among weights of small magnitude in the neighborhood of certain saddle points.</p></p class="citation"></blockquote><h3 id=2657--84261-leveraging-the-context-through-multi-round-interactions-for-jailbreaking-attacks-yixin-cheng-et-al-2024>(26/57 | 84/261) Leveraging the Context through Multi-Round Interactions for Jailbreaking Attacks (Yixin Cheng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yixin Cheng, Markos Georgopoulos, Volkan Cevher, Grigorios G. Chrysos. (2024)<br><strong>Leveraging the Context through Multi-Round Interactions for Jailbreaking Attacks</strong><br><button class=copy-to-clipboard title="Leveraging the Context through Multi-Round Interactions for Jailbreaking Attacks" index=84>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-84 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CL, cs-LG, cs.LG<br>Keyword Score: 25<br>Keywords: Black Box, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09177v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09177v1.pdf filename=2402.09177v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> are susceptible to Jailbreaking attacks, which aim to extract harmful information by subtly modifying the attack query. As defense mechanisms evolve, directly obtaining harmful information becomes increasingly challenging for Jailbreaking attacks. In this work, inspired by human practices of indirect context to elicit harmful information, we focus on a new attack form called Contextual Interaction Attack. The idea relies on the autoregressive nature of the generation process in <b>LLMs.</b> We contend that the prior context&ndash;the information preceding the attack query&ndash;plays a pivotal role in enabling potent Jailbreaking attacks. Specifically, we propose an approach that leverages preliminary question-answer pairs to interact with the <b>LLM.</b> By doing so, we guide the responses of the model toward revealing the &lsquo;desired&rsquo; harmful information. We conduct experiments on four different <b>LLMs</b> and demonstrate the efficacy of this attack, which is <b>black-box</b> <b>and</b> can also transfer across <b>LLMs.</b> We believe this can lead to further developments and understanding of the context vector in <b>LLMs.</b></p></p class="citation"></blockquote><h3 id=2757--85261-scalable-graph-self-supervised-learning-ali-saheb-pasand-et-al-2024>(27/57 | 85/261) Scalable Graph Self-Supervised Learning (Ali Saheb Pasand et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ali Saheb Pasand, Reza Moravej, Mahdi Biparva, Raika Karimi, Ali Ghodsi. (2024)<br><strong>Scalable Graph Self-Supervised Learning</strong><br><button class=copy-to-clipboard title="Scalable Graph Self-Supervised Learning" index=85>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-85 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 23<br>Keywords: Graph, Self-supervised Learning, Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09603v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09603v1.pdf filename=2402.09603v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In regularization <b>Self-Supervised</b> <b>Learning</b> (SSL) methods for <b>graphs,</b> computational complexity increases with the number of nodes in <b>graphs</b> and embedding dimensions. To mitigate the scalability of non-contrastive <b>graph</b> SSL, we propose a novel approach to reduce the cost of computing the covariance matrix for the pre-training loss function with volume-maximization terms. Our work focuses on reducing the cost associated with the loss computation via <b>graph</b> node or dimension sampling. We provide theoretical insight into why dimension sampling would result in accurate loss computations and support it with mathematical derivation of the novel approach. We develop our experimental setup on the node-level <b>graph</b> prediction tasks, where SSL pre-training has shown to be difficult due to the large size of real world <b>graphs.</b> Our experiments demonstrate that the cost associated with the loss computation can be reduced via node or dimension sampling without lowering the downstream performance. Our results demonstrate that sampling mostly results in improved downstream performance. Ablation studies and experimental analysis are provided to untangle the role of the different factors in the experimental setup.</p></p class="citation"></blockquote><h3 id=2857--86261-loss-shaping-constraints-for-long-term-time-series-forecasting-ignacio-hounie-et-al-2024>(28/57 | 86/261) Loss Shaping Constraints for Long-Term Time Series Forecasting (Ignacio Hounie et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ignacio Hounie, Javier Porras-Valenzuela, Alejandro Ribeiro. (2024)<br><strong>Loss Shaping Constraints for Long-Term Time Series Forecasting</strong><br><button class=copy-to-clipboard title="Loss Shaping Constraints for Long-Term Time Series Forecasting" index=86>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-86 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-ML<br>Keyword Score: 23<br>Keywords: Benchmarking, Constrained Learning, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09373v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09373v1.pdf filename=2402.09373v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Several applications in time series forecasting require predicting multiple steps ahead. Despite the vast amount of literature in the topic, both classical and recent deep learning based approaches have mostly focused on minimising performance averaged over the predicted window. We observe that this can lead to disparate distributions of errors across forecasting steps, especially for recent <b>transformer</b> architectures trained on popular forecasting <b>benchmarks.</b> That is, optimising performance on average can lead to undesirably large errors at specific time-steps. In this work, we present a <b>Constrained</b> <b>Learning</b> approach for long-term time series forecasting that aims to find the best model in terms of average performance that respects a user-defined upper bound on the loss at each time-step. We call our approach loss shaping constraints because it imposes constraints on the loss at each time step, and leverage recent duality results to show that despite its non-convexity, the resulting problem has a bounded duality gap. We propose a practical Primal-Dual algorithm to tackle it, and demonstrate that the proposed approach exhibits competitive average performance in time series forecasting <b>benchmarks,</b> while shaping the distribution of errors across the predicted window.</p></p class="citation"></blockquote><h3 id=2957--87261-tackling-negative-transfer-on-graphs-zehong-wang-et-al-2024>(29/57 | 87/261) Tackling Negative Transfer on Graphs (Zehong Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zehong Wang, Zheyuan Zhang, Chuxu Zhang, Yanfang Ye. (2024)<br><strong>Tackling Negative Transfer on Graphs</strong><br><button class=copy-to-clipboard title="Tackling Negative Transfer on Graphs" index=87>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-87 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs-SI, cs.LG<br>Keyword Score: 23<br>Keywords: Graph, Node Embedding, Transfer Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08907v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08907v1.pdf filename=2402.08907v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Transfer</b> <b>learning</b> aims to boost the learning on the target task leveraging knowledge learned from other relevant tasks. However, when the source and target are not closely related, the learning performance may be adversely affected, a phenomenon known as negative <b>transfer.</b> <b>In</b> this paper, we investigate the negative <b>transfer</b> <b>in</b> <b>graph</b> <b>transfer</b> <b>learning,</b> which is important yet underexplored. We reveal that, unlike image or text, negative <b>transfer</b> <b>commonly</b> occurs in <b>graph-structured</b> data, even when source and target <b>graphs</b> share semantic similarities. Specifically, we identify that structural differences significantly amplify the dissimilarities in the <b>node</b> <b>embeddings</b> across <b>graphs.</b> To mitigate this, we bring a new insight: for semantically similar <b>graphs,</b> although structural differences lead to significant distribution shift in <b>node</b> <b>embeddings,</b> their impact on subgraph embeddings could be marginal. Building on this insight, we introduce two effective yet elegant methods, Subgraph Pooling (SP) and Subgraph Pooling++ (SP++), that <b>transfer</b> <b>subgraph-level</b> knowledge across <b>graphs.</b> We theoretically analyze the role of SP in reducing <b>graph</b> discrepancy and conduct extensive experiments to evaluate its superiority under various settings. Our code and datasets are available at: <a href=https://github.com/Zehong-Wang/Subgraph-Pooling>https://github.com/Zehong-Wang/Subgraph-Pooling</a>.</p></p class="citation"></blockquote><h3 id=3057--88261-a-dynamical-view-of-the-question-of-why-mehdi-fatemi-et-al-2024>(30/57 | 88/261) A Dynamical View of the Question of Why (Mehdi Fatemi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mehdi Fatemi, Sindhu Gowda. (2024)<br><strong>A Dynamical View of the Question of Why</strong><br><button class=copy-to-clipboard title="A Dynamical View of the Question of Why" index=88>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-88 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs-SY, cs.LG, eess-SY<br>Keyword Score: 20<br>Keywords: Reinforcement Learning, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10240v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10240v1.pdf filename=2402.10240v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We address causal <b>reasoning</b> in multivariate time series data generated by stochastic processes. Existing approaches are largely restricted to static settings, ignoring the continuity and emission of variations across time. In contrast, we propose a learning paradigm that directly establishes causation between events in the course of time. We present two key lemmas to compute causal contributions and frame them as <b>reinforcement</b> <b>learning</b> problems. Our approach offers formal and computational tools for uncovering and quantifying causal relationships in diffusion processes, subsuming various important settings such as discrete-time Markov decision processes. Finally, in fairly intricate experiments and through sheer learning, our framework reveals and quantifies causal links, which otherwise seem inexplicable.</p></p class="citation"></blockquote><h3 id=3157--89261-hire-high-recall-approximate-top-k-estimation-for-efficient-llm-inference-yashas-samaga-b-l-et-al-2024>(31/57 | 89/261) HiRE: High Recall Approximate Top-$k$ Estimation for Efficient LLM Inference (Yashas Samaga B L et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yashas Samaga B L, Varun Yerram, Chong You, Srinadh Bhojanapalli, Sanjiv Kumar, Prateek Jain, Praneeth Netrapalli. (2024)<br><strong>HiRE: High Recall Approximate Top-$k$ Estimation for Efficient LLM Inference</strong><br><button class=copy-to-clipboard title="HiRE: High Recall Approximate Top-$k$ Estimation for Efficient LLM Inference" index=89>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-89 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09360v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09360v1.pdf filename=2402.09360v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Autoregressive decoding with generative <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> on accelerators (GPUs/TPUs) is often memory-bound where most of the time is spent on transferring model parameters from high bandwidth memory (HBM) to cache. On the other hand, recent works show that <b>LLMs</b> can maintain quality with significant sparsity/redundancy in the feedforward (FFN) layers by appropriately training the model to operate on a top-$k$ fraction of rows/columns (where $k \approx 0.05$), there by suggesting a way to reduce the transfer of model parameters, and hence latency. However, exploiting this sparsity for improving latency is hindered by the fact that identifying top rows/columns is data-dependent and is usually performed using full matrix operations, severely limiting potential gains. To address these issues, we introduce HiRE (High Recall Approximate Top-k Estimation). HiRE comprises of two novel components: (i) a compression scheme to cheaply predict top-$k$ rows/columns with high recall, followed by full computation restricted to the predicted subset, and (ii) DA-TOP-$k$: an efficient multi-device approximate top-$k$ operator. We demonstrate that on a one billion parameter model, HiRE applied to both the softmax as well as feedforward layers, achieves almost matching pretraining and downstream accuracy, and speeds up inference latency by $1.47\times$ on a single TPUv5e device.</p></p class="citation"></blockquote><h3 id=3257--90261-research-and-application-of-transformer-based-anomaly-detection-model-a-literature-review-mingrui-ma-et-al-2024>(32/57 | 90/261) Research and application of Transformer based anomaly detection model: A literature review (Mingrui Ma et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mingrui Ma, Lansheng Han, Chunjie Zhou. (2024)<br><strong>Research and application of Transformer based anomaly detection model: A literature review</strong><br><button class=copy-to-clipboard title="Research and application of Transformer based anomaly detection model: A literature review" index=90>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-90 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Anomaly Detection, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08975v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08975v1.pdf filename=2402.08975v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Transformer,</b> as one of the most advanced neural network models in Natural Language Processing (NLP), exhibits diverse applications in the field of <b>anomaly</b> <b>detection.</b> To inspire research on <b>Transformer-based</b> <b>anomaly</b> <b>detection,</b> this review offers a fresh perspective on the concept of <b>anomaly</b> <b>detection.</b> We explore the current challenges of <b>anomaly</b> <b>detection</b> and provide detailed insights into the operating principles of <b>Transformer</b> and its variants in <b>anomaly</b> <b>detection</b> tasks. Additionally, we delineate various application scenarios for <b>Transformer-based</b> <b>anomaly</b> <b>detection</b> models and discuss the datasets and evaluation metrics employed. Furthermore, this review highlights the key challenges in <b>Transformer-based</b> <b>anomaly</b> <b>detection</b> research and conducts a comprehensive analysis of future research trends in this domain. The review includes an extensive compilation of over 100 core references related to <b>Transformer-based</b> <b>anomaly</b> <b>detection.</b> To the best of our knowledge, this is the first comprehensive review that focuses on the research related to <b>Transformer</b> in the context of <b>anomaly</b> <b>detection.</b> We hope that this paper can provide detailed technical information to researchers interested in <b>Transformer-based</b> <b>anomaly</b> <b>detection</b> tasks.</p></p class="citation"></blockquote><h3 id=3357--91261-duel-duplicate-elimination-on-active-memory-for-self-supervised-class-imbalanced-learning-won-seok-choi-et-al-2024>(33/57 | 91/261) DUEL: Duplicate Elimination on Active Memory for Self-Supervised Class-Imbalanced Learning (Won-Seok Choi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Won-Seok Choi, Hyundo Lee, Dong-Sig Han, Junseok Park, Heeyeon Koo, Byoung-Tak Zhang. (2024)<br><strong>DUEL: Duplicate Elimination on Active Memory for Self-Supervised Class-Imbalanced Learning</strong><br><button class=copy-to-clipboard title="DUEL: Duplicate Elimination on Active Memory for Self-Supervised Class-Imbalanced Learning" index=91>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-91 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Self-supervised Learning, Self-supervised Pre-training<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08963v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08963v1.pdf filename=2402.08963v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent machine learning algorithms have been developed using well-curated datasets, which often require substantial cost and resources. On the other hand, the direct use of raw data often leads to overfitting towards frequently occurring class information. To address class imbalances cost-efficiently, we propose an active data filtering process during <b>self-supervised</b> <b>pre-training</b> in our novel framework, Duplicate Elimination (DUEL). This framework integrates an active memory inspired by human working memory and introduces distinctiveness information, which measures the diversity of the data in the memory, to optimize both the feature extractor and the memory. The DUEL policy, which replaces the most duplicated data with new samples, aims to enhance the distinctiveness information in the memory and thereby mitigate class imbalances. We validate the effectiveness of the DUEL framework in class-imbalanced environments, demonstrating its robustness and providing reliable results in downstream tasks. We also analyze the role of the DUEL policy in the training process through various metrics and visualizations.</p></p class="citation"></blockquote><h3 id=3457--92261-mean-field-analysis-for-learning-subspace-sparse-polynomials-with-gaussian-input-ziang-chen-et-al-2024>(34/57 | 92/261) Mean-Field Analysis for Learning Subspace-Sparse Polynomials with Gaussian Input (Ziang Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ziang Chen, Rong Ge. (2024)<br><strong>Mean-Field Analysis for Learning Subspace-Sparse Polynomials with Gaussian Input</strong><br><button class=copy-to-clipboard title="Mean-Field Analysis for Learning Subspace-Sparse Polynomials with Gaussian Input" index=92>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-92 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, math-AP<br>Keyword Score: 20<br>Keywords: Stochastic Gradient Descent, Stochastic Gradient Descent<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08948v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08948v1.pdf filename=2402.08948v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this work, we study the mean-field flow for learning subspace-sparse polynomials using <b>stochastic</b> <b>gradient</b> <b>descent</b> and two-layer neural networks, where the input distribution is standard Gaussian and the output only depends on the projection of the input onto a low-dimensional subspace. We propose a basis-free generalization of the merged-staircase property in Abbe et al. (2022) and establish a necessary condition for the <b>SGD-learnability.</b> In addition, we prove that the condition is almost sufficient, in the sense that a condition slightly stronger than the necessary condition can guarantee the exponential decay of the loss functional to zero.</p></p class="citation"></blockquote><h3 id=3557--93261-imuoptimize-a-data-driven-approach-to-optimal-imu-placement-for-human-pose-estimation-with-transformer-architecture-varun-ramani-et-al-2024>(35/57 | 93/261) IMUOptimize: A Data-Driven Approach to Optimal IMU Placement for Human Pose Estimation with Transformer Architecture (Varun Ramani et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Varun Ramani, Hossein Khayami, Yang Bai, Nakul Garg, Nirupam Roy. (2024)<br><strong>IMUOptimize: A Data-Driven Approach to Optimal IMU Placement for Human Pose Estimation with Transformer Architecture</strong><br><button class=copy-to-clipboard title="IMUOptimize: A Data-Driven Approach to Optimal IMU Placement for Human Pose Estimation with Transformer Architecture" index=93>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-93 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Recurrent Neural Network, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08923v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08923v2.pdf filename=2402.08923v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents a novel approach for predicting human poses using IMU data, diverging from previous studies such as DIP-IMU, IMUPoser, and TransPose, which use up to 6 IMUs in conjunction with bidirectional <b>RNNs.</b> We introduce two main innovations: a data-driven strategy for optimal IMU placement and a <b>transformer-based</b> model architecture for time series analysis. Our findings indicate that our approach not only outperforms traditional 6 IMU-based biRNN models but also that the <b>transformer</b> architecture significantly enhances pose reconstruction from data obtained from 24 IMU locations, with equivalent performance to biRNNs when using only 6 IMUs. The enhanced accuracy provided by our optimally chosen locations, when coupled with the parallelizability and performance of <b>transformers,</b> provides significant improvements to the field of IMU-based pose estimation.</p></p class="citation"></blockquote><h3 id=3657--94261-unifying-invariance-and-spuriousity-for-graph-out-of-distribution-via-probability-of-necessity-and-sufficiency-xuexin-chen-et-al-2024>(36/57 | 94/261) Unifying Invariance and Spuriousity for Graph Out-of-Distribution via Probability of Necessity and Sufficiency (Xuexin Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xuexin Chen, Ruichu Cai, Kaitao Zheng, Zhifan Jiang, Zhengting Huang, Zhifeng Hao, Zijian Li. (2024)<br><strong>Unifying Invariance and Spuriousity for Graph Out-of-Distribution via Probability of Necessity and Sufficiency</strong><br><button class=copy-to-clipboard title="Unifying Invariance and Spuriousity for Graph Out-of-Distribution via Probability of Necessity and Sufficiency" index=94>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-94 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 16<br>Keywords: Graph, Benchmarking, Out-of-distribution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09165v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09165v1.pdf filename=2402.09165v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Graph</b> <b>Out-of-Distribution</b> (OOD), requiring that models trained on biased data generalize to the unseen test data, has a massive of real-world applications. One of the most mainstream methods is to extract the invariant subgraph by aligning the original and augmented data with the help of environment augmentation. However, these solutions might lead to the loss or redundancy of semantic subgraph and further result in suboptimal generalization. To address this challenge, we propose a unified framework to exploit the Probability of Necessity and Sufficiency to extract the Invariant Substructure (PNSIS). Beyond that, this framework further leverages the spurious subgraph to boost the generalization performance in an ensemble manner to enhance the robustness on the noise data. Specificially, we first consider the data generation process for <b>graph</b> data. Under mild conditions, we show that the invariant subgraph can be extracted by minimizing an upper bound, which is built on the theoretical advance of probability of necessity and sufficiency. To further bridge the theory and algorithm, we devise the PNSIS model, which involves an invariant subgraph extractor for invariant <b>graph</b> learning as well invariant and spurious subgraph classifiers for generalization enhancement. Experimental results demonstrate that our \textbf{PNSIS} model outperforms the state-of-the-art techniques on <b>graph</b> OOD on several <b>benchmarks,</b> highlighting the effectiveness in real-world scenarios.</p></p class="citation"></blockquote><h3 id=3757--95261-exploring-federated-deep-learning-for-standardising-naming-conventions-in-radiotherapy-data-ali-haidar-et-al-2024>(37/57 | 95/261) Exploring Federated Deep Learning for Standardising Naming Conventions in Radiotherapy Data (Ali Haidar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ali Haidar, Daniel Al Mouiee, Farhannah Aly, David Thwaites, Lois Holloway. (2024)<br><strong>Exploring Federated Deep Learning for Standardising Naming Conventions in Radiotherapy Data</strong><br><button class=copy-to-clipboard title="Exploring Federated Deep Learning for Standardising Naming Conventions in Radiotherapy Data" index=95>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-95 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, physics-med-ph<br>Keyword Score: 16<br>Keywords: Federated Learning, Multi-modal, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08999v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08999v1.pdf filename=2402.08999v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Standardising structure volume names in radiotherapy (RT) data is necessary to enable data mining and analyses, especially across multi-institutional centres. This process is time and resource intensive, which highlights the need for new automated and efficient approaches to handle the task. Several machine learning-based methods have been proposed and evaluated to standardise nomenclature. However, no studies have considered that RT patient records are distributed across multiple data centres. This paper introduces a method that emulates real-world environments to establish standardised nomenclature. This is achieved by integrating decentralised real-time data and <b>federated</b> <b>learning</b> (FL). A <b>multimodal</b> deep artificial neural network was proposed to standardise RT data in <b>federated</b> <b>settings.</b> Three types of possible attributes were extracted from the structures to train the deep learning models: tabular, visual, and volumetric. Simulated experiments were carried out to train the models across several scenarios including multiple data centres, input modalities, and aggregation strategies. The models were compared against models developed with single modalities in <b>federated</b> <b>settings,</b> in addition to models trained in centralised settings. Categorical classification accuracy was calculated on hold-out samples to inform the models performance. Our results highlight the need for fusing multiple modalities when training such models, with better performance reported with tabular-volumetric models. In addition, we report comparable accuracy compared to models built in centralised settings. This demonstrates the suitability of FL for handling the standardization task. Additional ablation analyses showed that the total number of samples in the data centres and the number of data centres highly affects the training process and should be carefully considered when building standardisation models.</p></p class="citation"></blockquote><h3 id=3857--96261-layerwise-proximal-replay-a-proximal-point-method-for-online-continual-learning-jason-yoo-et-al-2024>(38/57 | 96/261) Layerwise Proximal Replay: A Proximal Point Method for Online Continual Learning (Jason Yoo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jason Yoo, Yunpeng Liu, Frank Wood, Geoff Pleiss. (2024)<br><strong>Layerwise Proximal Replay: A Proximal Point Method for Online Continual Learning</strong><br><button class=copy-to-clipboard title="Layerwise Proximal Replay: A Proximal Point Method for Online Continual Learning" index=96>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-96 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 15<br>Keywords: Continual Learning, Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09542v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09542v1.pdf filename=2402.09542v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In online <b>continual</b> <b>learning,</b> a neural network incrementally learns from a non-i.i.d. data stream. Nearly all online <b>continual</b> <b>learning</b> methods employ experience replay to simultaneously prevent catastrophic forgetting and underfitting on past data. Our work demonstrates a limitation of this approach: networks trained with experience replay tend to have unstable optimization trajectories, impeding their overall accuracy. Surprisingly, these instabilities persist even when the replay buffer stores all previous training examples, suggesting that this issue is orthogonal to catastrophic forgetting. We minimize these instabilities through a simple modification of the optimization <b>geometry.</b> Our solution, Layerwise Proximal Replay (LPR), balances learning from new and replay data while only allowing for gradual changes in the hidden activation of past data. We demonstrate that LPR consistently improves replay-based online <b>continual</b> <b>learning</b> methods across multiple problem settings, regardless of the amount of available replay memory.</p></p class="citation"></blockquote><h3 id=3957--97261-exact-fast-and-expressive-poisson-point-processes-via-squared-neural-families-russell-tsuchida-et-al-2024>(39/57 | 97/261) Exact, Fast and Expressive Poisson Point Processes via Squared Neural Families (Russell Tsuchida et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Russell Tsuchida, Cheng Soon Ong, Dino Sejdinovic. (2024)<br><strong>Exact, Fast and Expressive Poisson Point Processes via Squared Neural Families</strong><br><button class=copy-to-clipboard title="Exact, Fast and Expressive Poisson Point Processes via Squared Neural Families" index=97>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-97 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-ML<br>Keyword Score: 13<br>Keywords: Benchmarking, Gaussian Process<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09608v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09608v1.pdf filename=2402.09608v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce squared neural Poisson point processes (SNEPPPs) by parameterising the intensity function by the squared norm of a two layer neural network. When the hidden layer is fixed and the second layer has a single neuron, our approach resembles previous uses of squared <b>Gaussian</b> <b>process</b> or kernel methods, but allowing the hidden layer to be learnt allows for additional flexibility. In many cases of interest, the integrated intensity function admits a closed form and can be computed in quadratic time in the number of hidden neurons. We enumerate a far more extensive number of such cases than has previously been discussed. Our approach is more memory and time efficient than naive implementations of squared or exponentiated kernel methods or <b>Gaussian</b> <b>processes.</b> Maximum likelihood and maximum a posteriori estimates in a reparameterisation of the final layer of the intensity function can be obtained by solving a (strongly) convex optimisation problem using projected gradient descent. We demonstrate SNEPPPs on real, and synthetic <b>benchmarks,</b> and provide a software implementation. <a href=https://github.com/RussellTsuchida/snefy>https://github.com/RussellTsuchida/snefy</a></p></p class="citation"></blockquote><h3 id=4057--98261-momentum-approximation-in-asynchronous-private-federated-learning-tao-yu-et-al-2024>(40/57 | 98/261) Momentum Approximation in Asynchronous Private Federated Learning (Tao Yu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tao Yu, Congzheng Song, Jianyu Wang, Mona Chitnis. (2024)<br><strong>Momentum Approximation in Asynchronous Private Federated Learning</strong><br><button class=copy-to-clipboard title="Momentum Approximation in Asynchronous Private Federated Learning" index=98>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-98 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 13<br>Keywords: Benchmarking, Federated Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09247v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09247v1.pdf filename=2402.09247v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Asynchronous protocols have been shown to improve the scalability of <b>federated</b> <b>learning</b> (FL) with a massive number of clients. Meanwhile, momentum-based methods can achieve the best model quality in synchronous FL. However, naively applying momentum in asynchronous FL algorithms leads to slower convergence and degraded model performance. It is still unclear how to effective combinie these two techniques together to achieve a win-win. In this paper, we find that asynchrony introduces implicit bias to momentum updates. In order to address this problem, we propose momentum approximation that minimizes the bias by finding an optimal weighted average of all historical model updates. Momentum approximation is compatible with secure aggregation as well as differential privacy, and can be easily integrated in production FL systems with a minor communication and storage cost. We empirically demonstrate that on <b>benchmark</b> FL datasets, momentum approximation can achieve $1.15 \textrm{&ndash;}4\times$ speed up in convergence compared to existing asynchronous FL optimizers with momentum.</p></p class="citation"></blockquote><h3 id=4157--99261-pmgda-a-preference-based-multiple-gradient-descent-algorithm-xiaoyuan-zhang-et-al-2024>(41/57 | 99/261) PMGDA: A Preference-based Multiple Gradient Descent Algorithm (Xiaoyuan Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiaoyuan Zhang, Xi Lin, Qingfu Zhang. (2024)<br><strong>PMGDA: A Preference-based Multiple Gradient Descent Algorithm</strong><br><button class=copy-to-clipboard title="PMGDA: A Preference-based Multiple Gradient Descent Algorithm" index=99>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-99 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 13<br>Keywords: Benchmarking, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09492v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09492v2.pdf filename=2402.09492v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>It is desirable in many multi-objective machine learning applications, such as multi-task learning with conflicting objectives and multi-objective <b>reinforcement</b> <b>learning,</b> to find a Pareto solution that can match a given preference of a decision maker. These problems are often large-scale with available gradient information but cannot be handled very well by the existing algorithms. To tackle this critical issue, this paper proposes a novel predict-and-correct framework for locating a Pareto solution that fits the preference of a decision maker. In the proposed framework, a constraint function is introduced in the search progress to align the solution with a user-specific preference, which can be optimized simultaneously with multiple objective functions. Experimental results show that our proposed method can efficiently find a particular Pareto solution under the demand of a decision maker for standard multiobjective <b>benchmark,</b> multi-task learning, and multi-objective <b>reinforcement</b> <b>learning</b> problems with more than thousands of decision variables. Code is available at: <a href=https://github.com/xzhang2523/pmgda>https://github.com/xzhang2523/pmgda</a>. Our code is current provided in the pgmda.rar attached file and will be open-sourced after publication.}</p></p class="citation"></blockquote><h3 id=4257--100261-changes-by-butterflies-farsighted-forecasting-with-group-reservoir-transformer-md-kowsher-et-al-2024>(42/57 | 100/261) Changes by Butterflies: Farsighted Forecasting with Group Reservoir Transformer (Md Kowsher et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Md Kowsher, Jia Xu. (2024)<br><strong>Changes by Butterflies: Farsighted Forecasting with Group Reservoir Transformer</strong><br><button class=copy-to-clipboard title="Changes by Butterflies: Farsighted Forecasting with Group Reservoir Transformer" index=100>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-100 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CL, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09573v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09573v1.pdf filename=2402.09573v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In Chaos, a minor divergence between two initial conditions exhibits exponential amplification over time, leading to far-away outcomes, known as the butterfly effect. Thus, the distant future is full of uncertainty and hard to forecast. We introduce Group Reservoir <b>Transformer</b> to predict long-term events more accurately and robustly by overcoming two challenges in Chaos: (1) the extensive historical sequences and (2) the sensitivity to initial conditions. A reservoir is attached to a <b>Transformer</b> to efficiently handle arbitrarily long historical lengths, with an extension of a group of reservoirs to reduce the uncertainty due to the initialization variations. Our architecture consistently outperforms state-of-the-art DNN models in multivariate time series, including NLinear, Pyformer, Informer, Autoformer, and the baseline <b>Transformer,</b> with an error reduction of up to -89.43% in various fields such as ETTh, ETTm, and air quality, demonstrating that an ensemble of butterfly learning, the prediction can be improved to a more adequate and certain one, despite of the traveling time to the unknown future.</p></p class="citation"></blockquote><h3 id=4357--101261-the-manifold-density-function-an-intrinsic-method-for-the-validation-of-manifold-learning-benjamin-holmgren-et-al-2024>(43/57 | 101/261) The Manifold Density Function: An Intrinsic Method for the Validation of Manifold Learning (Benjamin Holmgren et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Benjamin Holmgren, Eli Quist, Jordan Schupbach, Brittany Terese Fasy, Bastian Rieck. (2024)<br><strong>The Manifold Density Function: An Intrinsic Method for the Validation of Manifold Learning</strong><br><button class=copy-to-clipboard title="The Manifold Density Function: An Intrinsic Method for the Validation of Manifold Learning" index=101>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-101 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: 57Z25, I-5-2, cs-LG, cs.LG, math-AT<br>Keyword Score: 10<br>Keywords: Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09529v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09529v1.pdf filename=2402.09529v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce the manifold density function, which is an intrinsic method to validate manifold learning techniques. Our approach adapts and extends Ripley&rsquo;s $K$-function, and categorizes in an <b>unsupervised</b> setting the extent to which an output of a manifold learning algorithm captures the structure of a latent manifold. Our manifold density function generalizes to broad classes of Riemannian manifolds. In particular, we extend the manifold density function to general two-manifolds using the Gauss-Bonnet theorem, and demonstrate that the manifold density function for hypersurfaces is well approximated using the first Laplacian eigenvalue. We prove desirable convergence and robustness properties.</p></p class="citation"></blockquote><h3 id=4457--102261-transformers-can-achieve-length-generalization-but-not-robustly-yongchao-zhou-et-al-2024>(44/57 | 102/261) Transformers Can Achieve Length Generalization But Not Robustly (Yongchao Zhou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yongchao Zhou, Uri Alon, Xinyun Chen, Xuezhi Wang, Rishabh Agarwal, Denny Zhou. (2024)<br><strong>Transformers Can Achieve Length Generalization But Not Robustly</strong><br><button class=copy-to-clipboard title="Transformers Can Achieve Length Generalization But Not Robustly" index=102>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-102 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CL, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09371v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09371v1.pdf filename=2402.09371v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Length generalization, defined as the ability to extrapolate from shorter training sequences to longer test ones, is a significant challenge for language models. This issue persists even with large-scale <b>Transformers</b> handling relatively straightforward tasks. In this paper, we test the <b>Transformer&rsquo;s</b> ability of length generalization using the task of addition of two integers. We show that the success of length generalization is intricately linked to the data format and the type of position encoding. Using the right combination of data format and position encodings, we show for the first time that standard <b>Transformers</b> can extrapolate to a sequence length that is 2.5x the input length. Nevertheless, unlike in-distribution generalization, length generalization remains fragile, significantly influenced by factors like random weight initialization and training data order, leading to large variances across different random seeds.</p></p class="citation"></blockquote><h3 id=4557--103261-information-complexity-of-stochastic-convex-optimization-applications-to-generalization-and-memorization-idan-attias-et-al-2024>(45/57 | 103/261) Information Complexity of Stochastic Convex Optimization: Applications to Generalization and Memorization (Idan Attias et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Idan Attias, Gintare Karolina Dziugaite, Mahdi Haghifam, Roi Livni, Daniel M. Roy. (2024)<br><strong>Information Complexity of Stochastic Convex Optimization: Applications to Generalization and Memorization</strong><br><button class=copy-to-clipboard title="Information Complexity of Stochastic Convex Optimization: Applications to Generalization and Memorization" index=103>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-103 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Mutual Information<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09327v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09327v1.pdf filename=2402.09327v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this work, we investigate the interplay between memorization and learning in the context of \emph{stochastic convex optimization} (SCO). We define memorization via the information a learning algorithm reveals about its training data points. We then quantify this information using the framework of conditional <b>mutual</b> <b>information</b> (CMI) proposed by Steinke and Zakynthinou (2020). Our main result is a precise characterization of the tradeoff between the accuracy of a learning algorithm and its CMI, answering an open question posed by Livni (2023). We show that, in the $L^2$ Lipschitz&ndash;bounded setting and under strong convexity, every learner with an excess error $\varepsilon$ has CMI bounded below by $\Omega(1/\varepsilon^2)$ and $\Omega(1/\varepsilon)$, respectively. We further demonstrate the essential role of memorization in learning problems in SCO by designing an adversary capable of accurately identifying a significant fraction of the training samples in specific SCO problems. Finally, we enumerate several implications of our results, such as a limitation of generalization bounds based on CMI and the incompressibility of samples in SCO problems.</p></p class="citation"></blockquote><h3 id=4657--104261-stability-and-multigroup-fairness-in-ranking-with-uncertain-predictions-siddartha-devic-et-al-2024>(46/57 | 104/261) Stability and Multigroup Fairness in Ranking with Uncertain Predictions (Siddartha Devic et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Siddartha Devic, Aleksandra Korolova, David Kempe, Vatsal Sharan. (2024)<br><strong>Stability and Multigroup Fairness in Ranking with Uncertain Predictions</strong><br><button class=copy-to-clipboard title="Stability and Multigroup Fairness in Ranking with Uncertain Predictions" index=104>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-104 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Fairness<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09326v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09326v1.pdf filename=2402.09326v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Rankings are ubiquitous across many applications, from search engines to hiring committees. In practice, many rankings are derived from the output of predictors. However, when predictors trained for classification tasks have intrinsic uncertainty, it is not obvious how this uncertainty should be represented in the derived rankings. Our work considers ranking functions: maps from individual predictions for a classification task to distributions over rankings. We focus on two aspects of ranking functions: stability to perturbations in predictions and <b>fairness</b> towards both individuals and subgroups. Not only is stability an important requirement for its own sake, but &ndash; as we show &ndash; it composes harmoniously with individual <b>fairness</b> in the sense of Dwork et al. (2012). While deterministic ranking functions cannot be stable aside from trivial scenarios, we show that the recently proposed uncertainty aware (UA) ranking functions of Singh et al. (2021) are stable. Our main result is that UA rankings also achieve multigroup <b>fairness</b> through successful composition with multiaccurate or multicalibrated predictors. Our work demonstrates that UA rankings naturally interpolate between group and individual level <b>fairness</b> guarantees, while simultaneously satisfying stability guarantees important whenever machine-learned predictions are used.</p></p class="citation"></blockquote><h3 id=4757--105261-hybrid-machine-learning-techniques-in-the-management-of-harmful-algal-blooms-impact-andres-molares-ulloa-et-al-2024>(47/57 | 105/261) Hybrid Machine Learning techniques in the management of harmful algal blooms impact (Andres Molares-Ulloa et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Andres Molares-Ulloa, Daniel Rivero, Jesus Gil Ruiz, Enrique Fernandez-Blanco, Luis de-la-Fuente-Valentín. (2024)<br><strong>Hybrid Machine Learning techniques in the management of harmful algal blooms impact</strong><br><button class=copy-to-clipboard title="Hybrid Machine Learning techniques in the management of harmful algal blooms impact" index=105>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-105 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, q-bio-QM<br>Keyword Score: 10<br>Keywords: BLOOM<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09271v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09271v1.pdf filename=2402.09271v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Harmful algal <b>blooms</b> (HABs) are episodes of high concentrations of algae that are potentially toxic for human consumption. Mollusc farming can be affected by HABs because, as filter feeders, they can accumulate high concentrations of marine biotoxins in their tissues. To avoid the risk to human consumption, harvesting is prohibited when toxicity is detected. At present, the closure of production areas is based on expert knowledge and the existence of a predictive model would help when conditions are complex and sampling is not possible. Although the concentration of toxin in meat is the method most commonly used by experts in the control of shellfish production areas, it is rarely used as a target by automatic prediction models. This is largely due to the irregularity of the data due to the established sampling programs. As an alternative, the activity status of production areas has been proposed as a target variable based on whether mollusc meat has a toxicity level below or above the legal limit. This new option is the most similar to the actual functioning of the control of shellfish production areas. For this purpose, we have made a comparison between hybrid machine learning models like Neural-Network-Adding Bootstrap (BAGNET) and Discriminative Nearest Neighbor Classification (SVM-KNN) when estimating the state of production areas. The study has been carried out in several estuaries with different levels of complexity in the episodes of algal <b>blooms</b> to demonstrate the generalization capacity of the models in <b>bloom</b> detection. As a result, we could observe that, with an average recall value of 93.41% and without dropping below 90% in any of the estuaries, BAGNET outperforms the other models both in terms of results and robustness.</p></p class="citation"></blockquote><h3 id=4857--106261-ur2m-uncertainty-and-resource-aware-event-detection-on-microcontrollers-hong-jia-et-al-2024>(48/57 | 106/261) UR2M: Uncertainty and Resource-Aware Event Detection on Microcontrollers (Hong Jia et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hong Jia, Young D. Kwon, Dong Ma, Nhat Pham, Lorena Qendro, Tam Vu, Cecilia Mascolo. (2024)<br><strong>UR2M: Uncertainty and Resource-Aware Event Detection on Microcontrollers</strong><br><button class=copy-to-clipboard title="UR2M: Uncertainty and Resource-Aware Event Detection on Microcontrollers" index=106>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-106 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-HC, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Event Detection<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09264v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09264v2.pdf filename=2402.09264v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Traditional machine learning techniques are prone to generating inaccurate predictions when confronted with shifts in the distribution of data between the training and testing phases. This vulnerability can lead to severe consequences, especially in applications such as mobile healthcare. Uncertainty estimation has the potential to mitigate this issue by assessing the reliability of a model&rsquo;s output. However, existing uncertainty estimation techniques often require substantial computational resources and memory, making them impractical for implementation on microcontrollers (MCUs). This limitation hinders the feasibility of many important on-device wearable <b>event</b> <b>detection</b> (WED) applications, such as heart attack detection. In this paper, we present UR2M, a novel Uncertainty and Resource-aware <b>event</b> <b>detection</b> framework for MCUs. Specifically, we (i) develop an uncertainty-aware WED based on evidential theory for accurate <b>event</b> <b>detection</b> and reliable uncertainty estimation; (ii) introduce a cascade ML framework to achieve efficient model inference via early exits, by sharing shallower model layers among different <b>event</b> <b>models;</b> (iii) optimize the deployment of the model and MCU library for system efficiency. We conducted extensive experiments and compared UR2M to traditional uncertainty baselines using three wearable datasets. Our results demonstrate that UR2M achieves up to 864% faster inference speed, 857% energy-saving for uncertainty estimation, 55% memory saving on two popular MCUs, and a 22% improvement in uncertainty quantification performance. UR2M can be deployed on a wide range of MCUs, significantly expanding real-time and reliable WED applications.</p></p class="citation"></blockquote><h3 id=4957--107261-improved-regret-for-bandit-convex-optimization-with-delayed-feedback-yuanyu-wan-et-al-2024>(49/57 | 107/261) Improved Regret for Bandit Convex Optimization with Delayed Feedback (Yuanyu Wan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuanyu Wan, Chang Yao, Mingli Song, Lijun Zhang. (2024)<br><strong>Improved Regret for Bandit Convex Optimization with Delayed Feedback</strong><br><button class=copy-to-clipboard title="Improved Regret for Bandit Convex Optimization with Delayed Feedback" index=107>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-107 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Bandit Algorithm<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09152v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09152v1.pdf filename=2402.09152v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We investigate <b>bandit</b> convex optimization (BCO) with delayed feedback, where only the loss value of the action is revealed under an arbitrary delay. Previous studies have established a regret bound of $O(T^{3/4}+d^{1/3}T^{2/3})$ for this problem, where $d$ is the maximum delay, by simply feeding delayed loss values to the classical <b>bandit</b> gradient descent (BGD) algorithm. In this paper, we develop a novel algorithm to enhance the regret, which carefully exploits the delayed <b>bandit</b> feedback via a blocking update mechanism. Our analysis first reveals that the proposed algorithm can decouple the joint effect of the delays and <b>bandit</b> feedback on the regret, and improve the regret bound to $O(T^{3/4}+\sqrt{dT})$ for convex functions. Compared with the previous result, our regret matches the $O(T^{3/4})$ regret of BGD in the non-delayed setting for a larger amount of delay, i.e., $d=O(\sqrt{T})$, instead of $d=O(T^{1/4})$. Furthermore, we consider the case with strongly convex functions, and prove that the proposed algorithm can enjoy a better regret bound of $O(T^{2/3}\log^{1/3}T+d\log T)$. Finally, we show that in a special case with unconstrained action sets, it can be simply extended to achieve a regret bound of $O(\sqrt{T\log T}+d\log T)$ for strongly convex and smooth functions.</p></p class="citation"></blockquote><h3 id=5057--108261-resqunnstowards-enabling-deep-learning-in-quantum-convolution-neural-networks-muhammad-kashif-et-al-2024>(50/57 | 108/261) ResQuNNs:Towards Enabling Deep Learning in Quantum Convolution Neural Networks (Muhammad Kashif et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Muhammad Kashif, Muhammad Shafique. (2024)<br><strong>ResQuNNs:Towards Enabling Deep Learning in Quantum Convolution Neural Networks</strong><br><button class=copy-to-clipboard title="ResQuNNs:Towards Enabling Deep Learning in Quantum Convolution Neural Networks" index=108>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-108 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, quant-ph<br>Keyword Score: 10<br>Keywords: Convolution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09146v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09146v1.pdf filename=2402.09146v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we present a novel framework for enhancing the performance of Quanvolutional Neural Networks (QuNNs) by introducing trainable quanvolutional layers and addressing the critical challenges associated with them. Traditional quanvolutional layers, although beneficial for feature extraction, have largely been static, offering limited adaptability. Unlike state-of-the-art, our research overcomes this limitation by enabling training within these layers, significantly increasing the flexibility and potential of QuNNs. However, the introduction of multiple trainable quanvolutional layers induces complexities in gradient-based optimization, primarily due to the difficulty in accessing gradients across these layers. To resolve this, we propose a novel architecture, Residual Quanvolutional Neural Networks (ResQuNNs), leveraging the concept of residual learning, which facilitates the flow of gradients by adding skip connections between layers. By inserting residual blocks between quanvolutional layers, we ensure enhanced gradient access throughout the network, leading to improved training performance. Moreover, we provide empirical evidence on the strategic placement of these residual blocks within QuNNs. Through extensive experimentation, we identify an efficient configuration of residual blocks, which enables gradients across all the layers in the network that eventually results in efficient training. Our findings suggest that the precise location of residual blocks plays a crucial role in maximizing the performance gains in QuNNs. Our results mark a substantial step forward in the evolution of quantum deep learning, offering new avenues for both theoretical development and practical quantum computing applications.</p></p class="citation"></blockquote><h3 id=5157--109261-exploiting-estimation-bias-in-deep-double-q-learning-for-actor-critic-methods-alberto-sinigaglia-et-al-2024>(51/57 | 109/261) Exploiting Estimation Bias in Deep Double Q-Learning for Actor-Critic Methods (Alberto Sinigaglia et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Alberto Sinigaglia, Niccolò Turcato, Alberto Dalla Libera, Ruggero Carli, Gian Antonio Susto. (2024)<br><strong>Exploiting Estimation Bias in Deep Double Q-Learning for Actor-Critic Methods</strong><br><button class=copy-to-clipboard title="Exploiting Estimation Bias in Deep Double Q-Learning for Actor-Critic Methods" index=109>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-109 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09078v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09078v1.pdf filename=2402.09078v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper introduces innovative methods in <b>Reinforcement</b> <b>Learning</b> (RL), focusing on addressing and exploiting estimation biases in Actor-Critic methods for continuous control tasks, using Deep Double Q-Learning. We propose two novel algorithms: Expectile Delayed Deep Deterministic Policy Gradient (ExpD3) and Bias Exploiting - Twin Delayed Deep Deterministic Policy Gradient (BE-TD3). ExpD3 aims to reduce overestimation bias with a single $Q$ estimate, offering a balance between computational efficiency and performance, while BE-TD3 is designed to dynamically select the most advantageous estimation bias during training. Our extensive experiments across various continuous control tasks demonstrate the effectiveness of our approaches. We show that these algorithms can either match or surpass existing methods like TD3, particularly in environments where estimation biases significantly impact learning. The results underline the importance of bias exploitation in improving policy learning in RL.</p></p class="citation"></blockquote><h3 id=5257--110261-reconstructing-the-geometry-of-random-geometric-graphs-han-huang-et-al-2024>(52/57 | 110/261) Reconstructing the Geometry of Random Geometric Graphs (Han Huang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Han Huang, Pakawut Jiradilok, Elchanan Mossel. (2024)<br><strong>Reconstructing the Geometry of Random Geometric Graphs</strong><br><button class=copy-to-clipboard title="Reconstructing the Geometry of Random Geometric Graphs" index=110>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-110 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, math-PR<br>Keyword Score: 8<br>Keywords: Graph, Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09591v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09591v1.pdf filename=2402.09591v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Random geometric <b>graphs</b> are random <b>graph</b> models defined on metric spaces. Such a model is defined by first sampling points from a metric space and then connecting each pair of sampled points with probability that depends on their distance, independently among pairs. In this work, we show how to efficiently reconstruct the <b>geometry</b> of the underlying space from the sampled <b>graph</b> under the manifold assumption, i.e., assuming that the underlying space is a low dimensional manifold and that the connection probability is a strictly decreasing function of the Euclidean distance between the points in a given embedding of the manifold in $\mathbb{R}^N$. Our work complements a large body of work on manifold learning, where the goal is to recover a manifold from sampled points sampled in the manifold along with their (approximate) distances.</p></p class="citation"></blockquote><h3 id=5357--111261-position-paper-challenges-and-opportunities-in-topological-deep-learning-theodore-papamarkou-et-al-2024>(53/57 | 111/261) Position Paper: Challenges and Opportunities in Topological Deep Learning (Theodore Papamarkou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Theodore Papamarkou, Tolga Birdal, Michael Bronstein, Gunnar Carlsson, Justin Curry, Yue Gao, Mustafa Hajij, Roland Kwitt, Pietro Liò, Paolo Di Lorenzo, Vasileios Maroulas, Nina Miolane, Farzana Nasrin, Karthikeyan Natesan Ramamurthy, Bastian Rieck, Simone Scardapane, Michael T. Schaub, Petar Veličković, Bei Wang, Yusu Wang, Guo-Wei Wei, Ghada Zamzmi. (2024)<br><strong>Position Paper: Challenges and Opportunities in Topological Deep Learning</strong><br><button class=copy-to-clipboard title="Position Paper: Challenges and Opportunities in Topological Deep Learning" index=111>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-111 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-ML<br>Keyword Score: 8<br>Keywords: Graph, Representation Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08871v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08871v1.pdf filename=2402.08871v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Topological deep learning (TDL) is a rapidly evolving field that uses topological features to understand and design deep learning models. This paper posits that TDL may complement <b>graph</b> <b>representation</b> <b>learning</b> and geometric deep learning by incorporating topological concepts, and can thus provide a natural choice for various machine learning settings. To this end, this paper discusses open problems in TDL, ranging from practical benefits to theoretical foundations. For each problem, it outlines potential solutions and future research opportunities. At the same time, this paper serves as an invitation to the scientific community to actively participate in TDL research to unlock the potential of this emerging field.</p></p class="citation"></blockquote><h3 id=5457--112261-implementing-local-explainability-in-gradient-boosting-trees-feature-contribution-ángel-delgado-panadero-et-al-2024>(54/57 | 112/261) Implementing local-explainability in Gradient Boosting Trees: Feature Contribution (Ángel Delgado-Panadero et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ángel Delgado-Panadero, Beatriz Hernández-Lorca, María Teresa García-Ordás, José Alberto Benítez-Andrades. (2024)<br><strong>Implementing local-explainability in Gradient Boosting Trees: Feature Contribution</strong><br><button class=copy-to-clipboard title="Implementing local-explainability in Gradient Boosting Trees: Feature Contribution" index=112>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-112 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs-LO, cs.LG<br>Keyword Score: 5<br>Keywords: Black Box<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09197v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09197v1.pdf filename=2402.09197v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Gradient Boost Decision Trees (GBDT) is a powerful additive model based on tree ensembles. Its nature makes GBDT a <b>black-box</b> <b>model</b> even though there are multiple explainable artificial intelligence (XAI) models obtaining information by reinterpreting the model globally and locally. Each tree of the ensemble is a transparent model itself but the final outcome is the result of a sum of these trees and it is not easy to clarify. In this paper, a feature contribution method for GBDT is developed. The proposed method takes advantage of the GBDT architecture to calculate the contribution of each feature using the residue of each node. This algorithm allows to calculate the sequence of node decisions given a prediction. Theoretical proofs and multiple experiments have been carried out to demonstrate the performance of our method which is not only a local explicability model for the GBDT algorithm but also a unique option that reflects GBDTs internal behavior. The proposal is aligned to the contribution of characteristics having impact in some artificial intelligence problems such as ethical analysis of Artificial Intelligence (AI) and comply with the new European laws such as the General Data Protection Regulation (GDPR) about the right to explain and nondiscrimination.</p></p class="citation"></blockquote><h3 id=5557--113261-when-representations-align-universality-in-representation-learning-dynamics-loek-van-rossem-et-al-2024>(55/57 | 113/261) When Representations Align: Universality in Representation Learning Dynamics (Loek van Rossem et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Loek van Rossem, Andrew M. Saxe. (2024)<br><strong>When Representations Align: Universality in Representation Learning Dynamics</strong><br><button class=copy-to-clipboard title="When Representations Align: Universality in Representation Learning Dynamics" index=113>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-113 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, q-bio-NC<br>Keyword Score: 5<br>Keywords: Representation Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09142v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09142v1.pdf filename=2402.09142v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep neural networks come in many sizes and architectures. The choice of architecture, in conjunction with the dataset and learning algorithm, is commonly understood to affect the learned neural <b>representations.</b> <b>Yet,</b> recent results have shown that different architectures learn <b>representations</b> <b>with</b> striking qualitative similarities. Here we derive an effective theory of <b>representation</b> <b>learning</b> under the assumption that the encoding map from input to hidden <b>representation</b> <b>and</b> the decoding map from <b>representation</b> <b>to</b> output are arbitrary smooth functions. This theory schematizes <b>representation</b> <b>learning</b> dynamics in the regime of complex, large architectures, where hidden <b>representations</b> <b>are</b> not strongly constrained by the parametrization. We show through experiments that the effective theory describes aspects of <b>representation</b> <b>learning</b> dynamics across a range of deep networks with different activation functions and architectures, and exhibits phenomena similar to the &ldquo;rich&rdquo; and &ldquo;lazy&rdquo; regime. While many network behaviors depend quantitatively on architecture, our findings point to certain behaviors that are widely conserved once models are sufficiently flexible.</p></p class="citation"></blockquote><h3 id=5657--114261-the-mirrored-influence-hypothesis-efficient-data-influence-estimation-by-harnessing-forward-passes-myeongseob-ko-et-al-2024>(56/57 | 114/261) The Mirrored Influence Hypothesis: Efficient Data Influence Estimation by Harnessing Forward Passes (Myeongseob Ko et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Myeongseob Ko, Feiyang Kang, Weiyan Shi, Ming Jin, Zhou Yu, Ruoxi Jia. (2024)<br><strong>The Mirrored Influence Hypothesis: Efficient Data Influence Estimation by Harnessing Forward Passes</strong><br><button class=copy-to-clipboard title="The Mirrored Influence Hypothesis: Efficient Data Influence Estimation by Harnessing Forward Passes" index=114>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-114 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-ML<br>Keyword Score: 5<br>Keywords: Black Box<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08922v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08922v1.pdf filename=2402.08922v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Large-scale <b>black-box</b> <b>models</b> have become ubiquitous across numerous applications. Understanding the influence of individual training data sources on predictions made by these models is crucial for improving their trustworthiness. Current influence estimation techniques involve computing gradients for every training point or repeated training on different subsets. These approaches face obvious computational challenges when scaled up to large datasets and models. In this paper, we introduce and explore the Mirrored Influence Hypothesis, highlighting a reciprocal nature of influence between training and test data. Specifically, it suggests that evaluating the influence of training data on test predictions can be reformulated as an equivalent, yet inverse problem: assessing how the predictions for training samples would be altered if the model were trained on specific test samples. Through both empirical and theoretical validations, we demonstrate the wide applicability of our hypothesis. Inspired by this, we introduce a new method for estimating the influence of training data, which requires calculating gradients for specific test samples, paired with a forward pass for each training point. This approach can capitalize on the common asymmetry in scenarios where the number of test samples under concurrent examination is much smaller than the scale of the training dataset, thus gaining a significant improvement in efficiency compared to existing approaches. We demonstrate the applicability of our method across a range of scenarios, including data attribution in diffusion models, data leakage detection, analysis of memorization, mislabeled data detection, and tracing behavior in language models. Our code will be made available at <a href=https://github.com/ruoxi-jia-group/Forward-INF>https://github.com/ruoxi-jia-group/Forward-INF</a>.</p></p class="citation"></blockquote><h3 id=5757--115261-dataset-clustering-for-improved-offline-policy-learning-qiang-wang-et-al-2024>(57/57 | 115/261) Dataset Clustering for Improved Offline Policy Learning (Qiang Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qiang Wang, Yixin Deng, Francisco Roldan Sanchez, Keru Wang, Kevin McGuinness, Noel O&rsquo;Connor, Stephen J. Redmond. (2024)<br><strong>Dataset Clustering for Improved Offline Policy Learning</strong><br><button class=copy-to-clipboard title="Dataset Clustering for Improved Offline Policy Learning" index=115>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-115 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs-RO, cs.LG<br>Keyword Score: 3<br>Keywords: Clustering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09550v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09550v1.pdf filename=2402.09550v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Offline policy learning aims to discover decision-making policies from previously-collected datasets without additional online interactions with the environment. As the training dataset is fixed, its quality becomes a crucial determining factor in the performance of the learned policy. This paper studies a dataset characteristic that we refer to as multi-behavior, indicating that the dataset is collected using multiple policies that exhibit distinct behaviors. In contrast, a uni-behavior dataset would be collected solely using one policy. We observed that policies learned from a uni-behavior dataset typically outperform those learned from multi-behavior datasets, despite the uni-behavior dataset having fewer examples and less diversity. Therefore, we propose a behavior-aware deep <b>clustering</b> approach that partitions multi-behavior datasets into several uni-behavior subsets, thereby benefiting downstream policy learning. Our approach is flexible and effective; it can adaptively estimate the number of clusters while demonstrating high <b>clustering</b> accuracy, achieving an average Adjusted Rand Index of 0.987 across various continuous control task datasets. Finally, we present improved policy learning examples using dataset <b>clustering</b> and discuss several potential scenarios where our approach might benefit the offline policy learning community.</p></p class="citation"></blockquote><h2 id=csro-10>cs.RO (10)</h2><h3 id=110--116261-how-secure-are-large-language-models-llms-for-navigation-in-urban-environments-congcong-wen-et-al-2024>(1/10 | 116/261) How Secure Are Large Language Models (LLMs) for Navigation in Urban Environments? (Congcong Wen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Congcong Wen, Jiazhao Liang, Shuaihang Yuan, Hao Huang, Yi Fang. (2024)<br><strong>How Secure Are Large Language Models (LLMs) for Navigation in Urban Environments?</strong><br><button class=copy-to-clipboard title="How Secure Are Large Language Models (LLMs) for Navigation in Urban Environments?" index=116>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-116 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-RO, cs.RO<br>Keyword Score: 75<br>Keywords: Black Box, Few-shot, Few-shot Learning, Fine-tuning, Reasoning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09546v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09546v1.pdf filename=2402.09546v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the field of robotics and automation, navigation systems based on <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have recently shown impressive performance. However, the security aspects of these systems have received relatively less attention. This paper pioneers the exploration of vulnerabilities in <b>LLM-based</b> navigation models in urban outdoor environments, a critical area given the technology&rsquo;s widespread application in autonomous driving, logistics, and emergency services. Specifically, we introduce a novel Navigational <b>Prompt</b> Suffix (NPS) Attack that manipulates <b>LLM-based</b> navigation models by appending gradient-derived suffixes to the original navigational <b>prompt,</b> leading to incorrect actions. We conducted comprehensive experiments on an <b>LLMs-based</b> navigation model that employs various <b>LLMs</b> for <b>reasoning.</b> Our results, derived from the Touchdown and Map2Seq street-view datasets under both <b>few-shot</b> <b>learning</b> and <b>fine-tuning</b> configurations, demonstrate notable performance declines across three metrics in the face of both white-box and <b>black-box</b> <b>attacks.</b> These results highlight the generalizability and transferability of the NPS Attack, emphasizing the need for enhanced security in <b>LLM-based</b> navigation systems. As an initial countermeasure, we propose the Navigational <b>Prompt</b> Engineering (NPE) Defense strategy, concentrating on navigation-relevant keywords to reduce the impact of adversarial suffixes. While initial findings indicate that this strategy enhances navigational safety, there remains a critical need for the wider research community to develop stronger defense methods to effectively tackle the real-world challenges faced by these systems.</p></p class="citation"></blockquote><h3 id=210--117261-a-digital-twin-prototype-for-traffic-sign-recognition-of-a-learning-enabled-autonomous-vehicle-mohamed-abdelsalam-et-al-2024>(2/10 | 117/261) A Digital Twin prototype for traffic sign recognition of a learning-enabled autonomous vehicle (Mohamed AbdElSalam et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mohamed AbdElSalam, Loai Ali, Saddek Bensalem, Weicheng He, Panagiotis Katsaros, Nikolaos Kekatos, Doron Peled, Anastasios Temperekidis, Changshun Wu. (2024)<br><strong>A Digital Twin prototype for traffic sign recognition of a learning-enabled autonomous vehicle</strong><br><button class=copy-to-clipboard title="A Digital Twin prototype for traffic sign recognition of a learning-enabled autonomous vehicle" index=117>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-117 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-RO, cs-SY, cs.RO, eess-SY<br>Keyword Score: 30<br>Keywords: Recommendation, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09097v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09097v1.pdf filename=2402.09097v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we present a novel digital twin prototype for a learning-enabled self-driving vehicle. The primary objective of this digital twin is to perform traffic sign recognition and lane keeping. The digital twin architecture relies on co-simulation and uses the Functional Mock-up Interface and SystemC Transaction Level Modeling standards. The digital twin consists of four clients, i) a vehicle model that is designed in Amesim tool, ii) an environment model developed in Prescan, iii) a lane-keeping controller designed in Robot Operating System, and iv) a perception and speed control module developed in the formal modeling language of BIP (Behavior, Interaction, Priority). These clients interface with the digital twin platform, PAVE360-Veloce System Interconnect (PAVE360-VSI). PAVE360-VSI acts as the co-simulation orchestrator and is responsible for synchronization, interconnection, and data exchange through a server. The server establishes connections among the different clients and also ensures adherence to the Ethernet protocol. We conclude with illustrative digital twin <b>simulations</b> and <b>recommendations</b> for future work.</p></p class="citation"></blockquote><h3 id=310--118261-design-and-realization-of-a-benchmarking-testbed-for-evaluating-autonomous-platooning-algorithms-michael-shaham-et-al-2024>(3/10 | 118/261) Design and Realization of a Benchmarking Testbed for Evaluating Autonomous Platooning Algorithms (Michael Shaham et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Michael Shaham, Risha Ranjan, Engin Kirda, Taskin Padir. (2024)<br><strong>Design and Realization of a Benchmarking Testbed for Evaluating Autonomous Platooning Algorithms</strong><br><button class=copy-to-clipboard title="Design and Realization of a Benchmarking Testbed for Evaluating Autonomous Platooning Algorithms" index=118>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-118 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-MA, cs-RO, cs-SY, cs.RO, eess-SY, math-OC<br>Keyword Score: 26<br>Keywords: Benchmarking, Benchmarking, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09233v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09233v1.pdf filename=2402.09233v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Autonomous vehicle platoons present near- and long-term opportunities to enhance operational efficiencies and save lives. The past 30 years have seen rapid development in the autonomous driving space, enabling new technologies that will alleviate the strain placed on human drivers and reduce vehicle emissions. This paper introduces a testbed for evaluating and <b>benchmarking</b> platooning algorithms on 1/10th scale vehicles with onboard sensors. To demonstrate the testbed&rsquo;s utility, we evaluate three algorithms, linear feedback and two variations of distributed model predictive control, and compare their results on a typical platooning scenario where the lead vehicle tracks a reference trajectory that changes speed multiple times. We validate our algorithms in <b>simulation</b> to analyze the performance as the platoon size increases, and find that the distributed model predictive control algorithms outperform linear feedback on hardware and in <b>simulation.</b></p></p class="citation"></blockquote><h3 id=410--119261-disgnet-a-distance-graph-neural-network-for-forward-kinematics-learning-of-gough-stewart-platform-huizhi-zhu-et-al-2024>(4/10 | 119/261) DisGNet: A Distance Graph Neural Network for Forward Kinematics Learning of Gough-Stewart Platform (Huizhi Zhu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Huizhi Zhu, Wenxia Xu, Jian Huang, Jiaxin Li. (2024)<br><strong>DisGNet: A Distance Graph Neural Network for Forward Kinematics Learning of Gough-Stewart Platform</strong><br><button class=copy-to-clipboard title="DisGNet: A Distance Graph Neural Network for Forward Kinematics Learning of Gough-Stewart Platform" index=119>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-119 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-LG, cs-RO, cs.RO<br>Keyword Score: 23<br>Keywords: Message-Passing, Graph, Graph Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09077v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09077v1.pdf filename=2402.09077v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we propose a <b>graph</b> <b>neural</b> <b>network,</b> DisGNet, for learning the <b>graph</b> <b>distance</b> <b>matrix</b> to address the forward kinematics problem of the Gough-Stewart platform. DisGNet employs the k-FWL algorithm for <b>message-passing,</b> providing high expressiveness with a small parameter count, making it suitable for practical deployment. Additionally, we introduce the GPU-friendly Newton-Raphson method, an efficient parallelized optimization method executed on the GPU to refine DisGNet&rsquo;s output poses, achieving ultra-high-precision pose. This novel two-stage approach delivers ultra-high precision output while meeting real-time requirements. Our results indicate that on our dataset, DisGNet can achieves error accuracys below 1mm and 1deg at 79.8% and 98.2%, respectively. As executed on a GPU, our two-stage method can ensure the requirement for real-time computation. Codes are released at <a href=https://github.com/FLAMEZZ5201/DisGNet>https://github.com/FLAMEZZ5201/DisGNet</a>.</p></p class="citation"></blockquote><h3 id=510--120261-auto-encoding-bayesian-inverse-games-xinjie-liu-et-al-2024>(5/10 | 120/261) Auto-Encoding Bayesian Inverse Games (Xinjie Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xinjie Liu, Lasse Peters, Javier Alonso-Mora, Ufuk Topcu, David Fridovich-Keil. (2024)<br><strong>Auto-Encoding Bayesian Inverse Games</strong><br><button class=copy-to-clipboard title="Auto-Encoding Bayesian Inverse Games" index=120>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-120 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-GT, cs-LG, cs-MA, cs-RO, cs-SY, cs.RO, eess-SY<br>Keyword Score: 23<br>Keywords: Autoencoder, Multi-modal, Variational Autoencoder<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08902v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08902v2.pdf filename=2402.08902v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>When multiple agents interact in a common environment, each agent&rsquo;s actions impact others&rsquo; future decisions, and noncooperative dynamic games naturally capture this coupling. In interactive motion planning, however, agents typically do not have access to a complete model of the game, e.g., due to unknown objectives of other players. Therefore, we consider the inverse game problem, in which some properties of the game are unknown a priori and must be inferred from observations. Existing maximum likelihood estimation (MLE) approaches to solve inverse games provide only point estimates of unknown parameters without quantifying uncertainty, and perform poorly when many parameter values explain the observed behavior. To address these limitations, we take a Bayesian perspective and construct posterior distributions of game parameters. To render inference tractable, we employ a <b>variational</b> <b>autoencoder</b> (VAE) with an embedded differentiable game solver. This structured VAE can be trained from an unlabeled dataset of observed interactions, naturally handles continuous, <b>multi-modal</b> distributions, and supports efficient sampling from the inferred posteriors without computing game solutions at runtime. Extensive evaluations in simulated driving scenarios demonstrate that the proposed approach successfully learns the prior and posterior objective distributions, provides more accurate objective estimates than MLE baselines, and facilitates safer and more efficient game-theoretic motion planning.</p></p class="citation"></blockquote><h3 id=610--121261-safe-distributed-control-of-multi-robot-systems-with-communication-delays-luca-ballotta-et-al-2024>(6/10 | 121/261) Safe Distributed Control of Multi-Robot Systems with Communication Delays (Luca Ballotta et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Luca Ballotta, Rajat Talak. (2024)<br><strong>Safe Distributed Control of Multi-Robot Systems with Communication Delays</strong><br><button class=copy-to-clipboard title="Safe Distributed Control of Multi-Robot Systems with Communication Delays" index=121>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-121 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: 68T40 (Primary), 68M14, 68T07, 68W15 (Secondary), C-2-4; I-2-6; I-2-9; I-2-11; J-2, cs-RO, cs.RO<br>Keyword Score: 13<br>Keywords: Graph, Graph Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09382v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09382v1.pdf filename=2402.09382v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Safe operation of multi-robot systems is critical, especially in communication-degraded environments such as underwater for seabed mapping, underground caves for navigation, and in extraterrestrial missions for assembly and construction. We address safety of networked autonomous systems where the information exchanged between robots incurs communication delays. We formalize a notion of distributed control barrier function (CBF) for multi-robot systems, a safety certificate amenable to a distributed implementation, which provides formal ground to using <b>graph</b> <b>neural</b> <b>networks</b> to learn safe distributed controllers. Further, we observe that learning a distributed controller ignoring delays can severely degrade safety. Our main contribution is a predictor-based framework to train a safe distributed controller under communication delays, where the current state of nearby robots is predicted from received data and age-of-information. Numerical experiments on multi-robot collision avoidance show that our predictor-based approach can significantly improve the safety of a learned distributed controller under communication delays</p></p class="citation"></blockquote><h3 id=710--122261-single-reset-divide--conquer-imitation-learning-alexandre-chenu-et-al-2024>(7/10 | 122/261) Single-Reset Divide & Conquer Imitation Learning (Alexandre Chenu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Alexandre Chenu, Olivier Serris, Olivier Sigaud, Nicolas Perrin-Gilbert. (2024)<br><strong>Single-Reset Divide & Conquer Imitation Learning</strong><br><button class=copy-to-clipboard title="Single-Reset Divide & Conquer Imitation Learning" index=122>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-122 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-RO, cs.RO<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09355v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09355v1.pdf filename=2402.09355v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Demonstrations are commonly used to speed up the learning process of Deep <b>Reinforcement</b> <b>Learning</b> algorithms. To cope with the difficulty of accessing multiple demonstrations, some algorithms have been developed to learn from a single demonstration. In particular, the Divide & Conquer Imitation Learning algorithms leverage a sequential bias to learn a control policy for complex robotic tasks using a single state-based demonstration. The latest version, DCIL-II demonstrates remarkable sample efficiency. This novel method operates within an extended Goal-Conditioned <b>Reinforcement</b> <b>Learning</b> framework, ensuring compatibility between intermediate and subsequent goals extracted from the demonstration. However, a fundamental limitation arises from the assumption that the system can be reset to specific states along the demonstrated trajectory, confining the application to simulated systems. In response, we introduce an extension called Single-Reset DCIL (SR-DCIL), designed to overcome this constraint by relying on a single initial state reset rather than sequential resets. To address this more challenging setting, we integrate two mechanisms inspired by the Learning from Demonstrations literature, including a Demo-Buffer and Value Cloning, to guide the agent toward compatible success states. In addition, we introduce Approximate Goal Switching to facilitate training to reach goals distant from the reset state. Our paper makes several contributions, highlighting the importance of the reset assumption in DCIL-II, presenting the mechanisms of SR-DCIL variants and evaluating their performance in challenging robotic tasks compared to DCIL-II. In summary, this work offers insights into the significance of reset assumptions in the framework of DCIL and proposes SR-DCIL, a first step toward a versatile algorithm capable of learning control policies under a weaker reset assumption.</p></p class="citation"></blockquote><h3 id=810--123261-traj-lio-a-resilient-multi-lidar-multi-imu-state-estimator-through-sparse-gaussian-process-xin-zheng-et-al-2024>(8/10 | 123/261) Traj-LIO: A Resilient Multi-LiDAR Multi-IMU State Estimator Through Sparse Gaussian Process (Xin Zheng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xin Zheng, Jianke Zhu. (2024)<br><strong>Traj-LIO: A Resilient Multi-LiDAR Multi-IMU State Estimator Through Sparse Gaussian Process</strong><br><button class=copy-to-clipboard title="Traj-LIO: A Resilient Multi-LiDAR Multi-IMU State Estimator Through Sparse Gaussian Process" index=123>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-123 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-CV, cs-RO, cs.RO<br>Keyword Score: 10<br>Keywords: Gaussian Process<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09189v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09189v1.pdf filename=2402.09189v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Nowadays, sensor suits have been equipped with redundant LiDARs and IMUs to mitigate the risks associated with sensor failure. It is challenging for the previous discrete-time and IMU-driven kinematic systems to incorporate multiple asynchronized sensors, which are susceptible to abnormal IMU data. To address these limitations, we introduce a multi-LiDAR multi-IMU state estimator by taking advantage of <b>Gaussian</b> <b>Process</b> (GP) that predicts a non-parametric continuous-time trajectory to capture sensors&rsquo; spatial-temporal movement with limited control states. Since the kinematic model driven by three types of linear time-invariant stochastic differential equations are independent of external sensor measurements, our proposed approach is capable of handling different sensor configurations and resilient to sensor failures. Moreover, we replace the conventional $\mathrm{SE}(3)$ state representation with the combination of $\mathrm{SO}(3)$ and vector space, which enables GP-based LiDAR-inertial system to fulfill the real-time requirement. Extensive experiments on the public datasets demonstrate the versatility and resilience of our proposed multi-LiDAR multi-IMU state estimator. To contribute to the community, we will make our source code publicly available.</p></p class="citation"></blockquote><h3 id=910--124261-awareness-in-robotics-an-early-perspective-from-the-viewpoint-of-the-eic-pathfinder-challenge-awareness-inside-cosimo-della-santina-et-al-2024>(9/10 | 124/261) Awareness in robotics: An early perspective from the viewpoint of the EIC Pathfinder Challenge &lsquo;Awareness Inside&rsquo;&rsquo; (Cosimo Della Santina et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Cosimo Della Santina, Carlos Hernandez Corbato, Burak Sisman, Luis A. Leiva, Ioannis Arapakis, Michalis Vakalellis, Jean Vanderdonckt, Luis Fernando D&rsquo;Haro, Guido Manzi, Cristina Becchio, Aïda Elamrani, Mohsen Alirezaei, Ginevra Castellano, Dimos V. Dimarogonas, Arabinda Ghosh, Sofie Haesaert, Sadegh Soudjani, Sybert Stroeve, Paul Verschure, Davide Bacciu, Ophelia Deroy, Bahador Bahrami, Claudio Gallicchio, Sabine Hauert, Ricardo Sanz, Pablo Lanillos, Giovanni Iacca, Stephan Sigg, Manel Gasulla, Luc Steels, Carles Sierra. (2024)<br><strong>Awareness in robotics: An early perspective from the viewpoint of the EIC Pathfinder Challenge &lsquo;Awareness Inside&rsquo;&rsquo;</strong><br><button class=copy-to-clipboard title="Awareness in robotics: An early perspective from the viewpoint of the EIC Pathfinder Challenge 'Awareness Inside''" index=124>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-124 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 10<br>Keywords: Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09030v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09030v1.pdf filename=2402.09030v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Consciousness has been historically a heavily debated topic in engineering, science, and philosophy. On the contrary, awareness had less success in raising the interest of scholars in the past. However, things are changing as more and more researchers are getting interested in answering questions concerning what awareness is and how it can be artificially generated. The landscape is rapidly evolving, with multiple voices and interpretations of the concept being conceived and techniques being developed. The goal of this paper is to <b>summarize</b> and discuss the ones among these voices connected with projects funded by the EIC Pathfinder Challenge called ``Awareness Inside&rsquo;&rsquo;, a nonrecurring call for proposals within Horizon Europe designed specifically for fostering research on natural and synthetic awareness. In this perspective, we dedicate special attention to challenges and promises of applying synthetic awareness in robotics, as the development of mature techniques in this new field is expected to have a special impact on generating more capable and trustworthy embodied systems.</p></p class="citation"></blockquote><h3 id=1010--125261-multi-task-learning-of-active-fault-tolerant-controller-for-leg-failures-in-quadruped-robots-taixian-hou-et-al-2024>(10/10 | 125/261) Multi-Task Learning of Active Fault-Tolerant Controller for Leg Failures in Quadruped robots (Taixian Hou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Taixian Hou, Jiaxin Tu, Xiaofei Gao, Zhiyan Dong, Peng Zhai, Lihua Zhang. (2024)<br><strong>Multi-Task Learning of Active Fault-Tolerant Controller for Leg Failures in Quadruped robots</strong><br><button class=copy-to-clipboard title="Multi-Task Learning of Active Fault-Tolerant Controller for Leg Failures in Quadruped robots" index=125>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-125 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 10<br>Keywords: Zero-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08996v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08996v1.pdf filename=2402.08996v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Electric quadruped robots used in outdoor exploration are susceptible to leg-related electrical or mechanical failures. Unexpected joint power loss and joint locking can immediately pose a falling threat. Typically, controllers lack the capability to actively sense the condition of their own joints and take proactive actions. Maintaining the original motion patterns could lead to disastrous consequences, as the controller may produce irrational output within a short period of time, further creating the risk of serious physical injuries. This paper presents a hierarchical fault-tolerant control scheme employing a multi-task training architecture capable of actively perceiving and overcoming two types of leg joint faults. The architecture simultaneously trains three joint task policies for health, power loss, and locking scenarios in parallel, introducing a symmetric reflection initialization technique to ensure rapid and stable gait skill transformations. Experiments demonstrate that the control scheme is robust in unexpected scenarios where a single leg experiences concurrent joint faults in two joints. Furthermore, the policy retains the robot&rsquo;s planar mobility, enabling rough velocity tracking. Finally, <b>zero-shot</b> Sim2Real transfer is achieved on the real-world SOLO8 robot, countering both electrical and mechanical failures.</p></p class="citation"></blockquote><h2 id=csce-1>cs.CE (1)</h2><h3 id=11--126261-multiscale-graph-neural-networks-with-adaptive-mesh-refinement-for-accelerating-mesh-based-simulations-roberto-perera-et-al-2024>(1/1 | 126/261) Multiscale graph neural networks with adaptive mesh refinement for accelerating mesh-based simulations (Roberto Perera et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Roberto Perera, Vinamra Agrawal. (2024)<br><strong>Multiscale graph neural networks with adaptive mesh refinement for accelerating mesh-based simulations</strong><br><button class=copy-to-clipboard title="Multiscale graph neural networks with adaptive mesh refinement for accelerating mesh-based simulations" index=126>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-126 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CE<br>Categories: cs-CE, cs.CE<br>Keyword Score: 73<br>Keywords: Message-Passing, Graph, Graph Neural Network, Graph Neural Network, Simulation, Simulator, Transfer Learning, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08863v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08863v1.pdf filename=2402.08863v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Mesh-based <b>Graph</b> <b>Neural</b> <b>Networks</b> <b>(GNNs)</b> have recently shown capabilities to simulate complex multiphysics problems with accelerated performance times. However, mesh-based <b>GNNs</b> require a large number of <b>message-passing</b> (MP) steps and suffer from over-smoothing for problems involving very fine mesh. In this work, we develop a multiscale mesh-based <b>GNN</b> framework mimicking a conventional iterative multigrid solver, coupled with adaptive mesh refinement (AMR), to mitigate challenges with conventional mesh-based <b>GNNs.</b> We use the framework to accelerate phase field (PF) fracture problems involving coupled partial differential equations with a near-singular operator due to near-zero modulus inside the crack. We define the initial <b>graph</b> <b>representation</b> <b>using</b> all mesh resolution levels. We perform a series of downsampling steps using <b>Transformer</b> MP <b>GNNs</b> to reach the coarsest <b>graph</b> <b>followed</b> <b>by</b> upsampling steps to reach the original <b>graph.</b> <b>We</b> <b>use</b> skip connectors from the generated embedding during coarsening to prevent over-smoothing. We use <b>Transfer</b> <b>Learning</b> (TL) to significantly reduce the size of training datasets needed to simulate different crack configurations and loading conditions. The trained framework showed accelerated <b>simulation</b> times, while maintaining high accuracy for all cases compared to physics-based PF fracture model. Finally, this work provides a new approach to accelerate a variety of mesh-based engineering multiphysics problems</p></p class="citation"></blockquote><h2 id=cssd-3>cs.SD (3)</h2><h3 id=13--127261-domain-adaptation-for-contrastive-audio-language-models-soham-deshmukh-et-al-2024>(1/3 | 127/261) Domain Adaptation for Contrastive Audio-Language Models (Soham Deshmukh et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Soham Deshmukh, Rita Singh, Bhiksha Raj. (2024)<br><strong>Domain Adaptation for Contrastive Audio-Language Models</strong><br><button class=copy-to-clipboard title="Domain Adaptation for Contrastive Audio-Language Models" index=127>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-127 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SD<br>Categories: cs-SD, cs.SD, eess-AS<br>Keyword Score: 70<br>Keywords: Few-shot, Few-shot Learning, Fine-tuning, Out-of-distribution, Zero-shot, Domain Adaptation, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09585v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09585v1.pdf filename=2402.09585v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Audio-Language Models (ALM) aim to be general-purpose audio models by providing <b>zero-shot</b> capabilities at test time. The <b>zero-shot</b> performance of ALM improves by using suitable text <b>prompts</b> for each <b>domain.</b> <b>The</b> text <b>prompts</b> are usually hand-crafted through an ad-hoc process and lead to a drop in ALM generalization and <b>out-of-distribution</b> performance. Existing approaches to improve <b>domain</b> <b>performance,</b> like <b>few-shot</b> <b>learning</b> or <b>fine-tuning,</b> require access to annotated data and iterations of training. Therefore, we propose a test-time <b>domain</b> <b>adaptation</b> method for ALMs that does not require access to annotations. Our method learns a <b>domain</b> <b>vector</b> by enforcing consistency across augmented views of the testing audio. We extensively evaluate our approach on 12 downstream tasks across <b>domains.</b> <b>With</b> just one example, our <b>domain</b> <b>adaptation</b> method leads to 3.2% (max 8.4%) average <b>zero-shot</b> performance improvement. After adaptation, the model still retains the generalization property of ALMs.</p></p class="citation"></blockquote><h3 id=23--128261-arrange-inpaint-and-refine-steerable-long-term-music-audio-generation-and-editing-via-content-based-controls-liwei-lin-et-al-2024>(2/3 | 128/261) Arrange, Inpaint, and Refine: Steerable Long-term Music Audio Generation and Editing via Content-based Controls (Liwei Lin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Liwei Lin, Gus Xia, Yixiao Zhang, Junyan Jiang. (2024)<br><strong>Arrange, Inpaint, and Refine: Steerable Long-term Music Audio Generation and Editing via Content-based Controls</strong><br><button class=copy-to-clipboard title="Arrange, Inpaint, and Refine: Steerable Long-term Music Audio Generation and Editing via Content-based Controls" index=128>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-128 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SD<br>Categories: cs-AI, cs-SD, cs.SD, eess-AS<br>Keyword Score: 40<br>Keywords: Fine-tuning, Fine-tuning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09508v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09508v1.pdf filename=2402.09508v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Controllable music generation plays a vital role in human-AI music co-creation. While <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have shown promise in generating high-quality music, their focus on autoregressive generation limits their utility in music editing tasks. To bridge this gap, we introduce a novel Parameter-Efficient <b>Fine-Tuning</b> (PEFT) method. This approach enables autoregressive language models to seamlessly address music inpainting tasks. Additionally, our PEFT method integrates frame-level content-based controls, facilitating track-conditioned music refinement and score-conditioned music arrangement. We apply this method to <b>fine-tune</b> MusicGen, a leading autoregressive music generation model. Our experiments demonstrate promising results across multiple music editing tasks, offering more flexible controls for future AI-driven music editing tools. A demo page\footnote{\url{https://kikyo-16.github.io/AIR/}.} showcasing our work and source codes\footnote{\url{https://github.com/Kikyo-16/airgen}.} are available online.</p></p class="citation"></blockquote><h3 id=33--129261-leveraging-pre-trained-autoencoders-for-interpretable-prototype-learning-of-music-audio-pablo-alonso-jiménez-et-al-2024>(3/3 | 129/261) Leveraging Pre-Trained Autoencoders for Interpretable Prototype Learning of Music Audio (Pablo Alonso-Jiménez et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Pablo Alonso-Jiménez, Leonardo Pepino, Roser Batlle-Roca, Pablo Zinemanas, Dmitry Bogdanov, Xavier Serra, Martín Rocamora. (2024)<br><strong>Leveraging Pre-Trained Autoencoders for Interpretable Prototype Learning of Music Audio</strong><br><button class=copy-to-clipboard title="Leveraging Pre-Trained Autoencoders for Interpretable Prototype Learning of Music Audio" index=129>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-129 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SD<br>Categories: cs-AI, cs-MM, cs-SD, cs.SD, eess-AS<br>Keyword Score: 20<br>Keywords: Autoencoder, Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09318v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09318v1.pdf filename=2402.09318v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present PECMAE, an interpretable model for music audio classification based on prototype learning. Our model is based on a previous method, APNet, which jointly learns an <b>autoencoder</b> and a prototypical network. Instead, we propose to decouple both training processes. This enables us to leverage existing <b>self-supervised</b> <b>autoencoders</b> pre-trained on much larger data (EnCodecMAE), providing representations with better generalization. APNet allows prototypes&rsquo; reconstruction to waveforms for interpretability relying on the nearest training data samples. In contrast, we explore using a diffusion decoder that allows reconstruction without such dependency. We evaluate our method on datasets for music instrument classification (Medley-Solos-DB) and genre recognition (GTZAN and a larger in-house dataset), the latter being a more challenging task not addressed with prototypical networks before. We find that the prototype-based models preserve most of the performance achieved with the <b>autoencoder</b> embeddings, while the sonification of prototypes benefits understanding the behavior of the classifier.</p></p class="citation"></blockquote><h2 id=cshc-6>cs.HC (6)</h2><h3 id=16--130261-advancing-building-energy-modeling-with-large-language-models-exploration-and-case-studies-liang-zhang-et-al-2024>(1/6 | 130/261) Advancing Building Energy Modeling with Large Language Models: Exploration and Case Studies (Liang Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Liang Zhang, Zhelun Chen, Vitaly Ford. (2024)<br><strong>Advancing Building Energy Modeling with Large Language Models: Exploration and Case Studies</strong><br><button class=copy-to-clipboard title="Advancing Building Energy Modeling with Large Language Models: Exploration and Case Studies" index=130>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-130 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-AI, cs-HC, cs.HC<br>Keyword Score: 70<br>Keywords: Retrieval-Augmented Generation, Retrieval-Augmented Generation, Simulation, Simulator, ChatGPT, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09579v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09579v1.pdf filename=2402.09579v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The rapid progression in artificial intelligence has facilitated the emergence of <b>large</b> <b>language</b> <b>models</b> like <b>ChatGPT,</b> offering potential applications extending into specialized engineering modeling, especially physics-based building energy modeling. This paper investigates the innovative integration of <b>large</b> <b>language</b> <b>models</b> with building energy modeling software, focusing specifically on the fusion of <b>ChatGPT</b> with EnergyPlus. A literature review is first conducted to reveal a growing trend of incorporating of <b>large</b> <b>language</b> <b>models</b> in engineering modeling, albeit limited research on their application in building energy modeling. We underscore the potential of <b>large</b> <b>language</b> <b>models</b> in addressing building energy modeling challenges and outline potential applications including 1) <b>simulation</b> input generation, 2) <b>simulation</b> output analysis and visualization, 3) conducting error analysis, 4) co-simulation, 5) <b>simulation</b> knowledge extraction and training, and 6) <b>simulation</b> optimization. Three case studies reveal the transformative potential of <b>large</b> <b>language</b> <b>models</b> in automating and optimizing building energy modeling tasks, underscoring the pivotal role of artificial intelligence in advancing sustainable building practices and energy efficiency. The case studies demonstrate that selecting the right <b>large</b> <b>language</b> <b>model</b> techniques is essential to enhance performance and reduce engineering efforts. Besides direct use of <b>large</b> <b>language</b> <b>models,</b> three specific techniques were utilized: 1) <b>prompt</b> engineering, 2) <b>retrieval-augmented</b> <b>generation,</b> <b>and</b> 3) multi-agent <b>large</b> <b>language</b> <b>models.</b> The findings advocate a multidisciplinary approach in future artificial intelligence research, with implications extending beyond building energy modeling to other specialized engineering modeling.</p></p class="citation"></blockquote><h3 id=26--131261-evaluating-the-experience-of-lgbtq-people-using-large-language-model-based-chatbots-for-mental-health-support-zilin-ma-et-al-2024>(2/6 | 131/261) Evaluating the Experience of LGBTQ+ People Using Large Language Model Based Chatbots for Mental Health Support (Zilin Ma et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zilin Ma, Yiyang Mei, Yinru Long, Zhaoyuan Su, Krzysztof Z. Gajos. (2024)<br><strong>Evaluating the Experience of LGBTQ+ People Using Large Language Model Based Chatbots for Mental Health Support</strong><br><button class=copy-to-clipboard title="Evaluating the Experience of LGBTQ+ People Using Large Language Model Based Chatbots for Mental Health Support" index=131>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-131 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-HC, cs.HC<br>Keyword Score: 50<br>Keywords: Fine-tuning, Chatbot, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09260v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09260v1.pdf filename=2402.09260v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>LGBTQ+ individuals are increasingly turning to <b>chatbots</b> powered by <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> to meet their mental health needs. However, little research has explored whether these <b>chatbots</b> can adequately and safely provide tailored support for this demographic. We interviewed 18 LGBTQ+ and 13 non-LGBTQ+ participants about their experiences with <b>LLM-based</b> <b>chatbots</b> for mental health needs. LGBTQ+ participants relied on these <b>chatbots</b> for mental health support, likely due to an absence of support in real life. Notably, while <b>LLMs</b> offer <b>prompt</b> support, they frequently fall short in grasping the nuances of LGBTQ-specific challenges. Although <b>fine-tuning</b> <b>LLMs</b> to address LGBTQ+ needs can be a step in the right direction, it isn&rsquo;t the panacea. The deeper issue is entrenched in societal discrimination. Consequently, we call on future researchers and designers to look beyond mere technical refinements and advocate for holistic strategies that confront and counteract the societal biases burdening the LGBTQ+ community.</p></p class="citation"></blockquote><h3 id=36--132261-agentlens-visual-analysis-for-agent-behaviors-in-llm-based-autonomous-systems-jiaying-lu-et-al-2024>(3/6 | 132/261) AgentLens: Visual Analysis for Agent Behaviors in LLM-based Autonomous Systems (Jiaying Lu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiaying Lu, Bo Pan, Jieyi Chen, Yingchaojie Feng, Jingyuan Hu, Yuchen Peng, Wei Chen. (2024)<br><strong>AgentLens: Visual Analysis for Agent Behaviors in LLM-based Autonomous Systems</strong><br><button class=copy-to-clipboard title="AgentLens: Visual Analysis for Agent Behaviors in LLM-based Autonomous Systems" index=132>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-132 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-AI, cs-HC, cs.HC<br>Keyword Score: 30<br>Keywords: Large Language Model, Large Language Model, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08995v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08995v1.pdf filename=2402.08995v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recently, <b>Large</b> <b>Language</b> <b>Model</b> based Autonomous system(LLMAS) has gained great popularity for its potential to simulate complicated behaviors of human societies. One of its main challenges is to present and analyze the dynamic events evolution of LLMAS. In this work, we present a visualization approach to explore detailed statuses and agents&rsquo; behavior within LLMAS. We propose a general pipeline that establishes a behavior structure from raw LLMAS execution events, leverages a behavior <b>summarization</b> algorithm to construct a hierarchical summary of the entire structure in terms of time sequence, and a cause trace method to mine the causal relationship between agent behaviors. We then develop AgentLens, a visual analysis system that leverages a hierarchical temporal visualization for illustrating the evolution of LLMAS, and supports users to interactively investigate details and causes of agents&rsquo; behaviors. Two usage scenarios and a user study demonstrate the effectiveness and usability of our AgentLens.</p></p class="citation"></blockquote><h3 id=46--133261-can-ai-and-humans-genuinely-communicate-constant-bonard-2024>(4/6 | 133/261) Can AI and humans genuinely communicate? (Constant Bonard, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Constant Bonard. (2024)<br><strong>Can AI and humans genuinely communicate?</strong><br><button class=copy-to-clipboard title="Can AI and humans genuinely communicate?" index=133>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-133 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-AI, cs-HC, cs.HC<br>Keyword Score: 5<br>Keywords: Black Box<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09494v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09494v1.pdf filename=2402.09494v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Can AI and humans genuinely communicate? In this article, after giving some background and motivating my proposal (sections 1 to 3), I explore a way to answer this question that I call the &ldquo;mental-behavioral methodology&rdquo; (sections 4 and 5). This methodology follows the following three steps: First, spell out what mental capacities are sufficient for human communication (as opposed to communication more generally). Second, spell out the experimental paradigms required to test whether a behavior exhibits these capacities. Third, apply or adapt these paradigms to test whether an AI displays the relevant behaviors. If the first two steps are successfully completed, and if the AI passes the tests with human-like results, this constitutes evidence that this AI and humans can genuinely communicate. This mental-behavioral methodology has the advantage that we don&rsquo;t need to understand the workings of <b>black-box</b> <b>algorithms,</b> such as standard deep neural networks. This is comparable to the fact that we don&rsquo;t need to understand how human brains work to know that humans can genuinely communicate. This methodology also has its disadvantages and I will discuss some of them (section 6).</p></p class="citation"></blockquote><h3 id=56--134261-visualization-requirements-for-business-intelligence-analytics-a-goal-based-iterative-framework-ana-lavalle-et-al-2024>(5/6 | 134/261) Visualization Requirements for Business Intelligence Analytics: A Goal-Based, Iterative Framework (Ana Lavalle et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ana Lavalle, Alejandro Maté, Juan Trujillo, Stefano Rizzi. (2024)<br><strong>Visualization Requirements for Business Intelligence Analytics: A Goal-Based, Iterative Framework</strong><br><button class=copy-to-clipboard title="Visualization Requirements for Business Intelligence Analytics: A Goal-Based, Iterative Framework" index=134>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-134 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-HC, cs.HC<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09491v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09491v1.pdf filename=2402.09491v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Information visualization plays a key role in business intelligence analytics. With ever larger amounts of data that need to be interpreted, using the right visualizations is crucial in order to understand the underlying patterns and results obtained by analysis algorithms. Despite its importance, defining the right visualization is still a challenging task. Business users are rarely experts in information visualization, and they may not exactly know the most adequate visualization tools or patterns for their goals. Consequently, misinterpreted <b>graphs</b> and wrong results can be obtained, leading to missed opportunities and significant losses for companies. The main problem underneath is a lack of tools and methodologies that allow non-expert users to define their visualization and data analysis goals in business terms. In order to tackle this problem, we present an iterative goal-oriented approach based on the i* language for the automatic derivation of data visualizations. Our approach links non-expert user requirements to the data to be analyzed, choosing the most suited visualization techniques in a semi-automatic way. The great advantage of our proposal is that we provide non-expert users with the best suited visualizations according to their information needs and their data with little effort and without requiring expertise in information visualization.</p></p class="citation"></blockquote><h3 id=66--135261-prismatic-interactive-multi-view-cluster-analysis-of-concept-stocks-wong-kam-kwai-et-al-2024>(6/6 | 135/261) Prismatic: Interactive Multi-View Cluster Analysis of Concept Stocks (Wong Kam-Kwai et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wong Kam-Kwai, Yan Luo, Xuanwu Yue, Wei Chen, Huamin Qu. (2024)<br><strong>Prismatic: Interactive Multi-View Cluster Analysis of Concept Stocks</strong><br><button class=copy-to-clipboard title="Prismatic: Interactive Multi-View Cluster Analysis of Concept Stocks" index=135>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-135 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-CE, cs-HC, cs-LG, cs.HC<br>Keyword Score: 3<br>Keywords: Clustering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08978v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08978v1.pdf filename=2402.08978v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Financial cluster analysis allows investors to discover investment alternatives and avoid undertaking excessive risks. However, this analytical task faces substantial challenges arising from many pairwise comparisons, the dynamic correlations across time spans, and the ambiguity in deriving implications from business relational knowledge. We propose Prismatic, a visual analytics system that integrates quantitative analysis of historical performance and qualitative analysis of business relational knowledge to cluster correlated businesses interactively. Prismatic features three <b>clustering</b> processes: dynamic cluster generation, knowledge-based cluster exploration, and correlation-based cluster validation. Utilizing a multi-view <b>clustering</b> approach, it enriches data-driven clusters with knowledge-driven similarity, providing a nuanced understanding of business correlations. Through well-coordinated visual views, Prismatic facilitates a comprehensive interpretation of intertwined quantitative and qualitative features, demonstrating its usefulness and effectiveness via case studies on formulating concept stocks and extensive interviews with domain experts.</p></p class="citation"></blockquote><h2 id=csdc-5>cs.DC (5)</h2><h3 id=15--136261-mpirigen-mpi-code-generation-through-domain-specific-language-models-nadav-schneider-et-al-2024>(1/5 | 136/261) MPIrigen: MPI Code Generation through Domain-Specific Language Models (Nadav Schneider et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nadav Schneider, Niranjan Hasabnis, Vy A. Vo, Tal Kadosh, Neva Krien, Mihai Capotă, Abdul Wasay, Guy Tamir, Ted Willke, Nesreen Ahmed, Yuval Pinter, Timothy Mattson, Gal Oren. (2024)<br><strong>MPIrigen: MPI Code Generation through Domain-Specific Language Models</strong><br><button class=copy-to-clipboard title="MPIrigen: MPI Code Generation through Domain-Specific Language Models" index=136>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-136 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DC<br>Categories: cs-AI, cs-CL, cs-DC, cs-LG, cs-SE, cs.DC<br>Keyword Score: 70<br>Keywords: Message-Passing, Fine-tuning, Zero-shot, GPT, GPT-3, GPT-3.5, Code Generation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09126v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09126v1.pdf filename=2402.09126v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The imperative need to scale computation across numerous nodes highlights the significance of efficient parallel computing, particularly in the realm of Message Passing Interface (MPI) integration. The challenging parallel programming task of generating MPI-based parallel programs has remained unexplored. This study first investigates the performance of state-of-the-art language models in generating MPI-based parallel programs. Findings reveal that widely used models such as <b>GPT-3.5</b> and PolyCoder (specialized multi-lingual <b>code</b> <b>models)</b> exhibit notable performance degradation, when generating MPI-based programs compared to general-purpose programs. In contrast, domain-specific models such as MonoCoder, which are pretrained on MPI-related programming languages of C and C++, outperform larger models. Subsequently, we introduce a dedicated downstream task of MPI-based program generation by <b>fine-tuning</b> MonoCoder on HPCorpusMPI. We call the resulting model as MPIrigen. We propose an innovative preprocessing for completion only after observing the whole <b>code,</b> <b>thus</b> enabling better completion with a wider context. Comparative analysis against <b>GPT-3.5</b> <b>zero-shot</b> performance, using a novel HPC-oriented evaluation method, demonstrates that MPIrigen excels in generating accurate MPI functions up to 0.8 accuracy in location and function predictions, and with more than 0.9 accuracy for argument predictions. The success of this tailored solution underscores the importance of domain-specific <b>fine-tuning</b> in optimizing language models for parallel computing <b>code</b> <b>generation,</b> paving the way for a new generation of automatic parallelization tools. The sources of this work are available at our GitHub MPIrigen repository: <a href=https://github.com/Scientific-Computing-Lab-NRCN/MPI-rigen>https://github.com/Scientific-Computing-Lab-NRCN/MPI-rigen</a></p></p class="citation"></blockquote><h3 id=25--137261-stabilizing-agreement-is-impossible-in-delayed-message-passing-models-stephan-felber-et-al-2024>(2/5 | 137/261) Stabilizing Agreement is Impossible in Delayed Message Passing Models (Stephan Felber et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Stephan Felber, Hugo Rincon Galeana. (2024)<br><strong>Stabilizing Agreement is Impossible in Delayed Message Passing Models</strong><br><button class=copy-to-clipboard title="Stabilizing Agreement is Impossible in Delayed Message Passing Models" index=137>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-137 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DC<br>Categories: C-2-4; F-2-0; G-2-2, cs-DC, cs.DC<br>Keyword Score: 10<br>Keywords: Message-Passing<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09168v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09168v1.pdf filename=2402.09168v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Most distributed computing research has focused on terminating problems like consensus and similar agreement problems. Non-terminating problems have been studied exhaustively in the context of self-stabilizing distributed algorithms, however, which may start from arbitrary initial states and can tolerate arbitrary transient faults. Somehow in-between is the stabilizing consensus problem, where the processes start from a well-defined initial state but do not need to decide irrevocably and need to agree on a common value only eventually. Charron-Bost and Moran studied stabilizing consensus in synchronous dynamic networks controlled by a message adversary. They introduced the simple and elegant class of min-max algorithms, which allow to solve stabilizing consensus under every message adversary that (i) allows at least one process to reach all other processes infinitely often, and (ii) does so within a bounded (but unknown) number of rounds. Moreover, the authors proved that (i) is a necessary condition. The question whether (i) is also sufficient, i.e., whether (ii) is also necessary, was left open. We answer this question by proving that stabilizing consensus is impossible if (ii) is dropped, i.e., even if some process reaches all other processes infinitely often but only within finite time. We accomplish this by introducing a novel class of arbitrarily delayed message adversaries, which also allows us to establish a connection between terminating task solvability under some message adversary to stabilizing task solvability under the corresponding arbitrarily delayed message adversary. Finally, we outline how to extend this relation to terminating task solvability in asynchronous message passing with guaranteed broadcasts, which highlights the asynchronous characteristics induced by arbitrary delays.</p></p class="citation"></blockquote><h3 id=35--138261-scheduling-for-on-board-federated-learning-with-satellite-clusters-nasrin-razmi-et-al-2024>(3/5 | 138/261) Scheduling for On-Board Federated Learning with Satellite Clusters (Nasrin Razmi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nasrin Razmi, Bho Matthiesen, Armin Dekorsy, Petar Popovski. (2024)<br><strong>Scheduling for On-Board Federated Learning with Satellite Clusters</strong><br><button class=copy-to-clipboard title="Scheduling for On-Board Federated Learning with Satellite Clusters" index=138>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-138 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DC<br>Categories: cs-DC, cs-LG, cs.DC<br>Keyword Score: 10<br>Keywords: Federated Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09105v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09105v1.pdf filename=2402.09105v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Mega-constellations of small satellites have evolved into a source of massive amount of valuable data. To manage this data efficiently, on-board <b>federated</b> <b>learning</b> (FL) enables satellites to train a machine learning (ML) model collaboratively without having to share the raw data. This paper introduces a scheme for scheduling on-board FL for constellations connected with intra-orbit inter-satellite links. The proposed scheme utilizes the predictable visibility pattern between satellites and ground station (GS), both at the individual satellite level and cumulatively within the entire orbit, to mitigate intermittent connectivity and best use of available time. To this end, two distinct schedulers are employed: one for coordinating the FL procedures among orbits, and the other for controlling those within each orbit. These two schedulers cooperatively determine the appropriate time to perform global updates in GS and then allocate suitable duration to satellites within each orbit for local training, proportional to usable time until next global update. This scheme leads to improved test accuracy within a shorter time.</p></p class="citation"></blockquote><h3 id=45--139261-an-evaluative-comparison-of-performance-portability-across-gpu-programming-models-joshua-h-davis-et-al-2024>(4/5 | 139/261) An Evaluative Comparison of Performance Portability across GPU Programming Models (Joshua H. Davis et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Joshua H. Davis, Pranav Sivaraman, Isaac Minn, Konstantinos Parasyris, Harshitha Menon, Giorgis Georgakoudis, Abhinav Bhatele. (2024)<br><strong>An Evaluative Comparison of Performance Portability across GPU Programming Models</strong><br><button class=copy-to-clipboard title="An Evaluative Comparison of Performance Portability across GPU Programming Models" index=139>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-139 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DC<br>Categories: cs-DC, cs-PF, cs.DC<br>Keyword Score: 10<br>Keywords: Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08950v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08950v1.pdf filename=2402.08950v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Ensuring high productivity in scientific software development necessitates developing and maintaining a single codebase that can run efficiently on a range of accelerator-based supercomputing platforms. While prior work has investigated the performance portability of a few selected proxy applications or programming models, this paper provides a comprehensive study of a range of proxy applications implemented in the major programming models suitable for GPU-based platforms. We present and analyze performance results across NVIDIA and AMD GPU hardware currently deployed in leadership-class computing facilities using a representative range of scientific codes and several programming models &ndash; CUDA, HIP, Kokkos, RAJA, OpenMP, OpenACC, and SYCL. Based on the specific characteristics of applications tested, we include <b>recommendations</b> to developers on how to choose the right programming model for their code. We find that Kokkos and RAJA in particular offer the most promise empirically as performance portable programming models. These results provide a comprehensive evaluation of the extent to which each programming model for heterogeneous systems provides true performance portability in real-world usage.</p></p class="citation"></blockquote><h3 id=55--140261-limitless-faas-overcoming-serverless-functions-execution-time-limits-with-invoke-driven-architecture-and-memory-checkpoints-rodrigo-landa-andraca-et-al-2024>(5/5 | 140/261) Limitless FaaS: Overcoming serverless functions execution time limits with invoke driven architecture and memory checkpoints (Rodrigo Landa Andraca et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rodrigo Landa Andraca, Mahdi Zareei. (2024)<br><strong>Limitless FaaS: Overcoming serverless functions execution time limits with invoke driven architecture and memory checkpoints</strong><br><button class=copy-to-clipboard title="Limitless FaaS: Overcoming serverless functions execution time limits with invoke driven architecture and memory checkpoints" index=140>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-140 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DC<br>Categories: C-4, cs-DC, cs.DC<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09377v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09377v1.pdf filename=2402.09377v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Function-as-a-Service (FaaS) allows to directly submit function code to a cloud provider without the burden of managing infrastructure resources. Each cloud provider establishes execution time limits to their FaaS offerings, which impose the risk of spending computation time without achieving partial results. In this work, a framework that enables limitless execution time in FaaS, with little to no modifications to the user-provided function code, is presented. After a thorough literature and theoretical framework review, Apache OpenWhisk Actions and the DMCTP checkpoint-and-restore (CR) tool were selected. With these, dependent successive serverless same-function invocations that exploit the persistence of partial results were implemented. The solution was submitted to the FaaSDom <b>benchmark</b> and time metrics were collected. Additionally, the solution was characterized in terms of the Serverless Trilemma. The resultant system, even at this proof-of-concept state, offers a lot of value to companies that rely heavily on serverless architecture.</p></p class="citation"></blockquote><h2 id=cscv-36>cs.CV (36)</h2><h3 id=136--141261-few-shot-object-detection-with-sparse-context-transformers-jie-mei-et-al-2024>(1/36 | 141/261) Few-Shot Object Detection with Sparse Context Transformers (Jie Mei et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jie Mei, Mingyuan Jiu, Hichem Sahbi, Xiaoheng Jiang, Mingliang Xu. (2024)<br><strong>Few-Shot Object Detection with Sparse Context Transformers</strong><br><button class=copy-to-clipboard title="Few-Shot Object Detection with Sparse Context Transformers" index=141>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-141 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 63<br>Keywords: Object Detection, Benchmarking, Few-shot, Fine-tuning, Fine-tuning, Transfer Learning, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09315v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09315v1.pdf filename=2402.09315v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Few-shot</b> detection is a major task in pattern recognition which seeks to localize <b>objects</b> <b>using</b> models trained with few labeled data. One of the mainstream <b>few-shot</b> methods is <b>transfer</b> <b>learning</b> which consists in pretraining a detection model in a source domain prior to its <b>fine-tuning</b> in a target domain. However, it is challenging for <b>fine-tuned</b> models to effectively identify new classes in the target domain, particularly when the underlying labeled training data are scarce. In this paper, we devise a novel sparse context <b>transformer</b> (SCT) that effectively leverages <b>object</b> <b>knowledge</b> in the source domain, and automatically learns a sparse context from only few training images in the target domain. As a result, it combines different relevant clues in order to enhance the discrimination power of the learned detectors and reduce class confusion. We evaluate the proposed method on two challenging <b>few-shot</b> <b>object</b> <b>detection</b> <b>benchmarks,</b> and empirical results show that the proposed method obtains competitive performance compared to the related state-of-the-art.</p></p class="citation"></blockquote><h3 id=236--142261-reducing-texture-bias-of-deep-neural-networks-via-edge-enhancing-diffusion-edgar-heinert-et-al-2024>(2/36 | 142/261) Reducing Texture Bias of Deep Neural Networks via Edge Enhancing Diffusion (Edgar Heinert et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Edgar Heinert, Matthias Rottmann, Kira Maag, Karsten Kahl. (2024)<br><strong>Reducing Texture Bias of Deep Neural Networks via Edge Enhancing Diffusion</strong><br><button class=copy-to-clipboard title="Reducing Texture Bias of Deep Neural Networks via Edge Enhancing Diffusion" index=142>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-142 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 60<br>Keywords: Vision Transformer, Convolution, Convolutional Neural Network, Convolutional Neural Network, Transformer, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09530v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09530v1.pdf filename=2402.09530v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Convolutional</b> <b>neural</b> <b>networks</b> <b>(CNNs)</b> for image processing tend to focus on localized texture patterns, commonly referred to as texture bias. While most of the previous works in the literature focus on the task of image classification, we go beyond this and study the texture bias of <b>CNNs</b> in semantic segmentation. In this work, we propose to train <b>CNNs</b> on pre-processed images with less texture to reduce the texture bias. Therein, the challenge is to suppress image texture while preserving shape information. To this end, we utilize edge enhancing diffusion (EED), an anisotropic image diffusion method initially introduced for image compression, to create texture reduced duplicates of existing datasets. Extensive numerical studies are performed with both <b>CNNs</b> and <b>vision</b> <b>transformer</b> models trained on original data and EED-processed data from the Cityscapes dataset and the CARLA driving simulator. We observe strong texture-dependence of <b>CNNs</b> and moderate texture-dependence of <b>transformers.</b> Training <b>CNNs</b> on EED-processed images enables the models to become completely ignorant with respect to texture, demonstrating resilience with respect to texture re-introduction to any degree. Additionally we analyze the performance reduction in depth on a level of connected components in the semantic segmentation and study the influence of EED pre-processing on domain generalization as well as adversarial robustness.</p></p class="citation"></blockquote><h3 id=336--143261-deepatlas-one-shot-localization-for-biomedical-data-peter-d-chang-2024>(3/36 | 143/261) DeepATLAS: One-Shot Localization for Biomedical Data (Peter D. Chang, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Peter D. Chang. (2024)<br><strong>DeepATLAS: One-Shot Localization for Biomedical Data</strong><br><button class=copy-to-clipboard title="DeepATLAS: One-Shot Localization for Biomedical Data" index=143>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-143 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 53<br>Keywords: Benchmarking, Few-shot, Fine-tuning, Self-supervised Learning, Supervised Learning, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09587v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09587v1.pdf filename=2402.09587v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper introduces the DeepATLAS foundational model for localization tasks in the domain of high-dimensional biomedical data. Upon convergence of the proposed <b>self-supervised</b> objective, a pretrained model maps an input to an anatomically-consistent embedding from which any point or set of points (e.g., boxes or segmentations) may be identified in a one-shot or <b>few-shot</b> approach. As a representative <b>benchmark,</b> a DeepATLAS model pretrained on a comprehensive cohort of 51,000+ unlabeled 3D computed tomography exams yields high one-shot segmentation performance on over 50 anatomic structures across four different external test sets, either matching or exceeding the performance of a standard <b>supervised</b> <b>learning</b> model. Further improvements in accuracy can be achieved by adding a small amount of labeled data using either a semisupervised or more conventional <b>fine-tuning</b> strategy.</p></p class="citation"></blockquote><h3 id=436--144261-prediction-of-activated-sludge-settling-characteristics-from-microscopy-images-with-deep-convolutional-neural-networks-and-transfer-learning-sina-borzooei-et-al-2024>(4/36 | 144/261) Prediction of Activated Sludge Settling Characteristics from Microscopy Images with Deep Convolutional Neural Networks and Transfer Learning (Sina Borzooei et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sina Borzooei, Leonardo Scabini, Gisele Miranda, Saba Daneshgar, Lukas Deblieck, Piet De Langhe, Odemir Bruno, Bernard De Baets, Ingmar Nopens, Elena Torfs. (2024)<br><strong>Prediction of Activated Sludge Settling Characteristics from Microscopy Images with Deep Convolutional Neural Networks and Transfer Learning</strong><br><button class=copy-to-clipboard title="Prediction of Activated Sludge Settling Characteristics from Microscopy Images with Deep Convolutional Neural Networks and Transfer Learning" index=144>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-144 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CE, cs-CV, cs.CV<br>Keyword Score: 50<br>Keywords: Convolution, Convolutional Neural Network, Convolutional Neural Network, Data Augmentation, Transfer Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09367v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09367v1.pdf filename=2402.09367v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Microbial communities play a key role in biological wastewater treatment processes. Activated sludge settling characteristics, for example, are affected by microbial community composition, varying by changes in operating conditions and influent characteristics of wastewater treatment plants (WWTPs). Timely assessment and prediction of changes in microbial composition leading to settling problems, such as filamentous bulking (FB), can prevent operational challenges, reductions in treatment efficiency, and adverse environmental impacts. This study presents an innovative computer vision-based approach to assess activated sludge-settling characteristics based on the morphological properties of flocs and filaments in microscopy images. Implementing the <b>transfer</b> <b>learning</b> of deep <b>convolutional</b> <b>neural</b> <b>network</b> <b>(CNN)</b> models, this approach aims to overcome the limitations of existing quantitative image analysis techniques. The offline microscopy image dataset was collected over two years, with weekly sampling at a full-scale industrial WWTP in Belgium. Multiple <b>data</b> <b>augmentation</b> techniques were employed to enhance the generalizability of the <b>CNN</b> models. Various <b>CNN</b> architectures, including Inception v3, ResNet18, ResNet152, ConvNeXt-nano, and ConvNeXt-S, were tested to evaluate their performance in predicting sludge settling characteristics. The sludge volume index was used as the final prediction variable, but the method can easily be adjusted to predict any other settling metric of choice. The results showed that the suggested <b>CNN-based</b> approach provides less labour-intensive, objective, and consistent assessments, while <b>transfer</b> <b>learning</b> notably minimises the training phase, resulting in a generalizable system that can be employed in real-time applications.</p></p class="citation"></blockquote><h3 id=536--145261-magic-me-identity-specific-video-customized-diffusion-ze-ma-et-al-2024>(5/36 | 145/261) Magic-Me: Identity-Specific Video Customized Diffusion (Ze Ma et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ze Ma, Daquan Zhou, Chun-Hsiao Yeh, Xue-She Wang, Xiuyu Li, Huanrui Yang, Zhen Dong, Kurt Keutzer, Jiashi Feng. (2024)<br><strong>Magic-Me: Identity-Specific Video Customized Diffusion</strong><br><button class=copy-to-clipboard title="Magic-Me: Identity-Specific Video Customized Diffusion" index=145>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-145 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Fine-tuning, Information Retrieval, Text2image, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09368v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09368v1.pdf filename=2402.09368v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Creating content for a specific identity (ID) has shown significant interest in the field of generative models. In the field of <b>text-to-image</b> generation (T2I), subject-driven content generation has achieved great progress with the ID in the images controllable. However, extending it to video generation is not well explored. In this work, we propose a simple yet effective subject identity controllable video generation framework, termed Video Custom Diffusion (VCD). With a specified subject ID defined by a few images, VCD reinforces the identity <b>information</b> <b>extraction</b> and injects frame-wise correlation at the initialization stage for stable video outputs with identity preserved to a large extent. To achieve this, we propose three novel components that are essential for high-quality ID preservation: 1) an ID module trained with the cropped identity by <b>prompt-to-segmentation</b> to disentangle the ID <b>information</b> <b>and</b> the background noise for more accurate ID token learning; 2) a text-to-video (T2V) VCD module with 3D Gaussian Noise Prior for better inter-frame consistency and 3) video-to-video (V2V) Face VCD and Tiled VCD modules to deblur the face and upscale the video for higher resolution. Despite its simplicity, we conducted extensive experiments to verify that VCD is able to generate stable and high-quality videos with better ID over the selected strong baselines. Besides, due to the transferability of the ID module, VCD is also working well with <b>finetuned</b> <b>text-to-image</b> models available publically, further improving its usability. The codes are available at <a href=https://github.com/Zhen-Dong/Magic-Me>https://github.com/Zhen-Dong/Magic-Me</a>.</p></p class="citation"></blockquote><h3 id=636--146261-is-my-data-in-your-ai-model-membership-inference-test-with-application-to-face-images-daniel-dealcala-et-al-2024>(6/36 | 146/261) Is my Data in your AI Model? Membership Inference Test with Application to Face Images (Daniel DeAlcala et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Daniel DeAlcala, Aythami Morales, Gonzalo Mancera, Julian Fierrez, Ruben Tolosana, Javier Ortega-Garcia. (2024)<br><strong>Is my Data in your AI Model? Membership Inference Test with Application to Face Images</strong><br><button class=copy-to-clipboard title="Is my Data in your AI Model? Membership Inference Test with Application to Face Images" index=146>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-146 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Face Recognition, Convolution, Convolutional Neural Network, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09225v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09225v1.pdf filename=2402.09225v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper introduces the Membership Inference Test (MINT), a novel approach that aims to empirically assess if specific data was used during the training of Artificial Intelligence (AI) models. Specifically, we propose two novel MINT architectures designed to learn the distinct activation patterns that emerge when an audited model is exposed to data used during its training process. The first architecture is based on a Multilayer Perceptron (MLP) network and the second one is based on <b>Convolutional</b> <b>Neural</b> <b>Networks</b> <b>(CNNs).</b> The proposed MINT architectures are evaluated on a challenging <b>face</b> <b>recognition</b> task, considering three state-of-the-art <b>face</b> <b>recognition</b> models. Experiments are carried out using six publicly available databases, comprising over 22 million <b>face</b> <b>images</b> in total. Also, different experimental scenarios are considered depending on the context available of the AI model to test. Promising results, up to 90% accuracy, are achieved using our proposed MINT approach, suggesting that it is possible to recognize if an AI model has been trained with specific data.</p></p class="citation"></blockquote><h3 id=736--147261-pyramid-attention-network-for-medical-image-registration-zhuoyuan-wang-et-al-2024>(7/36 | 147/261) Pyramid Attention Network for Medical Image Registration (Zhuoyuan Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhuoyuan Wang, Haiqiao Wang, Yi Wang. (2024)<br><strong>Pyramid Attention Network for Medical Image Registration</strong><br><button class=copy-to-clipboard title="Pyramid Attention Network for Medical Image Registration" index=147>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-147 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Convolution, Convolutional Neural Network, Convolutional Neural Network, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09016v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09016v1.pdf filename=2402.09016v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The advent of deep-learning-based registration networks has addressed the time-consuming challenge in traditional iterative methods.However, the potential of current registration networks for comprehensively capturing spatial relationships has not been fully explored, leading to inadequate performance in large-deformation image registration.The pure <b>convolutional</b> <b>neural</b> <b>networks</b> <b>(CNNs)</b> neglect feature enhancement, while current <b>Transformer-based</b> networks are susceptible to information redundancy.To alleviate these issues, we propose a pyramid attention network (PAN) for deformable medical image registration.Specifically, the proposed PAN incorporates a dual-stream pyramid encoder with channel-wise attention to boost the feature representation.Moreover, a multi-head local attention <b>Transformer</b> is introduced as decoder to analyze motion patterns and generate deformation fields.Extensive experiments on two public brain magnetic resonance imaging (MRI) datasets and one abdominal MRI dataset demonstrate that our method achieves favorable registration performance, while outperforming several <b>CNN-based</b> and <b>Transformer-based</b> registration networks.Our code is publicly available at <a href=https://github.com/JuliusWang-7/PAN>https://github.com/JuliusWang-7/PAN</a>.</p></p class="citation"></blockquote><h3 id=836--148261-pretraining-vision-language-model-for-difference-visual-question-answering-in-longitudinal-chest-x-rays-yeongjae-cho-et-al-2024>(8/36 | 148/261) Pretraining Vision-Language Model for Difference Visual Question Answering in Longitudinal Chest X-rays (Yeongjae Cho et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yeongjae Cho, Taehee Kim, Heejun Shin, Sungzoon Cho, Dongmyung Shin. (2024)<br><strong>Pretraining Vision-Language Model for Difference Visual Question Answering in Longitudinal Chest X-rays</strong><br><button class=copy-to-clipboard title="Pretraining Vision-Language Model for Difference Visual Question Answering in Longitudinal Chest X-rays" index=148>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-148 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CL, cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Question Answering, Visual Question Answering, Visual Question Answering, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08966v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08966v1.pdf filename=2402.08966v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Difference <b>visual</b> <b>question</b> <b>answering</b> (diff-VQA) is a challenging task that requires answering complex <b>questions</b> <b>based</b> on differences between a pair of images. This task is particularly important in reading chest X-ray images because radiologists often compare multiple images of the same patient taken at different times to track disease progression and changes in its severity in their clinical practice. However, previous works focused on designing specific network architectures for the diff-VQA task, missing opportunities to enhance the model&rsquo;s performance using a pretrained <b>vision-language</b> model (VLM). Here, we introduce a novel VLM called PLURAL, which is pretrained on natural and longitudinal chest X-ray data for the diff-VQA task. The model is developed using a step-by-step approach, starting with being pretrained on natural images and texts, followed by being trained using longitudinal chest X-ray data. The longitudinal data consist of pairs of X-ray images, along with <b>question-answer</b> <b>sets</b> and radiologist&rsquo;s reports that describe the changes in lung abnormalities and diseases over time. Our experimental results show that the PLURAL model outperforms state-of-the-art methods not only in diff-VQA for longitudinal X-rays but also in conventional <b>VQA</b> for a single X-ray image. Through extensive experiments, we demonstrate the effectiveness of the proposed VLM architecture and pretraining method in improving the model&rsquo;s performance.</p></p class="citation"></blockquote><h3 id=936--149261-moving-object-proposals-with-deep-learned-optical-flow-for-video-object-segmentation-ge-shi-et-al-2024>(9/36 | 149/261) Moving Object Proposals with Deep Learned Optical Flow for Video Object Segmentation (Ge Shi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ge Shi, Zhili Yang. (2024)<br><strong>Moving Object Proposals with Deep Learned Optical Flow for Video Object Segmentation</strong><br><button class=copy-to-clipboard title="Moving Object Proposals with Deep Learned Optical Flow for Video Object Segmentation" index=149>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-149 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: 68Txx, cs-CV, cs-LG, cs.CV<br>Keyword Score: 40<br>Keywords: Convolution, Convolutional Neural Network, Fine-tuning, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08882v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08882v1.pdf filename=2402.08882v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Dynamic scene understanding is one of the most conspicuous field of interest among computer vision community. In order to enhance dynamic scene understanding, pixel-wise segmentation with neural networks is widely accepted. The latest researches on pixel-wise segmentation combined semantic and motion information and produced good performance. In this work, we propose a state of art architecture of neural networks to accurately and efficiently get the moving object proposals (MOP). We first train an <b>unsupervised</b> <b>convolutional</b> <b>neural</b> <b>network</b> (UnFlow) to generate optical flow estimation. Then we render the output of optical flow net to a fully <b>convolutional</b> <b>SegNet</b> <b>model.</b> The main contribution of our work is (1) <b>Fine-tuning</b> the pretrained optical flow model on the brand new DAVIS Dataset; (2) Leveraging fully <b>convolutional</b> <b>neural</b> <b>networks</b> with Encoder-Decoder architecture to segment objects. We developed the codes with TensorFlow, and executed the training and evaluation processes on an AWS EC2 instance.</p></p class="citation"></blockquote><h3 id=1036--150261-tdvit-temporal-dilated-video-transformer-for-dense-video-tasks-guanxiong-sun-et-al-2024>(10/36 | 150/261) TDViT: Temporal Dilated Video Transformer for Dense Video Tasks (Guanxiong Sun et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Guanxiong Sun, Yang Hua, Guosheng Hu, Neil Robertson. (2024)<br><strong>TDViT: Temporal Dilated Video Transformer for Dense Video Tasks</strong><br><button class=copy-to-clipboard title="TDViT: Temporal Dilated Video Transformer for Dense Video Tasks" index=150>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-150 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 33<br>Keywords: Object Detection, Benchmarking, Convolutional Neural Network, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09257v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09257v1.pdf filename=2402.09257v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep video models, for example, 3D <b>CNNs</b> or video <b>transformers,</b> have achieved promising performance on sparse video tasks, i.e., predicting one result per video. However, challenges arise when adapting existing deep video models to dense video tasks, i.e., predicting one result per frame. Specifically, these models are expensive for deployment, less effective when handling redundant frames, and difficult to capture long-range temporal correlations. To overcome these issues, we propose a Temporal Dilated Video <b>Transformer</b> (TDViT) that consists of carefully designed temporal dilated <b>transformer</b> blocks (TDTB). TDTB can efficiently extract spatiotemporal representations and effectively alleviate the negative effect of temporal redundancy. Furthermore, by using hierarchical TDTBs, our approach obtains an exponentially expanded temporal receptive field and therefore can model long-range dynamics. Extensive experiments are conducted on two different dense video <b>benchmarks,</b> i.e., ImageNet VID for video <b>object</b> <b>detection</b> and YouTube VIS for video instance segmentation. Excellent experimental results demonstrate the superior efficiency, effectiveness, and compatibility of our method. The code is available at <a href=https://github.com/guanxiongsun/vfe.pytorch>https://github.com/guanxiongsun/vfe.pytorch</a>.</p></p class="citation"></blockquote><h3 id=1136--151261-can-text-to-image-model-assist-multi-modal-learning-for-visual-recognition-with-visual-modality-missing-tiantian-feng-et-al-2024>(11/36 | 151/261) Can Text-to-image Model Assist Multi-modal Learning for Visual Recognition with Visual Modality Missing? (Tiantian Feng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tiantian Feng, Daniel Yang, Digbalay Bose, Shrikanth Narayanan. (2024)<br><strong>Can Text-to-image Model Assist Multi-modal Learning for Visual Recognition with Visual Modality Missing?</strong><br><button class=copy-to-clipboard title="Can Text-to-image Model Assist Multi-modal Learning for Visual Recognition with Visual Modality Missing?" index=151>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-151 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 33<br>Keywords: Multi-modal, Transformer, Text2image, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09036v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09036v1.pdf filename=2402.09036v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Multi-modal</b> learning has emerged as an increasingly promising avenue in vision recognition, driving innovations across diverse domains ranging from media and education to healthcare and transportation. Despite its success, the robustness of <b>multi-modal</b> learning for visual recognition is often challenged by the unavailability of a subset of modalities, especially the visual modality. Conventional approaches to mitigate missing modalities in <b>multi-modal</b> learning rely heavily on algorithms and modality fusion schemes. In contrast, this paper explores the use of <b>text-to-image</b> models to assist <b>multi-modal</b> learning. Specifically, we propose a simple but effective <b>multi-modal</b> learning framework GTI-MM to enhance the data efficiency and model robustness against missing visual modality by imputing the missing data with generative <b>transformers.</b> Using multiple <b>multi-modal</b> datasets with visual recognition tasks, we present a comprehensive analysis of diverse conditions involving missing visual modality in data, including model training. Our findings reveal that synthetic images benefit training data efficiency with visual data missing in training and improve model robustness with visual data missing involving training and testing. Moreover, we demonstrate GTI-MM is effective with lower generation quantity and simple <b>prompt</b> techniques.</p></p class="citation"></blockquote><h3 id=1236--152261-quantified-task-misalignment-to-inform-peft-an-exploration-of-domain-generalization-and-catastrophic-forgetting-in-clip-laura-niss-et-al-2024>(12/36 | 152/261) Quantified Task Misalignment to Inform PEFT: An Exploration of Domain Generalization and Catastrophic Forgetting in CLIP (Laura Niss et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Laura Niss, Kevin Vogt-Lowell, Theodoros Tsiligkaridis. (2024)<br><strong>Quantified Task Misalignment to Inform PEFT: An Exploration of Domain Generalization and Catastrophic Forgetting in CLIP</strong><br><button class=copy-to-clipboard title="Quantified Task Misalignment to Inform PEFT: An Exploration of Domain Generalization and Catastrophic Forgetting in CLIP" index=152>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-152 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Fine-tuning, Zero-shot, Text Embedding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09613v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09613v1.pdf filename=2402.09613v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Foundations models are presented as generalists that often perform well over a myriad of tasks. <b>Fine-tuning</b> these models, even on limited data, provides an additional boost in task-specific performance but often at the cost of their wider generalization, an effect termed catastrophic forgetting. In this paper, we analyze the relation between task difficulty in the CLIP model and the performance of several simple parameter-efficient <b>fine-tuning</b> methods through the lens of domain generalization and catastrophic forgetting. We provide evidence that the silhouette score of the <b>zero-shot</b> image and <b>text</b> <b>embeddings</b> is a better measure of task difficulty than the average cosine similarity of correct image/label embeddings, and discuss observable relationships between task difficulty, <b>fine-tuning</b> method, domain generalization, and catastrophic forgetting. Additionally, the averaged results across tasks and performance measures demonstrate that a simplified method that trains only a subset of attention weights, which we call A-CLIP, yields a balance between domain generalization and catastrophic forgetting.</p></p class="citation"></blockquote><h3 id=1336--153261-domain-adaptive-and-subgroup-specific-cascaded-temperature-regression-for-out-of-distribution-calibration-jiexin-wang-et-al-2024>(13/36 | 153/261) Domain-adaptive and Subgroup-specific Cascaded Temperature Regression for Out-of-distribution Calibration (Jiexin Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiexin Wang, Jiahao Chen, Bing Su. (2024)<br><strong>Domain-adaptive and Subgroup-specific Cascaded Temperature Regression for Out-of-distribution Calibration</strong><br><button class=copy-to-clipboard title="Domain-adaptive and Subgroup-specific Cascaded Temperature Regression for Out-of-distribution Calibration" index=153>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-153 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: MNIST, Data Augmentation, Out-of-distribution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09204v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09204v1.pdf filename=2402.09204v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Although deep neural networks yield high classification accuracy given sufficient training <b>data,</b> <b>their</b> predictions are typically overconfident or under-confident, i.e., the prediction confidences cannot truly reflect the accuracy. Post-hoc calibration tackles this problem by calibrating the prediction confidences without re-training the classification model. However, current approaches assume congruence between test and validation <b>data</b> <b>distributions,</b> limiting their applicability to <b>out-of-distribution</b> scenarios. To this end, we propose a novel meta-set-based cascaded temperature regression method for post-hoc calibration. Our method tailors fine-grained scaling functions to distinct test sets by simulating various domain shifts through <b>data</b> <b>augmentation</b> on the validation set. We partition each meta-set into subgroups based on predicted category and confidence level, capturing diverse uncertainties. A regression network is then trained to derive category-specific and confidence-level-specific scaling, achieving calibration across meta-sets. Extensive experimental results on <b>MNIST,</b> CIFAR-10, and TinyImageNet demonstrate the effectiveness of the proposed method.</p></p class="citation"></blockquote><h3 id=1436--154261-affine-transformation-estimation-improves-visual-self-supervised-learning-david-torpey-et-al-2024>(14/36 | 154/261) Affine transformation estimation improves visual self-supervised learning (David Torpey et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>David Torpey, Richard Klein. (2024)<br><strong>Affine transformation estimation improves visual self-supervised learning</strong><br><button class=copy-to-clipboard title="Affine transformation estimation improves visual self-supervised learning" index=154>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-154 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Data Augmentation, Self-supervised Learning, Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09071v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09071v1.pdf filename=2402.09071v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The standard approach to modern <b>self-supervised</b> <b>learning</b> is to generate random views through <b>data</b> <b>augmentations</b> and minimise a loss computed from the representations of these views. This inherently encourages invariance to the transformations that comprise the <b>data</b> <b>augmentation</b> function. In this work, we show that adding a module to constrain the representations to be predictive of an affine transformation improves the performance and efficiency of the learning process. The module is agnostic to the base <b>self-supervised</b> <b>model</b> and manifests in the form of an additional loss term that encourages an aggregation of the encoder representations to be predictive of an affine transformation applied to the input images. We perform experiments in various modern <b>self-supervised</b> <b>models</b> and see a performance improvement in all cases. Further, we perform an ablation study on the components of the affine transformation to understand which of them is affecting performance the most, as well as on key architectural design decisions.</p></p class="citation"></blockquote><h3 id=1536--155261-open-vocabulary-segmentation-with-unpaired-mask-text-supervision-zhaoqing-wang-et-al-2024>(15/36 | 155/261) Open-Vocabulary Segmentation with Unpaired Mask-Text Supervision (Zhaoqing Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhaoqing Wang, Xiaobo Xia, Ziye Chen, Xiao He, Yandong Guo, Mingming Gong, Tongliang Liu. (2024)<br><strong>Open-Vocabulary Segmentation with Unpaired Mask-Text Supervision</strong><br><button class=copy-to-clipboard title="Open-Vocabulary Segmentation with Unpaired Mask-Text Supervision" index=155>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-155 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Weakly-supervised Learning, Image2text, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08960v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08960v1.pdf filename=2402.08960v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Contemporary cutting-edge open-vocabulary segmentation approaches commonly rely on image-mask-text triplets, yet this restricted annotation is labour-intensive and encounters scalability hurdles in complex real-world scenarios. Although some methods are proposed to reduce the annotation cost with only text supervision, the incompleteness of supervision severely limits the versatility and performance. In this paper, we liberate the strict correspondence between masks and texts by using independent image-mask and <b>image-text</b> pairs, which can be easily collected respectively. With this unpaired mask-text supervision, we propose a new <b>weakly-supervised</b> open-vocabulary segmentation framework (Uni-OVSeg) that leverages confident pairs of mask predictions and entities in text descriptions. Using the independent image-mask and <b>image-text</b> pairs, we predict a set of binary masks and associate them with entities by resorting to the CLIP embedding space. However, the inherent noise in the correspondence between masks and entities poses a significant challenge when obtaining reliable pairs. In light of this, we advocate using the large <b>vision-language</b> model (LVLM) to refine text descriptions and devise a multi-scale ensemble to stablise the matching between masks and entities. Compared to text-only <b>weakly-supervised</b> methods, our Uni-OVSeg achieves substantial improvements of 15.5% mIoU on the ADE20K datasets, and even surpasses fully-supervised methods on the challenging PASCAL Context-459 dataset.</p></p class="citation"></blockquote><h3 id=1636--156261-tiktokactions-a-tiktok-derived-video-dataset-for-human-action-recognition-yang-qian-et-al-2024>(16/36 | 156/261) TikTokActions: A TikTok-Derived Video Dataset for Human Action Recognition (Yang Qian et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yang Qian, Yinan Sun, Ali Kargarandehkordi, Onur Cezmi Mutlu, Saimourya Surabhi, Pingyi Chen, Zain Jabbar, Dennis Paul Wall, Peter Washington. (2024)<br><strong>TikTokActions: A TikTok-Derived Video Dataset for Human Action Recognition</strong><br><button class=copy-to-clipboard title="TikTokActions: A TikTok-Derived Video Dataset for Human Action Recognition" index=156>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-156 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Fine-tuning, Fine-tuning, Foundation Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08875v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08875v1.pdf filename=2402.08875v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The increasing variety and quantity of tagged multimedia content on platforms such as TikTok provides an opportunity to advance computer vision modeling. We have curated a distinctive dataset of 283,582 unique video clips categorized under 386 hashtags relating to modern human actions. We release this dataset as a valuable resource for building domain-specific <b>foundation</b> <b>models</b> for human movement modeling tasks such as action recognition. To validate this dataset, which we name TikTokActions, we perform two sets of experiments. First, we pretrain the state-of-the-art VideoMAEv2 with a ViT-base backbone on TikTokActions subset, and then <b>fine-tune</b> and evaluate on popular datasets such as UCF101 and the HMDB51. We find that the performance of the model pre-trained using our Tik-Tok dataset is comparable to models trained on larger action recognition datasets (95.3% on UCF101 and 53.24% on HMDB51). Furthermore, our investigation into the relationship between pre-training dataset size and <b>fine-tuning</b> performance reveals that beyond a certain threshold, the incremental benefit of larger training sets diminishes. This work introduces a useful TikTok video dataset that is available for public use and provides insights into the marginal benefit of increasing pre-training dataset sizes for video-based <b>foundation</b> <b>models.</b></p></p class="citation"></blockquote><h3 id=1736--157261-only-my-model-on-my-data-a-privacy-preserving-approach-protecting-one-model-and-deceiving-unauthorized-black-box-models-weiheng-chai-et-al-2024>(17/36 | 157/261) Only My Model On My Data: A Privacy Preserving Approach Protecting one Model and Deceiving Unauthorized Black-Box Models (Weiheng Chai et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Weiheng Chai, Brian Testa, Huantao Ren, Asif Salekin, Senem Velipasalar. (2024)<br><strong>Only My Model On My Data: A Privacy Preserving Approach Protecting one Model and Deceiving Unauthorized Black-Box Models</strong><br><button class=copy-to-clipboard title="Only My Model On My Data: A Privacy Preserving Approach Protecting one Model and Deceiving Unauthorized Black-Box Models" index=157>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-157 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 25<br>Keywords: Face Recognition, Black Box, Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09316v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09316v1.pdf filename=2402.09316v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep neural networks are extensively applied to real-world tasks, such as <b>face</b> <b>recognition</b> and medical image classification, where privacy and data protection are critical. Image data, if not protected, can be exploited to infer personal or contextual information. Existing privacy preservation methods, like encryption, generate perturbed images that are unrecognizable to even humans. <b>Adversarial</b> <b>attack</b> approaches prohibit automated inference even for authorized stakeholders, limiting practical incentives for commercial and widespread adaptation. This pioneering study tackles an unexplored practical privacy preservation use case by generating human-perceivable images that maintain accurate inference by an authorized model while evading other unauthorized <b>black-box</b> <b>models</b> of similar or dissimilar objectives, and addresses the previous research gaps. The datasets employed are ImageNet, for image classification, Celeba-HQ dataset, for identity classification, and AffectNet, for emotion classification. Our results show that the generated images can successfully maintain the accuracy of a protected model and degrade the average accuracy of the unauthorized <b>black-box</b> <b>models</b> to 11.97%, 6.63%, and 55.51% on ImageNet, Celeba-HQ, and AffectNet datasets, respectively.</p></p class="citation"></blockquote><h3 id=1836--158261-weatherproofing-retrieval-for-localization-with-generative-ai-and-geometric-consistency-yannis-kalantidis-et-al-2024>(18/36 | 158/261) Weatherproofing Retrieval for Localization with Generative AI and Geometric Consistency (Yannis Kalantidis et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yannis Kalantidis, Mert Bülent Sarıyıldız, Rafael S. Rezende, Philippe Weinzaepfel, Diane Larlus, Gabriela Csurka. (2024)<br><strong>Weatherproofing Retrieval for Localization with Generative AI and Geometric Consistency</strong><br><button class=copy-to-clipboard title="Weatherproofing Retrieval for Localization with Generative AI and Geometric Consistency" index=158>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-158 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 25<br>Keywords: Generative AI, Geometry, Text2image<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09237v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09237v1.pdf filename=2402.09237v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>State-of-the-art visual localization approaches generally rely on a first image retrieval step whose role is crucial. Yet, retrieval often struggles when facing varying conditions, due to e.g. weather or time of day, with dramatic consequences on the visual localization accuracy. In this paper, we improve this retrieval step and tailor it to the final localization task. Among the several changes we advocate for, we propose to synthesize variants of the training set images, obtained from <b>generative</b> <b>text-to-image</b> models, in order to automatically expand the training set towards a number of nameable variations that particularly hurt visual localization. After expanding the training set, we propose a training approach that leverages the specificities and the underlying <b>geometry</b> of this mix of real and synthetic images. We experimentally show that those changes translate into large improvements for the most challenging visual localization datasets. Project page: <a href=https://europe.naverlabs.com/ret4loc>https://europe.naverlabs.com/ret4loc</a></p></p class="citation"></blockquote><h3 id=1936--159261-synthesizing-knowledge-enhanced-features-for-real-world-zero-shot-food-detection-pengfei-zhou-et-al-2024>(19/36 | 159/261) Synthesizing Knowledge-enhanced Features for Real-world Zero-shot Food Detection (Pengfei Zhou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Pengfei Zhou, Weiqing Min, Jiajun Song, Yang Zhang, Shuqiang Jiang. (2024)<br><strong>Synthesizing Knowledge-enhanced Features for Real-world Zero-shot Food Detection</strong><br><button class=copy-to-clipboard title="Synthesizing Knowledge-enhanced Features for Real-world Zero-shot Food Detection" index=159>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-159 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 21<br>Keywords: Graph, Benchmarking, Knowledge Graph, Zero-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09242v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09242v1.pdf filename=2402.09242v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Food computing brings various perspectives to computer vision like vision-based food analysis for nutrition and health. As a fundamental task in food computing, food detection needs <b>Zero-Shot</b> Detection (ZSD) on novel unseen food objects to support real-world scenarios, such as intelligent kitchens and smart restaurants. Therefore, we first <b>benchmark</b> the task of <b>Zero-Shot</b> Food Detection (ZSFD) by introducing FOWA dataset with rich attribute annotations. Unlike ZSD, fine-grained problems in ZSFD like inter-class similarity make synthesized features inseparable. The complexity of food semantic attributes further makes it more difficult for current ZSD methods to distinguish various food categories. To address these problems, we propose a novel framework ZSFDet to tackle fine-grained problems by exploiting the interaction between complex attributes. Specifically, we model the correlation between food categories and attributes in ZSFDet by multi-source <b>graphs</b> to provide prior <b>knowledge</b> <b>for</b> distinguishing fine-grained features. Within ZSFDet, <b>Knowledge-Enhanced</b> <b>Feature</b> Synthesizer (KEFS) learns <b>knowledge</b> <b>representation</b> from multiple sources (e.g., ingredients correlation from <b>knowledge</b> <b>graph)</b> via the multi-source <b>graph</b> fusion. Conditioned on the fusion of semantic <b>knowledge</b> <b>representation,</b> the region feature diffusion model in KEFS can generate fine-grained features for training the effective <b>zero-shot</b> detector. Extensive evaluations demonstrate the superior performance of our method ZSFDet on FOWA and the widely-used food dataset UECFOOD-256, with significant improvements by 1.8% and 3.7% ZSD mAP compared with the strong baseline RRFS. Further experiments on PASCAL VOC and MS COCO prove that enhancement of the semantic <b>knowledge</b> <b>can</b> also improve the performance on general ZSD. Code and dataset are available at <a href=https://github.com/LanceZPF/KEFS>https://github.com/LanceZPF/KEFS</a>.</p></p class="citation"></blockquote><h3 id=2036--160261-yolov8-am-yolov8-with-attention-mechanisms-for-pediatric-wrist-fracture-detection-chun-tse-chien-et-al-2024>(20/36 | 160/261) YOLOv8-AM: YOLOv8 with Attention Mechanisms for Pediatric Wrist Fracture Detection (Chun-Tse Chien et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chun-Tse Chien, Rui-Yang Ju, Kuang-Yi Chou, Jen-Shiun Chiang. (2024)<br><strong>YOLOv8-AM: YOLOv8 with Attention Mechanisms for Pediatric Wrist Fracture Detection</strong><br><button class=copy-to-clipboard title="YOLOv8-AM: YOLOv8 with Attention Mechanisms for Pediatric Wrist Fracture Detection" index=160>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-160 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Yolo, Convolution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09329v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09329v2.pdf filename=2402.09329v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Wrist trauma and even fractures occur frequently in daily life, particularly among children who account for a significant proportion of fracture cases. Before performing surgery, surgeons often request patients to undergo X-ray imaging first and prepare for it based on the analysis of the radiologist. With the development of neural networks, You Only Look Once <b>(YOLO)</b> series models have been widely used in fracture detection as computer-assisted diagnosis (CAD). In 2023, Ultralytics presented the latest version of the <b>YOLO</b> models, which has been employed for detecting fractures across various parts of the body. Attention mechanism is one of the hottest methods to improve the model performance. This research work proposes YOLOv8-AM, which incorporates the attention mechanism into the original YOLOv8 architecture. Specifically, we respectively employ four attention modules, <b>Convolutional</b> Block Attention Module (CBAM), Global Attention Mechanism (GAM), Efficient Channel Attention (ECA), and Shuffle Attention (SA), to design the improved models and train them on GRAZPEDWRI-DX dataset. Experimental results demonstrate that the mean Average Precision at IoU 50 (mAP 50) of the YOLOv8-AM model based on ResBlock + CBAM (ResCBAM) increased from 63.6% to 65.8%, which achieves the state-of-the-art (SOTA) performance. Conversely, YOLOv8-AM model incorporating GAM obtains the mAP 50 value of 64.2%, which is not a satisfactory enhancement. Therefore, we combine ResBlock and GAM, introducing ResGAM to design another new YOLOv8-AM model, whose mAP 50 value is increased to 65.0%.</p></p class="citation"></blockquote><h3 id=2136--161261-immediate-generalisation-in-humans-but-a-generalisation-lag-in-deep-neural-networks----evidence-for-representational-divergence-lukas-s-huber-et-al-2024>(21/36 | 161/261) Immediate generalisation in humans but a generalisation lag in deep neural networks &ndash; evidence for representational divergence? (Lukas S. Huber et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lukas S. Huber, Fred W. Mast, Felix A. Wichmann. (2024)<br><strong>Immediate generalisation in humans but a generalisation lag in deep neural networks &ndash; evidence for representational divergence?</strong><br><button class=copy-to-clipboard title="Immediate generalisation in humans but a generalisation lag in deep neural networks -- evidence for representational divergence?" index=161>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-161 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-LG, cs.CV, q-bio-NC<br>Keyword Score: 20<br>Keywords: Supervised Learning, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09303v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09303v2.pdf filename=2402.09303v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent research has seen many behavioral comparisons between humans and deep neural networks (DNNs) in the domain of image classification. Often, comparison studies focus on the end-result of the learning process by measuring and comparing the similarities in the representations of object categories once they have been formed. However, the process of how these representations emerge &ndash; that is, the behavioral changes and intermediate stages observed during the acquisition &ndash; is less often directly and empirically compared. Here we report a detailed investigation of how transferable representations are acquired in human observers and various classic and state-of-the-art DNNs. We develop a constrained <b>supervised</b> <b>learning</b> environment in which we align learning-relevant parameters such as starting point, input modality, available input data and the feedback provided. Across the whole learning process we evaluate and compare how well learned representations can be generalized to previously unseen test data. Our findings indicate that in terms of absolute classification performance DNNs demonstrate a level of data efficiency comparable to &ndash; and sometimes even exceeding that &ndash; of human learners, challenging some prevailing assumptions in the field. However, comparisons across the entire learning process reveal significant representational differences: while DNNs&rsquo; learning is characterized by a pronounced generalisation lag, humans appear to immediately acquire generalizable representations without a preliminary phase of learning training set-specific information that is only later transferred to novel data.</p></p class="citation"></blockquote><h3 id=2236--162261-towards-realistic-landmark-guided-facial-video-inpainting-based-on-gans-fatemeh-ghorbani-lohesara-et-al-2024>(22/36 | 162/261) Towards Realistic Landmark-Guided Facial Video Inpainting Based on GANs (Fatemeh Ghorbani Lohesara et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fatemeh Ghorbani Lohesara, Karen Egiazarian, Sebastian Knorr. (2024)<br><strong>Towards Realistic Landmark-Guided Facial Video Inpainting Based on GANs</strong><br><button class=copy-to-clipboard title="Towards Realistic Landmark-Guided Facial Video Inpainting Based on GANs" index=162>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-162 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Generative Adversarial Network, Generative Adversarial Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09100v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09100v1.pdf filename=2402.09100v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Facial video inpainting plays a crucial role in a wide range of applications, including but not limited to the removal of obstructions in video conferencing and telemedicine, enhancement of facial expression analysis, privacy protection, integration of graphical overlays, and virtual makeup. This domain presents serious challenges due to the intricate nature of facial features and the inherent human familiarity with faces, heightening the need for accurate and persuasive completions. In addressing challenges specifically related to occlusion removal in this context, our focus is on the progressive task of generating complete images from facial data covered by masks, ensuring both spatial and temporal coherence. Our study introduces a network designed for expression-based video inpainting, employing <b>generative</b> <b>adversarial</b> <b>networks</b> <b>(GANs)</b> to handle static and moving occlusions across all frames. By utilizing facial landmarks and an occlusion-free reference image, our model maintains the user&rsquo;s identity consistently across frames. We further enhance emotional preservation through a customized facial expression recognition (FER) loss function, ensuring detailed inpainted outputs. Our proposed framework exhibits proficiency in eliminating occlusions from facial videos in an adaptive form, whether appearing static or dynamic on the frames, while providing realistic and coherent results.</p></p class="citation"></blockquote><h3 id=2336--163261-weakly-supervised-segmentation-of-vertebral-bodies-with-iterative-slice-propagation-shiqi-peng-et-al-2024>(23/36 | 163/261) Weakly Supervised Segmentation of Vertebral Bodies with Iterative Slice-propagation (Shiqi Peng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shiqi Peng, Bolin Lai, Guangyu Yao, Xiaoyun Zhang, Ya Zhang, Yan-Feng Wang, Hui Zhao. (2024)<br><strong>Weakly Supervised Segmentation of Vertebral Bodies with Iterative Slice-propagation</strong><br><button class=copy-to-clipboard title="Weakly Supervised Segmentation of Vertebral Bodies with Iterative Slice-propagation" index=163>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-163 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 20<br>Keywords: Supervised Learning, Weakly-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08892v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08892v1.pdf filename=2402.08892v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Vertebral body (VB) segmentation is an important preliminary step towards medical visual diagnosis for spinal diseases. However, most previous works require pixel/voxel-wise strong supervisions, which is expensive, tedious and time-consuming for experts to annotate. In this paper, we propose a Weakly <b>supervised</b> Iterative Spinal Segmentation (WISS) method leveraging only four corner landmark weak labels on a single sagittal slice to achieve automatic volumetric segmentation from CT images for VBs. WISS first segments VBs on an annotated sagittal slice in an iterative self-training manner. This self-training method alternates between training and refining labels in the training set. Then WISS proceeds to segment the whole VBs slice by slice with a slice-propagation method to obtain volumetric segmentations. We evaluate the performance of WISS on a private spinal metastases CT dataset and the public lumbar CT dataset. On the first dataset, WISS achieves distinct improvements with regard to two different backbones. For the second dataset, WISS achieves dice coefficients of $91.7%$ and $83.7%$ for mid-sagittal slices and 3D CT volumes, respectively, saving a lot of labeling costs and only sacrificing a little segmentation performance.</p></p class="citation"></blockquote><h3 id=2436--164261-multimedeval-a-benchmark-and-a-toolkit-for-evaluating-medical-vision-language-models-corentin-royer-et-al-2024>(24/36 | 164/261) MultiMedEval: A Benchmark and a Toolkit for Evaluating Medical Vision-Language Models (Corentin Royer et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Corentin Royer, Bjoern Menze, Anjany Sekuboyina. (2024)<br><strong>MultiMedEval: A Benchmark and a Toolkit for Evaluating Medical Vision-Language Models</strong><br><button class=copy-to-clipboard title="MultiMedEval: A Benchmark and a Toolkit for Evaluating Medical Vision-Language Models" index=164>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-164 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 19<br>Keywords: Benchmarking, Benchmarking, Multi-modal, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09262v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09262v2.pdf filename=2402.09262v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce MultiMedEval, an open-source toolkit for fair and reproducible evaluation of large, medical <b>vision-language</b> models (VLM). MultiMedEval comprehensively assesses the models&rsquo; performance on a broad array of six <b>multi-modal</b> tasks, conducted over 23 datasets, and spanning over 11 medical domains. The chosen tasks and performance metrics are based on their widespread adoption in the community and their diversity, ensuring a thorough evaluation of the model&rsquo;s overall generalizability. We open-source a Python toolkit (github.com/corentin-ryr/MultiMedEval) with a simple interface and setup process, enabling the evaluation of any VLM in just a few lines of code. Our goal is to simplify the intricate landscape of VLM evaluation, thus promoting fair and uniform <b>benchmarking</b> of future models.</p></p class="citation"></blockquote><h3 id=2536--165261-clip-mused-clip-guided-multi-subject-visual-neural-information-semantic-decoding-qiongyi-zhou-et-al-2024>(25/36 | 165/261) CLIP-MUSED: CLIP-Guided Multi-Subject Visual Neural Information Semantic Decoding (Qiongyi Zhou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qiongyi Zhou, Changde Du, Shengpei Wang, Huiguang He. (2024)<br><strong>CLIP-MUSED: CLIP-Guided Multi-Subject Visual Neural Information Semantic Decoding</strong><br><button class=copy-to-clipboard title="CLIP-MUSED: CLIP-Guided Multi-Subject Visual Neural Information Semantic Decoding" index=165>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-165 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 15<br>Keywords: Representation Learning, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08994v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08994v1.pdf filename=2402.08994v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The study of decoding visual neural information faces challenges in generalizing single-subject decoding models to multiple subjects, due to individual differences. Moreover, the limited availability of data from a single subject has a constraining impact on model performance. Although prior multi-subject decoding methods have made significant progress, they still suffer from several limitations, including difficulty in extracting global neural response features, linear scaling of model parameters with the number of subjects, and inadequate characterization of the relationship between neural responses of different subjects to various stimuli. To overcome these limitations, we propose a CLIP-guided Multi-sUbject visual neural information SEmantic Decoding (CLIP-MUSED) method. Our method consists of a <b>Transformer-based</b> feature extractor to effectively model global neural <b>representations.</b> <b>It</b> also incorporates learnable subject-specific tokens that facilitates the aggregation of multi-subject data without a linear increase of parameters. Additionally, we employ <b>representational</b> <b>similarity</b> analysis (RSA) to guide token <b>representation</b> <b>learning</b> based on the topological relationship of visual stimuli in the <b>representation</b> <b>space</b> of CLIP, enabling full characterization of the relationship between neural responses of different subjects under different stimuli. Finally, token <b>representations</b> <b>are</b> used for multi-subject semantic decoding. Our proposed method outperforms single-subject decoding methods and achieves state-of-the-art performance among the existing multi-subject methods on two fMRI datasets. Visualization results provide insights into the effectiveness of our proposed method. Code is available at <a href=https://github.com/CLIP-MUSED/CLIP-MUSED>https://github.com/CLIP-MUSED/CLIP-MUSED</a>.</p></p class="citation"></blockquote><h3 id=2636--166261-comment-aided-video-language-alignment-via-contrastive-pre-training-for-short-form-video-humor-detection-yang-liu-et-al-2024>(26/36 | 166/261) Comment-aided Video-Language Alignment via Contrastive Pre-training for Short-form Video Humor Detection (Yang Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yang Liu, Tongfei Shen, Dong Zhang, Qingying Sun, Shoushan Li, Guodong Zhou. (2024)<br><strong>Comment-aided Video-Language Alignment via Contrastive Pre-training for Short-form Video Humor Detection</strong><br><button class=copy-to-clipboard title="Comment-aided Video-Language Alignment via Contrastive Pre-training for Short-form Video Humor Detection" index=166>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-166 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 13<br>Keywords: Multi-modal, Video-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09055v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09055v1.pdf filename=2402.09055v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The growing importance of <b>multi-modal</b> humor detection within affective computing correlates with the expanding influence of short-form video sharing on social media platforms. In this paper, we propose a novel two-branch hierarchical model for short-form video humor detection (SVHD), named Comment-aided Video-Language Alignment (CVLA) via data-augmented <b>multi-modal</b> contrastive pre-training. Notably, our CVLA not only operates on raw signals across various modal channels but also yields an appropriate <b>multi-modal</b> representation by aligning the video and language components within a consistent semantic space. The experimental results on two humor detection datasets, including DY11k and UR-FUNNY, demonstrate that CVLA dramatically outperforms state-of-the-art and several competitive baseline approaches. Our dataset, code and model release at <a href=https://github.com/yliu-cs/CVLA>https://github.com/yliu-cs/CVLA</a>.</p></p class="citation"></blockquote><h3 id=2736--167261-automated-plaque-detection-and-agatston-score-estimation-on-non-contrast-ct-scans-a-multicenter-study-andrew-m-nguyen-et-al-2024>(27/36 | 167/261) Automated Plaque Detection and Agatston Score Estimation on Non-Contrast CT Scans: A Multicenter Study (Andrew M. Nguyen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Andrew M. Nguyen, Jianfei Liu, Tejas Sudharshan Mathai, Peter C. Grayson, Ronald M. Summers. (2024)<br><strong>Automated Plaque Detection and Agatston Score Estimation on Non-Contrast CT Scans: A Multicenter Study</strong><br><button class=copy-to-clipboard title="Automated Plaque Detection and Agatston Score Estimation on Non-Contrast CT Scans: A Multicenter Study" index=167>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-167 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Graph Attention Networks<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09569v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09569v1.pdf filename=2402.09569v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Coronary artery calcification (CAC) is a strong and independent predictor of cardiovascular disease (CVD). However, manual assessment of CAC often requires radiological expertise, time, and invasive imaging techniques. The purpose of this multicenter study is to validate an automated cardiac plaque detection model using a 3D multiclass nnU-Net for <b>gated</b> and non-gated non-contrast chest CT volumes. CT scans were performed at three tertiary care hospitals and collected as three datasets, respectively. Heart, aorta, and lung segmentations were determined using TotalSegmentator, while plaques in the coronary arteries and heart valves were manually labeled for 801 volumes. In this work we demonstrate how the nnU-Net semantic segmentation pipeline may be adapted to detect plaques in the coronary arteries and valves. With a linear correction, nnU-Net deep learning methods may also accurately estimate Agatston scores on chest non-contrast CT scans. Compared to manual Agatson scoring, automated Agatston scoring indicated a slope of the linear regression of 0.841 with an intercept of +16 HU (R2 = 0.97). These results are an improvement over previous work assessing automated Agatston score computation in non-gated CT scans.</p></p class="citation"></blockquote><h3 id=2836--168261-patch-based-adaptive-temporal-filter-and-residual-evaluation-weiying-zhao-et-al-2024>(28/36 | 168/261) Patch-based adaptive temporal filter and residual evaluation (Weiying Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Weiying Zhao, Paul Riot, Charles-Alban Deledalle, Henri Maître, Jean-Marie Nicolas, Florence Tupin. (2024)<br><strong>Patch-based adaptive temporal filter and residual evaluation</strong><br><button class=copy-to-clipboard title="Patch-based adaptive temporal filter and residual evaluation" index=168>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-168 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV, eess-IV<br>Keyword Score: 10<br>Keywords: Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09561v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09561v1.pdf filename=2402.09561v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In coherent imaging systems, speckle is a signal-dependent noise that visually strongly degrades images&rsquo; appearance. A huge amount of SAR data has been acquired from different sensors with different wavelengths, resolutions, incidences and polarizations. We extend the nonlocal filtering strategy to the temporal domain and propose a patch-based adaptive temporal filter (PATF) to take advantage of well-registered multi-temporal SAR images. A patch-based generalised likelihood ratio test is processed to suppress the changed object effects on the multitemporal denoising results. Then, the similarities are transformed into corresponding weights with an exponential function. The denoised value is calculated with a temporal weighted average. Spatial adaptive denoising methods can improve the patch-based weighted temporal average image when the time series is limited. The spatial adaptive denoising step is optional when the time series is large enough. Without reference image, we propose using a patch-based auto-covariance residual evaluation method to examine the ratio image between the noisy and denoised images and look for possible remaining structural contents. It can process automatically and does not rely on a <b>supervised</b> selection of homogeneous regions. It also provides a global score for the whole image. Numerous results demonstrate the effectiveness of the proposed time series denoising method and the usefulness of the residual evaluation method.</p></p class="citation"></blockquote><h3 id=2936--169261-fast-window-based-event-denoising-with-spatiotemporal-correlation-enhancement-huachen-fang-et-al-2024>(29/36 | 169/261) Fast Window-Based Event Denoising with Spatiotemporal Correlation Enhancement (Huachen Fang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Huachen Fang, Jinjian Wu, Qibin Hou, Weisheng Dong, Guangming Shi. (2024)<br><strong>Fast Window-Based Event Denoising with Spatiotemporal Correlation Enhancement</strong><br><button class=copy-to-clipboard title="Fast Window-Based Event Denoising with Spatiotemporal Correlation Enhancement" index=169>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-169 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Convolution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09270v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09270v1.pdf filename=2402.09270v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Previous deep learning-based event denoising methods mostly suffer from poor interpretability and difficulty in real-time processing due to their complex architecture designs. In this paper, we propose window-based event denoising, which simultaneously deals with a stack of events while existing element-based denoising focuses on one event each time. Besides, we give the theoretical analysis based on probability distributions in both temporal and spatial domains to improve interpretability. In temporal domain, we use timestamp deviations between processing events and central event to judge the temporal correlation and filter out temporal-irrelevant events. In spatial domain, we choose maximum a posteriori (MAP) to discriminate real-world event and noise, and use the learned <b>convolutional</b> sparse coding to optimize the objective function. Based on the theoretical analysis, we build Temporal Window (TW) module and Soft Spatial Feature Embedding (SSFE) module to process temporal and spatial information separately, and construct a novel multi-scale window-based event denoising network, named MSDNet. The high denoising accuracy and fast running speed of our MSDNet enables us to achieve real-time denoising in complex scenes. Extensive experimental results verify the effectiveness and robustness of our MSDNet. Our algorithm can remove event noise effectively and efficiently and improve the performance of downstream tasks.</p></p class="citation"></blockquote><h3 id=3036--170261-efficient-one-stage-video-object-detection-by-exploiting-temporal-consistency-guanxiong-sun-et-al-2024>(30/36 | 170/261) Efficient One-stage Video Object Detection by Exploiting Temporal Consistency (Guanxiong Sun et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Guanxiong Sun, Yang Hua, Guosheng Hu, Neil Robertson. (2024)<br><strong>Efficient One-stage Video Object Detection by Exploiting Temporal Consistency</strong><br><button class=copy-to-clipboard title="Efficient One-stage Video Object Detection by Exploiting Temporal Consistency" index=170>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-170 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Object Detection<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09241v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09241v1.pdf filename=2402.09241v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recently, one-stage detectors have achieved competitive accuracy and faster speed compared with traditional two-stage detectors on image data. However, in the field of video <b>object</b> <b>detection</b> (VOD), most existing VOD methods are still based on two-stage detectors. Moreover, directly adapting existing VOD methods to one-stage detectors introduces unaffordable computational costs. In this paper, we first analyse the computational bottlenecks of using one-stage detectors for VOD. Based on the analysis, we present a simple yet efficient framework to address the computational bottlenecks and achieve efficient one-stage VOD by exploiting the temporal consistency in video frames. Specifically, our method consists of a location-prior network to filter out background regions and a size-prior network to skip unnecessary computations on low-level feature maps for specific frames. We test our method on various modern one-stage detectors and conduct extensive experiments on the ImageNet VID dataset. Excellent experimental results demonstrate the superior effectiveness, efficiency, and compatibility of our method. The code is available at <a href=https://github.com/guanxiongsun/vfe.pytorch>https://github.com/guanxiongsun/vfe.pytorch</a>.</p></p class="citation"></blockquote><h3 id=3136--171261-gradient-alignment-with-prototype-feature-for-fully-test-time-adaptation-juhyeon-shin-et-al-2024>(31/36 | 171/261) Gradient Alignment with Prototype Feature for Fully Test-time Adaptation (Juhyeon Shin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Juhyeon Shin, Jonghyun Lee, Saehyung Lee, Minjun Park, Dongjun Lee, Uiwon Hwang, Sungroh Yoon. (2024)<br><strong>Gradient Alignment with Prototype Feature for Fully Test-time Adaptation</strong><br><button class=copy-to-clipboard title="Gradient Alignment with Prototype Feature for Fully Test-time Adaptation" index=171>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-171 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 10<br>Keywords: In-context Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09004v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09004v1.pdf filename=2402.09004v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In context of Test-time Adaptation(TTA), we propose a regularizer, dubbed Gradient Alignment with Prototype feature (GAP), which alleviates the inappropriate guidance from entropy minimization loss from misclassified pseudo label. We developed a gradient alignment loss to precisely manage the adaptation process, ensuring that changes made for some data don&rsquo;t negatively impact the model&rsquo;s performance on other data. We introduce a prototype feature of a class as a proxy measure of the negative impact. To make GAP regularizer feasible under the TTA constraints, where model can only access test data without labels, we tailored its formula in two ways: approximating prototype features with weight vectors of the classifier, calculating gradient without back-propagation. We demonstrate GAP significantly improves TTA methods across various datasets, which proves its versatility and effectiveness.</p></p class="citation"></blockquote><h3 id=3236--172261-depth-aware-volume-attention-for-texture-less-stereo-matching-tong-zhao-et-al-2024>(32/36 | 172/261) Depth-aware Volume Attention for Texture-less Stereo Matching (Tong Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tong Zhao, Mingyu Ding, Wei Zhan, Masayoshi Tomizuka, Yintao Wei. (2024)<br><strong>Depth-aware Volume Attention for Texture-less Stereo Matching</strong><br><button class=copy-to-clipboard title="Depth-aware Volume Attention for Texture-less Stereo Matching" index=172>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-172 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08931v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08931v1.pdf filename=2402.08931v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Stereo matching plays a crucial role in 3D perception and scenario understanding. Despite the proliferation of promising methods, addressing texture-less and texture-repetitive conditions remains challenging due to the insufficient availability of rich geometric and semantic information. In this paper, we propose a lightweight volume refinement scheme to tackle the texture deterioration in practical outdoor scenarios. Specifically, we introduce a depth volume <b>supervised</b> by the ground-truth depth map, capturing the relative hierarchy of image texture. Subsequently, the disparity discrepancy volume undergoes hierarchical filtering through the incorporation of depth-aware hierarchy attention and target-aware disparity attention modules. Local fine structure and context are emphasized to mitigate ambiguity and redundancy during volume aggregation. Furthermore, we propose a more rigorous evaluation metric that considers depth-wise relative error, providing comprehensive evaluations for universal stereo matching and depth estimation models. We extensively validate the superiority of our proposed methods on public datasets. Results demonstrate that our model achieves state-of-the-art performance, particularly excelling in scenarios with texture-less images. The code is available at <a href=https://github.com/ztsrxh/DVANet>https://github.com/ztsrxh/DVANet</a>.</p></p class="citation"></blockquote><h3 id=3336--173261-headset-human-emotion-awareness-under-partial-occlusions-multimodal-dataset-fatemeh-ghorbani-lohesara-et-al-2024>(33/36 | 173/261) Headset: Human emotion awareness under partial occlusions multimodal dataset (Fatemeh Ghorbani Lohesara et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fatemeh Ghorbani Lohesara, Davi Rabbouni Freitas, Christine Guillemot, Karen Eguiazarian, Sebastian Knorr. (2024)<br><strong>Headset: Human emotion awareness under partial occlusions multimodal dataset</strong><br><button class=copy-to-clipboard title="Headset: Human emotion awareness under partial occlusions multimodal dataset" index=173>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-173 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 6<br>Keywords: Multi-modal, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09107v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09107v1.pdf filename=2402.09107v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The volumetric representation of human interactions is one of the fundamental domains in the development of immersive media productions and telecommunication applications. Particularly in the context of the rapid advancement of Extended Reality (XR) applications, this volumetric data has proven to be an essential technology for future XR elaboration. In this work, we present a new <b>multimodal</b> database to help advance the development of immersive technologies. Our proposed database provides ethically compliant and diverse volumetric data, in particular 27 participants displaying posed facial expressions and subtle body movements while speaking, plus 11 participants wearing head-mounted displays (HMDs). The recording system consists of a volumetric capture (VoCap) studio, including 31 synchronized modules with 62 RGB cameras and 31 depth cameras. In addition to textured meshes, point clouds, and multi-view RGB-D data, we use one Lytro Illum camera for providing light field (LF) data simultaneously. Finally, we also provide an evaluation of our dataset employment with regard to the tasks of facial expression classification, HMDs removal, and point cloud reconstruction. The dataset can be helpful in the evaluation and performance testing of various XR algorithms, including but not limited to facial expression recognition and reconstruction, facial reenactment, and volumetric video. HEADSET and its all associated raw data and license agreement will be publicly available for research purposes.</p></p class="citation"></blockquote><h3 id=3436--174261-interpretable-measures-of-conceptual-similarity-by-complexity-constrained-descriptive-auto-encoding-alessandro-achille-et-al-2024>(34/36 | 174/261) Interpretable Measures of Conceptual Similarity by Complexity-Constrained Descriptive Auto-Encoding (Alessandro Achille et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Alessandro Achille, Greg Ver Steeg, Tian Yu Liu, Matthew Trager, Carson Klingenberg, Stefano Soatto. (2024)<br><strong>Interpretable Measures of Conceptual Similarity by Complexity-Constrained Descriptive Auto-Encoding</strong><br><button class=copy-to-clipboard title="Interpretable Measures of Conceptual Similarity by Complexity-Constrained Descriptive Auto-Encoding" index=174>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-174 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 6<br>Keywords: Benchmarking, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08919v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08919v1.pdf filename=2402.08919v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Quantifying the degree of similarity between images is a key copyright issue for image-based machine learning. In legal doctrine however, determining the degree of similarity between works requires subjective analysis, and fact-finders (judges and juries) can demonstrate considerable variability in these subjective judgement calls. Images that are structurally similar can be deemed dissimilar, whereas images of completely different scenes can be deemed similar enough to support a claim of copying. We seek to define and compute a notion of &ldquo;conceptual similarity&rdquo; among images that captures high-level relations even among images that do not share repeated elements or visually similar components. The idea is to use a base <b>multi-modal</b> model to generate &ldquo;explanations&rdquo; (captions) of visual data at increasing levels of complexity. Then, similarity can be measured by the length of the caption needed to discriminate between the two images: Two highly dissimilar images can be discriminated early in their description, whereas conceptually dissimilar ones will need more detail to be distinguished. We operationalize this definition and show that it correlates with subjective (averaged human evaluation) assessment, and beats existing baselines on both image-to-image and text-to-text similarity <b>benchmarks.</b> Beyond just providing a number, our method also offers interpretability by pointing to the specific level of granularity of the description where the source data are differentiated.</p></p class="citation"></blockquote><h3 id=3536--175261-loopy-slam-dense-neural-slam-with-loop-closures-lorenzo-liso-et-al-2024>(35/36 | 175/261) Loopy-SLAM: Dense Neural SLAM with Loop Closures (Lorenzo Liso et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lorenzo Liso, Erik Sandström, Vladimir Yugay, Luc Van Gool, Martin R. Oswald. (2024)<br><strong>Loopy-SLAM: Dense Neural SLAM with Loop Closures</strong><br><button class=copy-to-clipboard title="Loopy-SLAM: Dense Neural SLAM with Loop Closures" index=175>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-175 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09944v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09944v1.pdf filename=2402.09944v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Neural RGBD SLAM techniques have shown promise in dense Simultaneous Localization And Mapping (SLAM), yet face challenges such as error accumulation during camera tracking resulting in distorted maps. In response, we introduce Loopy-SLAM that globally optimizes poses and the dense 3D model. We use frame-to-model tracking using a data-driven point-based submap generation method and trigger loop closures online by performing global place recognition. Robust pose <b>graph</b> optimization is used to rigidly align the local submaps. As our representation is point based, map corrections can be performed efficiently without the need to store the entire history of input frames used for mapping as typically required by methods employing a grid based mapping structure. Evaluation on the synthetic Replica and real-world TUM-RGBD and ScanNet datasets demonstrate competitive or superior performance in tracking, mapping, and rendering accuracy when compared to existing dense neural RGBD SLAM methods. Project page: notchla.github.io/Loopy-SLAM.</p></p class="citation"></blockquote><h3 id=3636--176261-generalized-portrait-quality-assessment-nicolas-chahine-et-al-2024>(36/36 | 176/261) Generalized Portrait Quality Assessment (Nicolas Chahine et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nicolas Chahine, Sira Ferradans, Javier Vazquez-Corral, Jean Ponce. (2024)<br><strong>Generalized Portrait Quality Assessment</strong><br><button class=copy-to-clipboard title="Generalized Portrait Quality Assessment" index=176>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-176 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09178v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09178v1.pdf filename=2402.09178v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Automated and robust portrait quality assessment (PQA) is of paramount importance in high-impact applications such as smartphone photography. This paper presents FHIQA, a learning-based approach to PQA that introduces a simple but effective quality score rescaling method based on image semantics, to enhance the precision of fine-grained image quality metrics while ensuring robust generalization to various scene settings beyond the training dataset. The proposed approach is validated by extensive experiments on the PIQ23 <b>benchmark</b> and comparisons with the current state of the art. The source code of FHIQA will be made publicly available on the PIQ23 GitHub repository at <a href=https://github.com/DXOMARK-Research/PIQ2023>https://github.com/DXOMARK-Research/PIQ2023</a>.</p></p class="citation"></blockquote><h2 id=cscr-7>cs.CR (7)</h2><h3 id=17--177261-rapid-adoption-hidden-risks-the-dual-impact-of-large-language-model-customization-rui-zhang-et-al-2024>(1/7 | 177/261) Rapid Adoption, Hidden Risks: The Dual Impact of Large Language Model Customization (Rui Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rui Zhang, Hongwei Li, Rui Wen, Wenbo Jiang, Yuan Zhang, Michael Backes, Yun Shen, Yang Zhang. (2024)<br><strong>Rapid Adoption, Hidden Risks: The Dual Impact of Large Language Model Customization</strong><br><button class=copy-to-clipboard title="Rapid Adoption, Hidden Risks: The Dual Impact of Large Language Model Customization" index=177>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-177 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs-LG, cs.CR<br>Keyword Score: 63<br>Keywords: Benchmarking, Fine-tuning, GPT, Text Classification, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09179v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09179v2.pdf filename=2402.09179v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The increasing demand for customized <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> has led to the development of solutions like <b>GPTs.</b> These solutions facilitate tailored <b>LLM</b> creation via natural language <b>prompts</b> without coding. However, the trustworthiness of third-party custom versions of <b>LLMs</b> remains an essential concern. In this paper, we propose the first instruction backdoor attacks against applications integrated with untrusted customized <b>LLMs</b> (e.g., <b>GPTs).</b> Specifically, these attacks embed the backdoor into the custom version of <b>LLMs</b> by designing <b>prompts</b> with backdoor instructions, outputting the attacker&rsquo;s desired result when inputs contain the pre-defined triggers. Our attack includes 3 levels of attacks: word-level, syntax-level, and semantic-level, which adopt different types of triggers with progressive stealthiness. We stress that our attacks do not require <b>fine-tuning</b> or any modification to the backend <b>LLMs,</b> adhering strictly to <b>GPTs</b> development guidelines. We conduct extensive experiments on 4 prominent <b>LLMs</b> and 5 <b>benchmark</b> <b>text</b> <b>classification</b> datasets. The results show that our instruction backdoor attacks achieve the desired attack performance without compromising utility. Additionally, we propose an instruction-ignoring defense mechanism and demonstrate its partial effectiveness in mitigating such attacks. Our findings highlight the vulnerability and the potential risks of <b>LLM</b> customization such as <b>GPTs.</b></p></p class="citation"></blockquote><h3 id=27--178261-review-incorporated-model-agnostic-profile-injection-attacks-on-recommender-systems-shiyi-yang-et-al-2024>(2/7 | 178/261) Review-Incorporated Model-Agnostic Profile Injection Attacks on Recommender Systems (Shiyi Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shiyi Yang, Lina Yao, Chen Wang, Xiwei Xu, Liming Zhu. (2024)<br><strong>Review-Incorporated Model-Agnostic Profile Injection Attacks on Recommender Systems</strong><br><button class=copy-to-clipboard title="Review-Incorporated Model-Agnostic Profile Injection Attacks on Recommender Systems" index=178>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-178 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-AI, cs-CR, cs.CR<br>Keyword Score: 45<br>Keywords: Black Box, Generative Adversarial Network, Generative Adversarial Network, Recommender System, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09023v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09023v1.pdf filename=2402.09023v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent studies have shown that <b>recommender</b> <b>systems</b> (RSs) are highly vulnerable to data poisoning attacks. Understanding attack tactics helps improve the robustness of RSs. We intend to develop efficient attack methods that use limited resources to generate high-quality fake user profiles to achieve 1) transferability among <b>black-box</b> <b>RSs</b> 2) and imperceptibility among detectors. In order to achieve these goals, we introduce textual reviews of products to enhance the generation quality of the profiles. Specifically, we propose a novel attack framework named R-Trojan, which formulates the attack objectives as an optimization problem and adopts a tailored <b>transformer-based</b> <b>generative</b> <b>adversarial</b> <b>network</b> <b>(GAN)</b> to solve it so that high-quality attack profiles can be produced. Comprehensive experiments on real-world datasets demonstrate that R-Trojan greatly outperforms state-of-the-art attack methods on various victim RSs under <b>black-box</b> <b>settings</b> and show its good imperceptibility.</p></p class="citation"></blockquote><h3 id=37--179261-safedecoding-defending-against-jailbreak-attacks-via-safety-aware-decoding-zhangchen-xu-et-al-2024>(3/7 | 179/261) SafeDecoding: Defending against Jailbreak Attacks via Safety-Aware Decoding (Zhangchen Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhangchen Xu, Fengqing Jiang, Luyao Niu, Jinyuan Jia, Bill Yuchen Lin, Radha Poovendran. (2024)<br><strong>SafeDecoding: Defending against Jailbreak Attacks via Safety-Aware Decoding</strong><br><button class=copy-to-clipboard title="SafeDecoding: Defending against Jailbreak Attacks via Safety-Aware Decoding" index=179>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-179 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-AI, cs-CL, cs-CR, cs.CR<br>Keyword Score: 43<br>Keywords: Benchmarking, Chatbot, Code Generation, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08983v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08983v1.pdf filename=2402.08983v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>As <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> become increasingly integrated into real-world applications such as <b>code</b> <b>generation</b> and <b>chatbot</b> assistance, extensive efforts have been made to align <b>LLM</b> behavior with human values, including safety. Jailbreak attacks, aiming to provoke unintended and unsafe behaviors from <b>LLMs,</b> remain a significant/leading <b>LLM</b> safety threat. In this paper, we aim to defend <b>LLMs</b> against jailbreak attacks by introducing SafeDecoding, a safety-aware decoding strategy for <b>LLMs</b> to generate helpful and harmless responses to user queries. Our insight in developing SafeDecoding is based on the observation that, even though probabilities of tokens representing harmful contents outweigh those representing harmless responses, safety disclaimers still appear among the top tokens after sorting tokens by probability in descending order. This allows us to mitigate jailbreak attacks by identifying safety disclaimers and amplifying their token probabilities, while simultaneously attenuating the probabilities of token sequences that are aligned with the objectives of jailbreak attacks. We perform extensive experiments on five <b>LLMs</b> using six state-of-the-art jailbreak attacks and four <b>benchmark</b> datasets. Our results show that SafeDecoding significantly reduces the attack success rate and harmfulness of jailbreak attacks without compromising the helpfulness of responses to benign user queries. SafeDecoding outperforms six defense methods.</p></p class="citation"></blockquote><h3 id=47--180261-instruction-tuning-for-secure-code-generation-jingxuan-he-et-al-2024>(4/7 | 180/261) Instruction Tuning for Secure Code Generation (Jingxuan He et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jingxuan He, Mark Vero, Gabriela Krasnopolska, Martin Vechev. (2024)<br><strong>Instruction Tuning for Secure Code Generation</strong><br><button class=copy-to-clipboard title="Instruction Tuning for Secure Code Generation" index=180>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-180 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-AI, cs-CR, cs-LG, cs-SE, cs.CR<br>Keyword Score: 30<br>Keywords: Fine-tuning, Code Generation, Instruction Tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09497v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09497v1.pdf filename=2402.09497v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Modern language models (LMs) have gained widespread acceptance in everyday and professional contexts, particularly in programming. An essential procedure enabling this adoption is <b>instruction</b> <b>tuning,</b> which substantially enhances LMs&rsquo; practical utility by training them to follow user <b>instructions</b> <b>and</b> human preferences. However, existing <b>instruction</b> <b>tuning</b> schemes overlook a crucial aspect: the security of generated <b>code.</b> <b>As</b> a result, even the state-of-the-art <b>instruction-tuned</b> <b>LMs</b> frequently produce unsafe <b>code,</b> <b>posing</b> significant security risks. In this work, we introduce SafeCoder to address this gap. SafeCoder performs security-centric <b>fine-tuning</b> using a diverse and high-quality dataset that we collected using an automated pipeline. We integrate the security <b>fine-tuning</b> with standard <b>instruction</b> <b>tuning,</b> to facilitate a joint optimization of both security and utility. Despite its simplicity, we show that SafeCoder is effective across a variety of popular LMs and datasets. It is able to drastically improve security (by about 30%), while preserving utility.</p></p class="citation"></blockquote><h3 id=57--181261-combatting-deepfakes-policies-to-address-national-security-threats-and-rights-violations-andrea-miotti-et-al-2024>(5/7 | 181/261) Combatting deepfakes: Policies to address national security threats and rights violations (Andrea Miotti et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Andrea Miotti, Akash Wasil. (2024)<br><strong>Combatting deepfakes: Policies to address national security threats and rights violations</strong><br><button class=copy-to-clipboard title="Combatting deepfakes: Policies to address national security threats and rights violations" index=181>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-181 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-AI, cs-CR, cs-CY, cs.CR<br>Keyword Score: 10<br>Keywords: Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09581v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09581v2.pdf filename=2402.09581v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper provides policy <b>recommendations</b> to address threats from deepfakes. First, we provide background information about deepfakes and review the harms they pose. We describe how deepfakes are currently used to proliferate sexual abuse material, commit fraud, manipulate voter behavior, and pose threats to national security. Second, we review previous legislative proposals designed to address deepfakes. Third, we present a comprehensive policy proposal that focuses on addressing multiple parts of the deepfake supply chain. The deepfake supply chain begins with a small number of model developers, model providers, and compute providers, and it expands to include billions of potential deepfake creators. We describe this supply chain in greater detail and describe how entities at each step of the supply chain ought to take reasonable measures to prevent the creation and proliferation of deepfakes. Finally, we address potential counterpoints of our proposal. Overall, deepfakes will present increasingly severe threats to global security and individual liberties. To address these threats, we call on policymakers to enact legislation that addresses multiple parts of the deepfake supply chain.</p></p class="citation"></blockquote><h3 id=67--182261-discovering-command-and-control-c2-channels-on-tor-and-public-networks-using-reinforcement-learning-cheng-wang-et-al-2024>(6/7 | 182/261) Discovering Command and Control (C2) Channels on Tor and Public Networks Using Reinforcement Learning (Cheng Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Cheng Wang, Christopher Redino, Abdul Rahman, Ryan Clark, Daniel Radke, Tyler Cody, Dhruv Nandakumar, Edward Bowen. (2024)<br><strong>Discovering Command and Control (C2) Channels on Tor and Public Networks Using Reinforcement Learning</strong><br><button class=copy-to-clipboard title="Discovering Command and Control (C2) Channels on Tor and Public Networks Using Reinforcement Learning" index=182>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-182 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-AI, cs-CR, cs.CR<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09200v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09200v1.pdf filename=2402.09200v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Command and control (C2) channels are an essential component of many types of cyber attacks, as they enable attackers to remotely control their malware-infected machines and execute harmful actions, such as propagating malicious code across networks, exfiltrating confidential data, or initiating distributed denial of service (DDoS) attacks. Identifying these C2 channels is therefore crucial in helping to mitigate and prevent cyber attacks. However, identifying C2 channels typically involves a manual process, requiring deep knowledge and expertise in cyber operations. In this paper, we propose a <b>reinforcement</b> <b>learning</b> (RL) based approach to automatically emulate C2 attack campaigns using both the normal (public) and the Tor networks. In addition, payload size and network firewalls are configured to simulate real-world attack scenarios. Results on a typical network configuration show that the RL agent can automatically discover resilient C2 attack paths utilizing both Tor-based and conventional communication channels, while also bypassing network firewalls.</p></p class="citation"></blockquote><h3 id=77--183261-play-guessing-game-with-llm-indirect-jailbreak-attack-with-implicit-clues-zhiyuan-chang-et-al-2024>(7/7 | 183/261) Play Guessing Game with LLM: Indirect Jailbreak Attack with Implicit Clues (Zhiyuan Chang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhiyuan Chang, Mingyang Li, Yi Liu, Junjie Wang, Qing Wang, Yang Liu. (2024)<br><strong>Play Guessing Game with LLM: Indirect Jailbreak Attack with Implicit Clues</strong><br><button class=copy-to-clipboard title="Play Guessing Game with LLM: Indirect Jailbreak Attack with Implicit Clues" index=183>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-183 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-AI, cs-CR, cs-HC, cs.CR<br>Keyword Score: 10<br>Keywords: Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09091v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09091v2.pdf filename=2402.09091v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the development of <b>LLMs,</b> the security threats of <b>LLMs</b> are getting more and more attention. Numerous jailbreak attacks have been proposed to assess the security defense of <b>LLMs.</b> Current jailbreak attacks primarily utilize scenario camouflage techniques. However their explicitly mention of malicious intent will be easily recognized and defended by <b>LLMs.</b> In this paper, we propose an indirect jailbreak attack approach, Puzzler, which can bypass the <b>LLM&rsquo;s</b> defense strategy and obtain malicious response by implicitly providing <b>LLMs</b> with some clues about the original malicious query. In addition, inspired by the wisdom of &ldquo;When unable to attack, defend&rdquo; from Sun Tzu&rsquo;s Art of War, we adopt a defensive stance to gather clues about the original malicious query through <b>LLMs.</b> Extensive experimental results show that Puzzler achieves a query success rate of 96.6% on closed-source <b>LLMs,</b> which is 57.9%-82.7% higher than baselines. Furthermore, when tested against the state-of-the-art jailbreak detection approaches, Puzzler proves to be more effective at evading detection compared to baselines.</p></p class="citation"></blockquote><h2 id=csir-5>cs.IR (5)</h2><h3 id=15--184261-large-language-model-interaction-simulator-for-cold-start-item-recommendation-feiran-huang-et-al-2024>(1/5 | 184/261) Large Language Model Interaction Simulator for Cold-Start Item Recommendation (Feiran Huang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Feiran Huang, Zhenghang Yang, Junyi Jiang, Yuanchen Bei, Yijie Zhang, Hao Chen. (2024)<br><strong>Large Language Model Interaction Simulator for Cold-Start Item Recommendation</strong><br><button class=copy-to-clipboard title="Large Language Model Interaction Simulator for Cold-Start Item Recommendation" index=184>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-184 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 60<br>Keywords: Recommendation, Recommender System, Simulation, Simulator, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09176v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09176v1.pdf filename=2402.09176v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recommending cold items is a long-standing challenge for collaborative filtering models because these cold items lack historical user interactions to model their collaborative features. The gap between the content of cold items and their behavior patterns makes it difficult to generate accurate behavioral embeddings for cold items. Existing cold-start models use mapping functions to generate fake behavioral embeddings based on the content feature of cold items. However, these generated embeddings have significant differences from the real behavioral embeddings, leading to a negative impact on cold <b>recommendation</b> performance. To address this challenge, we propose an <b>LLM</b> Interaction Simulator <b>(LLM-InS)</b> to model users&rsquo; behavior patterns based on the content aspect. This simulator allows <b>recommender</b> <b>systems</b> to simulate vivid interactions for each cold item and transform them from cold to warm items directly. Specifically, we outline the designing and training process of a tailored <b>LLM-simulator</b> that can simulate the behavioral patterns of users and items. Additionally, we introduce an efficient &ldquo;filtering-and-refining&rdquo; approach to take full advantage of the <b>simulation</b> power of the <b>LLMs.</b> Finally, we propose an updating method to update the embeddings of the items. we unified trains for both cold and warm items within a <b>recommender</b> <b>model</b> based on the simulated and real interactions. Extensive experiments using real behavioral embeddings demonstrate that our proposed model, <b>LLM-InS,</b> outperforms nine state-of-the-art cold-start methods and three <b>LLM</b> models in cold-start item <b>recommendations.</b></p></p class="citation"></blockquote><h3 id=25--185261-rethinking-large-language-model-architectures-for-sequential-recommendations-hanbing-wang-et-al-2024>(2/5 | 185/261) Rethinking Large Language Model Architectures for Sequential Recommendations (Hanbing Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hanbing Wang, Xiaorui Liu, Wenqi Fan, Xiangyu Zhao, Venkataramana Kini, Devendra Yadav, Fei Wang, Zhen Wen, Jiliang Tang, Hui Liu. (2024)<br><strong>Rethinking Large Language Model Architectures for Sequential Recommendations</strong><br><button class=copy-to-clipboard title="Rethinking Large Language Model Architectures for Sequential Recommendations" index=185>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-185 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 30<br>Keywords: Recommendation, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09543v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09543v1.pdf filename=2402.09543v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recently, sequential <b>recommendation</b> has been adapted to the <b>LLM</b> paradigm to enjoy the power of <b>LLMs.</b> <b>LLM-based</b> methods usually formulate <b>recommendation</b> information into natural language and the model is trained to predict the next item in an auto-regressive manner. Despite their notable success, the substantial computational overhead of inference poses a significant obstacle to their real-world applicability. In this work, we endeavor to streamline existing <b>LLM-based</b> <b>recommendation</b> models and propose a simple yet highly effective model Lite-LLM4Rec. The primary goal of Lite-LLM4Rec is to achieve efficient inference for the sequential <b>recommendation</b> task. Lite-LLM4Rec circumvents the beam search decoding by using a straight item projection head for ranking scores generation. This design stems from our empirical observation that beam search decoding is ultimately unnecessary for sequential <b>recommendations.</b> Additionally, Lite-LLM4Rec introduces a hierarchical <b>LLM</b> structure tailored to efficiently handle the extensive contextual information associated with items, thereby reducing computational overhead while enjoying the capabilities of <b>LLMs.</b> Experiments on three publicly available datasets corroborate the effectiveness of Lite-LLM4Rec in both performance and inference efficiency (notably 46.8% performance improvement and 97.28% efficiency improvement on ML-1m) over existing <b>LLM-based</b> methods. Our implementations will be open sourced.</p></p class="citation"></blockquote><h3 id=35--186261-confidence-aware-fine-tuning-of-sequential-recommendation-systems-via-conformal-prediction-chen-wang-et-al-2024>(3/5 | 186/261) Confidence-aware Fine-tuning of Sequential Recommendation Systems via Conformal Prediction (Chen Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chen Wang, Fangxin Wang, Ruocheng Guo, Yueqing Liang, Kay Liu, Philip S. Yu. (2024)<br><strong>Confidence-aware Fine-tuning of Sequential Recommendation Systems via Conformal Prediction</strong><br><button class=copy-to-clipboard title="Confidence-aware Fine-tuning of Sequential Recommendation Systems via Conformal Prediction" index=186>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-186 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 20<br>Keywords: Fine-tuning, Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08976v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08976v1.pdf filename=2402.08976v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In Sequential <b>Recommendation</b> Systems, Cross-Entropy (CE) loss is commonly used but fails to harness item confidence scores during training. Recognizing the critical role of confidence in aligning training objectives with evaluation metrics, we propose CPFT, a versatile framework that enhances <b>recommendation</b> confidence by integrating Conformal Prediction (CP)-based losses with CE loss during <b>fine-tuning.</b> CPFT dynamically generates a set of items with a high probability of containing the ground truth, enriching the training process by incorporating validation data without compromising its role in model selection. This innovative approach, coupled with CP-based losses, sharpens the focus on refining <b>recommendation</b> sets, thereby elevating the confidence in potential item predictions. By <b>fine-tuning</b> item confidence through CP-based losses, CPFT significantly enhances model performance, leading to more precise and trustworthy <b>recommendations</b> that increase user trust and satisfaction. Our extensive evaluation across five diverse datasets and four distinct sequential models confirms CPFT&rsquo;s substantial impact on improving <b>recommendation</b> quality through strategic confidence optimization. Access to the framework&rsquo;s code will be provided following the acceptance of the paper.</p></p class="citation"></blockquote><h3 id=45--187261-recommendation-algorithm-based-on-recommendation-sessions-michał-malinowski-2024>(4/5 | 187/261) Recommendation Algorithm Based on Recommendation Sessions (Michał Malinowski, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Michał Malinowski. (2024)<br><strong>Recommendation Algorithm Based on Recommendation Sessions</strong><br><button class=copy-to-clipboard title="Recommendation Algorithm Based on Recommendation Sessions" index=187>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-187 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 10<br>Keywords: Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09130v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09130v1.pdf filename=2402.09130v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The enormous development of the Internet, both in the geographical scale and in the area of using its possibilities in everyday life, determines the creation and collection of huge amounts of data. Due to the scale, it is not possible to analyse them using traditional methods, therefore it makes a necessary to use modern methods and techniques. Such methods are provided, among others, by the area of <b>recommendations.</b> The aim of this study is to present a new algorithm in the area of <b>recommendation</b> systems, the algorithm based on data from various sets of information, both static (categories of objects, features of objects) and dynamic (user behaviour).</p></p class="citation"></blockquote><h3 id=55--188261-enhancing-id-and-text-fusion-via-alternative-training-in-session-based-recommendation-juanhui-li-et-al-2024>(5/5 | 188/261) Enhancing ID and Text Fusion via Alternative Training in Session-based Recommendation (Juanhui Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Juanhui Li, Haoyu Han, Zhikai Chen, Harry Shomer, Wei Jin, Amin Javari, Jiliang Tang. (2024)<br><strong>Enhancing ID and Text Fusion via Alternative Training in Session-based Recommendation</strong><br><button class=copy-to-clipboard title="Enhancing ID and Text Fusion via Alternative Training in Session-based Recommendation" index=188>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-188 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-AI, cs-IR, cs.IR<br>Keyword Score: 10<br>Keywords: Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08921v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08921v1.pdf filename=2402.08921v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Session-based <b>recommendation</b> has gained increasing attention in recent years, with its aim to offer tailored suggestions based on users&rsquo; historical behaviors within sessions. To advance this field, a variety of methods have been developed, with ID-based approaches typically demonstrating promising performance. However, these methods often face challenges with long-tail items and overlook other rich forms of information, notably valuable textual semantic information. To integrate text information, various methods have been introduced, mostly following a naive fusion framework. Surprisingly, we observe that fusing these two modalities does not consistently outperform the best single modality by following the naive fusion framework. Further investigation reveals an potential imbalance issue in naive fusion, where the ID dominates and text modality is undertrained. This suggests that the unexpected observation may stem from naive fusion&rsquo;s failure to effectively balance the two modalities, often over-relying on the stronger ID modality. This insight suggests that naive fusion might not be as effective in combining ID and text as previously expected. To address this, we propose a novel alternative training strategy AlterRec. It separates the training of ID and text, thereby avoiding the imbalance issue seen in naive fusion. Additionally, AlterRec designs a novel strategy to facilitate the interaction between the two modalities, enabling them to mutually learn from each other and integrate the text more effectively. Comprehensive experiments demonstrate the effectiveness of AlterRec in session-based <b>recommendation.</b> The implementation is available at <a href=https://github.com/Juanhui28/AlterRec>https://github.com/Juanhui28/AlterRec</a>.</p></p class="citation"></blockquote><h2 id=eesssy-5>eess.SY (5)</h2><h3 id=15--189261-uncertainty-aware-transient-stability-constrained-preventive-redispatch-a-distributional-reinforcement-learning-approach-zhengcheng-wang-et-al-2024>(1/5 | 189/261) Uncertainty-Aware Transient Stability-Constrained Preventive Redispatch: A Distributional Reinforcement Learning Approach (Zhengcheng Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhengcheng Wang, Fei Teng, Yanzhen Zhou, Qinglai Guo, Hongbin Sun. (2024)<br><strong>Uncertainty-Aware Transient Stability-Constrained Preventive Redispatch: A Distributional Reinforcement Learning Approach</strong><br><button class=copy-to-clipboard title="Uncertainty-Aware Transient Stability-Constrained Preventive Redispatch: A Distributional Reinforcement Learning Approach" index=189>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-189 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 53<br>Keywords: Graph, Graph Neural Network, Distributional Reinforcement Learning, Reinforcement Learning, Supervised Learning, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09263v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09263v1.pdf filename=2402.09263v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Transient stability-constrained preventive redispatch plays a crucial role in ensuring power system security and stability. Since redispatch strategies need to simultaneously satisfy complex transient constraints and the economic need, model-based formulation and optimization become extremely challenging. In addition, the increasing uncertainty and variability introduced by renewable sources start to drive the system stability consideration from deterministic to probabilistic, which further exaggerates the complexity. In this paper, a <b>Graph</b> <b>neural</b> <b>network</b> guided <b>Distributional</b> <b>Deep</b> <b>Reinforcement</b> <b>Learning</b> (GD2RL) method is proposed, for the first time, to solve the uncertainty-aware transient stability-constrained preventive redispatch problem. First, a <b>graph</b> <b>neural</b> <b>network-based</b> transient simulator is trained by <b>supervised</b> <b>learning</b> to efficiently generate post-contingency rotor angle curves with the steady-state and contingency as inputs, which serves as a feature extractor for operating states and a surrogate time-domain simulator during the environment interaction for <b>reinforcement</b> <b>learning.</b> <b>Distributional</b> <b>deep</b> <b>reinforcement</b> <b>learning</b> with explicit uncertainty distribution of system operational conditions is then applied to generate the redispatch strategy to balance the user-specified probabilistic stability performance and economy preferences. The full distribution of the post-control transient stability index is directly provided as the output. Case studies on the modified New England 39-bus system validate the proposed method.</p></p class="citation"></blockquote><h3 id=25--190261-a-practical-and-online-trajectory-planner-for-autonomous-ships-berthing-incorporating-speed-control-agnes-ngina-mwange-et-al-2024>(2/5 | 190/261) A Practical and Online Trajectory Planner for Autonomous Ships&rsquo; Berthing, Incorporating Speed Control (Agnes Ngina Mwange et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Agnes Ngina Mwange, Dimas Maulana Rachman, Rin Suyama, Atsuo Maki. (2024)<br><strong>A Practical and Online Trajectory Planner for Autonomous Ships&rsquo; Berthing, Incorporating Speed Control</strong><br><button class=copy-to-clipboard title="A Practical and Online Trajectory Planner for Autonomous Ships' Berthing, Incorporating Speed Control" index=190>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-190 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY, math-OC<br>Keyword Score: 30<br>Keywords: Human Intervention, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09009v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09009v1.pdf filename=2402.09009v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Autonomous ships are essentially designed and equipped to perceive their internal and external environment and subsequently perform appropriate actions depending on the predetermined objective(s) without <b>human</b> <b>intervention.</b> Consequently, trajectory planning algorithms for autonomous berthing must consider factors such as system dynamics, ship actuators, environmental disturbances, and the safety of the ship, other ships, and port structures, among others. In this study, basing the ship dynamics on the low-speed MMG model, trajectory planning for an autonomous ship is modeled as an optimal control problem (OCP) that is transcribed into a nonlinear programming problem (NLP) using the direct multiple shooting technique. To enhance berthing safety, besides considering wind disturbances, speed control, actuators&rsquo; limitations, and collision avoidance features are incorporated as constraints in the NLP, which is then solved using the Sequential Quadratic Programming (SQP) algorithm in MATLAB. Finally, the performance of the proposed planner is evaluated through (i) comparison with solutions obtained using CMA-ES for two different model ships, (ii) trajectory planning for different harbor entry and berth approach scenarios, and (iii) feasibility study using stochastically generated initial conditions and positions within the port boundaries. <b>Simulation</b> results indicate enhanced berthing safety as well as practical and computational feasibility making the planner suitable for real-time applications.</p></p class="citation"></blockquote><h3 id=35--191261-dynamic-modeling-and-predictive-control-of-a-microfluidic-system-jorge-vicente-martinez-et-al-2024>(3/5 | 191/261) Dynamic modeling and predictive control of a microfluidic system (Jorge Vicente Martinez et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jorge Vicente Martinez, Edgar Ramirez-Laboreo, Pablo Calderon Gil. (2024)<br><strong>Dynamic modeling and predictive control of a microfluidic system</strong><br><button class=copy-to-clipboard title="Dynamic modeling and predictive control of a microfluidic system" index=191>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-191 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09493v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09493v1.pdf filename=2402.09493v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Microfluidics, the study of fluids in microscopic channels, has led to important advances in fields as diverse as microelectronics, biotechnology and chemistry. Microfluidic research is primarily based on the use of microfluidic chips, low-cost devices that can be used to perform laboratory experiments using small amounts of fluid. These systems, however, require advanced control mechanisms in order to accurately achieve the flow rates and pressures required in the experiments. In this paper, we present the design of a model predictive controller intended to regulate the fluid flows in one of these systems. The results obtained, both through <b>simulations</b> and real experiments performed on the device, show that predictive control is an ideal technique to control these systems, especially taking into account all the existing constraints.</p></p class="citation"></blockquote><h3 id=45--192261-learning-enabled-flexible-job-shop-scheduling-for-scalable-smart-manufacturing-sihoon-moon-et-al-2024>(4/5 | 192/261) Learning-enabled Flexible Job-shop Scheduling for Scalable Smart Manufacturing (Sihoon Moon et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sihoon Moon, Sanghoon Lee, Kyung-Joon Park. (2024)<br><strong>Learning-enabled Flexible Job-shop Scheduling for Scalable Smart Manufacturing</strong><br><button class=copy-to-clipboard title="Learning-enabled Flexible Job-shop Scheduling for Scalable Smart Manufacturing" index=192>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-192 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-AI, cs-LG, cs-SY, eess-SY, eess.SY<br>Keyword Score: 16<br>Keywords: Graph, Benchmarking, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08979v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08979v1.pdf filename=2402.08979v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In smart manufacturing systems (SMSs), flexible job-shop scheduling with transportation constraints (FJSPT) is essential to optimize solutions for maximizing productivity, considering production flexibility based on automated guided vehicles (AGVs). Recent developments in deep <b>reinforcement</b> <b>learning</b> (DRL)-based methods for FJSPT have encountered a scale generalization challenge. These methods underperform when applied to environment at scales different from their training set, resulting in low-quality solutions. To address this, we introduce a novel <b>graph-based</b> DRL method, named the Heterogeneous <b>Graph</b> Scheduler (HGS). Our method leverages locally extracted relational knowledge among operations, machines, and vehicle nodes for scheduling, with a <b>graph-structured</b> decision-making framework that reduces encoding complexity and enhances scale generalization. Our performance evaluation, conducted with <b>benchmark</b> datasets, reveals that the proposed method outperforms traditional dispatching rules, meta-heuristics, and existing DRL-based approaches in terms of makespan performance, even on large-scale instances that have not been experienced during training.</p></p class="citation"></blockquote><h3 id=55--193261-steady-state-error-compensation-for-reinforcement-learning-with-quadratic-rewards-liyao-wang-et-al-2024>(5/5 | 193/261) Steady-State Error Compensation for Reinforcement Learning with Quadratic Rewards (Liyao Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Liyao Wang, Zishun Zheng, Yuan Lin. (2024)<br><strong>Steady-State Error Compensation for Reinforcement Learning with Quadratic Rewards</strong><br><button class=copy-to-clipboard title="Steady-State Error Compensation for Reinforcement Learning with Quadratic Rewards" index=193>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-193 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-LG, cs-SY, eess-SY, eess.SY<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09075v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09075v1.pdf filename=2402.09075v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The selection of a reward function in <b>Reinforcement</b> <b>Learning</b> (RL) has garnered significant attention because of its impact on system performance. Issues of steady-state error often manifest when quadratic reward functions are employed. Although existing solutions using absolute-value-type reward functions partially address this problem, they tend to induce substantial fluctuations in specific system states, leading to abrupt changes. In response to this challenge, this study proposes an approach that introduces an integral term. By integrating this term into quadratic-type reward functions, the RL algorithm is adeptly tuned, augmenting the system&rsquo;s consideration of long-term rewards and, consequently, alleviating concerns related to steady-state errors. Through experiments and performance evaluations on the Adaptive Cruise Control (ACC) model and lane change models, we validate that the proposed method not only effectively diminishes steady-state errors but also results in smoother variations in system states.</p></p class="citation"></blockquote><h2 id=eessas-4>eess.AS (4)</h2><h3 id=14--194261-unienc-cassnat-an-encoder-only-non-autoregressive-asr-for-speech-ssl-models-ruchao-fan-et-al-2024>(1/4 | 194/261) UniEnc-CASSNAT: An Encoder-only Non-autoregressive ASR for Speech SSL Models (Ruchao Fan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ruchao Fan, Natarajan Balaji Shanka, Abeer Alwan. (2024)<br><strong>UniEnc-CASSNAT: An Encoder-only Non-autoregressive ASR for Speech SSL Models</strong><br><button class=copy-to-clipboard title="UniEnc-CASSNAT: An Encoder-only Non-autoregressive ASR for Speech SSL Models" index=194>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-194 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.AS<br>Categories: cs-CL, cs-SD, eess-AS, eess.AS<br>Keyword Score: 50<br>Keywords: Foundation Model, Transformer, Automatic Speech Recognition, Automatic Speech Recognition, Automatic Speech Recognition<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08898v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08898v1.pdf filename=2402.08898v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Non-autoregressive <b>automatic</b> <b>speech</b> <b>recognition</b> (NASR) models have gained attention due to their parallelism and fast inference. The encoder-based NASR, e.g. connectionist temporal classification (CTC), can be initialized from the <b>speech</b> <b>foundation</b> <b>models</b> (SFM) but does not account for any dependencies among intermediate tokens. The encoder-decoder-based NASR, like CTC alignment-based single-step non-autoregressive <b>transformer</b> (CASS-NAT), can mitigate the dependency problem but is not able to efficiently integrate SFM. Inspired by the success of recent work of <b>speech-text</b> <b>joint</b> pre-training with a shared <b>transformer</b> encoder, we propose a new encoder-based NASR, UniEnc-CASSNAT, to combine the advantages of CTC and CASS-NAT. UniEnc-CASSNAT consists of only an encoder as the major module, which can be the SFM. The encoder plays the role of both the CASS-NAT encoder and decoder by two forward passes. The first pass of the encoder accepts the <b>speech</b> <b>signal</b> as input, while the concatenation of the <b>speech</b> <b>signal</b> and the token-level acoustic embedding is used as the input for the second pass. Examined on the Librispeech 100h, MyST, and Aishell1 datasets, the proposed UniEnc-CASSNAT achieves state-of-the-art NASR results and is better or comparable to CASS-NAT with only an encoder and hence, fewer model parameters. Our codes are publicly available.</p></p class="citation"></blockquote><h3 id=24--195261-listening-to-multi-talker-conversations-modular-and-end-to-end-perspectives-desh-raj-2024>(2/4 | 195/261) Listening to Multi-talker Conversations: Modular and End-to-end Perspectives (Desh Raj, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Desh Raj. (2024)<br><strong>Listening to Multi-talker Conversations: Modular and End-to-end Perspectives</strong><br><button class=copy-to-clipboard title="Listening to Multi-talker Conversations: Modular and End-to-end Perspectives" index=195>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-195 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.AS<br>Categories: cs-SD, eess-AS, eess.AS<br>Keyword Score: 43<br>Keywords: Clustering, Simulation, Simulator, Automatic Speech Recognition, Automatic Speech Recognition<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08932v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08932v1.pdf filename=2402.08932v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Since the first <b>speech</b> <b>recognition</b> systems were built more than 30 years ago, improvement in voice technology has enabled applications such as smart assistants and automated customer support. However, conversation intelligence of the future requires recognizing free-flowing multi-party conversations, which is a crucial and challenging component that still remains unsolved. In this dissertation, we focus on this problem of speaker-attributed multi-talker <b>speech</b> <b>recognition,</b> and propose two perspectives which result from its probabilistic formulation. In the modular perspective, we build a pipeline of sub-tasks involving speaker diarization, target speaker extraction, and <b>speech</b> <b>recognition.</b> Our first contribution is a method to perform overlap-aware diarization by reformulating spectral <b>clustering</b> as a constrained optimization problem. We also describe an algorithm to ensemble diarization outputs, either to combine overlap-aware systems or to perform multi-channel diarization by late fusion. Once speaker segments are identified, we robustly extract single-speaker utterances from the mixture using a GPU-accelerated implementation of guided source separation, which allows us to use an off-the-shelf <b>ASR</b> system to obtain speaker-attributed transcripts. Since the modular approach suffers from error propagation, we propose an alternate &ldquo;end-to-end&rdquo; perspective on the problem. For this, we describe the Streaming Unmixing and Recognition Transducer (SURT). We show how to train SURT models efficiently by carefully designing the network architecture, objective functions, and mixture <b>simulation</b> techniques. Finally, we add an auxiliary speaker branch to enable joint prediction of speaker labels synchronized with the <b>speech</b> <b>tokens.</b> We demonstrate that training on synthetic mixtures and adapting with real data helps these models transfer well for streaming transcription of real meeting sessions.</p></p class="citation"></blockquote><h3 id=34--196261-mobilespeech-a-fast-and-high-fidelity-framework-for-mobile-zero-shot-text-to-speech-shengpeng-ji-et-al-2024>(3/4 | 196/261) MobileSpeech: A Fast and High-Fidelity Framework for Mobile Zero-Shot Text-to-Speech (Shengpeng Ji et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shengpeng Ji, Ziyue Jiang, Hanting Wang, Jialong Zuo, Zhou Zhao. (2024)<br><strong>MobileSpeech: A Fast and High-Fidelity Framework for Mobile Zero-Shot Text-to-Speech</strong><br><button class=copy-to-clipboard title="MobileSpeech: A Fast and High-Fidelity Framework for Mobile Zero-Shot Text-to-Speech" index=196>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-196 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.AS<br>Categories: cs-SD, eess-AS, eess.AS<br>Keyword Score: 40<br>Keywords: Zero-shot, Text-to-speech, Text-to-speech, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09378v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09378v1.pdf filename=2402.09378v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Zero-shot</b> <b>text-to-speech</b> <b>(TTS)</b> has gained significant attention due to its powerful voice cloning capabilities, requiring only a few seconds of unseen speaker voice <b>prompts.</b> However, all previous work has been developed for cloud-based systems. Taking autoregressive models as an example, although these approaches achieve high-fidelity voice cloning, they fall short in terms of inference speed, model size, and robustness. Therefore, we propose MobileSpeech, which is a fast, lightweight, and robust <b>zero-shot</b> <b>text-to-speech</b> system based on mobile devices for the first time. Specifically: 1) leveraging discrete codec, we design a parallel speech mask decoder module called SMD, which incorporates hierarchical information from the speech codec and weight mechanisms across different codec layers during the generation process. Moreover, to bridge the gap between text and speech, we introduce a high-level probabilistic mask that simulates the progression of information flow from less to more during speech generation. 2) For speaker <b>prompts,</b> we extract fine-grained <b>prompt</b> duration from the <b>prompt</b> speech and incorporate text, <b>prompt</b> speech by cross attention in SMD. We demonstrate the effectiveness of MobileSpeech on multilingual datasets at different levels, achieving state-of-the-art results in terms of generating speed and speech quality. MobileSpeech achieves RTF of 0.09 on a single A100 GPU and we have successfully deployed MobileSpeech on mobile devices. Audio samples are available at \url{https://mobilespeech.github.io/} .</p></p class="citation"></blockquote><h3 id=44--197261-sound-field-reconstruction-using-a-compact-acoustics-informed-neural-network-fei-ma-et-al-2024>(4/4 | 197/261) Sound Field Reconstruction Using a Compact Acoustics-informed Neural Network (Fei Ma et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fei Ma, Sipei Zhao, Ian S. Burnett. (2024)<br><strong>Sound Field Reconstruction Using a Compact Acoustics-informed Neural Network</strong><br><button class=copy-to-clipboard title="Sound Field Reconstruction Using a Compact Acoustics-informed Neural Network" index=197>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-197 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.AS<br>Categories: cs-SD, eess-AS, eess.AS<br>Keyword Score: 10<br>Keywords: Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08904v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08904v1.pdf filename=2402.08904v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Sound field reconstruction (SFR) augments the information of a sound field captured by a microphone array. Conventional SFR methods using basis function decomposition are straightforward and computationally efficient, but may require more microphones than needed to measure the sound field. Recent studies show that pure data-driven and learning-based methods are promising in some SFR tasks, but they are usually computationally heavy and may fail to reconstruct a physically valid sound field. This paper proposes a compact acoustics-informed neural network (AINN) method for SFR, whereby the Helmholtz equation is exploited to regularize the neural network. As opposed to pure data-driven approaches that solely rely on measured sound pressures, the integration of the Helmholtz equation improves robustness of the neural network against variations during the measurement processes and <b>prompts</b> the generation of physically valid reconstructions. The AINN is designed to be compact, and is able to predict not only the sound pressures but also sound pressure gradients within a spatial region of interest based on measured sound pressures along the boundary. Numerical experiments with acoustic transfer functions measured in different environments demonstrate the superiority of the AINN method over the traditional cylinder harmonic decomposition and the singular value decomposition methods.</p></p class="citation"></blockquote><h2 id=eessiv-5>eess.IV (5)</h2><h3 id=15--198261-omnimedvqa-a-new-large-scale-comprehensive-evaluation-benchmark-for-medical-lvlm-yutao-hu-et-al-2024>(1/5 | 198/261) OmniMedVQA: A New Large-Scale Comprehensive Evaluation Benchmark for Medical LVLM (Yutao Hu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yutao Hu, Tianbin Li, Quanfeng Lu, Wenqi Shao, Junjun He, Yu Qiao, Ping Luo. (2024)<br><strong>OmniMedVQA: A New Large-Scale Comprehensive Evaluation Benchmark for Medical LVLM</strong><br><button class=copy-to-clipboard title="OmniMedVQA: A New Large-Scale Comprehensive Evaluation Benchmark for Medical LVLM" index=198>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-198 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 49<br>Keywords: Benchmarking, Multi-modal, Multi-modal, Question Answering, Visual Question Answering, Visual Question Answering, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09181v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09181v1.pdf filename=2402.09181v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Large <b>Vision-Language</b> Models (LVLMs) have demonstrated remarkable capabilities in various <b>multimodal</b> tasks. However, their potential in the medical domain remains largely unexplored. A significant challenge arises from the scarcity of diverse medical images spanning various modalities and anatomical regions, which is essential in real-world medical applications. To solve this problem, in this paper, we introduce OmniMedVQA, a novel comprehensive medical <b>Visual</b> <b>Question</b> <b>Answering</b> <b>(VQA)</b> <b>benchmark.</b> This <b>benchmark</b> is collected from 75 different medical datasets, including 12 different modalities and covering more than 20 distinct anatomical regions. Importantly, all images in this <b>benchmark</b> are sourced from authentic medical scenarios, ensuring alignment with the requirements of the medical field and suitability for evaluating LVLMs. Through our extensive experiments, we have found that existing LVLMs struggle to address these medical <b>VQA</b> problems effectively. Moreover, what surprises us is that medical-specialized LVLMs even exhibit inferior performance to those general-domain models, calling for a more versatile and robust LVLM in the biomedical field. The evaluation results not only reveal the current limitations of LVLM in understanding real medical images but also highlight our dataset&rsquo;s significance. Our dataset will be made publicly available.</p></p class="citation"></blockquote><h3 id=25--199261-destripecyclegan-stripe-simulation-cyclegan-for-unsupervised-infrared-image-destriping-shiqi-yang-et-al-2024>(2/5 | 199/261) DestripeCycleGAN: Stripe Simulation CycleGAN for Unsupervised Infrared Image Destriping (Shiqi Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shiqi Yang, Hanlin Qin, Shuai Yuan, Xiang Yan, Hossein Rahmani. (2024)<br><strong>DestripeCycleGAN: Stripe Simulation CycleGAN for Unsupervised Infrared Image Destriping</strong><br><button class=copy-to-clipboard title="DestripeCycleGAN: Stripe Simulation CycleGAN for Unsupervised Infrared Image Destriping" index=199>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-199 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 30<br>Keywords: Simulation, Simulator, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09101v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09101v1.pdf filename=2402.09101v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>CycleGAN has been proven to be an advanced approach for <b>unsupervised</b> image restoration. This framework consists of two generators: a denoising one for inference and an auxiliary one for modeling noise to fulfill cycle-consistency constraints. However, when applied to the infrared destriping task, it becomes challenging for the vanilla auxiliary generator to consistently produce vertical noise under <b>unsupervised</b> constraints. This poses a threat to the effectiveness of the cycle-consistency loss, leading to stripe noise residual in the denoised image. To address the above issue, we present a novel framework for single-frame infrared image destriping, named DestripeCycleGAN. In this model, the conventional auxiliary generator is replaced with a priori stripe generation model (SGM) to introduce vertical stripe noise in the clean data, and the gradient map is employed to re-establish cycle-consistency. Meanwhile, a Haar wavelet background guidance module (HBGM) has been designed to minimize the divergence of background details between the different domains. To preserve vertical edges, a multi-level wavelet U-Net (MWUNet) is proposed as the denoising generator, which utilizes the Haar wavelet transform as the sampler to decline directional information loss. Moreover, it incorporates the group fusion block (GFB) into skip connections to fuse the multi-scale features and build the context of long-distance dependencies. Extensive experiments on real and synthetic data demonstrate that our DestripeCycleGAN surpasses the state-of-the-art methods in terms of visual quality and quantitative evaluation. Our code will be made public at <a href=https://github.com/0wuji/DestripeCycleGAN>https://github.com/0wuji/DestripeCycleGAN</a>.</p></p class="citation"></blockquote><h3 id=35--200261-tai-gan-a-temporally-and-anatomically-informed-generative-adversarial-network-for-early-to-late-frame-conversion-in-dynamic-cardiac-pet-inter-frame-motion-correction-xueqi-guo-et-al-2024>(3/5 | 200/261) TAI-GAN: A Temporally and Anatomically Informed Generative Adversarial Network for early-to-late frame conversion in dynamic cardiac PET inter-frame motion correction (Xueqi Guo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xueqi Guo, Luyao Shi, Xiongchao Chen, Qiong Liu, Bo Zhou, Huidong Xie, Yi-Hwa Liu, Richard Palyo, Edward J. Miller, Albert J. Sinusas, Lawrence H. Staib, Bruce Spottiswoode, Chi Liu, Nicha C. Dvornek. (2024)<br><strong>TAI-GAN: A Temporally and Anatomically Informed Generative Adversarial Network for early-to-late frame conversion in dynamic cardiac PET inter-frame motion correction</strong><br><button class=copy-to-clipboard title="TAI-GAN: A Temporally and Anatomically Informed Generative Adversarial Network for early-to-late frame conversion in dynamic cardiac PET inter-frame motion correction" index=200>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-200 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 10<br>Keywords: Generative Adversarial Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09567v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09567v1.pdf filename=2402.09567v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Inter-frame motion in dynamic cardiac positron emission tomography (PET) using rubidium-82 (82-Rb) myocardial perfusion imaging impacts myocardial blood flow (MBF) quantification and the diagnosis accuracy of coronary artery diseases. However, the high cross-frame distribution variation due to rapid tracer kinetics poses a considerable challenge for inter-frame motion correction, especially for early frames where intensity-based image registration techniques often fail. To address this issue, we propose a novel method called Temporally and Anatomically Informed <b>Generative</b> <b>Adversarial</b> <b>Network</b> (TAI-GAN) that utilizes an all-to-one mapping to convert early frames into those with tracer distribution similar to the last reference frame. The TAI-GAN consists of a feature-wise linear modulation layer that encodes channel-wise parameters generated from temporal information and rough cardiac segmentation masks with local shifts that serve as anatomical information. Our proposed method was evaluated on a clinical 82-Rb PET dataset, and the results show that our TAI-GAN can produce converted early frames with high image quality, comparable to the real reference frames. After TAI-GAN conversion, the motion estimation accuracy and subsequent myocardial blood flow (MBF) quantification with both conventional and deep learning-based motion correction methods were improved compared to using the original frames.</p></p class="citation"></blockquote><h3 id=45--201261-pruning-sparse-tensor-neural-networks-enables-deep-learning-for-3d-ultrasound-localization-microscopy-brice-rauby-et-al-2024>(4/5 | 201/261) Pruning Sparse Tensor Neural Networks Enables Deep Learning for 3D Ultrasound Localization Microscopy (Brice Rauby et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Brice Rauby, Paul Xing, Jonathan Porée, Maxime Gasse, Jean Provost. (2024)<br><strong>Pruning Sparse Tensor Neural Networks Enables Deep Learning for 3D Ultrasound Localization Microscopy</strong><br><button class=copy-to-clipboard title="Pruning Sparse Tensor Neural Networks Enables Deep Learning for 3D Ultrasound Localization Microscopy" index=201>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-201 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: I-4-9, cs-CV, eess-IV, eess.IV<br>Keyword Score: 10<br>Keywords: Pruning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09359v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09359v1.pdf filename=2402.09359v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Ultrasound Localization Microscopy (ULM) is a non-invasive technique that allows for the imaging of micro-vessels in vivo, at depth and with a resolution on the order of ten microns. ULM is based on the sub-resolution localization of individual microbubbles injected in the bloodstream. Mapping the whole angioarchitecture requires the accumulation of microbubbles trajectories from thousands of frames, typically acquired over a few minutes. ULM acquisition times can be reduced by increasing the microbubble concentration, but requires more advanced algorithms to detect them individually. Several deep learning approaches have been proposed for this task, but they remain limited to 2D imaging, in part due to the associated large memory requirements. Herein, we propose to use sparse tensor neural networks to reduce memory usage in 2D and to improve the scaling of the memory requirement for the extension of deep learning architecture to 3D. We study several approaches to efficiently convert ultrasound data into a sparse format and study the impact of the associated loss of information. When applied in 2D, the sparse formulation reduces the memory requirements by a factor 2 at the cost of a small reduction of performance when compared against dense networks. In 3D, the proposed approach reduces memory requirements by two order of magnitude while largely outperforming conventional ULM in high concentration settings. We show that Sparse Tensor Neural Networks in 3D ULM allow for the same benefits as dense deep learning based method in 2D ULM i.e. the use of higher concentration in silico and reduced acquisition time.</p></p class="citation"></blockquote><h3 id=55--202261-deep-rib-fracture-instance-segmentation-and-classification-from-ct-on-the-ribfrac-challenge-jiancheng-yang-et-al-2024>(5/5 | 202/261) Deep Rib Fracture Instance Segmentation and Classification from CT on the RibFrac Challenge (Jiancheng Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiancheng Yang, Rui Shi, Liang Jin, Xiaoyang Huang, Kaiming Kuang, Donglai Wei, Shixuan Gu, Jianying Liu, Pengfei Liu, Zhizhong Chai, Yongjie Xiao, Hao Chen, Liming Xu, Bang Du, Xiangyi Yan, Hao Tang, Adam Alessio, Gregory Holste, Jiapeng Zhang, Xiaoming Wang, Jianye He, Lixuan Che, Hanspeter Pfister, Ming Li, Bingbing Ni. (2024)<br><strong>Deep Rib Fracture Instance Segmentation and Classification from CT on the RibFrac Challenge</strong><br><button class=copy-to-clipboard title="Deep Rib Fracture Instance Segmentation and Classification from CT on the RibFrac Challenge" index=202>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-202 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-AI, cs-CV, eess-IV, eess.IV<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09372v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09372v1.pdf filename=2402.09372v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Rib fractures are a common and potentially severe injury that can be challenging and labor-intensive to detect in CT scans. While there have been efforts to address this field, the lack of large-scale annotated datasets and evaluation <b>benchmarks</b> has hindered the development and validation of deep learning algorithms. To address this issue, the RibFrac Challenge was introduced, providing a <b>benchmark</b> dataset of over 5,000 rib fractures from 660 CT scans, with voxel-level instance mask annotations and diagnosis labels for four clinical categories (buckle, nondisplaced, displaced, or segmental). The challenge includes two tracks: a detection (instance segmentation) track evaluated by an FROC-style metric and a classification track evaluated by an F1-style metric. During the MICCAI 2020 challenge period, 243 results were evaluated, and seven teams were invited to participate in the challenge summary. The analysis revealed that several top rib fracture detection solutions achieved performance comparable or even better than human experts. Nevertheless, the current rib fracture classification solutions are hardly clinically applicable, which can be an interesting area in the future. As an active <b>benchmark</b> and research resource, the data and online evaluation of the RibFrac Challenge are available at the challenge website. As an independent contribution, we have also extended our previous internal baseline by incorporating recent advancements in large-scale pretrained networks and point-based rib segmentation techniques. The resulting FracNet+ demonstrates competitive performance in rib fracture detection, which lays a foundation for further research and development in AI-assisted rib fracture detection and diagnosis.</p></p class="citation"></blockquote><h2 id=eesssp-3>eess.SP (3)</h2><h3 id=13--203261-lightweight-deep-learning-based-channel-estimation-for-extremely-large-scale-massive-mimo-systems-shen-gao-et-al-2024>(1/3 | 203/261) Lightweight Deep Learning Based Channel Estimation for Extremely Large-Scale Massive MIMO Systems (Shen Gao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shen Gao, Peihao Dong, Zhiwen Pan, Xiaohu You. (2024)<br><strong>Lightweight Deep Learning Based Channel Estimation for Extremely Large-Scale Massive MIMO Systems</strong><br><button class=copy-to-clipboard title="Lightweight Deep Learning Based Channel Estimation for Extremely Large-Scale Massive MIMO Systems" index=203>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-203 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SP<br>Categories: cs-IT, eess-SP, eess.SP, math-IT<br>Keyword Score: 40<br>Keywords: Pruning, Quantization, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08916v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08916v1.pdf filename=2402.08916v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Extremely large-scale massive multiple-input multiple-output (XL-MIMO) systems introduce the much higher channel dimensionality and incur the additional near-field propagation effect, aggravating the computation load and the difficulty to acquire the prior knowledge for channel estimation. In this article, an XL-MIMO channel network (XLCNet) is developed to estimate the high-dimensional channel, which is a universal solution for both the near-field users and far-field users with different channel statistics. Furthermore, a compressed XLCNet (C-XLCNet) is designed via weight <b>pruning</b> and <b>quantization</b> to accelerate the model inference as well as to facilitate the model storage and transmission. <b>Simulation</b> results show the performance superiority and universality of XLCNet. Compared to XLCNet, C-XLCNet incurs the limited performance loss while reducing the computational complexity and model size by about $10 \times$ and $36 \times$, respectively.</p></p class="citation"></blockquote><h3 id=23--204261-dynamic-cooperative-mac-optimization-in-rsu-enhanced-vanets-a-distributed-approach-zhou-zhang-et-al-2024>(2/3 | 204/261) Dynamic Cooperative MAC Optimization in RSU-Enhanced VANETs: A Distributed Approach (Zhou Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhou Zhang, Saman Atapattu, Yizhu Wang, Sumei Sun, Kandeepan Sithamparanathan. (2024)<br><strong>Dynamic Cooperative MAC Optimization in RSU-Enhanced VANETs: A Distributed Approach</strong><br><button class=copy-to-clipboard title="Dynamic Cooperative MAC Optimization in RSU-Enhanced VANETs: A Distributed Approach" index=204>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-204 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SP<br>Categories: cs-NI, eess-SP, eess.SP, math-ST, stat-TH<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09619v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09619v1.pdf filename=2402.09619v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents an optimization approach for cooperative Medium Access Control (MAC) techniques in Vehicular Ad Hoc Networks (VANETs) equipped with Roadside Unit (RSU) to enhance network throughput. Our method employs a distributed cooperative MAC scheme based on Carrier Sense Multiple Access with Collision Avoidance (CSMA/CA) protocol, featuring selective RSU probing and adaptive transmission. It utilizes a dual timescale channel access framework, with a <code>large-scale'' phase accounting for gradual changes in vehicle locations and a </code>small-scale&rsquo;&rsquo; phase adapting to rapid channel fluctuations. We propose the RSU Probing and Cooperative Access (RPCA) strategy, a two-stage approach based on dynamic inter-vehicle distances from the RSU. Using optimal sequential planned decision theory, we rigorously prove its optimality in maximizing average system throughput per large-scale phase. For practical implementation in VANETs, we develop a distributed MAC algorithm with periodic location updates. It adjusts thresholds based on inter-vehicle and vehicle-RSU distances during the large-scale phase and accesses channels following the RPCA strategy with updated thresholds during the small-scale phase. <b>Simulation</b> results confirm the effectiveness and efficiency of our algorithm.</p></p class="citation"></blockquote><h3 id=33--205261-joint-and-robust-beamforming-framework-for-integrated-sensing-and-communication-systems-jinseok-choi-et-al-2024>(3/3 | 205/261) Joint and Robust Beamforming Framework for Integrated Sensing and Communication Systems (Jinseok Choi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jinseok Choi, Jeonghun Park, Namyoon Lee, Ahmed Alkhateeb. (2024)<br><strong>Joint and Robust Beamforming Framework for Integrated Sensing and Communication Systems</strong><br><button class=copy-to-clipboard title="Joint and Robust Beamforming Framework for Integrated Sensing and Communication Systems" index=205>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-205 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SP<br>Categories: cs-IT, eess-SP, eess.SP, math-IT<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09155v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09155v1.pdf filename=2402.09155v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Integrated sensing and communication (ISAC) is widely recognized as a fundamental enabler for future wireless communications. In this paper, we present a joint communication and radar beamforming framework for maximizing a sum spectral efficiency (SE) while guaranteeing desired radar performance with imperfect channel state information (CSI) in multi-user and multi-target ISAC systems. To this end, we adopt either a radar transmit beam mean square error (MSE) or receive signal-to-clutter-plus-noise ratio (SCNR) as a radar performance constraint of a sum SE maximization problem. To resolve inherent challenges such as non-convexity and imperfect CSI, we reformulate the problems and identify first-order optimality conditions for the joint radar and communication beamformer. Turning the condition to a nonlinear eigenvalue problem with eigenvector dependency (NEPv), we develop an alternating method which finds the joint beamformer through power iteration and a Lagrangian multiplier through binary search. The proposed framework encompasses both the radar metrics and is robust to channel estimation error with low complexity. <b>Simulations</b> validate the proposed methods. In particular, we observe that the MSE and SCNR constraints exhibit complementary performance depending on the operating environment, which manifests the importance of the proposed comprehensive and robust optimization framework.</p></p class="citation"></blockquote><h2 id=csma-2>cs.MA (2)</h2><h3 id=12--206261-abides-economist-agent-based-simulation-of-economic-systems-with-learning-agents-kshama-dwarakanath-et-al-2024>(1/2 | 206/261) ABIDES-Economist: Agent-Based Simulation of Economic Systems with Learning Agents (Kshama Dwarakanath et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kshama Dwarakanath, Svitlana Vyetrenko, Peyman Tavallali, Tucker Balch. (2024)<br><strong>ABIDES-Economist: Agent-Based Simulation of Economic Systems with Learning Agents</strong><br><button class=copy-to-clipboard title="ABIDES-Economist: Agent-Based Simulation of Economic Systems with Learning Agents" index=206>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-206 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.MA<br>Categories: cs-MA, cs.MA, econ-GN, q-fin-EC<br>Keyword Score: 30<br>Keywords: Reinforcement Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09563v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09563v1.pdf filename=2402.09563v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce a multi-agent simulator for economic systems comprised of heterogeneous Households, heterogeneous Firms, Central Bank and Government agents, that could be subjected to exogenous, stochastic shocks. The interaction between agents defines the production and consumption of goods in the economy alongside the flow of money. Each agent can be designed to act according to fixed, rule-based strategies or learn their strategies using interactions with others in the simulator. We ground our simulator by choosing agent heterogeneity parameters based on economic literature, while designing their action spaces in accordance with real data in the United States. Our simulator facilitates the use of <b>reinforcement</b> <b>learning</b> strategies for the agents via an OpenAI Gym style environment definition for the economic system. We demonstrate the utility of our simulator by simulating and analyzing two hypothetical (yet interesting) economic scenarios. The first scenario investigates the impact of heterogeneous household skills on their learned preferences to work at different firms. The second scenario examines the impact of a positive production shock to one of two firms on its pricing strategy in comparison to the second firm. We aspire that our platform sets a stage for subsequent research at the intersection of artificial intelligence and economics.</p></p class="citation"></blockquote><h3 id=22--207261-discovering-sensorimotor-agency-in-cellular-automata-using-diversity-search-gautier-hamon-et-al-2024>(2/2 | 207/261) Discovering Sensorimotor Agency in Cellular Automata using Diversity Search (Gautier Hamon et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Gautier Hamon, Mayalen Etcheverry, Bert Wang-Chak Chan, Clément Moulin-Frier, Pierre-Yves Oudeyer. (2024)<br><strong>Discovering Sensorimotor Agency in Cellular Automata using Diversity Search</strong><br><button class=copy-to-clipboard title="Discovering Sensorimotor Agency in Cellular Automata using Diversity Search" index=207>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-207 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.MA<br>Categories: cs-AI, cs-LG, cs-MA, cs.MA<br>Keyword Score: 30<br>Keywords: Curriculum Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10236v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10236v1.pdf filename=2402.10236v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The research field of Artificial Life studies how life-like phenomena such as autopoiesis, agency, or self-regulation can self-organize in computer <b>simulations.</b> In cellular automata (CA), a key open-question has been whether it it is possible to find environment rules that self-organize robust &ldquo;individuals&rdquo; from an initial state with no prior existence of things like &ldquo;bodies&rdquo;, &ldquo;brain&rdquo;, &ldquo;perception&rdquo; or &ldquo;action&rdquo;. In this paper, we leverage recent advances in machine learning, combining algorithms for diversity search, <b>curriculum</b> <b>learning</b> and gradient descent, to automate the search of such &ldquo;individuals&rdquo;, i.e. localized structures that move around with the ability to react in a coherent manner to external obstacles and maintain their integrity, hence primitive forms of sensorimotor agency. We show that this approach enables to find systematically environmental conditions in CA leading to self-organization of such basic forms of agency. Through multiple experiments, we show that the discovered agents have surprisingly robust capabilities to move, maintain their body integrity and navigate among various obstacles. They also show strong generalization abilities, with robustness to changes of scale, random updates or perturbations from the environment not seen during training. We discuss how this approach opens new perspectives in AI and synthetic bioengineering.</p></p class="citation"></blockquote><h2 id=physicsplasm-ph-1>physics.plasm-ph (1)</h2><h3 id=11--208261-active-disruption-avoidance-and-trajectory-design-for-tokamak-ramp-downs-with-neural-differential-equations-and-reinforcement-learning-allen-m-wang-et-al-2024>(1/1 | 208/261) Active Disruption Avoidance and Trajectory Design for Tokamak Ramp-downs with Neural Differential Equations and Reinforcement Learning (Allen M. Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Allen M. Wang, Oswin So, Charles Dawson, Darren T. Garnier, Cristina Rea, Chuchu Fan. (2024)<br><strong>Active Disruption Avoidance and Trajectory Design for Tokamak Ramp-downs with Neural Differential Equations and Reinforcement Learning</strong><br><button class=copy-to-clipboard title="Active Disruption Avoidance and Trajectory Design for Tokamak Ramp-downs with Neural Differential Equations and Reinforcement Learning" index=208>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-208 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: physics.plasm-ph<br>Categories: cs-LG, physics-plasm-ph, physics.plasm-ph<br>Keyword Score: 30<br>Keywords: Reinforcement Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09387v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09387v1.pdf filename=2402.09387v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The tokamak offers a promising path to fusion energy, but plasma disruptions pose a major economic risk, motivating considerable advances in disruption avoidance. This work develops a <b>reinforcement</b> <b>learning</b> approach to this problem by training a policy to safely ramp-down the plasma current while avoiding limits on a number of quantities correlated with disruptions. The policy training environment is a hybrid physics and machine learning model trained on <b>simulations</b> of the SPARC primary reference discharge (PRD) ramp-down, an upcoming burning plasma scenario which we use as a testbed. To address physics uncertainty and model inaccuracies, the <b>simulation</b> environment is massively parallelized on GPU with randomized physics parameters during policy training. The trained policy is then successfully transferred to a higher fidelity simulator where it successfully ramps down the plasma while avoiding user-specified disruptive limits. We also address the crucial issue of safety criticality by demonstrating that a constraint-conditioned policy can be used as a trajectory design assistant to design a library of feed-forward trajectories to handle different physics conditions and user settings. As a library of trajectories is more interpretable and verifiable offline, we argue such an approach is a promising path for leveraging the capabilities of <b>reinforcement</b> <b>learning</b> in the safety-critical context of burning plasma tokamaks. Finally, we demonstrate how the training environment can be a useful platform for other feed-forward optimization approaches by using an evolutionary algorithm to perform optimization of feed-forward trajectories that are robust to physics uncertainty</p></p class="citation"></blockquote><h2 id=hep-ph-1>hep-ph (1)</h2><h3 id=11--209261-a-language-model-for-particle-tracking-andris-huang-et-al-2024>(1/1 | 209/261) A Language Model for Particle Tracking (Andris Huang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Andris Huang, Yash Melkani, Paolo Calafiura, Alina Lazar, Daniel Thomas Murnane, Minh-Tuan Pham, Xiangyang Ju. (2024)<br><strong>A Language Model for Particle Tracking</strong><br><button class=copy-to-clipboard title="A Language Model for Particle Tracking" index=209>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-209 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: hep-ph<br>Categories: cs-LG, hep-ex, hep-ph, hep-ph<br>Keyword Score: 30<br>Keywords: Supervised Learning, Supervised Learning, BERT<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10239v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10239v1.pdf filename=2402.10239v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Particle tracking is crucial for almost all physics analysis programs at the Large Hadron Collider. Deep learning models are pervasively used in particle tracking related tasks. However, the current practice is to design and train one deep learning model for one task with <b>supervised</b> <b>learning</b> techniques. The trained models work well for tasks they are trained on but show no or little generalization capabilities. We propose to unify these models with a language model. In this paper, we present a tokenized detector representation that allows us to train a <b>BERT</b> model for particle tracking. The trained <b>BERT</b> model, namely TrackingBERT, offers latent detector module embedding that can be used for other tasks. This work represents the first step towards developing a foundational model for particle detector understanding.</p></p class="citation"></blockquote><h2 id=csse-8>cs.SE (8)</h2><h3 id=18--210261-trained-without-my-consent-detecting-code-inclusion-in-language-models-trained-on-code-vahid-majdinasab-et-al-2024>(1/8 | 210/261) Trained Without My Consent: Detecting Code Inclusion In Language Models Trained on Code (Vahid Majdinasab et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Vahid Majdinasab, Amin Nikanjam, Foutse Khomh. (2024)<br><strong>Trained Without My Consent: Detecting Code Inclusion In Language Models Trained on Code</strong><br><button class=copy-to-clipboard title="Trained Without My Consent: Detecting Code Inclusion In Language Models Trained on Code" index=210>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-210 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-LG, cs-SE, cs.SE<br>Keyword Score: 30<br>Keywords: Low-Resource, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09299v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09299v1.pdf filename=2402.09299v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Code auditing ensures that the developed code adheres to standards, regulations, and copyright protection by verifying that it does not contain code from protected sources. The recent advent of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> as coding assistants in the software development process poses new challenges for code auditing. The dataset for training these models is mainly collected from publicly available sources. This raises the issue of intellectual property infringement as developers&rsquo; codes are already included in the dataset. Therefore, auditing code developed using <b>LLMs</b> is challenging, as it is difficult to reliably assert if an <b>LLM</b> used during development has been trained on specific copyrighted codes, given that we do not have access to the training datasets of these models. Given the non-disclosure of the training datasets, traditional approaches such as code clone detection are insufficient for asserting copyright infringement. To address this challenge, we propose a new approach, TraWiC; a model-agnostic and interpretable method based on membership inference for detecting code inclusion in an <b>LLM&rsquo;s</b> training dataset. We extract syntactic and semantic identifiers unique to each program to train a classifier for detecting code inclusion. In our experiments, we observe that TraWiC is capable of detecting 83.87% of codes that were used to train an <b>LLM.</b> In comparison, the prevalent clone detection tool NiCad is only capable of detecting 47.64%. In addition to its remarkable performance, TraWiC has low resource overhead in contrast to pair-wise clone detection that is conducted during the auditing process of tools like CodeWhisperer reference tracker, across thousands of code snippets.</p></p class="citation"></blockquote><h3 id=28--211261-automated-unit-test-improvement-using-large-language-models-at-meta-nadia-alshahwan-et-al-2024>(2/8 | 211/261) Automated Unit Test Improvement using Large Language Models at Meta (Nadia Alshahwan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nadia Alshahwan, Jubin Chheda, Anastasia Finegenova, Beliz Gokkaya, Mark Harman, Inna Harper, Alexandru Marginean, Shubho Sengupta, Eddy Wang. (2024)<br><strong>Automated Unit Test Improvement using Large Language Models at Meta</strong><br><button class=copy-to-clipboard title="Automated Unit Test Improvement using Large Language Models at Meta" index=211>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-211 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-SE, cs.SE<br>Keyword Score: 30<br>Keywords: Recommendation, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09171v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09171v1.pdf filename=2402.09171v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper describes Meta&rsquo;s TestGen-LLM tool, which uses <b>LLMs</b> to automatically improve existing human-written tests. TestGen-LLM verifies that its generated test classes successfully clear a set of filters that assure measurable improvement over the original test suite, thereby eliminating problems due to <b>LLM</b> hallucination. We describe the deployment of TestGen-LLM at Meta test-a-thons for the Instagram and Facebook platforms. In an evaluation on Reels and Stories products for Instagram, 75% of TestGen-LLM&rsquo;s test cases built correctly, 57% passed reliably, and 25% increased coverage. During Meta&rsquo;s Instagram and Facebook test-a-thons, it improved 11.5% of all classes to which it was applied, with 73% of its <b>recommendations</b> being accepted for production deployment by Meta software engineers. We believe this is the first report on industrial scale deployment of <b>LLM-generated</b> code backed by such assurances of code improvement.</p></p class="citation"></blockquote><h3 id=38--212261-assessing-ai-based-code-assistants-in-method-generation-tasks-vincenzo-corso-et-al-2024>(3/8 | 212/261) Assessing AI-Based Code Assistants in Method Generation Tasks (Vincenzo Corso et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Vincenzo Corso, Leonardo Mariani, Daniela Micucci, Oliviero Riganelli. (2024)<br><strong>Assessing AI-Based Code Assistants in Method Generation Tasks</strong><br><button class=copy-to-clipboard title="Assessing AI-Based Code Assistants in Method Generation Tasks" index=212>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-212 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-SE, cs.SE<br>Keyword Score: 20<br>Keywords: Bard, ChatGPT<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09022v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09022v1.pdf filename=2402.09022v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>AI-based code assistants are increasingly popular as a means to enhance productivity and improve code quality. This study compares four AI-based code assistants, GitHub Copilot, Tabnine, <b>ChatGPT,</b> and Google <b>Bard,</b> in method generation tasks, assessing their ability to produce accurate, correct, and efficient code. Results show that code assistants are useful, with complementary capabilities, although they rarely generate ready-to-use correct code.</p></p class="citation"></blockquote><h3 id=48--213261-enhancing-source-code-representations-for-deep-learning-with-static-analysis-xueting-guan-et-al-2024>(4/8 | 213/261) Enhancing Source Code Representations for Deep Learning with Static Analysis (Xueting Guan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xueting Guan, Christoph Treude. (2024)<br><strong>Enhancing Source Code Representations for Deep Learning with Static Analysis</strong><br><button class=copy-to-clipboard title="Enhancing Source Code Representations for Deep Learning with Static Analysis" index=213>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-213 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-SE, cs.SE<br>Keyword Score: 10<br>Keywords: Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09557v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09557v1.pdf filename=2402.09557v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep learning techniques applied to program analysis tasks such as code classification, <b>summarization,</b> and bug detection have seen widespread interest. Traditional approaches, however, treat programming source code as natural language text, which may neglect significant structural or semantic details. Additionally, most current methods of representing source code focus solely on the code, without considering beneficial additional context. This paper explores the integration of static analysis and additional context such as bug reports and design patterns into source code representations for deep learning models. We use the Abstract Syntax Tree-based Neural Network (ASTNN) method and augment it with additional context information obtained from bug reports and design patterns, creating an enriched source code representation that significantly enhances the performance of common software engineering tasks such as code classification and code clone detection. Utilizing existing open-source code data, our approach improves the representation and processing of source code, thereby improving task performance.</p></p class="citation"></blockquote><h3 id=58--214261-context-composing-for-full-line-code-completion-anton-semenkin-et-al-2024>(5/8 | 214/261) Context Composing for Full Line Code Completion (Anton Semenkin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Anton Semenkin, Yaroslav Sokolov, Evgeniia Vu. (2024)<br><strong>Context Composing for Full Line Code Completion</strong><br><button class=copy-to-clipboard title="Context Composing for Full Line Code Completion" index=214>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-214 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-LG, cs-SE, cs.SE<br>Keyword Score: 10<br>Keywords: Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09230v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09230v1.pdf filename=2402.09230v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Code Completion is one of the most used Integrated Development Environment (IDE) features, which affects the everyday life of a software developer. Modern code completion approaches moved from the composition of several static analysis-based contributors to pipelines that involve neural networks. This change allows the proposal of longer code suggestions while maintaining the relatively short time spent on generation itself. At JetBrains, we put a lot of effort into perfecting the code completion workflow so it can be both helpful and non-distracting for a programmer. We managed to ship the Full Line Code Completion feature to PyCharm Pro IDE and proved its usefulness in A/B testing on hundreds of real Python users. The paper describes our approach to context composing for the <b>Transformer</b> model that is a core of the feature&rsquo;s implementation. In addition to that, we share our next steps to improve the feature and emphasize the importance of several research aspects in the area.</p></p class="citation"></blockquote><h3 id=68--215261-generative-ai-for-pull-request-descriptions-adoption-impact-and-developer-interventions-tao-xiao-et-al-2024>(6/8 | 215/261) Generative AI for Pull Request Descriptions: Adoption, Impact, and Developer Interventions (Tao Xiao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tao Xiao, Hideaki Hata, Christoph Treude, Kenichi Matsumoto. (2024)<br><strong>Generative AI for Pull Request Descriptions: Adoption, Impact, and Developer Interventions</strong><br><button class=copy-to-clipboard title="Generative AI for Pull Request Descriptions: Adoption, Impact, and Developer Interventions" index=215>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-215 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-SE, cs.SE<br>Keyword Score: 10<br>Keywords: Generative AI<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08967v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08967v1.pdf filename=2402.08967v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>GitHub&rsquo;s Copilot for Pull Requests (PRs) is a promising service aiming to automate various developer tasks related to PRs, such as generating summaries of changes or providing complete walkthroughs with links to the relevant code. As this innovative technology gains traction in the Open Source Software (OSS) community, it is crucial to examine its early adoption and its impact on the development process. Additionally, it offers a unique opportunity to observe how developers respond when they disagree with the generated content. In our study, we employ a mixed-methods approach, blending quantitative analysis with qualitative insights, to examine 18,256 PRs in which parts of the descriptions were crafted by <b>generative</b> <b>AI.</b> Our findings indicate that: (1) Copilot for PRs, though in its infancy, is seeing a marked uptick in adoption. (2) PRs enhanced by Copilot for PRs require less review time and have a higher likelihood of being merged. (3) Developers using Copilot for PRs often complement the automated descriptions with their manual input. These results offer valuable insights into the growing integration of <b>generative</b> <b>AI</b> in software development.</p></p class="citation"></blockquote><h3 id=78--216261-quantifying-and-characterizing-clones-of-self-admitted-technical-debt-in-build-systems-tao-xiao-et-al-2024>(7/8 | 216/261) Quantifying and Characterizing Clones of Self-Admitted Technical Debt in Build Systems (Tao Xiao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tao Xiao, Zhili Zeng, Dong Wang, Hideaki Hata, Shane McIntosh, Kenichi Matsumoto. (2024)<br><strong>Quantifying and Characterizing Clones of Self-Admitted Technical Debt in Build Systems</strong><br><button class=copy-to-clipboard title="Quantifying and Characterizing Clones of Self-Admitted Technical Debt in Build Systems" index=216>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-216 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-SE, cs.SE<br>Keyword Score: 10<br>Keywords: Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08920v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08920v1.pdf filename=2402.08920v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Self-Admitted Technical Debt (SATD) annotates development decisions that intentionally exchange long-term software artifact quality for short-term goals. Recent work explores the existence of SATD clones (duplicate or near duplicate SATD comments) in source code. Cloning of SATD in build systems (e.g., CMake and Maven) may propagate suboptimal design choices, threatening qualities of the build system that stakeholders rely upon (e.g., maintainability, reliability, repeatability). Hence, we conduct a large-scale study on 50,608 SATD comments extracted from Autotools, CMake, Maven, and Ant build systems to investigate the prevalence of SATD clones and to characterize their incidences. We observe that: (i) prior work suggests that 41-65% of SATD comments in source code are clones, but in our studied build system context, the rates range from 62% to 95%, suggesting that SATD clones are a more prevalent phenomenon in build systems than in source code; (ii) statements surrounding SATD clones are highly similar, with 76% of occurrences having similarity scores greater than 0.8; (iii) a quarter of SATD clones are introduced by the author of the original SATD statements; and (iv) among the most commonly cloned SATD comments, external factors (e.g., platform and tool configuration) are the most frequent locations, limitations in tools and libraries are the most frequent causes, and developers often copy SATD comments that describe issues to be fixed later. Our work presents the first step toward systematically understanding SATD clones in build systems and opens up avenues for future work, such as distinguishing different SATD clone behavior, as well as designing an automated <b>recommendation</b> system for repaying SATD effectively based on resolved clones.</p></p class="citation"></blockquote><h3 id=88--217261-omnibor-a-system-for-automatic-verifiable-artifact-resolution-across-software-supply-chains-bharathi-seshadri-et-al-2024>(8/8 | 217/261) OmniBOR: A System for Automatic, Verifiable Artifact Resolution across Software Supply Chains (Bharathi Seshadri et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bharathi Seshadri, Yongkui Han, Chris Olson, David Pollak, Vojislav Tomasevic. (2024)<br><strong>OmniBOR: A System for Automatic, Verifiable Artifact Resolution across Software Supply Chains</strong><br><button class=copy-to-clipboard title="OmniBOR: A System for Automatic, Verifiable Artifact Resolution across Software Supply Chains" index=217>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-217 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-CR, cs-SE, cs.SE<br>Keyword Score: 6<br>Keywords: Graph, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08980v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08980v1.pdf filename=2402.08980v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Software supply chain attacks, which exploit the build process or artifacts used in the process of building a software product, are increasingly of concern. To combat these attacks, one must be able to check that every artifact that a software product depends on does not contain vulnerabilities. In this paper, we introduce OmniBOR, (Universal Bill of Receipts) a minimalistic scheme for build tools to create an artifact dependency <b>graph</b> which can be used to track every software artifact incorporated into a built software product. We present the architecture of OmniBOR, the underlying data representations, and two implementations that produce OmniBOR data and embed an OmniBOR Identifier into built software, including a compiler-based approach and one based on tracing the build process. We demonstrate the efficacy of this approach on <b>benchmarks</b> including a Linux distribution for applications such as Common Vulnerabilities and Exposures (CVE) detection and software bill of materials (SBOM) computation.</p></p class="citation"></blockquote><h2 id=astro-phsr-1>astro-ph.SR (1)</h2><h3 id=11--218261-predicting-the-emergence-of-solar-active-regions-using-machine-learning-spiridon-kasapis-et-al-2024>(1/1 | 218/261) Predicting the Emergence of Solar Active Regions Using Machine Learning (Spiridon Kasapis et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Spiridon Kasapis, Irina N. Kitiashvili, Alexander G. Kosovichev, John T. Stefan, Bhairavi Apte. (2024)<br><strong>Predicting the Emergence of Solar Active Regions Using Machine Learning</strong><br><button class=copy-to-clipboard title="Predicting the Emergence of Solar Active Regions Using Machine Learning" index=218>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-218 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: astro-ph.SR<br>Categories: astro-ph-SR, astro-ph.SR, cs-LG<br>Keyword Score: 30<br>Keywords: LSTM, LSTM, LSTM<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08890v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08890v1.pdf filename=2402.08890v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>To create early warning capabilities for upcoming Space Weather disturbances, we have selected a dataset of 61 emerging active regions, which allows us to identify characteristic features in the evolution of acoustic power density to predict continuum intensity emergence. For our study, we have utilized Doppler shift and continuum intensity observations from the Helioseismic and Magnetic Imager (HMI) onboard the Solar Dynamics Observatory (SDO). The local tracking of 30.66 x 30.66-degree patches in the vicinity of active regions allowed us to trace the evolution of active regions starting from the pre-emergence state. We have developed a machine learning model to capture the acoustic power flux density variations associated with upcoming magnetic flux emergence. The trained <b>Long</b> <b>Short-Term</b> <b>Memory</b> <b>(LSTM)</b> model is able to predict 5 hours ahead whether, in a given area of the solar surface, continuum intensity values will decrease. The performed study allows us to investigate the potential of the machine learning approach to predict the emergence of active regions using acoustic power maps as input.</p></p class="citation"></blockquote><h2 id=cssi-4>cs.SI (4)</h2><h3 id=14--219261-signed-diverse-multiplex-networks-clustering-and-inference-marianna-pensky-2024>(1/4 | 219/261) Signed Diverse Multiplex Networks: Clustering and Inference (Marianna Pensky, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Marianna Pensky. (2024)<br><strong>Signed Diverse Multiplex Networks: Clustering and Inference</strong><br><button class=copy-to-clipboard title="Signed Diverse Multiplex Networks: Clustering and Inference" index=219>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-219 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SI<br>Categories: cs-LG, cs-SI, cs.SI, stat-ME<br>Keyword Score: 26<br>Keywords: Graph, Clustering, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10242v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10242v1.pdf filename=2402.10242v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The paper introduces a Signed Generalized Random Dot Product <b>Graph</b> (SGRDPG) model, which is a variant of the Generalized Random Dot Product <b>Graph</b> (GRDPG), where, in addition, edges can be positive or negative. The setting is extended to a multiplex version, where all layers have the same collection of nodes and follow the SGRDPG. The only common feature of the layers of the network is that they can be partitioned into groups with common subspace structures, while otherwise all matrices of connection probabilities can be all different. The setting above is extremely flexible and includes a variety of existing multiplex network models as its particular cases. The paper fulfills two objectives. First, it shows that keeping signs of the edges in the process of network construction leads to a better precision of estimation and <b>clustering</b> and, hence, is beneficial for tackling real world problems such as analysis of brain networks. Second, by employing novel algorithms, our paper ensures equivalent or superior accuracy than has been achieved in simpler multiplex network models. In addition to theoretical guarantees, both of those features are demonstrated using numerical <b>simulations</b> and a real data example.</p></p class="citation"></blockquote><h3 id=24--220261-insights-and-caveats-from-mining-local-and-global-temporal-motifs-in-cryptocurrency-transaction-networks-naomi-a-arnold-et-al-2024>(2/4 | 220/261) Insights and caveats from mining local and global temporal motifs in cryptocurrency transaction networks (Naomi A. Arnold et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Naomi A. Arnold, Peijie Zhong, Cheick Tidiane Ba, Ben Steer, Raul Mondragon, Felix Cuadrado, Renaud Lambiotte, Richard G. Clegg. (2024)<br><strong>Insights and caveats from mining local and global temporal motifs in cryptocurrency transaction networks</strong><br><button class=copy-to-clipboard title="Insights and caveats from mining local and global temporal motifs in cryptocurrency transaction networks" index=220>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-220 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SI<br>Categories: cs-SI, cs.SI<br>Keyword Score: 16<br>Keywords: Graph, Anomaly Detection, Clustering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09272v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09272v1.pdf filename=2402.09272v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Distributed ledger technologies have opened up a wealth of fine-grained transaction data from cryptocurrencies like Bitcoin and Ethereum. This allows research into problems like <b>anomaly</b> <b>detection,</b> anti-money laundering, pattern mining and activity <b>clustering</b> (where data from traditional currencies is rarely available). The formalism of temporal networks offers a natural way of representing this data and offers access to a wealth of metrics and models. However, the large scale of the data presents a challenge using standard <b>graph</b> analysis techniques. We use temporal motifs to analyse two Bitcoin datasets and one NFT dataset, using sequences of three transactions and up to three users. We show that the commonly used technique of simply counting temporal motifs over all users and all time can give misleading conclusions. Here we also study the motifs contributed by each user and discover that the motif distribution is heavy-tailed and that the key players have diverse motif signatures. We study the motifs that occur in different time periods and find events and anomalous activity that cannot be seen just by a count on the whole dataset. Studying motif completion time reveals dynamics driven by human behaviour as well as algorithmic behaviour.</p></p class="citation"></blockquote><h3 id=34--221261-finding-densest-subgraphs-with-edge-color-constraints-lutz-oettershagen-et-al-2024>(3/4 | 221/261) Finding Densest Subgraphs with Edge-Color Constraints (Lutz Oettershagen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lutz Oettershagen, Honglian Wang, Aristides Gionis. (2024)<br><strong>Finding Densest Subgraphs with Edge-Color Constraints</strong><br><button class=copy-to-clipboard title="Finding Densest Subgraphs with Edge-Color Constraints" index=221>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-221 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SI<br>Categories: cs-SI, cs.SI<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09124v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09124v1.pdf filename=2402.09124v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We consider a variant of the densest subgraph problem in networks with single or multiple edge attributes. For example, in a social network, the edge attributes may describe the type of relationship between users, such as friends, family, or acquaintances, or different types of communication. For conceptual simplicity, we view the attributes as edge colors. The new problem we address is to find a diverse densest subgraph that fulfills given requirements on the numbers of edges of specific colors. When searching for a dense social network community, our problem will enforce the requirement that the community is diverse according to criteria specified by the edge attributes. We show that the decision versions for finding exactly, at most, and at least $\textbf{h}$ colored edges densest subgraph, where $\textbf{h}$ is a vector of color requirements, are NP-complete, for already two colors. For the problem of finding a densest subgraph with at least $\textbf{h}$ colored edges, we provide a linear-time constant-factor approximation algorithm when the input <b>graph</b> is sparse. On the way, we introduce the related at least $h$ (non-colored) edges densest subgraph problem, show its hardness, and also provide a linear-time constant-factor approximation. In our experiments, we demonstrate the efficacy and efficiency of our new algorithms.</p></p class="citation"></blockquote><h3 id=44--222261-pearson-correlations-on-networks-corrigendum-michele-coscia-et-al-2024>(4/4 | 222/261) Pearson Correlations on Networks: Corrigendum (Michele Coscia et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Michele Coscia, Karel Devriendt. (2024)<br><strong>Pearson Correlations on Networks: Corrigendum</strong><br><button class=copy-to-clipboard title="Pearson Correlations on Networks: Corrigendum" index=222>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-222 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SI<br>Categories: cs-SI, cs.SI, physics-data-an, physics-soc-ph<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09489v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09489v1.pdf filename=2402.09489v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recently, the first author proposed a measure to calculate Pearson correlations for node values expressed in a network, by taking into account distances or metrics defined on the network. In this technical note, we show that using an arbitrary choice of distances might result in imaginary or unbounded correlation values, which is undesired. We prove that this problem is solved by restricting to a special class of distances: negative type metrics. We also discuss two natural classes of negative type metrics on <b>graphs,</b> for which the network correlations are properly defined.</p></p class="citation"></blockquote><h2 id=quant-ph-2>quant-ph (2)</h2><h3 id=12--223261-efficient-unitary-t-designs-from-random-sums-chi-fang-chen-et-al-2024>(1/2 | 223/261) Efficient Unitary T-designs from Random Sums (Chi-Fang Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chi-Fang Chen, Jordan Docter, Michelle Xu, Adam Bouland, Patrick Hayden. (2024)<br><strong>Efficient Unitary T-designs from Random Sums</strong><br><button class=copy-to-clipboard title="Efficient Unitary T-designs from Random Sums" index=223>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-223 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: quant-ph<br>Categories: cs-DS, math-PR, quant-ph, quant-ph<br>Keyword Score: 26<br>Keywords: Benchmarking, Benchmarking, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09335v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09335v1.pdf filename=2402.09335v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Unitary $T$-designs play an important role in quantum information, with diverse applications in quantum algorithms, <b>benchmarking,</b> tomography, and communication. Until now, the most efficient construction of unitary $T$-designs for $n$-qudit systems has been via random local quantum circuits, which have been shown to converge to approximate $T$-designs in the diamond norm using $O(T^{5+o(1)} n^2)$ quantum gates. In this work, we provide a new construction of $T$-designs via random matrix theory using $\tilde{O}(T^2 n^2)$ quantum gates. Our construction leverages two key ideas. First, in the spirit of central limit theorems, we approximate the Gaussian Unitary Ensemble (GUE) by an i.i.d. sum of random Hermitian matrices. Second, we show that the product of just two exponentiated GUE matrices is already approximately Haar random. Thus, multiplying two exponentiated sums over rather simple random matrices yields a unitary $T$-design, via Hamiltonian <b>simulation.</b> A central feature of our proof is a new connection between the polynomial method in quantum query complexity and the large-dimension ($N$) expansion in random matrix theory. In particular, we show that the polynomial method provides exponentially improved bounds on the high moments of certain random matrix ensembles, without requiring intricate Weingarten calculations. In doing so, we define and solve a new type of moment problem on the unit circle, asking whether a finite number of equally weighted points, corresponding to eigenvalues of unitary matrices, can reproduce a given set of moments.</p></p class="citation"></blockquote><h3 id=22--224261-guided-quantum-compression-for-higgs-identification-vasilis-belis-et-al-2024>(2/2 | 224/261) Guided Quantum Compression for Higgs Identification (Vasilis Belis et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Vasilis Belis, Patrick Odagiu, Michele Grossi, Florentin Reiter, Günther Dissertori, Sofia Vallecorsa. (2024)<br><strong>Guided Quantum Compression for Higgs Identification</strong><br><button class=copy-to-clipboard title="Guided Quantum Compression for Higgs Identification" index=224>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-224 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: quant-ph<br>Categories: cs-LG, hep-ex, quant-ph, quant-ph<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09524v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09524v1.pdf filename=2402.09524v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Quantum machine learning provides a fundamentally novel and promising approach to analyzing data. However, many data sets are too complex for currently available quantum computers. Consequently, quantum machine learning applications conventionally resort to dimensionality reduction algorithms, e.g., auto-encoders, before passing data through the quantum models. We show that using a classical auto-encoder as an independent preprocessing step can significantly decrease the classification performance of a quantum machine learning algorithm. To ameliorate this issue, we design an architecture that unifies the preprocessing and quantum classification algorithms into a single trainable model: the guided quantum compression model. The utility of this model is demonstrated by using it to identify the Higgs boson in proton-proton collisions at the LHC, where the conventional approach proves ineffective. Conversely, the guided quantum compression model excels at solving this classification problem, achieving a good accuracy. Additionally, the model developed herein shows better performance compared to the classical <b>benchmark</b> when using only low-level kinematic features.</p></p class="citation"></blockquote><h2 id=physicsflu-dyn-1>physics.flu-dyn (1)</h2><h3 id=11--225261-eulerian-formulation-of-the-tensor-based-morphology-equations-for-strain-based-blood-damage-modeling-nico-dirkes-et-al-2024>(1/1 | 225/261) Eulerian Formulation of the Tensor-Based Morphology Equations for Strain-Based Blood Damage Modeling (Nico Dirkes et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nico Dirkes, Fabian Key, Marek Behr. (2024)<br><strong>Eulerian Formulation of the Tensor-Based Morphology Equations for Strain-Based Blood Damage Modeling</strong><br><button class=copy-to-clipboard title="Eulerian Formulation of the Tensor-Based Morphology Equations for Strain-Based Blood Damage Modeling" index=225>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-225 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: physics.flu-dyn<br>Categories: cs-CE, cs-NA, math-NA, physics-flu-dyn, physics.flu-dyn<br>Keyword Score: 23<br>Keywords: Benchmarking, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09319v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09319v1.pdf filename=2402.09319v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The development of blood-handling medical devices, such as ventricular assist devices, requires the analysis of their biocompatibility. Among other aspects, this includes hemolysis, i.e., red blood cell damage. For this purpose, computational fluid dynamics (CFD) methods are employed to predict blood flow in prototypes. The most basic hemolysis models directly estimate red blood cell damage from fluid stress in the resulting flow field. More advanced models explicitly resolve cell deformation. On the downside, these models are typically written in a Lagrangian formulation, i.e., they require pathline tracking. We present a new Eulerian description of cell deformation, enabling the evaluation of the solution across the whole domain. The resulting hemolysis model can be applied to any converged CFD <b>simulation</b> due to one-way coupling with the fluid velocity field. We discuss the efficient numerical treatment of the model equations in a stabilized finite element context. We validate the model by comparison to the original Lagrangian formulation in selected <b>benchmark</b> flows. Two more complex test cases demonstrate the method&rsquo;s capabilities in real-world applications. The results highlight the advantages over previous hemolysis models. In conclusion, the model holds great potential for the design process of future generations of medical devices.</p></p class="citation"></blockquote><h2 id=econem-1>econ.EM (1)</h2><h3 id=11--226261-inference-for-an-algorithmic-fairness-accuracy-frontier-yiqi-liu-et-al-2024>(1/1 | 226/261) Inference for an Algorithmic Fairness-Accuracy Frontier (Yiqi Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yiqi Liu, Francesca Molinari. (2024)<br><strong>Inference for an Algorithmic Fairness-Accuracy Frontier</strong><br><button class=copy-to-clipboard title="Inference for an Algorithmic Fairness-Accuracy Frontier" index=226>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-226 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: econ.EM<br>Categories: cs-LG, econ-EM, econ.EM<br>Keyword Score: 23<br>Keywords: Fairness, Gaussian Process, Sample Size<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08879v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08879v1.pdf filename=2402.08879v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Decision-making processes increasingly rely on the use of algorithms. Yet, algorithms&rsquo; predictive ability frequently exhibit systematic variation across subgroups of the population. While both <b>fairness</b> and accuracy are desirable properties of an algorithm, they often come at the cost of one another. What should a <b>fairness-minded</b> policymaker do then, when confronted with finite data? In this paper, we provide a consistent estimator for a theoretical <b>fairness-accuracy</b> frontier put forward by Liang, Lu and Mu (2023) and propose inference methods to test hypotheses that have received much attention in the <b>fairness</b> literature, such as (i) whether fully excluding a covariate from use in training the algorithm is optimal and (ii) whether there are less discriminatory alternatives to an existing algorithm. We also provide an estimator for the distance between a given algorithm and the fairest point on the frontier, and characterize its asymptotic distribution. We leverage the fact that the <b>fairness-accuracy</b> frontier is part of the boundary of a convex set that can be fully represented by its support function. We show that the estimated support function converges to a tight <b>Gaussian</b> <b>process</b> as the <b>sample</b> <b>size</b> increases, and then express policy-relevant hypotheses as restrictions on the support function to construct valid test statistics.</p></p class="citation"></blockquote><h2 id=mathna-8>math.NA (8)</h2><h3 id=18--227261-numerical-study-of-a-strongly-coupled-two-scale-system-with-nonlinear-dispersion-surendra-nepal-et-al-2024>(1/8 | 227/261) Numerical Study of a Strongly Coupled Two-scale System with Nonlinear Dispersion (Surendra Nepal et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Surendra Nepal, Vishnu Raveendran, Michael Eden, Rainey Lyons, Adrian Muntean. (2024)<br><strong>Numerical Study of a Strongly Coupled Two-scale System with Nonlinear Dispersion</strong><br><button class=copy-to-clipboard title="Numerical Study of a Strongly Coupled Two-scale System with Nonlinear Dispersion" index=227>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-227 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: 65M60, 47J25, 35M30, 35G55, cs-NA, math-AP, math-NA, math.NA<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09607v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09607v1.pdf filename=2402.09607v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Thinking of flows crossing through regular porous media, we numerically explore the behavior of weak solutions to a two-scale elliptic-parabolic system that is strongly coupled by means of a suitable nonlinear dispersion term. The two-scale system of interest originates from the fast-drift periodic homogenization of a nonlinear convective-diffusion-reaction problem, where the structure of the non-linearity in the drift fits to the hydrodynamic limit of a totally asymmetric simple exclusion process for a population of particles. In this article, we focus exclusively on numerical <b>simulations</b> that employ two decoupled approximation schemes, viz. &lsquo;scheme 1&rsquo; - a Picard-type iteration - and &lsquo;scheme 2&rsquo; - a time discretization decoupling. Additionally, we describe a computational strategy which helps to drastically improve computation times. Finally, we provide several numerical experiments to illustrate what dispersion effects are introduced by a specific choice of microstructure and model ingredients.</p></p class="citation"></blockquote><h3 id=28--228261-conformal-finite-element-methods-for-nonlinear-rosenau-burgers-biharmonic-models-ankur-et-al-2024>(2/8 | 228/261) Conformal Finite Element Methods for Nonlinear Rosenau-Burgers-Biharmonic Models (Ankur et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ankur, Ram Jiwari, Akil Narayan. (2024)<br><strong>Conformal Finite Element Methods for Nonlinear Rosenau-Burgers-Biharmonic Models</strong><br><button class=copy-to-clipboard title="Conformal Finite Element Methods for Nonlinear Rosenau-Burgers-Biharmonic Models" index=228>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-228 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: 65M12, 65M60, 65N15, 65N30, cs-NA, math-NA, math.NA<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08926v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08926v1.pdf filename=2402.08926v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present a novel and comparative analysis of finite element discretizations for a nonlinear Rosenau-Burgers model including a biharmonic term. We analyze both continuous and mixed finite element approaches, providing stability, existence, and uniqueness statements of the corresponding variational methods. We also obtain optimal error estimates of the semidiscrete scheme in corresponding B^ochner spaces. Finally, we construct a fully discrete scheme through a backward Euler discretization of the time derivative, and prove well-posedness statements for this fully discrete scheme. Our findings show that the mixed approach removes some theoretical impediments to analysis and is numerically easier to implement. We provide numerical <b>simulations</b> for the mixed formulation approach using $C^0$ Taylor-Hood finite elements on several domains. Our numerical results confirm that the algorithm has optimal convergence in accordance with the observed theoretical results.</p></p class="citation"></blockquote><h3 id=38--229261-a-locally-mass-conservative-enriched-petrov-galerkin-method-without-penalty-for-the-darcy-flow-in-porous-media-huangxin-chen-et-al-2024>(3/8 | 229/261) A locally mass-conservative enriched Petrov-Galerkin method without penalty for the Darcy flow in porous media (Huangxin Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Huangxin Chen, Piaopiao Dong, Shuyu Sun, Zixuan Wang. (2024)<br><strong>A locally mass-conservative enriched Petrov-Galerkin method without penalty for the Darcy flow in porous media</strong><br><button class=copy-to-clipboard title="A locally mass-conservative enriched Petrov-Galerkin method without penalty for the Darcy flow in porous media" index=229>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-229 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: 65M60, 65N30, 76S05, cs-NA, math-NA, math.NA<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08909v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08909v1.pdf filename=2402.08909v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this work we present an enriched Petrov-Galerkin (EPG) method for the <b>simulation</b> of the Darcy flow in porous media. The new method enriches the approximation trial space of the conforming continuous Galerkin (CG) method with bubble functions and enriches the approximation test space of the CG method with piecewise constant functions, and it does not require any penalty term in the weak formulation. Moreover, we propose a framework for constructing the bubble functions and consider a decoupled algorithm for the EPG method based on this framework, which enables the process of solving pressure to be decoupled into two steps. The first step is to solve the pressure by the standard CG method, and the second step is a post-processing correction of the first step. Compared with the CG method, the proposed EPG method is locally mass-conservative, while keeping fewer degrees of freedom than the discontinuous Galerkin (DG) method. In addition, this method is more concise in the error analysis than the enriched Galerkin (EG) method. The coupled flow and transport in porous media is considered to illustrate the advantages of locally mass-conservative properties of the EPG method. We establish the optimal convergence of numerical solutions and present several numerical examples to illustrate the performance of the proposed method.</p></p class="citation"></blockquote><h3 id=48--230261-a-modular-deep-learning-based-approach-for-diffuse-optical-tomography-reconstruction-alessandro-benfenati-et-al-2024>(4/8 | 230/261) A Modular Deep Learning-based Approach for Diffuse Optical Tomography Reconstruction (Alessandro Benfenati et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Alessandro Benfenati, Paola Causin, Martina Quinteri. (2024)<br><strong>A Modular Deep Learning-based Approach for Diffuse Optical Tomography Reconstruction</strong><br><button class=copy-to-clipboard title="A Modular Deep Learning-based Approach for Diffuse Optical Tomography Reconstruction" index=230>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-230 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: 65Z05, 68T07, cs-NA, eess-IV, math-NA, math.NA<br>Keyword Score: 10<br>Keywords: Autoencoder<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09277v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09277v1.pdf filename=2402.09277v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Medical imaging is nowadays a pillar in diagnostics and therapeutic follow-up. Current research tries to integrate established - but ionizing - tomographic techniques with technologies offering reduced radiation exposure. Diffuse Optical Tomography (DOT) uses non-ionizing light in the Near-Infrared (NIR) window to reconstruct optical coefficients in living beings, providing functional indications about the composition of the investigated organ/tissue. Due to predominant light scattering at NIR wavelengths, DOT reconstruction is, however, a severely ill-conditioned inverse problem. Conventional reconstruction approaches show severe weaknesses when dealing also with mildly complex cases and/or are computationally very intensive. In this work we explore deep learning techniques for DOT inversion. Namely, we propose a fully data-driven approach based on a modularity concept: first data and originating signal are separately processed via <b>autoencoders,</b> then the corresponding low-dimensional latent spaces are connected via a bridging network which acts at the same time as a learned regularizer.</p></p class="citation"></blockquote><h3 id=58--231261-the-dimension-weighted-fast-multipole-method-for-scattered-data-approximation-helmut-harbrecht-et-al-2024>(5/8 | 231/261) The dimension weighted fast multipole method for scattered data approximation (Helmut Harbrecht et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Helmut Harbrecht, Michael Multerer, Jacopo Quizi. (2024)<br><strong>The dimension weighted fast multipole method for scattered data approximation</strong><br><button class=copy-to-clipboard title="The dimension weighted fast multipole method for scattered data approximation" index=231>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-231 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: cs-NA, math-NA, math.NA<br>Keyword Score: 5<br>Keywords: Black Box<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09531v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09531v1.pdf filename=2402.09531v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The present article is concerned scattered data approximation for higher dimensional data sets which exhibit an anisotropic behavior in the different dimensions. Tailoring sparse polynomial interpolation to this specific situation, we derive very efficient degenerate kernel approximations which we then use in a dimension weighted fast multipole method. This dimension weighted fast multipole method enables to deal with many more dimensions than the standard <b>black-box</b> <b>fast</b> multipole method based on interpolation. A thorough analysis of the method is provided including rigorous error estimates. The accuracy and the cost of the approach are validated by extensive numerical results. As a relevant application, we apply the approach to a shape uncertainty quantification problem.</p></p class="citation"></blockquote><h3 id=68--232261-analysis-of-an-adaptive-safeguarded-newton-anderson-algorithm-with-applications-to-fluid-problems-matt-dallas-et-al-2024>(6/8 | 232/261) Analysis of an Adaptive Safeguarded Newton-Anderson Algorithm with Applications to Fluid Problems (Matt Dallas et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Matt Dallas, Sara Pollock, Leo Rebholz. (2024)<br><strong>Analysis of an Adaptive Safeguarded Newton-Anderson Algorithm with Applications to Fluid Problems</strong><br><button class=copy-to-clipboard title="Analysis of an Adaptive Safeguarded Newton-Anderson Algorithm with Applications to Fluid Problems" index=232>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-232 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: 65J15, cs-NA, math-NA, math.NA<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09295v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09295v1.pdf filename=2402.09295v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The purpose of this paper is to develop a practical strategy to accelerate Newton&rsquo;s method in the vicinity of singular points. We do this by developing an adaptive safeguarding scheme, which we call gamma-safeguarding, that one can apply to Anderson accelerated Newton&rsquo;s method when solving problems near singular points. The key features of adaptive gamma-safeguarding are that it converges locally for singular problems, and it can detect nonsingular problems, in which case the Newton-Anderson iterates are scaled towards a standard Newton step. This leads to faster local convergence compared to both Newton&rsquo;s method and Newton-Anderson without safeguarding, at no additional computational cost. We demonstrate three strategies one can use when implementing Newton-Anderson and gamma-safeguarded Newton-Anderson to solve parameter-dependent problems near singular points. For our <b>benchmark</b> problems, we take two parameter-dependent incompressible flow systems: flow in a channel and Rayleigh-Benard convection.</p></p class="citation"></blockquote><h3 id=78--233261-an-asymptotic-preserving-scheme-for-euler-equations-i-non-ideal-gases-giuseppe-orlando-et-al-2024>(7/8 | 233/261) An asymptotic-preserving scheme for Euler equations I: non-ideal gases (Giuseppe Orlando et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Giuseppe Orlando, Luca Bonaventura. (2024)<br><strong>An asymptotic-preserving scheme for Euler equations I: non-ideal gases</strong><br><button class=copy-to-clipboard title="An asymptotic-preserving scheme for Euler equations I: non-ideal gases" index=233>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-233 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: cs-NA, math-NA, math.NA<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09252v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09252v1.pdf filename=2402.09252v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>{We analyze a general Implicit-Explicit (IMEX) time discretization for the compressible Euler equations of gas dynamics, showing that they are asymptotic-preserving (AP) in the low Mach number limit. The analysis is carried out for a general equation of state (EOS). We consider both a single asymptotic length scale and two length scales. We then show that, when coupling these time discretizations with a Discontinuous Galerkin (DG) space discretization with appropriate fluxes, an all Mach number numerical method is obtained. A number of relevant <b>benchmarks</b> for ideal gases and their non-trivial extension to non-ideal EOS validate the performed analysis.</p></p class="citation"></blockquote><h3 id=88--234261-comparison-of-two-models-to-predict-vertebral-failure-loads-on-the-same-experimental-dataset-v-allard-et-al-2024>(8/8 | 234/261) comparison of two models to predict vertebral failure loads on the same experimental dataset (V. Allard et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>V. Allard, C. Heidsieck, F. Bermond, C. Confavreux, C. Travert, L. Gajny, 3, W. Skalli, D. Mitton, H. Follet. (2024)<br><strong>comparison of two models to predict vertebral failure loads on the same experimental dataset</strong><br><button class=copy-to-clipboard title="comparison of two models to predict vertebral failure loads on the same experimental dataset" index=234>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-234 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: cs-NA, math-NA, math.NA<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09196v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09196v1.pdf filename=2402.09196v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Clinical use of finite element analysis requires validation and reproducibility studies. The current study compared two models of vertebral bodies including endplates, on the same experimental dataset and evaluated the influence of the operator on the failure load. Models used were strongly correlated (R2=0.91). The intra-operator reproducibility was 6.4% and 3.5 % for each model. Both simulated results were close to experimental results. The differences in performance could be associated to the differences in segmentation process, mesh (hexahedral vs tetrahedral), material representation and failure criteria. Linear analysis did not decrease model accuracy. Comparison with literature for accuracy and precision shows a wide range of values partly related to the different experimental datasets and the different modelling approaches. Models <b>benchmark</b> using the same experimental dataset are needed to go towards clinical applications.</p></p class="citation"></blockquote><h2 id=physicssoc-ph-1>physics.soc-ph (1)</h2><h3 id=11--235261-understanding-team-collapse-via-probabilistic-graphical-models-iasonas-nikolaou-et-al-2024>(1/1 | 235/261) Understanding team collapse via probabilistic graphical models (Iasonas Nikolaou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Iasonas Nikolaou, Konstantinos Pelechrinis, Evimaria Terzi. (2024)<br><strong>Understanding team collapse via probabilistic graphical models</strong><br><button class=copy-to-clipboard title="Understanding team collapse via probabilistic graphical models" index=235>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-235 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: physics.soc-ph<br>Categories: cs-LG, cs-SI, physics-soc-ph, physics.soc-ph<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.10243v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.10243v1.pdf filename=2402.10243v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this work, we develop a graphical model to capture team dynamics. We analyze the model and show how to learn its parameters from data. Using our model we study the phenomenon of team collapse from a computational perspective. We use <b>simulations</b> and real-world experiments to find the main causes of team collapse. We also provide the principles of building resilient teams, i.e., teams that avoid collapsing. Finally, we use our model to analyze the structure of NBA teams and dive deeper into games of interest.</p></p class="citation"></blockquote><h2 id=csit-2>cs.IT (2)</h2><h3 id=12--236261-zak-otfs-and-ldpc-codes-beyza-dabak-et-al-2024>(1/2 | 236/261) Zak-OTFS and LDPC Codes (Beyza Dabak et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Beyza Dabak, Venkatesh Khammammetti, Saif Khan Mohammed, Robert Calderbank. (2024)<br><strong>Zak-OTFS and LDPC Codes</strong><br><button class=copy-to-clipboard title="Zak-OTFS and LDPC Codes" index=236>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-236 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, eess-SP, math-IT<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09551v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09551v1.pdf filename=2402.09551v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Orthogonal Time Frequency Space (OTFS) is a framework for communications and active sensing that processes signals in the delay-Doppler (DD) domain. It is informed by 6G propagation environments, where Doppler spreads measured in kHz make it more and more difficult to estimate channels, and the standard model-dependent approach to wireless communication is starting to break down. We consider Zak-OTFS where inverse Zak transform converts information symbols mounted on DD domain pulses to the time domain for transmission. Zak-OTFS modulation is parameterized by a delay period $\tau_{p}$ and a Doppler period $\nu_{p}$, where the product $\tau_{p}\nu_{p}=1$. When the channel spread is less than the delay period, and the Doppler spread is less than the Doppler period, the Zak-OTFS input-output relation can be predicted from the response to a single pilot symbol. The highly reliable channel estimates concentrate around the pilot location, and we configure low-density parity-check (LDPC) codes that take advantage of this prior information about reliability. It is advantageous to allocate information symbols to more reliable bins in the DD domain. We report <b>simulation</b> results for a Veh-A channel model where it is not possible to resolve all the paths, showing that LDPC coding extends the range of Doppler spreads for which reliable model-free communication is possible. We show that LDPC coding reduces sensitivity to the choice of transmit filter, making bandwidth expansion less necessary. Finally, we compare BER performance of Zak-OTFS to that of a multicarrier approximation (MC-OTFS), showing LDPC coding amplifies the gains previously reported for uncoded transmission.</p></p class="citation"></blockquote><h3 id=22--237261-performance-complexity-latency-trade-offs-of-concatenated-rs-bch-codes-alvin-y-sukmadji-et-al-2024>(2/2 | 237/261) Performance-Complexity-Latency Trade-offs of Concatenated RS-BCH Codes (Alvin Y. Sukmadji et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Alvin Y. Sukmadji, Frank R. Kschischang. (2024)<br><strong>Performance-Complexity-Latency Trade-offs of Concatenated RS-BCH Codes</strong><br><button class=copy-to-clipboard title="Performance-Complexity-Latency Trade-offs of Concatenated RS-BCH Codes" index=237>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-237 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, math-IT<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09364v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09364v2.pdf filename=2402.09364v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Using a generating function approach, a computationally tractable expression is derived to predict the frame error rate arising at the output of the binary symmetric channel when a number of outer Reed&ndash;Solomon codes are concatenated with a number of inner Bose&ndash;Ray-Chaudhuri&ndash;Hocquenghem codes, thereby obviating the need for time-consuming Monte Carlo <b>simulations.</b> Measuring (a) code performance via the gap to the Shannon limit, (b) decoding complexity via an estimate of the number of operations per decoded bit, and (c) decoding latency by the overall frame length, a code search is performed to determine the Pareto frontier for performance-complexity-latency trade-offs.</p></p class="citation"></blockquote><h2 id=q-biobm-1>q-bio.BM (1)</h2><h3 id=11--238261-3d-based-rna-function-prediction-tools-in-rnaglib-carlos-oliver-et-al-2024>(1/1 | 238/261) 3D-based RNA function prediction tools in rnaglib (Carlos Oliver et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Carlos Oliver, Vincent Mallet, Jérôme Waldispühl. (2024)<br><strong>3D-based RNA function prediction tools in rnaglib</strong><br><button class=copy-to-clipboard title="3D-based RNA function prediction tools in rnaglib" index=238>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-238 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: q-bio.BM<br>Categories: cs-LG, q-bio-BM, q-bio.BM<br>Keyword Score: 20<br>Keywords: Supervised Learning, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09330v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09330v1.pdf filename=2402.09330v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Understanding the connection between complex structural features of RNA and biological function is a fundamental challenge in evolutionary studies and in RNA design. However, building datasets of RNA 3D structures and making appropriate modeling choices remains time-consuming and lacks standardization. In this chapter, we describe the use of rnaglib, to train <b>supervised</b> and <b>unsupervised</b> machine learning-based function prediction models on datasets of RNA 3D structures.</p></p class="citation"></blockquote><h2 id=cslo-5>cs.LO (5)</h2><h3 id=15--239261-unified-opinion-dynamic-modeling-as-concurrent-set-relations-in-rewriting-logic-carlos-olarte-et-al-2024>(1/5 | 239/261) Unified Opinion Dynamic Modeling as Concurrent Set Relations in Rewriting Logic (Carlos Olarte et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Carlos Olarte, Carlos Ramírez, Camilo Rocha, Frank Valencia. (2024)<br><strong>Unified Opinion Dynamic Modeling as Concurrent Set Relations in Rewriting Logic</strong><br><button class=copy-to-clipboard title="Unified Opinion Dynamic Modeling as Concurrent Set Relations in Rewriting Logic" index=239>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-239 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LO<br>Categories: cs-LO, cs.LO<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09021v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09021v1.pdf filename=2402.09021v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Social media platforms have played a key role in weaponizing the polarization of social, political, and democratic processes. This is, mainly, because they are a medium for opinion formation. Opinion dynamic models are a tool for understanding the role of specific social factors on the acceptance/rejection of opinions because they can be used to analyze certain assumptions on human behaviors. This work presents a framework that uses concurrent set relations as the formal basis to specify, simulate, and analyze social interaction systems with dynamic opinion models. Standard models for social learning are obtained as particular instances of the proposed framework. It has been implemented in the Maude system as a fully executable rewrite theory that can be used to better understand how opinions of a system of agents can be shaped. This paper also reports an initial exploration in Maude on the use of reachability analysis, probabilistic <b>simulation,</b> and statistical model checking of important properties related to opinion dynamic models.</p></p class="citation"></blockquote><h3 id=25--240261-correctly-communicating-software-distributed-asynchronous-and-beyond-extended-version-bas-van-den-heuvel-2024>(2/5 | 240/261) Correctly Communicating Software: Distributed, Asynchronous, and Beyond (extended version) (Bas van den Heuvel, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bas van den Heuvel. (2024)<br><strong>Correctly Communicating Software: Distributed, Asynchronous, and Beyond (extended version)</strong><br><button class=copy-to-clipboard title="Correctly Communicating Software: Distributed, Asynchronous, and Beyond (extended version)" index=240>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-240 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LO<br>Categories: cs-LO, cs.LO<br>Keyword Score: 10<br>Keywords: Message-Passing<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09595v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09595v1.pdf filename=2402.09595v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Much of the software we use in everyday life consists of distributed components (running on separate cores or even computers) that collaborate through communication (by exchanging messages). It is crucial to develop robust methods that can give reliable guarantees about the behavior of such <b>message-passing</b> software. With a focus on session types as communication protocols and their foundations in logic, this thesis revolves around the following question: How can we push the boundaries of the logical foundations of session types (binary and multiparty), extending their expressiveness and applicability, while preserving fundamental correctness properties? In this context, this thesis studies several intertwined aspects of <b>message-passing.</b></p></p class="citation"></blockquote><h3 id=35--241261-inferentialist-resource-semantics-alexander-v-gheorghiu-et-al-2024>(3/5 | 241/261) Inferentialist Resource Semantics (Alexander V. Gheorghiu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Alexander V. Gheorghiu, Tao Gu, David J. Pym. (2024)<br><strong>Inferentialist Resource Semantics</strong><br><button class=copy-to-clipboard title="Inferentialist Resource Semantics" index=241>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-241 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LO<br>Categories: cs-CR, cs-LO, cs-SY, cs.LO, eess-SY<br>Keyword Score: 10<br>Keywords: Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09217v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09217v1.pdf filename=2402.09217v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In systems modelling, a system typically comprises located resources relative to which processes execute. One important use of logic in informatics is in modelling such systems for the purpose of <b>reasoning</b> (perhaps automated) about their behaviour and properties. To this end, one requires an interpretation of logical formulae in terms of the resources and states of the system; such an interpretation is called a resource semantics of the logic. This paper shows how inferentialism &ndash; the view that meaning is given in terms of inferential behaviour &ndash; enables a versatile and expressive framework for resource semantics. Specifically, how inferentialism seamlessly incorporates the assertion-based approach of the logic of Bunched Implications, foundational in program verification (e.g., as the basis of Separation Logic), and the renowned number-of-uses reading of Linear Logic. This integration enables <b>reasoning</b> about shared and separated resources in intuitive and familiar ways, as well as about the composition and interfacing of system components.</p></p class="citation"></blockquote><h3 id=45--242261-identifying-tractable-quantified-temporal-constraints-within-ord-horn-jakub-rydval-et-al-2024>(4/5 | 242/261) Identifying Tractable Quantified Temporal Constraints within Ord-Horn (Jakub Rydval et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jakub Rydval, Žaneta Semanišinová, Michał Wrona. (2024)<br><strong>Identifying Tractable Quantified Temporal Constraints within Ord-Horn</strong><br><button class=copy-to-clipboard title="Identifying Tractable Quantified Temporal Constraints within Ord-Horn" index=242>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-242 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LO<br>Categories: cs-LO, cs.LO<br>Keyword Score: 10<br>Keywords: Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09187v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09187v1.pdf filename=2402.09187v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The constraint satisfaction problem, parameterized by a relational structure, provides a general framework for expressing computational decision problems. Already the restriction to the class of all finite structures forms an interesting microcosm on its own, but to express decision problems in temporal <b>reasoning</b> one has to take a step beyond the finite-domain realm. An important class of templates used in this context are temporal structures, i.e., structures over $\mathbb{Q}$ whose relations are first-order definable using the usual countable dense linear order without endpoints. In the standard setting, which allows only existential quantification over input variables, the complexity of finite and temporal constraints has been fully classified. In the quantified setting, i.e., when one also allows universal quantifiers, there is only a handful of partial classification results and many concrete cases of unknown complexity. This paper presents a significant progress towards understanding the complexity of the quantified constraint satisfaction problem for temporal structures. We provide a complexity dichotomy for quantified constraints over the Ord-Horn fragment, which played an important role in understanding the complexity of constraints both over temporal structures and in Allen&rsquo;s interval algebra. We show that all problems under consideration are in P or coNP-hard. In particular, we determine the complexity of the quantified constraint satisfaction problem for $(\mathbb{Q};x=y\Rightarrow x\geq z)$, hereby settling a question open for more than ten years.</p></p class="citation"></blockquote><h3 id=55--243261-an-algorithmic-meta-theorem-for-homomorphism-indistinguishability-tim-seppelt-2024>(5/5 | 243/261) An Algorithmic Meta Theorem for Homomorphism Indistinguishability (Tim Seppelt, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tim Seppelt. (2024)<br><strong>An Algorithmic Meta Theorem for Homomorphism Indistinguishability</strong><br><button class=copy-to-clipboard title="An Algorithmic Meta Theorem for Homomorphism Indistinguishability" index=243>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-243 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LO<br>Categories: cs-CC, cs-DM, cs-LO, cs.LO, math-CO<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08989v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08989v1.pdf filename=2402.08989v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Two <b>graphs</b> $G$ and $H$ are homomorphism indistinguishable over a family of <b>graphs</b> $\mathcal{F}$ if for all <b>graphs</b> $F \in \mathcal{F}$ the number of homomorphisms from $F$ to $G$ is equal to the number of homomorphism from $F$ to $H$. Many natural equivalence relations comparing <b>graphs</b> such as (quantum) isomorphism, cospectrality, and logical equivalences can be characterised as homomorphism indistinguishability relations over various <b>graph</b> classes. For a fixed <b>graph</b> class $\mathcal{F}$, the decision problem HomInd($\mathcal{F}$) asks to determine whether two input <b>graphs</b> $G$ and $H$ are homomorphism indistinguishable over $\mathcal{F}$. The problem HomInd($\mathcal{F}$) is known to be decidable only for few <b>graph</b> classes $\mathcal{F}$. We show that HomInd($\mathcal{F}$) admits a randomised polynomial-time algorithm for every <b>graph</b> class $\mathcal{F}$ of bounded treewidth which is definable in counting monadic second-order logic CMSO2. Thereby, we give the first general algorithm for deciding homomorphism indistinguishability. This result extends to a version of HomInd where the <b>graph</b> class $\mathcal{F}$ is specified by a CMSO2-sentence and a bound $k$ on the treewidth, which are given as input. For fixed $k$, this problem is randomised fixed-parameter tractable. If $k$ is part of the input then it is coNP- and coW[1]-hard. Addressing a problem posed by Berkholz (2012), we show coNP-hardness by establishing that deciding indistinguishability under the $k$-dimensional Weisfeiler&ndash;Leman algorithm is coNP-hard when $k$ is part of the input.</p></p class="citation"></blockquote><h2 id=csdb-1>cs.DB (1)</h2><h3 id=11--244261-computational-complexity-of-preferred-subset-repairs-on-data-graphs-nina-pardal-et-al-2024>(1/1 | 244/261) Computational Complexity of Preferred Subset Repairs on Data-Graphs (Nina Pardal et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nina Pardal, Santiago Cifuentes, Edwin Pin, Maria Vanina Martinez, Sergio Abriola. (2024)<br><strong>Computational Complexity of Preferred Subset Repairs on Data-Graphs</strong><br><button class=copy-to-clipboard title="Computational Complexity of Preferred Subset Repairs on Data-Graphs" index=244>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-244 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DB<br>Categories: 68P15, 68T27, 03B70, 68T37, cs-AI, cs-DB, cs-LO, cs.DB<br>Keyword Score: 13<br>Keywords: Graph, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09265v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09265v1.pdf filename=2402.09265v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The problem of repairing inconsistent knowledge bases has a long history within the communities of database theory and knowledge representation and <b>reasoning,</b> especially from the perspective of structured data. However, as the data available in real-world domains becomes more complex and interconnected, the need naturally arises for developing new types of repositories, representation languages, and semantics, to allow for more suitable ways to query and reason about it. <b>Graph</b> databases provide an effective way to represent relationships among semi-structured data, and allow processing and querying these connections efficiently. In this work, we focus on the problem of computing prioritized repairs over <b>graph</b> databases with data values, using a notion of consistency based on Reg-GXPath expressions as integrity constraints. We present several preference criteria based on the standard subset repair semantics, incorporating weights, multisets, and set-based priority levels. We study the most common repairing tasks, showing that it is possible to maintain the same computational complexity as in the case where no preference criterion is available for exploitation. To complete the picture, we explore the complexity of consistent query answering in this setting and obtain tight lower and upper bounds for all the preference criteria introduced.</p></p class="citation"></blockquote><h2 id=statml-5>stat.ML (5)</h2><h3 id=15--245261-conformalized-adaptive-forecasting-of-heterogeneous-trajectories-yanfei-zhou-et-al-2024>(1/5 | 245/261) Conformalized Adaptive Forecasting of Heterogeneous Trajectories (Yanfei Zhou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yanfei Zhou, Lars Lindemann, Matteo Sesia. (2024)<br><strong>Conformalized Adaptive Forecasting of Heterogeneous Trajectories</strong><br><button class=copy-to-clipboard title="Conformalized Adaptive Forecasting of Heterogeneous Trajectories" index=245>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-245 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-LG, stat-ML, stat.ML<br>Keyword Score: 10<br>Keywords: Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09623v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09623v1.pdf filename=2402.09623v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents a new conformal method for generating simultaneous forecasting bands guaranteed to cover the entire path of a new random trajectory with sufficiently high probability. <b>Prompted</b> by the need for dependable uncertainty estimates in motion planning applications where the behavior of diverse objects may be more or less unpredictable, we blend different techniques from online conformal prediction of single and multiple time series, as well as ideas for addressing heteroscedasticity in regression. This solution is both principled, providing precise finite-sample guarantees, and effective, often leading to more informative predictions than prior methods.</p></p class="citation"></blockquote><h3 id=25--246261-connecting-algorithmic-fairness-to-quality-dimensions-in-machine-learning-in-official-statistics-and-survey-production-patrick-oliver-schenk-et-al-2024>(2/5 | 246/261) Connecting Algorithmic Fairness to Quality Dimensions in Machine Learning in Official Statistics and Survey Production (Patrick Oliver Schenk et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Patrick Oliver Schenk, Christoph Kern. (2024)<br><strong>Connecting Algorithmic Fairness to Quality Dimensions in Machine Learning in Official Statistics and Survey Production</strong><br><button class=copy-to-clipboard title="Connecting Algorithmic Fairness to Quality Dimensions in Machine Learning in Official Statistics and Survey Production" index=246>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-246 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-LG, stat-ME, stat-ML, stat.ML<br>Keyword Score: 10<br>Keywords: Fairness<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09328v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09328v1.pdf filename=2402.09328v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>National Statistical Organizations (NSOs) increasingly draw on Machine Learning (ML) to improve the timeliness and cost-effectiveness of their products. When introducing ML solutions, NSOs must ensure that high standards with respect to robustness, reproducibility, and accuracy are upheld as codified, e.g., in the Quality Framework for Statistical Algorithms (QF4SA; Yung et al. 2022). At the same time, a growing body of research focuses on <b>fairness</b> as a pre-condition of a safe deployment of ML to prevent disparate social impacts in practice. However, <b>fairness</b> has not yet been explicitly discussed as a quality aspect in the context of the application of ML at NSOs. We employ Yung et al. (2022)&rsquo;s QF4SA quality framework and present a mapping of its quality dimensions to algorithmic <b>fairness.</b> We thereby extend the QF4SA framework in several ways: we argue for <b>fairness</b> as its own quality dimension, we investigate the interaction of <b>fairness</b> with other dimensions, and we explicitly address data, both on its own and its interaction with applied methodology. In parallel with empirical illustrations, we show how our mapping can contribute to methodology in the domains of official statistics, algorithmic <b>fairness,</b> and trustworthy machine learning.</p></p class="citation"></blockquote><h3 id=35--247261-mixed-output-gaussian-process-latent-variable-models-james-odgers-et-al-2024>(3/5 | 247/261) Mixed-Output Gaussian Process Latent Variable Models (James Odgers et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>James Odgers, Chrysoula Kappatou, Ruth Misener, Sarah Filippi. (2024)<br><strong>Mixed-Output Gaussian Process Latent Variable Models</strong><br><button class=copy-to-clipboard title="Mixed-Output Gaussian Process Latent Variable Models" index=247>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-247 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-LG, stat-ML, stat.ML<br>Keyword Score: 10<br>Keywords: Gaussian Process<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09122v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09122v1.pdf filename=2402.09122v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This work develops a Bayesian non-parametric approach to signal separation where the signals may vary according to latent variables. Our key contribution is to augment <b>Gaussian</b> <b>Process</b> Latent Variable Models (GPLVMs) to incorporate the case where each data point comprises the weighted sum of a known number of pure component signals, observed across several input locations. Our framework allows the use of a range of priors for the weights of each observation. This flexibility enables us to represent use cases including sum-to-one constraints for estimating fractional makeup, and binary weights for classification. Our contributions are particularly relevant to spectroscopy, where changing conditions may cause the underlying pure component signals to vary from sample to sample. To demonstrate the applicability to both spectroscopy and other domains, we consider several applications: a near-infrared spectroscopy data set with varying temperatures, a simulated data set for identifying flow configuration through a pipe, and a data set for determining the type of rock from its reflectance.</p></p class="citation"></blockquote><h3 id=45--248261-towards-robust-model-based-reinforcement-learning-against-adversarial-corruption-chenlu-ye-et-al-2024>(4/5 | 248/261) Towards Robust Model-Based Reinforcement Learning Against Adversarial Corruption (Chenlu Ye et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chenlu Ye, Jiafan He, Quanquan Gu, Tong Zhang. (2024)<br><strong>Towards Robust Model-Based Reinforcement Learning Against Adversarial Corruption</strong><br><button class=copy-to-clipboard title="Towards Robust Model-Based Reinforcement Learning Against Adversarial Corruption" index=248>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-248 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-LG, stat-ML, stat.ML<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.08991v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.08991v2.pdf filename=2402.08991v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This study tackles the challenges of adversarial corruption in model-based <b>reinforcement</b> <b>learning</b> (RL), where the transition dynamics can be corrupted by an adversary. Existing studies on corruption-robust RL mostly focus on the setting of model-free RL, where robust least-square regression is often employed for value function estimation. However, these techniques cannot be directly applied to model-based RL. In this paper, we focus on model-based RL and take the maximum likelihood estimation (MLE) approach to learn transition model. Our work encompasses both online and offline settings. In the online setting, we introduce an algorithm called corruption-robust optimistic MLE (CR-OMLE), which leverages total-variation (TV)-based information ratios as uncertainty weights for MLE. We prove that CR-OMLE achieves a regret of $\tilde{\mathcal{O}}(\sqrt{T} + C)$, where $C$ denotes the cumulative corruption level after $T$ episodes. We also prove a lower bound to show that the additive dependence on $C$ is optimal. We extend our weighting technique to the offline setting, and propose an algorithm named corruption-robust pessimistic MLE (CR-PMLE). Under a uniform coverage condition, CR-PMLE exhibits suboptimality worsened by $\mathcal{O}(C/n)$, nearly matching the lower bound. To the best of our knowledge, this is the first work on corruption-robust model-based RL algorithms with provable guarantees.</p></p class="citation"></blockquote><h3 id=55--249261-mcmc-driven-learning-alexandre-bouchard-côté-et-al-2024>(5/5 | 249/261) MCMC-driven learning (Alexandre Bouchard-Côté et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Alexandre Bouchard-Côté, Trevor Campbell, Geoff Pleiss, Nikola Surjanovic. (2024)<br><strong>MCMC-driven learning</strong><br><button class=copy-to-clipboard title="MCMC-driven learning" index=249>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-249 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-LG, math-ST, stat-CO, stat-ML, stat-TH, stat.ML<br>Keyword Score: 5<br>Keywords: Black Box<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09598v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09598v1.pdf filename=2402.09598v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper is intended to appear as a chapter for the Handbook of Markov Chain Monte Carlo. The goal of this chapter is to unify various problems at the intersection of Markov chain Monte Carlo (MCMC) and machine learning$\unicode{x2014}$which includes <b>black-box</b> <b>variational</b> inference, adaptive MCMC, normalizing flow construction and transport-assisted MCMC, surrogate-likelihood MCMC, coreset construction for MCMC with big data, Markov chain gradient descent, Markovian score climbing, and more$\unicode{x2014}$within one common framework. By doing so, the theory and methods developed for each may be translated and generalized.</p></p class="citation"></blockquote><h2 id=csgt-1>cs.GT (1)</h2><h3 id=11--250261-pareto-optimal-algorithms-for-learning-in-games-eshwar-ram-arunachaleswaran-et-al-2024>(1/1 | 250/261) Pareto-Optimal Algorithms for Learning in Games (Eshwar Ram Arunachaleswaran et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Eshwar Ram Arunachaleswaran, Natalie Collina, Jon Schneider. (2024)<br><strong>Pareto-Optimal Algorithms for Learning in Games</strong><br><button class=copy-to-clipboard title="Pareto-Optimal Algorithms for Learning in Games" index=250>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-250 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.GT<br>Categories: cs-GT, cs.GT<br>Keyword Score: 10<br>Keywords: Counter-factual<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09549v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09549v1.pdf filename=2402.09549v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We study the problem of characterizing optimal learning algorithms for playing repeated games against an adversary with unknown payoffs. In this problem, the first player (called the learner) commits to a learning algorithm against a second player (called the optimizer), and the optimizer best-responds by choosing the optimal dynamic strategy for their (unknown but well-defined) payoff. Classic learning algorithms (such as no-regret algorithms) provide some <b>counterfactual</b> guarantees for the learner, but might perform much more poorly than other learning algorithms against particular optimizer payoffs. In this paper, we introduce the notion of asymptotically Pareto-optimal learning algorithms. Intuitively, if a learning algorithm is Pareto-optimal, then there is no other algorithm which performs asymptotically at least as well against all optimizers and performs strictly better (by at least $\Omega(T)$) against some optimizer. We show that well-known no-regret algorithms such as Multiplicative Weights and Follow The Regularized Leader are Pareto-dominated. However, while no-regret is not enough to ensure Pareto-optimality, we show that a strictly stronger property, no-swap-regret, is a sufficient condition for Pareto-optimality. Proving these results requires us to address various technical challenges specific to repeated play, including the fact that there is no simple characterization of how optimizers who are rational in the long-term best-respond against a learning algorithm over multiple rounds of play. To address this, we introduce the idea of the asymptotic menu of a learning algorithm: the convex closure of all correlated distributions over strategy profiles that are asymptotically implementable by an adversary. We show that all no-swap-regret algorithms share the same asymptotic menu, implying that all no-swap-regret algorithms are ``strategically equivalent&rsquo;'.</p></p class="citation"></blockquote><h2 id=csmm-1>cs.MM (1)</h2><h3 id=11--251261-ll-gabr-energy-efficient-live-video-streaming-using-reinforcement-learning-adithya-raman-et-al-2024>(1/1 | 251/261) LL-GABR: Energy Efficient Live Video Streaming Using Reinforcement Learning (Adithya Raman et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Adithya Raman, Bekir Turkkan, Tevfik Kosar. (2024)<br><strong>LL-GABR: Energy Efficient Live Video Streaming Using Reinforcement Learning</strong><br><button class=copy-to-clipboard title="LL-GABR: Energy Efficient Live Video Streaming Using Reinforcement Learning" index=251>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-251 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.MM<br>Categories: cs-AI, cs-MM, cs.MM<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09392v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09392v1.pdf filename=2402.09392v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Over the recent years, research and development in adaptive bitrate (ABR) algorithms for live video streaming have been successful in improving users&rsquo; quality of experience (QoE) by reducing latency to near real-time levels while delivering higher bitrate videos with minimal rebuffering time. However, the QoE models used by these ABR algorithms do not take into account that a large portion of live video streaming clients use mobile devices where a higher bitrate does not necessarily translate into higher perceived quality. Ignoring perceived quality results in playing videos at higher bitrates without a significant increase in perceptual video quality and becomes a burden for battery-constrained mobile devices due to higher energy consumption. In this paper, we propose LL-GABR, a deep <b>reinforcement</b> <b>learning</b> approach that models the QoE using perceived video quality instead of bitrate and uses energy consumption along with other metrics like latency, rebuffering events, and smoothness. LL-GABR makes no assumptions about the underlying video, environment, or network settings and can operate flexibly on different video titles, each having a different bitrate encoding ladder without additional re-training, unlike existing learning-based ABRs. Trace-driven experimental results show that LL-GABR outperforms the state-of-the-art approaches by up to 44% in terms of perceptual QoE and a 73% increase in energy efficiency as a result of reducing net energy consumption by 11%.</p></p class="citation"></blockquote><h2 id=econth-1>econ.TH (1)</h2><h3 id=11--252261-persuasion-delegation-and-private-information-in-algorithm-assisted-decisions-ruqing-xu-2024>(1/1 | 252/261) Persuasion, Delegation, and Private Information in Algorithm-Assisted Decisions (Ruqing Xu, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ruqing Xu. (2024)<br><strong>Persuasion, Delegation, and Private Information in Algorithm-Assisted Decisions</strong><br><button class=copy-to-clipboard title="Persuasion, Delegation, and Private Information in Algorithm-Assisted Decisions" index=252>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-252 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: econ.TH<br>Categories: cs-AI, cs-CY, cs-GT, cs-HC, econ-TH, econ.TH<br>Keyword Score: 10<br>Keywords: human-in-the-loop<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09384v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09384v1.pdf filename=2402.09384v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A principal designs an algorithm that generates a publicly observable prediction of a binary state. She must decide whether to act directly based on the prediction or to delegate the decision to an agent with private information but potential misalignment. We study the optimal design of the prediction algorithm and the delegation rule in such environments. Three key findings emerge: (1) Delegation is optimal if and only if the principal would make the same binary decision as the agent had she observed the agent&rsquo;s information. (2) Providing the most informative algorithm may be suboptimal even if the principal can act on the algorithm&rsquo;s prediction. Instead, the optimal algorithm may provide more information about one state and restrict information about the other. (3) Common restrictions on algorithms, such as keeping a <b>&ldquo;human-in-the-loop&rdquo;</b> or requiring maximal prediction accuracy, strictly worsen decision quality in the absence of perfectly aligned agents and state-revealing signals. These findings predict the underperformance of human-machine collaborations if no measures are taken to mitigate common preference misalignment between algorithms and human decision-makers.</p></p class="citation"></blockquote><h2 id=physicscomp-ph-1>physics.comp-ph (1)</h2><h3 id=11--253261-neural-networks-asymptotic-behaviours-for-the-resolution-of-inverse-problems-luigi-del-debbio-et-al-2024>(1/1 | 253/261) Neural Networks Asymptotic Behaviours for the Resolution of Inverse Problems (Luigi Del Debbio et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Luigi Del Debbio, Manuel Naviglio, Francesco Tarantelli. (2024)<br><strong>Neural Networks Asymptotic Behaviours for the Resolution of Inverse Problems</strong><br><button class=copy-to-clipboard title="Neural Networks Asymptotic Behaviours for the Resolution of Inverse Problems" index=253>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-253 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: physics.comp-ph<br>Categories: cs-AI, hep-lat, hep-th, physics-comp-ph, physics.comp-ph<br>Keyword Score: 10<br>Keywords: Probabilistic Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09338v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09338v2.pdf filename=2402.09338v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents a study of the effectiveness of Neural Network (NN) techniques for deconvolution inverse problems relevant for applications in Quantum Field Theory, but also in more general contexts. We consider NN&rsquo;s asymptotic limits, corresponding to Gaussian Processes (GPs), where non-linearities in the parameters of the NN can be neglected. Using these resulting GPs, we address the deconvolution inverse problem in the case of a quantum harmonic oscillator simulated through Monte Carlo techniques on a lattice. In this simple toy model, the results of the inversion can be compared with the known analytical solution. Our findings indicate that solving the inverse problem with a NN yields less performing results than those obtained using the GPs derived from NN&rsquo;s asymptotic limits. Furthermore, we observe the trained NN&rsquo;s accuracy approaching that of GPs with increasing layer width. Notably, one of these GPs defies interpretation as a <b>probabilistic</b> <b>model,</b> offering a novel perspective compared to established methods in the literature. Our results suggest the need for detailed studies of the training dynamics in more realistic set-ups.</p></p class="citation"></blockquote><h2 id=csfl-1>cs.FL (1)</h2><h3 id=11--254261-inform-from-compartmental-models-to-stochastic-bounded-counter-machines-tim-leys-et-al-2024>(1/1 | 254/261) Inform: From Compartmental Models to Stochastic Bounded Counter Machines (Tim Leys et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tim Leys, Guillermo A. Perez. (2024)<br><strong>Inform: From Compartmental Models to Stochastic Bounded Counter Machines</strong><br><button class=copy-to-clipboard title="Inform: From Compartmental Models to Stochastic Bounded Counter Machines" index=254>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-254 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.FL<br>Categories: cs-FL, cs.FL<br>Keyword Score: 10<br>Keywords: Probabilistic Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09121v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09121v1.pdf filename=2402.09121v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Compartmental models are used in epidemiology to capture the evolution of infectious diseases such as COVID-19 in a population by assigning members of it to compartments with labels such as susceptible, infected, and recovered. In a stochastic compartmental model the flow of individuals between compartments is determined probabilistically. We establish that certain stochastic compartment models can be encoded as <b>probabilistic</b> <b>counter</b> machines where the configurations are bounded. Based on the latter, we obtain simple descriptions of the models in the PRISM language. This enables the analysis of such compartmental models via <b>probabilistic</b> <b>model</b> checkers. Finally, we report on experimental results where we analyze results from a Belgian COVID-19 model using a <b>probabilistic</b> <b>model</b> checkers.</p></p class="citation"></blockquote><h2 id=csar-1>cs.AR (1)</h2><h3 id=11--255261-stochastic-spiking-attention-accelerating-attention-with-stochastic-computing-in-spiking-networks-zihang-song-et-al-2024>(1/1 | 255/261) Stochastic Spiking Attention: Accelerating Attention with Stochastic Computing in Spiking Networks (Zihang Song et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zihang Song, Prabodh Katti, Osvaldo Simeone, Bipin Rajendran. (2024)<br><strong>Stochastic Spiking Attention: Accelerating Attention with Stochastic Computing in Spiking Networks</strong><br><button class=copy-to-clipboard title="Stochastic Spiking Attention: Accelerating Attention with Stochastic Computing in Spiking Networks" index=255>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-255 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AR<br>Categories: cs-AI, cs-AR, cs-LG, cs-NE, cs.AR, eess-SP<br>Keyword Score: 10<br>Keywords: Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09109v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09109v1.pdf filename=2402.09109v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Spiking Neural Networks (SNNs) have been recently integrated into <b>Transformer</b> architectures due to their potential to reduce computational demands and to improve power efficiency. Yet, the implementation of the attention mechanism using spiking signals on general-purpose computing platforms remains inefficient. In this paper, we propose a novel framework leveraging stochastic computing (SC) to effectively execute the dot-product attention for SNN-based <b>Transformers.</b> We demonstrate that our approach can achieve high classification accuracy ($83.53%$) on CIFAR-10 within 10 time steps, which is comparable to the performance of a baseline artificial neural network implementation ($83.66%$). We estimate that the proposed SC approach can lead to over $6.3\times$ reduction in computing energy and $1.7\times$ reduction in memory access costs for a digital CMOS-based ASIC design. We experimentally validate our stochastic attention block design through an FPGA implementation, which is shown to achieve $48\times$ lower latency as compared to a GPU implementation, while consuming $15\times$ less power.</p></p class="citation"></blockquote><h2 id=csdm-2>cs.DM (2)</h2><h3 id=12--256261-irreducible-markov-chains-on-spaces-of-graphs-with-fixed-degree-color-sequences-félix-almendra-hernández-et-al-2024>(1/2 | 256/261) Irreducible Markov Chains on spaces of graphs with fixed degree-color sequences (Félix Almendra-Hernández et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Félix Almendra-Hernández, Jesús A. De Loera, Sonja Petrović. (2024)<br><strong>Irreducible Markov Chains on spaces of graphs with fixed degree-color sequences</strong><br><button class=copy-to-clipboard title="Irreducible Markov Chains on spaces of graphs with fixed degree-color sequences" index=256>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-256 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DM<br>Categories: 05C40, 05C81, 05C25, 05C15, 05E40, 13P10, 13P25, 62R01, cs-DM, cs.DM, math-AC, math-CO<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09568v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09568v1.pdf filename=2402.09568v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We study a colored generalization of the famous simple-switch Markov chain for sampling the set of <b>graphs</b> with a fixed degree sequence. Here we consider the space of <b>graphs</b> with colored vertices, in which we fix the degree sequence and another statistic arising from the vertex coloring, and prove that the set can be connected with simple color-preserving switches or moves. These moves form a basis for defining an irreducible Markov chain necessary for testing statistical model fit to block-partitioned network data. Our methods further generalize well-known algebraic results from the 1990s: namely, that the corresponding moves can be used to construct a regular triangulation for a generalization of the second hypersimplex. On the other hand, in contrast to the monochromatic case, we show that for simple <b>graphs,</b> the 1-norm of the moves necessary to connect the space increases with the number of colors.</p></p class="citation"></blockquote><h3 id=22--257261-monotonicity-of-the-cops-and-robber-game-for-bounded-depth-treewidth-isolde-adler-et-al-2024>(2/2 | 257/261) Monotonicity of the cops and robber game for bounded depth treewidth (Isolde Adler et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Isolde Adler, Eva Fluck. (2024)<br><strong>Monotonicity of the cops and robber game for bounded depth treewidth</strong><br><button class=copy-to-clipboard title="Monotonicity of the cops and robber game for bounded depth treewidth" index=257>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-257 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DM<br>Categories: G-2-2, cs-DM, cs.DM, math-CO<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09139v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09139v1.pdf filename=2402.09139v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We study a variation of the cops and robber game characterising treewidth, where in each play at most q cops can be placed in order to catch the robber, where q is a parameter of the game. We prove that if k cops have a winning strategy in this game, then k cops have a monotone winning strategy. As a corollary we obtain a new characterisation of bounded depth treewidth, and we give a positive answer to an open question by Fluck, Seppelt and Spitzer (2024), thus showing that <b>graph</b> classes of bounded depth treewidth are homomorphism distinguishing closed. Our proof of monotonicity substantially reorganises a winning strategy by first transforming it into a pre-decomposition, which is inspired by decompositions of matroids, and then applying an intricate breadth-first &ldquo;cleaning up&rdquo; procedure along the pre-decomposition (which may temporarily lose the property of representing a strategy), in order to achieve monotonicity while controlling the number of cop placements simultaneously across all branches of the decomposition via a vertex exchange argument. We believe this can be useful in future research.</p></p class="citation"></blockquote><h2 id=cscy-1>cs.CY (1)</h2><h3 id=11--258261-identification-of-cohesive-subgroups-in-a-university-hall-of-residence-during-the-covid-19-pandemic-using-a-social-network-analysis-approach-pilar-marqués-sánchez-et-al-2024>(1/1 | 258/261) Identification of cohesive subgroups in a university hall of residence during the COVID-19 pandemic using a social network analysis approach (Pilar Marqués-Sánchez et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Pilar Marqués-Sánchez, Arrate Pinto-Carral, Tania Fernández-Villa, Ana Vázquez-Casares, Cristina Liébana-Presa, José Alberto Benítez-Andrades. (2024)<br><strong>Identification of cohesive subgroups in a university hall of residence during the COVID-19 pandemic using a social network analysis approach</strong><br><button class=copy-to-clipboard title="Identification of cohesive subgroups in a university hall of residence during the COVID-19 pandemic using a social network analysis approach" index=258>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-258 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CY<br>Categories: cs-CY, cs.CY, physics-soc-ph<br>Keyword Score: 3<br>Keywords: Clustering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09213v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09213v1.pdf filename=2402.09213v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The aims: (i) analyze connectivity between subgroups of university students, (ii) assess which bridges of relational contacts are essential for connecting or disconnecting subgroups and (iii) to explore the similarities between the attributes of the subgroup nodes in relation to the pandemic context. During the COVID-19 pandemic, young university students have experienced significant changes in their relationships, especially in the halls of residence. Previous research has shown the importance of relationship structure in contagion processes. However, there is a lack of studies in the university setting, where students live closely together. The case study methodology was applied to carry out a descriptive study. The participation consisted of 43 university students living in the same hall of residence. Social network analysis has been applied for data analysis. Factions and Girvan Newman algorithms have been applied to detect the existing cohesive subgroups. The UCINET tool was used for the calculation of the SNA measure. A visualization of the global network will be carried out using Gephi software. After applying the Girvan-Newman and Factions, in both cases it was found that the best division into subgroups was the one that divided the network into 4 subgroups. There is high degree of cohesion within the subgroups and a low cohesion between them. The relationship between subgroup membership and gender was significant. The degree of COVID-19 infection is related to the degree of <b>clustering</b> between the students. College students form subgroups in their residence. Social network analysis facilitates an understanding of structural behavior during the pandemic. The study provides evidence on the importance of gender, race and the building where they live in creating network structures that favor, or not, contagion during a pandemic.</p></p class="citation"></blockquote><h2 id=csds-2>cs.DS (2)</h2><h3 id=12--259261-better-decremental-and-fully-dynamic-sensitivity-oracles-for-subgraph-connectivity-yaowei-long-et-al-2024>(1/2 | 259/261) Better Decremental and Fully Dynamic Sensitivity Oracles for Subgraph Connectivity (Yaowei Long et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yaowei Long, Yunfan Wang. (2024)<br><strong>Better Decremental and Fully Dynamic Sensitivity Oracles for Subgraph Connectivity</strong><br><button class=copy-to-clipboard title="Better Decremental and Fully Dynamic Sensitivity Oracles for Subgraph Connectivity" index=259>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-259 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DS<br>Categories: cs-DS, cs.DS<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09150v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09150v1.pdf filename=2402.09150v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We study the \emph{sensitivity oracles problem for subgraph connectivity} in the \emph{decremental} and \emph{fully dynamic} settings. In the fully dynamic setting, we preprocess an $n$-vertices $m$-edges undirected <b>graph</b> $G$ with $n_{\rm off}$ deactivated vertices initially and the others are activated. Then we receive a single update $D\subseteq V(G)$ of size $|D| = d \leq d_{\star}$, representing vertices whose states will be switched. Finally, we get a sequence of queries, each of which asks the connectivity of two given vertices $u$ and $v$ in the activated subgraph. The decremental setting is a special case when there is no deactivated vertex initially, and it is also known as the \emph{vertex-failure connectivity oracles} problem. We present a better deterministic vertex-failure connectivity oracle with $\widehat{O}(d_{\star}m)$ preprocessing time, $\widetilde{O}(m)$ space, $\widetilde{O}(d^{2})$ update time and $O(d)$ query time, which improves the update time of the previous almost-optimal oracle [Long-Saranurak, FOCS 2022] from $\widehat{O}(d^{2})$ to $\widetilde{O}(d^{2})$. We also present a better deterministic fully dynamic sensitivity oracle for subgraph connectivity with $\widehat{O}(\min{m(n_{\rm off} + d_{\star}),n^{\omega}})$ preprocessing time, $\widetilde{O}(\min{m(n_{\rm off} + d_{\star}),n^{2}})$ space, $\widetilde{O}(d^{2})$ update time and $O(d)$ query time, which significantly improves the update time of the state of the art [Hu-Kosinas-Polak, 2023] from $\widetilde{O}(d^{4})$ to $\widetilde{O}(d^{2})$. Furthermore, our solution is even almost-optimal assuming popular fine-grained complexity conjectures.</p></p class="citation"></blockquote><h3 id=22--260261-improved-deterministic-distributed-maximum-weight-independent-set-approximation-in-sparse-graphs-yuval-gil-2024>(2/2 | 260/261) Improved Deterministic Distributed Maximum Weight Independent Set Approximation in Sparse Graphs (Yuval Gil, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuval Gil. (2024)<br><strong>Improved Deterministic Distributed Maximum Weight Independent Set Approximation in Sparse Graphs</strong><br><button class=copy-to-clipboard title="Improved Deterministic Distributed Maximum Weight Independent Set Approximation in Sparse Graphs" index=260>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-260 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DS<br>Categories: cs-DC, cs-DS, cs.DS<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09011v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09011v1.pdf filename=2402.09011v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We design new deterministic CONGEST approximation algorithms for \emph{maximum weight independent set (MWIS)} in \emph{sparse graphs}. As our main results, we obtain new $\Delta(1+\epsilon)$-approximation algorithms as well as algorithms whose approximation ratio depend strictly on $\alpha$, in <b>graphs</b> with maximum degree $\Delta$ and arboricity $\alpha$. For (deterministic) $\Delta(1+\epsilon)$-approximation, the current state-of-the-art is due to a recent breakthrough by Faour et al.\ [SODA 2023] that showed an $O(\log^{2} (\Delta W)\cdot \log (1/\epsilon)+\log ^{*}n)$-round algorithm, where $W$ is the largest node-weight (this bound translates to $O(\log^{2} n\cdot\log (1/\epsilon))$ under the common assumption that $W=\text{poly}(n)$). As for $\alpha$-dependent approximations, a deterministic CONGEST $(8(1+\epsilon)\cdot\alpha)$-approximation algorithm with runtime $O(\log^{3} n\cdot\log (1/\epsilon))$ can be derived by combining the aforementioned algorithm of Faour et al.\ with a method presented by Kawarabayashi et al.\ [DISC 2020].</p></p class="citation"></blockquote><h2 id=mathco-1>math.CO (1)</h2><h3 id=11--261261-general-penny-graphs-are-at-most-4318-dense-arsenii-sagdeev-2024>(1/1 | 261/261) General penny graphs are at most 43/18-dense (Arsenii Sagdeev, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Arsenii Sagdeev. (2024)<br><strong>General penny graphs are at most 43/18-dense</strong><br><button class=copy-to-clipboard title="General penny graphs are at most 43/18-dense" index=261>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-261 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.CO<br>Categories: 52C10, 52C15, 05C10, cs-CG, math-CO, math-MG, math.CO<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.09131v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.09131v1.pdf filename=2402.09131v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We prove that among $n$ points in the plane in general position, the shortest distance occurs at most $43n/18$ times, improving upon the upper bound of $17n/7$ obtained by T'oth in 1997.</p></p class="citation"></blockquote></div><div class="row pl-3 pr-3"><div class="col-md-6 share-buttons"></div></div><hr><div class="row next-prev-navigator"><div class="col-md-6 previous-article"><a href=/akitenkrad-blog/posts/arxiv/202402/20240215000000/ title="arXiv @ 2024.02.15" class="btn btn-outline-info"><div><i class="fas fa-chevron-circle-left"></i> Prev</div><div class=next-prev-text>arXiv @ 2024.02.15</div></a></div><div class="col-md-6 next-article"><a href=/akitenkrad-blog/posts/arxiv/202402/20240217000000/ title="arXiv @ 2024.02.17" class="btn btn-outline-info"><div>Next <i class="fas fa-chevron-circle-right"></i></div><div class=next-prev-text>arXiv @ 2024.02.17</div></a></div></div><hr></div></div></div><a id=scroll-to-top class=btn><i class="fas fa-chevron-circle-up"></i></a></section><section class=toc-section id=toc-section><div class=toc-holder><h5 class="text-center pl-3">Table of Contents</h5><hr><div class=toc><nav id=TableOfContents><ul><li><a href=#primary-categories>Primary Categories</a></li><li><a href=#keywords>Keywords</a></li><li><a href=#cscl-31>cs.CL (31)</a><ul><li><a href=#131--1261-leveraging-large-language-models-for-enhanced-nlp-task-performance-through-knowledge-distillation-and-optimized-training-strategies-yining-huang-2024>(1/31 | 1/261) Leveraging Large Language Models for Enhanced NLP Task Performance through Knowledge Distillation and Optimized Training Strategies (Yining Huang, 2024)</a></li><li><a href=#231--2261-personalized-large-language-models-stanisław-woźniak-et-al-2024>(2/31 | 2/261) Personalized Large Language Models (Stanisław Woźniak et al., 2024)</a></li><li><a href=#331--3261-multi-query-focused-disaster-summarization-via-instruction-based-prompting-philipp-seeberger-et-al-2024>(3/31 | 3/261) Multi-Query Focused Disaster Summarization via Instruction-Based Prompting (Philipp Seeberger et al., 2024)</a></li><li><a href=#431--4261-aqa-bench-an-interactive-benchmark-for-evaluating-llms-sequential-reasoning-ability-siwei-yang-et-al-2024>(4/31 | 4/261) AQA-Bench: An Interactive Benchmark for Evaluating LLMs&rsquo; Sequential Reasoning Ability (Siwei Yang et al., 2024)</a></li><li><a href=#531--5261-api-pack-a-massive-multilingual-dataset-for-api-call-generation-zhen-guo-et-al-2024>(5/31 | 5/261) API Pack: A Massive Multilingual Dataset for API Call Generation (Zhen Guo et al., 2024)</a></li><li><a href=#631--6261-towards-privacy-aware-sign-language-translation-at-scale-phillip-rust-et-al-2024>(6/31 | 6/261) Towards Privacy-Aware Sign Language Translation at Scale (Phillip Rust et al., 2024)</a></li><li><a href=#731--7261-maxmin-rlhf-towards-equitable-alignment-of-large-language-models-with-diverse-human-preferences-souradip-chakraborty-et-al-2024>(7/31 | 7/261) MaxMin-RLHF: Towards Equitable Alignment of Large Language Models with Diverse Human Preferences (Souradip Chakraborty et al., 2024)</a></li><li><a href=#831--8261-probabilistic-reasoning-in-generative-large-language-models-aliakbar-nafar-et-al-2024>(8/31 | 8/261) Probabilistic Reasoning in Generative Large Language Models (Aliakbar Nafar et al., 2024)</a></li><li><a href=#931--9261-icdpo-effectively-borrowing-alignment-capability-of-others-via-in-context-direct-preference-optimization-feifan-song-et-al-2024>(9/31 | 9/261) ICDPO: Effectively Borrowing Alignment Capability of Others via In-context Direct Preference Optimization (Feifan Song et al., 2024)</a></li><li><a href=#1031--10261-ten-words-only-still-help-improving-black-box-ai-generated-text-detection-via-proxy-guided-efficient-re-sampling-yuhui-shi-et-al-2024>(10/31 | 10/261) Ten Words Only Still Help: Improving Black-Box AI-Generated Text Detection via Proxy-Guided Efficient Re-Sampling (Yuhui Shi et al., 2024)</a></li><li><a href=#1131--11261-dolphcoder-echo-locating-code-large-language-models-with-diverse-and-multi-objective-instruction-tuning-yejie-wang-et-al-2024>(11/31 | 11/261) DolphCoder: Echo-Locating Code Large Language Models with Diverse and Multi-Objective Instruction Tuning (Yejie Wang et al., 2024)</a></li><li><a href=#1231--12261-dora-weight-decomposed-low-rank-adaptation-shih-yang-liu-et-al-2024>(12/31 | 12/261) DoRA: Weight-Decomposed Low-Rank Adaptation (Shih-Yang Liu et al., 2024)</a></li><li><a href=#1331--13261-self-alignment-for-factuality-mitigating-hallucinations-in-llms-via-self-evaluation-xiaoying-zhang-et-al-2024>(13/31 | 13/261) Self-Alignment for Factuality: Mitigating Hallucinations in LLMs via Self-Evaluation (Xiaoying Zhang et al., 2024)</a></li><li><a href=#1431--14261-rationality-report-cards-assessing-the-economic-rationality-of-large-language-models-narun-raman-et-al-2024>(14/31 | 14/261) Rationality Report Cards: Assessing the Economic Rationality of Large Language Models (Narun Raman et al., 2024)</a></li><li><a href=#1531--15261-generalization-in-healthcare-ai-evaluation-of-a-clinical-large-language-model-salman-rahman-et-al-2024>(15/31 | 15/261) Generalization in Healthcare AI: Evaluation of a Clinical Large Language Model (Salman Rahman et al., 2024)</a></li><li><a href=#1631--16261-scaling-the-authoring-of-autotutors-with-large-language-models-sankalan-pal-chowdhury-et-al-2024>(16/31 | 16/261) Scaling the Authoring of AutoTutors with Large Language Models (Sankalan Pal Chowdhury et al., 2024)</a></li><li><a href=#1731--17261-sleb-streamlining-llms-through-redundancy-verification-and-elimination-of-transformer-blocks-jiwon-song-et-al-2024>(17/31 | 17/261) SLEB: Streamlining LLMs through Redundancy Verification and Elimination of Transformer Blocks (Jiwon Song et al., 2024)</a></li><li><a href=#1831--18261-structured-language-generation-model-for-robust-structure-prediction-minho-lee-et-al-2024>(18/31 | 18/261) Structured Language Generation Model for Robust Structure Prediction (Minho Lee et al., 2024)</a></li><li><a href=#1931--19261-copyright-traps-for-large-language-models-matthieu-meeus-et-al-2024>(19/31 | 19/261) Copyright Traps for Large Language Models (Matthieu Meeus et al., 2024)</a></li><li><a href=#2031--20261-long-form-evaluation-of-model-editing-domenic-rosati-et-al-2024>(20/31 | 20/261) Long-form evaluation of model editing (Domenic Rosati et al., 2024)</a></li><li><a href=#2131--21261-tree-based-hard-attention-with-self-motivation-for-large-language-models-chenxi-lin-et-al-2024>(21/31 | 21/261) Tree-Based Hard Attention with Self-Motivation for Large Language Models (Chenxi Lin et al., 2024)</a></li><li><a href=#2231--22261-irrationality-and-cognitive-biases-in-large-language-models-olivia-macmillan-scott-et-al-2024>(22/31 | 22/261) (Ir)rationality and Cognitive Biases in Large Language Models (Olivia Macmillan-Scott et al., 2024)</a></li><li><a href=#2331--23261-advancing-nlp-models-with-strategic-text-augmentation-a-comprehensive-study-of-augmentation-methods-and-curriculum-strategies-himmet-toprak-kesgin-et-al-2024>(23/31 | 23/261) Advancing NLP Models with Strategic Text Augmentation: A Comprehensive Study of Augmentation Methods and Curriculum Strategies (Himmet Toprak Kesgin et al., 2024)</a></li><li><a href=#2431--24261-massively-multi-cultural-knowledge-acquisition--lm-benchmarking-yi-fung-et-al-2024>(24/31 | 24/261) Massively Multi-Cultural Knowledge Acquisition & LM Benchmarking (Yi Fung et al., 2024)</a></li><li><a href=#2531--25261-logicprpbank-a-corpus-for-logical-implication-and-equivalence-zhexiong-liu-et-al-2024>(25/31 | 25/261) LogicPrpBank: A Corpus for Logical Implication and Equivalence (Zhexiong Liu et al., 2024)</a></li><li><a href=#2631--26261-chinese-mentalbert-domain-adaptive-pre-training-on-social-media-for-chinese-mental-health-text-analysis-wei-zhai-et-al-2024>(26/31 | 26/261) Chinese MentalBERT: Domain-Adaptive Pre-training on Social Media for Chinese Mental Health Text Analysis (Wei Zhai et al., 2024)</a></li><li><a href=#2731--27261-attacks-defenses-and-evaluations-for-llm-conversation-safety-a-survey-zhichen-dong-et-al-2024>(27/31 | 27/261) Attacks, Defenses and Evaluations for LLM Conversation Safety: A Survey (Zhichen Dong et al., 2024)</a></li><li><a href=#2831--28261-syntaxshap-syntax-aware-explainability-method-for-text-generation-kenza-amara-et-al-2024>(28/31 | 28/261) SyntaxShap: Syntax-aware Explainability Method for Text Generation (Kenza Amara et al., 2024)</a></li><li><a href=#2931--29261-towards-better-human-agent-alignment-assessing-task-utility-in-llm-powered-applications-negar-arabzadeh-et-al-2024>(29/31 | 29/261) Towards better Human-Agent Alignment: Assessing Task Utility in LLM-Powered Applications (Negar Arabzadeh et al., 2024)</a></li><li><a href=#3031--30261-tell-me-more-towards-implicit-user-intention-understanding-of-language-model-driven-agents-cheng-qian-et-al-2024>(30/31 | 30/261) Tell Me More! Towards Implicit User Intention Understanding of Language Model Driven Agents (Cheng Qian et al., 2024)</a></li><li><a href=#3131--31261-generating-diverse-translation-with-perturbed-knn-mt-yuto-nishida-et-al-2024>(31/31 | 31/261) Generating Diverse Translation with Perturbed kNN-MT (Yuto Nishida et al., 2024)</a></li></ul></li><li><a href=#csai-27>cs.AI (27)</a><ul><li><a href=#127--32261-mustard-mastering-uniform-synthesis-of-theorem-and-proof-data-yinya-huang-et-al-2024>(1/27 | 32/261) MUSTARD: Mastering Uniform Synthesis of Theorem and Proof Data (Yinya Huang et al., 2024)</a></li><li><a href=#227--33261-l3go-language-agents-with-chain-of-3d-thoughts-for-generating-unconventional-objects-yutaro-yamada-et-al-2024>(2/27 | 33/261) L3GO: Language Agents with Chain-of-3D-Thoughts for Generating Unconventional Objects (Yutaro Yamada et al., 2024)</a></li><li><a href=#327--34261-llasmol-advancing-large-language-models-for-chemistry-with-a-large-scale-comprehensive-high-quality-instruction-tuning-dataset-botao-yu-et-al-2024>(3/27 | 34/261) LlaSMol: Advancing Large Language Models for Chemistry with a Large-Scale, Comprehensive, High-Quality Instruction Tuning Dataset (Botao Yu et al., 2024)</a></li><li><a href=#427--35261-llm-enhanced-user-item-interactions-leveraging-edge-information-for-optimized-recommendations-xinyuan-wang-et-al-2024>(4/27 | 35/261) LLM-Enhanced User-Item Interactions: Leveraging Edge Information for Optimized Recommendations (Xinyuan Wang et al., 2024)</a></li><li><a href=#527--36261-large-language-model-with-graph-convolution-for-recommendation-yingpeng-du-et-al-2024>(5/27 | 36/261) Large Language Model with Graph Convolution for Recommendation (Yingpeng Du et al., 2024)</a></li><li><a href=#627--37261-role-playing-simulation-games-using-chatgpt-rita-stampfl-et-al-2024>(6/27 | 37/261) Role-Playing Simulation Games using ChatGPT (Rita Stampfl et al., 2024)</a></li><li><a href=#727--38261-groundial-human-norm-grounded-safe-dialog-response-generation-siwon-kim-et-al-2024>(7/27 | 38/261) GrounDial: Human-norm Grounded Safe Dialog Response Generation (Siwon Kim et al., 2024)</a></li><li><a href=#827--39261-fgeo-tp-a-language-model-enhanced-solver-for-geometry-problems-yiming-he-et-al-2024>(8/27 | 39/261) FGeo-TP: A Language Model-Enhanced Solver for Geometry Problems (Yiming He et al., 2024)</a></li><li><a href=#927--40261-using-counterfactual-tasks-to-evaluate-the-generality-of-analogical-reasoning-in-large-language-models-martha-lewis-et-al-2024>(9/27 | 40/261) Using Counterfactual Tasks to Evaluate the Generality of Analogical Reasoning in Large Language Models (Martha Lewis et al., 2024)</a></li><li><a href=#1027--41261-hgot-hierarchical-graph-of-thoughts-for-retrieval-augmented-in-context-learning-in-factuality-evaluation-yihao-fang-et-al-2024>(10/27 | 41/261) HGOT: Hierarchical Graph of Thoughts for Retrieval-Augmented In-Context Learning in Factuality Evaluation (Yihao Fang et al., 2024)</a></li><li><a href=#1127--42261-premise-order-matters-in-reasoning-with-large-language-models-xinyun-chen-et-al-2024>(11/27 | 42/261) Premise Order Matters in Reasoning with Large Language Models (Xinyun Chen et al., 2024)</a></li><li><a href=#1227--43261-emerging-opportunities-of-using-large-language-models-for-translation-between-drug-molecules-and-indications-david-oniani-et-al-2024>(12/27 | 43/261) Emerging Opportunities of Using Large Language Models for Translation Between Drug Molecules and Indications (David Oniani et al., 2024)</a></li><li><a href=#1327--44261-large-language-model-based-interpretable-machine-learning-control-in-building-energy-systems-liang-zhang-et-al-2024>(13/27 | 44/261) Large Language Model-Based Interpretable Machine Learning Control in Building Energy Systems (Liang Zhang et al., 2024)</a></li><li><a href=#1427--45261-integrating-chatgpt-into-secure-hospital-networks-a-case-study-on-improving-radiology-report-analysis-kyungsu-kim-et-al-2024>(14/27 | 45/261) Integrating ChatGPT into Secure Hospital Networks: A Case Study on Improving Radiology Report Analysis (Kyungsu Kim et al., 2024)</a></li><li><a href=#1527--46261-developing-a-framework-for-auditing-large-language-models-using-human-in-the-loop-maryam-amirizaniani-et-al-2024>(15/27 | 46/261) Developing a Framework for Auditing Large Language Models Using Human-in-the-Loop (Maryam Amirizaniani et al., 2024)</a></li><li><a href=#1627--47261-exploring-the-adversarial-capabilities-of-large-language-models-lukas-struppek-et-al-2024>(16/27 | 47/261) Exploring the Adversarial Capabilities of Large Language Models (Lukas Struppek et al., 2024)</a></li><li><a href=#1727--48261-fgeo-drl-deductive-reasoning-for-geometric-problems-through-deep-reinforcement-learning-jia-zou-et-al-2024>(17/27 | 48/261) FGeo-DRL: Deductive Reasoning for Geometric Problems through Deep Reinforcement Learning (Jia Zou et al., 2024)</a></li><li><a href=#1827--49261-into-the-unknown-self-learning-large-language-models-teddy-ferdinan-et-al-2024>(18/27 | 49/261) Into the Unknown: Self-Learning Large Language Models (Teddy Ferdinan et al., 2024)</a></li><li><a href=#1927--50261-bidirectional-generative-pre-training-for-improving-time-series-representation-learning-ziyang-song-et-al-2024>(19/27 | 50/261) Bidirectional Generative Pre-training for Improving Time Series Representation Learning (Ziyang Song et al., 2024)</a></li><li><a href=#2027--51261-auditllm-a-tool-for-auditing-large-language-models-using-multiprobe-approach-maryam-amirizaniani-et-al-2024>(20/27 | 51/261) AuditLLM: A Tool for Auditing Large Language Models Using Multiprobe Approach (Maryam Amirizaniani et al., 2024)</a></li><li><a href=#2127--52261-spectral-filters-dark-signals-and-attention-sinks-nicola-cancedda-2024>(21/27 | 52/261) Spectral Filters, Dark Signals, and Attention Sinks (Nicola Cancedda, 2024)</a></li><li><a href=#2227--53261-exploring-neuron-interactions-and-emergence-in-llms-from-the-multifractal-analysis-perspective-xiongye-xiao-et-al-2024>(22/27 | 53/261) Exploring Neuron Interactions and Emergence in LLMs: From the Multifractal Analysis Perspective (Xiongye Xiao et al., 2024)</a></li><li><a href=#2327--54261-inference-of-abstraction-for-a-unified-account-of-reasoning-and-learning-hiroyuki-kido-2024>(23/27 | 54/261) Inference of Abstraction for a Unified Account of Reasoning and Learning (Hiroyuki Kido, 2024)</a></li><li><a href=#2427--55261-hycube-efficient-knowledge-hypergraph-3d-circular-convolutional-embedding-zhao-li-et-al-2024>(24/27 | 55/261) HyCubE: Efficient Knowledge Hypergraph 3D Circular Convolutional Embedding (Zhao Li et al., 2024)</a></li><li><a href=#2527--56261-scamspot-fighting-financial-fraud-in-instagram-comments-stefan-erben-et-al-2024>(25/27 | 56/261) ScamSpot: Fighting Financial Fraud in Instagram Comments (Stefan Erben et al., 2024)</a></li><li><a href=#2627--57261-graph-skeleton-1-nodes-are-sufficient-to-represent-billion-scale-graph-linfeng-cao-et-al-2024>(26/27 | 57/261) Graph-Skeleton: ~1% Nodes are Sufficient to Represent Billion-Scale Graph (Linfeng Cao et al., 2024)</a></li><li><a href=#2727--58261-machine-learning-in-management-of-precautionary-closures-caused-by-lipophilic-biotoxins-andres-molares-ulloa-et-al-2024>(27/27 | 58/261) Machine Learning in management of precautionary closures caused by lipophilic biotoxins (Andres Molares-Ulloa et al., 2024)</a></li></ul></li><li><a href=#cslg-57>cs.LG (57)</a><ul><li><a href=#157--59261-reinforcement-learning-from-human-feedback-with-active-queries-kaixuan-ji-et-al-2024>(1/57 | 59/261) Reinforcement Learning from Human Feedback with Active Queries (Kaixuan Ji et al., 2024)</a></li><li><a href=#257--60261-graph-inference-acceleration-by-learning-mlps-on-graphs-without-supervision-zehong-wang-et-al-2024>(2/57 | 60/261) Graph Inference Acceleration by Learning MLPs on Graphs without Supervision (Zehong Wang et al., 2024)</a></li><li><a href=#357--61261-low-rank-graph-contrastive-learning-for-node-classification-yancheng-wang-et-al-2024>(3/57 | 61/261) Low-Rank Graph Contrastive Learning for Node Classification (Yancheng Wang et al., 2024)</a></li><li><a href=#457--62261-learning-interpretable-policies-in-hindsight-observable-pomdps-through-partially-supervised-reinforcement-learning-michael-lanier-et-al-2024>(4/57 | 62/261) Learning Interpretable Policies in Hindsight-Observable POMDPs through Partially Supervised Reinforcement Learning (Michael Lanier et al., 2024)</a></li><li><a href=#557--63261-enhancing-sequential-model-performance-with-squared-sigmoid-tanh-sst-activation-under-data-constraints-barathi-subramanian-et-al-2024>(5/57 | 63/261) Enhancing Sequential Model Performance with Squared Sigmoid TanH (SST) Activation Under Data Constraints (Barathi Subramanian et al., 2024)</a></li><li><a href=#657--64261-grassrep-graph-based-self-supervised-learning-for-repeat-detection-in-metagenomic-assembly-ali-azizpour-et-al-2024>(6/57 | 64/261) GraSSRep: Graph-Based Self-Supervised Learning for Repeat Detection in Metagenomic Assembly (Ali Azizpour et al., 2024)</a></li><li><a href=#757--65261-multi-hierarchical-surrogate-learning-for-structural-dynamical-crash-simulations-using-graph-convolutional-neural-networks-jonas-kneifl-et-al-2024>(7/57 | 65/261) Multi-Hierarchical Surrogate Learning for Structural Dynamical Crash Simulations Using Graph Convolutional Neural Networks (Jonas Kneifl et al., 2024)</a></li><li><a href=#857--66261-parametric-learning-of-time-advancement-operators-for-unstable-flame-evolution-rixin-yu-et-al-2024>(8/57 | 66/261) Parametric Learning of Time-Advancement Operators for Unstable Flame Evolution (Rixin Yu et al., 2024)</a></li><li><a href=#957--67261-fedsikd-clients-similarity-and-knowledge-distillation-addressing-non-iid-and-constraints-in-federated-learning-yousef-alsenani-et-al-2024>(9/57 | 67/261) FedSiKD: Clients Similarity and Knowledge Distillation: Addressing Non-i.i.d. and Constraints in Federated Learning (Yousef Alsenani et al., 2024)</a></li><li><a href=#1057--68261-robust-training-of-temporal-gnns-using-nearest-neighbours-based-hard-negatives-shubham-gupta-et-al-2024>(10/57 | 68/261) Robust Training of Temporal GNNs using Nearest Neighbours based Hard Negatives (Shubham Gupta et al., 2024)</a></li><li><a href=#1157--69261-switch-ema-a-free-lunch-for-better-flatness-and-sharpness-siyuan-li-et-al-2024>(11/57 | 69/261) Switch EMA: A Free Lunch for Better Flatness and Sharpness (Siyuan Li et al., 2024)</a></li><li><a href=#1257--70261-attacking-large-language-models-with-projected-gradient-descent-simon-geisler-et-al-2024>(12/57 | 70/261) Attacking Large Language Models with Projected Gradient Descent (Simon Geisler et al., 2024)</a></li><li><a href=#1357--71261-measuring-exploration-in-reinforcement-learning-via-optimal-transport-in-policy-space-reabetswe-m-nkhumise-et-al-2024>(13/57 | 71/261) Measuring Exploration in Reinforcement Learning via Optimal Transport in Policy Space (Reabetswe M. Nkhumise et al., 2024)</a></li><li><a href=#1457--72261-werank-towards-rank-degradation-prevention-for-self-supervised-learning-using-weight-regularization-ali-saheb-pasand-et-al-2024>(14/57 | 72/261) WERank: Towards Rank Degradation Prevention for Self-Supervised Learning Using Weight Regularization (Ali Saheb Pasand et al., 2024)</a></li><li><a href=#1557--73261-get-more-with-less-synthesizing-recurrence-with-kv-cache-compression-for-efficient-llm-inference-harry-dong-et-al-2024>(15/57 | 73/261) Get More with LESS: Synthesizing Recurrence with KV Cache Compression for Efficient LLM Inference (Harry Dong et al., 2024)</a></li><li><a href=#1657--74261-mitigating-reward-hacking-via-information-theoretic-reward-modeling-yuchun-miao-et-al-2024>(16/57 | 74/261) Mitigating Reward Hacking via Information-Theoretic Reward Modeling (Yuchun Miao et al., 2024)</a></li><li><a href=#1757--75261-transformers-parallel-computation-and-logarithmic-depth-clayton-sanford-et-al-2024>(17/57 | 75/261) Transformers, parallel computation, and logarithmic depth (Clayton Sanford et al., 2024)</a></li><li><a href=#1857--76261-soft-prompt-threats-attacking-safety-alignment-and-unlearning-in-open-source-llms-through-the-embedding-space-leo-schwinn-et-al-2024>(18/57 | 76/261) Soft Prompt Threats: Attacking Safety Alignment and Unlearning in Open-Source LLMs through the Embedding Space (Leo Schwinn et al., 2024)</a></li><li><a href=#1957--77261-i-cant-see-it-but-i-can-fine-tune-it-on-encrypted-fine-tuning-of-transformers-using-fully-homomorphic-encryption-prajwal-panzade-et-al-2024>(19/57 | 77/261) I can&rsquo;t see it but I can Fine-tune it: On Encrypted Fine-tuning of Transformers using Fully Homomorphic Encryption (Prajwal Panzade et al., 2024)</a></li><li><a href=#2057--78261-towards-next-level-post-training-quantization-of-hyper-scale-transformers-junhan-kim-et-al-2024>(20/57 | 78/261) Towards Next-Level Post-Training Quantization of Hyper-Scale Transformers (Junhan Kim et al., 2024)</a></li><li><a href=#2157--79261-second-order-methods-for-bandit-optimization-and-control-arun-suggala-et-al-2024>(21/57 | 79/261) Second Order Methods for Bandit Optimization and Control (Arun Suggala et al., 2024)</a></li><li><a href=#2257--80261-embracing-the-black-box-heading-towards-foundation-models-for-causal-discovery-from-time-series-data-gideon-stein-et-al-2024>(22/57 | 80/261) Embracing the black box: Heading towards foundation models for causal discovery from time series data (Gideon Stein et al., 2024)</a></li><li><a href=#2357--81261-evolving-restricted-boltzmann-machine-kohonen-network-for-online-clustering-j-senthilnath-et-al-2024>(23/57 | 81/261) Evolving Restricted Boltzmann Machine-Kohonen Network for Online Clustering (J. Senthilnath et al., 2024)</a></li><li><a href=#2457--82261-learning-interpretable-concepts-unifying-causal-representation-learning-and-foundation-models-goutham-rajendran-et-al-2024>(24/57 | 82/261) Learning Interpretable Concepts: Unifying Causal Representation Learning and Foundation Models (Goutham Rajendran et al., 2024)</a></li><li><a href=#2557--83261-directional-convergence-near-small-initializations-and-saddles-in-two-homogeneous-neural-networks-akshay-kumar-et-al-2024>(25/57 | 83/261) Directional Convergence Near Small Initializations and Saddles in Two-Homogeneous Neural Networks (Akshay Kumar et al., 2024)</a></li><li><a href=#2657--84261-leveraging-the-context-through-multi-round-interactions-for-jailbreaking-attacks-yixin-cheng-et-al-2024>(26/57 | 84/261) Leveraging the Context through Multi-Round Interactions for Jailbreaking Attacks (Yixin Cheng et al., 2024)</a></li><li><a href=#2757--85261-scalable-graph-self-supervised-learning-ali-saheb-pasand-et-al-2024>(27/57 | 85/261) Scalable Graph Self-Supervised Learning (Ali Saheb Pasand et al., 2024)</a></li><li><a href=#2857--86261-loss-shaping-constraints-for-long-term-time-series-forecasting-ignacio-hounie-et-al-2024>(28/57 | 86/261) Loss Shaping Constraints for Long-Term Time Series Forecasting (Ignacio Hounie et al., 2024)</a></li><li><a href=#2957--87261-tackling-negative-transfer-on-graphs-zehong-wang-et-al-2024>(29/57 | 87/261) Tackling Negative Transfer on Graphs (Zehong Wang et al., 2024)</a></li><li><a href=#3057--88261-a-dynamical-view-of-the-question-of-why-mehdi-fatemi-et-al-2024>(30/57 | 88/261) A Dynamical View of the Question of Why (Mehdi Fatemi et al., 2024)</a></li><li><a href=#3157--89261-hire-high-recall-approximate-top-k-estimation-for-efficient-llm-inference-yashas-samaga-b-l-et-al-2024>(31/57 | 89/261) HiRE: High Recall Approximate Top-$k$ Estimation for Efficient LLM Inference (Yashas Samaga B L et al., 2024)</a></li><li><a href=#3257--90261-research-and-application-of-transformer-based-anomaly-detection-model-a-literature-review-mingrui-ma-et-al-2024>(32/57 | 90/261) Research and application of Transformer based anomaly detection model: A literature review (Mingrui Ma et al., 2024)</a></li><li><a href=#3357--91261-duel-duplicate-elimination-on-active-memory-for-self-supervised-class-imbalanced-learning-won-seok-choi-et-al-2024>(33/57 | 91/261) DUEL: Duplicate Elimination on Active Memory for Self-Supervised Class-Imbalanced Learning (Won-Seok Choi et al., 2024)</a></li><li><a href=#3457--92261-mean-field-analysis-for-learning-subspace-sparse-polynomials-with-gaussian-input-ziang-chen-et-al-2024>(34/57 | 92/261) Mean-Field Analysis for Learning Subspace-Sparse Polynomials with Gaussian Input (Ziang Chen et al., 2024)</a></li><li><a href=#3557--93261-imuoptimize-a-data-driven-approach-to-optimal-imu-placement-for-human-pose-estimation-with-transformer-architecture-varun-ramani-et-al-2024>(35/57 | 93/261) IMUOptimize: A Data-Driven Approach to Optimal IMU Placement for Human Pose Estimation with Transformer Architecture (Varun Ramani et al., 2024)</a></li><li><a href=#3657--94261-unifying-invariance-and-spuriousity-for-graph-out-of-distribution-via-probability-of-necessity-and-sufficiency-xuexin-chen-et-al-2024>(36/57 | 94/261) Unifying Invariance and Spuriousity for Graph Out-of-Distribution via Probability of Necessity and Sufficiency (Xuexin Chen et al., 2024)</a></li><li><a href=#3757--95261-exploring-federated-deep-learning-for-standardising-naming-conventions-in-radiotherapy-data-ali-haidar-et-al-2024>(37/57 | 95/261) Exploring Federated Deep Learning for Standardising Naming Conventions in Radiotherapy Data (Ali Haidar et al., 2024)</a></li><li><a href=#3857--96261-layerwise-proximal-replay-a-proximal-point-method-for-online-continual-learning-jason-yoo-et-al-2024>(38/57 | 96/261) Layerwise Proximal Replay: A Proximal Point Method for Online Continual Learning (Jason Yoo et al., 2024)</a></li><li><a href=#3957--97261-exact-fast-and-expressive-poisson-point-processes-via-squared-neural-families-russell-tsuchida-et-al-2024>(39/57 | 97/261) Exact, Fast and Expressive Poisson Point Processes via Squared Neural Families (Russell Tsuchida et al., 2024)</a></li><li><a href=#4057--98261-momentum-approximation-in-asynchronous-private-federated-learning-tao-yu-et-al-2024>(40/57 | 98/261) Momentum Approximation in Asynchronous Private Federated Learning (Tao Yu et al., 2024)</a></li><li><a href=#4157--99261-pmgda-a-preference-based-multiple-gradient-descent-algorithm-xiaoyuan-zhang-et-al-2024>(41/57 | 99/261) PMGDA: A Preference-based Multiple Gradient Descent Algorithm (Xiaoyuan Zhang et al., 2024)</a></li><li><a href=#4257--100261-changes-by-butterflies-farsighted-forecasting-with-group-reservoir-transformer-md-kowsher-et-al-2024>(42/57 | 100/261) Changes by Butterflies: Farsighted Forecasting with Group Reservoir Transformer (Md Kowsher et al., 2024)</a></li><li><a href=#4357--101261-the-manifold-density-function-an-intrinsic-method-for-the-validation-of-manifold-learning-benjamin-holmgren-et-al-2024>(43/57 | 101/261) The Manifold Density Function: An Intrinsic Method for the Validation of Manifold Learning (Benjamin Holmgren et al., 2024)</a></li><li><a href=#4457--102261-transformers-can-achieve-length-generalization-but-not-robustly-yongchao-zhou-et-al-2024>(44/57 | 102/261) Transformers Can Achieve Length Generalization But Not Robustly (Yongchao Zhou et al., 2024)</a></li><li><a href=#4557--103261-information-complexity-of-stochastic-convex-optimization-applications-to-generalization-and-memorization-idan-attias-et-al-2024>(45/57 | 103/261) Information Complexity of Stochastic Convex Optimization: Applications to Generalization and Memorization (Idan Attias et al., 2024)</a></li><li><a href=#4657--104261-stability-and-multigroup-fairness-in-ranking-with-uncertain-predictions-siddartha-devic-et-al-2024>(46/57 | 104/261) Stability and Multigroup Fairness in Ranking with Uncertain Predictions (Siddartha Devic et al., 2024)</a></li><li><a href=#4757--105261-hybrid-machine-learning-techniques-in-the-management-of-harmful-algal-blooms-impact-andres-molares-ulloa-et-al-2024>(47/57 | 105/261) Hybrid Machine Learning techniques in the management of harmful algal blooms impact (Andres Molares-Ulloa et al., 2024)</a></li><li><a href=#4857--106261-ur2m-uncertainty-and-resource-aware-event-detection-on-microcontrollers-hong-jia-et-al-2024>(48/57 | 106/261) UR2M: Uncertainty and Resource-Aware Event Detection on Microcontrollers (Hong Jia et al., 2024)</a></li><li><a href=#4957--107261-improved-regret-for-bandit-convex-optimization-with-delayed-feedback-yuanyu-wan-et-al-2024>(49/57 | 107/261) Improved Regret for Bandit Convex Optimization with Delayed Feedback (Yuanyu Wan et al., 2024)</a></li><li><a href=#5057--108261-resqunnstowards-enabling-deep-learning-in-quantum-convolution-neural-networks-muhammad-kashif-et-al-2024>(50/57 | 108/261) ResQuNNs:Towards Enabling Deep Learning in Quantum Convolution Neural Networks (Muhammad Kashif et al., 2024)</a></li><li><a href=#5157--109261-exploiting-estimation-bias-in-deep-double-q-learning-for-actor-critic-methods-alberto-sinigaglia-et-al-2024>(51/57 | 109/261) Exploiting Estimation Bias in Deep Double Q-Learning for Actor-Critic Methods (Alberto Sinigaglia et al., 2024)</a></li><li><a href=#5257--110261-reconstructing-the-geometry-of-random-geometric-graphs-han-huang-et-al-2024>(52/57 | 110/261) Reconstructing the Geometry of Random Geometric Graphs (Han Huang et al., 2024)</a></li><li><a href=#5357--111261-position-paper-challenges-and-opportunities-in-topological-deep-learning-theodore-papamarkou-et-al-2024>(53/57 | 111/261) Position Paper: Challenges and Opportunities in Topological Deep Learning (Theodore Papamarkou et al., 2024)</a></li><li><a href=#5457--112261-implementing-local-explainability-in-gradient-boosting-trees-feature-contribution-ángel-delgado-panadero-et-al-2024>(54/57 | 112/261) Implementing local-explainability in Gradient Boosting Trees: Feature Contribution (Ángel Delgado-Panadero et al., 2024)</a></li><li><a href=#5557--113261-when-representations-align-universality-in-representation-learning-dynamics-loek-van-rossem-et-al-2024>(55/57 | 113/261) When Representations Align: Universality in Representation Learning Dynamics (Loek van Rossem et al., 2024)</a></li><li><a href=#5657--114261-the-mirrored-influence-hypothesis-efficient-data-influence-estimation-by-harnessing-forward-passes-myeongseob-ko-et-al-2024>(56/57 | 114/261) The Mirrored Influence Hypothesis: Efficient Data Influence Estimation by Harnessing Forward Passes (Myeongseob Ko et al., 2024)</a></li><li><a href=#5757--115261-dataset-clustering-for-improved-offline-policy-learning-qiang-wang-et-al-2024>(57/57 | 115/261) Dataset Clustering for Improved Offline Policy Learning (Qiang Wang et al., 2024)</a></li></ul></li><li><a href=#csro-10>cs.RO (10)</a><ul><li><a href=#110--116261-how-secure-are-large-language-models-llms-for-navigation-in-urban-environments-congcong-wen-et-al-2024>(1/10 | 116/261) How Secure Are Large Language Models (LLMs) for Navigation in Urban Environments? (Congcong Wen et al., 2024)</a></li><li><a href=#210--117261-a-digital-twin-prototype-for-traffic-sign-recognition-of-a-learning-enabled-autonomous-vehicle-mohamed-abdelsalam-et-al-2024>(2/10 | 117/261) A Digital Twin prototype for traffic sign recognition of a learning-enabled autonomous vehicle (Mohamed AbdElSalam et al., 2024)</a></li><li><a href=#310--118261-design-and-realization-of-a-benchmarking-testbed-for-evaluating-autonomous-platooning-algorithms-michael-shaham-et-al-2024>(3/10 | 118/261) Design and Realization of a Benchmarking Testbed for Evaluating Autonomous Platooning Algorithms (Michael Shaham et al., 2024)</a></li><li><a href=#410--119261-disgnet-a-distance-graph-neural-network-for-forward-kinematics-learning-of-gough-stewart-platform-huizhi-zhu-et-al-2024>(4/10 | 119/261) DisGNet: A Distance Graph Neural Network for Forward Kinematics Learning of Gough-Stewart Platform (Huizhi Zhu et al., 2024)</a></li><li><a href=#510--120261-auto-encoding-bayesian-inverse-games-xinjie-liu-et-al-2024>(5/10 | 120/261) Auto-Encoding Bayesian Inverse Games (Xinjie Liu et al., 2024)</a></li><li><a href=#610--121261-safe-distributed-control-of-multi-robot-systems-with-communication-delays-luca-ballotta-et-al-2024>(6/10 | 121/261) Safe Distributed Control of Multi-Robot Systems with Communication Delays (Luca Ballotta et al., 2024)</a></li><li><a href=#710--122261-single-reset-divide--conquer-imitation-learning-alexandre-chenu-et-al-2024>(7/10 | 122/261) Single-Reset Divide & Conquer Imitation Learning (Alexandre Chenu et al., 2024)</a></li><li><a href=#810--123261-traj-lio-a-resilient-multi-lidar-multi-imu-state-estimator-through-sparse-gaussian-process-xin-zheng-et-al-2024>(8/10 | 123/261) Traj-LIO: A Resilient Multi-LiDAR Multi-IMU State Estimator Through Sparse Gaussian Process (Xin Zheng et al., 2024)</a></li><li><a href=#910--124261-awareness-in-robotics-an-early-perspective-from-the-viewpoint-of-the-eic-pathfinder-challenge-awareness-inside-cosimo-della-santina-et-al-2024>(9/10 | 124/261) Awareness in robotics: An early perspective from the viewpoint of the EIC Pathfinder Challenge &lsquo;Awareness Inside&rsquo;&rsquo; (Cosimo Della Santina et al., 2024)</a></li><li><a href=#1010--125261-multi-task-learning-of-active-fault-tolerant-controller-for-leg-failures-in-quadruped-robots-taixian-hou-et-al-2024>(10/10 | 125/261) Multi-Task Learning of Active Fault-Tolerant Controller for Leg Failures in Quadruped robots (Taixian Hou et al., 2024)</a></li></ul></li><li><a href=#csce-1>cs.CE (1)</a><ul><li><a href=#11--126261-multiscale-graph-neural-networks-with-adaptive-mesh-refinement-for-accelerating-mesh-based-simulations-roberto-perera-et-al-2024>(1/1 | 126/261) Multiscale graph neural networks with adaptive mesh refinement for accelerating mesh-based simulations (Roberto Perera et al., 2024)</a></li></ul></li><li><a href=#cssd-3>cs.SD (3)</a><ul><li><a href=#13--127261-domain-adaptation-for-contrastive-audio-language-models-soham-deshmukh-et-al-2024>(1/3 | 127/261) Domain Adaptation for Contrastive Audio-Language Models (Soham Deshmukh et al., 2024)</a></li><li><a href=#23--128261-arrange-inpaint-and-refine-steerable-long-term-music-audio-generation-and-editing-via-content-based-controls-liwei-lin-et-al-2024>(2/3 | 128/261) Arrange, Inpaint, and Refine: Steerable Long-term Music Audio Generation and Editing via Content-based Controls (Liwei Lin et al., 2024)</a></li><li><a href=#33--129261-leveraging-pre-trained-autoencoders-for-interpretable-prototype-learning-of-music-audio-pablo-alonso-jiménez-et-al-2024>(3/3 | 129/261) Leveraging Pre-Trained Autoencoders for Interpretable Prototype Learning of Music Audio (Pablo Alonso-Jiménez et al., 2024)</a></li></ul></li><li><a href=#cshc-6>cs.HC (6)</a><ul><li><a href=#16--130261-advancing-building-energy-modeling-with-large-language-models-exploration-and-case-studies-liang-zhang-et-al-2024>(1/6 | 130/261) Advancing Building Energy Modeling with Large Language Models: Exploration and Case Studies (Liang Zhang et al., 2024)</a></li><li><a href=#26--131261-evaluating-the-experience-of-lgbtq-people-using-large-language-model-based-chatbots-for-mental-health-support-zilin-ma-et-al-2024>(2/6 | 131/261) Evaluating the Experience of LGBTQ+ People Using Large Language Model Based Chatbots for Mental Health Support (Zilin Ma et al., 2024)</a></li><li><a href=#36--132261-agentlens-visual-analysis-for-agent-behaviors-in-llm-based-autonomous-systems-jiaying-lu-et-al-2024>(3/6 | 132/261) AgentLens: Visual Analysis for Agent Behaviors in LLM-based Autonomous Systems (Jiaying Lu et al., 2024)</a></li><li><a href=#46--133261-can-ai-and-humans-genuinely-communicate-constant-bonard-2024>(4/6 | 133/261) Can AI and humans genuinely communicate? (Constant Bonard, 2024)</a></li><li><a href=#56--134261-visualization-requirements-for-business-intelligence-analytics-a-goal-based-iterative-framework-ana-lavalle-et-al-2024>(5/6 | 134/261) Visualization Requirements for Business Intelligence Analytics: A Goal-Based, Iterative Framework (Ana Lavalle et al., 2024)</a></li><li><a href=#66--135261-prismatic-interactive-multi-view-cluster-analysis-of-concept-stocks-wong-kam-kwai-et-al-2024>(6/6 | 135/261) Prismatic: Interactive Multi-View Cluster Analysis of Concept Stocks (Wong Kam-Kwai et al., 2024)</a></li></ul></li><li><a href=#csdc-5>cs.DC (5)</a><ul><li><a href=#15--136261-mpirigen-mpi-code-generation-through-domain-specific-language-models-nadav-schneider-et-al-2024>(1/5 | 136/261) MPIrigen: MPI Code Generation through Domain-Specific Language Models (Nadav Schneider et al., 2024)</a></li><li><a href=#25--137261-stabilizing-agreement-is-impossible-in-delayed-message-passing-models-stephan-felber-et-al-2024>(2/5 | 137/261) Stabilizing Agreement is Impossible in Delayed Message Passing Models (Stephan Felber et al., 2024)</a></li><li><a href=#35--138261-scheduling-for-on-board-federated-learning-with-satellite-clusters-nasrin-razmi-et-al-2024>(3/5 | 138/261) Scheduling for On-Board Federated Learning with Satellite Clusters (Nasrin Razmi et al., 2024)</a></li><li><a href=#45--139261-an-evaluative-comparison-of-performance-portability-across-gpu-programming-models-joshua-h-davis-et-al-2024>(4/5 | 139/261) An Evaluative Comparison of Performance Portability across GPU Programming Models (Joshua H. Davis et al., 2024)</a></li><li><a href=#55--140261-limitless-faas-overcoming-serverless-functions-execution-time-limits-with-invoke-driven-architecture-and-memory-checkpoints-rodrigo-landa-andraca-et-al-2024>(5/5 | 140/261) Limitless FaaS: Overcoming serverless functions execution time limits with invoke driven architecture and memory checkpoints (Rodrigo Landa Andraca et al., 2024)</a></li></ul></li><li><a href=#cscv-36>cs.CV (36)</a><ul><li><a href=#136--141261-few-shot-object-detection-with-sparse-context-transformers-jie-mei-et-al-2024>(1/36 | 141/261) Few-Shot Object Detection with Sparse Context Transformers (Jie Mei et al., 2024)</a></li><li><a href=#236--142261-reducing-texture-bias-of-deep-neural-networks-via-edge-enhancing-diffusion-edgar-heinert-et-al-2024>(2/36 | 142/261) Reducing Texture Bias of Deep Neural Networks via Edge Enhancing Diffusion (Edgar Heinert et al., 2024)</a></li><li><a href=#336--143261-deepatlas-one-shot-localization-for-biomedical-data-peter-d-chang-2024>(3/36 | 143/261) DeepATLAS: One-Shot Localization for Biomedical Data (Peter D. Chang, 2024)</a></li><li><a href=#436--144261-prediction-of-activated-sludge-settling-characteristics-from-microscopy-images-with-deep-convolutional-neural-networks-and-transfer-learning-sina-borzooei-et-al-2024>(4/36 | 144/261) Prediction of Activated Sludge Settling Characteristics from Microscopy Images with Deep Convolutional Neural Networks and Transfer Learning (Sina Borzooei et al., 2024)</a></li><li><a href=#536--145261-magic-me-identity-specific-video-customized-diffusion-ze-ma-et-al-2024>(5/36 | 145/261) Magic-Me: Identity-Specific Video Customized Diffusion (Ze Ma et al., 2024)</a></li><li><a href=#636--146261-is-my-data-in-your-ai-model-membership-inference-test-with-application-to-face-images-daniel-dealcala-et-al-2024>(6/36 | 146/261) Is my Data in your AI Model? Membership Inference Test with Application to Face Images (Daniel DeAlcala et al., 2024)</a></li><li><a href=#736--147261-pyramid-attention-network-for-medical-image-registration-zhuoyuan-wang-et-al-2024>(7/36 | 147/261) Pyramid Attention Network for Medical Image Registration (Zhuoyuan Wang et al., 2024)</a></li><li><a href=#836--148261-pretraining-vision-language-model-for-difference-visual-question-answering-in-longitudinal-chest-x-rays-yeongjae-cho-et-al-2024>(8/36 | 148/261) Pretraining Vision-Language Model for Difference Visual Question Answering in Longitudinal Chest X-rays (Yeongjae Cho et al., 2024)</a></li><li><a href=#936--149261-moving-object-proposals-with-deep-learned-optical-flow-for-video-object-segmentation-ge-shi-et-al-2024>(9/36 | 149/261) Moving Object Proposals with Deep Learned Optical Flow for Video Object Segmentation (Ge Shi et al., 2024)</a></li><li><a href=#1036--150261-tdvit-temporal-dilated-video-transformer-for-dense-video-tasks-guanxiong-sun-et-al-2024>(10/36 | 150/261) TDViT: Temporal Dilated Video Transformer for Dense Video Tasks (Guanxiong Sun et al., 2024)</a></li><li><a href=#1136--151261-can-text-to-image-model-assist-multi-modal-learning-for-visual-recognition-with-visual-modality-missing-tiantian-feng-et-al-2024>(11/36 | 151/261) Can Text-to-image Model Assist Multi-modal Learning for Visual Recognition with Visual Modality Missing? (Tiantian Feng et al., 2024)</a></li><li><a href=#1236--152261-quantified-task-misalignment-to-inform-peft-an-exploration-of-domain-generalization-and-catastrophic-forgetting-in-clip-laura-niss-et-al-2024>(12/36 | 152/261) Quantified Task Misalignment to Inform PEFT: An Exploration of Domain Generalization and Catastrophic Forgetting in CLIP (Laura Niss et al., 2024)</a></li><li><a href=#1336--153261-domain-adaptive-and-subgroup-specific-cascaded-temperature-regression-for-out-of-distribution-calibration-jiexin-wang-et-al-2024>(13/36 | 153/261) Domain-adaptive and Subgroup-specific Cascaded Temperature Regression for Out-of-distribution Calibration (Jiexin Wang et al., 2024)</a></li><li><a href=#1436--154261-affine-transformation-estimation-improves-visual-self-supervised-learning-david-torpey-et-al-2024>(14/36 | 154/261) Affine transformation estimation improves visual self-supervised learning (David Torpey et al., 2024)</a></li><li><a href=#1536--155261-open-vocabulary-segmentation-with-unpaired-mask-text-supervision-zhaoqing-wang-et-al-2024>(15/36 | 155/261) Open-Vocabulary Segmentation with Unpaired Mask-Text Supervision (Zhaoqing Wang et al., 2024)</a></li><li><a href=#1636--156261-tiktokactions-a-tiktok-derived-video-dataset-for-human-action-recognition-yang-qian-et-al-2024>(16/36 | 156/261) TikTokActions: A TikTok-Derived Video Dataset for Human Action Recognition (Yang Qian et al., 2024)</a></li><li><a href=#1736--157261-only-my-model-on-my-data-a-privacy-preserving-approach-protecting-one-model-and-deceiving-unauthorized-black-box-models-weiheng-chai-et-al-2024>(17/36 | 157/261) Only My Model On My Data: A Privacy Preserving Approach Protecting one Model and Deceiving Unauthorized Black-Box Models (Weiheng Chai et al., 2024)</a></li><li><a href=#1836--158261-weatherproofing-retrieval-for-localization-with-generative-ai-and-geometric-consistency-yannis-kalantidis-et-al-2024>(18/36 | 158/261) Weatherproofing Retrieval for Localization with Generative AI and Geometric Consistency (Yannis Kalantidis et al., 2024)</a></li><li><a href=#1936--159261-synthesizing-knowledge-enhanced-features-for-real-world-zero-shot-food-detection-pengfei-zhou-et-al-2024>(19/36 | 159/261) Synthesizing Knowledge-enhanced Features for Real-world Zero-shot Food Detection (Pengfei Zhou et al., 2024)</a></li><li><a href=#2036--160261-yolov8-am-yolov8-with-attention-mechanisms-for-pediatric-wrist-fracture-detection-chun-tse-chien-et-al-2024>(20/36 | 160/261) YOLOv8-AM: YOLOv8 with Attention Mechanisms for Pediatric Wrist Fracture Detection (Chun-Tse Chien et al., 2024)</a></li><li><a href=#2136--161261-immediate-generalisation-in-humans-but-a-generalisation-lag-in-deep-neural-networks----evidence-for-representational-divergence-lukas-s-huber-et-al-2024>(21/36 | 161/261) Immediate generalisation in humans but a generalisation lag in deep neural networks &ndash; evidence for representational divergence? (Lukas S. Huber et al., 2024)</a></li><li><a href=#2236--162261-towards-realistic-landmark-guided-facial-video-inpainting-based-on-gans-fatemeh-ghorbani-lohesara-et-al-2024>(22/36 | 162/261) Towards Realistic Landmark-Guided Facial Video Inpainting Based on GANs (Fatemeh Ghorbani Lohesara et al., 2024)</a></li><li><a href=#2336--163261-weakly-supervised-segmentation-of-vertebral-bodies-with-iterative-slice-propagation-shiqi-peng-et-al-2024>(23/36 | 163/261) Weakly Supervised Segmentation of Vertebral Bodies with Iterative Slice-propagation (Shiqi Peng et al., 2024)</a></li><li><a href=#2436--164261-multimedeval-a-benchmark-and-a-toolkit-for-evaluating-medical-vision-language-models-corentin-royer-et-al-2024>(24/36 | 164/261) MultiMedEval: A Benchmark and a Toolkit for Evaluating Medical Vision-Language Models (Corentin Royer et al., 2024)</a></li><li><a href=#2536--165261-clip-mused-clip-guided-multi-subject-visual-neural-information-semantic-decoding-qiongyi-zhou-et-al-2024>(25/36 | 165/261) CLIP-MUSED: CLIP-Guided Multi-Subject Visual Neural Information Semantic Decoding (Qiongyi Zhou et al., 2024)</a></li><li><a href=#2636--166261-comment-aided-video-language-alignment-via-contrastive-pre-training-for-short-form-video-humor-detection-yang-liu-et-al-2024>(26/36 | 166/261) Comment-aided Video-Language Alignment via Contrastive Pre-training for Short-form Video Humor Detection (Yang Liu et al., 2024)</a></li><li><a href=#2736--167261-automated-plaque-detection-and-agatston-score-estimation-on-non-contrast-ct-scans-a-multicenter-study-andrew-m-nguyen-et-al-2024>(27/36 | 167/261) Automated Plaque Detection and Agatston Score Estimation on Non-Contrast CT Scans: A Multicenter Study (Andrew M. Nguyen et al., 2024)</a></li><li><a href=#2836--168261-patch-based-adaptive-temporal-filter-and-residual-evaluation-weiying-zhao-et-al-2024>(28/36 | 168/261) Patch-based adaptive temporal filter and residual evaluation (Weiying Zhao et al., 2024)</a></li><li><a href=#2936--169261-fast-window-based-event-denoising-with-spatiotemporal-correlation-enhancement-huachen-fang-et-al-2024>(29/36 | 169/261) Fast Window-Based Event Denoising with Spatiotemporal Correlation Enhancement (Huachen Fang et al., 2024)</a></li><li><a href=#3036--170261-efficient-one-stage-video-object-detection-by-exploiting-temporal-consistency-guanxiong-sun-et-al-2024>(30/36 | 170/261) Efficient One-stage Video Object Detection by Exploiting Temporal Consistency (Guanxiong Sun et al., 2024)</a></li><li><a href=#3136--171261-gradient-alignment-with-prototype-feature-for-fully-test-time-adaptation-juhyeon-shin-et-al-2024>(31/36 | 171/261) Gradient Alignment with Prototype Feature for Fully Test-time Adaptation (Juhyeon Shin et al., 2024)</a></li><li><a href=#3236--172261-depth-aware-volume-attention-for-texture-less-stereo-matching-tong-zhao-et-al-2024>(32/36 | 172/261) Depth-aware Volume Attention for Texture-less Stereo Matching (Tong Zhao et al., 2024)</a></li><li><a href=#3336--173261-headset-human-emotion-awareness-under-partial-occlusions-multimodal-dataset-fatemeh-ghorbani-lohesara-et-al-2024>(33/36 | 173/261) Headset: Human emotion awareness under partial occlusions multimodal dataset (Fatemeh Ghorbani Lohesara et al., 2024)</a></li><li><a href=#3436--174261-interpretable-measures-of-conceptual-similarity-by-complexity-constrained-descriptive-auto-encoding-alessandro-achille-et-al-2024>(34/36 | 174/261) Interpretable Measures of Conceptual Similarity by Complexity-Constrained Descriptive Auto-Encoding (Alessandro Achille et al., 2024)</a></li><li><a href=#3536--175261-loopy-slam-dense-neural-slam-with-loop-closures-lorenzo-liso-et-al-2024>(35/36 | 175/261) Loopy-SLAM: Dense Neural SLAM with Loop Closures (Lorenzo Liso et al., 2024)</a></li><li><a href=#3636--176261-generalized-portrait-quality-assessment-nicolas-chahine-et-al-2024>(36/36 | 176/261) Generalized Portrait Quality Assessment (Nicolas Chahine et al., 2024)</a></li></ul></li><li><a href=#cscr-7>cs.CR (7)</a><ul><li><a href=#17--177261-rapid-adoption-hidden-risks-the-dual-impact-of-large-language-model-customization-rui-zhang-et-al-2024>(1/7 | 177/261) Rapid Adoption, Hidden Risks: The Dual Impact of Large Language Model Customization (Rui Zhang et al., 2024)</a></li><li><a href=#27--178261-review-incorporated-model-agnostic-profile-injection-attacks-on-recommender-systems-shiyi-yang-et-al-2024>(2/7 | 178/261) Review-Incorporated Model-Agnostic Profile Injection Attacks on Recommender Systems (Shiyi Yang et al., 2024)</a></li><li><a href=#37--179261-safedecoding-defending-against-jailbreak-attacks-via-safety-aware-decoding-zhangchen-xu-et-al-2024>(3/7 | 179/261) SafeDecoding: Defending against Jailbreak Attacks via Safety-Aware Decoding (Zhangchen Xu et al., 2024)</a></li><li><a href=#47--180261-instruction-tuning-for-secure-code-generation-jingxuan-he-et-al-2024>(4/7 | 180/261) Instruction Tuning for Secure Code Generation (Jingxuan He et al., 2024)</a></li><li><a href=#57--181261-combatting-deepfakes-policies-to-address-national-security-threats-and-rights-violations-andrea-miotti-et-al-2024>(5/7 | 181/261) Combatting deepfakes: Policies to address national security threats and rights violations (Andrea Miotti et al., 2024)</a></li><li><a href=#67--182261-discovering-command-and-control-c2-channels-on-tor-and-public-networks-using-reinforcement-learning-cheng-wang-et-al-2024>(6/7 | 182/261) Discovering Command and Control (C2) Channels on Tor and Public Networks Using Reinforcement Learning (Cheng Wang et al., 2024)</a></li><li><a href=#77--183261-play-guessing-game-with-llm-indirect-jailbreak-attack-with-implicit-clues-zhiyuan-chang-et-al-2024>(7/7 | 183/261) Play Guessing Game with LLM: Indirect Jailbreak Attack with Implicit Clues (Zhiyuan Chang et al., 2024)</a></li></ul></li><li><a href=#csir-5>cs.IR (5)</a><ul><li><a href=#15--184261-large-language-model-interaction-simulator-for-cold-start-item-recommendation-feiran-huang-et-al-2024>(1/5 | 184/261) Large Language Model Interaction Simulator for Cold-Start Item Recommendation (Feiran Huang et al., 2024)</a></li><li><a href=#25--185261-rethinking-large-language-model-architectures-for-sequential-recommendations-hanbing-wang-et-al-2024>(2/5 | 185/261) Rethinking Large Language Model Architectures for Sequential Recommendations (Hanbing Wang et al., 2024)</a></li><li><a href=#35--186261-confidence-aware-fine-tuning-of-sequential-recommendation-systems-via-conformal-prediction-chen-wang-et-al-2024>(3/5 | 186/261) Confidence-aware Fine-tuning of Sequential Recommendation Systems via Conformal Prediction (Chen Wang et al., 2024)</a></li><li><a href=#45--187261-recommendation-algorithm-based-on-recommendation-sessions-michał-malinowski-2024>(4/5 | 187/261) Recommendation Algorithm Based on Recommendation Sessions (Michał Malinowski, 2024)</a></li><li><a href=#55--188261-enhancing-id-and-text-fusion-via-alternative-training-in-session-based-recommendation-juanhui-li-et-al-2024>(5/5 | 188/261) Enhancing ID and Text Fusion via Alternative Training in Session-based Recommendation (Juanhui Li et al., 2024)</a></li></ul></li><li><a href=#eesssy-5>eess.SY (5)</a><ul><li><a href=#15--189261-uncertainty-aware-transient-stability-constrained-preventive-redispatch-a-distributional-reinforcement-learning-approach-zhengcheng-wang-et-al-2024>(1/5 | 189/261) Uncertainty-Aware Transient Stability-Constrained Preventive Redispatch: A Distributional Reinforcement Learning Approach (Zhengcheng Wang et al., 2024)</a></li><li><a href=#25--190261-a-practical-and-online-trajectory-planner-for-autonomous-ships-berthing-incorporating-speed-control-agnes-ngina-mwange-et-al-2024>(2/5 | 190/261) A Practical and Online Trajectory Planner for Autonomous Ships&rsquo; Berthing, Incorporating Speed Control (Agnes Ngina Mwange et al., 2024)</a></li><li><a href=#35--191261-dynamic-modeling-and-predictive-control-of-a-microfluidic-system-jorge-vicente-martinez-et-al-2024>(3/5 | 191/261) Dynamic modeling and predictive control of a microfluidic system (Jorge Vicente Martinez et al., 2024)</a></li><li><a href=#45--192261-learning-enabled-flexible-job-shop-scheduling-for-scalable-smart-manufacturing-sihoon-moon-et-al-2024>(4/5 | 192/261) Learning-enabled Flexible Job-shop Scheduling for Scalable Smart Manufacturing (Sihoon Moon et al., 2024)</a></li><li><a href=#55--193261-steady-state-error-compensation-for-reinforcement-learning-with-quadratic-rewards-liyao-wang-et-al-2024>(5/5 | 193/261) Steady-State Error Compensation for Reinforcement Learning with Quadratic Rewards (Liyao Wang et al., 2024)</a></li></ul></li><li><a href=#eessas-4>eess.AS (4)</a><ul><li><a href=#14--194261-unienc-cassnat-an-encoder-only-non-autoregressive-asr-for-speech-ssl-models-ruchao-fan-et-al-2024>(1/4 | 194/261) UniEnc-CASSNAT: An Encoder-only Non-autoregressive ASR for Speech SSL Models (Ruchao Fan et al., 2024)</a></li><li><a href=#24--195261-listening-to-multi-talker-conversations-modular-and-end-to-end-perspectives-desh-raj-2024>(2/4 | 195/261) Listening to Multi-talker Conversations: Modular and End-to-end Perspectives (Desh Raj, 2024)</a></li><li><a href=#34--196261-mobilespeech-a-fast-and-high-fidelity-framework-for-mobile-zero-shot-text-to-speech-shengpeng-ji-et-al-2024>(3/4 | 196/261) MobileSpeech: A Fast and High-Fidelity Framework for Mobile Zero-Shot Text-to-Speech (Shengpeng Ji et al., 2024)</a></li><li><a href=#44--197261-sound-field-reconstruction-using-a-compact-acoustics-informed-neural-network-fei-ma-et-al-2024>(4/4 | 197/261) Sound Field Reconstruction Using a Compact Acoustics-informed Neural Network (Fei Ma et al., 2024)</a></li></ul></li><li><a href=#eessiv-5>eess.IV (5)</a><ul><li><a href=#15--198261-omnimedvqa-a-new-large-scale-comprehensive-evaluation-benchmark-for-medical-lvlm-yutao-hu-et-al-2024>(1/5 | 198/261) OmniMedVQA: A New Large-Scale Comprehensive Evaluation Benchmark for Medical LVLM (Yutao Hu et al., 2024)</a></li><li><a href=#25--199261-destripecyclegan-stripe-simulation-cyclegan-for-unsupervised-infrared-image-destriping-shiqi-yang-et-al-2024>(2/5 | 199/261) DestripeCycleGAN: Stripe Simulation CycleGAN for Unsupervised Infrared Image Destriping (Shiqi Yang et al., 2024)</a></li><li><a href=#35--200261-tai-gan-a-temporally-and-anatomically-informed-generative-adversarial-network-for-early-to-late-frame-conversion-in-dynamic-cardiac-pet-inter-frame-motion-correction-xueqi-guo-et-al-2024>(3/5 | 200/261) TAI-GAN: A Temporally and Anatomically Informed Generative Adversarial Network for early-to-late frame conversion in dynamic cardiac PET inter-frame motion correction (Xueqi Guo et al., 2024)</a></li><li><a href=#45--201261-pruning-sparse-tensor-neural-networks-enables-deep-learning-for-3d-ultrasound-localization-microscopy-brice-rauby-et-al-2024>(4/5 | 201/261) Pruning Sparse Tensor Neural Networks Enables Deep Learning for 3D Ultrasound Localization Microscopy (Brice Rauby et al., 2024)</a></li><li><a href=#55--202261-deep-rib-fracture-instance-segmentation-and-classification-from-ct-on-the-ribfrac-challenge-jiancheng-yang-et-al-2024>(5/5 | 202/261) Deep Rib Fracture Instance Segmentation and Classification from CT on the RibFrac Challenge (Jiancheng Yang et al., 2024)</a></li></ul></li><li><a href=#eesssp-3>eess.SP (3)</a><ul><li><a href=#13--203261-lightweight-deep-learning-based-channel-estimation-for-extremely-large-scale-massive-mimo-systems-shen-gao-et-al-2024>(1/3 | 203/261) Lightweight Deep Learning Based Channel Estimation for Extremely Large-Scale Massive MIMO Systems (Shen Gao et al., 2024)</a></li><li><a href=#23--204261-dynamic-cooperative-mac-optimization-in-rsu-enhanced-vanets-a-distributed-approach-zhou-zhang-et-al-2024>(2/3 | 204/261) Dynamic Cooperative MAC Optimization in RSU-Enhanced VANETs: A Distributed Approach (Zhou Zhang et al., 2024)</a></li><li><a href=#33--205261-joint-and-robust-beamforming-framework-for-integrated-sensing-and-communication-systems-jinseok-choi-et-al-2024>(3/3 | 205/261) Joint and Robust Beamforming Framework for Integrated Sensing and Communication Systems (Jinseok Choi et al., 2024)</a></li></ul></li><li><a href=#csma-2>cs.MA (2)</a><ul><li><a href=#12--206261-abides-economist-agent-based-simulation-of-economic-systems-with-learning-agents-kshama-dwarakanath-et-al-2024>(1/2 | 206/261) ABIDES-Economist: Agent-Based Simulation of Economic Systems with Learning Agents (Kshama Dwarakanath et al., 2024)</a></li><li><a href=#22--207261-discovering-sensorimotor-agency-in-cellular-automata-using-diversity-search-gautier-hamon-et-al-2024>(2/2 | 207/261) Discovering Sensorimotor Agency in Cellular Automata using Diversity Search (Gautier Hamon et al., 2024)</a></li></ul></li><li><a href=#physicsplasm-ph-1>physics.plasm-ph (1)</a><ul><li><a href=#11--208261-active-disruption-avoidance-and-trajectory-design-for-tokamak-ramp-downs-with-neural-differential-equations-and-reinforcement-learning-allen-m-wang-et-al-2024>(1/1 | 208/261) Active Disruption Avoidance and Trajectory Design for Tokamak Ramp-downs with Neural Differential Equations and Reinforcement Learning (Allen M. Wang et al., 2024)</a></li></ul></li><li><a href=#hep-ph-1>hep-ph (1)</a><ul><li><a href=#11--209261-a-language-model-for-particle-tracking-andris-huang-et-al-2024>(1/1 | 209/261) A Language Model for Particle Tracking (Andris Huang et al., 2024)</a></li></ul></li><li><a href=#csse-8>cs.SE (8)</a><ul><li><a href=#18--210261-trained-without-my-consent-detecting-code-inclusion-in-language-models-trained-on-code-vahid-majdinasab-et-al-2024>(1/8 | 210/261) Trained Without My Consent: Detecting Code Inclusion In Language Models Trained on Code (Vahid Majdinasab et al., 2024)</a></li><li><a href=#28--211261-automated-unit-test-improvement-using-large-language-models-at-meta-nadia-alshahwan-et-al-2024>(2/8 | 211/261) Automated Unit Test Improvement using Large Language Models at Meta (Nadia Alshahwan et al., 2024)</a></li><li><a href=#38--212261-assessing-ai-based-code-assistants-in-method-generation-tasks-vincenzo-corso-et-al-2024>(3/8 | 212/261) Assessing AI-Based Code Assistants in Method Generation Tasks (Vincenzo Corso et al., 2024)</a></li><li><a href=#48--213261-enhancing-source-code-representations-for-deep-learning-with-static-analysis-xueting-guan-et-al-2024>(4/8 | 213/261) Enhancing Source Code Representations for Deep Learning with Static Analysis (Xueting Guan et al., 2024)</a></li><li><a href=#58--214261-context-composing-for-full-line-code-completion-anton-semenkin-et-al-2024>(5/8 | 214/261) Context Composing for Full Line Code Completion (Anton Semenkin et al., 2024)</a></li><li><a href=#68--215261-generative-ai-for-pull-request-descriptions-adoption-impact-and-developer-interventions-tao-xiao-et-al-2024>(6/8 | 215/261) Generative AI for Pull Request Descriptions: Adoption, Impact, and Developer Interventions (Tao Xiao et al., 2024)</a></li><li><a href=#78--216261-quantifying-and-characterizing-clones-of-self-admitted-technical-debt-in-build-systems-tao-xiao-et-al-2024>(7/8 | 216/261) Quantifying and Characterizing Clones of Self-Admitted Technical Debt in Build Systems (Tao Xiao et al., 2024)</a></li><li><a href=#88--217261-omnibor-a-system-for-automatic-verifiable-artifact-resolution-across-software-supply-chains-bharathi-seshadri-et-al-2024>(8/8 | 217/261) OmniBOR: A System for Automatic, Verifiable Artifact Resolution across Software Supply Chains (Bharathi Seshadri et al., 2024)</a></li></ul></li><li><a href=#astro-phsr-1>astro-ph.SR (1)</a><ul><li><a href=#11--218261-predicting-the-emergence-of-solar-active-regions-using-machine-learning-spiridon-kasapis-et-al-2024>(1/1 | 218/261) Predicting the Emergence of Solar Active Regions Using Machine Learning (Spiridon Kasapis et al., 2024)</a></li></ul></li><li><a href=#cssi-4>cs.SI (4)</a><ul><li><a href=#14--219261-signed-diverse-multiplex-networks-clustering-and-inference-marianna-pensky-2024>(1/4 | 219/261) Signed Diverse Multiplex Networks: Clustering and Inference (Marianna Pensky, 2024)</a></li><li><a href=#24--220261-insights-and-caveats-from-mining-local-and-global-temporal-motifs-in-cryptocurrency-transaction-networks-naomi-a-arnold-et-al-2024>(2/4 | 220/261) Insights and caveats from mining local and global temporal motifs in cryptocurrency transaction networks (Naomi A. Arnold et al., 2024)</a></li><li><a href=#34--221261-finding-densest-subgraphs-with-edge-color-constraints-lutz-oettershagen-et-al-2024>(3/4 | 221/261) Finding Densest Subgraphs with Edge-Color Constraints (Lutz Oettershagen et al., 2024)</a></li><li><a href=#44--222261-pearson-correlations-on-networks-corrigendum-michele-coscia-et-al-2024>(4/4 | 222/261) Pearson Correlations on Networks: Corrigendum (Michele Coscia et al., 2024)</a></li></ul></li><li><a href=#quant-ph-2>quant-ph (2)</a><ul><li><a href=#12--223261-efficient-unitary-t-designs-from-random-sums-chi-fang-chen-et-al-2024>(1/2 | 223/261) Efficient Unitary T-designs from Random Sums (Chi-Fang Chen et al., 2024)</a></li><li><a href=#22--224261-guided-quantum-compression-for-higgs-identification-vasilis-belis-et-al-2024>(2/2 | 224/261) Guided Quantum Compression for Higgs Identification (Vasilis Belis et al., 2024)</a></li></ul></li><li><a href=#physicsflu-dyn-1>physics.flu-dyn (1)</a><ul><li><a href=#11--225261-eulerian-formulation-of-the-tensor-based-morphology-equations-for-strain-based-blood-damage-modeling-nico-dirkes-et-al-2024>(1/1 | 225/261) Eulerian Formulation of the Tensor-Based Morphology Equations for Strain-Based Blood Damage Modeling (Nico Dirkes et al., 2024)</a></li></ul></li><li><a href=#econem-1>econ.EM (1)</a><ul><li><a href=#11--226261-inference-for-an-algorithmic-fairness-accuracy-frontier-yiqi-liu-et-al-2024>(1/1 | 226/261) Inference for an Algorithmic Fairness-Accuracy Frontier (Yiqi Liu et al., 2024)</a></li></ul></li><li><a href=#mathna-8>math.NA (8)</a><ul><li><a href=#18--227261-numerical-study-of-a-strongly-coupled-two-scale-system-with-nonlinear-dispersion-surendra-nepal-et-al-2024>(1/8 | 227/261) Numerical Study of a Strongly Coupled Two-scale System with Nonlinear Dispersion (Surendra Nepal et al., 2024)</a></li><li><a href=#28--228261-conformal-finite-element-methods-for-nonlinear-rosenau-burgers-biharmonic-models-ankur-et-al-2024>(2/8 | 228/261) Conformal Finite Element Methods for Nonlinear Rosenau-Burgers-Biharmonic Models (Ankur et al., 2024)</a></li><li><a href=#38--229261-a-locally-mass-conservative-enriched-petrov-galerkin-method-without-penalty-for-the-darcy-flow-in-porous-media-huangxin-chen-et-al-2024>(3/8 | 229/261) A locally mass-conservative enriched Petrov-Galerkin method without penalty for the Darcy flow in porous media (Huangxin Chen et al., 2024)</a></li><li><a href=#48--230261-a-modular-deep-learning-based-approach-for-diffuse-optical-tomography-reconstruction-alessandro-benfenati-et-al-2024>(4/8 | 230/261) A Modular Deep Learning-based Approach for Diffuse Optical Tomography Reconstruction (Alessandro Benfenati et al., 2024)</a></li><li><a href=#58--231261-the-dimension-weighted-fast-multipole-method-for-scattered-data-approximation-helmut-harbrecht-et-al-2024>(5/8 | 231/261) The dimension weighted fast multipole method for scattered data approximation (Helmut Harbrecht et al., 2024)</a></li><li><a href=#68--232261-analysis-of-an-adaptive-safeguarded-newton-anderson-algorithm-with-applications-to-fluid-problems-matt-dallas-et-al-2024>(6/8 | 232/261) Analysis of an Adaptive Safeguarded Newton-Anderson Algorithm with Applications to Fluid Problems (Matt Dallas et al., 2024)</a></li><li><a href=#78--233261-an-asymptotic-preserving-scheme-for-euler-equations-i-non-ideal-gases-giuseppe-orlando-et-al-2024>(7/8 | 233/261) An asymptotic-preserving scheme for Euler equations I: non-ideal gases (Giuseppe Orlando et al., 2024)</a></li><li><a href=#88--234261-comparison-of-two-models-to-predict-vertebral-failure-loads-on-the-same-experimental-dataset-v-allard-et-al-2024>(8/8 | 234/261) comparison of two models to predict vertebral failure loads on the same experimental dataset (V. Allard et al., 2024)</a></li></ul></li><li><a href=#physicssoc-ph-1>physics.soc-ph (1)</a><ul><li><a href=#11--235261-understanding-team-collapse-via-probabilistic-graphical-models-iasonas-nikolaou-et-al-2024>(1/1 | 235/261) Understanding team collapse via probabilistic graphical models (Iasonas Nikolaou et al., 2024)</a></li></ul></li><li><a href=#csit-2>cs.IT (2)</a><ul><li><a href=#12--236261-zak-otfs-and-ldpc-codes-beyza-dabak-et-al-2024>(1/2 | 236/261) Zak-OTFS and LDPC Codes (Beyza Dabak et al., 2024)</a></li><li><a href=#22--237261-performance-complexity-latency-trade-offs-of-concatenated-rs-bch-codes-alvin-y-sukmadji-et-al-2024>(2/2 | 237/261) Performance-Complexity-Latency Trade-offs of Concatenated RS-BCH Codes (Alvin Y. Sukmadji et al., 2024)</a></li></ul></li><li><a href=#q-biobm-1>q-bio.BM (1)</a><ul><li><a href=#11--238261-3d-based-rna-function-prediction-tools-in-rnaglib-carlos-oliver-et-al-2024>(1/1 | 238/261) 3D-based RNA function prediction tools in rnaglib (Carlos Oliver et al., 2024)</a></li></ul></li><li><a href=#cslo-5>cs.LO (5)</a><ul><li><a href=#15--239261-unified-opinion-dynamic-modeling-as-concurrent-set-relations-in-rewriting-logic-carlos-olarte-et-al-2024>(1/5 | 239/261) Unified Opinion Dynamic Modeling as Concurrent Set Relations in Rewriting Logic (Carlos Olarte et al., 2024)</a></li><li><a href=#25--240261-correctly-communicating-software-distributed-asynchronous-and-beyond-extended-version-bas-van-den-heuvel-2024>(2/5 | 240/261) Correctly Communicating Software: Distributed, Asynchronous, and Beyond (extended version) (Bas van den Heuvel, 2024)</a></li><li><a href=#35--241261-inferentialist-resource-semantics-alexander-v-gheorghiu-et-al-2024>(3/5 | 241/261) Inferentialist Resource Semantics (Alexander V. Gheorghiu et al., 2024)</a></li><li><a href=#45--242261-identifying-tractable-quantified-temporal-constraints-within-ord-horn-jakub-rydval-et-al-2024>(4/5 | 242/261) Identifying Tractable Quantified Temporal Constraints within Ord-Horn (Jakub Rydval et al., 2024)</a></li><li><a href=#55--243261-an-algorithmic-meta-theorem-for-homomorphism-indistinguishability-tim-seppelt-2024>(5/5 | 243/261) An Algorithmic Meta Theorem for Homomorphism Indistinguishability (Tim Seppelt, 2024)</a></li></ul></li><li><a href=#csdb-1>cs.DB (1)</a><ul><li><a href=#11--244261-computational-complexity-of-preferred-subset-repairs-on-data-graphs-nina-pardal-et-al-2024>(1/1 | 244/261) Computational Complexity of Preferred Subset Repairs on Data-Graphs (Nina Pardal et al., 2024)</a></li></ul></li><li><a href=#statml-5>stat.ML (5)</a><ul><li><a href=#15--245261-conformalized-adaptive-forecasting-of-heterogeneous-trajectories-yanfei-zhou-et-al-2024>(1/5 | 245/261) Conformalized Adaptive Forecasting of Heterogeneous Trajectories (Yanfei Zhou et al., 2024)</a></li><li><a href=#25--246261-connecting-algorithmic-fairness-to-quality-dimensions-in-machine-learning-in-official-statistics-and-survey-production-patrick-oliver-schenk-et-al-2024>(2/5 | 246/261) Connecting Algorithmic Fairness to Quality Dimensions in Machine Learning in Official Statistics and Survey Production (Patrick Oliver Schenk et al., 2024)</a></li><li><a href=#35--247261-mixed-output-gaussian-process-latent-variable-models-james-odgers-et-al-2024>(3/5 | 247/261) Mixed-Output Gaussian Process Latent Variable Models (James Odgers et al., 2024)</a></li><li><a href=#45--248261-towards-robust-model-based-reinforcement-learning-against-adversarial-corruption-chenlu-ye-et-al-2024>(4/5 | 248/261) Towards Robust Model-Based Reinforcement Learning Against Adversarial Corruption (Chenlu Ye et al., 2024)</a></li><li><a href=#55--249261-mcmc-driven-learning-alexandre-bouchard-côté-et-al-2024>(5/5 | 249/261) MCMC-driven learning (Alexandre Bouchard-Côté et al., 2024)</a></li></ul></li><li><a href=#csgt-1>cs.GT (1)</a><ul><li><a href=#11--250261-pareto-optimal-algorithms-for-learning-in-games-eshwar-ram-arunachaleswaran-et-al-2024>(1/1 | 250/261) Pareto-Optimal Algorithms for Learning in Games (Eshwar Ram Arunachaleswaran et al., 2024)</a></li></ul></li><li><a href=#csmm-1>cs.MM (1)</a><ul><li><a href=#11--251261-ll-gabr-energy-efficient-live-video-streaming-using-reinforcement-learning-adithya-raman-et-al-2024>(1/1 | 251/261) LL-GABR: Energy Efficient Live Video Streaming Using Reinforcement Learning (Adithya Raman et al., 2024)</a></li></ul></li><li><a href=#econth-1>econ.TH (1)</a><ul><li><a href=#11--252261-persuasion-delegation-and-private-information-in-algorithm-assisted-decisions-ruqing-xu-2024>(1/1 | 252/261) Persuasion, Delegation, and Private Information in Algorithm-Assisted Decisions (Ruqing Xu, 2024)</a></li></ul></li><li><a href=#physicscomp-ph-1>physics.comp-ph (1)</a><ul><li><a href=#11--253261-neural-networks-asymptotic-behaviours-for-the-resolution-of-inverse-problems-luigi-del-debbio-et-al-2024>(1/1 | 253/261) Neural Networks Asymptotic Behaviours for the Resolution of Inverse Problems (Luigi Del Debbio et al., 2024)</a></li></ul></li><li><a href=#csfl-1>cs.FL (1)</a><ul><li><a href=#11--254261-inform-from-compartmental-models-to-stochastic-bounded-counter-machines-tim-leys-et-al-2024>(1/1 | 254/261) Inform: From Compartmental Models to Stochastic Bounded Counter Machines (Tim Leys et al., 2024)</a></li></ul></li><li><a href=#csar-1>cs.AR (1)</a><ul><li><a href=#11--255261-stochastic-spiking-attention-accelerating-attention-with-stochastic-computing-in-spiking-networks-zihang-song-et-al-2024>(1/1 | 255/261) Stochastic Spiking Attention: Accelerating Attention with Stochastic Computing in Spiking Networks (Zihang Song et al., 2024)</a></li></ul></li><li><a href=#csdm-2>cs.DM (2)</a><ul><li><a href=#12--256261-irreducible-markov-chains-on-spaces-of-graphs-with-fixed-degree-color-sequences-félix-almendra-hernández-et-al-2024>(1/2 | 256/261) Irreducible Markov Chains on spaces of graphs with fixed degree-color sequences (Félix Almendra-Hernández et al., 2024)</a></li><li><a href=#22--257261-monotonicity-of-the-cops-and-robber-game-for-bounded-depth-treewidth-isolde-adler-et-al-2024>(2/2 | 257/261) Monotonicity of the cops and robber game for bounded depth treewidth (Isolde Adler et al., 2024)</a></li></ul></li><li><a href=#cscy-1>cs.CY (1)</a><ul><li><a href=#11--258261-identification-of-cohesive-subgroups-in-a-university-hall-of-residence-during-the-covid-19-pandemic-using-a-social-network-analysis-approach-pilar-marqués-sánchez-et-al-2024>(1/1 | 258/261) Identification of cohesive subgroups in a university hall of residence during the COVID-19 pandemic using a social network analysis approach (Pilar Marqués-Sánchez et al., 2024)</a></li></ul></li><li><a href=#csds-2>cs.DS (2)</a><ul><li><a href=#12--259261-better-decremental-and-fully-dynamic-sensitivity-oracles-for-subgraph-connectivity-yaowei-long-et-al-2024>(1/2 | 259/261) Better Decremental and Fully Dynamic Sensitivity Oracles for Subgraph Connectivity (Yaowei Long et al., 2024)</a></li><li><a href=#22--260261-improved-deterministic-distributed-maximum-weight-independent-set-approximation-in-sparse-graphs-yuval-gil-2024>(2/2 | 260/261) Improved Deterministic Distributed Maximum Weight Independent Set Approximation in Sparse Graphs (Yuval Gil, 2024)</a></li></ul></li><li><a href=#mathco-1>math.CO (1)</a><ul><li><a href=#11--261261-general-penny-graphs-are-at-most-4318-dense-arsenii-sagdeev-2024>(1/1 | 261/261) General penny graphs are at most 43/18-dense (Arsenii Sagdeev, 2024)</a></li></ul></li></ul></nav></div></div></section></div><footer id=footer class="container-fluid text-center align-content-center footer pb-2"><div class="container pt-5"><div class="row text-left"><div class="col-md-4 col-sm-12"><h5>Navigation</h5><ul><li class=nav-item><a class=smooth-scroll href=https://akitenkrad.github.io/akitenkrad-blog/#about>About</a></li><li class=nav-item><a class=smooth-scroll href=https://akitenkrad.github.io/akitenkrad-blog/#recent-posts>Recent Posts</a></li></ul></div><div class="col-md-4 col-sm-12"><h5>Contact me:</h5><ul><li><a href=mailto:contact.to.akitenkrad@gmail.com target=_blank rel=noopener><span><i class="fas fa-envelope"></i></span> <span>contact.to.akitenkrad@gmail.com</span></a></li></ul></div></div></div><hr><div class=container><div class="row text-left"><div class=col-md-4><a id=theme href=https://github.com/hossainemruz/toha target=_blank rel=noopener><img src=/akitenkrad-blog/images/theme-logo_hu8376fd15465fef26ffe66b6bcf0ca686_13669_32x0_resize_box_3.png alt="Toha Theme Logo">
Toha</a></div><div class="col-md-4 text-center">© 2020 Akitenkrad.</div><div class="col-md-4 text-right"><a id=hugo href=https://gohugo.io/ target=_blank rel=noopener>Powered by
<img src=/akitenkrad-blog/images/hugo-logo.svg alt="Hugo Logo" height=18></a></div></div></div></footer><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/jquery-3.4.1.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/popper.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/bootstrap.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/navbar.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/plyr.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/main.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js></script><script src=https://akitenkrad.github.io/akitenkrad-blog/js/single.js></script><script>hljs.initHighlightingOnLoad()</script><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/katex/katex.min.css><script type=text/javascript defer src=https://akitenkrad.github.io/akitenkrad-blog/katex/katex.min.js></script><script type=text/javascript defer src=https://akitenkrad.github.io/akitenkrad-blog/katex/auto-render.min.js></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\[",right:"\\]",display:!0},{left:"\\(",right:"\\)",display:!1}],throwOnError:!0})})</script></body></html>