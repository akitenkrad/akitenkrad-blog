<!doctype html><html><head><title>arXiv @ 2024.02.22</title>
<meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/bootstrap.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/layouts/main.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/navigators/navbar.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/plyr.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/flag-icon.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/custom/style.css><link href=https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css rel=stylesheet integrity=sha384-9ndCyUaIbzAi2FUVXJi0CjmCapSmO7SnpJef0486qhLnuZ2cdeRhO02iuK6FUUVM crossorigin=anonymous><script async src=https://cdn.jsdelivr.net/npm/es-module-shims@1/dist/es-module-shims.min.js crossorigin=anonymous></script><script type=importmap>
{
  "imports": {
    "@popperjs/core": "https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.8/dist/esm/popper.min.js",
    "bootstrap": "https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.esm.min.js"
  }
}
</script><script src=https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js integrity=sha384-geWF76RCwLtnZ8qwWowPQNguL3RmwHVBC9FhGdlKrxdiJJigb/j/68SIy3Te4Bkz crossorigin=anonymous></script><script type=module src=https://akitenkrad.github.io/akitenkrad-blog/js/custom/custom.js></script><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Muli:wght@300;400;500;600"><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/fontawesome/css/all.min.css><link rel=icon type=image/png href=https://akitenkrad.github.io/akitenkrad-blog/images/favicons/favicon-96x96_huf1ee13f0caf27d1547f91fb46207d708_13005_42x0_resize_box_3.png><meta property="og:title" content="arXiv @ 2024.02.22"><meta property="og:description" content="Primary Categories cs.AI (9) cs.AR (2) cs.CC (1) cs.CG (3) cs.CL (89) cs.CR (5) cs.CV (45) cs.CY (2) cs.DB (2) cs.DC (2) cs.DL (1) cs.DS (7) cs.GR (2) cs.HC (3) cs.IR (8) cs.IT (1) cs.LG (67) cs.LO (1) cs.MM (2) cs.NE (2) cs.NI (3) cs.PL (1) cs.RO (6) cs.SD (4) cs.SE (8) cs.SI (3) econ.TH (1) eess.AS (3) eess.IV (2) eess.SP (1) eess.SY (8) hep-th (1) math.AG (1) math.CO (1) math."><meta property="og:type" content="article"><meta property="og:url" content="https://akitenkrad.github.io/akitenkrad-blog/posts/arxiv/202402/20240222000000/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-02-22T00:00:00+00:00"><meta property="article:modified_time" content="2024-02-22T00:00:00+00:00"><meta name=description content="arXiv @ 2024.02.22"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/atom-one-dark.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/layouts/single.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/navigators/sidebar.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/style.css><script type=text/javascript src=//ajax.googleapis.com/ajax/libs/jquery/3.1.1/jquery.min.js></script><script language=JavaScript>$(document).ready(function(){$("a[href^='http']:not([href*='"+location.hostname+"'])").attr("target","_blank")})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-1MYYZQG0WE"></script><script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-1MYYZQG0WE",{anonymize_ip:!1})}</script></head><body data-spy=scroll data-target=#TableOfContents data-offset=80><div class="container-fluid bg-dimmed wrapper"><nav class="navbar navbar-expand-xl top-navbar final-navbar shadow" style=position:fixed><div class=container><button class="navbar-toggler navbar-light navbar-for-sidebar" id=sidebar-toggler type=button onclick=toggleSidebar()>
<span class=navbar-toggler-icon></span>
</button>
<button class="navbar-toggler navbar-light" id=toc-toggler type=button onclick=toggleTOC()>
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse lang-selector" id=top-nav-items><ul class="navbar-nav mr-auto"><li class=nav-item><a class=navbar-brand href=/akitenkrad-blog><img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png alt=Logo>
Akitenkrad's Blog</a></li></ul><ul class="navbar-nav ml-auto"><li class=nav-item><a class=nav-link href=/akitenkrad-blog#home>Home</a></li><li class=nav-item><a class=nav-link href=/akitenkrad-blog#about>About</a></li><li class=nav-item><a class=nav-link href=/akitenkrad-blog#recent-posts>Recent Posts</a></li><li class=nav-item><a class=nav-link id=blog-link href=https://akitenkrad.github.io/akitenkrad-blog/posts>Posts</a></li><li class=nav-item><a class=nav-link id=tag-link href=https://akitenkrad.github.io/akitenkrad-blog/tags>Tags</a></li></ul></div></div><img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png class=d-none id=main-logo alt=Logo>
<img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png class=d-none id=inverted-logo alt="Inverted Logo"></nav><section class=sidebar-section id=sidebar-section><div class=sidebar-holder><div class=sidebar id=sidebar><form class=mx-auto method=get action=/akitenkrad-blog/search><input type=text name=keyword placeholder=Search data-search id=search-box></form><div class=sidebar-tree><ul class=tree id=tree><li id=list-heading><a href=/posts data-filter=all>Posts</a></li><div class=subtree><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/>Papers</a><ul><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202205/>2022.05</a><ul><li><a href=/akitenkrad-blog/posts/papers/202205/20220518224923/ title="A Context-Aware Citation Recommendation Model with BERT and Graph Convolutional Networks">A Context-Aware Citation Recommendation Model with BERT and Graph Convolutional Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220508162318/ title="A Deep Cascade Model for Multi-Document Reading Comprehension">A Deep Cascade Model for Multi-Document Reading Comprehension</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220506021208/ title="A Primer in BERTology: What We Know About How BERT Works">A Primer in BERTology: What We Know About How BERT Works</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220529131339/ title="Attention Is All You Need">Attention Is All You Need</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220503010000/ title="DeBERTa: Decoding-Enhanced BERT with Disentangled Attention">DeBERTa: Decoding-Enhanced BERT with Disentangled Attention</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220505222900/ title="Dense Passage Retrieval for Open-Domain Question Answering">Dense Passage Retrieval for Open-Domain Question Answering</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220511010217/ title="Multi-Style Generative Reading Comprehension">Multi-Style Generative Reading Comprehension</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220530102936/ title="Neural Machine Translation of Rare Words with Subword Units">Neural Machine Translation of Rare Words with Subword Units</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220523223206/ title="RoBERTa: A Robustly Optimized BERT Pretraining Approach">RoBERTa: A Robustly Optimized BERT Pretraining Approach</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220520124748/ title="Semi-Supervised Classification with Graph Convolutional Networks">Semi-Supervised Classification with Graph Convolutional Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220509110738/ title="Survey on graph embeddings and their applications to machine learning problems on graphs">Survey on graph embeddings and their applications to machine learning problems on graphs</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220514151839/ title="UnitedQA: A Hybrid Approach for Open Domain Question Answering">UnitedQA: A Hybrid Approach for Open Domain Question Answering</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202206/>2022.06</a><ul><li><a href=/akitenkrad-blog/posts/papers/202206/20220612105422/ title="Attributed Network Embedding for Learning in a Dynamic Environment">Attributed Network Embedding for Learning in a Dynamic Environment</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220608085622/ title="CodeBERT: A Pre-Trained Model for Programming and Natural Languages">CodeBERT: A Pre-Trained Model for Programming and Natural Languages</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220618223844/ title="High-order Proximity Preserved Embedding for Dynamic Networks">High-order Proximity Preserved Embedding for Dynamic Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220602171700/ title="S-Net: From Answer Extraction to Answer Generation for Machine Reading Comprehension">S-Net: From Answer Extraction to Answer Generation for Machine Reading Comprehension</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202207/>2022.07</a><ul><li><a href=/akitenkrad-blog/posts/papers/202207/20220727145036/ title="Dynamic Heterogeneous Graph Embedding Using Hierarchical Attentions">Dynamic Heterogeneous Graph Embedding Using Hierarchical Attentions</a></li><li><a href=/akitenkrad-blog/posts/papers/202207/20220726163444/ title="Dynamic Network Embedding Survey">Dynamic Network Embedding Survey</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202208/>2022.08</a><ul><li><a href=/akitenkrad-blog/posts/papers/202208/20220802103319/ title="Modeling Dynamic Heterogeneous Network for Link Prediction using Hierarchical Attention with Temporal RNN">Modeling Dynamic Heterogeneous Network for Link Prediction using Hierarchical Attention with Temporal RNN</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202209/>2022.09</a><ul><li><a href=/akitenkrad-blog/posts/papers/202209/20220909180827/ title="Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting">Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202305/>2023.05</a><ul><li><a href=/akitenkrad-blog/posts/papers/202305/20230513094804/ title="Improving Language Understanding by Generative Pre-Training">Improving Language Understanding by Generative Pre-Training</a></li><li><a href=/akitenkrad-blog/posts/papers/202305/20230514200241/ title="Language Models are Unsupervised Multitask Learners">Language Models are Unsupervised Multitask Learners</a></li></ul></li><li><a href=/akitenkrad-blog/posts/papers/202306/ title=2023.06>2023.06</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/akitenkrad-blog/posts/arxiv/>arXiv</a><ul class=active><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202307/>2023.07</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230701000000/ title="arXiv @ 2023.07.01">arXiv @ 2023.07.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230702000000/ title="arXiv @ 2023.07.02">arXiv @ 2023.07.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230703000000/ title="arXiv @ 2023.07.03">arXiv @ 2023.07.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230704000000/ title="arXiv @ 2023.07.04">arXiv @ 2023.07.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230705000000/ title="arXiv @ 2023.07.05">arXiv @ 2023.07.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230706000000/ title="arXiv @ 2023.07.06">arXiv @ 2023.07.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230707000000/ title="arXiv @ 2023.07.07">arXiv @ 2023.07.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230708000000/ title="arXiv @ 2023.07.08">arXiv @ 2023.07.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230709000000/ title="arXiv @ 2023.07.09">arXiv @ 2023.07.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230710000000/ title="arXiv @ 2023.07.10">arXiv @ 2023.07.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230711000000/ title="arXiv @ 2023.07.11">arXiv @ 2023.07.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230712000000/ title="arXiv @ 2023.07.12">arXiv @ 2023.07.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230713000000/ title="arXiv @ 2023.07.13">arXiv @ 2023.07.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230714000000/ title="arXiv @ 2023.07.14">arXiv @ 2023.07.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230715000000/ title="arXiv @ 2023.07.15">arXiv @ 2023.07.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230716000000/ title="arXiv @ 2023.07.16">arXiv @ 2023.07.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230717000000/ title="arXiv @ 2023.07.17">arXiv @ 2023.07.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230718000000/ title="arXiv @ 2023.07.18">arXiv @ 2023.07.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230719000000/ title="arXiv @ 2023.07.19">arXiv @ 2023.07.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230720000000/ title="arXiv @ 2023.07.20">arXiv @ 2023.07.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230721000000/ title="arXiv @ 2023.07.21">arXiv @ 2023.07.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230722000000/ title="arXiv @ 2023.07.22">arXiv @ 2023.07.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230723000000/ title="arXiv @ 2023.07.23">arXiv @ 2023.07.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230724000000/ title="arXiv @ 2023.07.24">arXiv @ 2023.07.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230725000000/ title="arXiv @ 2023.07.25">arXiv @ 2023.07.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230726000000/ title="arXiv @ 2023.07.26">arXiv @ 2023.07.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230727000000/ title="arXiv @ 2023.07.27">arXiv @ 2023.07.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230728000000/ title="arXiv @ 2023.07.28">arXiv @ 2023.07.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230729000000/ title="arXiv @ 2023.07.29">arXiv @ 2023.07.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230730000000/ title="arXiv @ 2023.07.30">arXiv @ 2023.07.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230731000000/ title="arXiv @ 2023.07.31">arXiv @ 2023.07.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202308/>2023.08</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230801000000/ title="arXiv @ 2023.08.01">arXiv @ 2023.08.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230802000000/ title="arXiv @ 2023.08.02">arXiv @ 2023.08.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230803000000/ title="arXiv @ 2023.08.03">arXiv @ 2023.08.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230804000000/ title="arXiv @ 2023.08.04">arXiv @ 2023.08.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230805000000/ title="arXiv @ 2023.08.05">arXiv @ 2023.08.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230806000000/ title="arXiv @ 2023.08.06">arXiv @ 2023.08.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230807000000/ title="arXiv @ 2023.08.07">arXiv @ 2023.08.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230808000000/ title="arXiv @ 2023.08.08">arXiv @ 2023.08.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230809000000/ title="arXiv @ 2023.08.09">arXiv @ 2023.08.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230810000000/ title="arXiv @ 2023.08.10">arXiv @ 2023.08.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230811000000/ title="arXiv @ 2023.08.11">arXiv @ 2023.08.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230812000000/ title="arXiv @ 2023.08.12">arXiv @ 2023.08.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230813000000/ title="arXiv @ 2023.08.13">arXiv @ 2023.08.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230814000000/ title="arXiv @ 2023.08.14">arXiv @ 2023.08.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230815000000/ title="arXiv @ 2023.08.15">arXiv @ 2023.08.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230816000000/ title="arXiv @ 2023.08.16">arXiv @ 2023.08.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230817000000/ title="arXiv @ 2023.08.17">arXiv @ 2023.08.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230818000000/ title="arXiv @ 2023.08.18">arXiv @ 2023.08.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230819000000/ title="arXiv @ 2023.08.19">arXiv @ 2023.08.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230820000000/ title="arXiv @ 2023.08.20">arXiv @ 2023.08.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230821000000/ title="arXiv @ 2023.08.21">arXiv @ 2023.08.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230822000000/ title="arXiv @ 2023.08.22">arXiv @ 2023.08.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230823000000/ title="arXiv @ 2023.08.23">arXiv @ 2023.08.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230824000000/ title="arXiv @ 2023.08.24">arXiv @ 2023.08.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230825000000/ title="arXiv @ 2023.08.25">arXiv @ 2023.08.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230826000000/ title="arXiv @ 2023.08.26">arXiv @ 2023.08.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230827000000/ title="arXiv @ 2023.08.27">arXiv @ 2023.08.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230828000000/ title="arXiv @ 2023.08.28">arXiv @ 2023.08.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230829000000/ title="arXiv @ 2023.08.29">arXiv @ 2023.08.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230830000000/ title="arXiv @ 2023.08.30">arXiv @ 2023.08.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230831000000/ title="arXiv @ 2023.08.31">arXiv @ 2023.08.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202309/>2023.09</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230901000000/ title="arXiv @ 2023.09.01">arXiv @ 2023.09.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230902000000/ title="arXiv @ 2023.09.02">arXiv @ 2023.09.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230903000000/ title="arXiv @ 2023.09.03">arXiv @ 2023.09.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230904000000/ title="arXiv @ 2023.09.04">arXiv @ 2023.09.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230905000000/ title="arXiv @ 2023.09.05">arXiv @ 2023.09.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230906000000/ title="arXiv @ 2023.09.06">arXiv @ 2023.09.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230907000000/ title="arXiv @ 2023.09.07">arXiv @ 2023.09.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230908000000/ title="arXiv @ 2023.09.08">arXiv @ 2023.09.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230909000000/ title="arXiv @ 2023.09.09">arXiv @ 2023.09.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230910000000/ title="arXiv @ 2023.09.10">arXiv @ 2023.09.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230911000000/ title="arXiv @ 2023.09.11">arXiv @ 2023.09.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230912000000/ title="arXiv @ 2023.09.12">arXiv @ 2023.09.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230913000000/ title="arXiv @ 2023.09.13">arXiv @ 2023.09.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230914000000/ title="arXiv @ 2023.09.14">arXiv @ 2023.09.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230915000000/ title="arXiv @ 2023.09.15">arXiv @ 2023.09.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230916000000/ title="arXiv @ 2023.09.16">arXiv @ 2023.09.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230917000000/ title="arXiv @ 2023.09.17">arXiv @ 2023.09.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230918000000/ title="arXiv @ 2023.09.18">arXiv @ 2023.09.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230919000000/ title="arXiv @ 2023.09.19">arXiv @ 2023.09.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230920000000/ title="arXiv @ 2023.09.20">arXiv @ 2023.09.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230921000000/ title="arXiv @ 2023.09.21">arXiv @ 2023.09.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230922000000/ title="arXiv @ 2023.09.22">arXiv @ 2023.09.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230923000000/ title="arXiv @ 2023.09.23">arXiv @ 2023.09.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230924000000/ title="arXiv @ 2023.09.24">arXiv @ 2023.09.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230925000000/ title="arXiv @ 2023.09.25">arXiv @ 2023.09.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230926000000/ title="arXiv @ 2023.09.26">arXiv @ 2023.09.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230927000000/ title="arXiv @ 2023.09.27">arXiv @ 2023.09.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230928000000/ title="arXiv @ 2023.09.28">arXiv @ 2023.09.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230929000000/ title="arXiv @ 2023.09.29">arXiv @ 2023.09.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230930000000/ title="arXiv @ 2023.09.30">arXiv @ 2023.09.30</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202310/>2023.1</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231001000000/ title="arXiv @ 2023.10.01">arXiv @ 2023.10.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231002000000/ title="arXiv @ 2023.10.02">arXiv @ 2023.10.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231003000000/ title="arXiv @ 2023.10.03">arXiv @ 2023.10.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231004000000/ title="arXiv @ 2023.10.04">arXiv @ 2023.10.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231005000000/ title="arXiv @ 2023.10.05">arXiv @ 2023.10.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231006000000/ title="arXiv @ 2023.10.06">arXiv @ 2023.10.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231007000000/ title="arXiv @ 2023.10.07">arXiv @ 2023.10.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231008000000/ title="arXiv @ 2023.10.08">arXiv @ 2023.10.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231009000000/ title="arXiv @ 2023.10.09">arXiv @ 2023.10.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231010000000/ title="arXiv @ 2023.10.10">arXiv @ 2023.10.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231011000000/ title="arXiv @ 2023.10.11">arXiv @ 2023.10.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231012000000/ title="arXiv @ 2023.10.12">arXiv @ 2023.10.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231013000000/ title="arXiv @ 2023.10.13">arXiv @ 2023.10.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231014000000/ title="arXiv @ 2023.10.14">arXiv @ 2023.10.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231015000000/ title="arXiv @ 2023.10.15">arXiv @ 2023.10.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231016000000/ title="arXiv @ 2023.10.16">arXiv @ 2023.10.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231017000000/ title="arXiv @ 2023.10.17">arXiv @ 2023.10.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231018000000/ title="arXiv @ 2023.10.18">arXiv @ 2023.10.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231019000000/ title="arXiv @ 2023.10.19">arXiv @ 2023.10.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231020000000/ title="arXiv @ 2023.10.20">arXiv @ 2023.10.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231021000000/ title="arXiv @ 2023.10.21">arXiv @ 2023.10.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231022000000/ title="arXiv @ 2023.10.22">arXiv @ 2023.10.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231023000000/ title="arXiv @ 2023.10.23">arXiv @ 2023.10.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231024000000/ title="arXiv @ 2023.10.24">arXiv @ 2023.10.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231025000000/ title="arXiv @ 2023.10.25">arXiv @ 2023.10.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231026000000/ title="arXiv @ 2023.10.26">arXiv @ 2023.10.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231027000000/ title="arXiv @ 2023.10.27">arXiv @ 2023.10.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231028000000/ title="arXiv @ 2023.10.28">arXiv @ 2023.10.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231029000000/ title="arXiv @ 2023.10.29">arXiv @ 2023.10.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231030000000/ title="arXiv @ 2023.10.30">arXiv @ 2023.10.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231031000000/ title="arXiv @ 2023.10.31">arXiv @ 2023.10.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202311/>2023.11</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231101000000/ title="arXiv @ 2023.11.01">arXiv @ 2023.11.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231102000000/ title="arXiv @ 2023.11.02">arXiv @ 2023.11.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231103000000/ title="arXiv @ 2023.11.03">arXiv @ 2023.11.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231104000000/ title="arXiv @ 2023.11.04">arXiv @ 2023.11.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231105000000/ title="arXiv @ 2023.11.05">arXiv @ 2023.11.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231106000000/ title="arXiv @ 2023.11.06">arXiv @ 2023.11.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231107000000/ title="arXiv @ 2023.11.07">arXiv @ 2023.11.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231108000000/ title="arXiv @ 2023.11.08">arXiv @ 2023.11.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231109000000/ title="arXiv @ 2023.11.09">arXiv @ 2023.11.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231110000000/ title="arXiv @ 2023.11.10">arXiv @ 2023.11.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231111000000/ title="arXiv @ 2023.11.11">arXiv @ 2023.11.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231112000000/ title="arXiv @ 2023.11.12">arXiv @ 2023.11.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231113000000/ title="arXiv @ 2023.11.13">arXiv @ 2023.11.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231114000000/ title="arXiv @ 2023.11.14">arXiv @ 2023.11.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231115000000/ title="arXiv @ 2023.11.15">arXiv @ 2023.11.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231116000000/ title="arXiv @ 2023.11.16">arXiv @ 2023.11.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231117000000/ title="arXiv @ 2023.11.17">arXiv @ 2023.11.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231118000000/ title="arXiv @ 2023.11.18">arXiv @ 2023.11.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231119000000/ title="arXiv @ 2023.11.19">arXiv @ 2023.11.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231120000000/ title="arXiv @ 2023.11.20">arXiv @ 2023.11.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231121000000/ title="arXiv @ 2023.11.21">arXiv @ 2023.11.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231122000000/ title="arXiv @ 2023.11.22">arXiv @ 2023.11.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231123000000/ title="arXiv @ 2023.11.23">arXiv @ 2023.11.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231124000000/ title="arXiv @ 2023.11.24">arXiv @ 2023.11.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231125000000/ title="arXiv @ 2023.11.25">arXiv @ 2023.11.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231126000000/ title="arXiv @ 2023.11.26">arXiv @ 2023.11.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231127000000/ title="arXiv @ 2023.11.27">arXiv @ 2023.11.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231128000000/ title="arXiv @ 2023.11.28">arXiv @ 2023.11.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231129000000/ title="arXiv @ 2023.11.29">arXiv @ 2023.11.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231130000000/ title="arXiv @ 2023.11.30">arXiv @ 2023.11.30</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202312/>2023.12</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231201000000/ title="arXiv @ 2023.12.01">arXiv @ 2023.12.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231202000000/ title="arXiv @ 2023.12.02">arXiv @ 2023.12.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231203000000/ title="arXiv @ 2023.12.03">arXiv @ 2023.12.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231204000000/ title="arXiv @ 2023.12.04">arXiv @ 2023.12.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231205000000/ title="arXiv @ 2023.12.05">arXiv @ 2023.12.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231206000000/ title="arXiv @ 2023.12.06">arXiv @ 2023.12.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231207000000/ title="arXiv @ 2023.12.07">arXiv @ 2023.12.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231208000000/ title="arXiv @ 2023.12.08">arXiv @ 2023.12.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231209000000/ title="arXiv @ 2023.12.09">arXiv @ 2023.12.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231210000000/ title="arXiv @ 2023.12.10">arXiv @ 2023.12.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231211000000/ title="arXiv @ 2023.12.11">arXiv @ 2023.12.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231212000000/ title="arXiv @ 2023.12.12">arXiv @ 2023.12.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231213000000/ title="arXiv @ 2023.12.13">arXiv @ 2023.12.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231214000000/ title="arXiv @ 2023.12.14">arXiv @ 2023.12.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231215000000/ title="arXiv @ 2023.12.15">arXiv @ 2023.12.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231216000000/ title="arXiv @ 2023.12.16">arXiv @ 2023.12.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231217000000/ title="arXiv @ 2023.12.17">arXiv @ 2023.12.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231218000000/ title="arXiv @ 2023.12.18">arXiv @ 2023.12.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231219000000/ title="arXiv @ 2023.12.19">arXiv @ 2023.12.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231220000000/ title="arXiv @ 2023.12.20">arXiv @ 2023.12.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231221000000/ title="arXiv @ 2023.12.21">arXiv @ 2023.12.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231222000000/ title="arXiv @ 2023.12.22">arXiv @ 2023.12.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231223000000/ title="arXiv @ 2023.12.23">arXiv @ 2023.12.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231224000000/ title="arXiv @ 2023.12.24">arXiv @ 2023.12.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231225000000/ title="arXiv @ 2023.12.25">arXiv @ 2023.12.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231226000000/ title="arXiv @ 2023.12.26">arXiv @ 2023.12.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231227000000/ title="arXiv @ 2023.12.27">arXiv @ 2023.12.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231228000000/ title="arXiv @ 2023.12.28">arXiv @ 2023.12.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231229000000/ title="arXiv @ 2023.12.29">arXiv @ 2023.12.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231230000000/ title="arXiv @ 2023.12.30">arXiv @ 2023.12.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231231000000/ title="arXiv @ 2023.12.31">arXiv @ 2023.12.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202401/>2024.01</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240101000000/ title="arXiv @ 2024.01.01">arXiv @ 2024.01.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240102000000/ title="arXiv @ 2024.01.02">arXiv @ 2024.01.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240103000000/ title="arXiv @ 2024.01.03">arXiv @ 2024.01.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240104000000/ title="arXiv @ 2024.01.04">arXiv @ 2024.01.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240105000000/ title="arXiv @ 2024.01.05">arXiv @ 2024.01.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240106000000/ title="arXiv @ 2024.01.06">arXiv @ 2024.01.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240107000000/ title="arXiv @ 2024.01.07">arXiv @ 2024.01.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240108000000/ title="arXiv @ 2024.01.08">arXiv @ 2024.01.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240109000000/ title="arXiv @ 2024.01.09">arXiv @ 2024.01.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240110000000/ title="arXiv @ 2024.01.10">arXiv @ 2024.01.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240111000000/ title="arXiv @ 2024.01.11">arXiv @ 2024.01.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240112000000/ title="arXiv @ 2024.01.12">arXiv @ 2024.01.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240113000000/ title="arXiv @ 2024.01.13">arXiv @ 2024.01.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240114000000/ title="arXiv @ 2024.01.14">arXiv @ 2024.01.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240115000000/ title="arXiv @ 2024.01.15">arXiv @ 2024.01.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240116000000/ title="arXiv @ 2024.01.16">arXiv @ 2024.01.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240117000000/ title="arXiv @ 2024.01.17">arXiv @ 2024.01.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240118000000/ title="arXiv @ 2024.01.18">arXiv @ 2024.01.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240119000000/ title="arXiv @ 2024.01.19">arXiv @ 2024.01.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240120000000/ title="arXiv @ 2024.01.20">arXiv @ 2024.01.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240121000000/ title="arXiv @ 2024.01.21">arXiv @ 2024.01.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240122000000/ title="arXiv @ 2024.01.22">arXiv @ 2024.01.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240123000000/ title="arXiv @ 2024.01.23">arXiv @ 2024.01.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240124000000/ title="arXiv @ 2024.01.24">arXiv @ 2024.01.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240125000000/ title="arXiv @ 2024.01.25">arXiv @ 2024.01.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240126000000/ title="arXiv @ 2024.01.26">arXiv @ 2024.01.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240127000000/ title="arXiv @ 2024.01.27">arXiv @ 2024.01.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240128000000/ title="arXiv @ 2024.01.28">arXiv @ 2024.01.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240129000000/ title="arXiv @ 2024.01.29">arXiv @ 2024.01.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240130000000/ title="arXiv @ 2024.01.30">arXiv @ 2024.01.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240131000000/ title="arXiv @ 2024.01.31">arXiv @ 2024.01.31</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/akitenkrad-blog/posts/arxiv/202402/>2024.02</a><ul class=active><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240201000000/ title="arXiv @ 2024.02.01">arXiv @ 2024.02.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240202000000/ title="arXiv @ 2024.02.02">arXiv @ 2024.02.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240203000000/ title="arXiv @ 2024.02.03">arXiv @ 2024.02.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240204000000/ title="arXiv @ 2024.02.04">arXiv @ 2024.02.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240205000000/ title="arXiv @ 2024.02.05">arXiv @ 2024.02.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240206000000/ title="arXiv @ 2024.02.06">arXiv @ 2024.02.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240207000000/ title="arXiv @ 2024.02.07">arXiv @ 2024.02.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240208000000/ title="arXiv @ 2024.02.08">arXiv @ 2024.02.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240209000000/ title="arXiv @ 2024.02.09">arXiv @ 2024.02.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240210000000/ title="arXiv @ 2024.02.10">arXiv @ 2024.02.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240211000000/ title="arXiv @ 2024.02.11">arXiv @ 2024.02.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240212000000/ title="arXiv @ 2024.02.12">arXiv @ 2024.02.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240213000000/ title="arXiv @ 2024.02.13">arXiv @ 2024.02.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240214000000/ title="arXiv @ 2024.02.14">arXiv @ 2024.02.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240215000000/ title="arXiv @ 2024.02.15">arXiv @ 2024.02.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240216000000/ title="arXiv @ 2024.02.16">arXiv @ 2024.02.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240217000000/ title="arXiv @ 2024.02.17">arXiv @ 2024.02.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240218000000/ title="arXiv @ 2024.02.18">arXiv @ 2024.02.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240219000000/ title="arXiv @ 2024.02.19">arXiv @ 2024.02.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240220000000/ title="arXiv @ 2024.02.20">arXiv @ 2024.02.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240221000000/ title="arXiv @ 2024.02.21">arXiv @ 2024.02.21</a></li><li><a class=active href=/akitenkrad-blog/posts/arxiv/202402/20240222000000/ title="arXiv @ 2024.02.22">arXiv @ 2024.02.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240223000000/ title="arXiv @ 2024.02.23">arXiv @ 2024.02.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240224000000/ title="arXiv @ 2024.02.24">arXiv @ 2024.02.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240225000000/ title="arXiv @ 2024.02.25">arXiv @ 2024.02.25</a></li></ul></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/algorithms/>Algorithms</a><ul><li><a href=/akitenkrad-blog/posts/algorithms/bandit-algorithm-basic/ title="Bandit Algorithm Basic">Bandit Algorithm Basic</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/conference/>Conference</a><ul><li><a href=/akitenkrad-blog/posts/conference/acl/ title="Annual Meeting of the Association for Computational Linguistics">Annual Meeting of the Association for Computational Linguistics</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/>Figures</a><ul><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/202208/>2022.08</a><ul><li><a href=/akitenkrad-blog/posts/figures/202208/20220822092748/ title=1ヶ月当たりの実労働時間の推移>1ヶ月当たりの実労働時間の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220813115200/ title=コンビニエンスストアの店舗数の推移>コンビニエンスストアの店舗数の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220821113322/ title=地域別1世帯当たり1ヶ月間の支出額の推移>地域別1世帯当たり1ヶ月間の支出額の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220828111836/ title="夫婦別子供の有無別生活時間 (2016)">夫婦別子供の有無別生活時間 (2016)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220827103615/ title=家族類型の変遷>家族類型の変遷</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220815131016/ title="年齢別大学院入学者数 (2021)">年齢別大学院入学者数 (2021)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220818122628/ title=情報サービス業における企業特殊的人的資本（名目）の推移>情報サービス業における企業特殊的人的資本（名目）の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220824095127/ title=有業者の社会生活における活動の変化>有業者の社会生活における活動の変化</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220814175022/ title=東京23区の乗用車保有台数>東京23区の乗用車保有台数</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220817121117/ title="業種別企業特殊的人的資本 (2018)">業種別企業特殊的人的資本 (2018)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220825094111/ title=無業者の社会生活における活動の変化>無業者の社会生活における活動の変化</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220823104100/ title=睡眠時間の推移>睡眠時間の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220826102652/ title=総人口の推移>総人口の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220820225528/ title=都道府県別国公立別学校数・学生数>都道府県別国公立別学校数・学生数</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220816120845/ title="都道府県別消費者物価指数/教育（全国平均=100）">都道府県別消費者物価指数/教育（全国平均=100）</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220812173814/ title=金沢市の夏の月平均気温の遷移>金沢市の夏の月平均気温の遷移</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/202209/>2022.09</a><ul><li><a href=/akitenkrad-blog/posts/figures/202209/20220907122906/ title="ACL-2022 WordCloud">ACL-2022 WordCloud</a></li></ul></li></ul></li><li><a href=/akitenkrad-blog/posts/latex/ title="Latex Mathematics Syntax Guide">Latex Mathematics Syntax Guide</a></li><li><a href=/akitenkrad-blog/posts/markdown/ title="Markdown Sample">Markdown Sample</a></li></div></ul></div></div></div></section><section class=content-section id=content-section><div class=content><div class="container p-0 read-area"><div class="hero-area col-sm-12" id=hero-area style=background-image:url(/akitenkrad-blog/posts/arxiv/202402/20240222000000/hero.png)></div><div class=page-content><div class="author-profile ml-auto align-self-lg-center"><img class=rounded-circle src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_120x120_fit_box_3.png alt="Author Image"><h5 class=author-name></h5><p>Thursday, Feb 22, 2024</p></div><div class=title><h1>arXiv @ 2024.02.22</h1></div><div class=taxonomy-terms><ul><li class=rounded><a href=/akitenkrad-blog/tags/arxiv class="btn, btn-sm">arXiv</a></li><li class=rounded><a href=/akitenkrad-blog/tags/published2024 class="btn, btn-sm">2024</a></li></ul></div><div class=post-content id=post-content><figure style=border:none;width:100%;display:flex;justify-content:center><iframe src=pie.html width=900 height=620 style=border:none></iframe></figure><h2 id=primary-categories>Primary Categories</h2><ul><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240222000000/#csai-9>cs.AI (9)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240222000000/#csar-2>cs.AR (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240222000000/#cscc-1>cs.CC (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240222000000/#cscg-3>cs.CG (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240222000000/#cscl-89>cs.CL (89)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240222000000/#cscr-5>cs.CR (5)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240222000000/#cscv-45>cs.CV (45)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240222000000/#cscy-2>cs.CY (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240222000000/#csdb-2>cs.DB (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240222000000/#csdc-2>cs.DC (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240222000000/#csdl-1>cs.DL (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240222000000/#csds-7>cs.DS (7)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240222000000/#csgr-2>cs.GR (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240222000000/#cshc-3>cs.HC (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240222000000/#csir-8>cs.IR (8)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240222000000/#csit-1>cs.IT (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240222000000/#cslg-67>cs.LG (67)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240222000000/#cslo-1>cs.LO (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240222000000/#csmm-2>cs.MM (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240222000000/#csne-2>cs.NE (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240222000000/#csni-3>cs.NI (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240222000000/#cspl-1>cs.PL (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240222000000/#csro-6>cs.RO (6)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240222000000/#cssd-4>cs.SD (4)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240222000000/#csse-8>cs.SE (8)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240222000000/#cssi-3>cs.SI (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240222000000/#econth-1>econ.TH (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240222000000/#eessas-3>eess.AS (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240222000000/#eessiv-2>eess.IV (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240222000000/#eesssp-1>eess.SP (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240222000000/#eesssy-8>eess.SY (8)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240222000000/#hep-th-1>hep-th (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240222000000/#mathag-1>math.AG (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240222000000/#mathco-1>math.CO (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240222000000/#mathna-4>math.NA (4)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240222000000/#mathoc-1>math.OC (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240222000000/#physicscomp-ph-1>physics.comp-ph (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240222000000/#physicsflu-dyn-1>physics.flu-dyn (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240222000000/#physicssoc-ph-1>physics.soc-ph (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240222000000/#q-bioqm-1>q-bio.QM (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240222000000/#q-fincp-1>q-fin.CP (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240222000000/#quant-ph-5>quant-ph (5)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240222000000/#statme-1>stat.ME (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240222000000/#statml-3>stat.ML (3)</a></li></ul><h2 id=keywords>Keywords</h2><table border=1 class=dataframe><thead><tr style=text-align:right><th>keyword</th><th>cs.CL</th><th>cs.CV</th><th>cs.LG</th></tr></thead><tbody><tr><td>Active Learning</td><td>1</td><td></td><td>1</td></tr><tr><td>Adversarial Attack</td><td></td><td></td><td>1</td></tr><tr><td>Adversarial Learning</td><td></td><td></td><td>1</td></tr><tr><td>Anomaly Detection</td><td></td><td></td><td>1</td></tr><tr><td>Autoencoder</td><td></td><td></td><td>1</td></tr><tr><td>Automatic Speech Recognition</td><td>5</td><td>2</td><td></td></tr><tr><td>BERT</td><td>2</td><td></td><td></td></tr><tr><td>BLOOM</td><td></td><td></td><td>1</td></tr><tr><td>Bandit Algorithm</td><td></td><td></td><td>5</td></tr><tr><td>Benchmarking</td><td>32</td><td>17</td><td>13</td></tr><tr><td>Black Box</td><td>1</td><td>1</td><td>2</td></tr><tr><td>Chain-of-thought Prompt</td><td>3</td><td></td><td></td></tr><tr><td>ChatGPT</td><td>7</td><td></td><td></td></tr><tr><td>Chatbot</td><td>1</td><td></td><td></td></tr><tr><td>Clustering</td><td>1</td><td></td><td>3</td></tr><tr><td>Code Generation</td><td>2</td><td></td><td></td></tr><tr><td>CodeGen</td><td>1</td><td></td><td></td></tr><tr><td>Common-sense Reasoning</td><td>2</td><td></td><td></td></tr><tr><td>Continual Learning</td><td>1</td><td>1</td><td>1</td></tr><tr><td>Contrastive Learning</td><td>1</td><td></td><td>2</td></tr><tr><td>ControlNet</td><td></td><td>1</td><td></td></tr><tr><td>Convolution</td><td></td><td>4</td><td>1</td></tr><tr><td>Convolutional Neural Network</td><td></td><td>3</td><td>1</td></tr><tr><td>Coreference Resolution</td><td>1</td><td></td><td></td></tr><tr><td>Counter-factual</td><td></td><td>1</td><td></td></tr><tr><td>Curriculum Learning</td><td></td><td>1</td><td>2</td></tr><tr><td>Data Augmentation</td><td>3</td><td>1</td><td></td></tr><tr><td>Dense Retrieval</td><td>1</td><td></td><td></td></tr><tr><td>Dependency Parsing</td><td>1</td><td></td><td></td></tr><tr><td>Dialogue State Tracking</td><td>1</td><td></td><td></td></tr><tr><td>Dialogue System</td><td>2</td><td></td><td></td></tr><tr><td>Differential Privacy</td><td></td><td></td><td>2</td></tr><tr><td>Diffusion Model</td><td></td><td>8</td><td>4</td></tr><tr><td>Disambiguation</td><td>2</td><td></td><td></td></tr><tr><td>Distribution Shift</td><td></td><td>2</td><td>8</td></tr><tr><td>Domain Adaptation</td><td></td><td>1</td><td>2</td></tr><tr><td>Explainable AI</td><td></td><td></td><td>2</td></tr><tr><td>Fact Verification</td><td>2</td><td></td><td></td></tr><tr><td>Fairness</td><td></td><td>1</td><td>5</td></tr><tr><td>Fake News Detection</td><td>1</td><td></td><td></td></tr><tr><td>Federated Learning</td><td></td><td></td><td>4</td></tr><tr><td>Few-shot</td><td>6</td><td>1</td><td>1</td></tr><tr><td>Few-shot Learning</td><td>3</td><td></td><td></td></tr><tr><td>Fine-tuning</td><td>26</td><td>5</td><td>7</td></tr><tr><td>Foundation Model</td><td>2</td><td></td><td></td></tr><tr><td>GPT</td><td>18</td><td>3</td><td>1</td></tr><tr><td>GPT-2</td><td>2</td><td></td><td>1</td></tr><tr><td>GPT-3</td><td>6</td><td></td><td></td></tr><tr><td>GPT-3.5</td><td>6</td><td></td><td></td></tr><tr><td>GPT-4</td><td>10</td><td></td><td></td></tr><tr><td>Gaussian Process</td><td></td><td></td><td>2</td></tr><tr><td>Gemini</td><td>1</td><td>1</td><td></td></tr><tr><td>Generative AI</td><td>1</td><td></td><td></td></tr><tr><td>Generative Adversarial Network</td><td></td><td>2</td><td></td></tr><tr><td>Geometry</td><td></td><td>1</td><td></td></tr><tr><td>Graph</td><td>6</td><td>3</td><td>15</td></tr><tr><td>Graph Anomaly Detection</td><td></td><td></td><td>1</td></tr><tr><td>Graph Convolutional Network</td><td></td><td>2</td><td></td></tr><tr><td>Graph Neural Network</td><td>4</td><td>2</td><td>12</td></tr><tr><td>Grounding</td><td>1</td><td>1</td><td></td></tr><tr><td>Hallucination Detection</td><td>2</td><td></td><td></td></tr><tr><td>High-Resource</td><td>1</td><td></td><td></td></tr><tr><td>Human Intervention</td><td>1</td><td></td><td></td></tr><tr><td>In-context Learning</td><td>9</td><td></td><td>3</td></tr><tr><td>Information Retrieval</td><td>3</td><td></td><td></td></tr><tr><td>Instruction Following</td><td>3</td><td></td><td></td></tr><tr><td>Instruction Tuning</td><td>8</td><td></td><td></td></tr><tr><td>Knowledge Distillation</td><td>13</td><td>2</td><td>7</td></tr><tr><td>Knowledge Graph</td><td>3</td><td>2</td><td>2</td></tr><tr><td>Knowledge Transfer</td><td></td><td>1</td><td></td></tr><tr><td>LLaMA</td><td>5</td><td></td><td>1</td></tr><tr><td>LSTM</td><td>1</td><td></td><td>3</td></tr><tr><td>Large Language Model</td><td>114</td><td>11</td><td>10</td></tr><tr><td>Low-Resource</td><td>2</td><td>2</td><td></td></tr><tr><td>Masked Language Model</td><td>3</td><td></td><td></td></tr><tr><td>Massive Multitask Language Understanding (MMLU)</td><td>2</td><td></td><td></td></tr><tr><td>Mathematical Reasoning</td><td>2</td><td></td><td></td></tr><tr><td>Message-Passing</td><td>1</td><td></td><td>2</td></tr><tr><td>Meta Learning</td><td>1</td><td></td><td>1</td></tr><tr><td>Mistral</td><td>2</td><td></td><td>2</td></tr><tr><td>Model Compression</td><td>2</td><td></td><td></td></tr><tr><td>Multi-modal</td><td>4</td><td>18</td><td></td></tr><tr><td>Named Entity Recognition</td><td>4</td><td></td><td></td></tr><tr><td>Neural Machine Translation</td><td>5</td><td></td><td>1</td></tr><tr><td>Node Classification</td><td>1</td><td></td><td>1</td></tr><tr><td>Node Embedding</td><td></td><td></td><td>1</td></tr><tr><td>Object Detection</td><td></td><td>5</td><td></td></tr><tr><td>Offline Reinforcement Learning</td><td>1</td><td></td><td>1</td></tr><tr><td>Online Reinforcement Learning</td><td></td><td></td><td>1</td></tr><tr><td>Out-of-distribution</td><td></td><td>1</td><td></td></tr><tr><td>Out-of-domain</td><td>2</td><td></td><td></td></tr><tr><td>PaLM</td><td></td><td></td><td>1</td></tr><tr><td>Perplexity</td><td>1</td><td></td><td></td></tr><tr><td>Pre-trained Language Model</td><td>6</td><td></td><td>1</td></tr><tr><td>Probabilistic Model</td><td></td><td>1</td><td>1</td></tr><tr><td>Prompt</td><td>21</td><td>8</td><td>4</td></tr><tr><td>Prompt Learning</td><td>1</td><td>1</td><td></td></tr><tr><td>Pruning</td><td></td><td>1</td><td>1</td></tr><tr><td>Question Answering</td><td>10</td><td>3</td><td></td></tr><tr><td>Reasoning</td><td>17</td><td>4</td><td>2</td></tr><tr><td>Recommendation</td><td>1</td><td></td><td>1</td></tr><tr><td>Recurrent Neural Network</td><td>1</td><td></td><td></td></tr><tr><td>Reinforcement Learning</td><td>1</td><td>1</td><td>6</td></tr><tr><td>Reinforcement Learning from Human Feedback</td><td></td><td></td><td>1</td></tr><tr><td>Relation Extraction</td><td>1</td><td></td><td></td></tr><tr><td>Representation Learning</td><td></td><td>1</td><td>1</td></tr><tr><td>Retrieval-Augmented Generation</td><td>4</td><td></td><td></td></tr><tr><td>RoBERTa</td><td>1</td><td></td><td></td></tr><tr><td>Sample Size</td><td></td><td></td><td>2</td></tr><tr><td>Self-Attention</td><td>1</td><td>5</td><td>2</td></tr><tr><td>Self-supervised Learning</td><td></td><td>6</td><td>4</td></tr><tr><td>Sentence Embedding</td><td>3</td><td></td><td></td></tr><tr><td>Simulation</td><td>1</td><td>2</td><td>3</td></tr><tr><td>Simulator</td><td>1</td><td>2</td><td>3</td></tr><tr><td>Speech-to-Speech Translation</td><td>2</td><td></td><td></td></tr><tr><td>Stemming</td><td></td><td></td><td>1</td></tr><tr><td>Stochastic Gradient Descent</td><td></td><td></td><td>2</td></tr><tr><td>Summarization</td><td>4</td><td></td><td>1</td></tr><tr><td>Supervised Learning</td><td>6</td><td>4</td><td>4</td></tr><tr><td>T5</td><td>1</td><td></td><td></td></tr><tr><td>Text Classification</td><td>2</td><td></td><td></td></tr><tr><td>Text Clustering</td><td>1</td><td></td><td></td></tr><tr><td>Text Generation</td><td>4</td><td>2</td><td></td></tr><tr><td>Text Summarization</td><td>1</td><td></td><td></td></tr><tr><td>Text2SQL</td><td>1</td><td></td><td></td></tr><tr><td>Text2image</td><td></td><td>5</td><td></td></tr><tr><td>Transfer Learning</td><td></td><td>2</td><td>3</td></tr><tr><td>Transformer</td><td>9</td><td>5</td><td>5</td></tr><tr><td>Unsupervised Learning</td><td>1</td><td>3</td><td>2</td></tr><tr><td>Vision-and-Language</td><td>3</td><td>3</td><td></td></tr><tr><td>Visual Question Answering</td><td></td><td>1</td><td></td></tr><tr><td>Weakly-supervised Learning</td><td>1</td><td></td><td></td></tr><tr><td>Word Embedding</td><td>1</td><td></td><td></td></tr><tr><td>Word Sense Disambiguation</td><td>1</td><td></td><td></td></tr><tr><td>Yolo</td><td></td><td>1</td><td></td></tr><tr><td>Zero-shot</td><td>9</td><td></td><td></td></tr><tr><td>Zero-shot Learning</td><td>1</td><td></td><td></td></tr><tr><td>falcon</td><td>1</td><td></td><td></td></tr><tr><td>human-in-the-loop</td><td>1</td><td></td><td>1</td></tr></tbody></table><script>$(function(){$("table").addClass("keyword-table table-bordered border-success"),$("table thead").addClass("sticky-top"),$("table tbody td").css("text-align","")})</script><h2 id=cscl-89>cs.CL (89)</h2><h3 id=189--1316-benchmarking-retrieval-augmented-generation-for-medicine-guangzhi-xiong-et-al-2024>(1/89 | 1/316) Benchmarking Retrieval-Augmented Generation for Medicine (Guangzhi Xiong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Guangzhi Xiong, Qiao Jin, Zhiyong Lu, Aidong Zhang. (2024)<br><strong>Benchmarking Retrieval-Augmented Generation for Medicine</strong><br><button class=copy-to-clipboard title="Benchmarking Retrieval-Augmented Generation for Medicine" index=1>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-1 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 146<br>Keywords: Benchmarking, Benchmarking, Retrieval-Augmented Generation, Retrieval-Augmented Generation, Retrieval-Augmented Generation, GPT, GPT-3, GPT-3.5, GPT-4, Information Retrieval, Question Answering, Question Answering, Chain-of-thought Prompt, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13178v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13178v2.pdf filename=2402.13178v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>While <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have achieved state-of-the-art performance on a wide range of medical <b>question</b> <b>answering</b> <b>(QA)</b> tasks, they still face challenges with hallucinations and outdated knowledge. <b>Retrieval-augmented</b> <b>generation</b> <b>(RAG)</b> is a promising solution and has been widely adopted. However, a <b>RAG</b> system can involve multiple flexible components, and there is a lack of best practices regarding the optimal <b>RAG</b> setting for various medical purposes. To systematically evaluate such systems, we propose the Medical <b>Information</b> <b>Retrieval-Augmented</b> <b>Generation</b> <b>Evaluation</b> (MIRAGE), a first-of-its-kind <b>benchmark</b> including 7,663 <b>questions</b> <b>from</b> five medical <b>QA</b> datasets. Using MIRAGE, we conducted <b>large-scale</b> <b>experiments</b> <b>with</b> over 1.8 trillion <b>prompt</b> tokens on 41 combinations of different corpora, retrievers, and backbone <b>LLMs</b> through the MedRAG toolkit introduced in this work. Overall, MedRAG improves the accuracy of six different <b>LLMs</b> by up to 18% over <b>chain-of-thought</b> <b>prompting,</b> elevating the performance of <b>GPT-3.5</b> and Mixtral to <b>GPT-4-level.</b> Our results show that the combination of various medical corpora and retrievers achieves the best performance. In addition, we discovered a log-linear scaling property and the &ldquo;lost-in-the-middle&rdquo; effects in medical <b>RAG.</b> We believe our comprehensive evaluations can serve as practical guidelines for implementing <b>RAG</b> systems for medicine.</p></p class="citation"></blockquote><h3 id=289--2316-can-gnn-be-good-adapter-for-llms-xuanwen-huang-et-al-2024>(2/89 | 2/316) Can GNN be Good Adapter for LLMs? (Xuanwen Huang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xuanwen Huang, Kaiqiao Han, Yang Yang, Dezheng Bao, Quanjin Tao, Ziwei Chai, Qi Zhu. (2024)<br><strong>Can GNN be Good Adapter for LLMs?</strong><br><button class=copy-to-clipboard title="Can GNN be Good Adapter for LLMs?" index=2>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-2 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 143<br>Keywords: Node Classification, Graph, Graph Neural Network, Graph Neural Network, Fine-tuning, Recommendation, Zero-shot, GPT, GPT-2, LLaMA, RoBERTa, Large Language Model, Large Language Model, Prompt, Zero-shot Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12984v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12984v1.pdf filename=2402.12984v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recently, <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have demonstrated superior capabilities in understanding and <b>zero-shot</b> <b>learning</b> on textual data, promising significant advances for many text-related domains. In the <b>graph</b> <b>domain,</b> <b>various</b> real-world scenarios also involve textual data, where tasks and <b>node</b> <b>features</b> can be described by text. These text-attributed <b>graphs</b> <b>(TAGs)</b> <b>have</b> broad applications in social media, <b>recommendation</b> systems, etc. Thus, this paper explores how to utilize <b>LLMs</b> to model TAGs. Previous methods for TAG modeling are based on million-scale LMs. When scaled up to billion-scale <b>LLMs,</b> they face huge challenges in computational costs. Additionally, they also ignore the <b>zero-shot</b> <b>inference</b> capabilities of <b>LLMs.</b> Therefore, we propose GraphAdapter, which uses a <b>graph</b> <b>neural</b> <b>network</b> <b>(GNN)</b> as an efficient adapter in collaboration with <b>LLMs</b> to tackle TAGs. In terms of efficiency, the <b>GNN</b> adapter introduces only a few trainable parameters and can be trained with low computation costs. The entire framework is trained using auto-regression on <b>node</b> <b>text</b> (next token prediction). Once trained, GraphAdapter can be seamlessly <b>fine-tuned</b> with task-specific <b>prompts</b> for various downstream tasks. Through extensive experiments across multiple real-world TAGs, GraphAdapter based on <b>Llama</b> 2 gains an average improvement of approximately 5% in terms of <b>node</b> <b>classification.</b> Furthermore, GraphAdapter can also adapt to other language models, including <b>RoBERTa,</b> <b>GPT-2.</b> The promising results demonstrate that <b>GNNs</b> can serve as effective adapters for <b>LLMs</b> in TAG modeling.</p></p class="citation"></blockquote><h3 id=389--3316-effective-and-efficient-conversation-retrieval-for-dialogue-state-tracking-with-implicit-text-summaries-seanie-lee-et-al-2024>(3/89 | 3/316) Effective and Efficient Conversation Retrieval for Dialogue State Tracking with Implicit Text Summaries (Seanie Lee et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Seanie Lee, Jianpeng Cheng, Joris Driesen, Alexandru Coca, Anders Johannsen. (2024)<br><strong>Effective and Efficient Conversation Retrieval for Dialogue State Tracking with Implicit Text Summaries</strong><br><button class=copy-to-clipboard title="Effective and Efficient Conversation Retrieval for Dialogue State Tracking with Implicit Text Summaries" index=3>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-3 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 130<br>Keywords: Few-shot, Fine-tuning, Fine-tuning, Knowledge Distillation, GPT, LLaMA, Dialogue State Tracking, In-context Learning, Large Language Model, Large Language Model, Prompt, Prompt Learning, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13043v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13043v2.pdf filename=2402.13043v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Few-shot</b> <b>dialogue</b> <b>state</b> <b>tracking</b> (DST) with <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLM)</b> relies on an effective and efficient conversation retriever to find similar <b>in-context</b> examples for <b>prompt</b> <b>learning.</b> Previous works use raw <b>dialogue</b> <b>context</b> <b>as</b> search keys and queries, and a retriever is <b>fine-tuned</b> with annotated <b>dialogues</b> <b>to</b> <b>achieve</b> superior performance. However, the approach is less suited for scaling to new domains or new annotation languages, where <b>fine-tuning</b> data is unavailable. To address this problem, we handle the task of conversation retrieval based on text summaries of the conversations. A <b>LLM-based</b> conversation summarizer is adopted for query and key generation, which enables effective maximum inner product search. To avoid the extra inference cost brought by <b>LLM-based</b> conversation <b>summarization,</b> we further <b>distill</b> a light-weight conversation encoder which produces query embeddings without decoding summaries for test conversations. We validate our retrieval approach on MultiWOZ datasets with <b>GPT-Neo-2.7B</b> and <b>LLaMA-7B/30B.</b> The experimental results show a significant improvement over relevant baselines in real <b>few-shot</b> DST settings.</p></p class="citation"></blockquote><h3 id=489--4316-promptkd-distilling-student-friendly-knowledge-for-generative-language-models-via-prompt-tuning-gyeongman-kim-et-al-2024>(4/89 | 4/316) PromptKD: Distilling Student-Friendly Knowledge for Generative Language Models via Prompt Tuning (Gyeongman Kim et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Gyeongman Kim, Doohyuk Jang, Eunho Yang. (2024)<br><strong>PromptKD: Distilling Student-Friendly Knowledge for Generative Language Models via Prompt Tuning</strong><br><button class=copy-to-clipboard title="PromptKD: Distilling Student-Friendly Knowledge for Generative Language Models via Prompt Tuning" index=4>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-4 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 120<br>Keywords: Fine-tuning, Knowledge Distillation, Knowledge Distillation, Knowledge Distillation, Knowledge Distillation, Model Compression, GPT, GPT-2, Instruction Following, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12842v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12842v1.pdf filename=2402.12842v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advancements in <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have raised concerns about inference costs, increasing the need for research into <b>model</b> <b>compression.</b> While <b>knowledge</b> <b>distillation</b> <b>(KD)</b> is a prominent method for this, research on <b>KD</b> for generative language <b>models</b> <b>like</b> <b>LLMs</b> is relatively sparse, and the approach of <b>distilling</b> student-friendly <b>knowledge,</b> <b>which</b> has shown promising performance in <b>KD</b> for classification <b>models,</b> <b>remains</b> unexplored in generative language <b>models.</b> <b>To</b> explore this approach, we propose PromptKD, a simple yet effective method that utilizes <b>prompt</b> tuning - for the first time in <b>KD</b> - to enable generative language <b>models</b> <b>to</b> transfer student-friendly <b>knowledge.</b> <b>Unlike</b> previous works in classification that require <b>fine-tuning</b> the entire teacher <b>model</b> <b>for</b> extracting student-friendly <b>knowledge,</b> <b>PromptKD</b> achieves similar effects by adding a small number of <b>prompt</b> tokens and tuning only the <b>prompt</b> with student guidance. Extensive experiments on <b>instruction-following</b> <b>datasets</b> using the <b>GPT-2</b> <b>model</b> <b>family</b> show that PromptKD achieves state-of-the-art performance while adding only 0.0007% of the teacher&rsquo;s parameters as <b>prompts.</b> Further analysis suggests that <b>distilling</b> student-friendly <b>knowledge</b> <b>alleviates</b> exposure bias effectively throughout the entire training process, leading to performance enhancements.</p></p class="citation"></blockquote><h3 id=589--5316-evograd-a-dynamic-take-on-the-winograd-schema-challenge-with-human-adversaries-jing-han-sun-et-al-2024>(5/89 | 5/316) EvoGrad: A Dynamic Take on the Winograd Schema Challenge with Human Adversaries (Jing Han Sun et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jing Han Sun, Ali Emami. (2024)<br><strong>EvoGrad: A Dynamic Take on the Winograd Schema Challenge with Human Adversaries</strong><br><button class=copy-to-clipboard title="EvoGrad: A Dynamic Take on the Winograd Schema Challenge with Human Adversaries" index=5>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-5 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 113<br>Keywords: Benchmarking, human-in-the-loop, ChatGPT, Disambiguation, GPT, GPT-3, GPT-3.5, Common-sense Reasoning, Coreference Resolution, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13372v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13372v2.pdf filename=2402.13372v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>While <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> excel at the Winograd Schema Challenge (WSC), a <b>coreference</b> <b>resolution</b> task testing <b>common-sense</b> <b>reasoning</b> through pronoun <b>disambiguation,</b> they struggle with instances that feature minor alterations or rewording. To address this, we introduce EvoGrad, an open-source platform that harnesses a <b>human-in-the-loop</b> approach to create a dynamic dataset tailored to such altered WSC instances. Leveraging <b>ChatGPT&rsquo;s</b> capabilities, we expand our task instances from 182 to 3,691, setting a new <b>benchmark</b> for diverse <b>common-sense</b> <b>reasoning</b> datasets. Additionally, we introduce the error depth metric, assessing model stability in dynamic tasks. Our results emphasize the challenge posed by EvoGrad: Even the best performing <b>LLM,</b> <b>GPT-3.5,</b> achieves an accuracy of 65.0% with an average error depth of 7.2, a stark contrast to human performance of 92. 8% accuracy without perturbation errors. This highlights ongoing model limitations and the value of dynamic datasets in uncovering them.</p></p class="citation"></blockquote><h3 id=689--6316-me-llama-foundation-large-language-models-for-medical-applications-qianqian-xie-et-al-2024>(6/89 | 6/316) Me LLaMA: Foundation Large Language Models for Medical Applications (Qianqian Xie et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qianqian Xie, Qingyu Chen, Aokun Chen, Cheng Peng, Yan Hu, Fongci Lin, Xueqing Peng, Jimin Huang, Jeffrey Zhang, Vipina Keloth, Huan He, Lucila Ohno-Machido, Yonghui Wu, Hua Xu, Jiang Bian. (2024)<br><strong>Me LLaMA: Foundation Large Language Models for Medical Applications</strong><br><button class=copy-to-clipboard title="Me LLaMA: Foundation Large Language Models for Medical Applications" index=6>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-6 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 113<br>Keywords: Benchmarking, Few-shot, Few-shot Learning, Foundation Model, Zero-shot, ChatGPT, GPT, GPT-4, LLaMA, Instruction Tuning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12749v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12749v1.pdf filename=2402.12749v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> like <b>ChatGPT</b> and <b>LLaMA</b> have shown great promise in many AI applications. However, their performance on medical tasks is suboptimal and can be further improved by training on <b>large</b> <b>domain-specific</b> <b>datasets.</b> This study introduces Me <b>LLaMA,</b> a medical <b>LLM</b> family including <b>foundation</b> <b>models</b> - Me <b>LLaMA</b> 13/70B and their chat-enhanced versions - Me <b>LLaMA</b> 13/70B-chat, developed through the continual pre-training and <b>instruction</b> <b>tuning</b> of LLaMA2 using <b>large</b> <b>medical</b> <b>data.</b> Our domain-specific data suite for training and evaluation, includes a <b>large-scale</b> <b>continual</b> <b>pre-training</b> dataset with 129B tokens, an <b>instruction</b> <b>tuning</b> dataset with 214k samples, and a medical evaluation <b>benchmark</b> (MIBE) across six tasks with 14 datasets. Our extensive evaluation using MIBE shows that Me <b>LLaMA</b> models surpass existing open-source medical <b>LLMs</b> in <b>zero-shot</b> and <b>few-shot</b> <b>learning</b> and outperform commercial giants like <b>ChatGPT</b> on 6 out of 8 datasets and <b>GPT-4</b> in 3 out of 8 datasets. In addition, we empirically investigated the catastrophic forgetting problem, and our results show that Me <b>LLaMA</b> models outperform other medical <b>LLMs.</b> Me <b>LLaMA</b> is one of the first and largest open-source <b>foundational</b> <b>LLMs</b> designed for the medical domain, using both biomedical and clinical data. It exhibits superior performance across both general and medical tasks compared to other medical <b>LLMs,</b> rendering it an attractive choice for medical AI applications. All resources are available at: <a href=https://github.com/BIDS-Xu-Lab/Me-LLaMA>https://github.com/BIDS-Xu-Lab/Me-LLaMA</a>.</p></p class="citation"></blockquote><h3 id=789--7316-a-survey-on-knowledge-distillation-of-large-language-models-xiaohan-xu-et-al-2024>(7/89 | 7/316) A Survey on Knowledge Distillation of Large Language Models (Xiaohan Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiaohan Xu, Ming Li, Chongyang Tao, Tao Shen, Reynold Cheng, Jinyang Li, Can Xu, Dacheng Tao, Tianyi Zhou. (2024)<br><strong>A Survey on Knowledge Distillation of Large Language Models</strong><br><button class=copy-to-clipboard title="A Survey on Knowledge Distillation of Large Language Models" index=7>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-7 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 110<br>Keywords: Data Augmentation, Knowledge Distillation, Knowledge Distillation, Knowledge Distillation, Model Compression, GPT, GPT-4, LLaMA, Mistral, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13116v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13116v2.pdf filename=2402.13116v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the era of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs),</b> <b>Knowledge</b> <b>Distillation</b> <b>(KD)</b> emerges as a pivotal methodology for transferring advanced capabilities from leading proprietary <b>LLMs,</b> such as <b>GPT-4,</b> to their open-source counterparts like <b>LLaMA</b> and <b>Mistral.</b> Additionally, as open-source <b>LLMs</b> flourish, <b>KD</b> plays a crucial role in both compressing these <b>models,</b> <b>and</b> facilitating their self-improvement by employing themselves as teachers. This paper presents a comprehensive survey of <b>KD&rsquo;s</b> role within the realm of <b>LLM,</b> highlighting its critical function in imparting advanced <b>knowledge</b> <b>to</b> smaller <b>models</b> <b>and</b> its utility in <b>model</b> <b>compression</b> and self-improvement. Our survey is meticulously structured around three foundational pillars: \textit{algorithm}, \textit{skill}, and \textit{verticalization} &ndash; providing a comprehensive examination of <b>KD</b> mechanisms, the enhancement of specific cognitive abilities, and their practical implications across diverse fields. Crucially, the survey navigates the intricate interplay between <b>data</b> <b>augmentation</b> (DA) and <b>KD,</b> illustrating how DA emerges as a powerful paradigm within the <b>KD</b> framework to bolster <b>LLMs&rsquo;</b> performance. By leveraging DA to generate context-rich, skill-specific training <b>data,</b> <b>KD</b> transcends traditional boundaries, enabling open-source <b>models</b> <b>to</b> approximate the contextual adeptness, ethical alignment, and deep semantic insights characteristic of their proprietary counterparts. This work aims to provide an insightful guide for researchers and practitioners, offering a detailed overview of current methodologies in <b>KD</b> and proposing future research directions. Importantly, we firmly advocate for compliance with the legal terms that regulate the use of <b>LLMs,</b> ensuring ethical and lawful application of <b>KD</b> of <b>LLMs.</b> An associated Github repository is available at <a href=https://github.com/Tebmer/Awesome-Knowledge-Distillation-of-LLMs>https://github.com/Tebmer/Awesome-Knowledge-Distillation-of-LLMs</a>.</p></p class="citation"></blockquote><h3 id=889--8316-opdai-at-semeval-2024-task-6-small-llms-can-accelerate-hallucination-detection-with-weakly-supervised-data-chengcheng-wei-et-al-2024>(8/89 | 8/316) OPDAI at SemEval-2024 Task 6: Small LLMs can Accelerate Hallucination Detection with Weakly Supervised Data (Chengcheng Wei et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chengcheng Wei, Ze Chen, Songtan Fang, Jiarong He, Max Gao. (2024)<br><strong>OPDAI at SemEval-2024 Task 6: Small LLMs can Accelerate Hallucination Detection with Weakly Supervised Data</strong><br><button class=copy-to-clipboard title="OPDAI at SemEval-2024 Task 6: Small LLMs can Accelerate Hallucination Detection with Weakly Supervised Data" index=8>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-8 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 110<br>Keywords: Few-shot, Few-shot Learning, Fine-tuning, Supervised Learning, Weakly-supervised Learning, GPT, GPT-4, Hallucination Detection, Text Generation, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12913v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12913v1.pdf filename=2402.12913v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper mainly describes a unified system for <b>hallucination</b> <b>detection</b> of <b>LLMs,</b> which wins the second prize in the model-agnostic track of the SemEval-2024 Task 6, and also achieves considerable results in the model-aware track. This task aims to detect <b>hallucination</b> <b>with</b> <b>LLMs</b> for three different <b>text-generation</b> <b>tasks</b> without labeled training data. We utilize <b>prompt</b> engineering and <b>few-shot</b> <b>learning</b> to verify the performance of different <b>LLMs</b> on the validation data. Then we select the <b>LLMs</b> with better performance to generate high-quality weakly <b>supervised</b> training data, which not only satisfies the consistency of different <b>LLMs,</b> but also satisfies the consistency of the optimal <b>LLM</b> with different sampling parameters. Furthermore, we <b>finetune</b> different <b>LLMs</b> by using the constructed training data, and finding that a relatively small <b>LLM</b> can achieve a competitive level of performance in <b>hallucination</b> <b>detection,</b> when compared to the large <b>LLMs</b> and the <b>prompt-based</b> approaches using <b>GPT-4.</b></p></p class="citation"></blockquote><h3 id=989--9316-moelora-contrastive-learning-guided-mixture-of-experts-on-parameter-efficient-fine-tuning-for-large-language-models-tongxu-luo-et-al-2024>(9/89 | 9/316) MoELoRA: Contrastive Learning Guided Mixture of Experts on Parameter-Efficient Fine-Tuning for Large Language Models (Tongxu Luo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tongxu Luo, Jiahe Lei, Fangyu Lei, Weihao Liu, Shizhu He, Jun Zhao, Kang Liu. (2024)<br><strong>MoELoRA: Contrastive Learning Guided Mixture of Experts on Parameter-Efficient Fine-Tuning for Large Language Models</strong><br><button class=copy-to-clipboard title="MoELoRA: Contrastive Learning Guided Mixture of Experts on Parameter-Efficient Fine-Tuning for Large Language Models" index=9>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-9 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 93<br>Keywords: Benchmarking, Contrastive Learning, Fine-tuning, GPT, GPT-3, GPT-3.5, Common-sense Reasoning, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12851v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12851v1.pdf filename=2402.12851v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Fine-tuning</b> is often necessary to enhance the adaptability of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLM)</b> to downstream tasks. Nonetheless, the process of updating billions of parameters demands significant computational resources and training time, which poses a substantial obstacle to the widespread application of <b>large-scale</b> <b>models</b> <b>in</b> various scenarios. To address this issue, Parameter-Efficient <b>Fine-Tuning</b> (PEFT) has emerged as a prominent paradigm in recent research. However, current PEFT approaches that employ a limited set of global parameters (such as LoRA, which adds low-rank approximation matrices to all weights) face challenges in flexibly combining different computational modules in downstream tasks. In this work, we introduce a novel PEFT method: MoELoRA. We consider LoRA as Mixture of Experts (MoE), and to mitigate the random routing phenomenon observed in MoE, we propose the utilization of <b>contrastive</b> <b>learning</b> to encourage experts to learn distinct features. We conducted experiments on 11 tasks in math <b>reasoning</b> and <b>common-sense</b> <b>reasoning</b> <b>benchmarks.</b> With the same number of parameters, our approach outperforms LoRA significantly. In math <b>reasoning,</b> MoELoRA achieved an average performance that was 4.2% higher than LoRA, and demonstrated competitive performance compared to the 175B <b>GPT-3.5</b> on several <b>benchmarks.</b></p></p class="citation"></blockquote><h3 id=1089--10316-a-simple-but-effective-approach-to-improve-structured-language-model-output-for-information-extraction-yinghao-li-et-al-2024>(10/89 | 10/316) A Simple but Effective Approach to Improve Structured Language Model Output for Information Extraction (Yinghao Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yinghao Li, Rampi Ramprasad, Chao Zhang. (2024)<br><strong>A Simple but Effective Approach to Improve Structured Language Model Output for Information Extraction</strong><br><button class=copy-to-clipboard title="A Simple but Effective Approach to Improve Structured Language Model Output for Information Extraction" index=10>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-10 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-IR, cs.CL<br>Keyword Score: 90<br>Keywords: Zero-shot, Information Retrieval, Named Entity Recognition, Named Entity Recognition, Relation Extraction, Text Generation, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13364v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13364v1.pdf filename=2402.13364v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have demonstrated impressive abilities in generating unstructured natural language according to instructions. However, their performance can be inconsistent when tasked with producing <b>text</b> <b>that</b> adheres to specific structured formats, which is crucial in applications like <b>named</b> <b>entity</b> <b>recognition</b> <b>(NER)</b> or <b>relation</b> <b>extraction</b> (RE). To address this issue, this paper introduces an efficient method, G&amp;O, to enhance their structured <b>text</b> <b>generation</b> capabilities. It breaks the generation into a two-step pipeline: initially, <b>LLMs</b> generate answers in natural language as intermediate responses. Subsequently, <b>LLMs</b> are asked to organize the output into the desired structure, using the intermediate responses as context. G&amp;O effectively separates the generation of content from the structuring process, reducing the pressure of completing two orthogonal tasks simultaneously. Tested on <b>zero-shot</b> <b>NER</b> and RE, the results indicate a significant improvement in <b>LLM</b> performance with minimal additional efforts. This straightforward and adaptable <b>prompting</b> technique can also be combined with other strategies, like self-consistency, to further elevate <b>LLM</b> capabilities in various structured <b>text</b> <b>generation</b> tasks.</p></p class="citation"></blockquote><h3 id=1189--11316-few-shot-clinical-entity-recognition-in-three-languages-masked-language-models-outperform-llm-prompting-marco-naguib-et-al-2024>(11/89 | 11/316) Few shot clinical entity recognition in three languages: Masked language models outperform LLM prompting (Marco Naguib et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Marco Naguib, Xavier Tannier, Aurélie Névéol. (2024)<br><strong>Few shot clinical entity recognition in three languages: Masked language models outperform LLM prompting</strong><br><button class=copy-to-clipboard title="Few shot clinical entity recognition in three languages: Masked language models outperform LLM prompting" index=11>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-11 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 90<br>Keywords: Few-shot, Few-shot Learning, Low-Resource, Supervised Learning, Named Entity Recognition, Large Language Model, Large Language Model, Masked Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12801v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12801v1.pdf filename=2402.12801v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> are becoming the go-to solution for many natural language processing tasks, including in specialized domains where their <b>few-shot</b> <b>capacities</b> are expected to yield high performance in <b>low-resource</b> settings. Herein, we aim to assess the performance of <b>Large</b> <b>Language</b> <b>Models</b> for few shot clinical entity recognition in multiple languages. We evaluate <b>named</b> <b>entity</b> <b>recognition</b> in English, French and Spanish using 8 in-domain (clinical) and 6 out-domain gold standard corpora. We assess the performance of 10 auto-regressive language models using <b>prompting</b> and 16 <b>masked</b> <b>language</b> <b>models</b> used for text encoding in a biLSTM-CRF <b>supervised</b> tagger. We create a <b>few-shot</b> <b>set-up</b> by limiting the amount of annotated data available to 100 sentences. Our experiments show that although larger <b>prompt-based</b> models tend to achieve competitive F-measure for <b>named</b> <b>entity</b> <b>recognition</b> outside the clinical domain, this level of performance does not carry over to the clinical domain where lighter <b>supervised</b> taggers relying on <b>masked</b> <b>language</b> <b>models</b> perform better, even with the performance drop incurred from the <b>few-shot</b> <b>set-up.</b> In all experiments, the CO2 impact of <b>masked</b> <b>language</b> <b>models</b> is inferior to that of auto-regressive models. Results are consistent over the three languages and suggest that <b>few-shot</b> <b>learning</b> using <b>Large</b> <b>language</b> <b>models</b> is not production ready for <b>named</b> <b>entity</b> <b>recognition</b> in the clinical domain. Instead, models could be used for speeding-up the production of gold standard annotated data.</p></p class="citation"></blockquote><h3 id=1289--12316-reliable-llm-based-user-simulator-for-task-oriented-dialogue-systems-ivan-sekulić-et-al-2024>(12/89 | 12/316) Reliable LLM-based User Simulator for Task-Oriented Dialogue Systems (Ivan Sekulić et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ivan Sekulić, Silvia Terragni, Victor Guimarães, Nghia Khau, Bruna Guedes, Modestas Filipavicius, André Ferreira Manso, Roland Mathis. (2024)<br><strong>Reliable LLM-based User Simulator for Task-Oriented Dialogue Systems</strong><br><button class=copy-to-clipboard title="Reliable LLM-based User Simulator for Task-Oriented Dialogue Systems" index=12>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-12 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 83<br>Keywords: Benchmarking, Data Augmentation, Fine-tuning, Fine-tuning, Simulation, Simulator, Dialogue System, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13374v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13374v1.pdf filename=2402.13374v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the realm of <b>dialogue</b> <b>systems,</b> user <b>simulation</b> techniques have emerged as a game-changer, redefining the evaluation and enhancement of task-oriented <b>dialogue</b> <b>(TOD)</b> systems. These methods are crucial for replicating real user interactions, enabling applications like synthetic <b>data</b> <b>augmentation,</b> error detection, and robust evaluation. However, existing approaches often rely on rigid rule-based methods or on annotated <b>data.</b> <b>This</b> paper introduces DAUS, a Domain-Aware User Simulator. Leveraging <b>large</b> <b>language</b> <b>models,</b> we <b>fine-tune</b> DAUS on real examples of task-oriented <b>dialogues.</b> <b>Results</b> on two relevant <b>benchmarks</b> showcase significant improvements in terms of user goal fulfillment. Notably, we have observed that <b>fine-tuning</b> enhances the simulator&rsquo;s coherence with user goals, effectively mitigating hallucinations &ndash; a major source of inconsistencies in simulator responses.</p></p class="citation"></blockquote><h3 id=1389--13316-grafford-a-benchmark-dataset-for-testing-the-knowledge-of-object-affordances-of-language-and-vision-models-sayantan-adak-et-al-2024>(13/89 | 13/316) GRAFFORD: A Benchmark Dataset for Testing the Knowledge of Object Affordances of Language and Vision Models (Sayantan Adak et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sayantan Adak, Daivik Agrawal, Animesh Mukherjee, Somak Aditya. (2024)<br><strong>GRAFFORD: A Benchmark Dataset for Testing the Knowledge of Object Affordances of Language and Vision Models</strong><br><button class=copy-to-clipboard title="GRAFFORD: A Benchmark Dataset for Testing the Knowledge of Object Affordances of Language and Vision Models" index=13>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-13 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 83<br>Keywords: Benchmarking, Few-shot, Fine-tuning, Transformer, Grounding, Reasoning, Pre-trained Language Model, Vision-and-Language, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12881v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12881v1.pdf filename=2402.12881v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We investigate the knowledge of object affordances in <b>pre-trained</b> <b>language</b> <b>models</b> (LMs) and <b>pre-trained</b> <b>Vision-Language</b> <b>models</b> (VLMs). <b>Transformers-based</b> large <b>pre-trained</b> <b>language</b> <b>models</b> (PTLM) learn contextual representation from massive amounts of unlabeled text and are shown to perform impressively in downstream NLU tasks. In parallel, a growing body of literature shows that PTLMs fail inconsistently and non-intuitively, showing a lack of <b>reasoning</b> and <b>grounding.</b> To take a first step toward quantifying the effect of <b>grounding</b> (or lack thereof), we curate a novel and comprehensive dataset of object affordances &ndash; GrAFFORD, characterized by 15 affordance classes. Unlike affordance datasets collected in vision and language domains, we annotate in-the-wild sentences with objects and affordances. Experimental results reveal that PTLMs exhibit limited <b>reasoning</b> abilities when it comes to uncommon object affordances. We also observe that <b>pre-trained</b> <b>VLMs</b> <b>do</b> not necessarily capture object affordances effectively. Through <b>few-shot</b> <b>fine-tuning,</b> we demonstrate improvement in affordance knowledge in PTLMs and VLMs. Our research contributes a novel dataset for language <b>grounding</b> tasks, and presents insights into LM capabilities, advancing the understanding of object affordances. Codes and data are available at <a href=https://github.com/sayantan11995/Affordance>https://github.com/sayantan11995/Affordance</a></p></p class="citation"></blockquote><h3 id=1489--14316-the-finben-an-holistic-financial-benchmark-for-large-language-models-qianqian-xie-et-al-2024>(14/89 | 14/316) The FinBen: An Holistic Financial Benchmark for Large Language Models (Qianqian Xie et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qianqian Xie, Weiguang Han, Zhengyu Chen, Ruoyu Xiang, Xiao Zhang, Yueru He, Mengxi Xiao, Dong Li, Yongfu Dai, Duanyu Feng, Yijing Xu, Haoqiang Kang, Ziyan Kuang, Chenhan Yuan, Kailai Yang, Zheheng Luo, Tianlin Zhang, Zhiwei Liu, Guojun Xiong, Zhiyang Deng, Yuechen Jiang, Zhiyuan Yao, Haohang Li, Yangyang Yu, Gang Hu, Jiajia Huang, Xiao-Yang Liu, Alejandro Lopez-Lira, Benyou Wang, Yanzhao Lai, Hao Wang, Min Peng, Sophia Ananiadou, Jimin Huang. (2024)<br><strong>The FinBen: An Holistic Financial Benchmark for Large Language Models</strong><br><button class=copy-to-clipboard title="The FinBen: An Holistic Financial Benchmark for Large Language Models" index=14>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-14 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CE, cs-CL, cs.CL<br>Keyword Score: 83<br>Keywords: Benchmarking, ChatGPT, GPT, GPT-4, Gemini, Reasoning, Instruction Tuning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12659v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12659v1.pdf filename=2402.12659v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>LLMs</b> have transformed NLP and shown promise in various fields, yet their potential in finance is underexplored due to a lack of thorough evaluations and the complexity of financial tasks. This along with the rapid development of <b>LLMs,</b> highlights the urgent need for a systematic financial evaluation <b>benchmark</b> for <b>LLMs.</b> In this paper, we introduce FinBen, the first comprehensive open-sourced evaluation <b>benchmark,</b> specifically designed to thoroughly assess the capabilities of <b>LLMs</b> in the financial domain. FinBen encompasses 35 datasets across 23 financial tasks, organized into three spectrums of difficulty inspired by the Cattell-Horn-Carroll theory, to evaluate <b>LLMs&rsquo;</b> cognitive abilities in inductive <b>reasoning,</b> associative memory, quantitative <b>reasoning,</b> crystallized intelligence, and more. Our evaluation of 15 representative <b>LLMs,</b> including <b>GPT-4,</b> <b>ChatGPT,</b> and the latest <b>Gemini,</b> reveals insights into their strengths and limitations within the financial domain. The findings indicate that <b>GPT-4</b> leads in quantification, extraction, numerical <b>reasoning,</b> and stock trading, while <b>Gemini</b> shines in generation and forecasting; however, both struggle with complex extraction and forecasting, showing a clear need for targeted enhancements. <b>Instruction</b> <b>tuning</b> boosts simple task performance but falls short in improving complex <b>reasoning</b> and forecasting abilities. FinBen seeks to continuously evaluate <b>LLMs</b> in finance, fostering AI development with regular updates of tasks and models.</p></p class="citation"></blockquote><h3 id=1589--15316-the-impact-of-demonstrations-on-multilingual-in-context-learning-a-multidimensional-analysis-miaoran-zhang-et-al-2024>(15/89 | 15/316) The Impact of Demonstrations on Multilingual In-Context Learning: A Multidimensional Analysis (Miaoran Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Miaoran Zhang, Vagrant Gautam, Mingyang Wang, Jesujoba O. Alabi, Xiaoyu Shen, Dietrich Klakow, Marius Mosbach. (2024)<br><strong>The Impact of Demonstrations on Multilingual In-Context Learning: A Multidimensional Analysis</strong><br><button class=copy-to-clipboard title="The Impact of Demonstrations on Multilingual In-Context Learning: A Multidimensional Analysis" index=15>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-15 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 80<br>Keywords: GPT, GPT-3, GPT-3.5, GPT-4, LLaMA, In-context Learning, In-context Learning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12976v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12976v1.pdf filename=2402.12976v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>In-context</b> <b>learning</b> is a popular inference strategy where <b>large</b> <b>language</b> <b>models</b> solve a task using only a few labelled demonstrations without needing any parameter updates. Compared to work on monolingual (English) <b>in-context</b> <b>learning,</b> multilingual <b>in-context</b> <b>learning</b> is under-explored, and we lack an in-depth understanding of the role of demonstrations in this context. To address this gap, we conduct a multidimensional analysis of multilingual <b>in-context</b> <b>learning,</b> experimenting with 5 models from different model families, 9 datasets covering classification and generation tasks, and 56 typologically diverse languages. Our results reveal that the effectiveness of demonstrations varies significantly across models, tasks, and languages. We also find that <b>Llama</b> 2-Chat, <b>GPT-3.5,</b> and <b>GPT-4</b> are largely insensitive to the quality of demonstrations. Instead, a carefully crafted template often eliminates the benefits of demonstrations for some tasks and languages altogether. These findings show that the importance of demonstrations might be overestimated. Our work highlights the need for granular evaluation across multiple axes towards a better understanding of <b>in-context</b> <b>learning.</b></p></p class="citation"></blockquote><h3 id=1689--16316-question-calibration-and-multi-hop-modeling-for-temporal-question-answering-chao-xue-et-al-2024>(16/89 | 16/316) Question Calibration and Multi-Hop Modeling for Temporal Question Answering (Chao Xue et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chao Xue, Di Liang, Pengfei Wang, Jing Zhang. (2024)<br><strong>Question Calibration and Multi-Hop Modeling for Temporal Question Answering</strong><br><button class=copy-to-clipboard title="Question Calibration and Multi-Hop Modeling for Temporal Question Answering" index=16>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-16 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 76<br>Keywords: Message-Passing, Graph, Graph Neural Network, Benchmarking, Knowledge Graph, Knowledge Graph, Question Answering, Question Answering, Pre-trained Language Model, Pre-trained Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13188v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13188v1.pdf filename=2402.13188v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Many models that leverage <b>knowledge</b> <b>graphs</b> <b>(KGs)</b> have recently demonstrated remarkable success in <b>question</b> <b>answering</b> <b>(QA)</b> tasks. In the real world, many facts contained in <b>KGs</b> are time-constrained thus temporal KGQA has received increasing attention. Despite the fruitful efforts of previous models in temporal KGQA, they still have several limitations. (I) They adopt <b>pre-trained</b> <b>language</b> <b>models</b> <b>(PLMs)</b> to obtain <b>question</b> <b>representations,</b> while <b>PLMs</b> tend to focus on entity information and ignore entity transfer caused by temporal constraints, and finally fail to learn specific temporal representations of entities. (II) They neither emphasize the <b>graph</b> structure between entities nor explicitly model the multi-hop relationship in the <b>graph,</b> which will make it difficult to solve complex multi-hop <b>question</b> <b>answering.</b> To alleviate this problem, we propose a novel <b>Question</b> <b>Calibration</b> and Multi-Hop Modeling (QC-MHM) approach. Specifically, We first calibrate the <b>question</b> <b>representation</b> by fusing the <b>question</b> <b>and</b> the time-constrained concepts in <b>KG.</b> Then, we construct the <b>GNN</b> layer to complete multi-hop message passing. Finally, the <b>question</b> <b>representation</b> is combined with the embedding output by the <b>GNN</b> to generate the final prediction. Empirical results verify that the proposed model achieves better performance than the state-of-the-art models in the <b>benchmark</b> dataset. Notably, the Hits@1 and Hits@10 results of QC-MHM on the CronQuestions dataset&rsquo;s complex <b>questions</b> <b>are</b> absolutely improved by 5.1% and 1.2% compared to the best-performing baseline. Moreover, QC-MHM can generate interpretable and trustworthy predictions.</p></p class="citation"></blockquote><h3 id=1789--17316-humaneval-on-latest-gpt-models----2024-daniel-li-et-al-2024>(17/89 | 17/316) HumanEval on Latest GPT Models &ndash; 2024 (Daniel Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Daniel Li, Lincoln Murr. (2024)<br><strong>HumanEval on Latest GPT Models &ndash; 2024</strong><br><button class=copy-to-clipboard title="HumanEval on Latest GPT Models -- 2024" index=17>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-17 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 73<br>Keywords: Benchmarking, Zero-shot, CodeGen, GPT, GPT-4, Code Generation, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14852v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14852v1.pdf filename=2402.14852v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In 2023, we are using the latest models of <b>GPT-4</b> to advance program synthesis. The <b>large</b> <b>language</b> <b>models</b> have significantly improved the state-of-the-art for this purpose. To make these advancements more accessible, we have created a repository that connects these models to Huamn Eval. This dataset was initally developed to be used with a language model called <b>CODEGEN</b> on natural and programming language data. The utility of these trained models is showcased by demonstrating their competitive performance in <b>zero-shot</b> Python <b>code</b> <b>generation</b> on HumanEval tasks compared to previous state-of-the-art solutions. Additionally, this gives way to developing more multi-step paradigm synthesis. This <b>benchmark</b> features 160 diverse problem sets factorized into multistep <b>prompts</b> that our analysis shows significantly improves program synthesis over single-turn inputs. All <b>code</b> <b>is</b> open source at <a href=https://github.com/daniel442li/gpt-human-eval>https://github.com/daniel442li/gpt-human-eval</a> .</p></p class="citation"></blockquote><h3 id=1889--18316-synthetic-data-almost-from-scratch-generalized-instruction-tuning-for-language-models-haoran-li-et-al-2024>(18/89 | 18/316) Synthetic Data (Almost) from Scratch: Generalized Instruction Tuning for Language Models (Haoran Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haoran Li, Qingxiu Dong, Zhengyang Tang, Chaojun Wang, Xingxing Zhang, Haoyang Huang, Shaohan Huang, Xiaolong Huang, Zeqiang Huang, Dongdong Zhang, Yuxian Gu, Xin Cheng, Xun Wang, Si-Qing Chen, Li Dong, Wei Lu, Zhifang Sui, Benyou Wang, Wai Lam, Furu Wei. (2024)<br><strong>Synthetic Data (Almost) from Scratch: Generalized Instruction Tuning for Language Models</strong><br><button class=copy-to-clipboard title="Synthetic Data (Almost) from Scratch: Generalized Instruction Tuning for Language Models" index=18>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-18 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 70<br>Keywords: Mistral, Instruction Following, Mathematical Reasoning, Reasoning, Instruction Tuning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13064v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13064v1.pdf filename=2402.13064v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce Generalized <b>Instruction</b> <b>Tuning</b> (called GLAN), a general and scalable method for <b>instruction</b> <b>tuning</b> of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs).</b> Unlike prior work that relies on seed examples or existing datasets to construct <b>instruction</b> <b>tuning</b> data, GLAN exclusively utilizes a pre-curated taxonomy of human knowledge and capabilities as input and generates <b>large-scale</b> <b>synthetic</b> <b>instruction</b> <b>data</b> across all disciplines. Specifically, inspired by the systematic structure in human education system, we build the taxonomy by decomposing human knowledge and capabilities to various fields, sub-fields and ultimately, distinct disciplines semi-automatically, facilitated by <b>LLMs.</b> Subsequently, we generate a comprehensive list of subjects for every discipline and proceed to design a syllabus tailored to each subject, again utilizing <b>LLMs.</b> With the fine-grained key concepts detailed in every class session of the syllabus, we are able to generate diverse <b>instructions</b> <b>with</b> a broad coverage across the entire spectrum of human knowledge and skills. Extensive experiments on <b>large</b> <b>language</b> <b>models</b> (e.g., <b>Mistral)</b> demonstrate that GLAN excels in multiple dimensions from <b>mathematical</b> <b>reasoning,</b> coding, academic exams, logical <b>reasoning</b> to general <b>instruction</b> <b>following</b> without using task-specific training data of these tasks. In addition, GLAN allows for easy customization and new fields or skills can be added by simply incorporating a new node into our taxonomy.</p></p class="citation"></blockquote><h3 id=1989--19316-umbclu-at-semeval-2024-task-1a-and-1c-semantic-textual-relatedness-with-and-without-machine-translation-shubhashis-roy-dipta-et-al-2024>(19/89 | 19/316) UMBCLU at SemEval-2024 Task 1A and 1C: Semantic Textual Relatedness with and without machine translation (Shubhashis Roy Dipta et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shubhashis Roy Dipta, Sai Vallurupalli. (2024)<br><strong>UMBCLU at SemEval-2024 Task 1A and 1C: Semantic Textual Relatedness with and without machine translation</strong><br><button class=copy-to-clipboard title="UMBCLU at SemEval-2024 Task 1A and 1C: Semantic Textual Relatedness with and without machine translation" index=19>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-19 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 70<br>Keywords: Fine-tuning, Supervised Learning, T5, Neural Machine Translation, Sentence Embedding, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12730v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12730v1.pdf filename=2402.12730v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper describes the system we developed for SemEval-2024 Task 1, &ldquo;Semantic Textual Relatedness for African and Asian Languages.&rdquo; The aim of the task is to build a model that can identify semantic textual relatedness (STR) between two <b>sentences</b> <b>of</b> a target language belonging to a collection of African and Asian languages. We participated in Subtasks A and C and explored <b>supervised</b> and cross-lingual training leveraging <b>large</b> <b>language</b> <b>models</b> <b>(LLMs).</b> Pre-trained <b>large</b> <b>language</b> <b>models</b> have been extensively used for <b>machine</b> <b>translation</b> and semantic similarity. Using a combination of <b>machine</b> <b>translation</b> and <b>sentence</b> <b>embedding</b> <b>LLMs,</b> we developed a unified STR model, TranSem, for subtask A and <b>fine-tuned</b> the <b>T5</b> family of models on the STR data, FineSem, for use in subtask C. Our model results for 7 languages in subtask A were better than the official baseline for 3 languages and on par with the baseline for the remaining 4 languages. Our model results for the 12 languages in subtask C resulted in 1st place for Africaans, 2nd place for Indonesian, and 3rd place for English with low performance for the remaining 9 languages.</p></p class="citation"></blockquote><h3 id=2089--20316-drbenchmark-a-large-language-understanding-evaluation-benchmark-for-french-biomedical-domain-yanis-labrak-et-al-2024>(20/89 | 20/316) DrBenchmark: A Large Language Understanding Evaluation Benchmark for French Biomedical Domain (Yanis Labrak et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yanis Labrak, Adrien Bazoge, Oumaima El Khettari, Mickael Rouvier, Pacome Constant dit Beaufils, Natalia Grabar, Beatrice Daille, Solen Quiniou, Emmanuel Morin, Pierre-Antoine Gourraud, Richard Dufour. (2024)<br><strong>DrBenchmark: A Large Language Understanding Evaluation Benchmark for French Biomedical Domain</strong><br><button class=copy-to-clipboard title="DrBenchmark: A Large Language Understanding Evaluation Benchmark for French Biomedical Domain" index=20>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-20 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 63<br>Keywords: Benchmarking, Named Entity Recognition, Question Answering, Masked Language Model, Masked Language Model, Pre-trained Language Model, Pre-trained Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13432v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13432v1.pdf filename=2402.13432v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The biomedical domain has sparked a significant interest in the field of Natural Language Processing (NLP), which has seen substantial advancements with <b>pre-trained</b> <b>language</b> <b>models</b> <b>(PLMs).</b> However, comparing these models has proven challenging due to variations in evaluation protocols across different models. A fair solution is to aggregate diverse downstream tasks into a <b>benchmark,</b> allowing for the assessment of intrinsic <b>PLMs</b> qualities from various perspectives. Although still limited to few languages, this initiative has been undertaken in the biomedical field, notably English and Chinese. This limitation hampers the evaluation of the latest French biomedical models, as they are either assessed on a minimal number of tasks with non-standardized protocols or evaluated using general downstream tasks. To bridge this research gap and account for the unique sensitivities of French, we present the first-ever publicly available French biomedical language understanding <b>benchmark</b> called DrBenchmark. It encompasses 20 diversified tasks, including <b>named-entity</b> <b>recognition,</b> <b>part-of-speech</b> tagging, <b>question-answering,</b> <b>semantic</b> textual similarity, and classification. We evaluate 8 state-of-the-art <b>pre-trained</b> <b>masked</b> <b>language</b> <b>models</b> <b>(MLMs)</b> on general and biomedical-specific data, as well as English specific <b>MLMs</b> to assess their cross-lingual capabilities. Our experiments reveal that no single model excels across all tasks, while generalist models are sometimes still competitive.</p></p class="citation"></blockquote><h3 id=2189--21316-cif-bench-a-chinese-instruction-following-benchmark-for-evaluating-the-generalizability-of-large-language-models-yizhi-li-et-al-2024>(21/89 | 21/316) CIF-Bench: A Chinese Instruction-Following Benchmark for Evaluating the Generalizability of Large Language Models (Yizhi LI et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yizhi LI, Ge Zhang, Xingwei Qu, Jiali Li, Zhaoqun Li, Zekun Wang, Hao Li, Ruibin Yuan, Yinghao Ma, Kai Zhang, Wangchunshu Zhou, Yiming Liang, Lei Zhang, Lei Ma, Jiajun Zhang, Zuowen Li, Stephen W. Huang, Chenghua Lin, Wenhu Chen, Jie Fu. (2024)<br><strong>CIF-Bench: A Chinese Instruction-Following Benchmark for Evaluating the Generalizability of Large Language Models</strong><br><button class=copy-to-clipboard title="CIF-Bench: A Chinese Instruction-Following Benchmark for Evaluating the Generalizability of Large Language Models" index=21>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-21 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 63<br>Keywords: Benchmarking, Low-Resource, Zero-shot, Instruction Following, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13109v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13109v1.pdf filename=2402.13109v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The advancement of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> has enhanced the ability to generalize across a wide range of unseen natural language processing (NLP) tasks through <b>instruction-following.</b> <b>Yet,</b> their effectiveness often diminishes in <b>low-resource</b> languages like Chinese, exacerbated by biased evaluations from data leakage, casting doubt on their true generalizability to new linguistic territories. In response, we introduce the Chinese <b>Instruction-Following</b> <b>Benchmark</b> (CIF-Bench), designed to evaluate the <b>zero-shot</b> generalizability of <b>LLMs</b> to the Chinese language. CIF-Bench comprises 150 tasks and 15,000 input-output pairs, developed by native speakers to test complex <b>reasoning</b> and Chinese cultural nuances across 20 categories. To mitigate evaluation bias, we release only half of the dataset publicly, with the remainder kept private, and introduce diversified <b>instructions</b> <b>to</b> minimize score variance, totaling 45,000 data instances. Our evaluation of 28 selected <b>LLMs</b> reveals a noticeable performance gap, with the best model scoring only 52.9%, highlighting the limitations of <b>LLMs</b> in less familiar language and task contexts. This work aims to uncover the current limitations of <b>LLMs</b> in handling Chinese tasks, pushing towards the development of more culturally informed and linguistically diverse models with the released data and <b>benchmark</b> (<a href=https://yizhilll.github.io/CIF-Bench/)>https://yizhilll.github.io/CIF-Bench/)</a>.</p></p class="citation"></blockquote><h3 id=2289--22316-chatel-entity-linking-with-chatbots-yifan-ding-et-al-2024>(22/89 | 22/316) ChatEL: Entity Linking with Chatbots (Yifan Ding et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yifan Ding, Qingkai Zeng, Tim Weninger. (2024)<br><strong>ChatEL: Entity Linking with Chatbots</strong><br><button class=copy-to-clipboard title="ChatEL: Entity Linking with Chatbots" index=22>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-22 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 60<br>Keywords: Fine-tuning, GPT, Chatbot, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14858v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14858v1.pdf filename=2402.14858v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Entity Linking (EL) is an essential and challenging task in natural language processing that seeks to link some text representing an entity within a document or sentence with its corresponding entry in a dictionary or knowledge base. Most existing approaches focus on creating elaborate contextual models that look for clues the words surrounding the entity-text to help solve the linking problem. Although these <b>fine-tuned</b> language models tend to work, they can be unwieldy, difficult to train, and do not transfer well to other domains. Fortunately, <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> like <b>GPT</b> provide a highly-advanced solution to the problems inherent in EL models, but simply naive <b>prompts</b> to <b>LLMs</b> do not work well. In the present work, we define ChatEL, which is a three-step framework to <b>prompt</b> <b>LLMs</b> to return accurate results. Overall the ChatEL framework improves the average F1 performance across 10 datasets by more than 2%. Finally, a thorough error analysis shows many instances with the ground truth labels were actually incorrect, and the labels predicted by ChatEL were actually correct. This indicates that the quantitative results presented in this paper may be a conservative estimate of the actual performance. All data and code are available as an open-source package on GitHub at <a href=https://github.com/yifding/In_Context_EL>https://github.com/yifding/In_Context_EL</a>.</p></p class="citation"></blockquote><h3 id=2389--23316-is-the-system-message-really-important-to-jailbreaks-in-large-language-models-xiaotian-zou-et-al-2024>(23/89 | 23/316) Is the System Message Really Important to Jailbreaks in Large Language Models? (Xiaotian Zou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiaotian Zou, Yongkang Chen, Ke Li. (2024)<br><strong>Is the System Message Really Important to Jailbreaks in Large Language Models?</strong><br><button class=copy-to-clipboard title="Is the System Message Really Important to Jailbreaks in Large Language Models?" index=23>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-23 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-CR, cs.CL<br>Keyword Score: 60<br>Keywords: GPT, GPT-3, GPT-3.5, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14857v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14857v1.pdf filename=2402.14857v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The rapid evolution of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> has rendered them indispensable in modern society. While security measures are typically in place to align <b>LLMs</b> with human values prior to release, recent studies have unveiled a concerning phenomenon named &ldquo;jailbreak.&rdquo; This term refers to the unexpected and potentially harmful responses generated by <b>LLMs</b> when <b>prompted</b> with malicious questions. Existing research focuses on generating jailbreak <b>prompts</b> but our study aim to answer a different question: Is the system message really important to jailbreak in LLMs? To address this question, we conducted experiments in a stable <b>GPT</b> version <b>gpt-3.5-turbo-0613</b> to generated jailbreak <b>prompts</b> with varying system messages: short, long, and none. We discover that different system messages have distinct resistances to jailbreak by experiments. Additionally, we explore the transferability of jailbreak across <b>LLMs.</b> This finding underscores the significant impact system messages can have on mitigating <b>LLMs</b> jailbreak. To generate system messages that are more resistant to jailbreak <b>prompts,</b> we propose System Messages Evolutionary Algorithms (SMEA). Through SMEA, we can get robust system messages population that demonstrate up to 98.9% resistance against jailbreak <b>prompts.</b> Our research not only bolsters <b>LLMs</b> security but also raises the bar for jailbreak, fostering advancements in this field of study.</p></p class="citation"></blockquote><h3 id=2489--24316-elad-explanation-guided-large-language-models-active-distillation-yifei-zhang-et-al-2024>(24/89 | 24/316) ELAD: Explanation-Guided Large Language Models Active Distillation (Yifei Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yifei Zhang, Bo Pan, Chen Ling, Yuntong Hu, Liang Zhao. (2024)<br><strong>ELAD: Explanation-Guided Large Language Models Active Distillation</strong><br><button class=copy-to-clipboard title="ELAD: Explanation-Guided Large Language Models Active Distillation" index=24>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-24 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 60<br>Keywords: Active Learning, Knowledge Distillation, Knowledge Distillation, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13098v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13098v1.pdf filename=2402.13098v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The deployment and application of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> is hindered by their memory inefficiency, computational demands, and the high costs of API inferences. Traditional <b>distillation</b> methods, which transfer the capabilities of <b>LLMs</b> to smaller models, often fail to determine whether the <b>knowledge</b> <b>has</b> been sufficiently transferred, potentially resulting in high costs or incomplete <b>distillation.</b> In this paper, we propose an Explanation-Guided <b>LLMs</b> <b>Active</b> <b>Distillation</b> (ELAD) framework that employs an <b>active</b> <b>learning</b> strategy to optimize the balance between annotation costs and model performance. To improve efficient sample selection, we introduce an explanation-guided sample selection method that identifies samples challenging its <b>reasoning</b> by exploiting uncertainties in explanation steps. Additionally, we present a customized <b>LLM-annotated</b> explanation revision technique where the teacher model detects and corrects flaws in the student model&rsquo;s <b>reasoning.</b> Our experiments across various <b>reasoning</b> datasets demonstrate that our framework significantly enhances the efficiency of <b>LLM</b> <b>knowledge</b> <b>distillation.</b></p></p class="citation"></blockquote><h3 id=2589--25316-learning-to-check-unleashing-potentials-for-self-correction-in-large-language-models-che-zhang-et-al-2024>(25/89 | 25/316) Learning to Check: Unleashing Potentials for Self-Correction in Large Language Models (Che Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Che Zhang, Zhenyang Xiao, Chengcheng Han, Yixin Lian, Yuejian Fang. (2024)<br><strong>Learning to Check: Unleashing Potentials for Self-Correction in Large Language Models</strong><br><button class=copy-to-clipboard title="Learning to Check: Unleashing Potentials for Self-Correction in Large Language Models" index=25>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-25 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 60<br>Keywords: Fine-tuning, Mathematical Reasoning, Reasoning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13035v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13035v2.pdf filename=2402.13035v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have made significant strides in <b>reasoning</b> capabilities, with ongoing efforts to refine their <b>reasoning</b> through self-correction. However, recent studies suggest that self-correction can be limited or even counterproductive without external accurate knowledge, raising questions about the limits and effectiveness of self-correction. In this paper, we aim to enhance <b>LLM&rsquo;s</b> self-checking capabilities by meticulously designing training data, thereby improving the accuracy of self-correction. We conduct a detailed analysis of error types in <b>mathematical</b> <b>reasoning</b> and develop a tailored <b>prompt,</b> termed &ldquo;Step CoT Check&rdquo;. Then we construct a checking-correction dataset for training models. After integrating the original CoT data and checking-correction data for training, we observe that models could improve their self-checking capabilities, thereby enhancing their self-correction capacity and eliminating the need for external feedback or ground truth labels to ascertain the endpoint of correction. We compare the performance of models <b>fine-tuned</b> with the &ldquo;Step CoT Check&rdquo; <b>prompt</b> against those refined using other promps within the context of checking-correction data. The &ldquo;Step CoT Check&rdquo; outperforms the other two check formats in model with lager parameters, providing more precise feedback thus achieving a higher rate of correctness. For reproducibility, all the datasets and codes are provided in <a href=https://github.com/bammt/Learn-to-check>https://github.com/bammt/Learn-to-check</a>.</p></p class="citation"></blockquote><h3 id=2689--26316-fine-tuning-prompting-in-context-learning-and-instruction-tuning-how-many-labelled-samples-do-we-need-branislav-pecher-et-al-2024>(26/89 | 26/316) Fine-Tuning, Prompting, In-Context Learning and Instruction-Tuning: How Many Labelled Samples Do We Need? (Branislav Pecher et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Branislav Pecher, Ivan Srba, Maria Bielikova. (2024)<br><strong>Fine-Tuning, Prompting, In-Context Learning and Instruction-Tuning: How Many Labelled Samples Do We Need?</strong><br><button class=copy-to-clipboard title="Fine-Tuning, Prompting, In-Context Learning and Instruction-Tuning: How Many Labelled Samples Do We Need?" index=26>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-26 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 60<br>Keywords: Fine-tuning, In-context Learning, In-context Learning, Instruction Tuning, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12819v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12819v1.pdf filename=2402.12819v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>When solving a task with limited labelled data, researchers can either use a general <b>large</b> <b>language</b> <b>model</b> without further update, or use the few examples to tune a specialised smaller model. When enough labels are available, the specialised models outperform the general ones on many NLP tasks. In this work, we aim to investigate how many labelled samples are required for the specialised models to achieve this superior performance, while taking the results variance into consideration. Observing the behaviour of <b>prompting,</b> <b>in-context</b> <b>learning,</b> <b>fine-tuning</b> and <b>instruction-tuning,</b> <b>identifying</b> their break-even points when increasing number of labelled training samples across three tasks of varying complexity, we find that the specialised models often need only few samples ($100-1000$) to be on par or better than the general ones. At the same time, the amount of required labelled data strongly depends on the task complexity and results variance.</p></p class="citation"></blockquote><h3 id=2789--27316-on-sensitivity-of-learning-with-limited-labelled-data-to-the-effects-of-randomness-impact-of-interactions-and-systematic-choices-branislav-pecher-et-al-2024>(27/89 | 27/316) On Sensitivity of Learning with Limited Labelled Data to the Effects of Randomness: Impact of Interactions and Systematic Choices (Branislav Pecher et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Branislav Pecher, Ivan Srba, Maria Bielikova. (2024)<br><strong>On Sensitivity of Learning with Limited Labelled Data to the Effects of Randomness: Impact of Interactions and Systematic Choices</strong><br><button class=copy-to-clipboard title="On Sensitivity of Learning with Limited Labelled Data to the Effects of Randomness: Impact of Interactions and Systematic Choices" index=27>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-27 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 60<br>Keywords: Fine-tuning, Meta Learning, Text Classification, In-context Learning, In-context Learning, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12817v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12817v1.pdf filename=2402.12817v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>While learning with limited labelled data can improve performance when the labels are lacking, it is also sensitive to the effects of uncontrolled randomness introduced by so-called randomness factors (e.g., varying order of data). We propose a method to systematically investigate the effects of randomness factors while taking the interactions between them into consideration. To measure the true effects of an individual randomness factor, our method mitigates the effects of other factors and observes how the performance varies across multiple runs. Applying our method to multiple randomness factors across <b>in-context</b> <b>learning</b> and <b>fine-tuning</b> approaches on 7 representative <b>text</b> <b>classification</b> tasks and <b>meta-learning</b> <b>on</b> 3 tasks, we show that: 1) disregarding interactions between randomness factors in existing works caused inconsistent findings due to incorrect attribution of the effects of randomness factors, such as disproving the consistent sensitivity of <b>in-context</b> <b>learning</b> to sample order even with random sample selection; and 2) besides mutual interactions, the effects of randomness factors, especially sample order, are also dependent on more systematic choices unexplored in existing works, such as number of classes, samples per class or choice of <b>prompt</b> format.</p></p class="citation"></blockquote><h3 id=2889--28316-formulaqa-a-question-answering-dataset-for-formula-based-numerical-reasoning-xiao-li-et-al-2024>(28/89 | 28/316) FormulaQA: A Question Answering Dataset for Formula-Based Numerical Reasoning (Xiao Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiao Li, Sichen Liu, Bolin Zhu, Yin Zhu, Yiwei Liu, Gong Cheng. (2024)<br><strong>FormulaQA: A Question Answering Dataset for Formula-Based Numerical Reasoning</strong><br><button class=copy-to-clipboard title="FormulaQA: A Question Answering Dataset for Formula-Based Numerical Reasoning" index=28>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-28 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 60<br>Keywords: Few-shot, Fine-tuning, Zero-shot, Question Answering, Reasoning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12692v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12692v2.pdf filename=2402.12692v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The application of formulas is a fundamental ability of humans when addressing numerical <b>reasoning</b> problems. However, existing numerical <b>reasoning</b> datasets seldom explicitly indicate the formulas employed during the <b>reasoning</b> steps. To bridge this gap, we propose a <b>question</b> <b>answering</b> dataset for formula-based numerical <b>reasoning</b> called FormulaQA, from junior high school physics examinations. We further conduct evaluations on <b>LLMs</b> with size ranging from 7B to over 100B parameters utilizing <b>zero-shot</b> and <b>few-shot</b> chain-of-thoughts methods and we explored the approach of using retrieval-augmented <b>LLMs</b> when providing an external formula database. We also <b>fine-tune</b> on smaller models with size not exceeding 2B. Our empirical findings underscore the significant potential for improvement in existing models when applied to our complex, formula-driven FormulaQA.</p></p class="citation"></blockquote><h3 id=2989--29316-chatatc-large-language-model-driven-conversational-agents-for-supporting-strategic-air-traffic-flow-management-sinan-abdulhak-et-al-2024>(29/89 | 29/316) CHATATC: Large Language Model-Driven Conversational Agents for Supporting Strategic Air Traffic Flow Management (Sinan Abdulhak et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sinan Abdulhak, Wayne Hubbard, Karthik Gopalakrishnan, Max Z. Li. (2024)<br><strong>CHATATC: Large Language Model-Driven Conversational Agents for Supporting Strategic Air Traffic Flow Management</strong><br><button class=copy-to-clipboard title="CHATATC: Large Language Model-Driven Conversational Agents for Supporting Strategic Air Traffic Flow Management" index=29>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-29 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 60<br>Keywords: Generative AI, ChatGPT, Text Generation, Large Language Model, Large Language Model, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14850v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14850v1.pdf filename=2402.14850v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Generative</b> <b>artificial</b> intelligence (AI) and <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have gained rapid popularity through publicly available tools such as <b>ChatGPT.</b> The adoption of <b>LLMs</b> for personal and professional use is fueled by the natural interactions between human users and computer applications such as <b>ChatGPT,</b> along with powerful <b>summarization</b> and <b>text</b> <b>generation</b> capabilities. Given the widespread use of such <b>generative</b> <b>AI</b> tools, in this work we investigate how these tools can be deployed in a non-safety critical, strategic traffic flow management setting. Specifically, we train an <b>LLM,</b> CHATATC, based on a <b>large</b> <b>historical</b> <b>data</b> set of Ground Delay Program (GDP) issuances, spanning 2000-2023 and consisting of over 80,000 GDP implementations, revisions, and cancellations. We test the query and response capabilities of CHATATC, documenting successes (e.g., providing correct GDP rates, durations, and reason) and shortcomings (e.g,. superlative questions). We also detail the design of a graphical user interface for future users to interact and collaborate with the CHATATC conversational agent.</p></p class="citation"></blockquote><h3 id=3089--30316-identifying-semantic-induction-heads-to-understand-in-context-learning-jie-ren-et-al-2024>(30/89 | 30/316) Identifying Semantic Induction Heads to Understand In-Context Learning (Jie Ren et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jie Ren, Qipeng Guo, Hang Yan, Dongrui Liu, Xipeng Qiu, Dahua Lin. (2024)<br><strong>Identifying Semantic Induction Heads to Understand In-Context Learning</strong><br><button class=copy-to-clipboard title="Identifying Semantic Induction Heads to Understand In-Context Learning" index=30>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-30 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 58<br>Keywords: Graph, Knowledge Graph, Transformer, In-context Learning, In-context Learning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13055v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13055v1.pdf filename=2402.13055v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Although <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have demonstrated remarkable performance, the lack of transparency in their inference logic raises concerns about their trustworthiness. To gain a better understanding of <b>LLMs,</b> we conduct a detailed analysis of the operations of attention heads and aim to better understand the <b>in-context</b> <b>learning</b> of <b>LLMs.</b> Specifically, we investigate whether attention heads encode two types of relationships between tokens present in natural languages: the syntactic dependency parsed from sentences and the relation within <b>knowledge</b> <b>graphs.</b> We find that certain attention heads exhibit a pattern where, when attending to head tokens, they recall tail tokens and increase the output logits of those tail tokens. More crucially, the formulation of such semantic induction heads has a close correlation with the emergence of the <b>in-context</b> <b>learning</b> ability of language models. The study of semantic attention heads advances our understanding of the intricate operations of attention heads in <b>transformers,</b> and further provides new insights into the <b>in-context</b> <b>learning</b> of <b>LLMs.</b></p></p class="citation"></blockquote><h3 id=3189--31316-structure-guided-prompt-instructing-large-language-model-in-multi-step-reasoning-by-exploring-graph-structure-of-the-text-kewei-cheng-et-al-2024>(31/89 | 31/316) Structure Guided Prompt: Instructing Large Language Model in Multi-Step Reasoning by Exploring Graph Structure of the Text (Kewei Cheng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kewei Cheng, Nesreen K. Ahmed, Theodore Willke, Yizhou Sun. (2024)<br><strong>Structure Guided Prompt: Instructing Large Language Model in Multi-Step Reasoning by Exploring Graph Structure of the Text</strong><br><button class=copy-to-clipboard title="Structure Guided Prompt: Instructing Large Language Model in Multi-Step Reasoning by Exploring Graph Structure of the Text" index=31>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-31 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 53<br>Keywords: Graph, Zero-shot, Reasoning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13415v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13415v1.pdf filename=2402.13415v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Although <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> excel at addressing straightforward <b>reasoning</b> tasks, they frequently struggle with difficulties when confronted by more complex multi-step <b>reasoning</b> due to a range of factors. Firstly, natural language often encompasses complex relationships among entities, making it challenging to maintain a clear <b>reasoning</b> chain over longer spans. Secondly, the abundance of linguistic diversity means that the same entities and relationships can be expressed using different terminologies and structures, complicating the task of identifying and establishing connections between multiple pieces of information. <b>Graphs</b> provide an effective solution to represent data rich in relational information and capture long-term dependencies among entities. To harness the potential of <b>graphs,</b> our paper introduces Structure Guided <b>Prompt,</b> an innovative three-stage task-agnostic <b>prompting</b> framework designed to improve the multi-step <b>reasoning</b> capabilities of <b>LLMs</b> in a <b>zero-shot</b> setting. This framework explicitly converts unstructured text into a <b>graph</b> via <b>LLMs</b> and instructs them to navigate this <b>graph</b> using task-specific strategies to formulate responses. By effectively organizing information and guiding navigation, it enables <b>LLMs</b> to provide more accurate and context-aware responses. Our experiments show that this framework significantly enhances the <b>reasoning</b> capabilities of <b>LLMs,</b> enabling them to excel in a broader spectrum of natural language scenarios.</p></p class="citation"></blockquote><h3 id=3289--32316-pirb-a-comprehensive-benchmark-of-polish-dense-and-hybrid-text-retrieval-methods-sławomir-dadas-et-al-2024>(32/89 | 32/316) PIRB: A Comprehensive Benchmark of Polish Dense and Hybrid Text Retrieval Methods (Sławomir Dadas et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sławomir Dadas, Michał Perełkiewicz, Rafał Poświata. (2024)<br><strong>PIRB: A Comprehensive Benchmark of Polish Dense and Hybrid Text Retrieval Methods</strong><br><button class=copy-to-clipboard title="PIRB: A Comprehensive Benchmark of Polish Dense and Hybrid Text Retrieval Methods" index=32>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-32 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 53<br>Keywords: Benchmarking, Fine-tuning, Knowledge Distillation, Knowledge Distillation, Supervised Learning, Information Retrieval<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13350v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13350v1.pdf filename=2402.13350v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present Polish <b>Information</b> <b>Retrieval</b> <b>Benchmark</b> (PIRB), a comprehensive evaluation framework encompassing 41 text <b>information</b> <b>retrieval</b> tasks for Polish. The <b>benchmark</b> incorporates existing datasets as well as 10 new, previously unpublished datasets covering diverse topics such as medicine, law, business, physics, and linguistics. We conduct an extensive evaluation of over 20 dense and sparse retrieval models, including the baseline models trained by us as well as other available Polish and multilingual methods. Finally, we introduce a three-step process for training highly effective language-specific retrievers, consisting of <b>knowledge</b> <b>distillation,</b> <b>supervised</b> <b>fine-tuning,</b> and building sparse-dense hybrid retrievers using a lightweight rescoring model. In order to validate our approach, we train new text encoders for Polish and compare their results with previously evaluated methods. Our dense models outperform the best solutions available to date, and the use of hybrid methods further improves their performance.</p></p class="citation"></blockquote><h3 id=3389--33316-tofueval-evaluating-hallucinations-of-llms-on-topic-focused-dialogue-summarization-liyan-tang-et-al-2024>(33/89 | 33/316) TofuEval: Evaluating Hallucinations of LLMs on Topic-Focused Dialogue Summarization (Liyan Tang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Liyan Tang, Igor Shalyminov, Amy Wing-mei Wong, Jon Burnsky, Jake W. Vincent, Yu&rsquo;an Yang, Siffi Singh, Song Feng, Hwanjun Song, Hang Su, Lijia Sun, Yi Zhang, Saab Mansour, Kathleen McKeown. (2024)<br><strong>TofuEval: Evaluating Hallucinations of LLMs on Topic-Focused Dialogue Summarization</strong><br><button class=copy-to-clipboard title="TofuEval: Evaluating Hallucinations of LLMs on Topic-Focused Dialogue Summarization" index=33>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-33 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 53<br>Keywords: Benchmarking, GPT, GPT-4, Text Summarization, Large Language Model, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13249v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13249v1.pdf filename=2402.13249v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Single document news <b>summarization</b> has seen substantial progress on faithfulness in recent years, driven by research on the evaluation of factual consistency, or hallucinations. We ask whether these advances carry over to other <b>text</b> <b>summarization</b> domains. We propose a new evaluation <b>benchmark</b> on topic-focused dialogue <b>summarization,</b> generated by <b>LLMs</b> of varying sizes. We provide binary sentence-level human annotations of the factual consistency of these summaries along with detailed explanations of factually inconsistent sentences. Our analysis shows that existing <b>LLMs</b> hallucinate significant amounts of factual errors in the dialogue domain, regardless of the model&rsquo;s size. On the other hand, when <b>LLMs,</b> including <b>GPT-4,</b> serve as binary factual evaluators, they perform poorly and can be outperformed by prevailing state-of-the-art specialized factuality evaluation metrics. Finally, we conducted an analysis of hallucination types with a curated error taxonomy. We find that there are diverse errors and error distributions in model-generated summaries and that non-LLM based metrics can capture all error types better than <b>LLM-based</b> evaluators.</p></p class="citation"></blockquote><h3 id=3489--34316-agentmd-empowering-language-agents-for-risk-prediction-with-large-scale-clinical-tool-learning-qiao-jin-et-al-2024>(34/89 | 34/316) AgentMD: Empowering Language Agents for Risk Prediction with Large-Scale Clinical Tool Learning (Qiao Jin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qiao Jin, Zhizheng Wang, Yifan Yang, Qingqing Zhu, Donald Wright, Thomas Huang, W John Wilbur, Zhe He, Andrew Taylor, Qingyu Chen, Zhiyong Lu. (2024)<br><strong>AgentMD: Empowering Language Agents for Risk Prediction with Large-Scale Clinical Tool Learning</strong><br><button class=copy-to-clipboard title="AgentMD: Empowering Language Agents for Risk Prediction with Large-Scale Clinical Tool Learning" index=34>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-34 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 53<br>Keywords: Benchmarking, GPT, GPT-4, Chain-of-thought Prompt, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13225v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13225v1.pdf filename=2402.13225v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Clinical calculators play a vital role in healthcare by offering accurate evidence-based predictions for various purposes such as prognosis. Nevertheless, their widespread utilization is frequently hindered by usability challenges, poor dissemination, and restricted functionality. Augmenting <b>large</b> <b>language</b> <b>models</b> with extensive collections of clinical calculators presents an opportunity to overcome these obstacles and improve workflow efficiency, but the scalability of the manual curation process poses a significant challenge. In response, we introduce AgentMD, a novel language agent capable of curating and applying clinical calculators across various clinical contexts. Using the published literature, AgentMD has automatically curated a collection of 2,164 diverse clinical calculators with executable functions and structured documentation, collectively named RiskCalcs. Manual evaluations show that RiskCalcs tools achieve an accuracy of over 80% on three quality metrics. At inference time, AgentMD can automatically select and apply the relevant RiskCalcs tools given any patient description. On the newly established RiskQA <b>benchmark,</b> AgentMD significantly outperforms <b>chain-of-thought</b> <b>prompting</b> with <b>GPT-4</b> (87.7% vs. 40.9% in accuracy). Additionally, we also applied AgentMD to real-world clinical notes for analyzing both population-level and risk-level patient characteristics. In summary, our study illustrates the utility of language agents augmented with clinical calculators for healthcare analytics and patient care.</p></p class="citation"></blockquote><h3 id=3589--35316-rocode-a-dataset-for-measuring-code-intelligence-from-problem-definitions-in-romanian-adrian-cosma-et-al-2024>(35/89 | 35/316) RoCode: A Dataset for Measuring Code Intelligence from Problem Definitions in Romanian (Adrian Cosma et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Adrian Cosma, Bogdan Iordache, Paolo Rosso. (2024)<br><strong>RoCode: A Dataset for Measuring Code Intelligence from Problem Definitions in Romanian</strong><br><button class=copy-to-clipboard title="RoCode: A Dataset for Measuring Code Intelligence from Problem Definitions in Romanian" index=35>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-35 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 53<br>Keywords: Benchmarking, Fine-tuning, Code Generation, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13222v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13222v1.pdf filename=2402.13222v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recently, <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have become increasingly powerful and have become capable of solving a plethora of tasks through proper instructions in natural language. However, the vast majority of testing suites assume that the instructions are written in English, the de facto <b>prompting</b> language. <b>Code</b> <b>intelligence</b> and problem solving still remain a difficult task, even for the most advanced <b>LLMs.</b> Currently, there are no datasets to measure the generalization power for <b>code-generation</b> <b>models</b> in a language other than English. In this work, we present RoCode, a competitive programming dataset, consisting of 2,642 problems written in Romanian, 11k solutions in C, C++ and Python and comprehensive testing suites for each problem. The purpose of RoCode is to provide a <b>benchmark</b> for evaluating the <b>code</b> <b>intelligence</b> of language models trained on Romanian / multilingual text as well as a <b>fine-tuning</b> set for pretrained Romanian models. Through our results and review of related works, we argue for the need to develop <b>code</b> <b>models</b> for languages other than English.</p></p class="citation"></blockquote><h3 id=3689--36316-heterogeneous-graph-reasoning-for-fact-checking-over-texts-and-tables-haisong-gong-et-al-2024>(36/89 | 36/316) Heterogeneous Graph Reasoning for Fact Checking over Texts and Tables (Haisong Gong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haisong Gong, Weizhi Xu, Shu wu, Qiang Liu, Liang Wang. (2024)<br><strong>Heterogeneous Graph Reasoning for Fact Checking over Texts and Tables</strong><br><button class=copy-to-clipboard title="Heterogeneous Graph Reasoning for Fact Checking over Texts and Tables" index=36>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-36 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 53<br>Keywords: Graph, Graph Neural Network, Fine-tuning, Fact Verification, Reasoning, Pre-trained Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13028v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13028v1.pdf filename=2402.13028v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Fact</b> <b>checking</b> aims to predict claim veracity by <b>reasoning</b> over multiple evidence pieces. It usually involves evidence retrieval and veracity <b>reasoning.</b> In this paper, we focus on the latter, <b>reasoning</b> over unstructured text and structured table information. Previous works have primarily relied on <b>fine-tuning</b> <b>pretrained</b> <b>language</b> <b>models</b> or training homogeneous-graph-based models. Despite their effectiveness, we argue that they fail to explore the rich semantic information underlying the evidence with different structures. To address this, we propose a novel word-level Heterogeneous-graph-based model for <b>Fact</b> <b>Checking</b> over unstructured and structured information, namely HeterFC. Our approach leverages a heterogeneous evidence <b>graph,</b> <b>with</b> <b>words</b> as nodes and thoughtfully designed edges representing different evidence properties. We perform information propagation via a relational <b>graph</b> <b>neural</b> <b>network,</b> facilitating interactions between claims and evidence. An attention-based method is utilized to integrate information, combined with a language model for generating predictions. We introduce a multitask loss function to account for potential inaccuracies in evidence retrieval. Comprehensive experiments on the large <b>fact</b> <b>checking</b> dataset FEVEROUS demonstrate the effectiveness of HeterFC. Code will be released at: <a href=https://github.com/Deno-V/HeterFC>https://github.com/Deno-V/HeterFC</a>.</p></p class="citation"></blockquote><h3 id=3789--37316-enhancing-modern-supervised-word-sense-disambiguation-models-by-semantic-lexical-resources-stefano-melacci-et-al-2024>(37/89 | 37/316) Enhancing Modern Supervised Word Sense Disambiguation Models by Semantic Lexical Resources (Stefano Melacci et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Stefano Melacci, Achille Globo, Leonardo Rigutini. (2024)<br><strong>Enhancing Modern Supervised Word Sense Disambiguation Models by Semantic Lexical Resources</strong><br><button class=copy-to-clipboard title="Enhancing Modern Supervised Word Sense Disambiguation Models by Semantic Lexical Resources" index=37>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-37 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 53<br>Keywords: Benchmarking, Supervised Learning, Disambiguation, Recurrent Neural Network, Word Sense Disambiguation, Word Embedding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13302v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13302v1.pdf filename=2402.13302v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Supervised</b> models for <b>Word</b> <b>Sense</b> <b>Disambiguation</b> (WSD) currently yield to state-of-the-art results in the most popular <b>benchmarks.</b> Despite the recent introduction of <b>Word</b> <b>Embeddings</b> <b>and</b> <b>Recurrent</b> <b>Neural</b> <b>Networks</b> to design powerful context-related features, the interest in improving WSD models using Semantic Lexical Resources (SLRs) is mostly restricted to knowledge-based approaches. In this paper, we enhance &ldquo;modern&rdquo; <b>supervised</b> WSD models exploiting two popular SLRs: WordNet and WordNet Domains. We propose an effective way to introduce semantic features into the classifiers, and we consider using the SLR structure to augment the training data. We study the effect of different types of semantic features, investigating their interaction with local contexts encoded by means of mixtures of <b>Word</b> <b>Embeddings</b> <b>or</b> <b>Recurrent</b> <b>Neural</b> <b>Networks,</b> and we extend the proposed model into a novel multi-layer architecture for WSD. A detailed experimental comparison in the recent Unified Evaluation Framework (Raganato et al., 2017) shows that the proposed approach leads to <b>supervised</b> models that compare favourably with the state-of-the art.</p></p class="citation"></blockquote><h3 id=3889--38316-symba-symbolic-backward-chaining-for-multi-step-natural-language-reasoning-jinu-lee-et-al-2024>(38/89 | 38/316) SymBa: Symbolic Backward Chaining for Multi-step Natural Language Reasoning (Jinu Lee et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jinu Lee, Wonseok Hwang. (2024)<br><strong>SymBa: Symbolic Backward Chaining for Multi-step Natural Language Reasoning</strong><br><button class=copy-to-clipboard title="SymBa: Symbolic Backward Chaining for Multi-step Natural Language Reasoning" index=38>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-38 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 53<br>Keywords: Benchmarking, Reasoning, Chain-of-thought Prompt, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12806v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12806v1.pdf filename=2402.12806v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have recently demonstrated remarkable <b>reasoning</b> ability as in <b>Chain-of-thought</b> <b>prompting,</b> but faithful multi-step <b>reasoning</b> remains a challenge. We specifically focus on backward chaining, where the query is recursively decomposed using logical rules until proven. To address the limitations of current backward chaining implementations, we propose SymBa (Symbolic Backward Chaining). In SymBa, the symbolic top-down solver controls the entire proof process and the <b>LLM</b> is called to generate a single <b>reasoning</b> step only when the solver encounters a dead end. By this novel solver-LLM integration, while being able to produce an interpretable, structured proof, SymBa achieves significant improvement in performance, proof faithfulness, and efficiency in diverse multi-step <b>reasoning</b> <b>benchmarks</b> (ProofWriter, Birds-Electricity, GSM8k, CLUTRR-TF, ECtHR Article 6) compared to backward chaining baselines.</p></p class="citation"></blockquote><h3 id=3989--39316-sillm-large-language-models-for-simultaneous-machine-translation-shoutao-guo-et-al-2024>(39/89 | 39/316) SiLLM: Large Language Models for Simultaneous Machine Translation (Shoutao Guo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shoutao Guo, Shaolei Zhang, Zhengrui Ma, Min Zhang, Yang Feng. (2024)<br><strong>SiLLM: Large Language Models for Simultaneous Machine Translation</strong><br><button class=copy-to-clipboard title="SiLLM: Large Language Models for Simultaneous Machine Translation" index=39>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-39 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: Fine-tuning, Transformer, Neural Machine Translation, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13036v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13036v1.pdf filename=2402.13036v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Simultaneous <b>Machine</b> <b>Translation</b> (SiMT) generates translations while reading the source sentence, necessitating a policy to determine the optimal timing for reading and generating words. Despite the remarkable performance achieved by <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLM)</b> across various NLP tasks, existing SiMT methods predominantly focus on conventional <b>transformers,</b> employing a single model to concurrently determine the policy and generate the translations. However, given the complexity of SiMT, it is challenging to effectively address both tasks with a single model. Therefore, there is a need to decouple the SiMT task into policy-decision and translation sub-tasks. We propose SiLLM, which delegates the two sub-tasks to separate agents, thereby incorporating <b>LLM</b> into SiMT. The policy-decision agent is managed by a conventional SiMT model, responsible for determining the translation policy. The translation agent, leveraging the capabilities of <b>LLM,</b> generates translation using the partial source sentence. The two agents collaborate to accomplish SiMT. To facilitate the application of token-level policies determined by conventional SiMT models to <b>LLM,</b> we propose a word-level policy adapted for <b>LLM.</b> Experiments on two datasets demonstrate that, with a small amount of data for <b>fine-tuning</b> <b>LLM,</b> SiLLM attains state-of-the-art performance.</p></p class="citation"></blockquote><h3 id=4089--40316-large-language-model-based-human-agent-collaboration-for-complex-task-solving-xueyang-feng-et-al-2024>(40/89 | 40/316) Large Language Model-based Human-Agent Collaboration for Complex Task Solving (Xueyang Feng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xueyang Feng, Zhi-Yuan Chen, Yujia Qin, Yankai Lin, Xu Chen, Zhiyuan Liu, Ji-Rong Wen. (2024)<br><strong>Large Language Model-based Human-Agent Collaboration for Complex Task Solving</strong><br><button class=copy-to-clipboard title="Large Language Model-based Human-Agent Collaboration for Complex Task Solving" index=40>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-40 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-HC, cs.CL<br>Keyword Score: 50<br>Keywords: Human Intervention, Offline Reinforcement Learning, Reinforcement Learning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12914v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12914v1.pdf filename=2402.12914v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In recent developments within the research community, the integration of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> in creating fully autonomous agents has garnered significant interest. Despite this, <b>LLM-based</b> agents frequently demonstrate notable shortcomings in adjusting to dynamic environments and fully grasping <b>human</b> <b>needs.</b> In this work, we introduce the problem of <b>LLM-based</b> <b>human-agent</b> <b>collaboration</b> for complex task-solving, exploring their synergistic potential. In addition, we propose a <b>Reinforcement</b> <b>Learning-based</b> <b>Human-Agent</b> <b>Collaboration</b> method, ReHAC. This approach includes a policy model designed to determine the most opportune stages for <b>human</b> <b>intervention</b> within the task-solving process. We construct a <b>human-agent</b> <b>collaboration</b> dataset to train this policy model in an <b>offline</b> <b>reinforcement</b> <b>learning</b> environment. Our validation tests confirm the model&rsquo;s effectiveness. The results demonstrate that the synergistic efforts of <b>humans</b> <b>and</b> <b>LLM-based</b> agents significantly improve performance in complex tasks, primarily through well-planned, limited <b>human</b> <b>intervention.</b> Datasets and code are available at: <a href=https://github.com/XueyangFeng/ReHAC>https://github.com/XueyangFeng/ReHAC</a>.</p></p class="citation"></blockquote><h3 id=4189--41316-exploring-the-impact-of-table-to-text-methods-on-augmenting-llm-based-question-answering-with-domain-hybrid-data-dehai-min-et-al-2024>(41/89 | 41/316) Exploring the Impact of Table-to-Text Methods on Augmenting LLM-based Question Answering with Domain Hybrid Data (Dehai Min et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dehai Min, Nan Hu, Rihui Jin, Nuo Lin, Jiaoyan Chen, Yongrui Chen, Yu Li, Guilin Qi, Yun Li, Nijun Li, Qianren Wang. (2024)<br><strong>Exploring the Impact of Table-to-Text Methods on Augmenting LLM-based Question Answering with Domain Hybrid Data</strong><br><button class=copy-to-clipboard title="Exploring the Impact of Table-to-Text Methods on Augmenting LLM-based Question Answering with Domain Hybrid Data" index=41>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-41 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: Retrieval-Augmented Generation, Question Answering, Question Answering, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12869v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12869v1.pdf filename=2402.12869v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Augmenting <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> for <b>Question</b> <b>Answering</b> <b>(QA)</b> with domain specific data has attracted wide attention. However, domain data often exists in a hybrid format, including text and semi-structured tables, posing challenges for the seamless integration of information. Table-to-Text Generation is a promising solution by facilitating the transformation of hybrid data into a uniformly text-formatted corpus. Although this technique has been widely studied by the NLP community, there is currently no comparative analysis on how corpora generated by different table-to-text methods affect the performance of <b>QA</b> systems. In this paper, we address this research gap in two steps. First, we innovatively integrate table-to-text generation into the framework of enhancing <b>LLM-based</b> <b>QA</b> systems with domain hybrid data. Then, we utilize this framework in real-world industrial data to conduct extensive experiments on two types of <b>QA</b> systems (DSFT and <b>RAG</b> frameworks) with four representative methods: Markdown format, Template serialization, TPLM-based method, and <b>LLM-based</b> method. Based on the experimental results, we draw some empirical findings and explore the underlying reasons behind the success of some methods. We hope the findings of this work will provide a valuable reference for the academic and industrial communities in developing robust <b>QA</b> systems.</p></p class="citation"></blockquote><h3 id=4289--42316-instruction-tuned-language-models-are-better-knowledge-learners-zhengbao-jiang-et-al-2024>(42/89 | 42/316) Instruction-tuned Language Models are Better Knowledge Learners (Zhengbao Jiang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhengbao Jiang, Zhiqing Sun, Weijia Shi, Pedro Rodriguez, Chunting Zhou, Graham Neubig, Xi Victoria Lin, Wen-tau Yih, Srinivasan Iyer. (2024)<br><strong>Instruction-tuned Language Models are Better Knowledge Learners</strong><br><button class=copy-to-clipboard title="Instruction-tuned Language Models are Better Knowledge Learners" index=42>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-42 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 50<br>Keywords: Question Answering, Instruction Tuning, Large Language Model, Large Language Model, Perplexity<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12847v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12847v1.pdf filename=2402.12847v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In order for <b>large</b> <b>language</b> <b>model</b> <b>(LLM)-based</b> assistants to effectively adapt to evolving information needs, it must be possible to update their factual knowledge through continued training on new data. The standard recipe for doing so involves continued pre-training on new documents followed by <b>instruction-tuning</b> <b>on</b> question-answer <b>(QA)</b> pairs. However, we find that <b>LLMs</b> trained with this recipe struggle to answer questions, even though the <b>perplexity</b> of documents is minimized. We found that <b>QA</b> pairs are generally straightforward, while documents are more complex, weaving many factual statements together in an intricate manner. Therefore, we hypothesize that it is beneficial to expose <b>LLMs</b> to <b>QA</b> pairs before continued pre-training on documents so that the process of encoding knowledge from complex documents takes into account how this knowledge is accessed through questions. Based on this, we propose pre-instruction-tuning (PIT), a method that <b>instruction-tunes</b> <b>on</b> questions prior to training on documents. This contrasts with standard <b>instruction-tuning,</b> <b>which</b> learns how to extract knowledge after training on documents. Extensive experiments and ablation studies demonstrate that PIT significantly enhances the ability of <b>LLMs</b> to absorb knowledge from new documents, outperforming standard <b>instruction-tuning</b> <b>by</b> 17.8%.</p></p class="citation"></blockquote><h3 id=4389--43316-owsm-ctc-an-open-encoder-only-speech-foundation-model-for-speech-recognition-translation-and-language-identification-yifan-peng-et-al-2024>(43/89 | 43/316) OWSM-CTC: An Open Encoder-Only Speech Foundation Model for Speech Recognition, Translation, and Language Identification (Yifan Peng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yifan Peng, Yui Sudo, Muhammad Shakeel, Shinji Watanabe. (2024)<br><strong>OWSM-CTC: An Open Encoder-Only Speech Foundation Model for Speech Recognition, Translation, and Language Identification</strong><br><button class=copy-to-clipboard title="OWSM-CTC: An Open Encoder-Only Speech Foundation Model for Speech Recognition, Translation, and Language Identification" index=43>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-43 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-SD, cs.CL, eess-AS<br>Keyword Score: 50<br>Keywords: Foundation Model, Automatic Speech Recognition, Automatic Speech Recognition, Automatic Speech Recognition, Speech-to-Speech Translation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12654v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12654v1.pdf filename=2402.12654v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>There has been an increasing interest in large <b>speech</b> <b>models</b> that can perform multiple <b>speech</b> <b>processing</b> tasks in a single model. Such models usually adopt the encoder-decoder or decoder-only architecture due to their popularity and good performance in many domains. However, autoregressive models can be slower during inference compared to non-autoregressive models and also have potential risks of hallucination. Though prior studies observed promising results of non-autoregressive models for certain tasks at small scales, it remains unclear if they can be scaled to <b>speech-to-text</b> <b>generation</b> in diverse languages and tasks. Inspired by the Open Whisper-style <b>Speech</b> <b>Model</b> (OWSM) project, we propose OWSM-CTC, a novel encoder-only <b>speech</b> <b>foundation</b> <b>model</b> based on Connectionist Temporal Classification (CTC). It is trained on 180k hours of public audio data for multilingual <b>automatic</b> <b>speech</b> <b>recognition</b> <b>(ASR),</b> <b>speech</b> <b>translation</b> (ST), and language identification (LID). Compared to encoder-decoder OWSM, our OWSM-CTC achieves competitive results on <b>ASR</b> and up to 25% relative improvement on ST, while it is more robust and 3 to 4 times faster for inference. OWSM-CTC also improves the long-form <b>ASR</b> result with 20x speed-up. We will publicly release our codebase, pre-trained model, and training logs to promote open science in <b>speech</b> <b>foundation</b> <b>models.</b></p></p class="citation"></blockquote><h3 id=4489--44316-more-discriminative-sentence-embeddings-via-semantic-graph-smoothing-chakib-fettal-et-al-2024>(44/89 | 44/316) More Discriminative Sentence Embeddings via Semantic Graph Smoothing (Chakib Fettal et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chakib Fettal, Lazhar Labiod, Mohamed Nadif. (2024)<br><strong>More Discriminative Sentence Embeddings via Semantic Graph Smoothing</strong><br><button class=copy-to-clipboard title="More Discriminative Sentence Embeddings via Semantic Graph Smoothing" index=44>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-44 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keyword Score: 49<br>Keywords: Graph, Benchmarking, Clustering, Supervised Learning, Unsupervised Learning, Sentence Embedding, Text Clustering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12890v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12890v1.pdf filename=2402.12890v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper explores an empirical approach to learn more discriminantive <b>sentence</b> <b>representations</b> in an <b>unsupervised</b> fashion. Leveraging semantic <b>graph</b> smoothing, we enhance <b>sentence</b> <b>embeddings</b> obtained from pretrained models to improve results for the <b>text</b> <b>clustering</b> and classification tasks. Our method, validated on eight <b>benchmarks,</b> demonstrates consistent improvements, showcasing the potential of semantic <b>graph</b> smoothing in improving <b>sentence</b> <b>embeddings</b> for the <b>supervised</b> and <b>unsupervised</b> document categorization tasks.</p></p class="citation"></blockquote><h3 id=4589--45316-somelvlm-a-large-vision-language-model-for-social-media-processing-xinnong-zhang-et-al-2024>(45/89 | 45/316) SoMeLVLM: A Large Vision Language Model for Social Media Processing (Xinnong Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xinnong Zhang, Haoyu Kuang, Xinyi Mou, Hanjia Lyu, Kun Wu, Siming Chen, Jiebo Luo, Xuanjing Huang, Zhongyu Wei. (2024)<br><strong>SoMeLVLM: A Large Vision Language Model for Social Media Processing</strong><br><button class=copy-to-clipboard title="SoMeLVLM: A Large Vision Language Model for Social Media Processing" index=45>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-45 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-MM, cs.CL<br>Keyword Score: 46<br>Keywords: Fine-tuning, Multi-modal, Multi-modal, Instruction Tuning, Prompt, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13022v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13022v1.pdf filename=2402.13022v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The growth of social media, characterized by its <b>multimodal</b> nature, has led to the emergence of diverse phenomena and challenges, which calls for an effective approach to uniformly solve automated tasks. The powerful Large Vision Language Models make it possible to handle a variety of tasks simultaneously, but even with carefully designed <b>prompting</b> methods, the general domain models often fall short in aligning with the unique speaking style and context of social media tasks. In this paper, we introduce a Large Vision Language Model for Social Media Processing (SoMeLVLM), which is a cognitive framework equipped with five key capabilities including knowledge & comprehension, application, analysis, evaluation, and creation. SoMeLVLM is designed to understand and generate realistic social media behavior. We have developed a 654k <b>multimodal</b> social media <b>instruction-tuning</b> <b>dataset</b> to support our cognitive framework and <b>fine-tune</b> our model. Our experiments demonstrate that SoMeLVLM achieves state-of-the-art performance in multiple social media tasks. Further analysis shows its significant advantages over baselines in terms of cognitive abilities.</p></p class="citation"></blockquote><h3 id=4689--46316-glória----a-generative-and-open-large-language-model-for-portuguese-ricardo-lopes-et-al-2024>(46/89 | 46/316) GlórIA &ndash; A Generative and Open Large Language Model for Portuguese (Ricardo Lopes et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ricardo Lopes, João Magalhães, David Semedo. (2024)<br><strong>GlórIA &ndash; A Generative and Open Large Language Model for Portuguese</strong><br><button class=copy-to-clipboard title="GlórIA -- A Generative and Open Large Language Model for Portuguese" index=46>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-46 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 43<br>Keywords: Benchmarking, High-Resource, Zero-shot, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12969v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12969v1.pdf filename=2402.12969v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Significant strides have been made in natural language tasks, largely attributed to the emergence of powerful <b>large</b> <b>language</b> <b>models</b> <b>(LLMs).</b> These models, pre-trained on extensive and diverse corpora, have become increasingly capable of comprehending the intricacies of language. Despite the abundance of <b>LLMs</b> for many <b>high-resource</b> languages, the availability of such models remains limited for European Portuguese. We introduce Gl'orIA, a robust European Portuguese decoder <b>LLM.</b> To pre-train Gl'orIA, we assembled a comprehensive PT-PT text corpus comprising 35 billion tokens from various sources. We present our pre-training methodology, followed by an assessment of the model&rsquo;s effectiveness on multiple downstream tasks. Additionally, to evaluate our models&rsquo; language modeling capabilities, we introduce CALAME-PT (Context-Aware LAnguage Modeling Evaluation for Portuguese), the first Portuguese <b>zero-shot</b> language-modeling <b>benchmark.</b> Evaluation shows that Gl'orIA significantly outperforms existing open PT decoder models in language modeling and that it can generate sound, knowledge-rich, and coherent PT-PT text. The model also exhibits strong potential for various downstream tasks.</p></p class="citation"></blockquote><h3 id=4789--47316-arabicmmlu-assessing-massive-multitask-language-understanding-in-arabic-fajri-koto-et-al-2024>(47/89 | 47/316) ArabicMMLU: Assessing Massive Multitask Language Understanding in Arabic (Fajri Koto et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fajri Koto, Haonan Li, Sara Shatnawi, Jad Doughman, Abdelrahman Boda Sadallah, Aisha Alraeesi, Khalid Almubarak, Zaid Alyafeai, Neha Sengupta, Shady Shehata, Nizar Habash, Preslav Nakov, Timothy Baldwin. (2024)<br><strong>ArabicMMLU: Assessing Massive Multitask Language Understanding in Arabic</strong><br><button class=copy-to-clipboard title="ArabicMMLU: Assessing Massive Multitask Language Understanding in Arabic" index=47>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-47 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 43<br>Keywords: Benchmarking, falcon, Massive Multitask Language Understanding (MMLU), Massive Multitask Language Understanding (MMLU), Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12840v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12840v1.pdf filename=2402.12840v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The focus of language model evaluation has transitioned towards <b>reasoning</b> and knowledge-intensive tasks, driven by advancements in pretraining large models. While state-of-the-art models are partially trained on large Arabic texts, evaluating their performance in Arabic remains challenging due to the limited availability of relevant datasets. To bridge this gap, we present ArabicMMLU, the first multi-task language understanding <b>benchmark</b> for Arabic language, sourced from school exams across diverse educational levels in different countries spanning North Africa, the Levant, and the Gulf regions. Our data comprises 40 tasks and 14,575 multiple-choice questions in Modern Standard Arabic (MSA), and is carefully constructed by collaborating with native speakers in the region. Our comprehensive evaluations of 35 models reveal substantial room for improvement, particularly among the best open-source models. Notably, BLOOMZ, mT0, LLama2, and <b>Falcon</b> struggle to achieve a score of 50%, while even the top-performing Arabic-centric model only achieves a score of 62.3%.</p></p class="citation"></blockquote><h3 id=4889--48316-tree-planted-transformers-large-language-models-with-implicit-syntactic-supervision-ryo-yoshida-et-al-2024>(48/89 | 48/316) Tree-Planted Transformers: Large Language Models with Implicit Syntactic Supervision (Ryo Yoshida et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ryo Yoshida, Taiga Someya, Yohei Oseki. (2024)<br><strong>Tree-Planted Transformers: Large Language Models with Implicit Syntactic Supervision</strong><br><button class=copy-to-clipboard title="Tree-Planted Transformers: Large Language Models with Implicit Syntactic Supervision" index=48>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-48 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 43<br>Keywords: Benchmarking, Continual Learning, Transformer, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12691v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12691v1.pdf filename=2402.12691v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have achieved remarkable success thanks to scalability on <b>large</b> <b>text</b> <b>corpora,</b> but have some drawback in training efficiency. In contrast, Syntactic Language Models (SLMs) can be trained efficiently to reach relatively high performance thanks to syntactic supervision, but have trouble with scalability. Thus, given these complementary advantages of <b>LLMs</b> and SLMs, it is necessary to develop an architecture that integrates the scalability of <b>LLMs</b> with the training efficiency of SLMs, namely Syntactic <b>Large</b> <b>Language</b> <b>Models</b> (SLLM). In this paper, we propose a novel method dubbed tree-planting: implicitly &ldquo;plant&rdquo; trees into attention weights of <b>Transformer</b> LMs to reflect syntactic structures of natural language. Specifically, <b>Transformer</b> LMs trained with tree-planting will be called Tree-Planted <b>Transformers</b> (TPT), which learn syntax on small treebanks via tree-planting and then scale on <b>large</b> <b>text</b> <b>corpora</b> via <b>continual</b> <b>learning</b> with syntactic scaffolding. Targeted syntactic evaluations on the SyntaxGym <b>benchmark</b> demonstrated that TPTs, despite the lack of explicit syntactic supervision, significantly outperformed various SLMs with explicit syntactic supervision that generate hundreds of syntactic structures in parallel, suggesting that tree-planting and TPTs are the promising foundation for SLLMs.</p></p class="citation"></blockquote><h3 id=4989--49316-healthcare-copilot-eliciting-the-power-of-general-llms-for-medical-consultation-zhiyao-ren-et-al-2024>(49/89 | 49/316) Healthcare Copilot: Eliciting the Power of General LLMs for Medical Consultation (Zhiyao Ren et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhiyao Ren, Yibing Zhan, Baosheng Yu, Liang Ding, Dacheng Tao. (2024)<br><strong>Healthcare Copilot: Eliciting the Power of General LLMs for Medical Consultation</strong><br><button class=copy-to-clipboard title="Healthcare Copilot: Eliciting the Power of General LLMs for Medical Consultation" index=49>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-49 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Fine-tuning, ChatGPT, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13408v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13408v1.pdf filename=2402.13408v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The copilot framework, which aims to enhance and tailor <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> for specific complex tasks without requiring <b>fine-tuning,</b> is gaining increasing attention from the community. In this paper, we introduce the construction of a Healthcare Copilot designed for medical consultation. The proposed Healthcare Copilot comprises three main components: 1) the Dialogue component, responsible for effective and safe patient interactions; 2) the Memory component, storing both current conversation data and historical patient information; and 3) the Processing component, summarizing the entire dialogue and generating reports. To evaluate the proposed Healthcare Copilot, we implement an auto-evaluation scheme using <b>ChatGPT</b> for two roles: as a virtual patient engaging in dialogue with the copilot, and as an evaluator to assess the quality of the dialogue. Extensive results demonstrate that the proposed Healthcare Copilot significantly enhances the capabilities of general <b>LLMs</b> for medical consultations in terms of inquiry capability, conversational fluency, response accuracy, and safety. Furthermore, we conduct ablation studies to highlight the contribution of each individual module in the Healthcare Copilot. Code will be made publicly available on GitHub.</p></p class="citation"></blockquote><h3 id=5089--50316-investigating-cultural-alignment-of-large-language-models-badr-alkhamissi-et-al-2024>(50/89 | 50/316) Investigating Cultural Alignment of Large Language Models (Badr AlKhamissi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Badr AlKhamissi, Muhammad ElNokrashy, Mai AlKhamissi, Mona Diab. (2024)<br><strong>Investigating Cultural Alignment of Large Language Models</strong><br><button class=copy-to-clipboard title="Investigating Cultural Alignment of Large Language Models" index=50>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-50 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-CY, cs.CL<br>Keyword Score: 40<br>Keywords: Reasoning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13231v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13231v1.pdf filename=2402.13231v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The intricate relationship between language and culture has long been a subject of exploration within the realm of linguistic anthropology. <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs),</b> promoted as repositories of collective human knowledge, raise a pivotal question: do these models genuinely encapsulate the diverse knowledge adopted by different cultures? Our study reveals that these models demonstrate greater cultural alignment along two dimensions &ndash; firstly, when <b>prompted</b> with the dominant language of a specific culture, and secondly, when pretrained with a refined mixture of languages employed by that culture. We quantify cultural alignment by simulating sociological surveys, comparing model responses to those of actual survey participants as references. Specifically, we replicate a survey conducted in various regions of Egypt and the United States through <b>prompting</b> <b>LLMs</b> with different pretraining data mixtures in both Arabic and English with the personas of the real respondents and the survey questions. Further analysis reveals that misalignment becomes more pronounced for underrepresented personas and for culturally sensitive topics, such as those probing social values. Finally, we introduce Anthropological <b>Prompting,</b> a novel method leveraging anthropological <b>reasoning</b> to enhance cultural alignment. Our study emphasizes the necessity for a more balanced multilingual pretraining dataset to better represent the diversity of human experience and the plurality of different cultures with many implications on the topic of cross-lingual transfer.</p></p class="citation"></blockquote><h3 id=5189--51316-smaug-fixing-failure-modes-of-preference-optimisation-with-dpo-positive-arka-pal-et-al-2024>(51/89 | 51/316) Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive (Arka Pal et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Arka Pal, Deep Karkhanis, Samuel Dooley, Manley Roberts, Siddartha Naidu, Colin White. (2024)<br><strong>Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive</strong><br><button class=copy-to-clipboard title="Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive" index=51>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-51 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 40<br>Keywords: Fine-tuning, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13228v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13228v1.pdf filename=2402.13228v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Direct Preference Optimisation (DPO) is effective at significantly improving the performance of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> on downstream tasks such as <b>reasoning,</b> summarisation, and alignment. Using pairs of preferred and dispreferred data, DPO models the \textit{relative} probability of picking one response over another. In this work, first we show theoretically that the standard DPO loss can lead to a \textit{reduction} of the model&rsquo;s likelihood of the preferred examples, as long as the relative probability between the preferred and dispreferred classes increases. We then show empirically that this phenomenon occurs when <b>fine-tuning</b> <b>LLMs</b> on common datasets, especially datasets in which the edit distance between pairs of completions is low. Using these insights, we design DPO-Positive (DPOP), a new loss function and training procedure which avoids this failure mode. Surprisingly, we also find that DPOP significantly outperforms DPO across a wide variety of datasets and downstream tasks, including datasets with high edit distances between completions. By <b>fine-tuning</b> with DPOP, we create and release Smaug-34B and Smaug-72B, which achieve state-of-the-art open-source performance. Notably, Smaug-72B is nearly 2% better than any other open-source model on the HuggingFace Open <b>LLM</b> Leaderboard and becomes the first open-source <b>LLM</b> to surpass an average accuracy of 80%.</p></p class="citation"></blockquote><h3 id=5289--52316-understanding-the-effects-of-language-specific-class-imbalance-in-multilingual-fine-tuning-vincent-jung-et-al-2024>(52/89 | 52/316) Understanding the effects of language-specific class imbalance in multilingual fine-tuning (Vincent Jung et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Vincent Jung, Lonneke van der Plas. (2024)<br><strong>Understanding the effects of language-specific class imbalance in multilingual fine-tuning</strong><br><button class=copy-to-clipboard title="Understanding the effects of language-specific class imbalance in multilingual fine-tuning" index=52>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-52 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Fine-tuning, Transformer, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13016v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13016v1.pdf filename=2402.13016v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We study the effect of one type of imbalance often present in real-life multilingual classification datasets: an uneven distribution of labels across languages. We show evidence that <b>fine-tuning</b> a <b>transformer-based</b> <b>Large</b> <b>Language</b> <b>Model</b> <b>(LLM)</b> on a dataset with this imbalance leads to worse performance, a more pronounced separation of languages in the latent space, and the promotion of uninformative features. We modify the traditional class weighing approach to imbalance by calculating class weights separately for each language and show that this helps mitigate those detrimental effects. These results create awareness of the negative effects of language-specific class imbalance in multilingual <b>fine-tuning</b> and the way in which the model learns to rely on the separation of languages to perform the task.</p></p class="citation"></blockquote><h3 id=5389--53316-gumbelsoft-diversified-language-model-watermarking-via-the-gumbelmax-trick-jiayi-fu-et-al-2024>(53/89 | 53/316) GumbelSoft: Diversified Language Model Watermarking via the GumbelMax-trick (Jiayi Fu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiayi Fu, Xuandong Zhao, Ruihan Yang, Yuansen Zhang, Jiangjie Chen, Yanghua Xiao. (2024)<br><strong>GumbelSoft: Diversified Language Model Watermarking via the GumbelMax-trick</strong><br><button class=copy-to-clipboard title="GumbelSoft: Diversified Language Model Watermarking via the GumbelMax-trick" index=53>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-53 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Fake News Detection, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12948v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12948v1.pdf filename=2402.12948v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> excellently generate human-like text, but also raise concerns about misuse in <b>fake</b> <b>news</b> and academic dishonesty. Decoding-based watermark, particularly the GumbelMax-trick-based watermark(GM watermark), is a standout solution for safeguarding machine-generated texts due to its notable detectability. However, GM watermark encounters a major challenge with generation diversity, always yielding identical outputs for the same <b>prompt,</b> negatively impacting generation diversity and user experience. To overcome this limitation, we propose a new type of GM watermark, the Logits-Addition watermark, and its three variants, specifically designed to enhance diversity. Among these, the GumbelSoft watermark (a softmax variant of the Logits-Addition watermark) demonstrates superior performance in high diversity settings, with its AUROC score outperforming those of the two alternative variants by 0.1 to 0.3 and surpassing other decoding-based watermarking methods by a minimum of 0.1.</p></p class="citation"></blockquote><h3 id=5489--54316-panda-preference-adaptation-for-enhancing-domain-specific-abilities-of-llms-an-liu-et-al-2024>(54/89 | 54/316) PANDA: Preference Adaptation for Enhancing Domain-Specific Abilities of LLMs (An Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>An Liu, Zonghan Yang, Zhenhe Zhang, Qingyuan Hu, Peng Li, Ming Yan, Ji Zhang, Fei Huang, Yang Liu. (2024)<br><strong>PANDA: Preference Adaptation for Enhancing Domain-Specific Abilities of LLMs</strong><br><button class=copy-to-clipboard title="PANDA: Preference Adaptation for Enhancing Domain-Specific Abilities of LLMs" index=54>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-54 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Fine-tuning, Text Classification, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12835v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12835v1.pdf filename=2402.12835v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>While <b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have demonstrated considerable capabilities across various natural language tasks, they often fall short of the performance achieved by domain-specific state-of-the-art models. One potential approach to enhance domain-specific capabilities of <b>LLMs</b> involves <b>fine-tuning</b> them using corresponding datasets. However, this method can be both resource and time-intensive, and not applicable to closed-source commercial <b>LLMs.</b> In this paper, we propose Preference Adaptation for Enhancing Domain-specific Abilities of <b>LLMs</b> (PANDA), a method designed to augment the domain-specific capabilities of <b>LLMs</b> by leveraging insights from the response preference of expert models without requiring <b>fine-tuning.</b> Our experimental results reveal that PANDA significantly enhances the domain-specific ability of <b>LLMs</b> on <b>text</b> <b>classification</b> and interactive decision tasks. Moreover, <b>LLM</b> with PANDA even outperforms the expert model that being learned on 4 tasks of ScienceWorld. This finding highlights the potential of exploring tuning-free approaches to achieve weak-to-strong generalization.</p></p class="citation"></blockquote><h3 id=5589--55316-identifying-factual-inconsistency-in-summaries-towards-effective-utilization-of-large-language-model-liyan-xu-et-al-2024>(55/89 | 55/316) Identifying Factual Inconsistency in Summaries: Towards Effective Utilization of Large Language Model (Liyan Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Liyan Xu, Zhenlin Su, Mo Yu, Jin Xu, Jinho D. Choi, Jie Zhou, Fei Liu. (2024)<br><strong>Identifying Factual Inconsistency in Summaries: Towards Effective Utilization of Large Language Model</strong><br><button class=copy-to-clipboard title="Identifying Factual Inconsistency in Summaries: Towards Effective Utilization of Large Language Model" index=55>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-55 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keyword Score: 40<br>Keywords: Knowledge Distillation, Zero-shot, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12821v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12821v1.pdf filename=2402.12821v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Factual inconsistency poses a significant hurdle for the commercial deployment of abstractive summarizers. Under this <b>Large</b> <b>Language</b> <b>Model</b> <b>(LLM)</b> era, this work focuses around two important questions: what is the best way to leverage <b>LLM</b> for factual inconsistency detection, and how could we <b>distill</b> a smaller <b>LLM</b> with both high efficiency and efficacy? Three <b>zero-shot</b> paradigms are firstly proposed and evaluated across five diverse datasets: direct inference on the entire summary or each summary window; entity verification through question generation and answering. Experiments suggest that <b>LLM</b> itself is capable to resolve this task train-free under the proper paradigm design, surpassing strong trained baselines by 2.8% on average. To further promote practical utility, we then propose training strategies aimed at <b>distilling</b> smaller open-source <b>LLM</b> that learns to score the entire summary at once with high accuracy, which outperforms the <b>zero-shot</b> approaches by much larger <b>LLM,</b> serving as an effective and efficient ready-to-use scorer.</p></p class="citation"></blockquote><h3 id=5689--56316-can-large-language-models-be-used-to-provide-psychological-counselling-an-analysis-of-gpt-4-generated-responses-using-role-play-dialogues-michimasa-inaba-et-al-2024>(56/89 | 56/316) Can Large Language Models be Used to Provide Psychological Counselling? An Analysis of GPT-4-Generated Responses Using Role-play Dialogues (Michimasa Inaba et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Michimasa Inaba, Mariko Ukiyo, Keiko Takamizo. (2024)<br><strong>Can Large Language Models be Used to Provide Psychological Counselling? An Analysis of GPT-4-Generated Responses Using Role-play Dialogues</strong><br><button class=copy-to-clipboard title="Can Large Language Models be Used to Provide Psychological Counselling? An Analysis of GPT-4-Generated Responses Using Role-play Dialogues" index=56>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-56 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-HC, cs.CL<br>Keyword Score: 40<br>Keywords: GPT, GPT-4, Dialogue System, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12738v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12738v1.pdf filename=2402.12738v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Mental health care poses an increasingly serious challenge to modern societies. In this context, there has been a surge in research that utilizes information technologies to address mental health problems, including those aiming to develop counseling <b>dialogue</b> <b>systems.</b> However, there is a need for more evaluations of the performance of counseling <b>dialogue</b> <b>systems</b> that use <b>large</b> <b>language</b> <b>models.</b> For this study, we collected counseling <b>dialogue</b> <b>data</b> via role-playing scenarios involving expert counselors, and the utterances were annotated with the intentions of the counselors. To determine the feasibility of a <b>dialogue</b> <b>system</b> in real-world counseling scenarios, third-party counselors evaluated the appropriateness of responses from human counselors and those generated by <b>GPT-4</b> in identical contexts in role-play <b>dialogue</b> <b>data.</b> Analysis of the evaluation results showed that the responses generated by <b>GPT-4</b> were competitive with those of human counselors.</p></p class="citation"></blockquote><h3 id=5789--57316-sql-craft-text-to-sql-through-interactive-refinement-and-enhanced-reasoning-hanchen-xia-et-al-2024>(57/89 | 57/316) SQL-CRAFT: Text-to-SQL through Interactive Refinement and Enhanced Reasoning (Hanchen Xia et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hanchen Xia, Feng Jiang, Naihao Deng, Cunxiang Wang, Guojiang Zhao, Rada Mihalcea, Yue Zhang. (2024)<br><strong>SQL-CRAFT: Text-to-SQL through Interactive Refinement and Enhanced Reasoning</strong><br><button class=copy-to-clipboard title="SQL-CRAFT: Text-to-SQL through Interactive Refinement and Enhanced Reasoning" index=57>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-57 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-DB, cs.CL<br>Keyword Score: 40<br>Keywords: Reasoning, Text2SQL, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14851v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14851v1.pdf filename=2402.14851v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Modern <b>LLMs</b> have become increasingly powerful, but they are still facing challenges in specialized tasks such as <b>Text-to-SQL.</b> We propose SQL-CRAFT, a framework to advance <b>LLMs&rsquo;</b> SQL generation Capabilities through inteRActive reFinemenT and enhanced <b>reasoning.</b> We leverage an Interactive Correction Loop (IC-Loop) for <b>LLMs</b> to interact with databases automatically, as well as Python-enhanced <b>reasoning.</b> We conduct experiments on two <b>Text-to-SQL</b> datasets, Spider and Bird, with performance improvements of up to 5.7% compared to the naive <b>prompting</b> method. Moreover, our method surpasses the current state-of-the-art on the Spider Leaderboard, demonstrating the effectiveness of our framework.</p></p class="citation"></blockquote><h3 id=5889--58316-softqe-learned-representations-of-queries-expanded-by-llms-varad-pimpalkhute-et-al-2024>(58/89 | 58/316) SoftQE: Learned Representations of Queries Expanded by LLMs (Varad Pimpalkhute et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Varad Pimpalkhute, John Heyer, Xusen Yin, Sameer Gupta. (2024)<br><strong>SoftQE: Learned Representations of Queries Expanded by LLMs</strong><br><button class=copy-to-clipboard title="SoftQE: Learned Representations of Queries Expanded by LLMs" index=58>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-58 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-IR, cs-LG, cs.CL<br>Keyword Score: 40<br>Keywords: Dense Retrieval, Out-of-domain, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12663v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12663v1.pdf filename=2402.12663v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We investigate the integration of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> into query encoders to improve <b>dense</b> <b>retrieval</b> without increasing latency and cost, by circumventing the dependency on <b>LLMs</b> at inference time. SoftQE incorporates knowledge from <b>LLMs</b> by mapping embeddings of input queries to those of the <b>LLM-expanded</b> queries. While improvements over various strong baselines on in-domain MS-MARCO metrics are marginal, SoftQE improves performance by 2.83 absolute percentage points on average on five <b>out-of-domain</b> BEIR tasks.</p></p class="citation"></blockquote><h3 id=5989--59316-a-unified-taxonomy-guided-instruction-tuning-framework-for-entity-set-expansion-and-taxonomy-expansion-yanzhen-shen-et-al-2024>(59/89 | 59/316) A Unified Taxonomy-Guided Instruction Tuning Framework for Entity Set Expansion and Taxonomy Expansion (Yanzhen Shen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yanzhen Shen, Yu Zhang, Yunyi Zhang, Jiawei Han. (2024)<br><strong>A Unified Taxonomy-Guided Instruction Tuning Framework for Entity Set Expansion and Taxonomy Expansion</strong><br><button class=copy-to-clipboard title="A Unified Taxonomy-Guided Instruction Tuning Framework for Entity Set Expansion and Taxonomy Expansion" index=59>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-59 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 33<br>Keywords: Benchmarking, Fine-tuning, Instruction Tuning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13405v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13405v1.pdf filename=2402.13405v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Entity Set Expansion, Taxonomy Expansion, and Seed-Guided Taxonomy Construction are three representative tasks that can be used to automatically populate an existing taxonomy with new entities. However, previous approaches often address these tasks separately with heterogeneous techniques, lacking a unified perspective. To tackle this issue, in this paper, we identify the common key skills needed for these tasks from the view of taxonomy structures &ndash; finding &lsquo;siblings&rsquo; and finding &lsquo;parents&rsquo; &ndash; and propose a unified taxonomy-guided <b>instruction</b> <b>tuning</b> framework to jointly solve the three tasks. To be specific, by leveraging the existing taxonomy as a rich source of entity relationships, we utilize <b>instruction</b> <b>tuning</b> to <b>fine-tune</b> a <b>large</b> <b>language</b> <b>model</b> to generate parent and sibling entities. Extensive experiments on multiple <b>benchmark</b> datasets demonstrate the effectiveness of TaxoInstruct, which outperforms task-specific baselines across all three tasks.</p></p class="citation"></blockquote><h3 id=6089--60316-bimedix-bilingual-medical-mixture-of-experts-llm-sara-pieri-et-al-2024>(60/89 | 60/316) BiMediX: Bilingual Medical Mixture of Experts LLM (Sara Pieri et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sara Pieri, Sahal Shaji Mullappilly, Fahad Shahbaz Khan, Rao Muhammad Anwer, Salman Khan, Timothy Baldwin, Hisham Cholakkal. (2024)<br><strong>BiMediX: Bilingual Medical Mixture of Experts LLM</strong><br><button class=copy-to-clipboard title="BiMediX: Bilingual Medical Mixture of Experts LLM" index=60>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-60 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 33<br>Keywords: Benchmarking, Question Answering, Instruction Tuning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13253v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13253v1.pdf filename=2402.13253v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we introduce BiMediX, the first bilingual medical mixture of experts <b>LLM</b> designed for seamless interaction in both English and Arabic. Our model facilitates a wide range of medical interactions in English and Arabic, including multi-turn chats to inquire about additional details such as patient symptoms and medical history, multiple-choice <b>question</b> <b>answering,</b> and open-ended <b>question</b> <b>answering.</b> We propose a semi-automated English-to-Arabic translation pipeline with human refinement to ensure high-quality translations. We also introduce a comprehensive evaluation <b>benchmark</b> for Arabic medical <b>LLMs.</b> Furthermore, we introduce BiMed1.3M, an extensive Arabic-English bilingual <b>instruction</b> <b>set</b> covering 1.3 Million diverse medical interactions, resulting in over 632 million healthcare specialized tokens for <b>instruction</b> <b>tuning.</b> Our BiMed1.3M dataset includes 250k synthesized multi-turn doctor-patient chats and maintains a 1:2 Arabic-to-English ratio. Our model outperforms state-of-the-art Med42 and Meditron by average absolute gains of 2.5% and 4.1%, respectively, computed across multiple medical evaluation <b>benchmarks</b> in English, while operating at 8-times faster inference. Moreover, our BiMediX outperforms the generic Arabic-English bilingual <b>LLM,</b> Jais-30B, by average absolute gains of 10% on our Arabic medical <b>benchmark</b> and 15% on bilingual evaluations across multiple datasets. Our project page with source code and trained model is available at <a href=https://github.com/mbzuai-oryx/BiMediX>https://github.com/mbzuai-oryx/BiMediX</a> .</p></p class="citation"></blockquote><h3 id=6189--61316-are-electras-sentence-embeddings-beyond-repair-the-case-of-semantic-textual-similarity-ivan-rep-et-al-2024>(61/89 | 61/316) Are ELECTRA&rsquo;s Sentence Embeddings Beyond Repair? The Case of Semantic Textual Similarity (Ivan Rep et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ivan Rep, David Dukić, Jan Šnajder. (2024)<br><strong>Are ELECTRA&rsquo;s Sentence Embeddings Beyond Repair? The Case of Semantic Textual Similarity</strong><br><button class=copy-to-clipboard title="Are ELECTRA's Sentence Embeddings Beyond Repair? The Case of Semantic Textual Similarity" index=61>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-61 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 33<br>Keywords: Benchmarking, Fine-tuning, BERT, Sentence Embedding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13130v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13130v1.pdf filename=2402.13130v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>While <b>BERT</b> produces high-quality <b>sentence</b> <b>embeddings,</b> its pre-training computational cost is a significant drawback. In contrast, ELECTRA delivers a cost-effective pre-training objective and downstream task performance improvements, but not as performant <b>sentence</b> <b>embeddings.</b> The community tacitly stopped utilizing ELECTRA&rsquo;s <b>sentence</b> <b>embeddings</b> for semantic textual similarity (STS). We notice a significant drop in performance when using the ELECTRA discriminator&rsquo;s last layer in comparison to earlier layers. We explore this drop and devise a way to repair ELECTRA&rsquo;s embeddings, proposing a novel truncated model <b>fine-tuning</b> (TMFT) method. TMFT improves the Spearman correlation coefficient by over 8 points while increasing parameter efficiency on the STS <b>benchmark</b> dataset. We extend our analysis to various model sizes and languages. Further, we discover the surprising efficacy of ELECTRA&rsquo;s generator model, which performs on par with <b>BERT,</b> using significantly fewer parameters and a substantially smaller embedding size. Finally, we observe further boosts by combining TMFT with a word similarity task or domain adaptive pre-training.</p></p class="citation"></blockquote><h3 id=6289--62316-code-needs-comments-enhancing-code-llms-with-comment-augmentation-demin-song-et-al-2024>(62/89 | 62/316) Code Needs Comments: Enhancing Code LLMs with Comment Augmentation (Demin Song et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Demin Song, Honglin Guo, Yunhua Zhou, Shuhao Xing, Yudong Wang, Zifan Song, Wenwei Zhang, Qipeng Guo, Hang Yan, Xipeng Qiu, Dahua Lin. (2024)<br><strong>Code Needs Comments: Enhancing Code LLMs with Comment Augmentation</strong><br><button class=copy-to-clipboard title="Code Needs Comments: Enhancing Code LLMs with Comment Augmentation" index=62>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-62 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 33<br>Keywords: Benchmarking, Data Augmentation, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13013v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13013v1.pdf filename=2402.13013v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The programming skill is one crucial ability for <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs),</b> necessitating a deep understanding of programming languages (PLs) and their correlation with natural languages (NLs). We examine the impact of pre-training <b>data</b> <b>on</b> code-focused <b>LLMs&rsquo;</b> performance by assessing the comment density as a measure of PL-NL alignment. Given the scarcity of code-comment aligned <b>data</b> <b>in</b> pre-training corpora, we introduce a novel <b>data</b> <b>augmentation</b> method that generates comments for existing code, coupled with a <b>data</b> <b>filtering</b> strategy that filters out code <b>data</b> <b>poorly</b> correlated with natural language. We conducted experiments on three code-focused <b>LLMs</b> and observed consistent improvements in performance on two widely-used programming skill <b>benchmarks.</b> Notably, the model trained on the augmented <b>data</b> <b>outperformed</b> both the model used for generating comments and the model further trained on the <b>data</b> <b>without</b> augmentation.</p></p class="citation"></blockquote><h3 id=6389--63316-nl2formula-generating-spreadsheet-formulas-from-natural-language-queries-wei-zhao-et-al-2024>(63/89 | 63/316) NL2Formula: Generating Spreadsheet Formulas from Natural Language Queries (Wei Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wei Zhao, Zhitao Hou, Siyuan Wu, Yan Gao, Haoyu Dong, Yao Wan, Hongyu Zhang, Yulei Sui, Haidong Zhang. (2024)<br><strong>NL2Formula: Generating Spreadsheet Formulas from Natural Language Queries</strong><br><button class=copy-to-clipboard title="NL2Formula: Generating Spreadsheet Formulas from Natural Language Queries" index=63>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-63 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 33<br>Keywords: Benchmarking, GPT, GPT-3, GPT-3.5<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14853v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14853v1.pdf filename=2402.14853v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Writing formulas on spreadsheets, such as Microsoft Excel and Google Sheets, is a widespread practice among users performing data analysis. However, crafting formulas on spreadsheets remains a tedious and error-prone task for many end-users, particularly when dealing with complex operations. To alleviate the burden associated with writing spreadsheet formulas, this paper introduces a novel <b>benchmark</b> task called NL2Formula, with the aim to generate executable formulas that are grounded on a spreadsheet table, given a Natural Language (NL) query as input. To accomplish this, we construct a comprehensive dataset consisting of 70,799 paired NL queries and corresponding spreadsheet formulas, covering 21,670 tables and 37 types of formula functions. We realize the NL2Formula task by providing a sequence-to-sequence baseline implementation called fCoder. Experimental results validate the effectiveness of fCoder, demonstrating its superior performance compared to the baseline models. Furthermore, we also compare fCoder with an initial <b>GPT-3.5</b> model (i.e., text-davinci-003). Lastly, through in-depth error analysis, we identify potential challenges in the NL2Formula task and advocate for further investigation.</p></p class="citation"></blockquote><h3 id=6489--64316-enhanced-hallucination-detection-in-neural-machine-translation-through-simple-detector-aggregation-anas-himmi-et-al-2024>(64/89 | 64/316) Enhanced Hallucination Detection in Neural Machine Translation through Simple Detector Aggregation (Anas Himmi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Anas Himmi, Guillaume Staerman, Marine Picot, Pierre Colombo, Nuno M. Guerreiro. (2024)<br><strong>Enhanced Hallucination Detection in Neural Machine Translation through Simple Detector Aggregation</strong><br><button class=copy-to-clipboard title="Enhanced Hallucination Detection in Neural Machine Translation through Simple Detector Aggregation" index=64>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-64 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Hallucination Detection, Neural Machine Translation, Neural Machine Translation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13331v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13331v1.pdf filename=2402.13331v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Hallucinated translations pose significant threats and safety concerns when it comes to the practical deployment of <b>machine</b> <b>translation</b> systems. Previous research works have identified that detectors exhibit complementary performance different detectors excel at detecting different types of <b>hallucinations.</b> <b>In</b> this paper, we propose to address the limitations of individual detectors by combining them and introducing a straightforward method for aggregating multiple detectors. Our results demonstrate the efficacy of our aggregated detector, providing a promising step towards evermore reliable <b>machine</b> <b>translation</b> systems.</p></p class="citation"></blockquote><h3 id=6589--65316-how-do-hyenas-deal-with-human-speech-speech-recognition-and-translation-with-confhyena-marco-gaido-et-al-2024>(65/89 | 65/316) How do Hyenas deal with Human Speech? Speech Recognition and Translation with ConfHyena (Marco Gaido et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Marco Gaido, Sara Papi, Matteo Negri, Luisa Bentivogli. (2024)<br><strong>How do Hyenas deal with Human Speech? Speech Recognition and Translation with ConfHyena</strong><br><button class=copy-to-clipboard title="How do Hyenas deal with Human Speech? Speech Recognition and Translation with ConfHyena" index=65>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-65 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Automatic Speech Recognition, Automatic Speech Recognition, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13208v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13208v1.pdf filename=2402.13208v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The attention mechanism, a cornerstone of state-of-the-art neural models, faces computational hurdles in processing long sequences due to its quadratic complexity. Consequently, research efforts in the last few years focused on finding more efficient alternatives. Among them, Hyena (Poli et al., 2023) stands out for achieving competitive results in both language modeling and image classification, while offering sub-quadratic memory and computational complexity. Building on these promising results, we propose ConfHyena, a Conformer whose encoder <b>self-attentions</b> are replaced with an adaptation of Hyena for <b>speech</b> <b>processing,</b> where the long input sequences cause high computational costs. Through experiments in <b>automatic</b> <b>speech</b> <b>recognition</b> (for English) and translation (from English into 8 target languages), we show that our best ConfHyena model significantly reduces the training time by 27%, at the cost of minimal quality degradation (~1%), which, in most cases, is not statistically significant.</p></p class="citation"></blockquote><h3 id=6689--66316-comparing-inferential-strategies-of-humans-and-large-language-models-in-deductive-reasoning-philipp-mondorf-et-al-2024>(66/89 | 66/316) Comparing Inferential Strategies of Humans and Large Language Models in Deductive Reasoning (Philipp Mondorf et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Philipp Mondorf, Barbara Plank. (2024)<br><strong>Comparing Inferential Strategies of Humans and Large Language Models in Deductive Reasoning</strong><br><button class=copy-to-clipboard title="Comparing Inferential Strategies of Humans and Large Language Models in Deductive Reasoning" index=66>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-66 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14856v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14856v1.pdf filename=2402.14856v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deductive <b>reasoning</b> plays a pivotal role in the formulation of sound and cohesive arguments. It allows individuals to draw conclusions that logically follow, given the truth value of the information provided. Recent progress in the domain of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> has showcased their capability in executing deductive <b>reasoning</b> tasks. Nonetheless, a significant portion of research primarily assesses the accuracy of <b>LLMs</b> in solving such tasks, often overlooking a deeper analysis of their <b>reasoning</b> behavior. In this study, we draw upon principles from cognitive psychology to examine inferential strategies employed by <b>LLMs,</b> through a detailed evaluation of their responses to propositional logic problems. Our findings indicate that <b>LLMs</b> display <b>reasoning</b> patterns akin to those observed in humans, including strategies like $\textit{supposition following}$ or $\textit{chain construction}$. Moreover, our research demonstrates that the architecture and scale of the model significantly affect its preferred method of <b>reasoning,</b> with more advanced models tending to adopt strategies more frequently than less sophisticated ones. Importantly, we assert that a model&rsquo;s accuracy, that is the correctness of its final conclusion, does not necessarily reflect the validity of its <b>reasoning</b> process. This distinction underscores the necessity for more nuanced evaluation procedures in the field.</p></p class="citation"></blockquote><h3 id=6789--67316-advancing-large-language-models-to-capture-varied-speaking-styles-and-respond-properly-in-spoken-conversations-guan-ting-lin-et-al-2024>(67/89 | 67/316) Advancing Large Language Models to Capture Varied Speaking Styles and Respond Properly in Spoken Conversations (Guan-Ting Lin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Guan-Ting Lin, Cheng-Han Chiang, Hung-yi Lee. (2024)<br><strong>Advancing Large Language Models to Capture Varied Speaking Styles and Respond Properly in Spoken Conversations</strong><br><button class=copy-to-clipboard title="Advancing Large Language Models to Capture Varied Speaking Styles and Respond Properly in Spoken Conversations" index=67>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-67 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL, eess-AS<br>Keyword Score: 30<br>Keywords: Speech-to-Speech Translation, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12786v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12786v1.pdf filename=2402.12786v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In spoken dialogue, even if two current turns are the same sentence, their responses might still differ when they are spoken in different styles. The spoken styles, containing paralinguistic and prosodic information, mark the most significant difference between text and speech modality. When using text-only <b>LLMs</b> to model spoken dialogue, text-only <b>LLMs</b> cannot give different responses based on the speaking style of the current turn. In this paper, we focus on enabling <b>LLMs</b> to listen to the speaking styles and respond properly. Our goal is to teach the <b>LLM</b> that &ldquo;even if the sentences are identical if they are spoken in different styles, their corresponding responses might be different&rdquo;. Since there is no suitable dataset for achieving this goal, we collect a <b>speech-to-speech</b> dataset, StyleTalk, with the following desired characteristics: when two current speeches have the same content but are spoken in different styles, their responses will be different. To teach <b>LLMs</b> to understand and respond properly to the speaking styles, we propose the Spoken-LLM framework that can model the linguistic content and the speaking styles. We train Spoken-LLM using the StyleTalk dataset and devise a two-stage training pipeline to help the Spoken-LLM better learn the speaking styles. Based on extensive experiments, we show that Spoken-LLM outperforms text-only baselines and prior speech <b>LLMs</b> methods.</p></p class="citation"></blockquote><h3 id=6889--68316-a-dual-prompting-for-interpretable-mental-health-language-models-hyolim-jeon-et-al-2024>(68/89 | 68/316) A Dual-Prompting for Interpretable Mental Health Language Models (Hyolim Jeon et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hyolim Jeon, Dongje Yoo, Daeun Lee, Sejung Son, Seungbae Kim, Jinyoung Han. (2024)<br><strong>A Dual-Prompting for Interpretable Mental Health Language Models</strong><br><button class=copy-to-clipboard title="A Dual-Prompting for Interpretable Mental Health Language Models" index=68>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-68 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Large Language Model, Large Language Model, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14854v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14854v1.pdf filename=2402.14854v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Despite the increasing demand for AI-based mental health monitoring tools, their practical utility for clinicians is limited by the lack of interpretability.The CLPsych 2024 Shared Task (Chim et al., 2024) aims to enhance the interpretability of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs),</b> particularly in mental health analysis, by providing evidence of suicidality through linguistic content. We propose a dual-prompting approach: (i) Knowledge-aware evidence extraction by leveraging the expert identity and a suicide dictionary with a mental health-specific <b>LLM;</b> and (ii) Evidence <b>summarization</b> by employing an <b>LLM-based</b> consistency evaluator. Comprehensive experiments demonstrate the effectiveness of combining domain-specific information, revealing performance improvements and the approach&rsquo;s potential to aid clinicians in assessing mental state progression.</p></p class="citation"></blockquote><h3 id=6989--69316-soft-self-consistency-improves-language-model-agents-han-wang-et-al-2024>(69/89 | 69/316) Soft Self-Consistency Improves Language Model Agents (Han Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Han Wang, Archiki Prasad, Elias Stengel-Eskin, Mohit Bansal. (2024)<br><strong>Soft Self-Consistency Improves Language Model Agents</strong><br><button class=copy-to-clipboard title="Soft Self-Consistency Improves Language Model Agents" index=69>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-69 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 25<br>Keywords: Black Box, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13212v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13212v1.pdf filename=2402.13212v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Generations from <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> can be improved by sampling and scoring multiple solutions to select a final answer. Current &ldquo;sample and select&rdquo; methods such as self-consistency (SC) rely on majority voting to score answers. However, when tasks have many distinct and valid answers, selection by voting requires a <b>large</b> <b>number</b> <b>of</b> samples. This makes SC prohibitively expensive for interactive tasks that involve generating multiple actions (answers) sequentially. After establishing that majority voting fails to provide consistent gains on such tasks, we demonstrate how to increase success rates by softening the scoring criterion. We introduce Soft Self-Consistency (Soft-SC), which replaces SC&rsquo;s discontinuous scoring with a continuous score computed from model likelihoods, allowing for selection even when actions are sparsely distributed. Soft-SC improves both performance and efficiency on long-horizon interactive tasks, requiring half as many samples as SC for comparable or better performance. For a fixed number of samples, Soft-SC leads to a 1.3% increase over SC in absolute success rate on writing bash programs, a 6.6% increase on online shopping (WebShop), and a 4.7% increase for an interactive household game (ALFWorld). Finally, we show that Soft-SC can be applied to both open-source and <b>black-box</b> <b>models.</b></p></p class="citation"></blockquote><h3 id=7089--70316-treeeval-benchmark-free-evaluation-of-large-language-models-through-tree-planning-xiang-li-et-al-2024>(70/89 | 70/316) TreeEval: Benchmark-Free Evaluation of Large Language Models through Tree Planning (Xiang Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiang Li, Yunshi Lan, Chao Yang. (2024)<br><strong>TreeEval: Benchmark-Free Evaluation of Large Language Models through Tree Planning</strong><br><button class=copy-to-clipboard title="TreeEval: Benchmark-Free Evaluation of Large Language Models through Tree Planning" index=70>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-70 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 23<br>Keywords: Benchmarking, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13125v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13125v1.pdf filename=2402.13125v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recently, numerous new <b>benchmarks</b> have been established to evaluate the performance of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> via either computing a holistic score or employing another <b>LLM</b> as a judge. However, these approaches suffer from data leakage due to the open access of the <b>benchmark</b> and inflexible evaluation process. To address this issue, we introduce $\textbf{TreeEval}$, a <b>benchmark-free</b> evaluation method for <b>LLMs</b> that let a high-performance <b>LLM</b> host an irreproducible evaluation session and essentially avoids the data leakage. Moreover, this <b>LLM</b> performs as an examiner to raise up a series of questions under a topic with a tree planing strategy, which considers the current evaluation status to decide the next question generation and ensures the completeness and efficiency of the evaluation process. We evaluate $6$ models of different parameter sizes, including $7$B, $13$B, and $33$B, and ultimately achieved the highest correlation coefficient with AlpacaEval2.0 using only around $45$ questions. We also conduct more analysis to show the robustness and reliability of TreeEval. Our code can be accessed via the provided <a href=https://github.com/Ashura5/TreeEval>https://github.com/Ashura5/TreeEval</a>.</p></p class="citation"></blockquote><h3 id=7189--71316-event-level-knowledge-editing-hao-peng-et-al-2024>(71/89 | 71/316) Event-level Knowledge Editing (Hao Peng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hao Peng, Xiaozhi Wang, Chunyang Li, Kaisheng Zeng, Jiangshan Duo, Yixin Cao, Lei Hou, Juanzi Li. (2024)<br><strong>Event-level Knowledge Editing</strong><br><button class=copy-to-clipboard title="Event-level Knowledge Editing" index=71>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-71 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 23<br>Keywords: Benchmarking, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13093v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13093v1.pdf filename=2402.13093v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Knowledge editing aims at updating knowledge of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> to prevent them from becoming outdated. Existing work edits <b>LLMs</b> at the level of factual knowledge triplets. However, natural knowledge updates in the real world come from the occurrences of new events rather than direct changes in factual triplets. In this paper, we propose a new task setting: event-level knowledge editing, which directly edits new events into <b>LLMs</b> and improves over conventional triplet-level editing on (1) Efficiency. A single event edit leads to updates in multiple entailed knowledge triplets. (2) Completeness. Beyond updating factual knowledge, event-level editing also requires considering the event influences and updating <b>LLMs&rsquo;</b> knowledge about future trends. We construct a high-quality event-level editing <b>benchmark</b> ELKEN, consisting of 1,515 event edits, 6,449 questions about factual knowledge, and 10,150 questions about future tendencies. We systematically evaluate the performance of various knowledge editing methods and <b>LLMs</b> on this <b>benchmark.</b> We find that ELKEN poses significant challenges to existing knowledge editing approaches. Our codes and dataset are publicly released to facilitate further research.</p></p class="citation"></blockquote><h3 id=7289--72316-bias-in-language-models-beyond-trick-tests-and-toward-ruted-evaluation-kristian-lum-et-al-2024>(72/89 | 72/316) Bias in Language Models: Beyond Trick Tests and Toward RUTEd Evaluation (Kristian Lum et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kristian Lum, Jacy Reese Anthis, Chirag Nagpal, Alexander D&rsquo;Amour. (2024)<br><strong>Bias in Language Models: Beyond Trick Tests and Toward RUTEd Evaluation</strong><br><button class=copy-to-clipboard title="Bias in Language Models: Beyond Trick Tests and Toward RUTEd Evaluation" index=72>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-72 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL, stat-AP<br>Keyword Score: 23<br>Keywords: Benchmarking, Text Generation, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12649v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12649v1.pdf filename=2402.12649v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Bias <b>benchmarks</b> are a popular method for studying the negative impacts of bias in <b>LLMs,</b> yet there has been little empirical investigation of whether these <b>benchmarks</b> are actually indicative of how real world harm may manifest in the real world. In this work, we study the correspondence between such decontextualized &ldquo;trick tests&rdquo; and evaluations that are more grounded in Realistic Use and Tangible {Effects (i.e. RUTEd evaluations). We explore this correlation in the context of gender-occupation bias&ndash;a popular genre of bias evaluation. We compare three de-contextualized evaluations adapted from the current literature to three analogous RUTEd evaluations applied to long-form content generation. We conduct each evaluation for seven instruction-tuned <b>LLMs.</b> For the RUTEd evaluations, we conduct repeated trials of three <b>text</b> <b>generation</b> tasks: children&rsquo;s bedtime stories, user personas, and English language learning exercises. We found no correspondence between trick tests and RUTEd evaluations. Specifically, selecting the least biased model based on the de-contextualized results coincides with selecting the model with the best performance on RUTEd evaluations only as often as random chance. We conclude that evaluations that are not based in realistic use are likely insufficient to mitigate and assess bias and real-world harms.</p></p class="citation"></blockquote><h3 id=7389--73316-softmax-probabilities-mostly-predict-large-language-model-correctness-on-multiple-choice-qa-benjamin-plaut-et-al-2024>(73/89 | 73/316) Softmax Probabilities (Mostly) Predict Large Language Model Correctness on Multiple-Choice Q&amp;A (Benjamin Plaut et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Benjamin Plaut, Khanh Nguyen, Tu Trinh. (2024)<br><strong>Softmax Probabilities (Mostly) Predict Large Language Model Correctness on Multiple-Choice Q&amp;A</strong><br><button class=copy-to-clipboard title="Softmax Probabilities (Mostly) Predict Large Language Model Correctness on Multiple-Choice Q&A" index=73>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-73 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13213v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13213v1.pdf filename=2402.13213v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Although <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> perform impressively on many tasks, overconfidence remains a problem. We hypothesized that on multiple-choice Q&amp;A tasks, wrong answers would be associated with smaller maximum softmax probabilities (MSPs) compared to correct answers. We comprehensively evaluate this hypothesis on ten open-source <b>LLMs</b> and five datasets, and find strong evidence for our hypothesis among models which perform well on the original Q&amp;A task. For the six <b>LLMs</b> with the best Q&amp;A performance, the AUROC derived from the MSP was better than random chance with p &lt; 10^{-4} in 59/60 instances. Among those six <b>LLMs,</b> the average AUROC ranged from 60% to 69%. Leveraging these findings, we propose a multiple-choice Q&amp;A task with an option to abstain and show that performance can be improved by selectively abstaining based on the MSP of the initial model response. We also run the same experiments with pre-softmax logits instead of softmax probabilities and find similar (but not identical) results.</p></p class="citation"></blockquote><h3 id=7489--74316-can-large-language-models-be-good-emotional-supporter-mitigating-preference-bias-on-emotional-support-conversation-dongjin-kang-et-al-2024>(74/89 | 74/316) Can Large Language Models be Good Emotional Supporter? Mitigating Preference Bias on Emotional Support Conversation (Dongjin Kang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dongjin Kang, Sunghwan Kim, Taeyoon Kwon, Seungjun Moon, Hyunsouk Cho, Youngjae Yu, Dongha Lee, Jinyoung Yeo. (2024)<br><strong>Can Large Language Models be Good Emotional Supporter? Mitigating Preference Bias on Emotional Support Conversation</strong><br><button class=copy-to-clipboard title="Can Large Language Models be Good Emotional Supporter? Mitigating Preference Bias on Emotional Support Conversation" index=74>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-74 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: I-2-7, cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13211v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13211v1.pdf filename=2402.13211v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Emotional Support Conversation (ESC) is a task aimed at alleviating individuals&rsquo; emotional distress through daily conversation. Given its inherent complexity and non-intuitive nature, ESConv dataset incorporates support strategies to facilitate the generation of appropriate responses. Recently, despite the remarkable conversational ability of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs),</b> previous studies have suggested that they often struggle with providing useful emotional support. Hence, this work initially analyzes the results of <b>LLMs</b> on ESConv, revealing challenges in selecting the correct strategy and a notable preference for a specific strategy. Motivated by these, we explore the impact of the inherent preference in <b>LLMs</b> on providing emotional support, and consequently, we observe that exhibiting high preference for specific strategies hinders effective emotional support, aggravating its robustness in predicting the appropriate strategy. Moreover, we conduct a methodological study to offer insights into the necessary approaches for <b>LLMs</b> to serve as proficient emotional supporters. Our findings emphasize that (1) low preference for specific strategies hinders the progress of emotional support, (2) external assistance helps reduce preference bias, and (3) <b>LLMs</b> alone cannot become good emotional supporters. These insights suggest promising avenues for future research to enhance the emotional intelligence of <b>LLMs.</b></p></p class="citation"></blockquote><h3 id=7589--75316-what-if-llms-have-different-world-views-simulating-alien-civilizations-with-llm-based-agents-mingyu-jin-et-al-2024>(75/89 | 75/316) What if LLMs Have Different World Views: Simulating Alien Civilizations with LLM-based Agents (Mingyu Jin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mingyu Jin, Beichen Wang, Zhaoqian Xue, Suiyuan Zhu, Wenyue Hua, Hua Tang, Kai Mei, Mengnan Du, Yongfeng Zhang. (2024)<br><strong>What if LLMs Have Different World Views: Simulating Alien Civilizations with LLM-based Agents</strong><br><button class=copy-to-clipboard title="What if LLMs Have Different World Views: Simulating Alien Civilizations with LLM-based Agents" index=75>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-75 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13184v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13184v2.pdf filename=2402.13184v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this study, we introduce &ldquo;CosmoAgent,&rdquo; an innovative artificial intelligence framework utilizing <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> to simulate complex interactions between human and extraterrestrial civilizations, with a special emphasis on Stephen Hawking&rsquo;s cautionary advice about not sending radio signals haphazardly into the universe. The goal is to assess the feasibility of peaceful coexistence while considering potential risks that could threaten well-intentioned civilizations. Employing mathematical models and state transition matrices, our approach quantitatively evaluates the development trajectories of civilizations, offering insights into future decision-making at critical points of growth and saturation. Furthermore, the paper acknowledges the vast diversity in potential living conditions across the universe, which could foster unique cosmologies, ethical codes, and worldviews among various civilizations. Recognizing the Earth-centric bias inherent in current <b>LLM</b> designs, we propose the novel concept of using <b>LLMs</b> with diverse ethical paradigms and simulating interactions between entities with distinct moral principles. This innovative research provides a new way to understand complex inter-civilizational dynamics, expanding our perspective while pioneering novel strategies for conflict resolution, crucial for preventing interstellar conflicts. We have also released the code and datasets to enable further academic investigation into this interesting area of research. The code is available at <a href=https://github.com/agiresearch/AlienAgent>https://github.com/agiresearch/AlienAgent</a>.</p></p class="citation"></blockquote><h3 id=7689--76316-when-only-time-will-tell-interpreting-how-transformers-process-local-ambiguities-through-the-lens-of-restart-incrementality-brielen-madureira-et-al-2024>(76/89 | 76/316) When Only Time Will Tell: Interpreting How Transformers Process Local Ambiguities Through the Lens of Restart-Incrementality (Brielen Madureira et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Brielen Madureira, Patrick Kahardipraja, David Schlangen. (2024)<br><strong>When Only Time Will Tell: Interpreting How Transformers Process Local Ambiguities Through the Lens of Restart-Incrementality</strong><br><button class=copy-to-clipboard title="When Only Time Will Tell: Interpreting How Transformers Process Local Ambiguities Through the Lens of Restart-Incrementality" index=76>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-76 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Transformer, Dependency Parsing<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13113v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13113v1.pdf filename=2402.13113v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Incremental models that process sentences one token at a time will sometimes encounter points where more than one interpretation is possible. Causal models are forced to output one interpretation and continue, whereas models that can revise may edit their previous output as the ambiguity is resolved. In this work, we look at how restart-incremental <b>Transformers</b> build and update internal states, in an effort to shed light on what processes cause revisions not viable in autoregressive models. We propose an interpretable way to analyse the incremental states, showing that their sequential structure encodes information on the garden path effect and its resolution. Our method brings insights on various bidirectional encoders for contextualised meaning representation and <b>dependency</b> <b>parsing,</b> contributing to show their advantage over causal models when it comes to revisions.</p></p class="citation"></blockquote><h3 id=7789--77316-stable-knowledge-editing-in-large-language-models-zihao-wei-et-al-2024>(77/89 | 77/316) Stable Knowledge Editing in Large Language Models (Zihao Wei et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zihao Wei, Liang Pang, Hanxing Ding, Jingcheng Deng, Huawei Shen, Xueqi Cheng. (2024)<br><strong>Stable Knowledge Editing in Large Language Models</strong><br><button class=copy-to-clipboard title="Stable Knowledge Editing in Large Language Models" index=77>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-77 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: ChatGPT, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13048v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13048v1.pdf filename=2402.13048v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Efficient knowledge editing of <b>large</b> <b>language</b> <b>models</b> is crucial for replacing obsolete information or incorporating specialized knowledge on a <b>large</b> <b>scale.</b> <b>However,</b> previous methods implicitly assume that knowledge is localized and isolated within the model, an assumption that oversimplifies the interconnected nature of model knowledge. The premise of localization results in an incomplete knowledge editing, whereas an isolated assumption may impair both other knowledge and general abilities. It introduces instability to the performance of the knowledge editing method. To transcend these assumptions, we introduce StableKE, a method adopts a novel perspective based on knowledge augmentation rather than knowledge localization. To overcome the expense of human labeling, StableKE integrates two automated knowledge augmentation strategies: Semantic Paraphrase Enhancement strategy, which diversifies knowledge descriptions to facilitate the teaching of new information to the model, and Contextual Description Enrichment strategy, expanding the surrounding knowledge to prevent the forgetting of related information. StableKE surpasses other knowledge editing methods, demonstrating stability both edited knowledge and multi-hop knowledge, while also preserving unrelated knowledge and general abilities. Moreover, StableKE can edit knowledge on <b>ChatGPT.</b></p></p class="citation"></blockquote><h3 id=7889--78316-acknowledgment-of-emotional-states-generating-validating-responses-for-empathetic-dialogue-zi-haur-pang-et-al-2024>(78/89 | 78/316) Acknowledgment of Emotional States: Generating Validating Responses for Empathetic Dialogue (Zi Haur Pang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zi Haur Pang, Yahui Fu, Divesh Lala, Keiko Ochi, Koji Inoue, Tatsuya Kawahara. (2024)<br><strong>Acknowledgment of Emotional States: Generating Validating Responses for Empathetic Dialogue</strong><br><button class=copy-to-clipboard title="Acknowledgment of Emotional States: Generating Validating Responses for Empathetic Dialogue" index=78>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-78 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: BERT, ChatGPT<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12770v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12770v1.pdf filename=2402.12770v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the realm of human-AI dialogue, the facilitation of empathetic responses is important. Validation is one of the key communication techniques in psychology, which entails recognizing, understanding, and acknowledging others&rsquo; emotional states, thoughts, and actions. This study introduces the first framework designed to engender empathetic dialogue with validating responses. Our approach incorporates a tripartite module system: 1) validation timing detection, 2) users&rsquo; emotional state identification, and 3) validating response generation. Utilizing Japanese EmpatheticDialogues dataset - a textual-based dialogue dataset consisting of 8 emotional categories from Plutchik&rsquo;s wheel of emotions - the Task Adaptive Pre-Training (TAPT) <b>BERT-based</b> model outperforms both random baseline and the <b>ChatGPT</b> performance, in term of F1-score, in all modules. Further validation of our model&rsquo;s efficacy is confirmed in its application to the TUT Emotional Storytelling Corpus (TESC), a speech-based dialogue dataset, by surpassing both random baseline and the <b>ChatGPT.</b> This consistent performance across both textual and speech-based dialogues underscores the effectiveness of our framework in fostering empathetic human-AI communication.</p></p class="citation"></blockquote><h3 id=7989--79316-an-llm-maturity-model-for-reliable-and-transparent-text-to-query-lei-yu-et-al-2024>(79/89 | 79/316) An LLM Maturity Model for Reliable and Transparent Text-to-Query (Lei Yu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lei Yu, Abir Ray. (2024)<br><strong>An LLM Maturity Model for Reliable and Transparent Text-to-Query</strong><br><button class=copy-to-clipboard title="An LLM Maturity Model for Reliable and Transparent Text-to-Query" index=79>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-79 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14855v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14855v1.pdf filename=2402.14855v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recognizing the imperative to address the reliability and transparency issues of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLM),</b> this work proposes an <b>LLM</b> maturity model tailored for text-to-query applications. This maturity model seeks to fill the existing void in evaluating <b>LLMs</b> in such applications by incorporating dimensions beyond mere correctness or accuracy. Moreover, this work introduces a real-world use case from the law enforcement domain and showcases QueryIQ, an <b>LLM-powered,</b> domain-specific text-to-query assistant to expedite user workflows and reveal hidden relationship in data.</p></p class="citation"></blockquote><h3 id=8089--80316-are-large-language-models-rational-investors-yuhang-zhou-et-al-2024>(80/89 | 80/316) Are Large Language Models Rational Investors? (Yuhang Zhou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuhang Zhou, Yuchen Ni, Xiang Liu, Jian Zhang, Sen Liu, Guangnan Ye, Hongfeng Chai. (2024)<br><strong>Are Large Language Models Rational Investors?</strong><br><button class=copy-to-clipboard title="Are Large Language Models Rational Investors?" index=80>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-80 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12713v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12713v1.pdf filename=2402.12713v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> are progressively being adopted in financial analysis to harness their extensive knowledge base for interpreting complex market data and trends. However, their application in the financial domain is challenged by intrinsic biases (i.e., risk-preference bias) and a superficial grasp of market intricacies, underscoring the need for a thorough assessment of their financial insight. This study introduces a novel framework, Financial Bias Indicators (FBI), to critically evaluate the financial rationality of <b>LLMs,</b> focusing on their ability to discern and navigate the subtleties of financial information and to identify any irrational biases that might skew market analysis. Our research adopts an innovative methodology to measure financial rationality, integrating principles of behavioral finance to scrutinize the biases and decision-making patterns of <b>LLMs.</b> We conduct a comprehensive evaluation of 19 leading <b>LLMs,</b> considering factors such as model scale, training datasets, input strategies, etc. The findings reveal varying degrees of financial irrationality among the models, influenced by their design and training. Models trained specifically on financial datasets might exhibit greater irrationality, and it&rsquo;s possible that even larger financial language models (FinLLMs) could display more biases than smaller, more generalized models. This outcomes provide profound insights into how these elements affect the financial rationality of <b>LLMs,</b> indicating that targeted training and structured input methods could improve model performance. This work enriches our understanding of <b>LLMs&rsquo;</b> strengths and weaknesses in financial applications, laying the groundwork for the development of more dependable and rational financial analysis tools.</p></p class="citation"></blockquote><h3 id=8189--81316-cfever-a-chinese-fact-extraction-and-verification-dataset-ying-jia-lin-et-al-2024>(81/89 | 81/316) CFEVER: A Chinese Fact Extraction and VERification Dataset (Ying-Jia Lin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ying-Jia Lin, Chun-Yi Lin, Chia-Jen Yeh, Yi-Ting Li, Yun-Yu Hu, Chih-Hao Hsu, Mei-Feng Lee, Hung-Yu Kao. (2024)<br><strong>CFEVER: A Chinese Fact Extraction and VERification Dataset</strong><br><button class=copy-to-clipboard title="CFEVER: A Chinese Fact Extraction and VERification Dataset" index=81>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-81 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 13<br>Keywords: Benchmarking, Fact Verification<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13025v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13025v1.pdf filename=2402.13025v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present CFEVER, a Chinese dataset designed for <b>Fact</b> <b>Extraction</b> and VERification. CFEVER comprises 30,012 manually created claims based on content in Chinese Wikipedia. Each claim in CFEVER is labeled as &ldquo;Supports&rdquo;, &ldquo;Refutes&rdquo;, or &ldquo;Not Enough Info&rdquo; to depict its degree of factualness. Similar to the FEVER dataset, claims in the &ldquo;Supports&rdquo; and &ldquo;Refutes&rdquo; categories are also annotated with corresponding evidence sentences sourced from single or multiple pages in Chinese Wikipedia. Our labeled dataset holds a Fleiss&rsquo; kappa value of 0.7934 for five-way inter-annotator agreement. In addition, through the experiments with the state-of-the-art approaches developed on the FEVER dataset and a simple baseline for CFEVER, we demonstrate that our dataset is a new rigorous <b>benchmark</b> for factual extraction and verification, which can be further used for developing automated systems to alleviate human <b>fact-checking</b> <b>efforts.</b> CFEVER is available at <a href=https://ikmlab.github.io/CFEVER>https://ikmlab.github.io/CFEVER</a>.</p></p class="citation"></blockquote><h3 id=8289--82316-explaining-relationships-among-research-papers-xiangci-li-et-al-2024>(82/89 | 82/316) Explaining Relationships Among Research Papers (Xiangci Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiangci Li, Jessica Ouyang. (2024)<br><strong>Explaining Relationships Among Research Papers</strong><br><button class=copy-to-clipboard title="Explaining Relationships Among Research Papers" index=82>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-82 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 10<br>Keywords: Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13426v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13426v1.pdf filename=2402.13426v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Due to the rapid pace of research publications, keeping up to date with all the latest related papers is very time-consuming, even with daily feed tools. There is a need for automatically generated, short, customized literature reviews of sets of papers to help researchers decide what to read. While several works in the last decade have addressed the task of explaining a single research paper, usually in the context of another paper citing it, the relationship among multiple papers has been ignored; prior works have focused on generating a single citation sentence in isolation, without addressing the expository and transition sentences needed to connect multiple papers in a coherent story. In this work, we explore a feature-based, <b>LLM-prompting</b> approach to generate richer citation texts, as well as generating multiple citations at once to capture the complex relationships among research papers. We perform an expert evaluation to investigate the impact of our proposed features on the quality of the generated paragraphs and find a strong correlation between human preference and integrative writing style, suggesting that humans prefer high-level, abstract citations, with transition sentences between them to provide an overall story.</p></p class="citation"></blockquote><h3 id=8389--83316-the-hidden-space-of-transformer-language-adapters-jesujoba-o-alabi-et-al-2024>(83/89 | 83/316) The Hidden Space of Transformer Language Adapters (Jesujoba O. Alabi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jesujoba O. Alabi, Marius Mosbach, Matan Eyal, Dietrich Klakow, Mor Geva. (2024)<br><strong>The Hidden Space of Transformer Language Adapters</strong><br><button class=copy-to-clipboard title="The Hidden Space of Transformer Language Adapters" index=83>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-83 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 10<br>Keywords: Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13137v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13137v1.pdf filename=2402.13137v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We analyze the operation of <b>transformer</b> language adapters, which are small modules trained on top of a frozen language model to adapt its predictions to new target languages. We show that adapted predictions mostly evolve in the source language the model was trained on, while the target language becomes pronounced only in the very last layers of the model. Moreover, the adaptation process is gradual and distributed across layers, where it is possible to skip small groups of adapters without decreasing adaptation performance. Last, we show that adapters operate on top of the model&rsquo;s frozen representation space while largely preserving its structure, rather than on an &lsquo;isolated&rsquo; subspace. Our findings provide a deeper view into the adaptation process of language models to new languages, showcasing the constraints imposed on it by the underlying model and introduces practical implications to enhance its efficiency.</p></p class="citation"></blockquote><h3 id=8489--84316-phonotactic-complexity-across-dialects-ryan-soh-eun-shim-et-al-2024>(84/89 | 84/316) Phonotactic Complexity across Dialects (Ryan Soh-Eun Shim et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ryan Soh-Eun Shim, Kalvin Chang, David R. Mortensen. (2024)<br><strong>Phonotactic Complexity across Dialects</strong><br><button class=copy-to-clipboard title="Phonotactic Complexity across Dialects" index=84>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-84 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 10<br>Keywords: LSTM<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12998v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12998v1.pdf filename=2402.12998v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Received wisdom in linguistic typology holds that if the structure of a language becomes more complex in one dimension, it will simplify in another, building on the assumption that all languages are equally complex (Joseph and Newmeyer, 2012). We study this claim on a micro-level, using a tightly-controlled sample of Dutch dialects (across 366 collection sites) and Min dialects (across 60 sites), which enables a more fair comparison across varieties. Even at the dialect level, we find empirical evidence for a tradeoff between word length and a computational measure of phonotactic complexity from a <b>LSTM-based</b> phone-level language model-a result previously documented only at the language level. A generalized additive model (GAM) shows that dialects with low phonotactic complexity concentrate around the capital regions, which we hypothesize to correspond to prior hypotheses that language varieties of greater or more diverse populations show reduced phonotactic complexity. We also experiment with incorporating the auxiliary task of predicting syllable constituency, but do not find an increase in the negative correlation observed.</p></p class="citation"></blockquote><h3 id=8589--85316-autism-detection-in-speech----a-survey-nadine-probol-et-al-2024>(85/89 | 85/316) Autism Detection in Speech &ndash; A Survey (Nadine Probol et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nadine Probol, Margot Mieskes. (2024)<br><strong>Autism Detection in Speech &ndash; A Survey</strong><br><button class=copy-to-clipboard title="Autism Detection in Speech -- A Survey" index=85>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-85 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 10<br>Keywords: Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12880v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12880v1.pdf filename=2402.12880v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>There has been a range of studies of how autism is displayed in voice, speech, and language. We analyse studies from the biomedical, as well as the psychological domain, but also from the NLP domain in order to find linguistic, prosodic and acoustic cues that could indicate autism. Our survey looks at all three domains. We define autism and which comorbidities might influence the correct detection of the disorder. We especially look at observations such as verbal and semantic fluency, prosodic features, but also disfluencies and speaking rate. We also show word-based approaches and describe machine learning and <b>transformer-based</b> approaches both on the audio data as well as the transcripts. Lastly, we conclude, while there already is a lot of research, female patients seem to be severely under-researched. Also, most NLP research focuses on traditional machine learning methods instead of <b>transformers</b> which could be beneficial in this context. Additionally, we were unable to find research combining both features from audio and transcripts.</p></p class="citation"></blockquote><h3 id=8689--86316-backward-lens-projecting-language-model-gradients-into-the-vocabulary-space-shahar-katz-et-al-2024>(86/89 | 86/316) Backward Lens: Projecting Language Model Gradients into the Vocabulary Space (Shahar Katz et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shahar Katz, Yonatan Belinkov, Mor Geva, Lior Wolf. (2024)<br><strong>Backward Lens: Projecting Language Model Gradients into the Vocabulary Space</strong><br><button class=copy-to-clipboard title="Backward Lens: Projecting Language Model Gradients into the Vocabulary Space" index=86>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-86 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 10<br>Keywords: Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12865v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12865v1.pdf filename=2402.12865v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Understanding how <b>Transformer-based</b> Language Models (LMs) learn and recall information is a key goal of the deep learning community. Recent interpretability methods project weights and hidden states obtained from the forward pass to the models&rsquo; vocabularies, helping to uncover how information flows within LMs. In this work, we extend this methodology to LMs&rsquo; backward pass and gradients. We first prove that a gradient matrix can be cast as a low-rank linear combination of its forward and backward passes&rsquo; inputs. We then develop methods to project these gradients into vocabulary items and explore the mechanics of how new information is stored in the LMs&rsquo; neurons.</p></p class="citation"></blockquote><h3 id=8789--87316-handling-ambiguity-in-emotion-from-out-of-domain-detection-to-distribution-estimation-wen-wu-et-al-2024>(87/89 | 87/316) Handling Ambiguity in Emotion: From Out-of-Domain Detection to Distribution Estimation (Wen Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wen Wu, Bo Li, Chao Zhang, Chung-Cheng Chiu, Qiujia Li, Junwen Bai, Tara N. Sainath, Philip C. Woodland. (2024)<br><strong>Handling Ambiguity in Emotion: From Out-of-Domain Detection to Distribution Estimation</strong><br><button class=copy-to-clipboard title="Handling Ambiguity in Emotion: From Out-of-Domain Detection to Distribution Estimation" index=87>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-87 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 10<br>Keywords: Out-of-domain<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12862v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12862v1.pdf filename=2402.12862v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The subjective perception of emotion leads to inconsistent labels from human annotators. Typically, utterances lacking majority-agreed labels are excluded when training an emotion classifier, which cause problems when encountering ambiguous emotional expressions during testing. This paper investigates three methods to handle ambiguous emotion. First, we show that incorporating utterances without majority-agreed labels as an additional class in the classifier reduces the classification performance of the other emotion classes. Then, we propose detecting utterances with ambiguous emotions as <b>out-of-domain</b> samples by quantifying the uncertainty in emotion classification using evidential deep learning. This approach retains the classification accuracy while effectively detects ambiguous emotion expressions. Furthermore, to obtain fine-grained distinctions among ambiguous emotions, we propose representing emotion as a distribution instead of a single class label. The task is thus re-framed from classification to distribution estimation where every individual annotation is taken into account, not just the majority opinion. The evidential uncertainty measure is extended to quantify the uncertainty in emotion distribution estimation. Experimental results on the IEMOCAP and CREMA-D datasets demonstrate the superior capability of the proposed method in terms of majority class prediction, emotion distribution estimation, and uncertainty estimation.</p></p class="citation"></blockquote><h3 id=8889--88316-simpsons-paradox-and-the-accuracy-fluency-tradeoff-in-translation-zheng-wei-lim-et-al-2024>(88/89 | 88/316) Simpson&rsquo;s Paradox and the Accuracy-Fluency Tradeoff in Translation (Zheng Wei Lim et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zheng Wei Lim, Ekaterina Vylomova, Trevor Cohn, Charles Kemp. (2024)<br><strong>Simpson&rsquo;s Paradox and the Accuracy-Fluency Tradeoff in Translation</strong><br><button class=copy-to-clipboard title="Simpson's Paradox and the Accuracy-Fluency Tradeoff in Translation" index=88>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-88 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 10<br>Keywords: Neural Machine Translation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12690v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12690v1.pdf filename=2402.12690v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A good translation should be faithful to the source and should respect the norms of the target language. We address a theoretical puzzle about the relationship between these objectives. On one hand, intuition and some prior work suggest that accuracy and fluency should trade off against each other, and that capturing every detail of the source can only be achieved at the cost of fluency. On the other hand, quality assessment researchers often suggest that accuracy and fluency are highly correlated and difficult for human raters to distinguish (Callison-Burch et al. 2007). We show that the tension between these views is an instance of Simpson&rsquo;s paradox, and that accuracy and fluency are positively correlated at the level of the corpus but trade off at the level of individual source segments. We further suggest that the relationship between accuracy and fluency is best evaluated at the segment (or sentence) level, and that the trade off between these dimensions has implications both for assessing translation quality and developing improved <b>MT</b> systems.</p></p class="citation"></blockquote><h3 id=8989--89316-styledubber-towards-multi-scale-style-learning-for-movie-dubbing-gaoxiang-cong-et-al-2024>(89/89 | 89/316) StyleDubber: Towards Multi-Scale Style Learning for Movie Dubbing (Gaoxiang Cong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Gaoxiang Cong, Yuankai Qi, Liang Li, Amin Beheshti, Zhedong Zhang, Anton van den Hengel, Ming-Hsuan Yang, Chenggang Yan, Qingming Huang. (2024)<br><strong>StyleDubber: Towards Multi-Scale Style Learning for Movie Dubbing</strong><br><button class=copy-to-clipboard title="StyleDubber: Towards Multi-Scale Style Learning for Movie Dubbing" index=89>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-89 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 9<br>Keywords: Benchmarking, Multi-modal, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12636v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12636v2.pdf filename=2402.12636v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Given a script, the challenge in Movie Dubbing (Visual Voice Cloning, V2C) is to generate speech that aligns well with the video in both time and emotion, based on the tone of a reference audio track. Existing state-of-the-art V2C models break the phonemes in the script according to the divisions between video frames, which solves the temporal alignment problem but leads to incomplete phoneme pronunciation and poor identity stability. To address this problem, we propose StyleDubber, which switches dubbing learning from the frame level to phoneme level. It contains three main components: (1) A <b>multimodal</b> style adaptor operating at the phoneme level to learn pronunciation style from the reference audio, and generate intermediate representations informed by the facial emotion presented in the video; (2) An utterance-level style learning module, which guides both the mel-spectrogram decoding and the refining processes from the intermediate embeddings to improve the overall style expression; And (3) a phoneme-guided lip aligner to maintain lip sync. Extensive experiments on two of the primary <b>benchmarks,</b> V2C and Grid, demonstrate the favorable performance of the proposed method as compared to the current state-of-the-art. The source code and trained models will be released to the public.</p></p class="citation"></blockquote><h2 id=cscv-45>cs.CV (45)</h2><h3 id=145--90316-cell-graph-transformer-for-nuclei-classification-wei-lou-et-al-2024>(1/45 | 90/316) Cell Graph Transformer for Nuclei Classification (Wei Lou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wei Lou, Guanbin Li, Xiang Wan, Haofeng Li. (2024)<br><strong>Cell Graph Transformer for Nuclei Classification</strong><br><button class=copy-to-clipboard title="Cell Graph Transformer for Nuclei Classification" index=90>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-90 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 93<br>Keywords: Graph Convolutional Network, Graph Convolutional Network, Graph, Graph Neural Network, Graph Neural Network, Convolution, Convolutional Neural Network, Fine-tuning, Transformer, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12946v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12946v1.pdf filename=2402.12946v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Nuclei classification is a critical step in computer-aided diagnosis with histopathology images. In the past, various methods have employed <b>graph</b> <b>neural</b> <b>networks</b> <b>(GNN)</b> to analyze cell <b>graphs</b> <b>that</b> <b>model</b> inter-cell relationships by considering nuclei as vertices. However, they are limited by the <b>GNN</b> mechanism that only passes messages among local nodes via fixed edges. To address the issue, we develop a cell <b>graph</b> <b>transformer</b> <b>(CGT)</b> that treats nodes and edges as input tokens to enable learnable adjacency and information exchange among all nodes. Nevertheless, training the <b>transformer</b> with a cell <b>graph</b> <b>presents</b> <b>another</b> challenge. Poorly initialized features can lead to noisy <b>self-attention</b> scores and inferior convergence, particularly when processing the cell <b>graphs</b> <b>with</b> <b>numerous</b> connections. Thus, we further propose a novel topology-aware pretraining method that leverages a <b>graph</b> <b>convolutional</b> <b>network</b> <b>(GCN)</b> to learn a feature extractor. The pre-trained features may suppress unreasonable correlations and hence ease the <b>finetuning</b> of CGT. Experimental results suggest that the proposed cell <b>graph</b> <b>transformer</b> <b>with</b> topology-aware pretraining significantly improves the nuclei classification results, and achieves the state-of-the-art performance. Code and models are available at <a href=https://github.com/lhaof/CGT>https://github.com/lhaof/CGT</a></p></p class="citation"></blockquote><h3 id=245--91316-modality-aware-integration-with-large-language-models-for-knowledge-based-visual-question-answering-junnan-dong-et-al-2024>(2/45 | 91/316) Modality-Aware Integration with Large Language Models for Knowledge-based Visual Question Answering (Junnan Dong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Junnan Dong, Qinggang Zhang, Huachi Zhou, Daochen Zha, Pai Zheng, Xiao Huang. (2024)<br><strong>Modality-Aware Integration with Large Language Models for Knowledge-based Visual Question Answering</strong><br><button class=copy-to-clipboard title="Modality-Aware Integration with Large Language Models for Knowledge-based Visual Question Answering" index=91>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-91 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CL, cs-CV, cs-IR, cs-LG, cs.CV<br>Keyword Score: 82<br>Keywords: Graph, Benchmarking, Knowledge Graph, Knowledge Graph, Multi-modal, Multi-modal, Question Answering, Reasoning, Visual Question Answering, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12728v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12728v1.pdf filename=2402.12728v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Knowledge-based</b> <b>visual</b> <b>question</b> <b>answering</b> (KVQA) has been extensively studied to answer <b>visual</b> <b>questions</b> <b>with</b> external <b>knowledge,</b> <b>e.g.,</b> <b>knowledge</b> <b>graphs</b> <b>(KGs).</b> While several attempts have been proposed to leverage <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> as an implicit <b>knowledge</b> <b>source,</b> it remains challenging since <b>LLMs</b> may generate hallucinations. Moreover, multiple <b>knowledge</b> <b>sources,</b> e.g., images, <b>KGs</b> and <b>LLMs,</b> cannot be readily aligned for complex scenarios. To tackle these, we present a novel modality-aware integration with <b>LLMs</b> for KVQA (MAIL). It carefully leverages <b>multimodal</b> <b>knowledge</b> <b>for</b> both image understanding and <b>knowledge</b> <b>reasoning.</b> Specifically, (i) we propose a two-stage <b>prompting</b> strategy with <b>LLMs</b> to densely embody the image into a scene <b>graph</b> with detailed <b>visual</b> <b>features;</b> <b>(ii)</b> We construct a coupled concept <b>graph</b> by linking the mentioned entities with external facts. (iii) A tailored pseudo-siamese <b>graph</b> medium fusion is designed for sufficient <b>multimodal</b> fusion. We utilize the shared mentioned entities in two <b>graphs</b> as mediums to bridge a tight inter-modal exchange, while maximally preserving insightful intra-modal learning by constraining the fusion within mediums. Extensive experiments on two <b>benchmark</b> datasets show the superiority of MAIL with 24x less resources.</p></p class="citation"></blockquote><h3 id=345--92316-countercurate-enhancing-physical-and-semantic-visio-linguistic-compositional-reasoning-via-counterfactual-examples-jianrui-zhang-et-al-2024>(3/45 | 92/316) CounterCurate: Enhancing Physical and Semantic Visio-Linguistic Compositional Reasoning via Counterfactual Examples (Jianrui Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jianrui Zhang, Mu Cai, Tengyang Xie, Yong Jae Lee. (2024)<br><strong>CounterCurate: Enhancing Physical and Semantic Visio-Linguistic Compositional Reasoning via Counterfactual Examples</strong><br><button class=copy-to-clipboard title="CounterCurate: Enhancing Physical and Semantic Visio-Linguistic Compositional Reasoning via Counterfactual Examples" index=92>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-92 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CL, cs-CV, cs-LG, cs.CV<br>Keyword Score: 69<br>Keywords: Benchmarking, Counter-factual, Data Augmentation, Fine-tuning, Multi-modal, Multi-modal, GPT, Reasoning, Text Generation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13254v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13254v1.pdf filename=2402.13254v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We propose CounterCurate, a framework to comprehensively improve the visio-linguistic compositional <b>reasoning</b> capability for both contrastive and generative <b>multimodal</b> models. In particular, we identify two under-explored critical problems: the neglect of the physically grounded <b>reasoning</b> (counting and position understanding) and the potential of using highly capable <b>text</b> <b>and</b> image generation models for semantic <b>counterfactual</b> <b>fine-tuning.</b> Our work pioneers an approach that addresses these gaps. We first spotlight the near-chance performance of <b>multimodal</b> models like CLIP and LLaVA in physically grounded compositional <b>reasoning.</b> We then apply simple <b>data</b> <b>augmentation</b> using a grounded image generation model, GLIGEN, to generate <b>finetuning</b> <b>data,</b> <b>resulting</b> in significant performance improvements: +33% and +37% for CLIP and LLaVA, respectively, on our newly curated Flickr30k-Positions <b>benchmark.</b> Moreover, we exploit the capabilities of high-performing <b>text</b> <b>generation</b> and image generation models, specifically <b>GPT-4V</b> and DALLE-3, to curate challenging semantic <b>counterfactuals,</b> thereby further enhancing compositional <b>reasoning</b> capabilities on <b>benchmarks</b> such as SugarCrepe, where CounterCurate outperforms <b>GPT-4V.</b></p></p class="citation"></blockquote><h3 id=445--93316-mulan-multimodal-llm-agent-for-progressive-multi-object-diffusion-sen-li-et-al-2024>(4/45 | 93/316) MuLan: Multimodal-LLM Agent for Progressive Multi-Object Diffusion (Sen Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sen Li, Ruochen Wang, Cho-Jui Hsieh, Minhao Cheng, Tianyi Zhou. (2024)<br><strong>MuLan: Multimodal-LLM Agent for Progressive Multi-Object Diffusion</strong><br><button class=copy-to-clipboard title="MuLan: Multimodal-LLM Agent for Progressive Multi-Object Diffusion" index=93>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-93 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 69<br>Keywords: Diffusion Model, Benchmarking, Multi-modal, Multi-modal, Text2image, Large Language Model, Large Language Model, Prompt, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12741v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12741v1.pdf filename=2402.12741v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Existing <b>text-to-image</b> models still struggle to generate images of multiple objects, especially in handling their spatial positions, relative sizes, overlapping, and attribute bindings. In this paper, we develop a training-free <b>Multimodal-LLM</b> agent (MuLan) to address these challenges by progressive multi-object generation with planning and feedback control, like a human painter. MuLan harnesses a <b>large</b> <b>language</b> <b>model</b> <b>(LLM)</b> to decompose a <b>prompt</b> to a sequence of sub-tasks, each generating only one object conditioned on previously generated objects by stable <b>diffusion.</b> <b>Unlike</b> existing <b>LLM-grounded</b> methods, MuLan only produces a high-level plan at the beginning while the exact size and location of each object are determined by an <b>LLM</b> and attention guidance upon each sub-task. Moreover, MuLan adopts a <b>vision-language</b> model (VLM) to provide feedback to the image generated in each sub-task and control the <b>diffusion</b> <b>model</b> to re-generate the image if it violates the original <b>prompt.</b> Hence, each model in every step of MuLan only needs to address an easy sub-task it is specialized for. We collect 200 <b>prompts</b> containing multi-objects with spatial relationships and attribute bindings from different <b>benchmarks</b> to evaluate MuLan. The results demonstrate the superiority of MuLan in generating multiple objects over baselines. The code is available on <a href=https://github.com/measure-infinity/mulan-code>https://github.com/measure-infinity/mulan-code</a>.</p></p class="citation"></blockquote><h3 id=545--94316-cross-domain-transfer-learning-with-corte-consistent-and-reliable-transfer-from-black-box-to-lightweight-segmentation-model-claudia-cuttano-et-al-2024>(5/45 | 94/316) Cross-Domain Transfer Learning with CoRTe: Consistent and Reliable Transfer from Black-Box to Lightweight Segmentation Model (Claudia Cuttano et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Claudia Cuttano, Antonio Tavera, Fabio Cermelli, Giuseppe Averta, Barbara Caputo. (2024)<br><strong>Cross-Domain Transfer Learning with CoRTe: Consistent and Reliable Transfer from Black-Box to Lightweight Segmentation Model</strong><br><button class=copy-to-clipboard title="Cross-Domain Transfer Learning with CoRTe: Consistent and Reliable Transfer from Black-Box to Lightweight Segmentation Model" index=94>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-94 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 58<br>Keywords: Benchmarking, Black Box, Knowledge Distillation, Low-Resource, Transfer Learning, Unsupervised Learning, Domain Adaptation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13122v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13122v1.pdf filename=2402.13122v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Many practical applications require training of semantic segmentation models on unlabelled datasets and their execution on <b>low-resource</b> hardware. <b>Distillation</b> from a trained source model may represent a solution for the first but does not account for the different distribution of the training data. <b>Unsupervised</b> <b>domain</b> <b>adaptation</b> (UDA) techniques claim to solve the <b>domain</b> <b>shift,</b> but in most cases assume the availability of the source data or an accessible white-box source model, which in practical applications are often unavailable for commercial and/or safety reasons. In this paper, we investigate a more challenging setting in which a lightweight model has to be trained on a target unlabelled dataset for semantic segmentation, under the assumption that we have access only to <b>black-box</b> <b>source</b> model predictions. Our method, named CoRTe, consists of (i) a pseudo-labelling function that extracts reliable knowledge from the <b>black-box</b> <b>source</b> model using its relative confidence, (ii) a pseudo label refinement method to retain and enhance the novel information learned by the student model on the target data, and (iii) a consistent training of the model using the extracted pseudo labels. We <b>benchmark</b> CoRTe on two synthetic-to-real settings, demonstrating remarkable results when using <b>black-box</b> <b>models</b> to <b>transfer</b> <b>knowledge</b> on lightweight models for a target data distribution.</p></p class="citation"></blockquote><h3 id=645--95316-cst-calibration-side-tuning-for-parameter-and-memory-efficient-transfer-learning-feng-chen-2024>(6/45 | 95/316) CST: Calibration Side-Tuning for Parameter and Memory Efficient Transfer Learning (Feng Chen, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Feng Chen. (2024)<br><strong>CST: Calibration Side-Tuning for Parameter and Memory Efficient Transfer Learning</strong><br><button class=copy-to-clipboard title="CST: Calibration Side-Tuning for Parameter and Memory Efficient Transfer Learning" index=95>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-95 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 53<br>Keywords: Object Detection, Benchmarking, Fine-tuning, Fine-tuning, Transfer Learning, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12736v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12736v1.pdf filename=2402.12736v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Achieving a universally high accuracy in <b>object</b> <b>detection</b> is quite challenging, and the mainstream focus in the industry currently lies on detecting specific classes of <b>objects.</b> <b>However,</b> deploying one or multiple <b>object</b> <b>detection</b> networks requires a certain amount of GPU memory for training and storage capacity for inference. This presents challenges in terms of how to effectively coordinate multiple <b>object</b> <b>detection</b> tasks under resource-constrained conditions. This paper introduces a lightweight <b>fine-tuning</b> strategy called Calibration side tuning, which integrates aspects of adapter tuning and side tuning to adapt the successful techniques employed in <b>transformers</b> for use with ResNet. The Calibration side tuning architecture that incorporates maximal transition calibration, utilizing a small number of additional parameters to enhance network performance while maintaining a smooth training process. Furthermore, this paper has conducted an analysis on multiple <b>fine-tuning</b> strategies and have implemented their application within ResNet, thereby expanding the research on <b>fine-tuning</b> strategies for <b>object</b> <b>detection</b> networks. Besides, this paper carried out extensive experiments using five <b>benchmark</b> datasets. The experimental results demonstrated that this method outperforms other compared state-of-the-art techniques, and a better balance between the complexity and performance of the <b>finetune</b> schemes is achieved.</p></p class="citation"></blockquote><h3 id=745--96316-visual-style-prompting-with-swapping-self-attention-jaeseok-jeong-et-al-2024>(7/45 | 96/316) Visual Style Prompting with Swapping Self-Attention (Jaeseok Jeong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jaeseok Jeong, Junho Kim, Yunjey Choi, Gayoung Lee, Youngjung Uh. (2024)<br><strong>Visual Style Prompting with Swapping Self-Attention</strong><br><button class=copy-to-clipboard title="Visual Style Prompting with Swapping Self-Attention" index=96>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-96 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 50<br>Keywords: Diffusion Model, Fine-tuning, Text2image, Prompt, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12974v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12974v2.pdf filename=2402.12974v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the evolving domain of <b>text-to-image</b> generation, <b>diffusion</b> <b>models</b> have emerged as powerful tools in content creation. Despite their remarkable capability, existing models still face challenges in achieving controlled generation with a consistent style, requiring costly <b>fine-tuning</b> or often inadequately transferring the visual elements due to content leakage. To address these challenges, we propose a novel approach, \ours, to produce a diverse range of images while maintaining specific style elements and nuances. During the denoising process, we keep the query from original features while swapping the key and value with those from reference features in the late <b>self-attention</b> layers. This approach allows for the visual style <b>prompting</b> without any <b>fine-tuning,</b> ensuring that generated images maintain a faithful style. Through extensive evaluation across various styles and text <b>prompts,</b> our method demonstrates superiority over existing approaches, best reflecting the style of the references and ensuring that resulting images match the text <b>prompts</b> most accurately. Our project page is available <a href=https://curryjung.github.io/VisualStylePrompt/>https://curryjung.github.io/VisualStylePrompt/</a>.</p></p class="citation"></blockquote><h3 id=845--97316-clipping-the-deception-adapting-vision-language-models-for-universal-deepfake-detection-sohail-ahmed-khan-et-al-2024>(8/45 | 97/316) CLIPping the Deception: Adapting Vision-Language Models for Universal Deepfake Detection (Sohail Ahmed Khan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sohail Ahmed Khan, Duc-Tien Dang-Nguyen. (2024)<br><strong>CLIPping the Deception: Adapting Vision-Language Models for Universal Deepfake Detection</strong><br><button class=copy-to-clipboard title="CLIPping the Deception: Adapting Vision-Language Models for Universal Deepfake Detection" index=97>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-97 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 50<br>Keywords: Diffusion Model, Generative Adversarial Network, Generative Adversarial Network, Prompt, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12927v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12927v1.pdf filename=2402.12927v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The recent advancements in <b>Generative</b> <b>Adversarial</b> <b>Networks</b> <b>(GANs)</b> and the emergence of <b>Diffusion</b> <b>models</b> have significantly streamlined the production of highly realistic and widely accessible synthetic content. As a result, there is a pressing need for effective general purpose detection mechanisms to mitigate the potential risks posed by deepfakes. In this paper, we explore the effectiveness of pre-trained <b>vision-language</b> models (VLMs) when paired with recent adaptation methods for universal deepfake detection. Following previous studies in this domain, we employ only a single dataset (ProGAN) in order to adapt CLIP for deepfake detection. However, in contrast to prior research, which rely solely on the visual part of CLIP while ignoring its textual component, our analysis reveals that retaining the text part is crucial. Consequently, the simple and lightweight <b>Prompt</b> Tuning based adaptation strategy that we employ outperforms the previous SOTA approach by 5.01% mAP and 6.61% accuracy while utilizing less than one third of the training data (200k images as compared to 720k). To assess the real-world applicability of our proposed models, we conduct a comprehensive evaluation across various scenarios. This involves rigorous testing on images sourced from 21 distinct datasets, including those generated by <b>GANs-based,</b> <b>Diffusion-based</b> <b>and</b> Commercial tools.</p></p class="citation"></blockquote><h3 id=945--98316-yolo-ant-a-lightweight-detector-via-depthwise-separable-convolutional-and-large-kernel-design-for-antenna-interference-source-detection-xiaoyu-tang-et-al-2024>(9/45 | 98/316) YOLO-Ant: A Lightweight Detector via Depthwise Separable Convolutional and Large Kernel Design for Antenna Interference Source Detection (Xiaoyu Tang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiaoyu Tang, Xingming Chen, Jintao Cheng, Jin Wu, Rui Fan, Chengxi Zhang, Zebo Zhou. (2024)<br><strong>YOLO-Ant: A Lightweight Detector via Depthwise Separable Convolutional and Large Kernel Design for Antenna Interference Source Detection</strong><br><button class=copy-to-clipboard title="YOLO-Ant: A Lightweight Detector via Depthwise Separable Convolutional and Large Kernel Design for Antenna Interference Source Detection" index=98>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-98 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 50<br>Keywords: Yolo, Object Detection, Convolution, Convolutional Neural Network, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12641v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12641v1.pdf filename=2402.12641v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the era of 5G communication, removing interference sources that affect communication is a resource-intensive task. The rapid development of computer vision has enabled unmanned aerial vehicles to perform various high-altitude detection tasks. Because the field of <b>object</b> <b>detection</b> for antenna interference sources has not been fully explored, this industry lacks dedicated learning samples and detection models for this specific task. In this article, an antenna dataset is created to address important antenna interference source detection issues and serves as the basis for subsequent research. We introduce <b>YOLO-Ant,</b> a lightweight <b>CNN</b> and <b>transformer</b> hybrid detector specifically designed for antenna interference source detection. Specifically, we initially formulated a lightweight design for the network depth and width, ensuring that subsequent investigations were conducted within a lightweight framework. Then, we propose a DSLK-Block module based on depthwise separable <b>convolution</b> and large <b>convolution</b> kernels to enhance the network&rsquo;s feature extraction ability, effectively improving small <b>object</b> <b>detection.</b> To address challenges such as complex backgrounds and large interclass differences in antenna detection, we construct DSLKVit-Block, a powerful feature extraction module that combines DSLK-Block and <b>transformer</b> structures. Considering both its lightweight design and accuracy, our method not only achieves optimal performance on the antenna dataset but also yields competitive results on public datasets.</p></p class="citation"></blockquote><h3 id=1045--99316-how-easy-is-it-to-fool-your-multimodal-llms-an-empirical-analysis-on-deceptive-prompts-yusu-qian-et-al-2024>(10/45 | 99/316) How Easy is It to Fool Your Multimodal LLMs? An Empirical Analysis on Deceptive Prompts (Yusu Qian et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yusu Qian, Haotian Zhang, Yinfei Yang, Zhe Gan. (2024)<br><strong>How Easy is It to Fool Your Multimodal LLMs? An Empirical Analysis on Deceptive Prompts</strong><br><button class=copy-to-clipboard title="How Easy is It to Fool Your Multimodal LLMs? An Empirical Analysis on Deceptive Prompts" index=99>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-99 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CL, cs-CV, cs.CV<br>Keyword Score: 49<br>Keywords: Benchmarking, Multi-modal, Multi-modal, GPT, Gemini, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13220v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13220v1.pdf filename=2402.13220v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The remarkable advancements in <b>Multimodal</b> <b>Large</b> <b>Language</b> <b>Models</b> (MLLMs) have not rendered them immune to challenges, particularly in the context of handling deceptive information in <b>prompts,</b> thus producing hallucinated responses under such conditions. To quantitatively assess this vulnerability, we present MAD-Bench, a carefully curated <b>benchmark</b> that contains 850 test samples divided into 6 categories, such as non-existent objects, count of objects, spatial relationship, and visual confusion. We provide a comprehensive analysis of popular MLLMs, ranging from <b>GPT-4V,</b> <b>Gemini-Pro,</b> to open-sourced models, such as LLaVA-1.5 and CogVLM. Empirically, we observe significant performance gaps between <b>GPT-4V</b> and other models; and previous robust instruction-tuned models, such as LRV-Instruction and LLaVA-RLHF, are not effective on this new <b>benchmark.</b> While <b>GPT-4V</b> achieves 75.02% accuracy on MAD-Bench, the accuracy of any other model in our experiments ranges from 5% to 35%. We further propose a remedy that adds an additional paragraph to the deceptive <b>prompts</b> to encourage models to think twice before answering the question. Surprisingly, this simple method can even double the accuracy; however, the absolute numbers are still too low to be satisfactory. We hope MAD-Bench can serve as a valuable <b>benchmark</b> to stimulate further research to enhance models&rsquo; resilience against deceptive <b>prompts.</b></p></p class="citation"></blockquote><h3 id=1145--100316-olvit-multi-modal-state-tracking-via-attention-based-embeddings-for-video-grounded-dialog-adnen-abdessaied-et-al-2024>(11/45 | 100/316) OLViT: Multi-Modal State Tracking via Attention-Based Embeddings for Video-Grounded Dialog (Adnen Abdessaied et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Adnen Abdessaied, Manuel von Hochmeister, Andreas Bulling. (2024)<br><strong>OLViT: Multi-Modal State Tracking via Attention-Based Embeddings for Video-Grounded Dialog</strong><br><button class=copy-to-clipboard title="OLViT: Multi-Modal State Tracking via Attention-Based Embeddings for Video-Grounded Dialog" index=100>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-100 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 43<br>Keywords: Multi-modal, Transformer, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13146v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13146v1.pdf filename=2402.13146v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present the Object Language Video <b>Transformer</b> (OLViT) - a novel model for video dialog operating over a <b>multi-modal</b> attention-based dialog state tracker. Existing video dialog models struggle with questions requiring both spatial and temporal localization within videos, long-term temporal <b>reasoning,</b> and accurate object tracking across multiple dialog turns. OLViT addresses these challenges by maintaining a global dialog state based on the output of an Object State Tracker (OST) and a Language State Tracker (LST): while the OST attends to the most important objects within the video, the LST keeps track of the most important linguistic co-references to previous dialog turns. In stark contrast to previous works, our approach is generic by nature and is therefore capable of learning continuous <b>multi-modal</b> dialog state representations of the most relevant objects and rounds. As a result, they can be seamlessly integrated into <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> and offer high flexibility in dealing with different datasets and tasks. Evaluations on the challenging DVD (response classification) and SIMMC 2.1 (response generation) datasets show that OLViT achieves new state-of-the-art performance across both datasets.</p></p class="citation"></blockquote><h3 id=1245--101316-layout-to-image-generation-with-localized-descriptions-using-controlnet-with-cross-attention-control-denis-lukovnikov-et-al-2024>(12/45 | 101/316) Layout-to-Image Generation with Localized Descriptions using ControlNet with Cross-Attention Control (Denis Lukovnikov et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Denis Lukovnikov, Asja Fischer. (2024)<br><strong>Layout-to-Image Generation with Localized Descriptions using ControlNet with Cross-Attention Control</strong><br><button class=copy-to-clipboard title="Layout-to-Image Generation with Localized Descriptions using ControlNet with Cross-Attention Control" index=101>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-101 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: ControlNet, Diffusion Model, Text2image, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13404v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13404v1.pdf filename=2402.13404v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>While <b>text-to-image</b> <b>diffusion</b> <b>models</b> can generate highquality images from textual descriptions, they generally lack fine-grained control over the visual composition of the generated images. Some recent works tackle this problem by training the model to condition the generation process on additional input describing the desired image layout. Arguably the most popular among such methods, <b>ControlNet,</b> enables a high degree of control over the generated image using various types of conditioning inputs (e.g. segmentation maps). However, it still lacks the ability to take into account localized textual descriptions that indicate which image region is described by which phrase in the <b>prompt.</b> In this work, we show the limitations of <b>ControlNet</b> for the layout-to-image task and enable it to use localized descriptions using a training-free approach that modifies the crossattention scores during generation. We adapt and investigate several existing cross-attention control methods in the context of <b>ControlNet</b> and identify shortcomings that cause failure (concept bleeding) or image degradation under specific conditions. To address these shortcomings, we develop a novel cross-attention manipulation method in order to maintain image quality while improving control. Qualitative and quantitative experimental studies focusing on challenging cases are presented, demonstrating the effectiveness of the investigated general approach, and showing the improvements obtained by the proposed cross-attention control method.</p></p class="citation"></blockquote><h3 id=1345--102316-combining-unsupervised-and-supervised-learning-in-microscopy-enables-defect-analysis-of-a-full-4h-sic-wafer-binh-duong-nguyen-et-al-2024>(13/45 | 102/316) Combining unsupervised and supervised learning in microscopy enables defect analysis of a full 4H-SiC wafer (Binh Duong Nguyen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Binh Duong Nguyen, Johannes Steiner, Peter Wellmann, Stefan Sandfeld. (2024)<br><strong>Combining unsupervised and supervised learning in microscopy enables defect analysis of a full 4H-SiC wafer</strong><br><button class=copy-to-clipboard title="Combining unsupervised and supervised learning in microscopy enables defect analysis of a full 4H-SiC wafer" index=102>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-102 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cond-mat-mtrl-sci, cs-CV, cs-LG, cs.CV<br>Keyword Score: 40<br>Keywords: Object Detection, Supervised Learning, Supervised Learning, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13353v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13353v1.pdf filename=2402.13353v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Detecting and analyzing various defect types in semiconductor materials is an important prerequisite for understanding the underlying mechanisms as well as tailoring the production processes. Analysis of microscopy images that reveal defects typically requires image analysis tasks such as segmentation and <b>object</b> <b>detection.</b> With the permanently increasing amount of data that is produced by experiments, handling these tasks manually becomes more and more impossible. In this work, we combine various image analysis and data mining techniques for creating a robust and accurate, automated image analysis pipeline. This allows for extracting the type and position of all defects in a microscopy image of a KOH-etched 4H-SiC wafer that was stitched together from approximately 40,000 individual images.</p></p class="citation"></blockquote><h3 id=1445--103316-solarpanel-segmentation-self-supervised-learning-for-imperfect-datasets-sankarshanaa-sagaram-et-al-2024>(14/45 | 103/316) SolarPanel Segmentation :Self-Supervised Learning for Imperfect Datasets (Sankarshanaa Sagaram et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sankarshanaa Sagaram, Aditya Kasliwal, Krish Didwania, Laven Srivastava, Pallavi Kailas, Ujjwal Verma. (2024)<br><strong>SolarPanel Segmentation :Self-Supervised Learning for Imperfect Datasets</strong><br><button class=copy-to-clipboard title="SolarPanel Segmentation :Self-Supervised Learning for Imperfect Datasets" index=103>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-103 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Self-supervised Learning, Self-supervised Learning, Supervised Learning, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12843v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12843v1.pdf filename=2402.12843v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The increasing adoption of solar energy necessitates advanced methodologies for monitoring and maintenance to ensure optimal performance of solar panel installations. A critical component in this context is the accurate segmentation of solar panels from aerial or satellite imagery, which is essential for identifying operational issues and assessing efficiency. This paper addresses the significant challenges in panel segmentation, particularly the scarcity of annotated data and the labour-intensive nature of manual annotation for <b>supervised</b> <b>learning.</b> We explore and apply <b>Self-Supervised</b> <b>Learning</b> (SSL) to solve these challenges. We demonstrate that SSL significantly enhances model generalization under various conditions and reduces dependency on manually annotated data, paving the way for robust and adaptable solar panel segmentation solutions.</p></p class="citation"></blockquote><h3 id=1545--104316-rhythmformer-extracting-rppg-signals-based-on-hierarchical-temporal-periodic-transformer-bochao-zou-et-al-2024>(15/45 | 104/316) RhythmFormer: Extracting rPPG Signals Based on Hierarchical Temporal Periodic Transformer (Bochao Zou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bochao Zou, Zizheng Guo, Jiansheng Chen, Huimin Ma. (2024)<br><strong>RhythmFormer: Extracting rPPG Signals Based on Hierarchical Temporal Periodic Transformer</strong><br><button class=copy-to-clipboard title="RhythmFormer: Extracting rPPG Signals Based on Hierarchical Temporal Periodic Transformer" index=104>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-104 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Convolution, Convolutional Neural Network, Transformer, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12788v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12788v1.pdf filename=2402.12788v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Remote photoplethysmography (rPPG) is a non-contact method for detecting physiological signals based on facial videos, holding high potential in various applications such as healthcare, affective computing, anti-spoofing, etc. Due to the periodicity nature of rPPG, the long-range dependency capturing capacity of the <b>Transformer</b> was assumed to be advantageous for such signals. However, existing approaches have not conclusively demonstrated the superior performance of <b>Transformer</b> over traditional <b>convolutional</b> <b>neural</b> <b>network</b> methods, this gap may stem from a lack of thorough exploration of rPPG periodicity. In this paper, we propose RhythmFormer, a fully end-to-end <b>transformer-based</b> method for extracting rPPG signals by explicitly leveraging the quasi-periodic nature of rPPG. The core module, Hierarchical Temporal Periodic <b>Transformer,</b> hierarchically extracts periodic features from multiple temporal scales. It utilizes dynamic sparse attention based on periodicity in the temporal domain, allowing for fine-grained modeling of rPPG features. Furthermore, a fusion stem is proposed to guide <b>self-attention</b> to rPPG features effectively, and it can be easily transferred to existing methods to enhance their performance significantly. RhythmFormer achieves state-of-the-art performance with fewer parameters and reduced computational complexity in comprehensive experiments compared to previous approaches. The codes are available at <a href=https://github.com/zizheng-guo/RhythmFormer>https://github.com/zizheng-guo/RhythmFormer</a>.</p></p class="citation"></blockquote><h3 id=1645--105316-learning-domain-invariant-temporal-dynamics-for-few-shot-action-recognition-yuke-li-et-al-2024>(16/45 | 105/316) Learning Domain-Invariant Temporal Dynamics for Few-Shot Action Recognition (Yuke Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuke Li, Guangyi Chen, Ben Abramowitz, Stefano Anzellott, Donglai Wei. (2024)<br><strong>Learning Domain-Invariant Temporal Dynamics for Few-Shot Action Recognition</strong><br><button class=copy-to-clipboard title="Learning Domain-Invariant Temporal Dynamics for Few-Shot Action Recognition" index=105>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-105 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Distribution Shift, Distribution Shift, Few-shot, Knowledge Transfer, Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12706v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12706v1.pdf filename=2402.12706v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Few-shot</b> action recognition aims at quickly adapting a pre-trained model to the novel data with a <b>distribution</b> <b>shift</b> using only a limited number of samples. Key challenges include how to identify and leverage the transferable <b>knowledge</b> <b>learned</b> by the pre-trained model. Our central hypothesis is that temporal invariance in the dynamic system between latent variables lends itself to transferability (domain-invariance). We therefore propose DITeD, or Domain-Invariant Temporal Dynamics for <b>knowledge</b> <b>transfer.</b> To detect the temporal invariance part, we propose a generative framework with a two-stage training strategy during pre-training. Specifically, we explicitly model invariant dynamics including temporal dynamic generation and transitions, and the variant visual and domain encoders. Then we pre-train the model with the <b>self-supervised</b> signals to learn the representation. After that, we fix the whole representation model and tune the classifier. During adaptation, we fix the transferable temporal dynamics and update the image encoder. The efficacy of our approach is revealed by the superior accuracy of DITeD over leading alternatives across standard <b>few-shot</b> action recognition datasets. Moreover, we validate that the learned temporal dynamic transition and temporal dynamic generation modules possess transferable qualities.</p></p class="citation"></blockquote><h3 id=1745--106316-a-touch-vision-and-language-dataset-for-multimodal-alignment-letian-fu-et-al-2024>(17/45 | 106/316) A Touch, Vision, and Language Dataset for Multimodal Alignment (Letian Fu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Letian Fu, Gaurav Datta, Huang Huang, William Chung-Ho Panitch, Jaimyn Drake, Joseph Ortiz, Mustafa Mukadam, Mike Lambeta, Roberto Calandra, Ken Goldberg. (2024)<br><strong>A Touch, Vision, and Language Dataset for Multimodal Alignment</strong><br><button class=copy-to-clipboard title="A Touch, Vision, and Language Dataset for Multimodal Alignment" index=106>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-106 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-RO, cs.CV<br>Keyword Score: 39<br>Keywords: Benchmarking, Multi-modal, Multi-modal, GPT, Text Generation, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13232v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13232v1.pdf filename=2402.13232v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Touch is an important sensing modality for humans, but it has not yet been incorporated into a <b>multimodal</b> generative language model. This is partially due to the difficulty of obtaining natural language labels for tactile data and the complexity of aligning tactile readings with both visual observations and language descriptions. As a step towards bridging that gap, this work introduces a new dataset of 44K in-the-wild vision-touch pairs, with English language labels annotated by humans (10%) and textual pseudo-labels from <b>GPT-4V</b> (90%). We use this dataset to train a <b>vision-language-aligned</b> tactile encoder for open-vocabulary classification and a touch-vision-language (TVL) model for <b>text</b> <b>generation</b> using the trained encoder. Results suggest that by incorporating touch, the TVL model improves (+29% classification accuracy) touch-vision-language alignment over existing models trained on any pair of those modalities. Although only a small fraction of the dataset is human-labeled, the TVL model demonstrates improved visual-tactile understanding over <b>GPT-4V</b> (+12%) and open-source <b>vision-language</b> models (+32%) on a new touch-vision understanding <b>benchmark.</b> Code and data: <a href=https://tactile-vlm.github.io>https://tactile-vlm.github.io</a>.</p></p class="citation"></blockquote><h3 id=1845--107316-videoprism-a-foundational-visual-encoder-for-video-understanding-long-zhao-et-al-2024>(18/45 | 107/316) VideoPrism: A Foundational Visual Encoder for Video Understanding (Long Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Long Zhao, Nitesh B. Gundavarapu, Liangzhe Yuan, Hao Zhou, Shen Yan, Jennifer J. Sun, Luke Friedman, Rui Qian, Tobias Weyand, Yue Zhao, Rachel Hornung, Florian Schroff, Ming-Hsuan Yang, David A. Ross, Huisheng Wang, Hartwig Adam, Mikhail Sirotenko, Ting Liu, Boqing Gong. (2024)<br><strong>VideoPrism: A Foundational Visual Encoder for Video Understanding</strong><br><button class=copy-to-clipboard title="VideoPrism: A Foundational Visual Encoder for Video Understanding" index=107>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-107 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 33<br>Keywords: Benchmarking, Knowledge Distillation, Automatic Speech Recognition, Question Answering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13217v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13217v1.pdf filename=2402.13217v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce VideoPrism, a general-purpose video encoder that tackles diverse video understanding tasks with a single frozen model. We pretrain VideoPrism on a heterogeneous corpus containing 36M high-quality video-caption pairs and 582M video clips with noisy parallel text (e.g., <b>ASR</b> transcripts). The pretraining approach improves upon masked autoencoding by global-local <b>distillation</b> of semantic video embeddings and a token shuffling scheme, enabling VideoPrism to focus primarily on the video modality while leveraging the invaluable text associated with videos. We extensively test VideoPrism on four broad groups of video understanding tasks, from web video <b>question</b> <b>answering</b> to CV for science, achieving state-of-the-art performance on 30 out of 33 video understanding <b>benchmarks.</b></p></p class="citation"></blockquote><h3 id=1945--108316-slot-vlm-slowfast-slots-for-video-language-modeling-jiaqi-xu-et-al-2024>(19/45 | 108/316) Slot-VLM: SlowFast Slots for Video-Language Modeling (Jiaqi Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiaqi Xu, Cuiling Lan, Wenxuan Xie, Xuejin Chen, Yan Lu. (2024)<br><strong>Slot-VLM: SlowFast Slots for Video-Language Modeling</strong><br><button class=copy-to-clipboard title="Slot-VLM: SlowFast Slots for Video-Language Modeling" index=108>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-108 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Question Answering, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13088v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13088v1.pdf filename=2402.13088v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Video-Language Models (VLMs), powered by the advancements in <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs),</b> are charting new frontiers in video understanding. A pivotal challenge is the development of an efficient method to encapsulate video content into a set of representative tokens to align with <b>LLMs.</b> In this work, we introduce Slot-VLM, a novel framework designed to generate semantically decomposed video tokens, in terms of object-wise and event-wise visual representations, to facilitate <b>LLM</b> inference. Particularly, we design a SlowFast Slots module, i.e., SF-Slots, that adaptively aggregates the dense video tokens from the CLIP vision encoder to a set of representative slots. In order to take into account both the spatial object details and the varied temporal dynamics, SF-Slots is built with a dual-branch structure. The Slow-Slots branch focuses on extracting object-centric slots from features at high spatial resolution but low (slow) frame sample rate, emphasizing detailed object information. Conversely, Fast-Slots branch is engineered to learn event-centric slots from high temporal sample rate but low spatial resolution features. These complementary slots are combined to form the vision context, serving as the input to the <b>LLM</b> for efficient <b>question</b> <b>answering.</b> Our experimental results demonstrate the effectiveness of our Slot-VLM, which achieves the state-of-the-art performance on video question-answering.</p></p class="citation"></blockquote><h3 id=2045--109316-mvdiffusion-a-dense-high-resolution-multi-view-diffusion-model-for-single-or-sparse-view-3d-object-reconstruction-shitao-tang-et-al-2024>(20/45 | 109/316) MVDiffusion++: A Dense High-resolution Multi-view Diffusion Model for Single or Sparse-view 3D Object Reconstruction (Shitao Tang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shitao Tang, Jiacheng Chen, Dilin Wang, Chengzhou Tang, Fuyang Zhang, Yuchen Fan, Vikas Chandra, Yasutaka Furukawa, Rakesh Ranjan. (2024)<br><strong>MVDiffusion++: A Dense High-resolution Multi-view Diffusion Model for Single or Sparse-view 3D Object Reconstruction</strong><br><button class=copy-to-clipboard title="MVDiffusion++: A Dense High-resolution Multi-view Diffusion Model for Single or Sparse-view 3D Object Reconstruction" index=109>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-109 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Diffusion Model, Text2image, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12712v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12712v1.pdf filename=2402.12712v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents a neural architecture MVDiffusion++ for 3D object reconstruction that synthesizes dense and high-resolution views of an object given one or a few images without camera poses. MVDiffusion++ achieves superior flexibility and scalability with two surprisingly simple ideas: 1) A <code>pose-free architecture'' where standard &lt;b>self-attention&lt;/b> among 2D latent features learns 3D consistency across an arbitrary number of conditional and generation views without explicitly using camera pose information; and 2) A </code>view dropout strategy&rsquo;&rsquo; that discards a substantial number of output views during training, which reduces the training-time memory footprint and enables dense and high-resolution view synthesis at test time. We use the Objaverse for training and the Google Scanned Objects for evaluation with standard novel view synthesis and 3D reconstruction metrics, where MVDiffusion++ significantly outperforms the current state of the arts. We also demonstrate a text-to-3D application example by combining MVDiffusion++ with a <b>text-to-image</b> generative model.</p></p class="citation"></blockquote><h3 id=2145--110316-advancing-monocular-video-based-gait-analysis-using-motion-imitation-with-physics-based-simulation-nikolaos-smyrnakis-et-al-2024>(21/45 | 110/316) Advancing Monocular Video-Based Gait Analysis Using Motion Imitation with Physics-Based Simulation (Nikolaos Smyrnakis et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nikolaos Smyrnakis, Tasos Karakostas, R. James Cotton. (2024)<br><strong>Advancing Monocular Video-Based Gait Analysis Using Motion Imitation with Physics-Based Simulation</strong><br><button class=copy-to-clipboard title="Advancing Monocular Video-Based Gait Analysis Using Motion Imitation with Physics-Based Simulation" index=110>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-110 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Reinforcement Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12676v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12676v1.pdf filename=2402.12676v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Gait analysis from videos obtained from a smartphone would open up many clinical opportunities for detecting and quantifying gait impairments. However, existing approaches for estimating gait parameters from videos can produce physically implausible results. To overcome this, we train a policy using <b>reinforcement</b> <b>learning</b> to control a physics <b>simulation</b> of human movement to replicate the movement seen in video. This forces the inferred movements to be physically plausible, while improving the accuracy of the inferred step length and walking velocity.</p></p class="citation"></blockquote><h3 id=2245--111316-efficient-parameter-mining-and-freezing-for-continual-object-detection-angelo-g-menezes-et-al-2024>(22/45 | 111/316) Efficient Parameter Mining and Freezing for Continual Object Detection (Angelo G. Menezes et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Angelo G. Menezes, Augusto J. Peterlevitz, Mateus A. Chinelatto, André C. P. L. F. de Carvalho. (2024)<br><strong>Efficient Parameter Mining and Freezing for Continual Object Detection</strong><br><button class=copy-to-clipboard title="Efficient Parameter Mining and Freezing for Continual Object Detection" index=111>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-111 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Object Detection, Continual Learning, Pruning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12624v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12624v1.pdf filename=2402.12624v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Continual</b> <b>Object</b> <b>Detection</b> is essential for enabling intelligent agents to interact proactively with humans in real-world settings. While parameter-isolation strategies have been extensively explored in the context of <b>continual</b> <b>learning</b> for classification, they have yet to be fully harnessed for incremental <b>object</b> <b>detection</b> scenarios. Drawing inspiration from prior research that focused on mining individual neuron responses and integrating insights from recent developments in neural <b>pruning,</b> we proposed efficient ways to identify which layers are the most important for a network to maintain the performance of a detector across sequential updates. The presented findings highlight the substantial advantages of layer-level parameter isolation in facilitating incremental learning within <b>object</b> <b>detection</b> models, offering promising avenues for future research and application in real-world scenarios.</p></p class="citation"></blockquote><h3 id=2345--112316-model-composition-for-multimodal-large-language-models-chi-chen-et-al-2024>(23/45 | 112/316) Model Composition for Multimodal Large Language Models (Chi Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chi Chen, Yiyang Du, Zheng Fang, Ziyue Wang, Fuwen Luo, Peng Li, Ming Yan, Ji Zhang, Fei Huang, Maosong Sun, Yang Liu. (2024)<br><strong>Model Composition for Multimodal Large Language Models</strong><br><button class=copy-to-clipboard title="Model Composition for Multimodal Large Language Models" index=112>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-112 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CL, cs-CV, cs.CV<br>Keyword Score: 29<br>Keywords: Benchmarking, Multi-modal, Multi-modal, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12750v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12750v1.pdf filename=2402.12750v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent developments in <b>Multimodal</b> <b>Large</b> <b>Language</b> <b>Models</b> (MLLMs) have shown rapid progress, moving towards the goal of creating versatile MLLMs that understand inputs from various modalities. However, existing methods typically rely on joint training with paired <b>multimodal</b> instruction data, which is resource-intensive and challenging to extend to new modalities. In this paper, we propose a new paradigm through the model composition of existing MLLMs to create a new model that retains the modal understanding capabilities of each original model. Our basic implementation, NaiveMC, demonstrates the effectiveness of this paradigm by reusing modality encoders and merging <b>LLM</b> parameters. Furthermore, we introduce DAMC to address parameter interference and mismatch issues during the merging process, thereby enhancing the model performance. To facilitate research in this area, we propose MCUB, a <b>benchmark</b> for assessing ability of MLLMs to understand inputs from diverse modalities. Experiments on this <b>benchmark</b> and four other <b>multimodal</b> understanding tasks show significant improvements over baselines, proving that model composition can create a versatile model capable of processing inputs from multiple modalities.</p></p class="citation"></blockquote><h3 id=2445--113316-visual-reasoning-in-object-centric-deep-neural-networks-a-comparative-cognition-approach-guillermo-puebla-et-al-2024>(24/45 | 113/316) Visual Reasoning in Object-Centric Deep Neural Networks: A Comparative Cognition Approach (Guillermo Puebla et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Guillermo Puebla, Jeffrey S. Bowers. (2024)<br><strong>Visual Reasoning in Object-Centric Deep Neural Networks: A Comparative Cognition Approach</strong><br><button class=copy-to-clipboard title="Visual Reasoning in Object-Centric Deep Neural Networks: A Comparative Cognition Approach" index=113>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-113 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 25<br>Keywords: Out-of-distribution, Representation Learning, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12675v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12675v1.pdf filename=2402.12675v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Achieving visual <b>reasoning</b> is a long-term goal of artificial intelligence. In the last decade, several studies have applied deep neural networks (DNNs) to the task of learning visual relations from images, with modest results in terms of generalization of the relations learned. However, in recent years, object-centric <b>representation</b> <b>learning</b> has been put forward as a way to achieve visual <b>reasoning</b> within the deep learning framework. Object-centric models attempt to model input scenes as compositions of objects and relations between them. To this end, these models use several kinds of attention mechanisms to segregate the individual objects in a scene from the background and from other objects. In this work we tested relation learning and generalization in several object-centric models, as well as a ResNet-50 baseline. In contrast to previous research, which has focused heavily in the same-different task in order to asses relational <b>reasoning</b> in DNNs, we use a set of tasks &ndash; with varying degrees of difficulty &ndash; derived from the comparative cognition literature. Our results show that object-centric models are able to segregate the different objects in a scene, even in many <b>out-of-distribution</b> cases. In our simpler tasks, this improves their capacity to learn and generalize visual relations in comparison to the ResNet-50 baseline. However, object-centric models still struggle in our more difficult tasks and conditions. We conclude that abstract visual <b>reasoning</b> remains an open challenge for DNNs, including object-centric models.</p></p class="citation"></blockquote><h3 id=2545--114316-annotheia-a-semi-automatic-annotation-toolkit-for-audio-visual-speech-technologies-josé-m-acosta-triana-et-al-2024>(25/45 | 114/316) AnnoTheia: A Semi-Automatic Annotation Toolkit for Audio-Visual Speech Technologies (José-M. Acosta-Triana et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>José-M. Acosta-Triana, David Gimeno-Gómez, Carlos-D. Martínez-Hinarejos. (2024)<br><strong>AnnoTheia: A Semi-Automatic Annotation Toolkit for Audio-Visual Speech Technologies</strong><br><button class=copy-to-clipboard title="AnnoTheia: A Semi-Automatic Annotation Toolkit for Audio-Visual Speech Technologies" index=114>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-114 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CL, cs-CV, cs.CV<br>Keyword Score: 23<br>Keywords: Benchmarking, Low-Resource, Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13152v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13152v1.pdf filename=2402.13152v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>More than 7,000 known languages are spoken around the world. However, due to the lack of annotated resources, only a small fraction of them are currently covered by speech technologies. Albeit <b>self-supervised</b> speech representations, recent massive speech corpora collections, as well as the organization of challenges, have alleviated this inequality, most studies are mainly <b>benchmarked</b> on English. This situation is aggravated when tasks involving both acoustic and visual speech modalities are addressed. In order to promote research on <b>low-resource</b> languages for audio-visual speech technologies, we present AnnoTheia, a semi-automatic annotation toolkit that detects when a person speaks on the scene and the corresponding transcription. In addition, to show the complete process of preparing AnnoTheia for a language of interest, we also describe the adaptation of a pre-trained model for active speaker detection to Spanish, using a database not initially conceived for this type of task. The AnnoTheia toolkit, tutorials, and pre-trained models are available on GitHub.</p></p class="citation"></blockquote><h3 id=2645--115316-unicell-universal-cell-nucleus-classification-via-prompt-learning-junjia-huang-et-al-2024>(26/45 | 115/316) UniCell: Universal Cell Nucleus Classification via Prompt Learning (Junjia Huang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Junjia Huang, Haofeng Li, Xiang Wan, Guanbin Li. (2024)<br><strong>UniCell: Universal Cell Nucleus Classification via Prompt Learning</strong><br><button class=copy-to-clipboard title="UniCell: Universal Cell Nucleus Classification via Prompt Learning" index=115>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-115 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 23<br>Keywords: Benchmarking, Prompt, Prompt Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12938v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12938v1.pdf filename=2402.12938v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The recognition of multi-class cell nuclei can significantly facilitate the process of histopathological diagnosis. Numerous pathological datasets are currently available, but their annotations are inconsistent. Most existing methods require individual training on each dataset to deduce the relevant labels and lack the use of common knowledge across datasets, consequently restricting the quality of recognition. In this paper, we propose a universal cell nucleus classification framework (UniCell), which employs a novel <b>prompt</b> <b>learning</b> mechanism to uniformly predict the corresponding categories of pathological images from different dataset domains. In particular, our framework adopts an end-to-end architecture for nuclei detection and classification, and utilizes flexible prediction heads for adapting various datasets. Moreover, we develop a Dynamic <b>Prompt</b> <b>Module</b> (DPM) that exploits the properties of multiple datasets to enhance features. The DPM first integrates the embeddings of datasets and semantic categories, and then employs the integrated <b>prompts</b> <b>to</b> refine image representations, efficiently harvesting the shared knowledge among the related cell types and data sources. Experimental results demonstrate that the proposed method effectively achieves the state-of-the-art results on four nucleus detection and classification <b>benchmarks.</b> Code and models are available at <a href=https://github.com/lhaof/UniCell>https://github.com/lhaof/UniCell</a></p></p class="citation"></blockquote><h3 id=2745--116316-diffusionnocs-managing-symmetry-and-uncertainty-in-sim2real-multi-modal-category-level-pose-estimation-takuya-ikeda-et-al-2024>(27/45 | 116/316) DiffusionNOCS: Managing Symmetry and Uncertainty in Sim2Real Multi-Modal Category-level Pose Estimation (Takuya Ikeda et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Takuya Ikeda, Sergey Zakharov, Tianyi Ko, Muhammad Zubair Irshad, Robert Lee, Katherine Liu, Rares Ambrus, Koichi Nishiwaki. (2024)<br><strong>DiffusionNOCS: Managing Symmetry and Uncertainty in Sim2Real Multi-Modal Category-level Pose Estimation</strong><br><button class=copy-to-clipboard title="DiffusionNOCS: Managing Symmetry and Uncertainty in Sim2Real Multi-Modal Category-level Pose Estimation" index=116>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-116 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-RO, cs.CV<br>Keyword Score: 23<br>Keywords: Diffusion Model, Multi-modal, Probabilistic Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12647v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12647v1.pdf filename=2402.12647v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper addresses the challenging problem of category-level pose estimation. Current state-of-the-art methods for this task face challenges when dealing with symmetric objects and when attempting to generalize to new environments solely through synthetic data training. In this work, we address these challenges by proposing a <b>probabilistic</b> <b>model</b> that relies on <b>diffusion</b> <b>to</b> estimate dense canonical maps crucial for recovering partial object shapes as well as establishing correspondences essential for pose estimation. Furthermore, we introduce critical components to enhance performance by leveraging the strength of the <b>diffusion</b> <b>models</b> with <b>multi-modal</b> input representations. We demonstrate the effectiveness of our method by testing it on a range of real datasets. Despite being trained solely on our generated synthetic data, our approach achieves state-of-the-art performance and unprecedented generalization qualities, outperforming baselines, even those specifically trained on the target domain.</p></p class="citation"></blockquote><h3 id=2845--117316-realcompo-dynamic-equilibrium-between-realism-and-compositionality-improves-text-to-image-diffusion-models-xinchen-zhang-et-al-2024>(28/45 | 117/316) RealCompo: Dynamic Equilibrium between Realism and Compositionality Improves Text-to-Image Diffusion Models (Xinchen Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xinchen Zhang, Ling Yang, Yaqi Cai, Zhaochen Yu, Jiake Xie, Ye Tian, Minkai Xu, Yong Tang, Yujiu Yang, Bin Cui. (2024)<br><strong>RealCompo: Dynamic Equilibrium between Realism and Compositionality Improves Text-to-Image Diffusion Models</strong><br><button class=copy-to-clipboard title="RealCompo: Dynamic Equilibrium between Realism and Compositionality Improves Text-to-Image Diffusion Models" index=117>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-117 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Diffusion Model, Text2image<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12908v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12908v1.pdf filename=2402.12908v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Diffusion</b> <b>models</b> have achieved remarkable advancements in <b>text-to-image</b> generation. However, existing models still have many difficulties when faced with multiple-object compositional generation. In this paper, we propose a new training-free and transferred-friendly <b>text-to-image</b> generation framework, namely RealCompo, which aims to leverage the advantages of <b>text-to-image</b> and layout-to-image models to enhance both realism and compositionality of the generated images. An intuitive and novel balancer is proposed to dynamically balance the strengths of the two models in denoising process, allowing plug-and-play use of any model without extra training. Extensive experiments show that our RealCompo consistently outperforms state-of-the-art <b>text-to-image</b> models and layout-to-image models in multiple-object compositional generation while keeping satisfactory realism and compositionality of the generated images. Code is available at <a href=https://github.com/YangLing0818/RealCompo>https://github.com/YangLing0818/RealCompo</a></p></p class="citation"></blockquote><h3 id=2945--118316-mind-the-exit-pupil-gap-revisiting-the-intrinsics-of-a-standard-plenoptic-camera-tim-michels-et-al-2024>(29/45 | 118/316) Mind the Exit Pupil Gap: Revisiting the Intrinsics of a Standard Plenoptic Camera (Tim Michels et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tim Michels, Daniel Mäckelmann, Reinhard Koch. (2024)<br><strong>Mind the Exit Pupil Gap: Revisiting the Intrinsics of a Standard Plenoptic Camera</strong><br><button class=copy-to-clipboard title="Mind the Exit Pupil Gap: Revisiting the Intrinsics of a Standard Plenoptic Camera" index=118>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-118 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12891v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12891v1.pdf filename=2402.12891v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Among the common applications of plenoptic cameras are depth reconstruction and post-shot refocusing. These require a calibration relating the camera-side light field to that of the scene. Numerous methods with this goal have been developed based on thin lens models for the plenoptic camera&rsquo;s main lens and microlenses. Our work addresses the often-overlooked role of the main lens exit pupil in these models and specifically in the decoding process of standard plenoptic camera (SPC) images. We formally deduce the connection between the refocusing distance and the resampling parameter for the decoded light field and provide an analysis of the errors that arise when the exit pupil is not considered. In addition, previous work is revisited with respect to the exit pupil&rsquo;s role and all theoretical results are validated through a ray-tracing-based <b>simulation.</b> With the public release of the evaluated SPC designs alongside our <b>simulation</b> and experimental data we aim to contribute to a more accurate and nuanced understanding of plenoptic camera optics.</p></p class="citation"></blockquote><h3 id=3045--119316-occflownet-towards-self-supervised-occupancy-estimation-via-differentiable-rendering-and-occupancy-flow-simon-boeder-et-al-2024>(30/45 | 119/316) OccFlowNet: Towards Self-supervised Occupancy Estimation via Differentiable Rendering and Occupancy Flow (Simon Boeder et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Simon Boeder, Fabian Gigengack, Benjamin Risse. (2024)<br><strong>OccFlowNet: Towards Self-supervised Occupancy Estimation via Differentiable Rendering and Occupancy Flow</strong><br><button class=copy-to-clipboard title="OccFlowNet: Towards Self-supervised Occupancy Estimation via Differentiable Rendering and Occupancy Flow" index=119>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-119 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Self-supervised Learning, Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12792v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12792v1.pdf filename=2402.12792v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Semantic occupancy has recently gained significant traction as a prominent 3D scene representation. However, most existing methods rely on large and costly datasets with fine-grained 3D voxel labels for training, which limits their practicality and scalability, increasing the need for self-monitored learning in this domain. In this work, we present a novel approach to occupancy estimation inspired by neural radiance field (NeRF) using only 2D labels, which are considerably easier to acquire. In particular, we employ differentiable volumetric rendering to predict depth and semantic maps and train a 3D network based on 2D supervision only. To enhance geometric accuracy and increase the supervisory signal, we introduce temporal rendering of adjacent time steps. Additionally, we introduce occupancy flow as a mechanism to handle dynamic objects in the scene and ensure their temporal consistency. Through extensive experimentation we demonstrate that 2D supervision only is sufficient to achieve state-of-the-art performance compared to methods using 3D labels, while outperforming concurrent 2D approaches. When combining 2D supervision with 3D labels, temporal rendering and occupancy flow we outperform all previous occupancy estimation models significantly. We conclude that the proposed rendering supervision and occupancy flow advances occupancy estimation and further bridges the gap towards <b>self-supervised</b> <b>learning</b> in this domain.</p></p class="citation"></blockquote><h3 id=3145--120316-convqg-contrastive-visual-question-generation-with-multimodal-guidance-li-mi-et-al-2024>(31/45 | 120/316) ConVQG: Contrastive Visual Question Generation with Multimodal Guidance (Li Mi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Li Mi, Syrielle Montariol, Javiera Castillo-Navarro, Xianjie Dai, Antoine Bosselut, Devis Tuia. (2024)<br><strong>ConVQG: Contrastive Visual Question Generation with Multimodal Guidance</strong><br><button class=copy-to-clipboard title="ConVQG: Contrastive Visual Question Generation with Multimodal Guidance" index=120>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-120 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 19<br>Keywords: Benchmarking, Multi-modal, Multi-modal, Grounding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12846v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12846v1.pdf filename=2402.12846v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Asking questions about visual environments is a crucial way for intelligent agents to understand rich multi-faceted scenes, raising the importance of Visual Question Generation (VQG) systems. Apart from being grounded to the image, existing VQG systems can use textual constraints, such as expected answers or knowledge triplets, to generate focused questions. These constraints allow VQG systems to specify the question content or leverage external commonsense knowledge that can not be obtained from the image content only. However, generating focused questions using textual constraints while enforcing a high relevance to the image content remains a challenge, as VQG systems often ignore one or both forms of <b>grounding.</b> In this work, we propose Contrastive Visual Question Generation (ConVQG), a method using a dual contrastive objective to discriminate questions generated using both modalities from those based on a single one. Experiments on both knowledge-aware and standard VQG <b>benchmarks</b> demonstrate that ConVQG outperforms the state-of-the-art methods and generates image-grounded, text-guided, and knowledge-rich questions. Our human evaluation results also show preference for ConVQG questions compared to non-contrastive baselines.</p></p class="citation"></blockquote><h3 id=3245--121316-aria-everyday-activities-dataset-zhaoyang-lv-et-al-2024>(32/45 | 121/316) Aria Everyday Activities Dataset (Zhaoyang Lv et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhaoyang Lv, Nicholas Charron, Pierre Moulon, Alexander Gamino, Cheng Peng, Chris Sweeney, Edward Miller, Huixuan Tang, Jeff Meissner, Jing Dong, Kiran Somasundaram, Luis Pesqueira, Mark Schwesinger, Omkar Parkhi, Qiao Gu, Renzo De Nardi, Shangyi Cheng, Steve Saarinen, Vijay Baiyya, Yuyang Zou, Richard Newcombe, Jakob Julian Engel, Xiaqing Pan, Carl Ren. (2024)<br><strong>Aria Everyday Activities Dataset</strong><br><button class=copy-to-clipboard title="Aria Everyday Activities Dataset" index=121>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-121 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs-HC, cs.CV<br>Keyword Score: 16<br>Keywords: Multi-modal, Multi-modal, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13349v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13349v2.pdf filename=2402.13349v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present Aria Everyday Activities (AEA) Dataset, an egocentric <b>multimodal</b> open dataset recorded using Project Aria glasses. AEA contains 143 daily activity sequences recorded by multiple wearers in five geographically diverse indoor locations. Each of the recording contains <b>multimodal</b> sensor data recorded through the Project Aria glasses. In addition, AEA provides machine perception data including high frequency globally aligned 3D trajectories, scene point cloud, per-frame 3D eye gaze vector and time aligned speech transcription. In this paper, we demonstrate a few exemplar research applications enabled by this dataset, including neural scene reconstruction and <b>prompted</b> segmentation. AEA is an open source dataset that can be downloaded from <a href=https://www.projectaria.com/datasets/aea/>https://www.projectaria.com/datasets/aea/</a>. We are also providing open-source implementations and examples of how to use the dataset in Project Aria Tools <a href=https://github.com/facebookresearch/projectaria_tools>https://github.com/facebookresearch/projectaria_tools</a>.</p></p class="citation"></blockquote><h3 id=3345--122316-improving-robustness-for-joint-optimization-of-camera-poses-and-decomposed-low-rank-tensorial-radiance-fields-bo-yu-cheng-et-al-2024>(33/45 | 122/316) Improving Robustness for Joint Optimization of Camera Poses and Decomposed Low-Rank Tensorial Radiance Fields (Bo-Yu Cheng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bo-Yu Cheng, Wei-Chen Chiu, Yu-Lun Liu. (2024)<br><strong>Improving Robustness for Joint Optimization of Camera Poses and Decomposed Low-Rank Tensorial Radiance Fields</strong><br><button class=copy-to-clipboard title="Improving Robustness for Joint Optimization of Camera Poses and Decomposed Low-Rank Tensorial Radiance Fields" index=122>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-122 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 15<br>Keywords: Convolution, Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13252v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13252v1.pdf filename=2402.13252v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we propose an algorithm that allows joint refinement of camera pose and scene <b>geometry</b> represented by decomposed low-rank tensor, using only 2D images as supervision. First, we conduct a pilot study based on a 1D signal and relate our findings to 3D scenarios, where the naive joint pose optimization on voxel-based NeRFs can easily lead to sub-optimal solutions. Moreover, based on the analysis of the frequency spectrum, we propose to apply <b>convolutional</b> Gaussian filters on 2D and 3D radiance fields for a coarse-to-fine training schedule that enables joint camera pose optimization. Leveraging the decomposition property in decomposed low-rank tensor, our method achieves an equivalent effect to brute-force 3D <b>convolution</b> with only incurring little computational overhead. To further improve the robustness and stability of joint optimization, we also propose techniques of smoothed 2D supervision, randomly scaled kernel parameters, and edge-guided loss mask. Extensive quantitative and qualitative evaluations demonstrate that our proposed framework achieves superior performance in novel view synthesis as well as rapid convergence for optimization.</p></p class="citation"></blockquote><h3 id=3445--123316-video-recap-recursive-captioning-of-hour-long-videos-md-mohaiminul-islam-et-al-2024>(34/45 | 123/316) Video ReCap: Recursive Captioning of Hour-Long Videos (Md Mohaiminul Islam et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Md Mohaiminul Islam, Ngan Ho, Xitong Yang, Tushar Nagarajan, Lorenzo Torresani, Gedas Bertasius. (2024)<br><strong>Video ReCap: Recursive Captioning of Hour-Long Videos</strong><br><button class=copy-to-clipboard title="Video ReCap: Recursive Captioning of Hour-Long Videos" index=123>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-123 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Curriculum Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13250v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13250v2.pdf filename=2402.13250v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Most video captioning models are designed to process short video clips of few seconds and output text describing low-level visual concepts (e.g., objects, scenes, atomic actions). However, most real-world videos last for minutes or hours and have a complex hierarchical structure spanning different temporal granularities. We propose Video ReCap, a recursive video captioning model that can process video inputs of dramatically different lengths (from 1 second to 2 hours) and output video captions at multiple hierarchy levels. The recursive video-language architecture exploits the synergy between different video hierarchies and can process hour-long videos efficiently. We utilize a <b>curriculum</b> <b>learning</b> training scheme to learn the hierarchical structure of videos, starting from clip-level captions describing atomic actions, then focusing on segment-level descriptions, and concluding with generating summaries for hour-long videos. Furthermore, we introduce Ego4D-HCap dataset by augmenting Ego4D with 8,267 manually collected long-range video summaries. Our recursive model can flexibly generate captions at different hierarchy levels while also being useful for other complex video understanding tasks, such as VideoQA on EgoSchema. Data, code, and models are available at: <a href=https://sites.google.com/view/vidrecap>https://sites.google.com/view/vidrecap</a></p></p class="citation"></blockquote><h3 id=3545--124316-uniedit-a-unified-tuning-free-framework-for-video-motion-and-appearance-editing-jianhong-bai-et-al-2024>(35/45 | 124/316) UniEdit: A Unified Tuning-Free Framework for Video Motion and Appearance Editing (Jianhong Bai et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jianhong Bai, Tianyu He, Yuchi Wang, Junliang Guo, Haoji Hu, Zuozhu Liu, Jiang Bian. (2024)<br><strong>UniEdit: A Unified Tuning-Free Framework for Video Motion and Appearance Editing</strong><br><button class=copy-to-clipboard title="UniEdit: A Unified Tuning-Free Framework for Video Motion and Appearance Editing" index=124>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-124 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13185v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13185v2.pdf filename=2402.13185v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advances in text-guided video editing have showcased promising results in appearance editing (e.g., stylization). However, video motion editing in the temporal dimension (e.g., from eating to waving), which distinguishes video editing from image editing, is underexplored. In this work, we present UniEdit, a tuning-free framework that supports both video motion and appearance editing by harnessing the power of a pre-trained text-to-video generator within an inversion-then-generation framework. To realize motion editing while preserving source video content, based on the insights that temporal and spatial <b>self-attention</b> layers encode inter-frame and intra-frame dependency respectively, we introduce auxiliary motion-reference and reconstruction branches to produce text-guided motion and source features respectively. The obtained features are then injected into the main editing path via temporal and spatial <b>self-attention</b> layers. Extensive experiments demonstrate that UniEdit covers video motion editing and various appearance editing scenarios, and surpasses the state-of-the-art methods. Our code will be publicly available.</p></p class="citation"></blockquote><h3 id=3645--125316-toward-fairness-via-maximum-mean-discrepancy-regularization-on-logits-space-hao-wei-chung-et-al-2024>(36/45 | 125/316) Toward Fairness via Maximum Mean Discrepancy Regularization on Logits Space (Hao-Wei Chung et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hao-Wei Chung, Ching-Hao Chiu, Yu-Jen Chen, Yiyu Shi, Tsung-Yi Ho. (2024)<br><strong>Toward Fairness via Maximum Mean Discrepancy Regularization on Logits Space</strong><br><button class=copy-to-clipboard title="Toward Fairness via Maximum Mean Discrepancy Regularization on Logits Space" index=125>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-125 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Fairness<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13061v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13061v1.pdf filename=2402.13061v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Fairness</b> has become increasingly pivotal in machine learning for high-risk applications such as machine learning in healthcare and facial recognition. However, we see the deficiency in the previous logits space constraint methods. Therefore, we propose a novel framework, Logits-MMD, that achieves the <b>fairness</b> condition by imposing constraints on output logits with Maximum Mean Discrepancy. Moreover, quantitative analysis and experimental results show that our framework has a better property that outperforms previous methods and achieves state-of-the-art on two facial recognition datasets and one animal dataset. Finally, we show experimental results and demonstrate that our debias approach achieves the <b>fairness</b> condition effectively.</p></p class="citation"></blockquote><h3 id=3745--126316-comparison-of-conventional-hybrid-and-ctcattention-decoders-for-continuous-visual-speech-recognition-david-gimeno-gómez-et-al-2024>(37/45 | 126/316) Comparison of Conventional Hybrid and CTC/Attention Decoders for Continuous Visual Speech Recognition (David Gimeno-Gómez et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>David Gimeno-Gómez, Carlos-D. Martínez-Hinarejos. (2024)<br><strong>Comparison of Conventional Hybrid and CTC/Attention Decoders for Continuous Visual Speech Recognition</strong><br><button class=copy-to-clipboard title="Comparison of Conventional Hybrid and CTC/Attention Decoders for Continuous Visual Speech Recognition" index=126>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-126 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Automatic Speech Recognition<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13004v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13004v1.pdf filename=2402.13004v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Thanks to the rise of deep learning and the availability of large-scale audio-visual databases, recent advances have been achieved in Visual <b>Speech</b> <b>Recognition</b> (VSR). Similar to other <b>speech</b> <b>processing</b> tasks, these end-to-end VSR systems are usually based on encoder-decoder architectures. While encoders are somewhat general, multiple decoding approaches have been explored, such as the conventional hybrid model based on Deep Neural Networks combined with Hidden Markov Models (DNN-HMM) or the Connectionist Temporal Classification (CTC) paradigm. However, there are languages and tasks in which data is scarce, and in this situation, there is not a clear comparison between different types of decoders. Therefore, we focused our study on how the conventional DNN-HMM decoder and its state-of-the-art CTC/Attention counterpart behave depending on the amount of data used for their estimation. We also analyzed to what extent our visual <b>speech</b> <b>features</b> were able to adapt to scenarios for which they were not explicitly trained, either considering a similar dataset or another collected for a different language. Results showed that the conventional paradigm reached recognition rates that improve the CTC/Attention model in data-scarcity scenarios along with a reduced training time and fewer parameters.</p></p class="citation"></blockquote><h3 id=3845--127316-two-stage-rainfall-forecasting-diffusion-model-xudong-ling-et-al-2024>(38/45 | 127/316) Two-stage Rainfall-Forecasting Diffusion Model (XuDong Ling et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>XuDong Ling, ChaoRong Li, FengQing Qin, LiHong Zhu, Yuanyuan Huang. (2024)<br><strong>Two-stage Rainfall-Forecasting Diffusion Model</strong><br><button class=copy-to-clipboard title="Two-stage Rainfall-Forecasting Diffusion Model" index=127>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-127 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Diffusion Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12779v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12779v1.pdf filename=2402.12779v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep neural networks have made great achievements in rainfall prediction.However, the current forecasting methods have certain limitations, such as with blurry generated images and incorrect spatial positions. To overcome these challenges, we propose a Two-stage Rainfall-Forecasting <b>Diffusion</b> <b>Model</b> (TRDM) aimed at improving the accuracy of long-term rainfall forecasts and addressing the imbalance in performance between temporal and spatial modeling. TRDM is a two-stage method for rainfall prediction tasks. The task of the first stage is to capture robust temporal information while preserving spatial information under low-resolution conditions. The task of the second stage is to reconstruct the low-resolution images generated in the first stage into high-resolution images. We demonstrate state-of-the-art results on the MRMS and Swedish radar datasets. Our project is open source and available on GitHub at: \href{https://github.com/clearlyzerolxd/TRDM}{https://github.com/clearlyzerolxd/TRDM}.</p></p class="citation"></blockquote><h3 id=3945--128316-good-towards-domain-generalized-orientated-object-detection-qi-bi-et-al-2024>(39/45 | 128/316) GOOD: Towards Domain Generalized Orientated Object Detection (Qi Bi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qi Bi, Beichen Zhou, Jingjun Yi, Wei Ji, Haolan Zhan, Gui-Song Xia. (2024)<br><strong>GOOD: Towards Domain Generalized Orientated Object Detection</strong><br><button class=copy-to-clipboard title="GOOD: Towards Domain Generalized Orientated Object Detection" index=128>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-128 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Object Detection<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12765v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12765v1.pdf filename=2402.12765v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Oriented <b>object</b> <b>detection</b> has been rapidly developed in the past few years, but most of these methods assume the training and testing images are under the same statistical distribution, which is far from reality. In this paper, we propose the task of domain generalized oriented <b>object</b> <b>detection,</b> which intends to explore the generalization of oriented <b>object</b> <b>detectors</b> on arbitrary unseen target domains. Learning domain generalized oriented <b>object</b> <b>detectors</b> is particularly challenging, as the cross-domain style variation not only negatively impacts the content representation, but also leads to unreliable orientation predictions. To address these challenges, we propose a generalized oriented <b>object</b> <b>detector</b> (GOOD). After style hallucination by the emerging contrastive language-image pre-training (CLIP), it consists of two key components, namely, rotation-aware content consistency learning (RAC) and style consistency learning (SEC). The proposed RAC allows the oriented <b>object</b> <b>detector</b> to learn stable orientation representation from style-diversified samples. The proposed SEC further stabilizes the generalization ability of content representation from different image styles. Extensive experiments on multiple cross-domain settings show the state-of-the-art performance of GOOD. Source code will be publicly available.</p></p class="citation"></blockquote><h3 id=4045--129316-neuromorphic-synergy-for-video-binarization-shijie-lin-et-al-2024>(40/45 | 129/316) Neuromorphic Synergy for Video Binarization (Shijie Lin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shijie Lin, Xiang Zhang, Lei Yang, Lei Yu, Bin Zhou, Xiaowei Luo, Wenping Wang, Jia Pan. (2024)<br><strong>Neuromorphic Synergy for Video Binarization</strong><br><button class=copy-to-clipboard title="Neuromorphic Synergy for Video Binarization" index=129>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-129 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12644v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12644v1.pdf filename=2402.12644v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Bimodal objects, such as the checkerboard pattern used in camera calibration, markers for object tracking, and text on road signs, to name a few, are prevalent in our daily lives and serve as a visual form to embed information that can be easily recognized by vision systems. While binarization from intensity images is crucial for extracting the embedded information in the bimodal objects, few previous works consider the task of binarization of blurry images due to the relative motion between the vision sensor and the environment. The blurry images can result in a loss in the binarization quality and thus degrade the downstream applications where the vision system is in motion. Recently, neuromorphic cameras offer new capabilities for alleviating motion blur, but it is non-trivial to first deblur and then binarize the images in a real-time manner. In this work, we propose an event-based binary reconstruction method that leverages the prior knowledge of the bimodal target&rsquo;s properties to perform inference independently in both event space and image space and merge the results from both domains to generate a sharp binary image. We also develop an efficient integration method to propagate this binary image to high frame rate binary video. Finally, we develop a novel method to naturally fuse events and images for <b>unsupervised</b> threshold identification. The proposed method is evaluated in publicly available and our collected data sequence, and shows the proposed method can outperform the SOTA methods to generate high frame rate binary video in real-time on CPU-only devices.</p></p class="citation"></blockquote><h3 id=4145--130316-bronchotrack-airway-lumen-tracking-for-branch-level-bronchoscopic-localization-qingyao-tian-et-al-2024>(41/45 | 130/316) BronchoTrack: Airway Lumen Tracking for Branch-Level Bronchoscopic Localization (Qingyao Tian et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qingyao Tian, Huai Liao, Xinyan Huang, Bingyu Yang, Jinlin Wu, Jian Chen, Lujie Li, Hongbin Liu. (2024)<br><strong>BronchoTrack: Airway Lumen Tracking for Branch-Level Bronchoscopic Localization</strong><br><button class=copy-to-clipboard title="BronchoTrack: Airway Lumen Tracking for Branch-Level Bronchoscopic Localization" index=130>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-130 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 6<br>Keywords: Graph, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12763v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12763v1.pdf filename=2402.12763v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Localizing the bronchoscope in real time is essential for ensuring intervention quality. However, most existing methods struggle to balance between speed and generalization. To address these challenges, we present BronchoTrack, an innovative real-time framework for accurate branch-level localization, encompassing lumen detection, tracking, and airway association.To achieve real-time performance, we employ a <b>benchmark</b> lightweight detector for efficient lumen detection. We are the first to introduce multi-object tracking to bronchoscopic localization, mitigating temporal confusion in lumen identification caused by rapid bronchoscope movement and complex airway structures. To ensure generalization across patient cases, we propose a training-free detection-airway association method based on a semantic airway <b>graph</b> that encodes the hierarchy of bronchial tree structures.Experiments on nine patient datasets demonstrate BronchoTrack&rsquo;s localization accuracy of 85.64 %, while accessing up to the 4th generation of airways.Furthermore, we tested BronchoTrack in an in-vivo animal study using a porcine model, where it successfully localized the bronchoscope into the 8th generation airway.Experimental evaluation underscores BronchoTrack&rsquo;s real-time performance in both satisfying accuracy and generalization, demonstrating its potential for clinical applications.</p></p class="citation"></blockquote><h3 id=4245--131316-vadv2-end-to-end-vectorized-autonomous-driving-via-probabilistic-planning-shaoyu-chen-et-al-2024>(42/45 | 131/316) VADv2: End-to-End Vectorized Autonomous Driving via Probabilistic Planning (Shaoyu Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shaoyu Chen, Bo Jiang, Hao Gao, Bencheng Liao, Qing Xu, Qian Zhang, Chang Huang, Wenyu Liu, Xinggang Wang. (2024)<br><strong>VADv2: End-to-End Vectorized Autonomous Driving via Probabilistic Planning</strong><br><button class=copy-to-clipboard title="VADv2: End-to-End Vectorized Autonomous Driving via Probabilistic Planning" index=131>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-131 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-RO, cs.CV<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13243v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13243v1.pdf filename=2402.13243v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Learning a human-like driving policy from large-scale driving demonstrations is promising, but the uncertainty and non-deterministic nature of planning make it challenging. In this work, to cope with the uncertainty problem, we propose VADv2, an end-to-end driving model based on probabilistic planning. VADv2 takes multi-view image sequences as input in a streaming manner, transforms sensor data into environmental token embeddings, outputs the probabilistic distribution of action, and samples one action to control the vehicle. Only with camera sensors, VADv2 achieves state-of-the-art closed-loop performance on the CARLA Town05 <b>benchmark,</b> significantly outperforming all existing methods. It runs stably in a fully end-to-end manner, even without the rule-based wrapper. Closed-loop demos are presented at <a href=https://hgao-cv.github.io/VADv2>https://hgao-cv.github.io/VADv2</a>.</p></p class="citation"></blockquote><h3 id=4345--132316-maptrack-tracking-in-the-map-fei-wang-et-al-2024>(43/45 | 132/316) MapTrack: Tracking in the Map (Fei Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fei Wang, Ruohui Zhang, Chenglin Chen, Min Yang, Yun Bai. (2024)<br><strong>MapTrack: Tracking in the Map</strong><br><button class=copy-to-clipboard title="MapTrack: Tracking in the Map" index=132>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-132 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12968v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12968v1.pdf filename=2402.12968v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Multi-Object Tracking (MOT) aims to maintain stable and uninterrupted trajectories for each target. Most state-of-the-art approaches first detect objects in each frame and then implement data association between new detections and existing tracks using motion models and appearance similarities. Despite achieving satisfactory results, occlusion and crowds can easily lead to missing and distorted detections, followed by missing and false associations. In this paper, we first revisit the classic tracker DeepSORT, enhancing its robustness over crowds and occlusion significantly by placing greater trust in predictions when detections are unavailable or of low quality in crowded and occluded scenes. Specifically, we propose a new framework comprising of three lightweight and plug-and-play algorithms: the probability map, the prediction map, and the covariance adaptive Kalman filter. The probability map identifies whether undetected objects have genuinely disappeared from view (e.g., out of the image or entered a building) or are only temporarily undetected due to occlusion or other reasons. Trajectories of undetected targets that are still within the probability map are extended by state estimations directly. The prediction map determines whether an object is in a crowd, and we prioritize state estimations over observations when severe deformation of observations occurs, accomplished through the covariance adaptive Kalman filter. The proposed method, named MapTrack, achieves state-of-the-art results on popular multi-object tracking <b>benchmarks</b> such as MOT17 and MOT20. Despite its superior performance, our method remains simple, online, and real-time. The code will be open-sourced later.</p></p class="citation"></blockquote><h3 id=4445--133316-pac-fno-parallel-structured-all-component-fourier-neural-operators-for-recognizing-low-quality-images-jinsung-jeon-et-al-2024>(44/45 | 133/316) PAC-FNO: Parallel-Structured All-Component Fourier Neural Operators for Recognizing Low-Quality Images (Jinsung Jeon et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jinsung Jeon, Hyundong Jin, Jonghyun Choi, Sanghyun Hong, Dongeun Lee, Kookjin Lee, Noseong Park. (2024)<br><strong>PAC-FNO: Parallel-Structured All-Component Fourier Neural Operators for Recognizing Low-Quality Images</strong><br><button class=copy-to-clipboard title="PAC-FNO: Parallel-Structured All-Component Fourier Neural Operators for Recognizing Low-Quality Images" index=133>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-133 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12721v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12721v1.pdf filename=2402.12721v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A standard practice in developing image recognition models is to train a model on a specific image resolution and then deploy it. However, in real-world inference, models often encounter images different from the training sets in resolution and/or subject to natural variations such as weather changes, noise types and compression artifacts. While traditional solutions involve training multiple models for different resolutions or input variations, these methods are computationally expensive and thus do not scale in practice. To this end, we propose a novel neural network model, parallel-structured and all-component Fourier neural operator (PAC-FNO), that addresses the problem. Unlike conventional feed-forward neural networks, PAC-FNO operates in the frequency domain, allowing it to handle images of varying resolutions within a single model. We also propose a two-stage algorithm for training PAC-FNO with a minimal modification to the original, downstream model. Moreover, the proposed PAC-FNO is ready to work with existing image recognition models. Extensively evaluating methods with seven image recognition <b>benchmarks,</b> we show that the proposed PAC-FNO improves the performance of existing baseline models on images with various resolutions by up to 77.1% and various types of natural variations in the images at inference.</p></p class="citation"></blockquote><h3 id=4545--134316-object-level-geometric-structure-preserving-for-natural-image-stitching-wenxiao-cai-et-al-2024>(45/45 | 134/316) Object-level Geometric Structure Preserving for Natural Image Stitching (Wenxiao Cai et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wenxiao Cai, Wankou Yang. (2024)<br><strong>Object-level Geometric Structure Preserving for Natural Image Stitching</strong><br><button class=copy-to-clipboard title="Object-level Geometric Structure Preserving for Natural Image Stitching" index=134>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-134 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12677v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12677v1.pdf filename=2402.12677v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The topic of stitching images with globally natural structures holds paramount significance. Current methodologies exhibit the ability to preserve local geometric structures, yet fall short in maintaining relationships between these geometric structures. In this paper, we endeavor to safeguard the overall, OBJect-level structures within images based on Global Similarity Prior, while concurrently mitigating distortion and ghosting artifacts with OBJ-GSP. Our approach leverages the Segment Anything Model to extract geometric structures with semantic information, enhancing the algorithm&rsquo;s ability to preserve objects in a manner that aligns more intuitively with human perception. We seek to identify spatial constraints that govern the relationships between various geometric boundaries. Recognizing that multiple geometric boundaries collectively define complete objects, we employ triangular meshes to safeguard not only individual geometric structures but also the overall shapes of objects within the images. Empirical evaluations across multiple image stitching datasets demonstrate that our method establishes a new state-of-the-art <b>benchmark</b> in image stitching. Our implementation and dataset is publicly available at <a href=https://github.com/RussRobin/OBJ-GSP>https://github.com/RussRobin/OBJ-GSP</a> .</p></p class="citation"></blockquote><h2 id=cslg-67>cs.LG (67)</h2><h3 id=167--135316-reflect-rl-two-player-online-rl-fine-tuning-for-lms-runlong-zhou-et-al-2024>(1/67 | 135/316) Reflect-RL: Two-Player Online RL Fine-Tuning for LMs (Runlong Zhou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Runlong Zhou, Simon S. Du, Beibin Li. (2024)<br><strong>Reflect-RL: Two-Player Online RL Fine-Tuning for LMs</strong><br><button class=copy-to-clipboard title="Reflect-RL: Two-Player Online RL Fine-Tuning for LMs" index=135>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-135 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CL, cs-LG, cs.LG<br>Keyword Score: 90<br>Keywords: Curriculum Learning, Fine-tuning, Fine-tuning, Online Reinforcement Learning, Reinforcement Learning, Supervised Learning, GPT, GPT-2, Mistral<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12621v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12621v1.pdf filename=2402.12621v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>As language models (LMs) demonstrate their capabilities in various fields, their application to tasks requiring multi-round interactions has become increasingly popular. These tasks usually have complex dynamics, so <b>supervised</b> <b>fine-tuning</b> (SFT) on a limited offline dataset does not yield good performance. However, only a few works attempted to directly train the LMs within interactive decision-making environments. We aim to create an effective mechanism to <b>fine-tune</b> LMs with <b>online</b> <b>reinforcement</b> <b>learning</b> (RL) in these environments. We propose Reflect-RL, a two-player system to <b>fine-tune</b> an LM using <b>online</b> <b>RL,</b> <b>where</b> a frozen reflection model assists the policy model. To generate data for the warm-up SFT stage, we use negative example generation to enhance the error-correction ability of the reflection model. Furthermore, we designed single-prompt action enumeration and applied <b>curriculum</b> <b>learning</b> to allow the policy model to learn more efficiently. Empirically, we verify that Reflect-RL outperforms SFT and <b>online</b> <b>RL</b> <b>without</b> reflection. Testing results indicate <b>GPT-2-xl</b> after Reflect-RL also outperforms those of untuned pre-trained LMs, such as <b>Mistral</b> 7B.</p></p class="citation"></blockquote><h3 id=267--136316-indiscriminate-data-poisoning-attacks-on-pre-trained-feature-extractors-yiwei-lu-et-al-2024>(2/67 | 136/316) Indiscriminate Data Poisoning Attacks on Pre-trained Feature Extractors (Yiwei Lu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yiwei Lu, Matthew Y. R. Yang, Gautam Kamath, Yaoliang Yu. (2024)<br><strong>Indiscriminate Data Poisoning Attacks on Pre-trained Feature Extractors</strong><br><button class=copy-to-clipboard title="Indiscriminate Data Poisoning Attacks on Pre-trained Feature Extractors" index=136>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-136 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CR, cs-LG, cs.LG<br>Keyword Score: 70<br>Keywords: Fine-tuning, Self-supervised Learning, Self-supervised Learning, Supervised Learning, Supervised Learning, Transfer Learning, Domain Adaptation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12626v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12626v1.pdf filename=2402.12626v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Machine learning models have achieved great success in <b>supervised</b> <b>learning</b> tasks for end-to-end training, which requires a large amount of labeled data that is not always feasible. Recently, many practitioners have shifted to <b>self-supervised</b> <b>learning</b> methods that utilize cheap unlabeled data to learn a general feature extractor via pre-training, which can be further applied to personalized downstream tasks by simply training an additional linear layer with limited labeled data. However, such a process may also raise concerns regarding data poisoning attacks. For instance, indiscriminate data poisoning attacks, which aim to decrease model utility by injecting a small number of poisoned data into the training set, pose a security risk to machine learning models, but have only been studied for end-to-end <b>supervised</b> <b>learning.</b> In this paper, we extend the exploration of the threat of indiscriminate attacks on downstream tasks that apply pre-trained feature extractors. Specifically, we propose two types of attacks: (1) the input space attacks, where we modify existing attacks to directly craft poisoned data in the input space. However, due to the difficulty of optimization under constraints, we further propose (2) the feature targeted attacks, where we mitigate the challenge with three stages, firstly acquiring target parameters for the linear head; secondly finding poisoned features by treating the learned feature representations as a dataset; and thirdly inverting the poisoned features back to the input space. Our experiments examine such attacks in popular downstream tasks of <b>fine-tuning</b> on the same dataset and <b>transfer</b> <b>learning</b> that considers <b>domain</b> <b>adaptation.</b> Empirical results reveal that <b>transfer</b> <b>learning</b> is more vulnerable to our attacks. Additionally, input space attacks are a strong threat if no countermeasures are posed, but are otherwise weaker than feature targeted attacks.</p></p class="citation"></blockquote><h3 id=367--137316-conditional-logical-message-passing-transformer-for-complex-query-answering-chongzhi-zhang-et-al-2024>(3/67 | 137/316) Conditional Logical Message Passing Transformer for Complex Query Answering (Chongzhi Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chongzhi Zhang, Zhiping Peng, Junhao Zheng, Qianli Ma. (2024)<br><strong>Conditional Logical Message Passing Transformer for Complex Query Answering</strong><br><button class=copy-to-clipboard title="Conditional Logical Message Passing Transformer for Complex Query Answering" index=137>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-137 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs-LO, cs.LG<br>Keyword Score: 63<br>Keywords: Message-Passing, Graph, Node Embedding, Knowledge Graph, Knowledge Graph, Transformer, Reasoning, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12954v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12954v1.pdf filename=2402.12954v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Complex Query Answering (CQA) over <b>Knowledge</b> <b>Graphs</b> <b>(KGs)</b> is a challenging task. Given that <b>KGs</b> are usually incomplete, neural models are proposed to solve CQA by performing multi-hop logical <b>reasoning.</b> However, most of them cannot perform well on both one-hop and multi-hop queries simultaneously. Recent work proposes a logical message passing mechanism based on the pre-trained neural link predictors. While effective on both one-hop and multi-hop queries, it ignores the difference between the constant and variable <b>nodes</b> <b>in</b> a query <b>graph.</b> In addition, during the <b>node</b> <b>embedding</b> update stage, this mechanism cannot dynamically measure the importance of different messages, and whether it can capture the implicit logical dependencies related to a <b>node</b> <b>and</b> received messages remains unclear. In this paper, we propose Conditional Logical Message Passing <b>Transformer</b> (CLMPT), which considers the difference between constants and variables in the case of using pre-trained neural link predictors and performs message passing conditionally on the <b>node</b> <b>type.</b> We empirically verified that this approach can reduce computational costs without affecting performance. Furthermore, CLMPT uses the <b>transformer</b> to aggregate received messages and update the corresponding <b>node</b> <b>embedding.</b> Through the <b>self-attention</b> mechanism, CLMPT can assign adaptive weights to elements in an input set consisting of received messages and the corresponding <b>node</b> <b>and</b> explicitly model logical dependencies between various elements. Experimental results show that CLMPT is a new state-of-the-art neural CQA model.</p></p class="citation"></blockquote><h3 id=467--138316-fgad-self-boosted-knowledge-distillation-for-an-effective-federated-graph-anomaly-detection-framework-jinyu-cai-et-al-2024>(4/67 | 138/316) FGAD: Self-boosted Knowledge Distillation for An Effective Federated Graph Anomaly Detection Framework (Jinyu Cai et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jinyu Cai, Yunhe Zhang, Zhoumin Lu, Wenzhong Guo, See-kiong Ng. (2024)<br><strong>FGAD: Self-boosted Knowledge Distillation for An Effective Federated Graph Anomaly Detection Framework</strong><br><button class=copy-to-clipboard title="FGAD: Self-boosted Knowledge Distillation for An Effective Federated Graph Anomaly Detection Framework" index=138>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-138 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CR, cs-LG, cs.LG<br>Keyword Score: 63<br>Keywords: Graph Anomaly Detection, Graph, Anomaly Detection, Federated Learning, Knowledge Distillation, Knowledge Distillation, Knowledge Distillation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12761v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12761v1.pdf filename=2402.12761v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Graph</b> <b>anomaly</b> <b>detection</b> (GAD) aims to identify anomalous <b>graphs</b> <b>that</b> <b>significantly</b> deviate from other ones, which has raised growing attention due to the broad existence and complexity of <b>graph-structured</b> <b>data</b> <b>in</b> many real-world scenarios. However, existing GAD methods usually execute with centralized training, which may lead to privacy leakage risk in some sensitive cases, thereby impeding collaboration among organizations seeking to collectively develop robust GAD models. Although <b>federated</b> <b>learning</b> offers a promising solution, the prevalent non-IID problems and high communication costs present significant challenges, particularly pronounced in collaborations with <b>graph</b> <b>data</b> <b>distributed</b> among different participants. To tackle these challenges, we propose an effective <b>federated</b> <b>graph</b> <b>anomaly</b> <b>detection</b> framework (FGAD). We first introduce an <b>anomaly</b> <b>generator</b> to perturb the normal <b>graphs</b> <b>to</b> <b>be</b> anomalous, and train a powerful <b>anomaly</b> <b>detector</b> by distinguishing generated anomalous <b>graphs</b> <b>from</b> <b>normal</b> ones. Then, we leverage a student model to <b>distill</b> <b>knowledge</b> <b>from</b> the trained <b>anomaly</b> <b>detector</b> (teacher model), which aims to maintain the personality of local models and alleviate the adverse impact of non-IID problems. Moreover, we design an effective collaborative learning mechanism that facilitates the personalization preservation of local models and significantly reduces communication costs among clients. Empirical results of the GAD tasks on non-IID <b>graphs</b> <b>compared</b> <b>with</b> state-of-the-art baselines demonstrate the superiority and efficiency of the proposed FGAD method.</p></p class="citation"></blockquote><h3 id=567--139316-bayesian-reward-models-for-llm-alignment-adam-x-yang-et-al-2024>(5/67 | 139/316) Bayesian Reward Models for LLM Alignment (Adam X. Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Adam X. Yang, Maxime Robeyns, Thomas Coste, Jun Wang, Haitham Bou-Ammar, Laurence Aitchison. (2024)<br><strong>Bayesian Reward Models for LLM Alignment</strong><br><button class=copy-to-clipboard title="Bayesian Reward Models for LLM Alignment" index=139>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-139 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 60<br>Keywords: Fine-tuning, Reinforcement Learning, Reinforcement Learning from Human Feedback, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13210v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13210v1.pdf filename=2402.13210v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>To ensure that <b>large</b> <b>language</b> <b>model</b> <b>(LLM)</b> responses are helpful and non-toxic, we usually <b>fine-tune</b> a reward model on human preference data. We then select policy responses with high rewards (best-of-n sampling) or further optimize the policy to produce responses with high rewards <b>(reinforcement</b> <b>learning</b> <b>from</b> <b>human</b> <b>feedback).</b> However, this process is vulnerable to reward overoptimization or hacking, in which the responses selected have high rewards due to errors in the reward model rather than a genuine preference. This is especially problematic as the <b>prompt</b> or response diverges from the training data. It should be possible to mitigate these issues by training a Bayesian reward model, which signals higher uncertainty further from the training data distribution. Therefore, we trained Bayesian reward models using Laplace-LoRA (Yang et al., 2024) and found that the resulting uncertainty estimates can successfully mitigate reward overoptimization in best-of-n sampling.</p></p class="citation"></blockquote><h3 id=667--140316-defending-jailbreak-prompts-via-in-context-adversarial-game-yujun-zhou-et-al-2024>(6/67 | 140/316) Defending Jailbreak Prompts via In-Context Adversarial Game (Yujun Zhou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yujun Zhou, Yufei Han, Haomin Zhuang, Taicheng Guo, Kehan Guo, Zhenwen Liang, Hongyan Bao, Xiangliang Zhang. (2024)<br><strong>Defending Jailbreak Prompts via In-Context Adversarial Game</strong><br><button class=copy-to-clipboard title="Defending Jailbreak Prompts via In-Context Adversarial Game" index=140>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-140 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CR, cs-LG, cs.LG<br>Keyword Score: 60<br>Keywords: Adversarial Learning, Fine-tuning, In-context Learning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13148v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13148v1.pdf filename=2402.13148v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> demonstrate remarkable capabilities across diverse applications. However, concerns regarding their security, particularly the vulnerability to jailbreak attacks, persist. Drawing inspiration from <b>adversarial</b> <b>training</b> in deep learning and <b>LLM</b> agent learning processes, we introduce the <b>In-Context</b> <b>Adversarial</b> <b>Game</b> (ICAG) for defending against jailbreaks without the need for <b>fine-tuning.</b> ICAG leverages agent learning to conduct an <b>adversarial</b> <b>game,</b> aiming to dynamically extend knowledge to defend against jailbreaks. Unlike traditional methods that rely on static datasets, ICAG employs an iterative process to enhance both the defense and attack agents. This continuous improvement process strengthens defenses against newly generated jailbreak <b>prompts.</b> Our empirical studies affirm ICAG&rsquo;s efficacy, where <b>LLMs</b> safeguarded by ICAG exhibit significantly reduced jailbreak success rates across various attack scenarios. Moreover, ICAG demonstrates remarkable transferability to other <b>LLMs,</b> indicating its potential as a versatile defense mechanism.</p></p class="citation"></blockquote><h3 id=767--141316-harnessing-large-language-models-as-post-hoc-correctors-zhiqiang-zhong-et-al-2024>(7/67 | 141/316) Harnessing Large Language Models as Post-hoc Correctors (Zhiqiang Zhong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhiqiang Zhong, Kuangyu Zhou, Davide Mottin. (2024)<br><strong>Harnessing Large Language Models as Post-hoc Correctors</strong><br><button class=copy-to-clipboard title="Harnessing Large Language Models as Post-hoc Correctors" index=141>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-141 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CL, cs-LG, cs.LG<br>Keyword Score: 50<br>Keywords: Fine-tuning, In-context Learning, In-context Learning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13414v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13414v1.pdf filename=2402.13414v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>As Machine Learning (ML) models grow in size and demand higher-quality training data, the expenses associated with re-training and <b>fine-tuning</b> these models are escalating rapidly. Inspired by recent impressive achievements of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> in different fields, this paper delves into the question: can <b>LLMs</b> efficiently improve an ML&rsquo;s performance at a minimal cost? We show that, through our proposed training-free framework LlmCorr, an <b>LLM</b> can work as a post-hoc corrector to propose corrections for the predictions of an arbitrary ML model. In particular, we form a contextual knowledge database by incorporating the dataset&rsquo;s label information and the ML model&rsquo;s predictions on the validation dataset. Leveraging the <b>in-context</b> <b>learning</b> capability of <b>LLMs,</b> we ask the <b>LLM</b> to summarise the instances in which the ML model makes mistakes and the correlation between primary predictions and true labels. Following this, the <b>LLM</b> can transfer its acquired knowledge to suggest corrections for the ML model&rsquo;s predictions. Our experimental results on the challenging molecular predictions show that LlmCorr improves the performance of a number of models by up to 39%.</p></p class="citation"></blockquote><h3 id=867--142316-a-microstructure-based-graph-neural-network-for-accelerating-multiscale-simulations-j-storm-et-al-2024>(8/67 | 142/316) A Microstructure-based Graph Neural Network for Accelerating Multiscale Simulations (J. Storm et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>J. Storm, I. B. C. M. Rocha, F. P. van der Meer. (2024)<br><strong>A Microstructure-based Graph Neural Network for Accelerating Multiscale Simulations</strong><br><button class=copy-to-clipboard title="A Microstructure-based Graph Neural Network for Accelerating Multiscale Simulations" index=142>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-142 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs-NA, cs.LG, math-NA<br>Keyword Score: 43<br>Keywords: Graph, Graph Neural Network, Graph Neural Network, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13101v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13101v1.pdf filename=2402.13101v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Simulating the mechanical response of advanced materials can be done more accurately using concurrent multiscale models than with single-scale <b>simulations.</b> However, the computational costs stand in the way of the practical application of this approach. The costs originate from microscale Finite Element (FE) models that must be solved at every macroscopic integration point. A plethora of surrogate modeling strategies attempt to alleviate this cost by learning to predict macroscopic stresses from macroscopic strains, completely replacing the microscale models. In this work, we introduce an alternative surrogate modeling strategy that allows for keeping the multiscale nature of the problem, allowing it to be used interchangeably with an FE solver for any time step. Our surrogate provides all microscopic quantities, which are then homogenized to obtain macroscopic quantities of interest. We achieve this for an elasto-plastic material by predicting full-field microscopic strains using a <b>graph</b> <b>neural</b> <b>network</b> <b>(GNN)</b> while retaining the microscopic constitutive material model to obtain the stresses. This hybrid data-physics <b>graph-based</b> <b>approach</b> <b>avoids</b> the high dimensionality originating from predicting full-field responses while allowing non-locality to arise. By training the <b>GNN</b> on a variety of meshes, it learns to generalize to unseen meshes, allowing a single model to be used for a range of microstructures. The embedded microscopic constitutive model in the <b>GNN</b> implicitly tracks history-dependent variables and leads to improved accuracy. We demonstrate for several challenging scenarios that the surrogate can predict complex macroscopic stress-strain paths. As the computation time of our method scales favorably with the number of elements in the microstructure compared to the FE method, our method can significantly accelerate FE2 <b>simulations.</b></p></p class="citation"></blockquote><h3 id=967--143316-towards-accelerating-physical-discovery-via-non-interactive-and-interactive-multi-fidelity-bayesian-optimization-current-challenges-and-future-opportunities-arpan-biswas-et-al-2024>(9/67 | 143/316) Towards accelerating physical discovery via non-interactive and interactive multi-fidelity Bayesian Optimization: Current challenges and future opportunities (Arpan Biswas et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Arpan Biswas, Sai Mani Prudhvi Valleti, Rama Vasudevan, Maxim Ziatdinov, Sergei V. Kalinin. (2024)<br><strong>Towards accelerating physical discovery via non-interactive and interactive multi-fidelity Bayesian Optimization: Current challenges and future opportunities</strong><br><button class=copy-to-clipboard title="Towards accelerating physical discovery via non-interactive and interactive multi-fidelity Bayesian Optimization: Current challenges and future opportunities" index=143>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-143 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 40<br>Keywords: Active Learning, Simulation, Simulator, human-in-the-loop<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13402v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13402v1.pdf filename=2402.13402v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Both computational and experimental material discovery bring forth the challenge of exploring multidimensional and often non-differentiable parameter spaces, such as phase diagrams of Hamiltonians with multiple interactions, composition spaces of combinatorial libraries, processing spaces, and molecular embedding spaces. Often these systems are expensive or time-consuming to evaluate a single instance, and hence classical approaches based on exhaustive grid or random search are too data intensive. This resulted in strong interest towards <b>active</b> <b>learning</b> methods such as Bayesian optimization (BO) where the adaptive exploration occurs based on human learning (discovery) objective. However, classical BO is based on a predefined optimization target, and policies balancing exploration and exploitation are purely data driven. In practical settings, the domain expert can pose prior knowledge on the system in form of partially known physics laws and often varies exploration policies during the experiment. Here, we explore interactive workflows building on multi-fidelity BO (MFBO), starting with classical (data-driven) MFBO, then structured (physics-driven) sMFBO, and extending it to allow human in the loop interactive iMFBO workflows for adaptive and domain expert aligned exploration. These approaches are demonstrated over highly non-smooth multi-fidelity <b>simulation</b> data generated from an Ising model, considering spin-spin interaction as parameter space, lattice sizes as fidelity spaces, and the objective as maximizing heat capacity. Detailed analysis and comparison show the impact of physics knowledge injection and on-the-fly human decisions for improved exploration, current challenges, and potential opportunities for algorithm development with combining data, physics and real time human decisions.</p></p class="citation"></blockquote><h3 id=1067--144316-transformer-tricks-precomputing-the-first-layer-nils-graef-2024>(10/67 | 144/316) Transformer tricks: Precomputing the first layer (Nils Graef, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nils Graef. (2024)<br><strong>Transformer tricks: Precomputing the first layer</strong><br><button class=copy-to-clipboard title="Transformer tricks: Precomputing the first layer" index=144>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-144 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 40<br>Keywords: LLaMA, Mistral, PaLM, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13388v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13388v2.pdf filename=2402.13388v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This micro-paper describes a trick to speed up inference of <b>transformers</b> with RoPE (such as <b>LLaMA,</b> <b>Mistral,</b> <b>PaLM,</b> and Gemma). For these models, a large portion of the first <b>transformer</b> layer can be precomputed, which results in slightly lower latency and lower cost-per-token. Because this trick optimizes only one layer, the relative savings depend on the total number of layers. For example, the maximum savings for a model with only 4 layers (such as Whisper tiny) is limited to 25%, while a 32-layer model (such as <b>Mistral-7B)</b> is limited to 3% savings.</p></p class="citation"></blockquote><h3 id=1167--145316-referee-meta-learning-for-fast-adaptation-of-locational-fairness-weiye-chen-et-al-2024>(11/67 | 145/316) Referee-Meta-Learning for Fast Adaptation of Locational Fairness (Weiye Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Weiye Chen, Yiqun Xie, Xiaowei Jia, Erhu He, Han Bao, Bang An, Xun Zhou. (2024)<br><strong>Referee-Meta-Learning for Fast Adaptation of Locational Fairness</strong><br><button class=copy-to-clipboard title="Referee-Meta-Learning for Fast Adaptation of Locational Fairness" index=145>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-145 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CY, cs-LG, cs.LG<br>Keyword Score: 40<br>Keywords: Fairness, Few-shot, Fine-tuning, Meta Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13379v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13379v1.pdf filename=2402.13379v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>When dealing with data from distinct locations, machine learning algorithms tend to demonstrate an implicit preference of some locations over the others, which constitutes biases that sabotage the spatial <b>fairness</b> of the algorithm. This unfairness can easily introduce biases in subsequent decision-making given broad adoptions of learning-based solutions in practice. However, locational biases in AI are largely understudied. To mitigate biases over locations, we propose a locational <b>meta-referee</b> <b>(Meta-Ref)</b> <b>to</b> oversee the <b>few-shot</b> <b>meta-training</b> <b>and</b> <b>meta-testing</b> <b>of</b> a deep neural network. <b>Meta-Ref</b> <b>dynamically</b> adjusts the learning rates for training samples of given locations to advocate a fair performance across locations, through an explicit consideration of locational biases and the characteristics of input data. We present a three-phase training framework to learn both a <b>meta-learning-based</b> <b>predictor</b> and an integrated <b>Meta-Ref</b> <b>that</b> governs the <b>fairness</b> of the model. Once trained with a distribution of spatial tasks, <b>Meta-Ref</b> <b>is</b> applied to samples from new spatial tasks (i.e., regions outside the training area) to promote <b>fairness</b> during the <b>fine-tune</b> step. We carried out experiments with two case studies on crop monitoring and transportation safety, which show <b>Meta-Ref</b> <b>can</b> improve locational <b>fairness</b> while keeping the overall prediction quality at a similar level.</p></p class="citation"></blockquote><h3 id=1267--146316-chain-of-thought-empowers-transformers-to-solve-inherently-serial-problems-zhiyuan-li-et-al-2024>(12/67 | 146/316) Chain of Thought Empowers Transformers to Solve Inherently Serial Problems (Zhiyuan Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhiyuan Li, Hong Liu, Denny Zhou, Tengyu Ma. (2024)<br><strong>Chain of Thought Empowers Transformers to Solve Inherently Serial Problems</strong><br><button class=copy-to-clipboard title="Chain of Thought Empowers Transformers to Solve Inherently Serial Problems" index=146>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-146 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CC, cs-LG, cs.LG, stat-ML<br>Keyword Score: 40<br>Keywords: Transformer, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12875v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12875v1.pdf filename=2402.12875v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Instructing the model to generate a sequence of intermediate steps, a.k.a., a chain of thought (CoT), is a highly effective method to improve the accuracy of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> on arithmetics and symbolic <b>reasoning</b> tasks. However, the mechanism behind CoT remains unclear. This work provides a theoretical understanding of the power of CoT for decoder-only <b>transformers</b> through the lens of expressiveness. Conceptually, CoT empowers the model with the ability to perform inherently serial computation, which is otherwise lacking in <b>transformers,</b> especially when depth is low. Given input length $n$, previous works have shown that constant-depth <b>transformers</b> with finite precision $\mathsf{poly}(n)$ embedding size can only solve problems in $\mathsf{TC}^0$ without CoT. We first show an even tighter expressiveness upper bound for constant-depth <b>transformers</b> with constant-bit precision, which can only solve problems in $\mathsf{AC}^0$, a proper subset of $ \mathsf{TC}^0$. However, with $T$ steps of CoT, constant-depth <b>transformers</b> using constant-bit precision and $O(\log n)$ embedding size can solve any problem solvable by boolean circuits of size $T$. Empirically, enabling CoT dramatically improves the accuracy for tasks that are hard for parallel computation, including the composition of permutation groups, iterated squaring, and circuit value problems, especially for low-depth <b>transformers.</b></p></p class="citation"></blockquote><h3 id=1367--147316-beyond-worst-case-attacks-robust-rl-with-adaptive-defense-via-non-dominated-policies-xiangyu-liu-et-al-2024>(13/67 | 147/316) Beyond Worst-case Attacks: Robust RL with Adaptive Defense via Non-dominated Policies (Xiangyu Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiangyu Liu, Chenghao Deng, Yanchao Sun, Yongyuan Liang, Furong Huang. (2024)<br><strong>Beyond Worst-case Attacks: Robust RL with Adaptive Defense via Non-dominated Policies</strong><br><button class=copy-to-clipboard title="Beyond Worst-case Attacks: Robust RL with Adaptive Defense via Non-dominated Policies" index=147>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-147 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 40<br>Keywords: Bandit Algorithm, Reinforcement Learning, Prompt, Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12673v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12673v1.pdf filename=2402.12673v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In light of the burgeoning success of <b>reinforcement</b> <b>learning</b> (RL) in diverse real-world applications, considerable focus has been directed towards ensuring RL policies are robust to <b>adversarial</b> <b>attacks</b> during test time. Current approaches largely revolve around solving a minimax problem to prepare for potential worst-case scenarios. While effective against strong attacks, these methods often compromise performance in the absence of attacks or the presence of only weak attacks. To address this, we study policy robustness under the well-accepted state-adversarial attack model, extending our focus beyond only worst-case attacks. We first formalize this task at test time as a regret minimization problem and establish its intrinsic hardness in achieving sublinear regret when the baseline policy is from a general continuous policy class, $\Pi$. This finding <b>prompts</b> us to \textit{refine} the baseline policy class $\Pi$ prior to test time, aiming for efficient adaptation within a finite policy class $\Tilde{\Pi}$, which can resort to an <b>adversarial</b> <b>bandit</b> subroutine. In light of the importance of a small, finite $\Tilde{\Pi}$, we propose a novel training-time algorithm to iteratively discover \textit{non-dominated policies}, forming a near-optimal and minimal $\Tilde{\Pi}$, thereby ensuring both robustness and test-time efficiency. Empirical validation on the Mujoco corroborates the superiority of our approach in terms of natural and robust performance, as well as adaptability to various attack scenarios.</p></p class="citation"></blockquote><h3 id=1467--148316-trap-targeted-random-adversarial-prompt-honeypot-for-black-box-identification-martin-gubri-et-al-2024>(14/67 | 148/316) TRAP: Targeted Random Adversarial Prompt Honeypot for Black-Box Identification (Martin Gubri et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Martin Gubri, Dennis Ulmer, Hwaran Lee, Sangdoo Yun, Seong Joon Oh. (2024)<br><strong>TRAP: Targeted Random Adversarial Prompt Honeypot for Black-Box Identification</strong><br><button class=copy-to-clipboard title="TRAP: Targeted Random Adversarial Prompt Honeypot for Black-Box Identification" index=148>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-148 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CL, cs-CR, cs-LG, cs.LG<br>Keyword Score: 35<br>Keywords: Black Box, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12991v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12991v1.pdf filename=2402.12991v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Model</b> <b>(LLM)</b> services and models often come with legal rules on who can use them and how they must use them. Assessing the compliance of the released <b>LLMs</b> is crucial, as these rules protect the interests of the <b>LLM</b> contributor and prevent misuse. In this context, we describe the novel problem of <b>Black-box</b> <b>Identity</b> Verification (BBIV). The goal is to determine whether a third-party application uses a certain <b>LLM</b> through its chat function. We propose a method called Targeted Random Adversarial <b>Prompt</b> (TRAP) that identifies the specific <b>LLM</b> in use. We repurpose adversarial suffixes, originally proposed for jailbreaking, to get a pre-defined answer from the target <b>LLM,</b> while other models give random answers. TRAP detects the target <b>LLMs</b> with over 95% true positive rate at under 0.2% false positive rate even after a single interaction. TRAP remains effective even if the <b>LLM</b> has minor changes that do not significantly alter the original function.</p></p class="citation"></blockquote><h3 id=1567--149316-linksage-optimizing-job-matching-using-graph-neural-networks-ping-liu-et-al-2024>(15/67 | 149/316) LinkSAGE: Optimizing Job Matching Using Graph Neural Networks (Ping Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ping Liu, Haichao Wei, Xiaochen Hou, Jianqiang Shen, Shihai He, Kay Qianqi Shen, Zhujun Chen, Fedor Borisyuk, Daniel Hewlett, Liang Wu, Srikant Veeraraghavan, Alex Tsun, Chengming Jiang, Wenjing Zhang. (2024)<br><strong>LinkSAGE: Optimizing Job Matching Using Graph Neural Networks</strong><br><button class=copy-to-clipboard title="LinkSAGE: Optimizing Job Matching Using Graph Neural Networks" index=149>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-149 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs-SI, cs.LG<br>Keyword Score: 33<br>Keywords: Graph, Graph Neural Network, Graph Neural Network, Transfer Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13430v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13430v1.pdf filename=2402.13430v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present LinkSAGE, an innovative framework that integrates <b>Graph</b> <b>Neural</b> <b>Networks</b> <b>(GNNs)</b> into large-scale personalized job matching systems, designed to address the complex dynamics of LinkedIns extensive professional network. Our approach capitalizes on a novel job marketplace <b>graph,</b> <b>the</b> <b>largest</b> and most intricate of its kind in industry, with billions of nodes and edges. This <b>graph</b> <b>is</b> <b>not</b> merely extensive but also richly detailed, encompassing member and job nodes along with key attributes, thus creating an expansive and interwoven network. A key innovation in LinkSAGE is its training and serving methodology, which effectively combines inductive <b>graph</b> <b>learning</b> <b>on</b> a heterogeneous, evolving <b>graph</b> <b>with</b> <b>an</b> encoder-decoder <b>GNN</b> model. This methodology decouples the training of the <b>GNN</b> model from that of existing Deep Neural Nets (DNN) models, eliminating the need for frequent <b>GNN</b> retraining while maintaining up-to-date <b>graph</b> <b>signals</b> <b>in</b> near realtime, allowing for the effective integration of <b>GNN</b> insights through <b>transfer</b> <b>learning.</b> The subsequent nearline inference system serves the <b>GNN</b> encoder within a real-world setting, significantly reducing online latency and obviating the need for costly real-time <b>GNN</b> infrastructure. Validated across multiple online A/B tests in diverse product scenarios, LinkSAGE demonstrates marked improvements in member engagement, relevance matching, and member retention, confirming its generalizability and practical impact.</p></p class="citation"></blockquote><h3 id=1667--150316-buffgraph-enhancing-class-imbalanced-node-classification-via-buffer-nodes-qian-wang-et-al-2024>(16/67 | 150/316) BuffGraph: Enhancing Class-Imbalanced Node Classification via Buffer Nodes (Qian Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qian Wang, Zemin Liu, Zhen Zhang, Bingsheng He. (2024)<br><strong>BuffGraph: Enhancing Class-Imbalanced Node Classification via Buffer Nodes</strong><br><button class=copy-to-clipboard title="BuffGraph: Enhancing Class-Imbalanced Node Classification via Buffer Nodes" index=150>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-150 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 33<br>Keywords: Node Classification, Graph, Graph Neural Network, Graph Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13114v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13114v1.pdf filename=2402.13114v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Class imbalance in <b>graph-structured</b> <b>data,</b> <b>where</b> minor classes are significantly underrepresented, poses a critical challenge for <b>Graph</b> <b>Neural</b> <b>Networks</b> <b>(GNNs).</b> To address this challenge, existing studies generally generate new minority <b>nodes</b> <b>and</b> edges connecting new <b>nodes</b> <b>to</b> the original <b>graph</b> <b>to</b> <b>make</b> classes balanced. However, they do not solve the problem that majority classes still propagate information to minority <b>nodes</b> <b>by</b> edges in the original <b>graph</b> <b>which</b> <b>introduces</b> bias towards majority classes. To address this, we introduce BuffGraph, which inserts buffer <b>nodes</b> <b>into</b> the <b>graph,</b> <b>modulating</b> <b>the</b> impact of majority classes to improve minor class representation. Our extensive experiments across diverse real-world datasets empirically demonstrate that BuffGraph outperforms existing baseline methods in class-imbalanced <b>node</b> <b>classification</b> in both natural settings and imbalanced settings. Code is available at <a href=https://anonymous.4open.science/r/BuffGraph-730A>https://anonymous.4open.science/r/BuffGraph-730A</a>.</p></p class="citation"></blockquote><h3 id=1767--151316-align-your-intents-offline-imitation-learning-via-optimal-transport-maksim-bobrin-et-al-2024>(17/67 | 151/316) Align Your Intents: Offline Imitation Learning via Optimal Transport (Maksim Bobrin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Maksim Bobrin, Nazar Buzun, Dmitrii Krylov, Dmitry V. Dylov. (2024)<br><strong>Align Your Intents: Offline Imitation Learning via Optimal Transport</strong><br><button class=copy-to-clipboard title="Align Your Intents: Offline Imitation Learning via Optimal Transport" index=151>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-151 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 33<br>Keywords: Benchmarking, Knowledge Distillation, Offline Reinforcement Learning, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13037v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13037v1.pdf filename=2402.13037v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Offline</b> <b>reinforcement</b> <b>learning</b> (RL) addresses the problem of sequential decision-making by learning optimal policy through pre-collected data, without interacting with the environment. As yet, it has remained somewhat impractical, because one rarely knows the reward explicitly and it is hard to <b>distill</b> it retrospectively. Here, we show that an imitating agent can still learn the desired behavior merely from observing the expert, despite the absence of explicit rewards or action labels. In our method, AILOT (Aligned Imitation Learning via Optimal Transport), we involve special representation of states in a form of intents that incorporate pairwise spatial distances within the data. Given such representations, we define intrinsic reward function via optimal transport distance between the expert&rsquo;s and the agent&rsquo;s trajectories. We report that AILOT outperforms state-of-the art <b>offline</b> <b>imitation</b> <b>learning</b> algorithms on D4RL <b>benchmarks</b> and improves the performance of other <b>offline</b> <b>RL</b> <b>algorithms</b> in the sparse-reward tasks.</p></p class="citation"></blockquote><h3 id=1867--152316-graphgini-fostering-individual-and-group-fairness-in-graph-neural-networks-anuj-kumar-sirohi-et-al-2024>(18/67 | 152/316) GRAPHGINI: Fostering Individual and Group Fairness in Graph Neural Networks (Anuj Kumar Sirohi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Anuj Kumar Sirohi, Anjali Gupta, Sayan Ranu, Sandeep Kumar, Amitabha Bagchi. (2024)<br><strong>GRAPHGINI: Fostering Individual and Group Fairness in Graph Neural Networks</strong><br><button class=copy-to-clipboard title="GRAPHGINI: Fostering Individual and Group Fairness in Graph Neural Networks" index=152>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-152 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs-SI, cs.LG<br>Keyword Score: 33<br>Keywords: Graph, Graph Neural Network, Graph Neural Network, Fairness<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12937v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12937v1.pdf filename=2402.12937v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We address the growing apprehension that <b>GNNs,</b> in the absence of <b>fairness</b> constraints, might produce biased decisions that disproportionately affect underprivileged groups or individuals. Departing from previous work, we introduce for the first time a method for incorporating the Gini coefficient as a measure of <b>fairness</b> to be used within the <b>GNN</b> framework. Our proposal, GRAPHGINI, works with the two different goals of individual and group <b>fairness</b> in a single system, while maintaining high prediction accuracy. GRAPHGINI enforces individual <b>fairness</b> through learnable attention scores that help in aggregating more information through similar nodes. A heuristic-based maximum Nash social welfare constraint ensures the maximum possible group <b>fairness.</b> Both the individual <b>fairness</b> constraint and the group <b>fairness</b> constraint are stated in terms of a differentiable approximation of the Gini coefficient. This approximation is a contribution that is likely to be of interest even beyond the scope of the problem studied in this paper. Unlike other state-of-the-art, GRAPHGINI automatically balances all three optimization objectives (utility, individual, and group <b>fairness)</b> of the <b>GNN</b> and is free from any manual tuning of weight parameters. Extensive experimentation on real-world datasets showcases the efficacy of GRAPHGINI in making significant improvements in individual <b>fairness</b> compared to all currently available state-of-the-art methods while maintaining utility and group equality.</p></p class="citation"></blockquote><h3 id=1967--153316-scalable-and-reliable-deep-transfer-learning-for-intelligent-fault-detection-via-multi-scale-neural-processes-embedded-with-knowledge-zhongzhi-li-et-al-2024>(19/67 | 153/316) Scalable and reliable deep transfer learning for intelligent fault detection via multi-scale neural processes embedded with knowledge (Zhongzhi Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhongzhi Li, Jingqi Tu, Jiacheng Zhu, Jianliang Ai, Yiqun Dong. (2024)<br><strong>Scalable and reliable deep transfer learning for intelligent fault detection via multi-scale neural processes embedded with knowledge</strong><br><button class=copy-to-clipboard title="Scalable and reliable deep transfer learning for intelligent fault detection via multi-scale neural processes embedded with knowledge" index=153>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-153 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 33<br>Keywords: Graph, Convolution, Convolutional Neural Network, Transfer Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12729v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12729v1.pdf filename=2402.12729v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep <b>transfer</b> <b>learning</b> (DTL) is a fundamental method in the field of Intelligent Fault Detection (IFD). It aims to mitigate the degradation of method performance that arises from the discrepancies in data distribution between training set (source domain) and testing set (target domain). Considering the fact that fault data collection is challenging and certain faults are scarce, DTL-based methods face the limitation of available observable data, which reduces the detection performance of the methods in the target domain. Furthermore, DTL-based methods lack comprehensive uncertainty analysis that is essential for building reliable IFD systems. To address the aforementioned problems, this paper proposes a novel DTL-based method known as Neural Processes-based deep <b>transfer</b> <b>learning</b> with <b>graph</b> <b>convolution</b> <b>network</b> (GTNP). Feature-based <b>transfer</b> <b>strategy</b> of GTNP bridges the data distribution discrepancies of source domain and target domain in high-dimensional space. Both the joint modeling based on global and local latent variables and sparse sampling strategy reduce the demand of observable data in the target domain. The multi-scale uncertainty analysis is obtained by using the distribution characteristics of global and local latent variables. Global analysis of uncertainty enables GTNP to provide quantitative values that reflect the complexity of methods and the difficulty of tasks. Local analysis of uncertainty allows GTNP to model uncertainty (confidence of the fault detection result) at each sample affected by noise and bias. The validation of the proposed method is conducted across 3 IFD tasks, consistently showing the superior detection performance of GTNP compared to the other DTL-based methods.</p></p class="citation"></blockquote><h3 id=2067--154316-it-intrusion-detection-using-statistical-learning-and-testbed-measurements-xiaoxuan-wang-et-al-2024>(20/67 | 154/316) IT Intrusion Detection Using Statistical Learning and Testbed Measurements (Xiaoxuan Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiaoxuan Wang, Rolf Stadler. (2024)<br><strong>IT Intrusion Detection Using Statistical Learning and Testbed Measurements</strong><br><button class=copy-to-clipboard title="IT Intrusion Detection Using Statistical Learning and Testbed Measurements" index=154>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-154 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CR, cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: LSTM, LSTM, LSTM<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13081v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13081v1.pdf filename=2402.13081v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We study automated intrusion detection in an IT infrastructure, specifically the problem of identifying the start of an attack, the type of attack, and the sequence of actions an attacker takes, based on continuous measurements from the infrastructure. We apply statistical learning methods, including Hidden Markov Model (HMM), <b>Long</b> <b>Short-Term</b> <b>Memory</b> <b>(LSTM),</b> and Random Forest Classifier (RFC) to map sequences of observations to sequences of predicted attack actions. In contrast to most related research, we have abundant data to train the models and evaluate their predictive power. The data comes from traces we generate on an in-house testbed where we run attacks against an emulated IT infrastructure. Central to our work is a machine-learning pipeline that maps measurements from a high-dimensional observation space to a space of low dimensionality or to a small set of observation symbols. Investigating intrusions in offline as well as online scenarios, we find that both HMM and <b>LSTM</b> can be effective in predicting attack start time, attack type, and attack actions. If sufficient training data is available, <b>LSTM</b> achieves higher prediction accuracy than HMM. HMM, on the other hand, requires less computational resources and less training data for effective prediction. Also, we find that the methods we study benefit from data produced by traditional intrusion detection systems like SNORT.</p></p class="citation"></blockquote><h3 id=2167--155316-improve-cross-architecture-generalization-on-dataset-distillation-binglin-zhou-et-al-2024>(21/67 | 155/316) Improve Cross-Architecture Generalization on Dataset Distillation (Binglin Zhou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Binglin Zhou, Linhao Zhong, Wentao Chen. (2024)<br><strong>Improve Cross-Architecture Generalization on Dataset Distillation</strong><br><button class=copy-to-clipboard title="Improve Cross-Architecture Generalization on Dataset Distillation" index=155>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-155 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CV, cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Knowledge Distillation, Knowledge Distillation, Knowledge Distillation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13007v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13007v1.pdf filename=2402.13007v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Dataset <b>distillation,</b> a pragmatic approach in machine learning, aims to create a smaller synthetic dataset from a larger existing dataset. However, existing <b>distillation</b> methods primarily adopt a model-based paradigm, where the synthetic dataset inherits model-specific biases, limiting its generalizability to alternative models. In response to this constraint, we propose a novel methodology termed &ldquo;model pool&rdquo;. This approach involves selecting models from a diverse model pool based on a specific probability distribution during the data <b>distillation</b> process. Additionally, we integrate our model pool with the established <b>knowledge</b> <b>distillation</b> approach and apply <b>knowledge</b> <b>distillation</b> to the test process of the <b>distilled</b> dataset. Our experimental results validate the effectiveness of the model pool approach across a range of existing models while testing, demonstrating superior performance compared to existing methodologies.</p></p class="citation"></blockquote><h3 id=2267--156316-investigating-the-impact-of-model-instability-on-explanations-and-uncertainty-sara-vera-marjanović-et-al-2024>(22/67 | 156/316) Investigating the Impact of Model Instability on Explanations and Uncertainty (Sara Vera Marjanović et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sara Vera Marjanović, Isabelle Augenstein, Christina Lioma. (2024)<br><strong>Investigating the Impact of Model Instability on Explanations and Uncertainty</strong><br><button class=copy-to-clipboard title="Investigating the Impact of Model Instability on Explanations and Uncertainty" index=156>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-156 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CL, cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Explainable AI, Transformer, Pre-trained Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13006v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13006v1.pdf filename=2402.13006v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Explainable</b> <b>AI</b> methods facilitate the understanding of model behaviour, yet, small, imperceptible perturbations to inputs can vastly distort explanations. As these explanations are typically evaluated holistically, before model deployment, it is difficult to assess when a particular explanation is trustworthy. Some studies have tried to create confidence estimators for explanations, but none have investigated an existing link between uncertainty and explanation quality. We artificially simulate epistemic uncertainty in text input by introducing noise at inference time. In this large-scale empirical study, we insert different levels of noise perturbations and measure the effect on the output of <b>pre-trained</b> <b>language</b> <b>models</b> and different uncertainty metrics. Realistic perturbations have minimal effect on performance and explanations, yet masking has a drastic effect. We find that high uncertainty doesn&rsquo;t necessarily imply low explanation plausibility; the correlation between the two metrics can be moderately positive when noise is exposed during the training process. This suggests that noise-augmented models may be better at identifying salient tokens when uncertain. Furthermore, when predictive and epistemic uncertainty measures are over-confident, the robustness of a saliency map to perturbation can indicate model stability issues. Integrated Gradients shows the overall greatest robustness to perturbation, while still showing model-specific patterns in performance; however, this phenomenon is limited to smaller <b>Transformer-based</b> language models.</p></p class="citation"></blockquote><h3 id=2367--157316-stochastic-approximation-approach-to-federated-machine-learning-srihari-p-v-et-al-2024>(23/67 | 157/316) Stochastic Approximation Approach to Federated Machine Learning (Srihari P V et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Srihari P V, Bharath Bhikkaji. (2024)<br><strong>Stochastic Approximation Approach to Federated Machine Learning</strong><br><button class=copy-to-clipboard title="Stochastic Approximation Approach to Federated Machine Learning" index=157>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-157 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Federated Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12945v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12945v1.pdf filename=2402.12945v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper examines <b>Federated</b> <b>learning</b> (FL) in a Stochastic Approximation (SA) framework. FL is a collaborative way to train neural network models across various participants or clients without centralizing their data. Each client will train a model on their respective data and send the weights across to a the server periodically for aggregation. The server aggregates these weights which are then used by the clients to re-initialize their neural network and continue the training. SA is an iterative algorithm that uses approximate sample gradients and tapering step size to locate a minimizer of a cost function. In this paper the clients use a stochastic approximation iterate to update the weights of its neural network. It is shown that the aggregated weights track an autonomous ODE. Numerical <b>simulations</b> are performed and the results are compared with standard algorithms like FedAvg and FedProx. It is observed that the proposed algorithm is robust and gives more reliable estimates of the weights, in particular when the clients data are not identically distributed.</p></p class="citation"></blockquote><h3 id=2467--158316-analysis-of-using-sigmoid-loss-for-contrastive-learning-chungpa-lee-et-al-2024>(24/67 | 158/316) Analysis of Using Sigmoid Loss for Contrastive Learning (Chungpa Lee et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chungpa Lee, Joonhwan Chang, Jy-yong Sohn. (2024)<br><strong>Analysis of Using Sigmoid Loss for Contrastive Learning</strong><br><button class=copy-to-clipboard title="Analysis of Using Sigmoid Loss for Contrastive Learning" index=158>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-158 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Contrastive Learning, Self-supervised Learning, Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12613v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12613v1.pdf filename=2402.12613v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Contrastive</b> <b>learning</b> has emerged as a prominent branch of <b>self-supervised</b> <b>learning</b> for several years. Especially, CLIP, which applies <b>contrastive</b> <b>learning</b> to large sets of captioned images, has garnered significant attention. Recently, SigLIP, a variant of CLIP, has been proposed, which uses the sigmoid loss instead of the standard InfoNCE loss. SigLIP achieves the performance comparable to CLIP in a more efficient manner by eliminating the need for a global view. However, theoretical understanding of using the sigmoid loss in <b>contrastive</b> <b>learning</b> is underexplored. In this paper, we provide a theoretical analysis of using the sigmoid loss in <b>contrastive</b> <b>learning,</b> in the perspective of the geometric structure of learned embeddings. First, we propose the double-Constant Embedding Model (CCEM), a framework for parameterizing various well-known embedding structures by a single variable. Interestingly, the proposed CCEM is proven to contain the optimal embedding with respect to the sigmoid loss. Second, we mathematically analyze the optimal embedding minimizing the sigmoid loss for <b>contrastive</b> <b>learning.</b> The optimal embedding ranges from simplex equiangular-tight-frame to antipodal structure, depending on the temperature parameter used in the sigmoid loss. Third, our experimental results on synthetic datasets coincide with the theoretical results on the optimal embedding structures.</p></p class="citation"></blockquote><h3 id=2567--159316-structural-knowledge-informed-continual-multivariate-time-series-forecasting-zijie-pan-et-al-2024>(25/67 | 159/316) Structural Knowledge Informed Continual Multivariate Time Series Forecasting (Zijie Pan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zijie Pan, Yushan Jiang, Dongjin Song, Sahil Garg, Kashif Rasul, Anderson Schneider, Yuriy Nevmyvaka. (2024)<br><strong>Structural Knowledge Informed Continual Multivariate Time Series Forecasting</strong><br><button class=copy-to-clipboard title="Structural Knowledge Informed Continual Multivariate Time Series Forecasting" index=159>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-159 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 26<br>Keywords: Graph, Benchmarking, Continual Learning, Neural Machine Translation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12722v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12722v1.pdf filename=2402.12722v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent studies in multivariate time series <b>(MTS)</b> forecasting reveal that explicitly modeling the hidden dependencies among different time series can yield promising forecasting performance and reliable explanations. However, modeling variable dependencies remains underexplored when <b>MTS</b> is continuously accumulated under different regimes (stages). Due to the potential distribution and dependency disparities, the underlying model may encounter the catastrophic forgetting problem, i.e., it is challenging to memorize and infer different types of variable dependencies across different regimes while maintaining forecasting performance. To address this issue, we propose a novel Structural Knowledge Informed <b>Continual</b> <b>Learning</b> (SKI-CL) framework to perform <b>MTS</b> forecasting within a <b>continual</b> <b>learning</b> paradigm, which leverages structural knowledge to steer the forecasting model toward identifying and adapting to different regimes, and selects representative <b>MTS</b> samples from each regime for memory replay. Specifically, we develop a forecasting model based on <b>graph</b> structure learning, where a consistency regularization scheme is imposed between the learned variable dependencies and the structural knowledge while optimizing the forecasting objective over the <b>MTS</b> data. As such, <b>MTS</b> representations learned in each regime are associated with distinct structural knowledge, which helps the model memorize a variety of conceivable scenarios and results in accurate forecasts in the <b>continual</b> <b>learning</b> context. Meanwhile, we develop a representation-matching memory replay scheme that maximizes the temporal coverage of <b>MTS</b> data to efficiently preserve the underlying temporal dynamics and dependency structures of each regime. Thorough empirical studies on synthetic and real-world <b>benchmarks</b> validate SKI-CL&rsquo;s efficacy and advantages over the state-of-the-art for <b>continual</b> <b>MTS</b> forecasting tasks.</p></p class="citation"></blockquote><h3 id=2667--160316-enhancing-real-world-complex-network-representations-with-hyperedge-augmentation-xiangyu-zhao-et-al-2024>(26/67 | 160/316) Enhancing Real-World Complex Network Representations with Hyperedge Augmentation (Xiangyu Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiangyu Zhao, Zehui Li, Mingzhu Shen, Guy-Bart Stan, Pietro Liò, Yiren Zhao. (2024)<br><strong>Enhancing Real-World Complex Network Representations with Hyperedge Augmentation</strong><br><button class=copy-to-clipboard title="Enhancing Real-World Complex Network Representations with Hyperedge Augmentation" index=160>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-160 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-IR, cs-LG, cs-SI, cs.LG<br>Keyword Score: 23<br>Keywords: Graph, Graph Neural Network, Graph Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13033v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13033v1.pdf filename=2402.13033v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Graph</b> <b>augmentation</b> <b>methods</b> play a crucial role in improving the performance and enhancing generalisation capabilities in <b>Graph</b> <b>Neural</b> <b>Networks</b> <b>(GNNs).</b> Existing <b>graph</b> <b>augmentation</b> <b>methods</b> mainly perturb the <b>graph</b> <b>structures</b> <b>and</b> are usually limited to pairwise node relations. These methods cannot fully address the complexities of real-world large-scale networks that often involve higher-order node relations beyond only being pairwise. Meanwhile, real-world <b>graph</b> <b>datasets</b> <b>are</b> predominantly modelled as simple <b>graphs,</b> <b>due</b> <b>to</b> the scarcity of data that can be used to form higher-order edges. Therefore, reconfiguring the higher-order edges as an integration into <b>graph</b> <b>augmentation</b> <b>strategies</b> lights up a promising research path to address the aforementioned issues. In this paper, we present Hyperedge Augmentation (HyperAug), a novel <b>graph</b> <b>augmentation</b> <b>method</b> that constructs virtual hyperedges directly form the raw data, and produces auxiliary node features by extracting from the virtual hyperedge information, which are used for enhancing <b>GNN</b> performances on downstream tasks. We design three diverse virtual hyperedge construction strategies to accompany the augmentation scheme: (1) via <b>graph</b> <b>statistics,</b> <b>(2)</b> from multiple data perspectives, and (3) utilising multi-modality. Furthermore, to facilitate HyperAug evaluation, we provide 23 novel real-world <b>graph</b> <b>datasets</b> <b>across</b> various domains including social media, biology, and e-commerce. Our empirical study shows that HyperAug consistently and significantly outperforms <b>GNN</b> baselines and other <b>graph</b> <b>augmentation</b> <b>methods,</b> across a variety of application contexts, which clearly indicates that it can effectively incorporate higher-order node relations into <b>graph</b> <b>augmentation</b> <b>methods</b> for real-world complex networks.</p></p class="citation"></blockquote><h3 id=2767--161316-partial-search-in-a-frozen-network-is-enough-to-find-a-strong-lottery-ticket-hikari-otsuka-et-al-2024>(27/67 | 161/316) Partial Search in a Frozen Network is Enough to Find a Strong Lottery Ticket (Hikari Otsuka et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hikari Otsuka, Daiki Chijiwa, Ángel López García-Arias, Yasuyuki Okoshi, Kazushi Kawamura, Thiem Van Chu, Daichi Fujiki, Susumu Takeuchi, Masato Motomura. (2024)<br><strong>Partial Search in a Frozen Network is Enough to Find a Strong Lottery Ticket</strong><br><button class=copy-to-clipboard title="Partial Search in a Frozen Network is Enough to Find a Strong Lottery Ticket" index=161>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-161 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG, stat-ML<br>Keyword Score: 23<br>Keywords: Graph, Graph Neural Network, Pruning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14029v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14029v1.pdf filename=2402.14029v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Randomly initialized dense networks contain subnetworks that achieve high accuracy without weight learning &ndash; strong lottery tickets (SLTs). Recently, Gadhikar et al. (2023) demonstrated theoretically and experimentally that SLTs can also be found within a randomly pruned source network, thus reducing the SLT search space. However, this limits the search to SLTs that are even sparser than the source, leading to worse accuracy due to unintentionally high sparsity. This paper proposes a method that reduces the SLT search space by an arbitrary ratio that is independent of the desired SLT sparsity. A random subset of the initial weights is excluded from the search space by freezing it &ndash; i.e., by either permanently <b>pruning</b> them or locking them as a fixed part of the SLT. Indeed, the SLT existence in such a reduced search space is theoretically guaranteed by our subset-sum approximation with randomly frozen variables. In addition to reducing search space, the random freezing pattern can also be exploited to reduce model size in inference. Furthermore, experimental results show that the proposed method finds SLTs with better accuracy and model size trade-off than the SLTs obtained from dense or randomly pruned source networks. In particular, the SLT found in a frozen <b>graph</b> <b>neural</b> <b>network</b> achieves higher accuracy than its weight trained counterpart while reducing model size by $40.3\times$.</p></p class="citation"></blockquote><h3 id=2867--162316-bayesian-neural-networks-with-domain-knowledge-priors-dylan-sam-et-al-2024>(28/67 | 162/316) Bayesian Neural Networks with Domain Knowledge Priors (Dylan Sam et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dylan Sam, Rattana Pukdee, Daniel P. Jeong, Yewon Byun, J. Zico Kolter. (2024)<br><strong>Bayesian Neural Networks with Domain Knowledge Priors</strong><br><button class=copy-to-clipboard title="Bayesian Neural Networks with Domain Knowledge Priors" index=162>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-162 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-ML<br>Keyword Score: 20<br>Keywords: Fairness, Gaussian Process<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13410v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13410v1.pdf filename=2402.13410v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Bayesian neural networks (BNNs) have recently gained popularity due to their ability to quantify model uncertainty. However, specifying a prior for BNNs that captures relevant domain knowledge is often extremely challenging. In this work, we propose a framework for integrating general forms of domain knowledge (i.e., any knowledge that can be represented by a loss function) into a BNN prior through variational inference, while enabling computationally efficient posterior inference and sampling. Specifically, our approach results in a prior over neural network weights that assigns high probability mass to models that better align with our domain knowledge, leading to posterior samples that also exhibit this behavior. We show that BNNs using our proposed domain knowledge priors outperform those with standard priors (e.g., isotropic <b>Gaussian,</b> <b>Gaussian</b> <b>process),</b> successfully incorporating diverse types of prior information such as <b>fairness,</b> physics rules, and healthcare knowledge and achieving better predictive performance. We also present techniques for transferring the learned priors across different model architectures, demonstrating their broad utility across various settings.</p></p class="citation"></blockquote><h3 id=2967--163316-the-uncanny-valley-a-comprehensive-analysis-of-diffusion-models-karam-ghanem-et-al-2024>(29/67 | 163/316) The Uncanny Valley: A Comprehensive Analysis of Diffusion Models (Karam Ghanem et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Karam Ghanem, Danilo Bzdok. (2024)<br><strong>The Uncanny Valley: A Comprehensive Analysis of Diffusion Models</strong><br><button class=copy-to-clipboard title="The Uncanny Valley: A Comprehensive Analysis of Diffusion Models" index=163>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-163 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: I-2-10; I-4-8; I-4-5; I-4-m, cs-AI, cs-CV, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Diffusion Model, Probabilistic Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13369v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13369v1.pdf filename=2402.13369v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Through <b>Diffusion</b> <b>Models</b> (DMs), we have made significant advances in generating high-quality images. Our exploration of these models delves deeply into their core operational principles by systematically investigating key aspects across various DM architectures: i) noise schedules, ii) samplers, and iii) guidance. Our comprehensive examination of these models sheds light on their hidden fundamental mechanisms, revealing the concealed foundational elements that are essential for their effectiveness. Our analyses emphasize the hidden key factors that determine model performance, offering insights that contribute to the advancement of DMs. Past findings show that the configuration of noise schedules, samplers, and guidance is vital to the quality of generated images; however, models reach a stable level of quality across different configurations at a remarkably similar point, revealing that the decisive factors for optimal performance predominantly reside in the <b>diffusion</b> <b>process</b> dynamics and the structural design of the model&rsquo;s network, rather than the specifics of configuration details. Our comparative analysis reveals that Denoising <b>Diffusion</b> <b>Probabilistic</b> <b>Model</b> (DDPM)-based <b>diffusion</b> <b>dynamics</b> consistently outperform the Noise Conditioned Score Network (NCSN)-based ones, not only when evaluated in their original forms but also when continuous through Stochastic Differential Equation (SDE)-based implementations.</p></p class="citation"></blockquote><h3 id=3067--164316-incentivized-exploration-via-filtered-posterior-sampling-anand-kalvit-et-al-2024>(30/67 | 164/316) Incentivized Exploration via Filtered Posterior Sampling (Anand Kalvit et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Anand Kalvit, Aleksandrs Slivkins, Yonatan Gur. (2024)<br><strong>Incentivized Exploration via Filtered Posterior Sampling</strong><br><button class=copy-to-clipboard title="Incentivized Exploration via Filtered Posterior Sampling" index=164>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-164 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, econ-TH<br>Keyword Score: 20<br>Keywords: Bandit Algorithm, Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13338v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13338v1.pdf filename=2402.13338v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We study &ldquo;incentivized exploration&rdquo; (IE) in social learning problems where the principal (a <b>recommendation</b> algorithm) can leverage information asymmetry to incentivize sequentially-arriving agents to take exploratory actions. We identify posterior sampling, an algorithmic approach that is well known in the multi-armed <b>bandits</b> literature, as a general-purpose solution for IE. In particular, we expand the existing scope of IE in several practically-relevant dimensions, from private agent types to informative <b>recommendations</b> to correlated Bayesian priors. We obtain a general analysis of posterior sampling in IE which allows us to subsume these extended settings as corollaries, while also recovering existing results as special cases.</p></p class="citation"></blockquote><h3 id=3167--165316-smore-similarity-based-hyperdimensional-domain-adaptation-for-multi-sensor-time-series-classification-junyao-wang-et-al-2024>(31/67 | 165/316) SMORE: Similarity-based Hyperdimensional Domain Adaptation for Multi-Sensor Time Series Classification (Junyao Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Junyao Wang, Mohammad Abdullah Al Faruque. (2024)<br><strong>SMORE: Similarity-based Hyperdimensional Domain Adaptation for Multi-Sensor Time Series Classification</strong><br><button class=copy-to-clipboard title="SMORE: Similarity-based Hyperdimensional Domain Adaptation for Multi-Sensor Time Series Classification" index=165>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-165 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Distribution Shift, Distribution Shift, Domain Adaptation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13233v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13233v1.pdf filename=2402.13233v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Many real-world applications of the Internet of Things (IoT) employ machine learning (ML) algorithms to analyze time series information collected by interconnected sensors. However, <b>distribution</b> <b>shift,</b> a fundamental challenge in data-driven ML, arises when a model is deployed on a data <b>distribution</b> <b>different</b> from the training data and can substantially degrade model performance. Additionally, increasingly sophisticated deep neural networks (DNNs) are required to capture intricate spatial and temporal dependencies in multi-sensor time series data, often exceeding the capabilities of today&rsquo;s edge devices. In this paper, we propose SMORE, a novel resource-efficient <b>domain</b> <b>adaptation</b> (DA) algorithm for multi-sensor time series classification, leveraging the efficient and parallel operations of hyperdimensional computing. SMORE dynamically customizes test-time models with explicit consideration of the <b>domain</b> <b>context</b> of each sample to mitigate the negative impacts of <b>domain</b> <b>shifts.</b> Our evaluation on a variety of multi-sensor time series classification tasks shows that SMORE achieves on average 1.98% higher accuracy than state-of-the-art (SOTA) DNN-based DA algorithms with 18.81x faster training and 4.63x faster inference.</p></p class="citation"></blockquote><h3 id=3267--166316-neural-network-diffusion-kai-wang-et-al-2024>(32/67 | 166/316) Neural Network Diffusion (Kai Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kai Wang, Zhaopan Xu, Yukun Zhou, Zelin Zang, Trevor Darrell, Zhuang Liu, Yang You. (2024)<br><strong>Neural Network Diffusion</strong><br><button class=copy-to-clipboard title="Neural Network Diffusion" index=166>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-166 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CV, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Diffusion Model, Autoencoder<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13144v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13144v1.pdf filename=2402.13144v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Diffusion</b> <b>models</b> have achieved remarkable success in image and video generation. In this work, we demonstrate that <b>diffusion</b> <b>models</b> can also \textit{generate high-performing neural network parameters}. Our approach is simple, utilizing an <b>autoencoder</b> and a standard latent <b>diffusion</b> <b>model.</b> The <b>autoencoder</b> extracts latent representations of a subset of the trained network parameters. A <b>diffusion</b> <b>model</b> is then trained to synthesize these latent parameter representations from random noise. It then generates new representations that are passed through the <b>autoencoder&rsquo;s</b> decoder, whose outputs are ready to use as new subsets of network parameters. Across various architectures and datasets, our <b>diffusion</b> <b>process</b> consistently generates models of comparable or improved performance over trained networks, with minimal additional cost. Notably, we empirically find that the generated models perform differently with the trained networks. Our results encourage more exploration on the versatile use of <b>diffusion</b> <b>models.</b></p></p class="citation"></blockquote><h3 id=3367--167316-fair-classifiers-without-fair-training-an-influence-guided-data-sampling-approach-jinlong-pang-et-al-2024>(33/67 | 167/316) Fair Classifiers Without Fair Training: An Influence-Guided Data Sampling Approach (Jinlong Pang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jinlong Pang, Jialu Wang, Zhaowei Zhu, Yuanshun Yao, Chen Qian, Yang Liu. (2024)<br><strong>Fair Classifiers Without Fair Training: An Influence-Guided Data Sampling Approach</strong><br><button class=copy-to-clipboard title="Fair Classifiers Without Fair Training: An Influence-Guided Data Sampling Approach" index=167>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-167 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Distribution Shift, Distribution Shift, Fairness<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12789v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12789v1.pdf filename=2402.12789v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A fair classifier should ensure the benefit of people from different groups, while the group information is often sensitive and unsuitable for model training. Therefore, learning a fair classifier but excluding sensitive attributes in the training dataset is important. In this paper, we study learning fair classifiers without implementing fair training algorithms to avoid possible leakage of sensitive information. Our theoretical analyses validate the possibility of this approach, that traditional training on a dataset with an appropriate <b>distribution</b> <b>shift</b> can reduce both the upper bound for <b>fairness</b> disparity and model generalization error, indicating that <b>fairness</b> and accuracy can be improved simultaneously with simply traditional training. We then propose a tractable solution to progressively shift the original training data during training by sampling influential data, where the sensitive attribute of new data is not accessed in sampling or used in training. Extensive experiments on real-world data demonstrate the effectiveness of our proposed algorithm.</p></p class="citation"></blockquote><h3 id=3467--168316-achieving-near-optimal-regret-for-bandit-algorithms-with-uniform-last-iterate-guarantee-junyan-liu-et-al-2024>(34/67 | 168/316) Achieving Near-Optimal Regret for Bandit Algorithms with Uniform Last-Iterate Guarantee (Junyan Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Junyan Liu, Yunfan Li, Lin Yang. (2024)<br><strong>Achieving Near-Optimal Regret for Bandit Algorithms with Uniform Last-Iterate Guarantee</strong><br><button class=copy-to-clipboard title="Achieving Near-Optimal Regret for Bandit Algorithms with Uniform Last-Iterate Guarantee" index=168>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-168 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-ML<br>Keyword Score: 20<br>Keywords: Bandit Algorithm, Bandit Algorithm<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12711v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12711v1.pdf filename=2402.12711v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Existing performance measures for <b>bandit</b> <b>algorithms</b> such as regret, PAC bounds, or uniform-PAC (Dann et al., 2017), typically evaluate the cumulative performance, while allowing the play of an arbitrarily bad arm at any finite time t. Such a behavior can be highly detrimental in high-stakes applications. This paper introduces a stronger performance measure, the uniform last-iterate (ULI) guarantee, capturing both cumulative and instantaneous performance of <b>bandit</b> <b>algorithms.</b> Specifically, ULI characterizes the instantaneous performance since it ensures that the per-round regret of the played arm is bounded by a function, monotonically decreasing w.r.t. (large) round t, preventing revisits to bad arms when sufficient samples are available. We demonstrate that a near-optimal ULI guarantee directly implies near-optimal cumulative performance across aforementioned performance measures. To examine the achievability of ULI in the finite arm setting, we first provide two positive results that some elimination-based algorithms and high-probability adversarial algorithms with stronger analysis or additional designs, can attain near-optimal ULI guarantees. Then, we also provide a negative result, indicating that optimistic algorithms cannot achieve a near-optimal ULI guarantee. Finally, we propose an efficient algorithm for linear <b>bandits</b> <b>with</b> infinitely many arms, which achieves the ULI guarantee, given access to an optimization oracle.</p></p class="citation"></blockquote><h3 id=3567--169316-training-artificial-neural-networks-by-coordinate-search-algorithm-ehsan-rokhsatyazdi-et-al-2024>(35/67 | 169/316) Training Artificial Neural Networks by Coordinate Search Algorithm (Ehsan Rokhsatyazdi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ehsan Rokhsatyazdi, Shahryar Rahnamayan, Sevil Zanjani Miyandoab, Azam Asilian Bidgoli, H. R. Tizhoosh. (2024)<br><strong>Training Artificial Neural Networks by Coordinate Search Algorithm</strong><br><button class=copy-to-clipboard title="Training Artificial Neural Networks by Coordinate Search Algorithm" index=169>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-169 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: I-2-6, cs-AI, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Stochastic Gradient Descent, Stochastic Gradient Descent<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12646v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12646v1.pdf filename=2402.12646v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Training Artificial Neural Networks poses a challenging and critical problem in machine learning. Despite the effectiveness of gradient-based learning methods, such as <b>Stochastic</b> <b>Gradient</b> <b>Descent</b> <b>(SGD),</b> in training neural networks, they do have several limitations. For instance, they require differentiable activation functions, and cannot optimize a model based on several independent non-differentiable loss functions simultaneously; for example, the F1-score, which is used during testing, can be used during training when a gradient-free optimization algorithm is utilized. Furthermore, the training in any DNN can be possible with a small size of the training dataset. To address these concerns, we propose an efficient version of the gradient-free Coordinate Search (CS) algorithm, an instance of General Pattern Search methods, for training neural networks. The proposed algorithm can be used with non-differentiable activation functions and tailored to multi-objective/multi-loss problems. Finding the optimal values for weights of ANNs is a large-scale optimization problem. Therefore instead of finding the optimal value for each variable, which is the common technique in classical CS, we accelerate optimization and convergence by bundling the weights. In fact, this strategy is a form of dimension reduction for optimization problems. Based on the experimental results, the proposed method, in some cases, outperforms the gradient-based approach, particularly, in situations with insufficient labeled training data. The performance plots demonstrate a high convergence rate, highlighting the capability of our suggested method to find a reasonable solution with fewer function calls. As of now, the only practical and efficient way of training ANNs with hundreds of thousands of weights is gradient-based algorithms such as <b>SGD</b> or Adam. In this paper we introduce an alternative method for training ANN.</p></p class="citation"></blockquote><h3 id=3667--170316-unsupervised-concept-discovery-mitigates-spurious-correlations-md-rifat-arefin-et-al-2024>(36/67 | 170/316) Unsupervised Concept Discovery Mitigates Spurious Correlations (Md Rifat Arefin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Md Rifat Arefin, Yan Zhang, Aristide Baratin, Francesco Locatello, Irina Rish, Dianbo Liu, Kenji Kawaguchi. (2024)<br><strong>Unsupervised Concept Discovery Mitigates Spurious Correlations</strong><br><button class=copy-to-clipboard title="Unsupervised Concept Discovery Mitigates Spurious Correlations" index=170>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-170 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CV, cs-LG, cs.LG<br>Keyword Score: 18<br>Keywords: Benchmarking, Representation Learning, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13368v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13368v1.pdf filename=2402.13368v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Models prone to spurious correlations in training data often produce brittle predictions and introduce unintended biases. Addressing this challenge typically involves methods relying on prior knowledge and group annotation to remove spurious correlations, which may not be readily available in many applications. In this paper, we establish a novel connection between <b>unsupervised</b> object-centric learning and mitigation of spurious correlations. Instead of directly inferring sub-groups with varying correlations with labels, our approach focuses on discovering concepts: discrete ideas that are shared across input samples. Leveraging existing object-centric <b>representation</b> <b>learning,</b> we introduce CoBalT: a concept balancing technique that effectively mitigates spurious correlations without requiring human labeling of subgroups. Evaluation across the Waterbirds, CelebA and ImageNet-9 <b>benchmark</b> datasets for subpopulation shifts demonstrate superior or competitive performance compared state-of-the-art baselines, without the need for group annotation.</p></p class="citation"></blockquote><h3 id=3767--171316-towards-robust-graph-incremental-learning-on-evolving-graphs-junwei-su-et-al-2024>(37/67 | 171/316) Towards Robust Graph Incremental Learning on Evolving Graphs (Junwei Su et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Junwei Su, Difan Zou, Zijun Zhang, Chuan Wu. (2024)<br><strong>Towards Robust Graph Incremental Learning on Evolving Graphs</strong><br><button class=copy-to-clipboard title="Towards Robust Graph Incremental Learning on Evolving Graphs" index=171>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-171 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 16<br>Keywords: Graph, Graph Neural Network, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12987v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12987v1.pdf filename=2402.12987v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Incremental learning is a machine learning approach that involves training a model on a sequence of tasks, rather than all tasks at once. This ability to learn incrementally from a stream of tasks is crucial for many real-world applications. However, incremental learning is a challenging problem on <b>graph-structured</b> data, as many <b>graph-related</b> problems involve prediction tasks for each individual node, known as Node-wise <b>Graph</b> Incremental Learning (NGIL). This introduces non-independent and non-identically distributed characteristics in the sample data generation process, making it difficult to maintain the performance of the model as new tasks are added. In this paper, we focus on the inductive NGIL problem, which accounts for the evolution of <b>graph</b> structure (structural shift) induced by emerging tasks. We provide a formal formulation and analysis of the problem, and propose a novel regularization-based technique called Structural-Shift-Risk-Mitigation (SSRM) to mitigate the impact of the structural shift on catastrophic forgetting of the inductive NGIL problem. We show that the structural shift can lead to a shift in the input distribution for the existing tasks, and further lead to an increased risk of catastrophic forgetting. Through comprehensive empirical studies with several <b>benchmark</b> datasets, we demonstrate that our proposed method, Structural-Shift-Risk-Mitigation (SSRM), is flexible and easy to adapt to improve the performance of state-of-the-art <b>GNN</b> incremental learning frameworks in the inductive setting.</p></p class="citation"></blockquote><h3 id=3867--172316-differentiable-mapper-for-topological-optimization-of-data-representation-ziyad-oulhaj-et-al-2024>(38/67 | 172/316) Differentiable Mapper For Topological Optimization Of Data Representation (Ziyad Oulhaj et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ziyad Oulhaj, Mathieu Carrière, Bertrand Michel. (2024)<br><strong>Differentiable Mapper For Topological Optimization Of Data Representation</strong><br><button class=copy-to-clipboard title="Differentiable Mapper For Topological Optimization Of Data Representation" index=172>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-172 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CG, cs-LG, cs.LG, math-AT<br>Keyword Score: 16<br>Keywords: Graph, Clustering, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12854v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12854v1.pdf filename=2402.12854v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Unsupervised</b> data representation and visualization using tools from topology is an active and growing field of Topological Data Analysis (TDA) and data science. Its most prominent line of work is based on the so-called Mapper <b>graph,</b> which is a combinatorial <b>graph</b> whose topological structures (connected components, branches, loops) are in correspondence with those of the data itself. While highly generic and applicable, its use has been hampered so far by the manual tuning of its many parameters-among these, a crucial one is the so-called filter: it is a continuous function whose variations on the data set are the main ingredient for both building the Mapper representation and assessing the presence and sizes of its topological structures. However, while a few parameter tuning methods have already been investigated for the other Mapper parameters (i.e., resolution, gain, <b>clustering),</b> there is currently no method for tuning the filter itself. In this work, we build on a recently proposed optimization framework incorporating topology to provide the first filter optimization scheme for Mapper <b>graphs.</b> In order to achieve this, we propose a relaxed and more general version of the Mapper <b>graph,</b> whose convergence properties are investigated. Finally, we demonstrate the usefulness of our approach by optimizing Mapper <b>graph</b> representations on several datasets, and showcasing the superiority of the optimized representation over arbitrary ones.</p></p class="citation"></blockquote><h3 id=3967--173316-statistical-curriculum-learning-an-elimination-algorithm-achieving-an-oracle-risk-omer-cohen-et-al-2024>(39/67 | 173/316) Statistical curriculum learning: An elimination algorithm achieving an oracle risk (Omer Cohen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Omer Cohen, Ron Meir, Nir Weinberger. (2024)<br><strong>Statistical curriculum learning: An elimination algorithm achieving an oracle risk</strong><br><button class=copy-to-clipboard title="Statistical curriculum learning: An elimination algorithm achieving an oracle risk" index=173>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-173 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-ML<br>Keyword Score: 13<br>Keywords: Benchmarking, Curriculum Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13366v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13366v1.pdf filename=2402.13366v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We consider a statistical version of <b>curriculum</b> <b>learning</b> (CL) in a parametric prediction setting. The learner is required to estimate a target parameter vector, and can adaptively collect samples from either the target model, or other source models that are similar to the target model, but less noisy. We consider three types of learners, depending on the level of side-information they receive. The first two, referred to as strong/weak-oracle learners, receive high/low degrees of information about the models, and use these to learn. The third, a fully adaptive learner, estimates the target parameter vector without any prior information. In the single source case, we propose an elimination learning method, whose risk matches that of a strong-oracle learner. In the multiple source case, we advocate that the risk of the weak-oracle learner is a realistic <b>benchmark</b> for the risk of adaptive learners. We develop an adaptive multiple elimination-rounds CL algorithm, and characterize instance-dependent conditions for its risk to match that of the weak-oracle learner. We consider instance-dependent minimax lower bounds, and discuss the challenges associated with defining the class of instances for the bound. We derive two minimax lower bounds, and determine the conditions under which the performance weak-oracle learner is minimax optimal.</p></p class="citation"></blockquote><h3 id=4067--174316-discovering-behavioral-modes-in-deep-reinforcement-learning-policies-using-trajectory-clustering-in-latent-space-sindre-benjamin-remman-et-al-2024>(40/67 | 174/316) Discovering Behavioral Modes in Deep Reinforcement Learning Policies Using Trajectory Clustering in Latent Space (Sindre Benjamin Remman et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sindre Benjamin Remman, Anastasios M. Lekkas. (2024)<br><strong>Discovering Behavioral Modes in Deep Reinforcement Learning Policies Using Trajectory Clustering in Latent Space</strong><br><button class=copy-to-clipboard title="Discovering Behavioral Modes in Deep Reinforcement Learning Policies Using Trajectory Clustering in Latent Space" index=174>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-174 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 13<br>Keywords: Clustering, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12939v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12939v1.pdf filename=2402.12939v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Understanding the behavior of deep <b>reinforcement</b> <b>learning</b> (DRL) agents is crucial for improving their performance and reliability. However, the complexity of their policies often makes them challenging to understand. In this paper, we introduce a new approach for investigating the behavior modes of DRL policies, which involves utilizing dimensionality reduction and trajectory <b>clustering</b> in the latent space of neural networks. Specifically, we use Pairwise Controlled Manifold Approximation Projection (PaCMAP) for dimensionality reduction and TRACLUS for trajectory <b>clustering</b> to analyze the latent space of a DRL policy trained on the Mountain Car control task. Our methodology helps identify diverse behavior patterns and suboptimal choices by the policy, thus allowing for targeted improvements. We demonstrate how our approach, combined with domain knowledge, can enhance a policy&rsquo;s performance in specific regions of the state space.</p></p class="citation"></blockquote><h3 id=4167--175316-federated-multi-task-learning-on-non-iid-data-silos-an-experimental-study-yuwen-yang-et-al-2024>(41/67 | 175/316) Federated Multi-Task Learning on Non-IID Data Silos: An Experimental Study (Yuwen Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuwen Yang, Yuxiang Lu, Suizhi Huang, Shalayiding Sirejiding, Hongtao Lu, Yue Ding. (2024)<br><strong>Federated Multi-Task Learning on Non-IID Data Silos: An Experimental Study</strong><br><button class=copy-to-clipboard title="Federated Multi-Task Learning on Non-IID Data Silos: An Experimental Study" index=175>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-175 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CR, cs-DC, cs-LG, cs.LG<br>Keyword Score: 13<br>Keywords: Benchmarking, Federated Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12876v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12876v1.pdf filename=2402.12876v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The innovative <b>Federated</b> <b>Multi-Task</b> Learning (FMTL) approach consolidates the benefits of <b>Federated</b> <b>Learning</b> (FL) and Multi-Task Learning (MTL), enabling collaborative model training on multi-task learning datasets. However, a comprehensive evaluation method, integrating the unique features of both FL and MTL, is currently absent in the field. This paper fills this void by introducing a novel framework, FMTL-Bench, for systematic evaluation of the FMTL paradigm. This <b>benchmark</b> covers various aspects at the data, model, and optimization algorithm levels, and comprises seven sets of comparative experiments, encapsulating a wide array of non-independent and identically distributed (Non-IID) data partitioning scenarios. We propose a systematic process for comparing baselines of diverse indicators and conduct a case study on communication expenditure, time, and energy consumption. Through our exhaustive experiments, we aim to provide valuable insights into the strengths and limitations of existing baseline methods, contributing to the ongoing discourse on optimal FMTL application in practical scenarios. The source code will be made available for results replication.</p></p class="citation"></blockquote><h3 id=4267--176316-ccfc-enhancing-federated-clustering-through-feature-decorrelation-jie-yan-et-al-2024>(42/67 | 176/316) CCFC++: Enhancing Federated Clustering through Feature Decorrelation (Jie Yan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jie Yan, Jing Liu, Yi-Zi Ning, Zhong-Yuan Zhang. (2024)<br><strong>CCFC++: Enhancing Federated Clustering through Feature Decorrelation</strong><br><button class=copy-to-clipboard title="CCFC++: Enhancing Federated Clustering through Feature Decorrelation" index=176>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-176 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 13<br>Keywords: Clustering, Contrastive Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12852v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12852v1.pdf filename=2402.12852v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In federated <b>clustering,</b> multiple data-holding clients collaboratively group data without exchanging raw data. This field has seen notable advancements through its marriage with <b>contrastive</b> <b>learning,</b> exemplified by Cluster-Contrastive Federated <b>Clustering</b> (CCFC). However, CCFC suffers from heterogeneous data across clients, leading to poor and unrobust performance. Our study conducts both empirical and theoretical analyses to understand the impact of heterogeneous data on CCFC. Findings indicate that increased data heterogeneity exacerbates dimensional collapse in CCFC, evidenced by increased correlations across multiple dimensions of the learned representations. To address this, we introduce a decorrelation regularizer to CCFC. Benefiting from the regularizer, the improved method effectively mitigates the detrimental effects of data heterogeneity, and achieves superior performance, as evidenced by a marked increase in NMI scores, with the gain reaching as high as 0.32 in the most pronounced case.</p></p class="citation"></blockquote><h3 id=4367--177316-tackling-byzantine-clients-in-federated-learning-youssef-allouah-et-al-2024>(43/67 | 177/316) Tackling Byzantine Clients in Federated Learning (Youssef Allouah et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Youssef Allouah, Sadegh Farhadkhani, Rachid GuerraouI, Nirupam Gupta, Rafael Pinot, Geovani Rizk, Sasha Voitovych. (2024)<br><strong>Tackling Byzantine Clients in Federated Learning</strong><br><button class=copy-to-clipboard title="Tackling Byzantine Clients in Federated Learning" index=177>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-177 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 13<br>Keywords: Federated Learning, Sample Size<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12780v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12780v1.pdf filename=2402.12780v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The possibility of adversarial (a.k.a., {\em Byzantine}) clients makes <b>federated</b> <b>learning</b> (FL) prone to arbitrary manipulation. The natural approach to robustify FL against adversarial clients is to replace the simple averaging operation at the server in the standard $\mathsf{FedAvg}$ algorithm by a \emph{robust averaging rule}. While a significant amount of work has been devoted to studying the convergence of <b>federated</b> <b>{\em</b> robust averaging} (which we denote by $\mathsf{FedRo}$), prior work has largely ignored the impact of {\em client subsampling} and {\em local steps}, two fundamental FL characteristics. While client subsampling increases the effective fraction of Byzantine clients, local steps increase the drift between the local updates computed by honest (i.e., non-Byzantine) clients. Consequently, a careless deployment of $\mathsf{FedRo}$ could yield poor performance. We validate this observation by presenting an in-depth analysis of $\mathsf{FedRo}$ tightly analyzing the impact of client subsampling and local steps. Specifically, we present a sufficient condition on client subsampling for nearly-optimal convergence of $\mathsf{FedRo}$ (for smooth non-convex loss). Also, we show that the rate of improvement in learning accuracy {\em diminishes} with respect to the number of clients subsampled, as soon as the <b>sample</b> <b>size</b> exceeds a threshold value. Interestingly, we also observe that under a careful choice of step-sizes, the learning error due to Byzantine clients decreases with the number of local steps. We validate our theory by experiments on the FEMNIST and CIFAR-$10$ image classification tasks.</p></p class="citation"></blockquote><h3 id=4467--178316-when-and-how-learning-identifiable-latent-states-for-nonstationary-time-series-forecasting-zijian-li-et-al-2024>(44/67 | 178/316) When and How: Learning Identifiable Latent States for Nonstationary Time Series Forecasting (Zijian Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zijian Li, Ruichu Cai, Zhenhui Yang, Haiqin Huang, Guangyi Chen, Yifan Shen, Zhengming Chen, Xiangchen Song, Zhifeng Hao, Kun Zhang. (2024)<br><strong>When and How: Learning Identifiable Latent States for Nonstationary Time Series Forecasting</strong><br><button class=copy-to-clipboard title="When and How: Learning Identifiable Latent States for Nonstationary Time Series Forecasting" index=178>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-178 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 13<br>Keywords: Benchmarking, Distribution Shift, Distribution Shift<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12767v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12767v1.pdf filename=2402.12767v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Temporal <b>distribution</b> <b>shifts</b> are ubiquitous in time series data. One of the most popular methods assumes that the temporal <b>distribution</b> <b>shift</b> occurs uniformly to disentangle the stationary and nonstationary dependencies. But this assumption is difficult to meet, as we do not know when the <b>distribution</b> <b>shifts</b> occur. To solve this problem, we propose to learn IDentifiable latEnt stAtes (IDEA) to detect when the <b>distribution</b> <b>shifts</b> occur. Beyond that, we further disentangle the stationary and nonstationary latent states via sufficient observation assumption to learn how the latent states change. Specifically, we formalize the causal process with environment-irrelated stationary and environment-related nonstationary variables. Under mild conditions, we show that latent environments and stationary/nonstationary variables are identifiable. Based on these theories, we devise the IDEA model, which incorporates an autoregressive hidden Markov model to estimate latent environments and modular prior networks to identify latent states. The IDEA model outperforms several latest nonstationary forecasting methods on various <b>benchmark</b> datasets, highlighting its advantages in real-world scenarios.</p></p class="citation"></blockquote><h3 id=4567--179316-spurious-correlations-in-machine-learning-a-survey-wenqian-ye-et-al-2024>(45/67 | 179/316) Spurious Correlations in Machine Learning: A Survey (Wenqian Ye et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wenqian Ye, Guangtao Zheng, Xu Cao, Yunsheng Ma, Xia Hu, Aidong Zhang. (2024)<br><strong>Spurious Correlations in Machine Learning: A Survey</strong><br><button class=copy-to-clipboard title="Spurious Correlations in Machine Learning: A Survey" index=179>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-179 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 13<br>Keywords: Benchmarking, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12715v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12715v1.pdf filename=2402.12715v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Machine learning systems are known to be sensitive to spurious correlations between biased features of the inputs (e.g., background, texture, and secondary objects) and the corresponding labels. These features and their correlations with the labels are known as &ldquo;spurious&rdquo; because they tend to change with shifts in real-world data distributions, which can negatively impact the model&rsquo;s generalization and robustness. In this survey, we provide a comprehensive review of this issue, along with a taxonomy of current state-of-the-art methods for addressing spurious correlations in machine learning models. Additionally, we <b>summarize</b> existing datasets, <b>benchmarks,</b> and metrics to aid future research. The paper concludes with a discussion of the recent advancements and future research challenges in this field, aiming to provide valuable insights for researchers in the related domains.</p></p class="citation"></blockquote><h3 id=4667--180316-equivariant-pretrained-transformer-for-unified-geometric-learning-on-multi-domain-3d-molecules-rui-jiao-et-al-2024>(46/67 | 180/316) Equivariant Pretrained Transformer for Unified Geometric Learning on Multi-Domain 3D Molecules (Rui Jiao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rui Jiao, Xiangzhe Kong, Ziyang Yu, Wenbing Huang, Yang Liu. (2024)<br><strong>Equivariant Pretrained Transformer for Unified Geometric Learning on Multi-Domain 3D Molecules</strong><br><button class=copy-to-clipboard title="Equivariant Pretrained Transformer for Unified Geometric Learning on Multi-Domain 3D Molecules" index=180>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-180 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, physics-chem-ph<br>Keyword Score: 13<br>Keywords: Benchmarking, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12714v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12714v1.pdf filename=2402.12714v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Pretraining on a large number of unlabeled 3D molecules has showcased superiority in various scientific applications. However, prior efforts typically focus on pretraining models on a specific domain, either proteins or small molecules, missing the opportunity to leverage the cross-domain knowledge. To mitigate this gap, we introduce Equivariant Pretrained <b>Transformer</b> (EPT), a novel pretraining framework designed to harmonize the geometric learning of small molecules and proteins. To be specific, EPT unifies the geometric modeling of multi-domain molecules via the block-enhanced representation that can attend a broader context of each atom. Upon <b>transformer</b> framework, EPT is further enhanced with E(3) equivariance to facilitate the accurate representation of 3D structures. Another key innovation of EPT is its block-level pretraining task, which allows for joint pretraining on datasets comprising both small molecules and proteins. Experimental evaluations on a diverse group of <b>benchmarks,</b> including ligand binding affinity prediction, molecular property prediction, and protein property prediction, show that EPT significantly outperforms previous SOTA methods for affinity prediction, and achieves the best or comparable performance with existing domain-specific pretraining models for other tasks.</p></p class="citation"></blockquote><h3 id=4767--181316-evolmpnn-predicting-mutational-effect-on-homologous-proteins-by-evolution-encoding-zhiqiang-zhong-et-al-2024>(47/67 | 181/316) EvolMPNN: Predicting Mutational Effect on Homologous Proteins by Evolution Encoding (Zhiqiang Zhong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhiqiang Zhong, Davide Mottin. (2024)<br><strong>EvolMPNN: Predicting Mutational Effect on Homologous Proteins by Evolution Encoding</strong><br><button class=copy-to-clipboard title="EvolMPNN: Predicting Mutational Effect on Homologous Proteins by Evolution Encoding" index=181>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-181 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, q-bio-BM<br>Keyword Score: 10<br>Keywords: Message-Passing<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13418v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13418v1.pdf filename=2402.13418v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Predicting protein properties is paramount for biological and medical advancements. Current protein engineering mutates on a typical protein, called the wild-type, to construct a family of homologous proteins and study their properties. Yet, existing methods easily neglect subtle mutations, failing to capture the effect on the protein properties. To this end, we propose EvolMPNN, Evolution-aware Message Passing Neural Network, to learn evolution-aware protein embeddings. EvolMPNN samples sets of anchor proteins, computes evolutionary information by means of residues and employs a differentiable evolution-aware aggregation scheme over these sampled anchors. This way EvolMPNNcan capture the mutation effect on proteins with respect to the anchor proteins. Afterwards, the aggregated evolution-aware embeddings are integrated with sequence embeddings to generate final comprehensive protein embeddings. Our model shows up to 6.4% better than state-of-the-art methods and attains 36x inference speedup in comparison with large pre-trained models.</p></p class="citation"></blockquote><h3 id=4867--182316-fairness-risks-for-group-conditionally-missing-demographics-kaiqi-jiang-et-al-2024>(48/67 | 182/316) Fairness Risks for Group-conditionally Missing Demographics (Kaiqi Jiang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kaiqi Jiang, Wenzhe Fan, Mao Li, Xinhua Zhang. (2024)<br><strong>Fairness Risks for Group-conditionally Missing Demographics</strong><br><button class=copy-to-clipboard title="Fairness Risks for Group-conditionally Missing Demographics" index=182>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-182 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CY, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Fairness<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13393v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13393v1.pdf filename=2402.13393v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Fairness-aware</b> classification models have gained increasing attention in recent years as concerns grow on discrimination against some demographic groups. Most existing models require full knowledge of the sensitive features, which can be impractical due to privacy, legal issues, and an individual&rsquo;s fear of discrimination. The key challenge we will address is the group dependency of the unavailability, e.g., people of some age range may be more reluctant to reveal their age. Our solution augments general <b>fairness</b> risks with probabilistic imputations of the sensitive features, while jointly learning the group-conditionally missing probabilities in a variational auto-encoder. Our model is demonstrated effective on both image and tabular datasets, achieving an improved balance between accuracy and <b>fairness.</b></p></p class="citation"></blockquote><h3 id=4967--183316-order-optimal-regret-in-distributed-kernel-bandits-using-uniform-sampling-with-shared-randomness-nikola-pavlovic-et-al-2024>(49/67 | 183/316) Order-Optimal Regret in Distributed Kernel Bandits using Uniform Sampling with Shared Randomness (Nikola Pavlovic et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nikola Pavlovic, Sudeep Salgia, Qing Zhao. (2024)<br><strong>Order-Optimal Regret in Distributed Kernel Bandits using Uniform Sampling with Shared Randomness</strong><br><button class=copy-to-clipboard title="Order-Optimal Regret in Distributed Kernel Bandits using Uniform Sampling with Shared Randomness" index=183>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-183 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-DC, cs-LG, cs.LG, stat-ML<br>Keyword Score: 10<br>Keywords: Bandit Algorithm<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13182v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13182v1.pdf filename=2402.13182v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We consider distributed kernel <b>bandits</b> where $N$ agents aim to collaboratively maximize an unknown reward function that lies in a reproducing kernel Hilbert space. Each agent sequentially queries the function to obtain noisy observations at the query points. Agents can share information through a central server, with the objective of minimizing regret that is accumulating over time $T$ and aggregating over agents. We develop the first algorithm that achieves the optimal regret order (as defined by centralized learning) with a communication cost that is sublinear in both $N$ and $T$. The key features of the proposed algorithm are the uniform exploration at the local agents and shared randomness with the central server. Working together with the sparse approximation of the GP model, these two key components make it possible to preserve the learning rate of the centralized setting at a diminishing rate of communication.</p></p class="citation"></blockquote><h3 id=5067--184316-how-does-selection-leak-privacy-revisiting-private-selection-and-improved-results-for-hyper-parameter-tuning-zihang-xiang-et-al-2024>(50/67 | 184/316) How Does Selection Leak Privacy: Revisiting Private Selection and Improved Results for Hyper-parameter Tuning (Zihang Xiang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zihang Xiang, Chenglong Wang, Di Wang. (2024)<br><strong>How Does Selection Leak Privacy: Revisiting Private Selection and Improved Results for Hyper-parameter Tuning</strong><br><button class=copy-to-clipboard title="How Does Selection Leak Privacy: Revisiting Private Selection and Improved Results for Hyper-parameter Tuning" index=184>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-184 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CR, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Differential Privacy<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13087v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13087v1.pdf filename=2402.13087v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We study the problem of guaranteeing <b>Differential</b> <b>Privacy</b> (DP) in hyper-parameter tuning, a crucial process in machine learning involving the selection of the best run from several. Unlike many private algorithms, including the prevalent DP-SGD, the privacy implications of tuning remain insufficiently understood. Recent works propose a generic private solution for the tuning process, yet a fundamental question still persists: is the current privacy bound for this solution tight? This paper contributes both positive and negative answers to this question. Initially, we provide studies affirming the current privacy analysis is indeed tight in a general sense. However, when we specifically study the hyper-parameter tuning problem, such tightness no longer holds. This is first demonstrated by applying privacy audit on the tuning process. Our findings underscore a substantial gap between the current theoretical privacy bound and the empirical bound derived even under the strongest audit setup. The gap found is not a fluke. Our subsequent study provides an improved privacy result for private hyper-parameter tuning due to its distinct properties. Our privacy results are also more generalizable compared to prior analyses that are only easily applicable in specific setups.</p></p class="citation"></blockquote><h3 id=5167--185316-harmful-algal-bloom-forecasting-a-comparison-between-stream-and-batch-learning-andres-molares-ulloa-et-al-2024>(51/67 | 185/316) Harmful algal bloom forecasting. A comparison between stream and batch learning (Andres Molares-Ulloa et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Andres Molares-Ulloa, Elisabet Rocruz, Daniel Rivero, Xosé A. Padin, Rita Nolasco, Jesús Dubert, Enrique Fernandez-Blanco. (2024)<br><strong>Harmful algal bloom forecasting. A comparison between stream and batch learning</strong><br><button class=copy-to-clipboard title="Harmful algal bloom forecasting. A comparison between stream and batch learning" index=185>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-185 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: BLOOM<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13304v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13304v1.pdf filename=2402.13304v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Diarrhetic Shellfish Poisoning (DSP) is a global health threat arising from shellfish contaminated with toxins produced by dinoflagellates. The condition, with its widespread incidence, high morbidity rate, and persistent shellfish toxicity, poses risks to public health and the shellfish industry. High biomass of toxin-producing algae such as DSP are known as Harmful Algal <b>Blooms</b> (HABs). Monitoring and forecasting systems are crucial for mitigating HABs impact. Predicting harmful algal <b>blooms</b> involves a time-series-based problem with a strong historical seasonal component, however, recent anomalies due to changes in meteorological and oceanographic events have been observed. Stream Learning stands out as one of the most promising approaches for addressing time-series-based problems with concept drifts. However, its efficacy in predicting HABs remains unproven and needs to be tested in comparison with Batch Learning. Historical data availability is a critical point in developing predictive systems. In oceanography, the available data collection can have some constrains and limitations, which has led to exploring new tools to obtain more exhaustive time series. In this study, a machine learning workflow for predicting the number of cells of a toxic dinoflagellate, Dinophysis acuminata, was developed with several key advancements. Seven machine learning algorithms were compared within two learning paradigms. Notably, the output data from CROCO, the ocean hydrodynamic model, was employed as the primary dataset, palliating the limitation of time-continuous historical data. This study highlights the value of models interpretability, fair models comparison methodology, and the incorporation of Stream Learning models. The model DoME, with an average R2 of 0.77 in the 3-day-ahead prediction, emerged as the most effective and interpretable predictor, outperforming the other algorithms.</p></p class="citation"></blockquote><h3 id=5267--186316-text-guided-molecule-generation-with-diffusion-language-model-haisong-gong-et-al-2024>(52/67 | 186/316) Text-Guided Molecule Generation with Diffusion Language Model (Haisong Gong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haisong Gong, Qiang Liu, Shu Wu, Liang Wang. (2024)<br><strong>Text-Guided Molecule Generation with Diffusion Language Model</strong><br><button class=copy-to-clipboard title="Text-Guided Molecule Generation with Diffusion Language Model" index=186>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-186 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CE, cs-CL, cs-LG, cs.LG, q-bio-BM<br>Keyword Score: 10<br>Keywords: Diffusion Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13040v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13040v1.pdf filename=2402.13040v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Text-guided molecule generation is a task where molecules are generated to match specific textual descriptions. Recently, most existing SMILES-based molecule generation methods rely on an autoregressive architecture. In this work, we propose the Text-Guided Molecule Generation with <b>Diffusion</b> <b>Language</b> Model (TGM-DLM), a novel approach that leverages <b>diffusion</b> <b>models</b> to address the limitations of autoregressive methods. TGM-DLM updates token embeddings within the SMILES string collectively and iteratively, using a two-phase <b>diffusion</b> <b>generation</b> process. The first phase optimizes embeddings from random noise, guided by the text description, while the second phase corrects invalid SMILES strings to form valid molecular representations. We demonstrate that TGM-DLM outperforms MolT5-Base, an autoregressive model, without the need for additional data resources. Our findings underscore the remarkable effectiveness of TGM-DLM in generating coherent and precise molecules with specific properties, opening new avenues in drug discovery and related scientific domains. Code will be released at: <a href=https://github.com/Deno-V/tgm-dlm>https://github.com/Deno-V/tgm-dlm</a>.</p></p class="citation"></blockquote><h3 id=5367--187316-skill-or-luck-return-decomposition-via-advantage-functions-hsiao-ru-pan-et-al-2024>(53/67 | 187/316) Skill or Luck? Return Decomposition via Advantage Functions (Hsiao-Ru Pan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hsiao-Ru Pan, Bernhard Schölkopf. (2024)<br><strong>Skill or Luck? Return Decomposition via Advantage Functions</strong><br><button class=copy-to-clipboard title="Skill or Luck? Return Decomposition via Advantage Functions" index=187>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-187 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12874v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12874v1.pdf filename=2402.12874v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Learning from off-policy data is essential for sample-efficient <b>reinforcement</b> <b>learning.</b> In the present work, we build on the insight that the advantage function can be understood as the causal effect of an action on the return, and show that this allows us to decompose the return of a trajectory into parts caused by the agent&rsquo;s actions (skill) and parts outside of the agent&rsquo;s control (luck). Furthermore, this decomposition enables us to naturally extend Direct Advantage Estimation (DAE) to off-policy settings (Off-policy DAE). The resulting method can learn from off-policy trajectories without relying on importance sampling techniques or truncating off-policy actions. We draw connections between Off-policy DAE and previous methods to demonstrate how it can speed up learning and when the proposed off-policy corrections are important. Finally, we use the MinAtar environments to illustrate how ignoring off-policy corrections can lead to suboptimal policy optimization performance.</p></p class="citation"></blockquote><h3 id=5467--188316-bounding-reconstruction-attack-success-of-adversaries-without-data-priors-alexander-ziller-et-al-2024>(54/67 | 188/316) Bounding Reconstruction Attack Success of Adversaries Without Data Priors (Alexander Ziller et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Alexander Ziller, Anneliese Riess, Kristian Schwethelm, Tamara T. Mueller, Daniel Rueckert, Georgios Kaissis. (2024)<br><strong>Bounding Reconstruction Attack Success of Adversaries Without Data Priors</strong><br><button class=copy-to-clipboard title="Bounding Reconstruction Attack Success of Adversaries Without Data Priors" index=188>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-188 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CR, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Differential Privacy<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12861v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12861v1.pdf filename=2402.12861v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Reconstruction attacks on machine learning (ML) models pose a strong risk of leakage of sensitive data. In specific contexts, an adversary can (almost) perfectly reconstruct training data samples from a trained model using the model&rsquo;s gradients. When training ML models with <b>differential</b> <b>privacy</b> (DP), formal upper bounds on the success of such reconstruction attacks can be provided. So far, these bounds have been formulated under worst-case assumptions that might not hold high realistic practicality. In this work, we provide formal upper bounds on reconstruction success under realistic adversarial settings against ML models trained with DP and support these bounds with empirical results. With this, we show that in realistic scenarios, (a) the expected reconstruction success can be bounded appropriately in different contexts and by different metrics, which (b) allows for a more educated choice of a privacy parameter.</p></p class="citation"></blockquote><h3 id=5567--189316-from-movements-to-metrics-evaluating-explainable-ai-methods-in-skeleton-based-human-activity-recognition-kimji-n-pellano-et-al-2024>(55/67 | 189/316) From Movements to Metrics: Evaluating Explainable AI Methods in Skeleton-Based Human Activity Recognition (Kimji N. Pellano et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kimji N. Pellano, Inga Strümke, Espen Alexander F. Ihlen. (2024)<br><strong>From Movements to Metrics: Evaluating Explainable AI Methods in Skeleton-Based Human Activity Recognition</strong><br><button class=copy-to-clipboard title="From Movements to Metrics: Evaluating Explainable AI Methods in Skeleton-Based Human Activity Recognition" index=189>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-189 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Explainable AI<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12790v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12790v1.pdf filename=2402.12790v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The advancement of deep learning in human activity recognition (HAR) using 3D skeleton data is critical for applications in healthcare, security, sports, and human-computer interaction. This paper tackles a well-known gap in the field, which is the lack of testing in the applicability and reliability of XAI evaluation metrics in the skeleton-based HAR domain. We have tested established XAI metrics namely faithfulness and stability on Class Activation Mapping (CAM) and Gradient-weighted Class Activation Mapping (Grad-CAM) to address this problem. The study also introduces a perturbation method that respects human biomechanical constraints to ensure realistic variations in human movement. Our findings indicate that \textit{faithfulness} may not be a reliable metric in certain contexts, such as with the EfficientGCN model. Conversely, stability emerges as a more dependable metric when there is slight input data perturbations. CAM and Grad-CAM are also found to produce almost identical explanations, leading to very similar XAI metric performance. This calls for the need for more diversified metrics and new XAI methods applied in skeleton-based HAR.</p></p class="citation"></blockquote><h3 id=5667--190316-static-vs-dynamic-databases-for-indoor-localization-based-on-wi-fi-fingerprinting-a-discussion-from-a-data-perspective-zhe-tang-et-al-2024>(56/67 | 190/316) Static vs. Dynamic Databases for Indoor Localization based on Wi-Fi Fingerprinting: A Discussion from a Data Perspective (Zhe Tang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhe Tang, Ruocheng Gu, Sihao Li, Kyeong Soo Kim, Jeremy S. Smith. (2024)<br><strong>Static vs. Dynamic Databases for Indoor Localization based on Wi-Fi Fingerprinting: A Discussion from a Data Perspective</strong><br><button class=copy-to-clipboard title="Static vs. Dynamic Databases for Indoor Localization based on Wi-Fi Fingerprinting: A Discussion from a Data Perspective" index=190>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-190 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs-NI, cs.LG<br>Keyword Score: 10<br>Keywords: Gaussian Process<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12756v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12756v1.pdf filename=2402.12756v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Wi-Fi fingerprinting has emerged as the most popular approach to indoor localization. The use of ML algorithms has greatly improved the localization performance of Wi-Fi fingerprinting, but its success depends on the availability of fingerprint databases composed of a large number of RSSIs, the MAC addresses of access points, and the other measurement information. However, most fingerprint databases do not reflect well the time varying nature of electromagnetic interferences in complicated modern indoor environment. This could result in significant changes in statistical characteristics of training/validation and testing datasets, which are often constructed at different times, and even the characteristics of the testing datasets could be different from those of the data submitted by users during the operation of localization systems after their deployment. In this paper, we consider the implications of time-varying Wi-Fi fingerprints on indoor localization from a data-centric point of view and discuss the differences between static and dynamic databases. As a case study, we have constructed a dynamic database covering three floors of the IR building of XJTLU based on RSSI measurements, over 44 days, and investigated the differences between static and dynamic databases in terms of statistical characteristics and localization performance. The analyses based on variance calculations and Isolation Forest show the temporal shifts in RSSIs, which result in a noticeable trend of the increase in the localization error of a <b>Gaussian</b> <b>process</b> regression model with the maximum error of 6.65 m after 14 days of training without model adjustments. The results of the case study with the XJTLU dynamic database clearly demonstrate the limitations of static databases and the importance of the creation and adoption of dynamic databases for future indoor localization research and real-world deployment.</p></p class="citation"></blockquote><h3 id=5767--191316-diffusion-posterior-sampling-is-computationally-intractable-shivam-gupta-et-al-2024>(57/67 | 191/316) Diffusion Posterior Sampling is Computationally Intractable (Shivam Gupta et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shivam Gupta, Ajil Jalal, Aditya Parulekar, Eric Price, Zhiyang Xun. (2024)<br><strong>Diffusion Posterior Sampling is Computationally Intractable</strong><br><button class=copy-to-clipboard title="Diffusion Posterior Sampling is Computationally Intractable" index=191>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-191 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG, math-ST, stat-ML, stat-TH<br>Keyword Score: 10<br>Keywords: Diffusion Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12727v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12727v1.pdf filename=2402.12727v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Diffusion</b> <b>models</b> are a remarkably effective way of learning and sampling from a distribution $p(x)$. In posterior sampling, one is also given a measurement model $p(y \mid x)$ and a measurement $y$, and would like to sample from $p(x \mid y)$. Posterior sampling is useful for tasks such as inpainting, super-resolution, and MRI reconstruction, so a number of recent works have given algorithms to heuristically approximate it; but none are known to converge to the correct distribution in polynomial time. In this paper we show that posterior sampling is \emph{computationally intractable}: under the most basic assumption in cryptography &ndash; that one-way functions exist &ndash; there are instances for which \emph{every} algorithm takes superpolynomial time, even though \emph{unconditional} sampling is provably fast. We also show that the exponential-time rejection sampling algorithm is essentially optimal under the stronger plausible assumption that there are one-way functions that take exponential time to invert.</p></p class="citation"></blockquote><h3 id=5867--192316-revitalizing-multivariate-time-series-forecasting-learnable-decomposition-with-inter-series-dependencies-and-intra-series-variations-modeling-guoqi-yu-et-al-2024>(58/67 | 192/316) Revitalizing Multivariate Time Series Forecasting: Learnable Decomposition with Inter-Series Dependencies and Intra-Series Variations Modeling (Guoqi Yu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Guoqi Yu, Jing Zou, Xiaowei Hu, Angelica I. Aviles-Rivero, Jing Qin, Shujun Wang. (2024)<br><strong>Revitalizing Multivariate Time Series Forecasting: Learnable Decomposition with Inter-Series Dependencies and Intra-Series Variations Modeling</strong><br><button class=copy-to-clipboard title="Revitalizing Multivariate Time Series Forecasting: Learnable Decomposition with Inter-Series Dependencies and Intra-Series Variations Modeling" index=192>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-192 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12694v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12694v1.pdf filename=2402.12694v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Predicting multivariate time series is crucial, demanding precise modeling of intricate patterns, including inter-series dependencies and intra-series variations. Distinctive trend characteristics in each time series pose challenges, and existing methods, relying on basic moving average kernels, may struggle with the non-linear structure and complex trends in real-world data. Given that, we introduce a learnable decomposition strategy to capture dynamic trend information more reasonably. Additionally, we propose a dual attention module tailored to capture inter-series dependencies and intra-series variations simultaneously for better time series forecasting, which is implemented by channel-wise <b>self-attention</b> and autoregressive <b>self-attention.</b> To evaluate the effectiveness of our method, we conducted experiments across eight open-source datasets and compared it with the state-of-the-art methods. Through the comparison results, our Leddam (LEarnable Decomposition and Dual Attention Module) not only demonstrates significant advancements in predictive performance, but also the proposed decomposition strategy can be plugged into other methods with a large performance-boosting, from 11.87% to 48.56% MSE error degradation.</p></p class="citation"></blockquote><h3 id=5967--193316-learning-on-manifolds-without-manifold-learning-h-n-mhaskar-et-al-2024>(59/67 | 193/316) Learning on manifolds without manifold learning (H. N. Mhaskar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>H. N. Mhaskar, Ryan O&rsquo;Dowd. (2024)<br><strong>Learning on manifolds without manifold learning</strong><br><button class=copy-to-clipboard title="Learning on manifolds without manifold learning" index=193>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-193 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-ML<br>Keyword Score: 10<br>Keywords: Stemming<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12687v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12687v1.pdf filename=2402.12687v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Function approximation based on data drawn randomly from an unknown distribution is an important problem in machine learning. In contrast to the prevalent paradigm of solving this problem by minimizing a loss functional, we have given a direct one-shot construction together with optimal error bounds under the manifold assumption; i.e., one assumes that the data is sampled from an unknown sub-manifold of a high dimensional Euclidean space. A great deal of research deals with obtaining information about this manifold, such as the eigendecomposition of the Laplace-Beltrami operator or coordinate charts, and using this information for function approximation. This two step approach implies some extra errors in the approximation <b>stemming</b> from basic quantities of the data in addition to the errors inherent in function approximation. In Neural Networks, 132:253268, 2020, we have proposed a one-shot direct method to achieve function approximation without requiring the extraction of any information about the manifold other than its dimension. However, one cannot pin down the class of approximants used in that paper. In this paper, we view the unknown manifold as a sub-manifold of an ambient hypersphere and study the question of constructing a one-shot approximation using the spherical polynomials based on the hypersphere. Our approach does not require preprocessing of the data to obtain information about the manifold other than its dimension. We give optimal rates of approximation for relatively &ldquo;rough&rdquo; functions.</p></p class="citation"></blockquote><h3 id=6067--194316-a-comprehensive-review-of-machine-learning-advances-on-data-change-a-cross-field-perspective-jeng-lin-li-et-al-2024>(60/67 | 194/316) A Comprehensive Review of Machine Learning Advances on Data Change: A Cross-Field Perspective (Jeng-Lin Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jeng-Lin Li, Chih-Fan Hsu, Ming-Ching Chang, Wei-Chao Chen. (2024)<br><strong>A Comprehensive Review of Machine Learning Advances on Data Change: A Cross-Field Perspective</strong><br><button class=copy-to-clipboard title="A Comprehensive Review of Machine Learning Advances on Data Change: A Cross-Field Perspective" index=194>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-194 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CV, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Distribution Shift, Distribution Shift<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12627v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12627v1.pdf filename=2402.12627v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent artificial intelligence (AI) technologies show remarkable evolution in various academic fields and industries. However, in the real world, dynamic data lead to principal challenges for deploying AI models. An unexpected data change brings about severe performance degradation in AI models. We identify two major related research fields, domain shift and concept drift according to the setting of the data change. Although these two popular research fields aim to solve <b>distribution</b> <b>shift</b> and non-stationary data stream problems, the underlying properties remain similar which also encourages similar technical approaches. In this review, we regroup domain shift and concept drift into a single research problem, namely the data change problem, with a systematic overview of state-of-the-art methods in the two research fields. We propose a three-phase problem categorization scheme to link the key ideas in the two technical fields. We thus provide a novel scope for researchers to explore contemporary technical strategies, learn industrial applications, and identify future directions for addressing data change challenges.</p></p class="citation"></blockquote><h3 id=6167--195316-multi-objective-binary-coordinate-search-for-feature-selection-sevil-zanjani-miyandoab-et-al-2024>(61/67 | 195/316) Multi-objective Binary Coordinate Search for Feature Selection (Sevil Zanjani Miyandoab et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sevil Zanjani Miyandoab, Shahryar Rahnamayan, Azam Asilian Bidgoli. (2024)<br><strong>Multi-objective Binary Coordinate Search for Feature Selection</strong><br><button class=copy-to-clipboard title="Multi-objective Binary Coordinate Search for Feature Selection" index=195>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-195 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: I-2-6, cs-LG, cs-NE, cs.LG<br>Keyword Score: 10<br>Keywords: Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12616v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12616v1.pdf filename=2402.12616v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A <b>supervised</b> feature selection method selects an appropriate but concise set of features to differentiate classes, which is highly expensive for large-scale datasets. Therefore, feature selection should aim at both minimizing the number of selected features and maximizing the accuracy of classification, or any other task. However, this crucial task is computationally highly demanding on many real-world datasets and requires a very efficient algorithm to reach a set of optimal features with a limited number of fitness evaluations. For this purpose, we have proposed the binary multi-objective coordinate search (MOCS) algorithm to solve large-scale feature selection problems. To the best of our knowledge, the proposed algorithm in this paper is the first multi-objective coordinate search algorithm. In this method, we generate new individuals by flipping a variable of the candidate solutions on the Pareto front. This enables us to investigate the effectiveness of each feature in the corresponding subset. In fact, this strategy can play the role of crossover and mutation operators to generate distinct subsets of features. The reported results indicate the significant superiority of our method over NSGA-II, on five real-world large-scale datasets, particularly when the computing budget is limited. Moreover, this simple hyper-parameter-free algorithm can solve feature selection much faster and more efficiently than NSGA-II.</p></p class="citation"></blockquote><h3 id=6267--196316-chili-chemically-informed-large-scale-inorganic-nanomaterials-dataset-for-advancing-graph-machine-learning-ulrik-friis-jensen-et-al-2024>(62/67 | 196/316) CHILI: Chemically-Informed Large-scale Inorganic Nanomaterials Dataset for Advancing Graph Machine Learning (Ulrik Friis-Jensen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ulrik Friis-Jensen, Frederik L. Johansen, Andy S. Anker, Erik B. Dam, Kirsten M. Ø. Jensen, Raghavendra Selvan. (2024)<br><strong>CHILI: Chemically-Informed Large-scale Inorganic Nanomaterials Dataset for Advancing Graph Machine Learning</strong><br><button class=copy-to-clipboard title="CHILI: Chemically-Informed Large-scale Inorganic Nanomaterials Dataset for Advancing Graph Machine Learning" index=196>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-196 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-ML<br>Keyword Score: 9<br>Keywords: Graph, Benchmarking, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13221v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13221v2.pdf filename=2402.13221v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Advances in <b>graph</b> machine learning (ML) have been driven by applications in chemistry as <b>graphs</b> have remained the most expressive representations of molecules. While early <b>graph</b> ML methods focused primarily on small organic molecules, recently, the scope of <b>graph</b> ML has expanded to include inorganic materials. Modelling the periodicity and symmetry of inorganic crystalline materials poses unique challenges, which existing <b>graph</b> ML methods are unable to address. Moving to inorganic nanomaterials increases complexity as the scale of number of nodes within each <b>graph</b> can be broad ($10$ to $10^5$). The bulk of existing <b>graph</b> ML focuses on characterising molecules and materials by predicting target properties with <b>graphs</b> as input. However, the most exciting applications of <b>graph</b> ML will be in their generative capabilities, which is currently not at par with other domains such as images or text. We invite the <b>graph</b> ML community to address these open challenges by presenting two new chemically-informed large-scale inorganic (CHILI) nanomaterials datasets: A medium-scale dataset (with overall >6M nodes, >49M edges) of mono-metallic oxide nanomaterials generated from 12 selected crystal types (CHILI-3K) and a large-scale dataset (with overall >183M nodes, >1.2B edges) of nanomaterials generated from experimentally determined crystal structures (CHILI-100K). We define 11 property prediction tasks and 6 structure prediction tasks, which are of special interest for nanomaterial research. We <b>benchmark</b> the performance of a wide array of baseline methods and use these <b>benchmarking</b> results to highlight areas which need future work. To the best of our knowledge, CHILI-3K and CHILI-100K are the first open-source nanomaterial datasets of this scale &ndash; both on the individual <b>graph</b> level and of the dataset as a whole &ndash; and the only nanomaterials datasets with high structural and elemental diversity.</p></p class="citation"></blockquote><h3 id=6367--197316-testing-calibration-in-subquadratic-time-lunjia-hu-et-al-2024>(63/67 | 197/316) Testing Calibration in Subquadratic Time (Lunjia Hu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lunjia Hu, Kevin Tian, Chutong Yang. (2024)<br><strong>Testing Calibration in Subquadratic Time</strong><br><button class=copy-to-clipboard title="Testing Calibration in Subquadratic Time" index=197>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-197 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-DS, cs-LG, cs.LG, stat-CO, stat-ML<br>Keyword Score: 8<br>Keywords: Black Box, Sample Size<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13187v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13187v1.pdf filename=2402.13187v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the recent literature on machine learning and decision making, calibration has emerged as a desirable and widely-studied statistical property of the outputs of binary prediction models. However, the algorithmic aspects of measuring model calibration have remained relatively less well-explored. Motivated by [BGHN23], which proposed a rigorous framework for measuring distances to calibration, we initiate the algorithmic study of calibration through the lens of property testing. We define the problem of calibration testing from <b>samples</b> <b>where</b> given $n$ draws from a distribution $\mathcal{D}$ on (predictions, binary outcomes), our goal is to distinguish between the case where $\mathcal{D}$ is perfectly calibrated, and the case where $\mathcal{D}$ is $\varepsilon$-far from calibration. We design an algorithm based on approximate linear programming, which solves calibration testing information-theoretically optimally (up to constant factors) in time $O(n^{1.5} \log(n))$. This improves upon state-of-the-art <b>black-box</b> <b>linear</b> program solvers requiring $\Omega(n^\omega)$ time, where $\omega > 2$ is the exponent of matrix multiplication. We also develop algorithms for tolerant variants of our testing problem, and give <b>sample</b> <b>complexity</b> lower bounds for alternative calibration distances to the one considered in this work. Finally, we present preliminary experiments showing that the testing problem we define faithfully captures standard notions of calibration, and that our algorithms scale to accommodate moderate <b>sample</b> <b>sizes.</b></p></p class="citation"></blockquote><h3 id=6467--198316-double-machine-learning-for-causal-hybrid-modeling----applications-in-the-earth-sciences-kai-hendrik-cohrs-et-al-2024>(64/67 | 198/316) Double machine learning for causal hybrid modeling &ndash; applications in the Earth sciences (Kai-Hendrik Cohrs et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kai-Hendrik Cohrs, Gherardo Varando, Nuno Carvalhais, Markus Reichstein, Gustau Camps-Valls. (2024)<br><strong>Double machine learning for causal hybrid modeling &ndash; applications in the Earth sciences</strong><br><button class=copy-to-clipboard title="Double machine learning for causal hybrid modeling -- applications in the Earth sciences" index=198>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-198 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-ME<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13332v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13332v1.pdf filename=2402.13332v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Hybrid modeling integrates machine learning with scientific knowledge with the goal of enhancing interpretability, generalization, and adherence to natural laws. Nevertheless, equifinality and regularization biases pose challenges in hybrid modeling to achieve these purposes. This paper introduces a novel approach to estimating hybrid models via a causal inference framework, specifically employing Double Machine Learning (DML) to estimate causal effects. We showcase its use for the Earth sciences on two problems related to carbon dioxide fluxes. In the $Q_{10}$ model, we demonstrate that DML-based hybrid modeling is superior in estimating causal parameters over end-to-end deep neural network (DNN) approaches, proving efficiency, robustness to bias from regularization methods, and circumventing equifinality. Our approach, applied to carbon flux partitioning, exhibits flexibility in accommodating heterogeneous causal effects. The study emphasizes the necessity of explicitly defining causal <b>graphs</b> and relationships, advocating for this as a general best practice. We encourage the continued exploration of causality in hybrid models for more interpretable and trustworthy results in knowledge-guided machine learning.</p></p class="citation"></blockquote><h3 id=6567--199316-subiq-inverse-soft-q-learning-for-offline-imitation-with-suboptimal-demonstrations-huy-hoang-et-al-2024>(65/67 | 199/316) SubIQ: Inverse Soft-Q Learning for Offline Imitation with Suboptimal Demonstrations (Huy Hoang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Huy Hoang, Tien Mai, Pradeep Varakantham. (2024)<br><strong>SubIQ: Inverse Soft-Q Learning for Offline Imitation with Suboptimal Demonstrations</strong><br><button class=copy-to-clipboard title="SubIQ: Inverse Soft-Q Learning for Offline Imitation with Suboptimal Demonstrations" index=199>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-199 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13147v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13147v1.pdf filename=2402.13147v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We consider offline imitation learning (IL), which aims to mimic the expert&rsquo;s behavior from its demonstration without further interaction with the environment. One of the main challenges in offline IL is dealing with the limited support of expert demonstrations that cover only a small fraction of the state-action spaces. In this work, we consider offline IL, where expert demonstrations are limited but complemented by a larger set of sub-optimal demonstrations of lower expertise levels. Most of the existing offline IL methods developed for this setting are based on behavior cloning or distribution matching, where the aim is to match the occupancy distribution of the imitation policy with that of the expert policy. Such an approach often suffers from over-fitting, as expert demonstrations are limited to accurately represent any occupancy distribution. On the other hand, since sub-optimal sets are much larger, there is a high chance that the imitation policy is trained towards sub-optimal policies. In this paper, to address these issues, we propose a new approach based on inverse soft-Q learning, where a regularization term is added to the training objective, with the aim of aligning the learned rewards with a pre-assigned reward function that allocates higher weights to state-action pairs from expert demonstrations, and lower weights to those from lower expertise levels. On standard <b>benchmarks,</b> our inverse soft-Q learning significantly outperforms other offline IL baselines by a large margin.</p></p class="citation"></blockquote><h3 id=6667--200316-scalable-decentralized-algorithms-for-online-personalized-mean-estimation-franco-galante-et-al-2024>(66/67 | 200/316) Scalable Decentralized Algorithms for Online Personalized Mean Estimation (Franco Galante et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Franco Galante, Giovanni Neglia, Emilio Leonardi. (2024)<br><strong>Scalable Decentralized Algorithms for Online Personalized Mean Estimation</strong><br><button class=copy-to-clipboard title="Scalable Decentralized Algorithms for Online Personalized Mean Estimation" index=200>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-200 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-DC, cs-LG, cs.LG<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12812v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12812v1.pdf filename=2402.12812v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In numerous settings, agents lack sufficient data to directly learn a model. Collaborating with other agents may help, but it introduces a bias-variance trade-off, when local data distributions differ. A key challenge is for each agent to identify clients with similar distributions while learning the model, a problem that remains largely unresolved. This study focuses on a simplified version of the overarching problem, where each agent collects samples from a real-valued distribution over time to estimate its mean. Existing algorithms face impractical space and time complexities (quadratic in the number of agents A). To address scalability challenges, we propose a framework where agents self-organize into a <b>graph,</b> allowing each agent to communicate with only a selected number of peers r. We introduce two collaborative mean estimation algorithms: one draws inspiration from belief propagation, while the other employs a consensus-based approach, with complexity of O( r |A| log |A|) and O(r |A|), respectively. We establish conditions under which both algorithms yield asymptotically optimal estimates and offer a theoretical characterization of their performance.</p></p class="citation"></blockquote><h3 id=6767--201316-discriminant-distance-aware-representation-on-deterministic-uncertainty-quantification-methods-jiaxin-zhang-et-al-2024>(67/67 | 201/316) Discriminant Distance-Aware Representation on Deterministic Uncertainty Quantification Methods (Jiaxin Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiaxin Zhang, Kamalika Das, Sricharan Kumar. (2024)<br><strong>Discriminant Distance-Aware Representation on Deterministic Uncertainty Quantification Methods</strong><br><button class=copy-to-clipboard title="Discriminant Distance-Aware Representation on Deterministic Uncertainty Quantification Methods" index=201>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-201 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12664v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12664v1.pdf filename=2402.12664v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Uncertainty estimation is a crucial aspect of deploying dependable deep learning models in safety-critical systems. In this study, we introduce a novel and efficient method for deterministic uncertainty estimation called Discriminant Distance-Awareness Representation (DDAR). Our approach involves constructing a DNN model that incorporates a set of prototypes in its latent representations, enabling us to analyze valuable feature information from the input data. By leveraging a distinction maximization layer over optimal trainable prototypes, DDAR can learn a discriminant distance-awareness representation. We demonstrate that DDAR overcomes feature collapse by relaxing the Lipschitz constraint that hinders the practicality of deterministic uncertainty methods (DUMs) architectures. Our experiments show that DDAR is a flexible and architecture-agnostic method that can be easily integrated as a pluggable layer with distance-sensitive metrics, outperforming state-of-the-art uncertainty estimation methods on multiple <b>benchmark</b> problems.</p></p class="citation"></blockquote><h2 id=csro-6>cs.RO (6)</h2><h3 id=16--202316-tiny-reinforcement-learning-for-quadruped-locomotion-using-decision-transformers-orhan-eren-akgün-et-al-2024>(1/6 | 202/316) Tiny Reinforcement Learning for Quadruped Locomotion using Decision Transformers (Orhan Eren Akgün et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Orhan Eren Akgün, Néstor Cuevas, Matheus Farias, Daniel Garces. (2024)<br><strong>Tiny Reinforcement Learning for Quadruped Locomotion using Decision Transformers</strong><br><button class=copy-to-clipboard title="Tiny Reinforcement Learning for Quadruped Locomotion using Decision Transformers" index=202>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-202 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-LG, cs-RO, cs.RO<br>Keyword Score: 60<br>Keywords: Pruning, Quantization, Reinforcement Learning, Simulation, Simulator, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13201v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13201v1.pdf filename=2402.13201v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Resource-constrained robotic platforms are particularly useful for tasks that require low-cost hardware alternatives due to the risk of losing the robot, like in search-and-rescue applications, or the need for a large number of devices, like in swarm robotics. For this reason, it is crucial to find mechanisms for adapting <b>reinforcement</b> <b>learning</b> techniques to the constraints imposed by lower computational power and smaller memory capacities of these ultra low-cost robotic platforms. We try to address this need by proposing a method for making imitation learning deployable onto resource-constrained robotic platforms. Here we cast the imitation learning problem as a conditional sequence modeling task and we train a decision <b>transformer</b> using expert demonstrations augmented with a custom reward. Then, we compress the resulting generative model using software optimization schemes, including <b>quantization</b> and <b>pruning.</b> We test our method in <b>simulation</b> using Isaac Gym, a realistic physics <b>simulation</b> environment designed for <b>reinforcement</b> <b>learning.</b> We empirically demonstrate that our method achieves natural looking gaits for Bittle, a resource-constrained quadruped robot. We also run multiple <b>simulations</b> to show the effects of <b>pruning</b> and <b>quantization</b> on the performance of the model. Our results show that <b>quantization</b> (down to 4 bits) and <b>pruning</b> reduce model size by around 30% while maintaining a competitive reward, making the model deployable in a resource-constrained system.</p></p class="citation"></blockquote><h3 id=26--203316-pre-trained-transformer-enabled-strategies-with-human-guided-fine-tuning-for-end-to-end-navigation-of-autonomous-vehicles-dong-hu-et-al-2024>(2/6 | 203/316) Pre-trained Transformer-Enabled Strategies with Human-Guided Fine-Tuning for End-to-end Navigation of Autonomous Vehicles (Dong Hu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dong Hu, Chao Huang, Jingda Wu, Hongbo Gao. (2024)<br><strong>Pre-trained Transformer-Enabled Strategies with Human-Guided Fine-Tuning for End-to-end Navigation of Autonomous Vehicles</strong><br><button class=copy-to-clipboard title="Pre-trained Transformer-Enabled Strategies with Human-Guided Fine-Tuning for End-to-end Navigation of Autonomous Vehicles" index=203>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-203 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 50<br>Keywords: Fine-tuning, Reinforcement Learning, Simulation, Simulator, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12666v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12666v1.pdf filename=2402.12666v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Autonomous driving (AD) technology, leveraging artificial intelligence, strives for vehicle automation. End-toend strategies, emerging to simplify traditional driving systems by integrating perception, decision-making, and control, offer new avenues for advanced driving functionalities. Despite their potential, current challenges include data efficiency, training complexities, and poor generalization. This study addresses these issues with a novel end-to-end AD training model, enhancing system adaptability and intelligence. The model incorporates a <b>Transformer</b> module into the policy network, undergoing initial behavior cloning (BC) pre-training for update gradients. Subsequently, <b>fine-tuning</b> through <b>reinforcement</b> <b>learning</b> with human guidance (RLHG) adapts the model to specific driving environments, aiming to surpass the performance limits of imitation learning (IL). The <b>fine-tuning</b> process involves human interactions, guiding the model to acquire more efficient and safer driving behaviors through supervision, intervention, demonstration, and reward feedback. <b>Simulation</b> results demonstrate that this framework accelerates learning, achieving precise control and significantly enhancing safety and reliability. Compared to other advanced baseline methods, the proposed approach excels in challenging AD tasks. The introduction of the <b>Transformer</b> module and human-guided <b>fine-tuning</b> provides valuable insights and methods for research and applications in the AD field.</p></p class="citation"></blockquote><h3 id=36--204316-dinobot-robot-manipulation-via-retrieval-and-alignment-with-vision-foundation-models-norman-di-palo-et-al-2024>(3/6 | 204/316) DINOBot: Robot Manipulation via Retrieval and Alignment with Vision Foundation Models (Norman Di Palo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Norman Di Palo, Edward Johns. (2024)<br><strong>DINOBot: Robot Manipulation via Retrieval and Alignment with Vision Foundation Models</strong><br><button class=copy-to-clipboard title="DINOBot: Robot Manipulation via Retrieval and Alignment with Vision Foundation Models" index=204>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-204 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-LG, cs-RO, cs.RO<br>Keyword Score: 40<br>Keywords: Vision Transformer, Foundation Model, Transformer, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13181v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13181v1.pdf filename=2402.13181v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We propose DINOBot, a novel imitation learning framework for robot manipulation, which leverages the image-level and pixel-level capabilities of features extracted from <b>Vision</b> <b>Transformers</b> trained with DINO. When interacting with a novel object, DINOBot first uses these features to retrieve the most visually similar object experienced during human demonstrations, and then uses this object to align its end-effector with the novel object to enable effective interaction. Through a series of real-world experiments on everyday tasks, we show that exploiting both the image-level and pixel-level properties of <b>vision</b> <b>foundation</b> <b>models</b> enables unprecedented learning efficiency and generalisation. Videos and code are available at <a href=https://www.robot-learning.uk/dinobot>https://www.robot-learning.uk/dinobot</a>.</p></p class="citation"></blockquote><h3 id=46--205316-a-recurrent-neural-network-enhanced-unscented-kalman-filter-for-human-motion-prediction-wansong-liu-et-al-2024>(4/6 | 205/316) A Recurrent Neural Network Enhanced Unscented Kalman Filter for Human Motion Prediction (Wansong Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wansong Liu, Sibo Tian, Boyi Hu, Xiao Liang, Minghui Zheng. (2024)<br><strong>A Recurrent Neural Network Enhanced Unscented Kalman Filter for Human Motion Prediction</strong><br><button class=copy-to-clipboard title="A Recurrent Neural Network Enhanced Unscented Kalman Filter for Human Motion Prediction" index=205>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-205 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 30<br>Keywords: Recurrent Neural Network, Recurrent Neural Network, Stemming<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13045v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13045v1.pdf filename=2402.13045v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents a deep learning enhanced adaptive unscented Kalman filter (UKF) for predicting human arm motion in the context of manufacturing. Unlike previous network-based methods that solely rely on captured human motion data, which is represented as bone vectors in this paper, we incorporate a human arm dynamic model into the motion prediction algorithm and use the UKF to iteratively forecast human arm motions. Specifically, a Lagrangian-mechanics-based physical model is employed to correlate arm motions with associated muscle forces. Then a <b>Recurrent</b> <b>Neural</b> <b>Network</b> <b>(RNN)</b> is integrated into the framework to predict future muscle forces, which are transferred back to future arm motions based on the dynamic model. Given the absence of measurement data for future human motions that can be input into the UKF to update the state, we integrate another <b>RNN</b> to directly predict human future motions and treat the prediction as surrogate measurement data fed into the UKF. A noteworthy aspect of this study involves the quantification of uncertainties associated with both the data-driven and physical models in one unified framework. These quantified uncertainties are used to dynamically adapt the measurement and process noises of the UKF over time. This adaption, driven by the uncertainties of the <b>RNN</b> models, addresses inaccuracies <b>stemming</b> from the data-driven model and mitigates discrepancies between the assumed and true physical models, ultimately enhancing the accuracy and robustness of our predictions. Compared to the traditional <b>RNN-based</b> prediction, our method demonstrates improved accuracy and robustness in extensive experimental validations of various types of human motions.</p></p class="citation"></blockquote><h3 id=56--206316-n-mpc-for-deep-neural-network-based-collision-avoidance-exploiting-depth-images-martin-jacquet-et-al-2024>(5/6 | 206/316) N-MPC for Deep Neural Network-Based Collision Avoidance exploiting Depth Images (Martin Jacquet et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Martin Jacquet, Kostas Alexis. (2024)<br><strong>N-MPC for Deep Neural Network-Based Collision Avoidance exploiting Depth Images</strong><br><button class=copy-to-clipboard title="N-MPC for Deep Neural Network-Based Collision Avoidance exploiting Depth Images" index=206>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-206 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13038v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13038v1.pdf filename=2402.13038v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper introduces a Nonlinear Model Predictive Control (N-MPC) framework exploiting a Deep Neural Network for processing onboard-captured depth images for collision avoidance in trajectory-tracking tasks with UAVs. The network is trained on simulated depth images to output a collision score for queried 3D points within the sensor field of view. Then, this network is translated into an algebraic symbolic equation and included in the N-MPC, explicitly constraining predicted positions to be collision-free throughout the receding horizon. The N-MPC achieves real time control of a UAV with a control frequency of 100Hz. The proposed framework is validated through statistical analysis of the collision classifier network, as well as Gazebo <b>simulations</b> and real experiments to assess the resulting capabilities of the N-MPC to effectively avoid collisions in cluttered environments. The associated code is released open-source along with the training images.</p></p class="citation"></blockquote><h3 id=66--207316-autonomous-reality-modelling-for-cultural-heritage-sites-employing-cooperative-quadrupedal-robots-and-unmanned-aerial-vehicles-nikolaos-giakoumidis-et-al-2024>(6/6 | 207/316) Autonomous Reality Modelling for Cultural Heritage Sites employing cooperative quadrupedal robots and unmanned aerial vehicles (Nikolaos Giakoumidis et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nikolaos Giakoumidis, Christos-Nikolaos Anagnostopoulos. (2024)<br><strong>Autonomous Reality Modelling for Cultural Heritage Sites employing cooperative quadrupedal robots and unmanned aerial vehicles</strong><br><button class=copy-to-clipboard title="Autonomous Reality Modelling for Cultural Heritage Sites employing cooperative quadrupedal robots and unmanned aerial vehicles" index=207>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-207 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-LG, cs-RO, cs.RO<br>Keyword Score: 10<br>Keywords: Human Intervention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12794v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12794v1.pdf filename=2402.12794v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Nowadays, the use of advanced sensors, such as terrestrial 3D laser scanners, mobile LiDARs and Unmanned Aerial Vehicles (UAV) photogrammetric imaging, has become the prevalent practice for 3D Reality Modeling and digitization of large-scale monuments of Cultural Heritage (CH). In practice, this process is heavily related to the expertise of the surveying team, handling the laborious planning and time-consuming execution of the 3D mapping process that is tailored to the specific requirements and constraints of each site. To minimize <b>human</b> <b>intervention,</b> this paper introduces a novel methodology for autonomous 3D Reality Modeling for CH monuments by employing au-tonomous biomimetic quadrupedal robotic agents and UAVs equipped with the appropriate sensors. These autonomous robotic agents carry out the 3D RM process in a systematic and repeatable ap-proach. The outcomes of this automated process may find applications in digital twin platforms, facilitating secure monitoring and management of cultural heritage sites and spaces, in both indoor and outdoor environments.</p></p class="citation"></blockquote><h2 id=statml-3>stat.ML (3)</h2><h3 id=13--208316-mode-estimation-with-partial-feedback-charles-arnal-et-al-2024>(1/3 | 208/316) Mode Estimation with Partial Feedback (Charles Arnal et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Charles Arnal, Vivien Cabannes, Vianney Perchet. (2024)<br><strong>Mode Estimation with Partial Feedback</strong><br><button class=copy-to-clipboard title="Mode Estimation with Partial Feedback" index=208>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-208 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: 62L05, 62B86, 62D10, 62B10, cs-IR, cs-IT, cs-LG, math-IT, stat-ML, stat.ML<br>Keyword Score: 60<br>Keywords: Active Learning, Bandit Algorithm, Bandit Algorithm, Fine-tuning, Supervised Learning, Weakly-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13079v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13079v1.pdf filename=2402.13079v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The combination of lightly <b>supervised</b> pre-training and online <b>fine-tuning</b> has played a key role in recent AI developments. These new learning pipelines call for new theoretical frameworks. In this paper, we formalize core aspects of weakly <b>supervised</b> and <b>active</b> <b>learning</b> with a simple problem: the estimation of the mode of a distribution using partial feedback. We show how entropy coding allows for optimal information acquisition from partial feedback, develop coarse sufficient statistics for mode identification, and adapt <b>bandit</b> <b>algorithms</b> to our new setting. Finally, we combine those contributions into a statistically and computationally efficient solution to our problem.</p></p class="citation"></blockquote><h3 id=23--209316-sgd-with-clipping-is-secretly-estimating-the-median-gradient-fabian-schaipp-et-al-2024>(2/3 | 209/316) SGD with Clipping is Secretly Estimating the Median Gradient (Fabian Schaipp et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fabian Schaipp, Guillaume Garrigos, Umut Simsekli, Robert Gower. (2024)<br><strong>SGD with Clipping is Secretly Estimating the Median Gradient</strong><br><button class=copy-to-clipboard title="SGD with Clipping is Secretly Estimating the Median Gradient" index=209>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-209 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: 90C26, 68T07, 62-08, cs-LG, math-OC, stat-ML, stat.ML<br>Keyword Score: 10<br>Keywords: Stochastic Gradient Descent<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12828v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12828v1.pdf filename=2402.12828v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>There are several applications of stochastic optimization where one can benefit from a robust estimate of the gradient. For example, domains such as distributed learning with corrupted nodes, the presence of large outliers in the training data, learning under privacy constraints, or even heavy-tailed noise due to the dynamics of the algorithm itself. Here we study <b>SGD</b> with robust gradient estimators based on estimating the median. We first consider computing the median gradient across samples, and show that the resulting method can converge even under heavy-tailed, state-dependent noise. We then derive iterative methods based on the stochastic proximal point method for computing the geometric median and generalizations thereof. Finally we propose an algorithm estimating the median gradient across iterations, and find that several well known methods - in particular different forms of clipping - are particular cases of this framework.</p></p class="citation"></blockquote><h3 id=33--210316-learning-under-singularity-an-information-criterion-improving-wbic-and-sbic-lirui-liu-et-al-2024>(3/3 | 210/316) Learning under Singularity: An Information Criterion improving WBIC and sBIC (Lirui Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lirui Liu, Joe Suzuki. (2024)<br><strong>Learning under Singularity: An Information Criterion improving WBIC and sBIC</strong><br><button class=copy-to-clipboard title="Learning under Singularity: An Information Criterion improving WBIC and sBIC" index=210>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-210 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-LG, stat-ML, stat.ML<br>Keyword Score: 3<br>Keywords: Sample Size<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12762v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12762v2.pdf filename=2402.12762v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce a novel Information Criterion (IC), termed Learning under Singularity (LS), designed to enhance the functionality of the Widely Applicable Bayes Information Criterion (WBIC) and the Singular Bayesian Information Criterion (sBIC). LS is effective without regularity constraints and demonstrates stability. Watanabe defined a statistical model or a learning machine as regular if the mapping from a parameter to a probability distribution is one-to-one and its Fisher information matrix is positive definite. In contrast, models not meeting these conditions are termed singular. Over the past decade, several information criteria for singular cases have been proposed, including WBIC and sBIC. WBIC is applicable in non-regular scenarios but faces challenges with large <b>sample</b> <b>sizes</b> and redundant estimation of known learning coefficients. Conversely, sBIC is limited in its broader application due to its dependence on maximum likelihood estimates. LS addresses these limitations by enhancing the utility of both WBIC and sBIC. It incorporates the empirical loss from the Widely Applicable Information Criterion (WAIC) to represent the goodness of fit to the statistical model, along with a penalty term similar to that of sBIC. This approach offers a flexible and robust method for model selection, free from regularity constraints.</p></p class="citation"></blockquote><h2 id=csai-9>cs.AI (9)</h2><h3 id=19--211316-more-3smultimodal-based-offline-reinforcement-learning-with-shared-semantic-spaces-tianyu-zheng-et-al-2024>(1/9 | 211/316) MORE-3S:Multimodal-based Offline Reinforcement Learning with Shared Semantic Spaces (Tianyu Zheng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tianyu Zheng, Ge Zhang, Xingwei Qu, Ming Kuang, Stephen W. Huang, Zhaofeng He. (2024)<br><strong>MORE-3S:Multimodal-based Offline Reinforcement Learning with Shared Semantic Spaces</strong><br><button class=copy-to-clipboard title="MORE-3S:Multimodal-based Offline Reinforcement Learning with Shared Semantic Spaces" index=211>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-211 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-GT, cs.AI<br>Keyword Score: 56<br>Keywords: Multi-modal, Multi-modal, Offline Reinforcement Learning, Reinforcement Learning, Supervised Learning, Supervised Learning, Pre-trained Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12845v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12845v1.pdf filename=2402.12845v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Drawing upon the intuition that aligning different modalities to the same semantic embedding space would allow models to understand states and actions more easily, we propose a new perspective to the <b>offline</b> <b>reinforcement</b> <b>learning</b> (RL) challenge. More concretely, we transform it into a <b>supervised</b> <b>learning</b> task by integrating <b>multimodal</b> and <b>pre-trained</b> <b>language</b> <b>models.</b> Our approach incorporates state information derived from images and action-related data obtained from text, thereby bolstering RL training performance and promoting long-term strategic thinking. We emphasize the contextual understanding of language and demonstrate how decision-making in RL can benefit from aligning states&rsquo; and actions&rsquo; representation with languages&rsquo; representation. Our method significantly outperforms current baselines as evidenced by evaluations conducted on Atari and OpenAI Gym environments. This contributes to advancing <b>offline</b> <b>RL</b> <b>performance</b> and efficiency while providing a novel perspective on <b>offline</b> <b>RL.Our</b> <b>code</b> and data are available at <a href=https://github.com/Zheng0428/MORE>https://github.com/Zheng0428/MORE</a>_.</p></p class="citation"></blockquote><h3 id=29--212316-toward-transformers-revolutionizing-the-solution-of-mixed-integer-programs-with-transformers-joshua-f-cooper-et-al-2024>(2/9 | 212/316) Toward TransfORmers: Revolutionizing the Solution of Mixed Integer Programs with Transformers (Joshua F. Cooper et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Joshua F. Cooper, Seung Jin Choi, I. Esra Buyuktahtakin. (2024)<br><strong>Toward TransfORmers: Revolutionizing the Solution of Mixed Integer Programs with Transformers</strong><br><button class=copy-to-clipboard title="Toward TransfORmers: Revolutionizing the Solution of Mixed Integer Programs with Transformers" index=212>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-212 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-LG, cs.AI, math-CO, math-OC, stat-ML<br>Keyword Score: 43<br>Keywords: Benchmarking, LSTM, LSTM, LSTM, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13380v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13380v1.pdf filename=2402.13380v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this study, we introduce an innovative deep learning framework that employs a <b>transformer</b> model to address the challenges of mixed-integer programs, specifically focusing on the Capacitated Lot Sizing Problem (CLSP). Our approach, to our knowledge, is the first to utilize <b>transformers</b> to predict the binary variables of a mixed-integer programming (MIP) problem. Specifically, our approach harnesses the encoder decoder <b>transformer&rsquo;s</b> ability to process sequential data, making it well-suited for predicting binary variables indicating production setup decisions in each period of the CLSP. This problem is inherently dynamic, and we need to handle sequential decision making under constraints. We present an efficient algorithm in which CLSP solutions are learned through a <b>transformer</b> neural network. The proposed post-processed <b>transformer</b> algorithm surpasses the state-of-the-art solver, CPLEX and <b>Long</b> <b>Short-Term</b> <b>Memory</b> <b>(LSTM)</b> in solution time, optimal gap, and percent infeasibility over 240K <b>benchmark</b> CLSP instances tested. After the ML model is trained, conducting inference on the model, including post-processing, reduces the MIP into a linear program (LP). This transforms the ML-based algorithm, combined with an LP solver, into a polynomial-time approximation algorithm to solve a well-known NP-Hard problem, with almost perfect solution quality.</p></p class="citation"></blockquote><h3 id=39--213316-from-cloud-to-edge-rethinking-generative-ai-for-low-resource-design-challenges-sai-krishna-revanth-vuruma-et-al-2024>(3/9 | 213/316) From Cloud to Edge: Rethinking Generative AI for Low-Resource Design Challenges (Sai Krishna Revanth Vuruma et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sai Krishna Revanth Vuruma, Ashley Margetts, Jianhai Su, Faez Ahmed, Biplav Srivastava. (2024)<br><strong>From Cloud to Edge: Rethinking Generative AI for Low-Resource Design Challenges</strong><br><button class=copy-to-clipboard title="From Cloud to Edge: Rethinking Generative AI for Low-Resource Design Challenges" index=213>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-213 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CY, cs.AI<br>Keyword Score: 30<br>Keywords: Generative AI, Low-Resource, Model Compression<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12702v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12702v1.pdf filename=2402.12702v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Generative</b> <b>Artificial</b> Intelligence (AI) has shown tremendous prospects in all aspects of technology, including design. However, due to its heavy demand on resources, it is usually trained on large computing infrastructure and often made available as a cloud-based service. In this position paper, we consider the potential, challenges, and promising approaches for <b>generative</b> <b>AI</b> for design on the edge, i.e., in resource-constrained settings where memory, compute, energy (battery) and network connectivity may be limited. Adapting <b>generative</b> <b>AI</b> for such settings involves overcoming significant hurdles, primarily in how to streamline complex <b>models</b> <b>to</b> function efficiently in <b>low-resource</b> environments. This necessitates innovative approaches in <b>model</b> <b>compression,</b> efficient algorithmic design, and perhaps even leveraging edge computing. The objective is to harness the power of <b>generative</b> <b>AI</b> in creating bespoke solutions for design problems, such as medical interventions, farm equipment maintenance, and educational material design, tailored to the unique constraints and needs of remote areas. These efforts could democratize access to advanced technology and foster sustainable development, ensuring universal accessibility and environmental consideration of AI-driven design benefits.</p></p class="citation"></blockquote><h3 id=49--214316-xrl-bench-a-benchmark-for-evaluating-and-comparing-explainable-reinforcement-learning-techniques-yu-xiong-et-al-2024>(4/9 | 214/316) XRL-Bench: A Benchmark for Evaluating and Comparing Explainable Reinforcement Learning Techniques (Yu Xiong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yu Xiong, Zhipeng Hu, Ye Huang, Runze Wu, Kai Guan, Xingchen Fang, Ji Jiang, Tianze Zhou, Yujing Hu, Haoyu Liu, Tangjie Lyu, Changjie Fan. (2024)<br><strong>XRL-Bench: A Benchmark for Evaluating and Comparing Explainable Reinforcement Learning Techniques</strong><br><button class=copy-to-clipboard title="XRL-Bench: A Benchmark for Evaluating and Comparing Explainable Reinforcement Learning Techniques" index=214>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-214 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 23<br>Keywords: Benchmarking, Explainable AI, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12685v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12685v1.pdf filename=2402.12685v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Reinforcement</b> <b>Learning</b> (RL) has demonstrated substantial potential across diverse fields, yet understanding its decision-making process, especially in real-world scenarios where rationality and safety are paramount, is an ongoing challenge. This paper delves in to <b>Explainable</b> <b>RL</b> (XRL), a subfield of <b>Explainable</b> <b>AI</b> (XAI) aimed at unravelling the complexities of RL models. Our focus rests on state-explaining techniques, a crucial subset within XRL methods, as they reveal the underlying factors influencing an agent&rsquo;s actions at any given time. Despite their significant role, the lack of a unified evaluation framework hinders assessment of their accuracy and effectiveness. To address this, we introduce XRL-Bench, a unified standardized <b>benchmark</b> tailored for the evaluation and comparison of XRL methods, encompassing three main modules: standard RL environments, explainers based on state importance, and standard evaluators. XRL-Bench supports both tabular and image data for state explanation. We also propose TabularSHAP, an innovative and competitive XRL method. We demonstrate the practical utility of TabularSHAP in real-world online gaming services and offer an open-source <b>benchmark</b> platform for the straightforward implementation and evaluation of XRL methods. Our contributions facilitate the continued progression of XRL technology.</p></p class="citation"></blockquote><h3 id=59--215316-analyzing-operator-states-and-the-impact-of-ai-enhanced-decision-support-in-control-rooms-a-human-in-the-loop-specialized-reinforcement-learning-framework-for-intervention-strategies-ammar-n-abbas-et-al-2024>(5/9 | 215/316) Analyzing Operator States and the Impact of AI-Enhanced Decision Support in Control Rooms: A Human-in-the-Loop Specialized Reinforcement Learning Framework for Intervention Strategies (Ammar N. Abbas et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ammar N. Abbas, Chidera W. Amazu, Joseph Mietkiewicz, Houda Briwa, Andres Alonzo Perez, Gabriele Baldissone, Micaela Demichela, Georgios G. Chasparis, John D. Kelleher, Maria Chiara Leva. (2024)<br><strong>Analyzing Operator States and the Impact of AI-Enhanced Decision Support in Control Rooms: A Human-in-the-Loop Specialized Reinforcement Learning Framework for Intervention Strategies</strong><br><button class=copy-to-clipboard title="Analyzing Operator States and the Impact of AI-Enhanced Decision Support in Control Rooms: A Human-in-the-Loop Specialized Reinforcement Learning Framework for Intervention Strategies" index=215>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-215 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-HC, cs-LG, cs-MA, cs-SY, cs.AI, eess-SY<br>Keyword Score: 20<br>Keywords: Reinforcement Learning, human-in-the-loop<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13219v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13219v1.pdf filename=2402.13219v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In complex industrial and chemical process control rooms, effective decision-making is crucial for safety and efficiency. The experiments in this paper evaluate the impact and applications of an AI-based decision support system integrated into an improved human-machine interface, using dynamic influence diagrams, a hidden Markov model, and deep <b>reinforcement</b> <b>learning.</b> The enhanced support system aims to reduce operator workload, improve situational awareness, and provide different intervention strategies to the operator adapted to the current state of both the system and human performance. Such a system can be particularly useful in cases of information overload when many alarms and inputs are presented all within the same time window, or for junior operators during training. A comprehensive cross-data analysis was conducted, involving 47 participants and a diverse range of data sources such as smartwatch metrics, eye-tracking data, process logs, and responses from questionnaires. The results indicate interesting insights regarding the effectiveness of the approach in aiding decision-making, decreasing perceived workload, and increasing situational awareness for the scenarios considered. Additionally, the results provide valuable insights to compare differences between styles of information gathering when using the system by individual participants. These findings are particularly relevant when predicting the overall performance of the individual participant and their capacity to successfully handle a plant upset and the alarms connected to it using process and human-machine interaction logs in real-time. These predictions enable the development of more effective intervention strategies.</p></p class="citation"></blockquote><h3 id=69--216316-improving-neural-based-classification-with-logical-background-knowledge-arthur-ledaguenel-et-al-2024>(6/9 | 216/316) Improving Neural-based Classification with Logical Background Knowledge (Arthur Ledaguenel et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Arthur Ledaguenel, Céline Hudelot, Mostepha Khouadjia. (2024)<br><strong>Improving Neural-based Classification with Logical Background Knowledge</strong><br><button class=copy-to-clipboard title="Improving Neural-based Classification with Logical Background Knowledge" index=216>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-216 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-LG, cs-SC, cs.AI<br>Keyword Score: 20<br>Keywords: Supervised Learning, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13019v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13019v1.pdf filename=2402.13019v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Neurosymbolic AI is a growing field of research aiming to combine neural networks learning capabilities with the <b>reasoning</b> abilities of symbolic systems. This hybridization can take many shapes. In this paper, we propose a new formalism for <b>supervised</b> multi-label classification with propositional background knowledge. We introduce a new neurosymbolic technique called semantic conditioning at inference, which only constrains the system during inference while leaving the training unaffected. We discuss its theoritical and practical advantages over two other popular neurosymbolic techniques: semantic conditioning and semantic regularization. We develop a new multi-scale methodology to evaluate how the benefits of a neurosymbolic technique evolve with the scale of the network. We then evaluate experimentally and compare the benefits of all three techniques across model scales on several datasets. Our results demonstrate that semantic conditioning at inference can be used to build more accurate neural-based systems with fewer resources while guaranteeing the semantic consistency of outputs.</p></p class="citation"></blockquote><h3 id=79--217316-patient-centric-knowledge-graphs-a-survey-of-current-methods-challenges-and-applications-hassan-s-al-khatib-et-al-2024>(7/9 | 217/316) Patient-Centric Knowledge Graphs: A Survey of Current Methods, Challenges, and Applications (Hassan S. Al Khatib et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hassan S. Al Khatib, Subash Neupane, Harish Kumar Manchukonda, Noorbakhsh Amiri Golilarz, Sudip Mittal, Amin Amirlatifi, Shahram Rahimi. (2024)<br><strong>Patient-Centric Knowledge Graphs: A Survey of Current Methods, Challenges, and Applications</strong><br><button class=copy-to-clipboard title="Patient-Centric Knowledge Graphs: A Survey of Current Methods, Challenges, and Applications" index=217>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-217 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 18<br>Keywords: Graph, Knowledge Graph, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12608v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12608v1.pdf filename=2402.12608v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Patient-Centric <b>Knowledge</b> <b>Graphs</b> (PCKGs) represent an important shift in healthcare that focuses on individualized patient care by mapping the patient&rsquo;s health information in a holistic and multi-dimensional way. PCKGs integrate various types of health data to provide healthcare professionals with a comprehensive understanding of a patient&rsquo;s health, enabling more personalized and effective care. This literature review explores the methodologies, challenges, and opportunities associated with PCKGs, focusing on their role in integrating disparate healthcare data and enhancing patient care through a unified health perspective. In addition, this review also discusses the complexities of PCKG development, including ontology design, data integration techniques, <b>knowledge</b> <b>extraction,</b> and structured representation of <b>knowledge.</b> <b>It</b> highlights advanced techniques such as <b>reasoning,</b> semantic search, and inference mechanisms essential in constructing and evaluating PCKGs for actionable healthcare insights. We further explore the practical applications of PCKGs in personalized medicine, emphasizing their significance in improving disease prediction and formulating effective treatment plans. Overall, this review provides a foundational perspective on the current state-of-the-art and best practices of PCKGs, guiding future research and applications in this dynamic field.</p></p class="citation"></blockquote><h3 id=89--218316-random-graph-set-and-evidence-pattern-reasoning-model-tianxiang-zhan-et-al-2024>(8/9 | 218/316) Random Graph Set and Evidence Pattern Reasoning Model (Tianxiang Zhan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tianxiang Zhan, Zhen Li, Yong Deng. (2024)<br><strong>Random Graph Set and Evidence Pattern Reasoning Model</strong><br><button class=copy-to-clipboard title="Random Graph Set and Evidence Pattern Reasoning Model" index=218>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-218 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 13<br>Keywords: Graph, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13058v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13058v1.pdf filename=2402.13058v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Evidence theory is widely used in decision-making and <b>reasoning</b> systems. In previous research, Transferable Belief Model (TBM) is a commonly used evidential decision making model, but TBM is a non-preference model. In order to better fit the decision making goals, the Evidence Pattern <b>Reasoning</b> Model (EPRM) is proposed. By defining pattern operators and decision making operators, corresponding preferences can be set for different tasks. Random Permutation Set (RPS) expands order information for evidence theory. It is hard for RPS to characterize the complex relationship between samples such as cycling, paralleling relationships. Therefore, Random <b>Graph</b> Set (RGS) were proposed to model complex relationships and represent more event types. In order to illustrate the significance of RGS and EPRM, an experiment of aircraft velocity ranking was designed and 10,000 cases were simulated. The implementation of EPRM called Conflict Resolution Decision optimized 18.17% of the cases compared to Mean Velocity Decision, effectively improving the aircraft velocity ranking. EPRM provides a unified solution for evidence-based decision making.</p></p class="citation"></blockquote><h3 id=99--219316-learning-and-sustaining-shared-normative-systems-via-bayesian-rule-induction-in-markov-games-ninell-oldenburg-et-al-2024>(9/9 | 219/316) Learning and Sustaining Shared Normative Systems via Bayesian Rule Induction in Markov Games (Ninell Oldenburg et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ninell Oldenburg, Tan Zhi-Xuan. (2024)<br><strong>Learning and Sustaining Shared Normative Systems via Bayesian Rule Induction in Markov Games</strong><br><button class=copy-to-clipboard title="Learning and Sustaining Shared Normative Systems via Bayesian Rule Induction in Markov Games" index=219>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-219 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: I-2-0; I-6-5; G-3, cs-AI, cs.AI<br>Keyword Score: 10<br>Keywords: Markov Game<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13399v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13399v2.pdf filename=2402.13399v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A universal feature of human societies is the adoption of systems of rules and norms in the service of cooperative ends. How can we build learning agents that do the same, so that they may flexibly cooperate with the human institutions they are embedded in? We hypothesize that agents can achieve this by assuming there exists a shared set of norms that most others comply with while pursuing their individual desires, even if they do not know the exact content of those norms. By assuming shared norms, a newly introduced agent can infer the norms of an existing population from observations of compliance and violation. Furthermore, groups of agents can converge to a shared set of norms, even if they initially diverge in their beliefs about what the norms are. This in turn enables the stability of the normative system: since agents can bootstrap common knowledge of the norms, this leads the norms to be widely adhered to, enabling new entrants to rapidly learn those norms. We formalize this framework in the context of <b>Markov</b> <b>games</b> and demonstrate its operation in a multi-agent environment via approximately Bayesian rule induction of obligative and prohibitive norms. Using our approach, agents are able to rapidly learn and sustain a variety of cooperative institutions, including resource management norms and compensation for pro-social labor, promoting collective welfare while still allowing agents to act in their own interests.</p></p class="citation"></blockquote><h2 id=eessas-3>eess.AS (3)</h2><h3 id=13--220316-emo-superb-an-in-depth-look-at-speech-emotion-recognition-haibin-wu-et-al-2024>(1/3 | 220/316) EMO-SUPERB: An In-depth Look at Speech Emotion Recognition (Haibin Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haibin Wu, Huang-Cheng Chou, Kai-Wei Chang, Lucas Goncalves, Jiawei Du, Jyh-Shing Roger Jang, Chi-Chun Lee, Hung-Yi Lee. (2024)<br><strong>EMO-SUPERB: An In-depth Look at Speech Emotion Recognition</strong><br><button class=copy-to-clipboard title="EMO-SUPERB: An In-depth Look at Speech Emotion Recognition" index=220>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-220 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.AS<br>Categories: cs-SD, eess-AS, eess.AS<br>Keyword Score: 53<br>Keywords: Benchmarking, Self-supervised Learning, Self-supervised Learning, ChatGPT, Emotion Recognition, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13018v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13018v2.pdf filename=2402.13018v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Speech <b>emotion</b> <b>recognition</b> (SER) is a pivotal technology for human-computer interaction systems. However, 80.77% of SER papers yield results that cannot be reproduced. We develop EMO-SUPERB, short for <b>EMOtion</b> <b>Speech</b> Universal PERformance <b>Benchmark,</b> which aims to enhance open-source initiatives for SER. EMO-SUPERB includes a user-friendly codebase to leverage 15 state-of-the-art speech <b>self-supervised</b> <b>learning</b> models (SSLMs) for exhaustive evaluation across six open-source SER datasets. EMO-SUPERB streamlines result sharing via an online leaderboard, fostering collaboration within a community-driven <b>benchmark</b> and thereby enhancing the development of SER. On average, 2.58% of annotations are annotated using natural language. SER relies on classification models and is unable to process natural languages, leading to the discarding of these valuable annotations. We <b>prompt</b> <b>ChatGPT</b> to mimic annotators, comprehend natural language annotations, and subsequently re-label the data. By utilizing labels generated by <b>ChatGPT,</b> we consistently achieve an average relative gain of 3.08% across all settings.</p></p class="citation"></blockquote><h3 id=23--221316-plugin-speech-enhancement-a-universal-speech-enhancement-framework-inspired-by-dynamic-neural-network-yanan-chen-et-al-2024>(2/3 | 221/316) Plugin Speech Enhancement: A Universal Speech Enhancement Framework Inspired by Dynamic Neural Network (Yanan Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yanan Chen, Zihao Cui, Yingying Gao, Junlan Feng, Chao Deng, Shilei Zhang. (2024)<br><strong>Plugin Speech Enhancement: A Universal Speech Enhancement Framework Inspired by Dynamic Neural Network</strong><br><button class=copy-to-clipboard title="Plugin Speech Enhancement: A Universal Speech Enhancement Framework Inspired by Dynamic Neural Network" index=221>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-221 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.AS<br>Categories: cs-SD, eess-AS, eess.AS<br>Keyword Score: 10<br>Keywords: Data Augmentation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12746v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12746v1.pdf filename=2402.12746v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The expectation to deploy a universal neural network for speech enhancement, with the aim of improving noise robustness across diverse speech processing tasks, faces challenges due to the existing lack of awareness within static speech enhancement frameworks regarding the expected speech in downstream modules. These limitations impede the effectiveness of static speech enhancement approaches in achieving optimal performance for a range of speech processing tasks, thereby challenging the notion of universal applicability. The fundamental issue in achieving universal speech enhancement lies in effectively informing the speech enhancement module about the features of downstream modules. In this study, we present a novel weighting prediction approach, which explicitly learns the task relationships from downstream training information to address the core challenge of universal speech enhancement. We found the role of deciding whether to employ <b>data</b> <b>augmentation</b> techniques as crucial downstream training information. This decision significantly impacts the expected speech and the performance of the speech enhancement module. Moreover, we introduce a novel speech enhancement network, the Plugin Speech Enhancement (Plugin-SE). The Plugin-SE is a dynamic neural network that includes the speech enhancement module, gate module, and weight prediction module. Experimental results demonstrate that the proposed Plugin-SE approach is competitive or superior to other joint training methods across various downstream tasks.</p></p class="citation"></blockquote><h3 id=33--222316-codec-superb-an-in-depth-analysis-of-sound-codec-models-haibin-wu-et-al-2024>(3/3 | 222/316) Codec-SUPERB: An In-Depth Analysis of Sound Codec Models (Haibin Wu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haibin Wu, Ho-Lam Chung, Yi-Cheng Lin, Yuan-Kuei Wu, Xuanjun Chen, Yu-Chi Pai, Hsiu-Hsuan Wang, Kai-Wei Chang, Alexander H. Liu, Hung-yi Lee. (2024)<br><strong>Codec-SUPERB: An In-Depth Analysis of Sound Codec Models</strong><br><button class=copy-to-clipboard title="Codec-SUPERB: An In-Depth Analysis of Sound Codec Models" index=222>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-222 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.AS<br>Categories: cs-SD, eess-AS, eess.AS<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13071v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13071v1.pdf filename=2402.13071v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The sound codec&rsquo;s dual roles in minimizing data transmission latency and serving as tokenizers underscore its critical importance. Recent years have witnessed significant developments in codec models. The ideal sound codec should preserve content, paralinguistics, speakers, and audio information. However, the question of which codec achieves optimal sound information preservation remains unanswered, as in different papers, models are evaluated on their selected experimental settings. This study introduces Codec-SUPERB, an acronym for Codec sound processing Universal PERformance <b>Benchmark.</b> It is an ecosystem designed to assess codec models across representative sound applications and signal-level metrics rooted in sound domain knowledge.Codec-SUPERB simplifies result sharing through an online leaderboard, promoting collaboration within a community-driven <b>benchmark</b> database, thereby stimulating new development cycles for codecs. Furthermore, we undertake an in-depth analysis to offer insights into codec models from both application and signal perspectives, diverging from previous codec papers mainly concentrating on signal-level comparisons. Finally, we will release codes, the leaderboard, and data to accelerate progress within the community.</p></p class="citation"></blockquote><h2 id=csir-8>cs.IR (8)</h2><h3 id=18--223316-distributionally-robust-graph-based-recommendation-system-bohao-wang-et-al-2024>(1/8 | 223/316) Distributionally Robust Graph-based Recommendation System (Bohao Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bohao Wang, Jiawei Chen, Changdong Li, Sheng Zhou, Qihao Shi, Yang Gao, Yan Feng, Chun Chen, Can Wang. (2024)<br><strong>Distributionally Robust Graph-based Recommendation System</strong><br><button class=copy-to-clipboard title="Distributionally Robust Graph-based Recommendation System" index=223>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-223 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 53<br>Keywords: Graph, Graph Neural Network, Graph Neural Network, Distribution Shift, Distribution Shift, Recommendation, Recommender System<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12994v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12994v2.pdf filename=2402.12994v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the capacity to capture high-order collaborative signals, <b>Graph</b> <b>Neural</b> <b>Networks</b> <b>(GNNs)</b> have emerged as powerful methods in <b>Recommender</b> <b>Systems</b> (RS). However, their efficacy often hinges on the assumption that training and testing data share the same <b>distribution</b> <b>(a.k.a.</b> IID assumption), and exhibits significant declines under <b>distribution</b> <b>shifts.</b> <b>Distribution</b> <b>shifts</b> commonly arises in RS, often attributed to the dynamic nature of user preferences or ubiquitous biases during data collection in RS. Despite its significance, researches on <b>GNN-based</b> <b>recommendation</b> against <b>distribution</b> <b>shift</b> are still sparse. To bridge this gap, we propose Distributionally Robust <b>GNN</b> (DR-GNN) that incorporates <b>Distributional</b> <b>Robust</b> Optimization (DRO) into the <b>GNN-based</b> <b>recommendation.</b> DR-GNN addresses two core challenges: 1) To enable DRO to cater to <b>graph</b> <b>data</b> <b>intertwined</b> with <b>GNN,</b> we reinterpret <b>GNN</b> as a <b>graph</b> <b>smoothing</b> <b>regularizer,</b> thereby facilitating the nuanced application of DRO; 2) Given the typically sparse nature of <b>recommendation</b> data, which might impede robust optimization, we introduce slight perturbations in the training <b>distribution</b> <b>to</b> expand its support. Notably, while DR-GNN involves complex optimization, it can be implemented easily and efficiently. Our extensive experiments validate the effectiveness of DR-GNN against three typical <b>distribution</b> <b>shifts.</b> The code is available at <a href=https://github.com/WANGBohaO-jpg/DR-GNN>https://github.com/WANGBohaO-jpg/DR-GNN</a>.</p></p class="citation"></blockquote><h3 id=28--224316-understanding-and-mitigating-the-threat-of-vec2text-to-dense-retrieval-systems-shengyao-zhuang-et-al-2024>(2/8 | 224/316) Understanding and Mitigating the Threat of Vec2Text to Dense Retrieval Systems (Shengyao Zhuang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shengyao Zhuang, Bevan Koopman, Xiaoran Chu, Guido Zuccon. (2024)<br><strong>Understanding and Mitigating the Threat of Vec2Text to Dense Retrieval Systems</strong><br><button class=copy-to-clipboard title="Understanding and Mitigating the Threat of Vec2Text to Dense Retrieval Systems" index=224>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-224 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-CL, cs-IR, cs.IR<br>Keyword Score: 40<br>Keywords: Dense Retrieval, Quantization, Cohere, Text Embedding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12784v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12784v1.pdf filename=2402.12784v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The introduction of Vec2Text, a technique for inverting <b>text</b> <b>embeddings,</b> has raised serious privacy concerns within <b>dense</b> <b>retrieval</b> systems utilizing <b>text</b> <b>embeddings,</b> including those provided by OpenAI and <b>Cohere.</b> This threat comes from the ability for a malicious attacker with access to <b>text</b> <b>embeddings</b> to reconstruct the original <b>text.</b> <b>In</b> this paper, we investigate various aspects of embedding models that could influence the recoverability of <b>text</b> <b>using</b> Vec2Text. Our exploration involves factors such as distance metrics, pooling functions, bottleneck pre-training, training with noise addition, embedding <b>quantization,</b> and embedding dimensions &ndash; aspects not previously addressed in the original Vec2Text paper. Through a thorough analysis of these factors, our aim is to gain a deeper understanding of the critical elements impacting the trade-offs between <b>text</b> <b>recoverability</b> and retrieval effectiveness in <b>dense</b> <b>retrieval</b> systems. This analysis provides valuable insights for practitioners involved in designing privacy-aware <b>dense</b> <b>retrieval</b> systems. Additionally, we propose a straightforward fix for embedding transformation that ensures equal ranking effectiveness while mitigating the risk of <b>text</b> <b>recoverability.</b> Furthermore, we extend the application of Vec2Text to the separate task of corpus poisoning, where, theoretically, Vec2Text presents a more potent threat compared to previous attack methods. Notably, Vec2Text does not require access to the <b>dense</b> <b>retriever&rsquo;s</b> model parameters and can efficiently generate numerous adversarial passages. In summary, this study highlights the potential threat posed by Vec2Text to existing <b>dense</b> <b>retrieval</b> systems, while also presenting effective methods to patch and strengthen such systems against such risks.</p></p class="citation"></blockquote><h3 id=38--225316-unlocking-the-why-of-buying-introducing-a-new-dataset-and-benchmark-for-purchase-reason-and-post-purchase-experience-tao-chen-et-al-2024>(3/8 | 225/316) Unlocking the `Why&rsquo; of Buying: Introducing a New Dataset and Benchmark for Purchase Reason and Post-Purchase Experience (Tao Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tao Chen, Siqi Zuo, Cheng Li, Mingyang Zhang, Qiaozhu Mei, Michael Bendersky. (2024)<br>**Unlocking the <code>Why' of Buying: Introducing a New Dataset and Benchmark for Purchase Reason and Post-Purchase Experience** &lt;br/> &lt;button class="copy-to-clipboard" title="Unlocking the </code>Why&rsquo; of Buying: Introducing a New Dataset and Benchmark for Purchase Reason and Post-Purchase Experience" index=225>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-225 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 33<br>Keywords: Benchmarking, Recommendation, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13417v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13417v1.pdf filename=2402.13417v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Explanations are crucial for enhancing user trust and understanding within modern <b>recommendation</b> systems. To build truly explainable systems, we need high-quality datasets that elucidate why users make choices. While previous efforts have focused on extracting users&rsquo; post-purchase sentiment in reviews, they ignore the reasons behind the decision to buy. In our work, we propose a novel purchase reason explanation task. To this end, we introduce an <b>LLM-based</b> approach to generate a dataset that consists of textual explanations of why real users make certain purchase decisions. We induce <b>LLMs</b> to explicitly distinguish between the reasons behind purchasing a product and the experience after the purchase in a user review. An automated, <b>LLM-driven</b> evaluation, as well as a small scale human evaluation, confirms the effectiveness of our approach to obtaining high-quality, personalized explanations. We <b>benchmark</b> this dataset on two personalized explanation generation tasks. We release the code and <b>prompts</b> to spur further research.</p></p class="citation"></blockquote><h3 id=48--226316-an-autonomous-large-language-model-agent-for-chemical-literature-data-mining-kexin-chen-et-al-2024>(4/8 | 226/316) An Autonomous Large Language Model Agent for Chemical Literature Data Mining (Kexin Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kexin Chen, Hanqun Cao, Junyou Li, Yuyang Du, Menghao Guo, Xin Zeng, Lanqing Li, Jiezhong Qiu, Pheng Ann Heng, Guangyong Chen. (2024)<br><strong>An Autonomous Large Language Model Agent for Chemical Literature Data Mining</strong><br><button class=copy-to-clipboard title="An Autonomous Large Language Model Agent for Chemical Literature Data Mining" index=226>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-226 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-AI, cs-IR, cs-LG, cs.IR, q-bio-QM<br>Keyword Score: 30<br>Keywords: Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12993v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12993v1.pdf filename=2402.12993v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Chemical synthesis, which is crucial for advancing material synthesis and drug discovery, impacts various sectors including environmental science and healthcare. The rise of technology in chemistry has generated extensive chemical data, challenging researchers to discern patterns and refine synthesis processes. Artificial intelligence (AI) helps by analyzing data to optimize synthesis and increase yields. However, AI faces challenges in processing literature data due to the unstructured format and diverse writing style of chemical literature. To overcome these difficulties, we introduce an end-to-end AI agent framework capable of high-fidelity extraction from extensive chemical literature. This AI agent employs <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> for <b>prompt</b> generation and iterative optimization. It functions as a chemistry assistant, automating data collection and analysis, thereby saving manpower and enhancing performance. Our framework&rsquo;s efficacy is evaluated using accuracy, recall, and F1 score of reaction condition data, and we compared our method with human experts in terms of content correctness and time efficiency. The proposed approach marks a significant advancement in automating chemical literature extraction and demonstrates the potential for AI to revolutionize data management and utilization in chemistry.</p></p class="citation"></blockquote><h3 id=58--227316-towards-trustworthy-reranking-a-simple-yet-effective-abstention-mechanism-hippolyte-gisserot-boukhlef-et-al-2024>(5/8 | 227/316) Towards Trustworthy Reranking: A Simple yet Effective Abstention Mechanism (Hippolyte Gisserot-Boukhlef et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hippolyte Gisserot-Boukhlef, Manuel Faysse, Emmanuel Malherbe, Céline Hudelot, Pierre Colombo. (2024)<br><strong>Towards Trustworthy Reranking: A Simple yet Effective Abstention Mechanism</strong><br><button class=copy-to-clipboard title="Towards Trustworthy Reranking: A Simple yet Effective Abstention Mechanism" index=227>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-227 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-CL, cs-IR, cs.IR<br>Keyword Score: 25<br>Keywords: Black Box, Rerank, Information Retrieval<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12997v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12997v2.pdf filename=2402.12997v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Neural <b>Information</b> <b>Retrieval</b> (NIR) has significantly improved upon heuristic-based IR systems. Yet, failures remain frequent, the models used often being unable to retrieve documents relevant to the user&rsquo;s query. We address this challenge by proposing a lightweight abstention mechanism tailored for real-world constraints, with particular emphasis placed on the <b>reranking</b> phase. We introduce a protocol for evaluating abstention strategies in a <b>black-box</b> <b>scenario,</b> demonstrating their efficacy, and propose a simple yet effective data-driven mechanism. We provide open-source code for experiment replication and abstention implementation, fostering wider adoption and application in diverse contexts.</p></p class="citation"></blockquote><h3 id=68--228316-unlocking-insights-semantic-search-in-jupyter-notebooks-lan-li-et-al-2024>(6/8 | 228/316) Unlocking Insights: Semantic Search in Jupyter Notebooks (Lan Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lan Li, Jinpeng Lv. (2024)<br><strong>Unlocking Insights: Semantic Search in Jupyter Notebooks</strong><br><button class=copy-to-clipboard title="Unlocking Insights: Semantic Search in Jupyter Notebooks" index=228>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-228 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-CL, cs-IR, cs.IR<br>Keyword Score: 20<br>Keywords: Information Retrieval, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13234v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13234v1.pdf filename=2402.13234v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Semantic search, a process aimed at delivering highly relevant search results by comprehending the searcher&rsquo;s intent and the contextual meaning of terms within a searchable dataspace, plays a pivotal role in <b>information</b> <b>retrieval.</b> In this paper, we investigate the application of <b>large</b> <b>language</b> <b>models</b> to enhance semantic search capabilities, specifically tailored for the domain of Jupyter Notebooks. Our objective is to retrieve generated outputs, such as figures or tables, associated functions and methods, and other pertinent <b>information.</b> <b>We</b> demonstrate a semantic search framework that achieves a comprehensive semantic understanding of the entire notebook&rsquo;s contents, enabling it to effectively handle various types of user queries. Key components of this framework include: 1). A data preprocessor is designed to handle diverse types of cells within Jupyter Notebooks, encompassing both markdown and code cells. 2). An innovative methodology is devised to address token size limitations that arise with code-type cells. We implement a finer-grained approach to data input, transitioning from the cell level to the function level, effectively resolving these issues.</p></p class="citation"></blockquote><h3 id=78--229316-interpreting-conversational-dense-retrieval-by-rewriting-enhanced-inversion-of-session-embedding-yiruo-cheng-et-al-2024>(7/8 | 229/316) Interpreting Conversational Dense Retrieval by Rewriting-Enhanced Inversion of Session Embedding (Yiruo Cheng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yiruo Cheng, Kelong Mao, Zhicheng Dou. (2024)<br><strong>Interpreting Conversational Dense Retrieval by Rewriting-Enhanced Inversion of Session Embedding</strong><br><button class=copy-to-clipboard title="Interpreting Conversational Dense Retrieval by Rewriting-Enhanced Inversion of Session Embedding" index=229>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-229 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 13<br>Keywords: Benchmarking, Dense Retrieval<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12774v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12774v1.pdf filename=2402.12774v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Conversational <b>dense</b> <b>retrieval</b> has shown to be effective in conversational search. However, a major limitation of conversational <b>dense</b> <b>retrieval</b> is their lack of interpretability, hindering intuitive understanding of model behaviors for targeted improvements. This paper presents CONVINV, a simple yet effective approach to shed light on interpretable conversational <b>dense</b> <b>retrieval</b> models. CONVINV transforms opaque conversational session embeddings into explicitly interpretable text while faithfully maintaining their original retrieval performance as much as possible. Such transformation is achieved by training a recently proposed Vec2Text model based on the ad-hoc query encoder, leveraging the fact that the session and query embeddings share the same space in existing conversational <b>dense</b> <b>retrieval.</b> To further enhance interpretability, we propose to incorporate external interpretable query rewrites into the transformation process. Extensive evaluations on three conversational search <b>benchmarks</b> demonstrate that CONVINV can yield more interpretable text and faithfully preserve original retrieval performance than baselines. Our work connects opaque session embeddings with transparent query rewriting, paving the way toward trustworthy conversational search.</p></p class="citation"></blockquote><h3 id=88--230316-bmlp-behavior-aware-mlp-for-heterogeneous-sequential-recommendation-weixin-li-et-al-2024>(8/8 | 230/316) BMLP: Behavior-aware MLP for Heterogeneous Sequential Recommendation (Weixin Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Weixin Li, Yuhao Wu, Yang Liu, Weike Pan, Zhong Ming. (2024)<br><strong>BMLP: Behavior-aware MLP for Heterogeneous Sequential Recommendation</strong><br><button class=copy-to-clipboard title="BMLP: Behavior-aware MLP for Heterogeneous Sequential Recommendation" index=230>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-230 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-AI, cs-IR, cs.IR<br>Keyword Score: 10<br>Keywords: Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12733v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12733v1.pdf filename=2402.12733v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In real <b>recommendation</b> scenarios, users often have different types of behaviors, such as clicking and buying. Existing research methods show that it is possible to capture the heterogeneous interests of users through different types of behaviors. However, most multi-behavior approaches have limitations in learning the relationship between different behaviors. In this paper, we propose a novel multilayer perceptron (MLP)-based heterogeneous sequential <b>recommendation</b> method, namely behavior-aware multilayer perceptron (BMLP). Specifically, it has two main modules, including a heterogeneous interest perception (HIP) module, which models behaviors at multiple granularities through behavior types and transition relationships, and a purchase intent perception (PIP) module, which adaptively fuses subsequences of auxiliary behaviors to capture users&rsquo; purchase intent. Compared with mainstream sequence models, MLP is competitive in terms of accuracy and has unique advantages in simplicity and efficiency. Extensive experiments show that BMLP achieves significant improvement over state-of-the-art algorithms on four public datasets. In addition, its pure MLP architecture leads to a linear time complexity.</p></p class="citation"></blockquote><h2 id=cscr-5>cs.CR (5)</h2><h3 id=15--231316-prompt-stealing-attacks-against-large-language-models-zeyang-sha-et-al-2024>(1/5 | 231/316) Prompt Stealing Attacks Against Large Language Models (Zeyang Sha et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zeyang Sha, Yang Zhang. (2024)<br><strong>Prompt Stealing Attacks Against Large Language Models</strong><br><button class=copy-to-clipboard title="Prompt Stealing Attacks Against Large Language Models" index=231>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-231 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CL, cs-CR, cs.CR<br>Keyword Score: 50<br>Keywords: ChatGPT, In-context Learning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12959v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12959v1.pdf filename=2402.12959v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The increasing reliance on <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> such as <b>ChatGPT</b> in various fields emphasizes the importance of ``prompt engineering,&rsquo;&rsquo; a technology to improve the quality of model outputs. With companies investing significantly in expert <b>prompt</b> engineers and educational resources rising to meet market demand, designing high-quality <b>prompts</b> has become an intriguing challenge. In this paper, we propose a novel attack against <b>LLMs,</b> named <b>prompt</b> stealing attacks. Our proposed <b>prompt</b> stealing attack aims to steal these well-designed <b>prompts</b> based on the generated answers. The <b>prompt</b> stealing attack contains two primary modules: the parameter extractor and the <b>prompt</b> reconstruction. The goal of the parameter extractor is to figure out the properties of the original <b>prompts.</b> We first observe that most <b>prompts</b> fall into one of three categories: direct <b>prompt,</b> role-based <b>prompt,</b> and <b>in-context</b> <b>prompt.</b> Our parameter extractor first tries to distinguish the type of <b>prompts</b> based on the generated answers. Then, it can further predict which role or how many contexts are used based on the types of <b>prompts.</b> Following the parameter extractor, the <b>prompt</b> reconstructor can be used to reconstruct the original <b>prompts</b> based on the generated answers and the extracted features. The final goal of the <b>prompt</b> reconstructor is to generate the reversed <b>prompts,</b> which are similar to the original <b>prompts.</b> Our experimental results show the remarkable performance of our proposed attacks. Our proposed attacks add a new dimension to the study of <b>prompt</b> engineering and call for more attention to the security issues on <b>LLMs.</b></p></p class="citation"></blockquote><h3 id=25--232316-the-wolf-within-covert-injection-of-malice-into-mllm-societies-via-an-mllm-operative-zhen-tan-et-al-2024>(2/5 | 232/316) The Wolf Within: Covert Injection of Malice into MLLM Societies via an MLLM Operative (Zhen Tan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhen Tan, Chengshuai Zhao, Raha Moraffah, Yifan Li, Yu Kong, Tianlong Chen, Huan Liu. (2024)<br><strong>The Wolf Within: Covert Injection of Malice into MLLM Societies via an MLLM Operative</strong><br><button class=copy-to-clipboard title="The Wolf Within: Covert Injection of Malice into MLLM Societies via an MLLM Operative" index=232>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-232 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-AI, cs-CR, cs-CY, cs-LG, cs.CR<br>Keyword Score: 26<br>Keywords: Multi-modal, Multi-modal, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14859v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14859v1.pdf filename=2402.14859v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Due to their unprecedented ability to process and respond to various types of data, <b>Multimodal</b> <b>Large</b> <b>Language</b> <b>Models</b> (MLLMs) are constantly defining the new boundary of Artificial General Intelligence (AGI). As these advanced generative models increasingly form collaborative networks for complex tasks, the integrity and security of these systems are crucial. Our paper, <code>The Wolf Within'', explores a novel vulnerability in MLLM societies - the indirect propagation of malicious content. Unlike direct harmful output generation for MLLMs, our research demonstrates how a single MLLM agent can be subtly influenced to generate &lt;b>prompts&lt;/b> that, in turn, induce other MLLM agents in the society to output malicious content. This subtle, yet potent method of indirect influence marks a significant escalation in the security risks associated with MLLMs. Our findings reveal that, with minimal or even no access to MLLMs' parameters, an MLLM agent, when manipulated to produce specific &lt;b>prompts&lt;/b> or instructions, can effectively </code>infect&rsquo;&rsquo; other agents within a society of MLLMs. This infection leads to the generation and circulation of harmful outputs, such as dangerous instructions or misinformation, across the society. We also show the transferability of these indirectly generated <b>prompts,</b> highlighting their possibility in propagating malice through inter-agent communication. This research provides a critical insight into a new dimension of threat posed by MLLMs, where a single agent can act as a catalyst for widespread malevolent influence. Our work underscores the urgent need for developing robust mechanisms to detect and mitigate such covert manipulations within MLLM societies, ensuring their safe and ethical utilization in societal applications. Our implementation is released at \url{https://github.com/ChengshuaiZhao0/The-Wolf-Within.git}.</p></p class="citation"></blockquote><h3 id=35--233316-generative-ai-security-challenges-and-countermeasures-banghua-zhu-et-al-2024>(3/5 | 233/316) Generative AI Security: Challenges and Countermeasures (Banghua Zhu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Banghua Zhu, Norman Mu, Jiantao Jiao, David Wagner. (2024)<br><strong>Generative AI Security: Challenges and Countermeasures</strong><br><button class=copy-to-clipboard title="Generative AI Security: Challenges and Countermeasures" index=233>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-233 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-AI, cs-CL, cs-CR, cs-CY, cs-LG, cs.CR<br>Keyword Score: 20<br>Keywords: Generative AI, AI Security<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12617v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12617v1.pdf filename=2402.12617v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Generative</b> <b>AI&rsquo;s</b> <b>expanding</b> footprint across numerous industries has led to both excitement and increased scrutiny. This paper delves into the unique security challenges posed by <b>Generative</b> <b>AI,</b> <b>and</b> outlines potential research directions for managing these risks.</p></p class="citation"></blockquote><h3 id=45--234316-apt-mmf-an-advanced-persistent-threat-actor-attribution-method-based-on-multimodal-and-multilevel-feature-fusion-nan-xiao-et-al-2024>(4/5 | 234/316) APT-MMF: An advanced persistent threat actor attribution method based on multimodal and multilevel feature fusion (Nan Xiao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nan Xiao, Bo Lang, Ting Wang, Yikai Chen. (2024)<br><strong>APT-MMF: An advanced persistent threat actor attribution method based on multimodal and multilevel feature fusion</strong><br><button class=copy-to-clipboard title="APT-MMF: An advanced persistent threat actor attribution method based on multimodal and multilevel feature fusion" index=234>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-234 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs-LG, cs.CR<br>Keyword Score: 19<br>Keywords: Graph Attention Networks, Graph, Multi-modal, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12743v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12743v1.pdf filename=2402.12743v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Threat actor attribution is a crucial defense strategy for combating advanced persistent threats (APTs). Cyber threat intelligence (CTI), which involves analyzing multisource heterogeneous data from APTs, plays an important role in APT actor attribution. The current attribution methods extract features from different CTI perspectives and employ machine learning models to classify CTI reports according to their threat actors. However, these methods usually extract only one kind of feature and ignore heterogeneous information, especially the attributes and relations of indicators of compromise (IOCs), which form the core of CTI. To address these problems, we propose an APT actor attribution method based on <b>multimodal</b> and multilevel feature fusion (APT-MMF). First, we leverage a heterogeneous attributed <b>graph</b> <b>to</b> <b>characterize</b> APT reports and their IOC information. Then, we extract and fuse <b>multimodal</b> features, including attribute type features, natural language text features and topological relationship features, to construct comprehensive node representations. Furthermore, we design multilevel heterogeneous <b>graph</b> <b>attention</b> <b>networks</b> to learn the deep hidden features of APT report nodes; these networks integrate IOC type-level, metapath-based neighbor node-level, and metapath semantic-level attention. Utilizing multisource threat intelligence, we construct a heterogeneous attributed <b>graph</b> <b>dataset</b> <b>for</b> verification purposes. The experimental results show that our method not only outperforms the existing methods but also demonstrates its good interpretability for attribution analysis tasks.</p></p class="citation"></blockquote><h3 id=55--235316-robust-wide-robust-watermarking-against-instruction-driven-image-editing-runyi-hu-et-al-2024>(5/5 | 235/316) Robust-Wide: Robust Watermarking against Instruction-driven Image Editing (Runyi Hu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Runyi Hu, Jie Zhang, Ting Xu, Tianwei Zhang, Jiwei Li. (2024)<br><strong>Robust-Wide: Robust Watermarking against Instruction-driven Image Editing</strong><br><button class=copy-to-clipboard title="Robust-Wide: Robust Watermarking against Instruction-driven Image Editing" index=235>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-235 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs.CR<br>Keyword Score: 10<br>Keywords: ControlNet<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12688v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12688v2.pdf filename=2402.12688v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Instruction-driven image editing allows users to quickly edit an image according to text instructions in a forward pass. Nevertheless, malicious users can easily exploit this technique to create fake images, which could cause a crisis of trust and harm the rights of the original image owners. Watermarking is a common solution to trace such malicious behavior. Unfortunately, instruction-driven image editing can significantly change the watermarked image at the semantic level, making it less robust and effective. We propose Robust-Wide, the first robust watermarking methodology against instruction-driven image editing. Specifically, we adopt the widely-used encoder-decoder framework for watermark embedding and extraction. To achieve robustness against semantic distortions, we introduce a novel Partial Instruction-driven Denoising Sampling Guidance (PIDSG) module, which consists of a large variety of instruction injections and substantial modifications of images at different semantic levels. With PIDSG, the encoder tends to embed the watermark into more robust and semantic-aware areas, which remains in existence even after severe image editing. Experiments demonstrate that Robust-Wide can effectively extract the watermark from the edited image with a low bit error rate of nearly 2.6% for 64-bit watermark messages. Meanwhile, it only induces a neglectable influence on the visual quality and editability of the original images. Moreover, Robust-Wide holds general robustness against different sampling configurations and other image editing methods such as <b>ControlNet-InstructPix2Pix,</b> MagicBrush, Inpainting and DDIM Inversion.</p></p class="citation"></blockquote><h2 id=eesssy-8>eess.SY (8)</h2><h3 id=18--236316-ascend-accurate-yet-efficient-end-to-end-stochastic-computing-acceleration-of-vision-transformer-tong-xie-et-al-2024>(1/8 | 236/316) ASCEND: Accurate yet Efficient End-to-End Stochastic Computing Acceleration of Vision Transformer (Tong Xie et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tong Xie, Yixuan Hu, Renjie Wei, Meng Li, Yuan Wang, Runsheng Wang, Ru Huang. (2024)<br><strong>ASCEND: Accurate yet Efficient End-to-End Stochastic Computing Acceleration of Vision Transformer</strong><br><button class=copy-to-clipboard title="ASCEND: Accurate yet Efficient End-to-End Stochastic Computing Acceleration of Vision Transformer" index=236>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-236 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 50<br>Keywords: Vision Transformer, Convolution, Convolutional Neural Network, Transformer, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12820v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12820v1.pdf filename=2402.12820v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Stochastic computing (SC) has emerged as a promising computing paradigm for neural acceleration. However, how to accelerate the state-of-the-art <b>Vision</b> <b>Transformer</b> (ViT) with SC remains unclear. Unlike <b>convolutional</b> <b>neural</b> <b>networks,</b> ViTs introduce notable compatibility and efficiency challenges because of their nonlinear functions, e.g., softmax and Gaussian Error Linear Units (GELU). In this paper, for the first time, a ViT accelerator based on end-to-end SC, dubbed ASCEND, is proposed. ASCEND co-designs the SC circuits and ViT networks to enable accurate yet efficient acceleration. To overcome the compatibility challenges, ASCEND proposes a novel deterministic SC block for GELU and leverages an SC-friendly iterative approximate algorithm to design an accurate and efficient softmax circuit. To improve inference efficiency, ASCEND develops a two-stage training pipeline to produce accurate low-precision ViTs. With extensive experiments, we show the proposed GELU and softmax blocks achieve 56.3% and 22.6% error reduction compared to existing SC designs, respectively and reduce the area-delay product (ADP) by 5.29x and 12.6x, respectively. Moreover, compared to the baseline low-precision ViTs, ASCEND also achieves significant accuracy improvements on CIFAR10 and CIFAR100.</p></p class="citation"></blockquote><h3 id=28--237316-electric-field-evaluation-of-reconfigurable-intelligent-surface-in-wireless-networks-zhuangzhuang-cui-et-al-2024>(2/8 | 237/316) Electric Field Evaluation of Reconfigurable Intelligent Surface in Wireless Networks (Zhuangzhuang Cui et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhuangzhuang Cui, Franco Minucci, Rizqi Hersyandika, Rodney Martinez Alonso, Andrea P. Guevara, Hazem Sallouha, Sofie Pollin. (2024)<br><strong>Electric Field Evaluation of Reconfigurable Intelligent Surface in Wireless Networks</strong><br><button class=copy-to-clipboard title="Electric Field Evaluation of Reconfigurable Intelligent Surface in Wireless Networks" index=237>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-237 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13132v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13132v1.pdf filename=2402.13132v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Reconfigurable intelligent surface (RIS) used as infrastructure in wireless networks has been a trend, thanks to its low cost and high flexibility. Working in many ways including reflective mirrors and phase-shifted surfaces, RIS is able to enhance the coverage in communications and provide more degrees of freedom for sensing. However, the key issue lies in how to place RIS in accordance with the regulations for electromagnetic field (EMF) exposure, which requires refined evaluations. In this paper, we first investigate the regulations in terms of E-field. Then, relevant deployment characteristics are evaluated jointly: the minimum distance from the base station (BS) to the RIS, and the minimum height of the RIS are given for a given BS power limit and as function of the number of RIS elements. The ray-tracing <b>simulations</b> verify the correctness of our analysis. Besides, different frequency ranges (FRs) and radiation patterns of RIS elements are investigated. The results show that the EMF exposure risk is negligible when RIS works in the reflective-only (RO) mode. However, when it works in the beamforming (BO) mode, its placement should be well specified based on our analytical framework to comply with the regulations of E-field limit in general public scenarios. Finally, we provide an E-field measurement methodology and low-cost solutions in terms of general wireless networks and 5G standalone networks, which pave the way for real-world evaluation in future work.</p></p class="citation"></blockquote><h3 id=38--238316-autoencoder-with-ordered-variance-for-nonlinear-model-identification-midhun-t-augustine-et-al-2024>(3/8 | 238/316) Autoencoder with Ordered Variance for Nonlinear Model Identification (Midhun T. Augustine et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Midhun T. Augustine, Parag Patil, Mani Bhushan, Sharad Bhartiya. (2024)<br><strong>Autoencoder with Ordered Variance for Nonlinear Model Identification</strong><br><button class=copy-to-clipboard title="Autoencoder with Ordered Variance for Nonlinear Model Identification" index=238>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-238 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-LG, cs-SY, eess-SY, eess.SY<br>Keyword Score: 20<br>Keywords: Autoencoder, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.14031v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.14031v1.pdf filename=2402.14031v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents a novel <b>autoencoder</b> with ordered variance (AEO) in which the loss function is modified with a variance regularization term to enforce order in the latent space. Further, the <b>autoencoder</b> is modified using ResNets, which results in a ResNet AEO (RAEO). The paper also illustrates the effectiveness of AEO and RAEO in extracting nonlinear relationships among input variables in an <b>unsupervised</b> setting.</p></p class="citation"></blockquote><h3 id=48--239316-gimbal-actuator-modeling-for-a-spin-stabilized-spacecraft-equipped-with-a-1dof-gimbaled-thruster-and-two-reaction-wheels-hamed-kouhi-et-al-2024>(4/8 | 239/316) Gimbal Actuator Modeling for a Spin-Stabilized Spacecraft Equipped with a 1DoF Gimbaled-Thruster and two Reaction Wheels (Hamed Kouhi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hamed Kouhi, Mansour Kabganian, Farhad Fani Saberi, Fatemeh Ghorbani. (2024)<br><strong>Gimbal Actuator Modeling for a Spin-Stabilized Spacecraft Equipped with a 1DoF Gimbaled-Thruster and two Reaction Wheels</strong><br><button class=copy-to-clipboard title="Gimbal Actuator Modeling for a Spin-Stabilized Spacecraft Equipped with a 1DoF Gimbaled-Thruster and two Reaction Wheels" index=239>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-239 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12922v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12922v1.pdf filename=2402.12922v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Attitude control of spacecraft during an impulsive orbital maneuver is a vital task. Many spacecraft and launchers use the gimbaled thrust vector control (TVC) in their attitude control system during an orbital maneuver. Mathematical modeling of the gimbal actuator is an important task because we should show the applicability of the gimbaled-TVC in a spacecraft. In this paper, a spin-stabilized spacecraft equipped with one degree of freedom (DoF) gimbaled-thruster and two reaction wheels (RWs) is considered. The control goals are disturbance rejection and thrust vector (spin-axis) stabilization based on one DoF gimbal actuator and two RWs. The gimbal is assumed to be equipped with a gearbox and a DC electric motor. This actuator must supply the gimbal torque to rotate the spacecraft nozzle. The mathematical model of the mentioned spacecraft is extended with respect to the DC motor equations. In order to investigate the applicability of the proposed method, an industrial DC electric motor is considered for the gimbal actuator. The <b>simulation</b> results prove that an industrial DC electric motor is able to be used for attitude control of the mentioned spacecraft. The <b>simulation</b> results indicate the applicability of the proposed control method in an impulsive orbital maneuver.</p></p class="citation"></blockquote><h3 id=58--240316-smart-mobility-digital-twin-based-automated-vehicle-navigation-system-a-proof-of-concept-kui-wang-et-al-2024>(5/8 | 240/316) Smart Mobility Digital Twin Based Automated Vehicle Navigation System: A Proof of Concept (Kui Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kui Wang, Zongdian Li, Kazuma Nonomura, Tao Yu, Kei Sakaguchi, Omar Hashash, Walid Saad. (2024)<br><strong>Smart Mobility Digital Twin Based Automated Vehicle Navigation System: A Proof of Concept</strong><br><button class=copy-to-clipboard title="Smart Mobility Digital Twin Based Automated Vehicle Navigation System: A Proof of Concept" index=240>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-240 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12682v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12682v1.pdf filename=2402.12682v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Digital twins (DTs) have driven major advancements across various industrial domains over the past two decades. With the rapid advancements in autonomous driving and vehicle-to-everything (V2X) technologies, integrating DTs into vehicular platforms is anticipated to further revolutionize smart mobility systems. In this paper, a new smart mobility DT (SMDT) platform is proposed for the control of connected and automated vehicles (CAVs) over next-generation wireless networks. In particular, the proposed platform enables cloud services to leverage the abilities of DTs to promote the autonomous driving experience. To enhance traffic efficiency and road safety measures, a novel navigation system that exploits available DT information is designed. The SMDT platform and navigation system are implemented with state-of-the-art products, e.g., CAVs and roadside units (RSUs), and emerging technologies, e.g., cloud and cellular V2X (C-V2X). In addition, proof-of-concept (PoC) experiments are conducted to validate system performance. The performance of SMDT is evaluated from two standpoints: (i) the rewards of the proposed navigation system on traffic efficiency and safety and, (ii) the latency and reliability of the SMDT platform. Our experimental results using SUMO-based large-scale traffic <b>simulations</b> show that the proposed SMDT can reduce the average travel time and the blocking probability due to unexpected traffic incidents. Furthermore, the results record a peak overall latency for DT modeling and route planning services to be 155.15 ms and 810.59 ms, respectively, which validates that our proposed design aligns with the 3GPP requirements for emerging V2X use cases and fulfills the targets of the proposed design. Our demonstration video can be found at <a href=https://youtu.be/3waQwlaHQkk>https://youtu.be/3waQwlaHQkk</a>.</p></p class="citation"></blockquote><h3 id=68--241316-robust-model-predictive-control-for-nonlinear-discrete-time-systems-using-iterative-time-varying-constraint-tightening-daniel-d-leister-et-al-2024>(6/8 | 241/316) Robust Model Predictive Control for nonlinear discrete-time systems using iterative time-varying constraint tightening (Daniel D. Leister et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Daniel D. Leister, Justin P. Koeln. (2024)<br><strong>Robust Model Predictive Control for nonlinear discrete-time systems using iterative time-varying constraint tightening</strong><br><button class=copy-to-clipboard title="Robust Model Predictive Control for nonlinear discrete-time systems using iterative time-varying constraint tightening" index=241>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-241 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 10<br>Keywords: Discrete Time, Discrete Time<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13183v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13183v1.pdf filename=2402.13183v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Robust Model Predictive Control (MPC) for nonlinear systems is a problem that poses significant challenges as highlighted by the diversity of approaches proposed in the last decades. Often compromises with respect to computational load, conservatism, generality, or implementation complexity have to be made, and finding an approach that provides the right balance is still a challenge to the research community. This work provides a contribution by proposing a novel shrinking-horizon robust MPC formulation for nonlinear <b>discrete-time</b> <b>systems.</b> By explicitly accounting for how disturbances and linearization errors are propagated through the nonlinear dynamics, a constraint tightening-based formulation is obtained, with guarantees of robust constraint satisfaction. The proposed controller relies on iteratively solving a Nonlinear Program (NLP) to simultaneously optimize system operation and the required constraint tightening. Numerical experiments show the effectiveness of the proposed controller with three different choices of NLP solvers as well as significantly improved computational speed, better scalability, and generally reduced conservatism when compared to an existing technique from the literature.</p></p class="citation"></blockquote><h3 id=78--242316-formal-synthesis-of-controllers-for-safety-critical-autonomous-systems-developments-and-challenges-xiang-yin-et-al-2024>(7/8 | 242/316) Formal Synthesis of Controllers for Safety-Critical Autonomous Systems: Developments and Challenges (Xiang Yin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiang Yin, Bingzhao Gao, Xiao Yu. (2024)<br><strong>Formal Synthesis of Controllers for Safety-Critical Autonomous Systems: Developments and Challenges</strong><br><button class=copy-to-clipboard title="Formal Synthesis of Controllers for Safety-Critical Autonomous Systems: Developments and Challenges" index=242>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-242 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-RO, cs-SY, eess-SY, eess.SY<br>Keyword Score: 10<br>Keywords: Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13075v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13075v1.pdf filename=2402.13075v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In recent years, formal methods have been extensively used in the design of autonomous systems. By employing mathematically rigorous techniques, formal methods can provide fully automated <b>reasoning</b> processes with provable safety guarantees for complex dynamic systems with intricate interactions between continuous dynamics and discrete logics. This paper provides a comprehensive review of formal controller synthesis techniques for safety-critical autonomous systems. Specifically, we categorize the formal control synthesis problem based on diverse system models, encompassing deterministic, non-deterministic, and stochastic, and various formal safety-critical specifications involving logic, real-time, and real-valued domains. The review covers fundamental formal control synthesis techniques, including abstraction-based approaches and abstraction-free methods. We explore the integration of data-driven synthesis approaches in formal control synthesis. Furthermore, we review formal techniques tailored for multi-agent systems (MAS), with a specific focus on various approaches to address the scalability challenges in large-scale systems. Finally, we discuss some recent trends and highlight research challenges in this area.</p></p class="citation"></blockquote><h3 id=88--243316-antifragile-perimeter-control-anticipating-and-gaining-from-disruptions-with-reinforcement-learning-linghang-sun-et-al-2024>(8/8 | 243/316) Antifragile Perimeter Control: Anticipating and Gaining from Disruptions with Reinforcement Learning (Linghang Sun et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Linghang Sun, Michail A. Makridis, Alexander Genser, Cristian Axenie, Margherita Grossi, Anastasios Kouvelas. (2024)<br><strong>Antifragile Perimeter Control: Anticipating and Gaining from Disruptions with Reinforcement Learning</strong><br><button class=copy-to-clipboard title="Antifragile Perimeter Control: Anticipating and Gaining from Disruptions with Reinforcement Learning" index=243>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-243 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12665v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12665v1.pdf filename=2402.12665v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The optimal operation of transportation networks is often susceptible to unexpected disruptions, such as traffic incidents and social events. Many established control strategies rely on mathematical models that struggle to cope with real-world uncertainties, leading to a significant decline in effectiveness when faced with substantial disruptions. While previous research works have dedicated efforts to improving the robustness or resilience of transportation systems against disruptions, this paper applies the cutting-edge concept of antifragility to better design a traffic control strategy for urban road networks. Antifragility sets itself apart from robustness and resilience as it represents a system&rsquo;s ability to not only withstand stressors, shocks, and volatility but also thrive and enhance performance in the presence of such adversarial events. Hence, modern transportation systems call for solutions that are antifragile. In this work, we propose a model-free deep <b>Reinforcement</b> <b>Learning</b> (RL) scheme to control a two-region urban traffic perimeter network. The system exploits the learning capability of RL under disruptions to achieve antifragility. By monitoring the change rate and curvature of the traffic state with the RL framework, the proposed algorithm anticipates imminent disruptions. An additional term is also integrated into the RL algorithm as redundancy to improve the performance under disruption scenarios. When compared to a state-of-the-art model predictive control approach and a state-of-the-art RL algorithm, our proposed method demonstrates two antifragility-related properties: (a) gradual performance improvement under disruptions of constant magnitude; and (b) increasingly superior performance under growing disruptions.</p></p class="citation"></blockquote><h2 id=q-fincp-1>q-fin.CP (1)</h2><h3 id=11--244316-deep-hedging-with-market-impact-andrei-neagu-et-al-2024>(1/1 | 244/316) Deep Hedging with Market Impact (Andrei Neagu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Andrei Neagu, Frédéric Godin, Clarence Simard, Leila Kosseim. (2024)<br><strong>Deep Hedging with Market Impact</strong><br><button class=copy-to-clipboard title="Deep Hedging with Market Impact" index=244>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-244 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: q-fin.CP<br>Categories: cs-AI, q-fin-CP, q-fin.CP<br>Keyword Score: 40<br>Keywords: Reinforcement Learning, Simulation, Simulator, In-context Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13326v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13326v2.pdf filename=2402.13326v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Dynamic hedging is the practice of periodically transacting financial instruments to offset the risk caused by an investment or a liability. Dynamic hedging optimization can be framed as a sequential decision problem; thus, <b>Reinforcement</b> <b>Learning</b> (RL) models were recently proposed to tackle this task. However, existing RL works for hedging do not consider market impact caused by the finite liquidity of traded instruments. Integrating such feature can be crucial to achieve optimal performance when hedging options on stocks with limited liquidity. In this paper, we propose a novel general market impact dynamic hedging model based on Deep <b>Reinforcement</b> <b>Learning</b> (DRL) that considers several realistic features such as convex market impacts, and impact persistence through time. The optimal policy obtained from the DRL model is analysed using several option hedging <b>simulations</b> and compared to commonly used procedures such as delta hedging. Results show our DRL model behaves better in contexts of low liquidity by, among others: 1) learning the extent to which portfolio rebalancing actions should be dampened or delayed to avoid high costs, 2) factoring in the impact of features not considered by conventional approaches, such as previous hedging errors through the portfolio value, and the underlying asset&rsquo;s drift (i.e. the magnitude of its expected return).</p></p class="citation"></blockquote><h2 id=csgr-2>cs.GR (2)</h2><h3 id=12--245316-flashtex-fast-relightable-mesh-texturing-with-lightcontrolnet-kangle-deng-et-al-2024>(1/2 | 245/316) FlashTex: Fast Relightable Mesh Texturing with LightControlNet (Kangle Deng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kangle Deng, Timothy Omernick, Alexander Weiss, Deva Ramanan, Jun-Yan Zhu, Tinghui Zhou, Maneesh Agrawala. (2024)<br><strong>FlashTex: Fast Relightable Mesh Texturing with LightControlNet</strong><br><button class=copy-to-clipboard title="FlashTex: Fast Relightable Mesh Texturing with LightControlNet" index=245>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-245 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.GR<br>Categories: cs-CV, cs-GR, cs-LG, cs.GR<br>Keyword Score: 40<br>Keywords: ControlNet, Knowledge Distillation, Text2image, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13251v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13251v1.pdf filename=2402.13251v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Manually creating textures for 3D meshes is time-consuming, even for expert visual content creators. We propose a fast approach for automatically texturing an input 3D mesh based on a user-provided text <b>prompt.</b> Importantly, our approach disentangles lighting from surface material/reflectance in the resulting texture so that the mesh can be properly relit and rendered in any lighting environment. We introduce LightControlNet, a new <b>text-to-image</b> model based on the <b>ControlNet</b> architecture, which allows the specification of the desired lighting as a conditioning image to the model. Our text-to-texture pipeline then constructs the texture in two stages. The first stage produces a sparse set of visually consistent reference views of the mesh using LightControlNet. The second stage applies a texture optimization based on Score <b>Distillation</b> Sampling (SDS) that works with LightControlNet to increase the texture quality while disentangling surface material from lighting. Our pipeline is significantly faster than previous text-to-texture methods, while producing high-quality and relightable textures.</p></p class="citation"></blockquote><h3 id=22--246316-real-time-high-resolution-view-synthesis-of-complex-scenes-with-explicit-3d-visibility-reasoning-tiansong-zhou-et-al-2024>(2/2 | 246/316) Real-time High-resolution View Synthesis of Complex Scenes with Explicit 3D Visibility Reasoning (Tiansong Zhou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tiansong Zhou, Yebin Liu, Xuangeng Chu, Chengkun Cao, Changyin Zhou, Fei Yu, Yu Li. (2024)<br><strong>Real-time High-resolution View Synthesis of Complex Scenes with Explicit 3D Visibility Reasoning</strong><br><button class=copy-to-clipboard title="Real-time High-resolution View Synthesis of Complex Scenes with Explicit 3D Visibility Reasoning" index=246>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-246 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.GR<br>Categories: cs-GR, cs.GR<br>Keyword Score: 15<br>Keywords: Geometry, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12886v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12886v1.pdf filename=2402.12886v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Rendering photo-realistic novel-view images of complex scenes has been a long-standing challenge in computer graphics. In recent years, great research progress has been made on enhancing rendering quality and accelerating rendering speed in the realm of view synthesis. However, when rendering complex dynamic scenes with sparse views, the rendering quality remains limited due to occlusion problems. Besides, for rendering high-resolution images on dynamic scenes, the rendering speed is still far from real-time. In this work, we propose a generalizable view synthesis method that can render high-resolution novel-view images of complex static and dynamic scenes in real-time from sparse views. To address the occlusion problems arising from the sparsity of input views and the complexity of captured scenes, we introduce an explicit 3D visibility <b>reasoning</b> approach that can efficiently estimate the visibility of sampled 3D points to the input views. The proposed visibility <b>reasoning</b> approach is fully differentiable and can gracefully fit inside the volume rendering pipeline, allowing us to train our networks with only multi-view images as supervision while refining <b>geometry</b> and texture simultaneously. Besides, each module in our pipeline is carefully designed to bypass the time-consuming MLP querying process and enhance the rendering quality of high-resolution images, enabling us to render high-resolution novel-view images in real-time.Experimental results show that our method outperforms previous view synthesis methods in both rendering quality and speed, particularly when dealing with complex dynamic scenes with sparse views.</p></p class="citation"></blockquote><h2 id=csdc-2>cs.DC (2)</h2><h3 id=12--247316-energy-efficient-wireless-federated-learning-via-doubly-adaptive-quantization-xuefeng-han-et-al-2024>(1/2 | 247/316) Energy-Efficient Wireless Federated Learning via Doubly Adaptive Quantization (Xuefeng Han et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xuefeng Han, Wen Chen, Jun Li, Ming Ding, Qingqing Wu, Kang Wei, Xiumei Deng, Zhen Mei. (2024)<br><strong>Energy-Efficient Wireless Federated Learning via Doubly Adaptive Quantization</strong><br><button class=copy-to-clipboard title="Energy-Efficient Wireless Federated Learning via Doubly Adaptive Quantization" index=247>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-247 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DC<br>Categories: cs-DC, cs.DC<br>Keyword Score: 40<br>Keywords: Federated Learning, Karush-Kuhn-Tucker, Model Quantization, Quantization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12957v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12957v1.pdf filename=2402.12957v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Federated</b> <b>learning</b> (FL) has been recognized as a viable distributed learning paradigm for training a machine learning <b>model</b> <b>across</b> distributed clients without uploading raw data. However, FL in wireless networks still faces two major challenges, i.e., large communication overhead and high energy consumption, which are exacerbated by client heterogeneity in dataset sizes and wireless channels. While <b>model</b> <b>quantization</b> is effective for energy reduction, existing works ignore adapting <b>quantization</b> to heterogeneous clients and FL convergence. To address these challenges, this paper develops an energy optimization problem of jointly designing <b>quantization</b> levels, scheduling clients, allocating channels, and controlling computation frequencies (QCCF) in wireless FL. Specifically, we derive an upper bound identifying the influence of client scheduling and <b>quantization</b> errors on FL convergence. Under the longterm convergence constraints and wireless constraints, the problem is established and transformed into an instantaneous problem with Lyapunov optimization. Solving <b>Karush-Kuhn-Tucker</b> conditions, our closed-form solution indicates that the doubly adaptive <b>quantization</b> level rises with the training process and correlates negatively with dataset sizes. Experiment results validate our theoretical results, showing that QCCF consumes less energy with faster convergence compared with state-of-the-art baselines.</p></p class="citation"></blockquote><h3 id=22--248316-fog-enabled-distributed-training-architecture-for-federated-learning-aditya-kumar-et-al-2024>(2/2 | 248/316) Fog enabled distributed training architecture for federated learning (Aditya Kumar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Aditya Kumar, Satish Narayana Srirama. (2024)<br><strong>Fog enabled distributed training architecture for federated learning</strong><br><button class=copy-to-clipboard title="Fog enabled distributed training architecture for federated learning" index=248>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-248 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DC<br>Categories: cs-DC, cs.DC<br>Keyword Score: 10<br>Keywords: Federated Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12906v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12906v1.pdf filename=2402.12906v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The amount of data being produced at every epoch of second is increasing every moment. Various sensors, cameras and smart gadgets produce continuous data throughout its installation. Processing and analyzing raw data at a cloud server faces several challenges such as bandwidth, congestion, latency, privacy and security. Fog computing brings computational resources closer to IoT that addresses some of these issues. These IoT devices have low computational capability, which is insufficient to train machine learning. Mining hidden patterns and inferential rules from continuously growing data is crucial for various applications. Due to growing privacy concerns, privacy preserving machine learning is another aspect that needs to be inculcated. In this paper, we have proposed a fog enabled distributed training architecture for machine learning tasks using resources constrained devices. The proposed architecture trains machine learning model on rapidly changing data using online learning. The network is inlined with privacy preserving <b>federated</b> <b>learning</b> training. Further, the learning capability of architecture is tested on a real world IIoT use case. We trained a neural network model for human position detection in IIoT setup on rapidly changing data.</p></p class="citation"></blockquote><h2 id=csse-8>cs.SE (8)</h2><h3 id=18--249316-advancing-genai-assisted-programming--a-comparative-study-on-prompt-efficiency-and-code-quality-between-gpt-4-and-glm-4-angus-yang-et-al-2024>(1/8 | 249/316) Advancing GenAI Assisted Programming&ndash;A Comparative Study on Prompt Efficiency and Code Quality Between GPT-4 and GLM-4 (Angus Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Angus Yang, Zehan Li, Jie Li. (2024)<br><strong>Advancing GenAI Assisted Programming&ndash;A Comparative Study on Prompt Efficiency and Code Quality Between GPT-4 and GLM-4</strong><br><button class=copy-to-clipboard title="Advancing GenAI Assisted Programming--A Comparative Study on Prompt Efficiency and Code Quality Between GPT-4 and GLM-4" index=249>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-249 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: D-2-3, cs-AI, cs-SE, cs.SE<br>Keyword Score: 40<br>Keywords: GPT, GPT-4, Code Generation, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12782v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12782v1.pdf filename=2402.12782v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This study aims to explore the best practices for utilizing GenAI as a programming tool, through a comparative analysis between <b>GPT-4</b> and GLM-4. By evaluating <b>prompting</b> strategies at different levels of complexity, we identify that simplest and straightforward <b>prompting</b> strategy yields best <b>code</b> <b>generation</b> results. Additionally, adding a CoT-like preliminary confirmation step would further increase the success rate. Our results reveal that while <b>GPT-4</b> marginally outperforms GLM-4, the difference is minimal for average users. In our simplified evaluation model, we see a remarkable 30 to 100-fold increase in <b>code</b> <b>generation</b> efficiency over traditional coding norms. Our GenAI Coding Workshop highlights the effectiveness and accessibility of the <b>prompting</b> methodology developed in this study. We observe that GenAI-assisted coding would trigger a paradigm shift in programming landscape, which necessitates developers to take on new roles revolving around supervising and guiding GenAI, and to focus more on setting high-level objectives and engaging more towards innovation.</p></p class="citation"></blockquote><h3 id=28--250316-go-static-contextualized-logging-statement-generation-yichen-li-et-al-2024>(2/8 | 250/316) Go Static: Contextualized Logging Statement Generation (Yichen Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yichen Li, Yintong Huo, Renyi Zhong, Zhihan Jiang, Jinyang Liu, Junjie Huang, Jiazhen Gu, Pinjia He, Michael R. Lyu. (2024)<br><strong>Go Static: Contextualized Logging Statement Generation</strong><br><button class=copy-to-clipboard title="Go Static: Contextualized Logging Statement Generation" index=250>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-250 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-SE, cs.SE<br>Keyword Score: 30<br>Keywords: BLEU, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12958v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12958v1.pdf filename=2402.12958v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Logging practices have been extensively investigated to assist developers in writing appropriate logging statements for documenting software behaviors. Although numerous automatic logging approaches have been proposed, their performance remains unsatisfactory due to the constraint of the single-method input, without informative programming context outside the method. Specifically, we identify three inherent limitations with single-method context: limited static scope of logging statements, inconsistent logging styles, and missing type information of logging variables. To tackle these limitations, we propose SCLogger, the first contextualized logging statement generation approach with inter-method static contexts. First, SCLogger extracts inter-method contexts with static analysis to construct the contextualized <b>prompt</b> for language models to generate a tentative logging statement. The contextualized <b>prompt</b> consists of an extended static scope and sampled similar methods, ordered by the chain-of-thought (COT) strategy. Second, SCLogger refines the access of logging variables by formulating a new refinement <b>prompt</b> for language models, which incorporates detailed type information of variables in the tentative logging statement. The evaluation results show that SCLogger surpasses the state-of-the-art approach by 8.7% in logging position accuracy, 32.1% in level accuracy, 19.6% in variable precision, and 138.4% in text <b>BLEU-4</b> score. Furthermore, SCLogger consistently boosts the performance of logging statement generation across a range of <b>large</b> <b>language</b> <b>models,</b> thereby showcasing the generalizability of this approach.</p></p class="citation"></blockquote><h3 id=38--251316-a-disruptive-research-playbook-for-studying-disruptive-innovations-margaret-anne-storey-et-al-2024>(3/8 | 251/316) A Disruptive Research Playbook for Studying Disruptive Innovations (Margaret-Anne Storey et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Margaret-Anne Storey, Daniel Russo, Nicole Novielli, Takashi Kobayashi, Dong Wang. (2024)<br><strong>A Disruptive Research Playbook for Studying Disruptive Innovations</strong><br><button class=copy-to-clipboard title="A Disruptive Research Playbook for Studying Disruptive Innovations" index=251>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-251 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-SE, cs.SE<br>Keyword Score: 20<br>Keywords: Generative AI, GPT<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13329v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13329v1.pdf filename=2402.13329v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>As researchers, we are now witnessing a fundamental change in our technologically-enabled world due to the advent and diffusion of highly disruptive technologies such as <b>generative</b> <b>AI,</b> Augmented Reality (AR) and Virtual Reality (VR). In particular, software engineering has been profoundly affected by the transformative power of disruptive innovations for decades, with a significant impact of technical advancements on social dynamics due to its the socio-technical nature. In this paper, we reflect on the importance of formulating and addressing research in software engineering through a socio-technical lens, thus ensuring a holistic understanding of the complex phenomena in this field. We propose a research playbook with the goal of providing a guide to formulate compelling and socially relevant research questions and to identify the appropriate research strategies for empirical investigations, with an eye on the long-term implications of technologies or their use. We showcase how to apply the research playbook. Firstly, we show how it can be used retrospectively to reflect on a prior disruptive technology, Stack Overflow, and its impact on software development. Secondly, we show it can be used to question the impact of two current disruptive technologies: AI and AR/VR. Finally, we introduce a specialized <b>GPT</b> model to support the researcher in framing future investigations. We conclude by discussing the broader implications of adopting the playbook for both researchers and practitioners in software engineering and beyond.</p></p class="citation"></blockquote><h3 id=48--252316-multi-level-ml-based-burst-aware-autoscaling-for-slo-assurance-and-cost-efficiency-chunyang-meng-et-al-2024>(4/8 | 252/316) Multi-Level ML Based Burst-Aware Autoscaling for SLO Assurance and Cost Efficiency (Chunyang Meng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chunyang Meng, Haogang Tong, Tianyang Wu, Maolin Pan, Yang Yu. (2024)<br><strong>Multi-Level ML Based Burst-Aware Autoscaling for SLO Assurance and Cost Efficiency</strong><br><button class=copy-to-clipboard title="Multi-Level ML Based Burst-Aware Autoscaling for SLO Assurance and Cost Efficiency" index=252>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-252 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-SE, cs.SE<br>Keyword Score: 20<br>Keywords: Human Intervention, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12962v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12962v1.pdf filename=2402.12962v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Autoscaling is a technology to automatically scale the resources provided to their applications without <b>human</b> <b>intervention</b> to guarantee runtime Quality of Service (QoS) while saving costs. However, user-facing cloud applications serve dynamic workloads that often exhibit variable and contain bursts, posing challenges to autoscaling for maintaining QoS within Service-Level Objectives (SLOs). Conservative strategies risk over-provisioning, while aggressive ones may cause SLO violations, making it more challenging to design effective autoscaling. This paper introduces BAScaler, a Burst-Aware Autoscaling framework for containerized cloud services or applications under complex workloads, combining multi-level machine learning (ML) techniques to mitigate SLO violations while saving costs. BAScaler incorporates a novel prediction-based burst detection mechanism that distinguishes between predictable periodic workload spikes and actual bursts. When bursts are detected, BAScaler appropriately overestimates them and allocates resources accordingly to address the rapid growth in resource demand. On the other hand, BAScaler employs <b>reinforcement</b> <b>learning</b> to rectify potential inaccuracies in resource estimation, enabling more precise resource allocation during non-bursts. Experiments across ten real-world workloads demonstrate BAScaler&rsquo;s effectiveness, achieving a 57% average reduction in SLO violations and cutting resource costs by 10% compared to other prominent methods.</p></p class="citation"></blockquote><h3 id=58--253316-measuring-impacts-of-poisoning-on-model-parameters-and-neuron-activations-a-case-study-of-poisoning-codebert-aftab-hussain-et-al-2024>(5/8 | 253/316) Measuring Impacts of Poisoning on Model Parameters and Neuron Activations: A Case Study of Poisoning CodeBERT (Aftab Hussain et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Aftab Hussain, Md Rafiqul Islam Rabin, Navid Ayoobi, Mohammad Amin Alipour. (2024)<br><strong>Measuring Impacts of Poisoning on Model Parameters and Neuron Activations: A Case Study of Poisoning CodeBERT</strong><br><button class=copy-to-clipboard title="Measuring Impacts of Poisoning on Model Parameters and Neuron Activations: A Case Study of Poisoning CodeBERT" index=253>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-253 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-SE, cs.SE<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12936v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12936v1.pdf filename=2402.12936v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have revolutionized software development practices, yet concerns about their safety have arisen, particularly regarding hidden backdoors, aka trojans. Backdoor attacks involve the insertion of triggers into training data, allowing attackers to manipulate the behavior of the model maliciously. In this paper, we focus on analyzing the model parameters to detect potential backdoor signals in code models. Specifically, we examine attention weights and biases, activation values, and context embeddings of the clean and poisoned CodeBERT models. Our results suggest noticeable patterns in activation values and context embeddings of poisoned samples for the poisoned CodeBERT model; however, attention weights and biases do not show any significant differences. This work contributes to ongoing efforts in white-box detection of backdoor signals in <b>LLMs</b> of code through the analysis of parameters and activations.</p></p class="citation"></blockquote><h3 id=68--254316-towards-mlops-a-devops-tools-recommender-system-for-machine-learning-system-pir-sami-ullah-shah-et-al-2024>(6/8 | 254/316) Towards MLOps: A DevOps Tools Recommender System for Machine Learning System (Pir Sami Ullah Shah et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Pir Sami Ullah Shah, Naveed Ahmad, Mirza Omer Beg. (2024)<br><strong>Towards MLOps: A DevOps Tools Recommender System for Machine Learning System</strong><br><button class=copy-to-clipboard title="Towards MLOps: A DevOps Tools Recommender System for Machine Learning System" index=254>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-254 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-LG, cs-SE, cs.SE<br>Keyword Score: 20<br>Keywords: Recommendation, Recommender System<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12867v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12867v1.pdf filename=2402.12867v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Applying DevOps practices to machine learning system is termed as MLOps and machine learning systems evolve on new data unlike traditional systems on requirements. The objective of MLOps is to establish a connection between different open-source tools to construct a pipeline that can automatically perform steps to construct a dataset, train the machine learning model and deploy the model to the production as well as store different versions of model and dataset. Benefits of MLOps is to make sure the fast delivery of the new trained models to the production to have accurate results. Furthermore, MLOps practice impacts the overall quality of the software products and is completely dependent on open-source tools and selection of relevant open-source tools is considered as challenged while a generalized method to select an appropriate open-source tools is desirable. In this paper, we present a framework for <b>recommendation</b> system that processes the contextual information (e.g., nature of data, type of the data) of the machine learning project and recommends a relevant toolchain (tech-stack) for the operationalization of machine learning systems. To check the applicability of the proposed framework, four different approaches i.e., rule-based, random forest, decision trees and k-nearest neighbors were investigated where precision, recall and f-score is measured, the random forest out classed other approaches with highest f-score value of 0.66.</p></p class="citation"></blockquote><h3 id=78--255316-scaling-laws-behind-code-understanding-model-jiayi-lin-et-al-2024>(7/8 | 255/316) Scaling Laws Behind Code Understanding Model (Jiayi Lin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiayi Lin, Hande Dong, Yutao Xie, Lei Zhang. (2024)<br><strong>Scaling Laws Behind Code Understanding Model</strong><br><button class=copy-to-clipboard title="Scaling Laws Behind Code Understanding Model" index=255>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-255 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-SE, cs.SE<br>Keyword Score: 20<br>Keywords: Large Language Model, Scaling Law<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12813v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12813v1.pdf filename=2402.12813v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The <b>scaling</b> <b>law</b> is becoming a fundamental law in many machine learning areas. That is, test error falls off with the power law when increasing training data, model size, and computing resource. However, whether this law is suitable for the task of code understanding is not well studied, and most current language models for code understanding are about 100M parameters, which are relatively &ldquo;small&rdquo; compared to <b>large</b> <b>language</b> <b>models.</b> In this paper, we conduct extensive experiments to investigate the <b>scaling</b> <b>law</b> for the code understanding task by varying training data, model size, and computing resource. We validate that the test error of code understanding models falls off with the power law when using larger models, indicating that the <b>scaling</b> <b>law</b> is suitable for the code understanding task. Besides, we apply different scales of models to two downstream code understanding tasks, and find that the performance increases with larger scale of models. Finally, we train a <b>large-scale</b> <b>code</b> <b>understanding</b> model named CoLSBERT with 1.5B parameters on a <b>large</b> <b>dataset</b> <b>using</b> more computing resource, which outperforms previous work by a <b>large</b> <b>margin.</b> <b>We</b> will release our code and the CoLSBERT model when our paper is published.</p></p class="citation"></blockquote><h3 id=88--256316-choosing-a-suitable-requirement-prioritization-method-a-survey-esraa-alhenawi-et-al-2024>(8/8 | 256/316) Choosing a Suitable Requirement Prioritization Method: A Survey (Esraa Alhenawi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Esraa Alhenawi, Shatha Awawdeh, Ruba Abu Khurma, Maribel García-Arenas, Pedro A. Castillo, Amjad Hudaib. (2024)<br><strong>Choosing a Suitable Requirement Prioritization Method: A Survey</strong><br><button class=copy-to-clipboard title="Choosing a Suitable Requirement Prioritization Method: A Survey" index=256>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-256 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-SE, cs.SE<br>Keyword Score: 10<br>Keywords: Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13149v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13149v1.pdf filename=2402.13149v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Software requirements prioritization plays a crucial role in software development. It can be viewed as the process of ordering requirements by determining which requirements must be done first and which can be done later. Powerful requirements prioritization techniques are of paramount importance to finish the implementation on time and within budget. Many factors affect requirement prioritization such as stakeholder expectations, complexity, dependency, scalability, risk, and cost. Therefore, finding the proper order of requirements is a challenging process. Hence, different types of requirements prioritization techniques have been developed to support this task. In this survey, we propose a novel classification that can classify the prioritization techniques under two major classes: relative and exact prioritization techniques class, where each class is divided into two subclasses. We depend in our classification on the way the value of ranking is given to the requirement, either explicitly as a specific value in the case of the exact prioritization techniques class, or implicitly in the case of the Relative prioritization technique class. An overview of fifteen different requirements prioritization techniques are presented and organized according to the proposed classification criteria&rsquo;s. Moreover, we make a comparison between methods that are related to the same subclass to analyze their strengths and weaknesses. Based on the comparison results, the properties for each proposed subclass of techniques are identified. Depending on these properties, we present some <b>recommendations</b> to help project managers in the process of selecting the most suitable technique to prioritize requirements based on their project characteristics (number of requirements, time, cost, and accuracy).</p></p class="citation"></blockquote><h2 id=statme-1>stat.ME (1)</h2><h3 id=11--257316-integrating-active-learning-in-causal-inference-with-interference-a-novel-approach-in-online-experiments-hongtao-zhu-et-al-2024>(1/1 | 257/316) Integrating Active Learning in Causal Inference with Interference: A Novel Approach in Online Experiments (Hongtao Zhu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hongtao Zhu, Sizhe Zhang, Yang Su, Zhenyu Zhao, Nan Chen. (2024)<br><strong>Integrating Active Learning in Causal Inference with Interference: A Novel Approach in Online Experiments</strong><br><button class=copy-to-clipboard title="Integrating Active Learning in Causal Inference with Interference: A Novel Approach in Online Experiments" index=257>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-257 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ME<br>Categories: cs-LG, stat-ME, stat-ML, stat.ME<br>Keyword Score: 40<br>Keywords: Active Learning, Gaussian Process, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12710v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12710v1.pdf filename=2402.12710v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the domain of causal inference research, the prevalent potential outcomes framework, notably the Rubin Causal Model (RCM), often overlooks individual interference and assumes independent treatment effects. This assumption, however, is frequently misaligned with the intricate realities of real-world scenarios, where interference is not merely a possibility but a common occurrence. Our research endeavors to address this discrepancy by focusing on the estimation of direct and spillover treatment effects under two assumptions: (1) network-based interference, where treatments on neighbors within connected networks affect one&rsquo;s outcomes, and (2) non-random treatment assignments influenced by confounders. To improve the efficiency of estimating potentially complex effects functions, we introduce an novel <b>active</b> <b>learning</b> approach: <b>Active</b> <b>Learning</b> in Causal Inference with Interference (ACI). This approach uses <b>Gaussian</b> <b>process</b> to flexibly model the direct and spillover treatment effects as a function of a continuous measure of neighbors&rsquo; treatment assignment. The ACI framework sequentially identifies the experimental settings that demand further data. It further optimizes the treatment assignments under the network interference structure using genetic algorithms to achieve efficient learning outcome. By applying our method to <b>simulation</b> data and a Tencent game dataset, we demonstrate its feasibility in achieving accurate effects estimations with reduced data requirements. This ACI approach marks a significant advancement in the realm of data efficiency for causal inference, offering a robust and efficient alternative to traditional methodologies, particularly in scenarios characterized by complex interference patterns.</p></p class="citation"></blockquote><h2 id=mathna-4>math.NA (4)</h2><h3 id=14--258316-pdeformer-towards-a-foundation-model-for-one-dimensional-partial-differential-equations-zhanhong-ye-et-al-2024>(1/4 | 258/316) PDEformer: Towards a Foundation Model for One-Dimensional Partial Differential Equations (Zhanhong Ye et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhanhong Ye, Xiang Huang, Leheng Chen, Hongsheng Liu, Zidong Wang, Bin Dong. (2024)<br><strong>PDEformer: Towards a Foundation Model for One-Dimensional Partial Differential Equations</strong><br><button class=copy-to-clipboard title="PDEformer: Towards a Foundation Model for One-Dimensional Partial Differential Equations" index=258>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-258 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: cs-NA, math-NA, math.NA<br>Keyword Score: 36<br>Keywords: Graph, Benchmarking, Foundation Model, Zero-shot, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12652v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12652v1.pdf filename=2402.12652v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper introduces PDEformer, a neural solver for partial differential equations (PDEs) capable of simultaneously addressing various types of PDEs. We advocate representing the PDE in the form of a computational <b>graph,</b> facilitating the seamless integration of both symbolic and numerical information inherent in a PDE. A <b>graph</b> <b>Transformer</b> and an implicit neural representation (INR) are employed to generate mesh-free predicted solutions. Following pretraining on data exhibiting a certain level of diversity, our model achieves <b>zero-shot</b> accuracies on <b>benchmark</b> datasets that surpass those of adequately trained expert models. Additionally, PDEformer demonstrates promising results in the inverse problem of PDE coefficient recovery.</p></p class="citation"></blockquote><h3 id=24--259316-contractivity-of-neural-odes-an-eigenvalue-optimization-problem-nicola-guglielmi-et-al-2024>(2/4 | 259/316) Contractivity of neural ODEs: an eigenvalue optimization problem (Nicola Guglielmi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nicola Guglielmi, Arturo De Marinis, Anton Savostianov, Francesco Tudisco. (2024)<br><strong>Contractivity of neural ODEs: an eigenvalue optimization problem</strong><br><button class=copy-to-clipboard title="Contractivity of neural ODEs: an eigenvalue optimization problem" index=259>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-259 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: cs-NA, math-NA, math-OC, math.NA<br>Keyword Score: 10<br>Keywords: MNIST<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13092v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13092v2.pdf filename=2402.13092v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We propose a novel methodology to solve a key eigenvalue optimization problem which arises in the contractivity analysis of neural ODEs. When looking at contractivity properties of a one layer weight-tied neural ODE $\dot{u}(t)=\sigma(Au(t)+b)$ (with $u,b \in {\mathbb R}^n$, $A$ is a given $n \times n$ matrix, $\sigma : {\mathbb R} \to {\mathbb R}^+$ denotes an activation function and for a vector $z \in {\mathbb R}^n$, $\sigma(z) \in {\mathbb R}^n$ has to be interpreted entry-wise), we are led to study the logarithmic norm of a set of products of type $D A$, where $D$ is a diagonal matrix such that ${\mathrm{diag}}(D) \in \sigma&rsquo;({\mathbb R}^n)$. Specifically, given a real number $c$ (usually $c=0$), the problem consists in finding the largest positive interval $\chi\subseteq \mathbb [0,\infty)$ such that the logarithmic norm $\mu(DA) \le c$ for all diagonal matrices $D$ with $D_{ii}\in \chi$. We propose a two-level nested methodology: an inner level where, for a given $\chi$, we compute an optimizer $D^\star(\chi)$ by a gradient system approach, and an outer level where we tune $\chi$ so that the value $c$ is reached by $\mu(D^\star(\chi)A)$. We extend the proposed two-level approach to the general multilayer, and possibly time-dependent, case $\dot{u}(t) = \sigma( A_k(t) \ldots \sigma ( A_{1}(t) u(t) + b_{1}(t) ) \ldots + b_{k}(t) )$ and we propose several numerical examples to illustrate its behaviour, including its stabilizing performance on a one-layer neural ODE applied to the classification of the <b>MNIST</b> handwritten digits dataset.</p></p class="citation"></blockquote><h3 id=34--260316-tensor-completion-with-bmd-factor-nuclear-norm-minimization-fan-tian-et-al-2024>(3/4 | 260/316) Tensor Completion with BMD Factor Nuclear Norm Minimization (Fan Tian et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fan Tian, Mirjeta Pasha, Misha E. Kilmer, Eric Miller, Abani Patra. (2024)<br><strong>Tensor Completion with BMD Factor Nuclear Norm Minimization</strong><br><button class=copy-to-clipboard title="Tensor Completion with BMD Factor Nuclear Norm Minimization" index=260>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-260 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: cs-NA, math-NA, math.NA<br>Keyword Score: 10<br>Keywords: Tensor Decomposition<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13068v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13068v1.pdf filename=2402.13068v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper is concerned with the problem of recovering third-order <b>tensor</b> <b>data</b> from limited samples. A recently proposed <b>tensor</b> <b>decomposition</b> (BMD) method has been shown to efficiently compress third-order spatiotemporal data. Using the BMD, we formulate a slicewise nuclear norm penalized algorithm to recover a third-order <b>tensor</b> <b>from</b> limited observed samples. We develop an efficient alternating direction method of multipliers (ADMM) scheme to solve the resulting minimization problem. Experimental results on real data show our method to give reconstruction comparable to those of HaLRTC (Liu et al., IEEE Trans Ptrn Anal Mchn Int, 2012), a well-known <b>tensor</b> <b>completion</b> method, in about the same number of iterations. However, our method has the advantage of smaller subproblems and higher parallelizability per iteration.</p></p class="citation"></blockquote><h3 id=44--261316-tree-semi-separable-matrices-a-simultaneous-generalization-of-sequentially-and-hierarchically-semi-separable-representations-nithin-govindarajan-et-al-2024>(4/4 | 261/316) Tree semi-separable matrices: a simultaneous generalization of sequentially and hierarchically semi-separable representations (Nithin Govindarajan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nithin Govindarajan, Shivkumar Chandrasekaran. (2024)<br><strong>Tree semi-separable matrices: a simultaneous generalization of sequentially and hierarchically semi-separable representations</strong><br><button class=copy-to-clipboard title="Tree semi-separable matrices: a simultaneous generalization of sequentially and hierarchically semi-separable representations" index=261>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-261 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: 15A23, cs-NA, math-NA, math.NA<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13381v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13381v1.pdf filename=2402.13381v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present a unification and generalization of sequentially and hierarchically semi-separable (SSS and HSS) matrices called tree semi-separable (TSS) matrices. Our main result is to show that any dense matrix can be expressed in a TSS format. Here, the dimensions of the generators are specified by the ranks of the Hankel blocks of the matrix. TSS matrices satisfy a <b>graph-induced</b> rank structure (GIRS) property. It is shown that TSS matrices generalize the algebraic properties of SSS and HSS matrices under addition, products, and inversion. Subsequently, TSS matrices admit linear time matrix-vector multiply, matrix-matrix multiply, matrix-matrix addition, inversion, and solvers.</p></p class="citation"></blockquote><h2 id=csmm-2>cs.MM (2)</h2><h3 id=12--262316-a-user-friendly-framework-for-generating-model-preferred-prompts-in-text-to-image-synthesis-nailei-hei-et-al-2024>(1/2 | 262/316) A User-Friendly Framework for Generating Model-Preferred Prompts in Text-to-Image Synthesis (Nailei Hei et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nailei Hei, Qianyu Guo, Zihao Wang, Yan Wang, Haofen Wang, Wenqiang Zhang. (2024)<br><strong>A User-Friendly Framework for Generating Model-Preferred Prompts in Text-to-Image Synthesis</strong><br><button class=copy-to-clipboard title="A User-Friendly Framework for Generating Model-Preferred Prompts in Text-to-Image Synthesis" index=262>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-262 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.MM<br>Categories: cs-AI, cs-CV, cs-MM, cs.MM<br>Keyword Score: 30<br>Keywords: Text Generation, Text2image, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12760v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12760v1.pdf filename=2402.12760v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Well-designed <b>prompts</b> have demonstrated the potential to guide <b>text-to-image</b> <b>models</b> in generating amazing images. Although existing <b>prompt</b> engineering methods can provide high-level guidance, it is challenging for novice users to achieve the desired results by manually entering <b>prompts</b> due to a discrepancy between novice-user-input <b>prompts</b> and the model-preferred <b>prompts.</b> To bridge the distribution gap between user input behavior and model training datasets, we first construct a novel Coarse-Fine Granularity <b>Prompts</b> dataset (CFP) and propose a novel User-Friendly Fine-Grained <b>Text</b> <b>Generation</b> framework (UF-FGTG) for automated <b>prompt</b> optimization. For CFP, we construct a novel dataset for <b>text-to-image</b> <b>tasks</b> that combines coarse and fine-grained <b>prompts</b> to facilitate the development of automated <b>prompt</b> generation methods. For UF-FGTG, we propose a novel framework that automatically translates user-input <b>prompts</b> into model-preferred <b>prompts.</b> Specifically, we propose a <b>prompt</b> refiner that continually rewrites <b>prompts</b> to empower users to select results that align with their unique needs. Meanwhile, we integrate image-related loss functions from the <b>text-to-image</b> <b>model</b> into the training process of <b>text</b> <b>generation</b> to generate model-preferred <b>prompts.</b> Additionally, we propose an adaptive feature extraction module to ensure diversity in the generated results. Experiments demonstrate that our approach is capable of generating more visually appealing and diverse images than previous state-of-the-art methods, achieving an average improvement of 5% across six quality and aesthetic metrics.</p></p class="citation"></blockquote><h3 id=22--263316-television-discourse-decoded-comprehensive-multimodal-analytics-at-scale-anmol-agarwal-et-al-2024>(2/2 | 263/316) Television Discourse Decoded: Comprehensive Multimodal Analytics at Scale (Anmol Agarwal et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Anmol Agarwal, Pratyush Priyadarshi, Shiven Sinha, Shrey Gupta, Hitkul Jangra, Kiran Garimella, Ponnurangam Kumaraguru. (2024)<br><strong>Television Discourse Decoded: Comprehensive Multimodal Analytics at Scale</strong><br><button class=copy-to-clipboard title="Television Discourse Decoded: Comprehensive Multimodal Analytics at Scale" index=263>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-263 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.MM<br>Categories: cs-CY, cs-MM, cs-SI, cs.MM<br>Keyword Score: 6<br>Keywords: Multi-modal, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12629v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12629v1.pdf filename=2402.12629v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we tackle the complex task of analyzing televised debates, with a focus on a prime time news debate show from India. Previous methods, which often relied solely on text, fall short in capturing the multimedia essence of these debates. To address this gap, we introduce a comprehensive automated toolkit that employs advanced computer vision and speech-to-text techniques for large-scale multimedia analysis. Utilizing state-of-the-art computer vision algorithms and speech-to-text methods, we transcribe, diarize, and analyze thousands of YouTube videos of prime-time television debates in India. These debates are a central part of Indian media but have been criticized for compromised journalistic integrity and excessive dramatization. Our toolkit provides concrete metrics to assess bias and incivility, capturing a comprehensive multimedia perspective that includes text, audio utterances, and video frames. Our findings reveal significant biases in topic selection and panelist representation, along with alarming levels of incivility. This work offers a scalable, automated approach for future research in multimedia analysis, with profound implications for the quality of public discourse and democratic debate. We will make our data analysis pipeline and collected data publicly available to catalyze further research in this domain.</p></p class="citation"></blockquote><h2 id=quant-ph-5>quant-ph (5)</h2><h3 id=15--264316-quantum-embedding-with-transformer-for-high-dimensional-data-hao-yuan-chen-et-al-2024>(1/5 | 264/316) Quantum Embedding with Transformer for High-dimensional Data (Hao-Yuan Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hao-Yuan Chen, Yen-Jui Chang, Shih-Wei Liao, Ching-Ray Chang. (2024)<br><strong>Quantum Embedding with Transformer for High-dimensional Data</strong><br><button class=copy-to-clipboard title="Quantum Embedding with Transformer for High-dimensional Data" index=264>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-264 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: quant-ph<br>Categories: cs-LG, quant-ph, quant-ph<br>Keyword Score: 30<br>Keywords: Vision Transformer, Transformer, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12704v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12704v1.pdf filename=2402.12704v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Quantum embedding with <b>transformers</b> is a novel and promising architecture for quantum machine learning to deliver exceptional capability on near-term devices or simulators. The research incorporated a <b>vision</b> <b>transformer</b> (ViT) to advance quantum significantly embedding ability and results for a single qubit classifier with around 3 percent in the median F1 score on the BirdCLEF-2021, a challenging high-dimensional dataset. The study showcases and analyzes empirical evidence that our <b>transformer-based</b> architecture is a highly versatile and practical approach to modern quantum machine learning problems.</p></p class="citation"></blockquote><h3 id=25--265316-ketgpt----dataset-augmentation-of-quantum-circuits-using-transformers-boran-apak-et-al-2024>(2/5 | 265/316) KetGPT &ndash; Dataset Augmentation of Quantum Circuits using Transformers (Boran Apak et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Boran Apak, Medina Bandic, Aritra Sarkar, Sebastian Feld. (2024)<br><strong>KetGPT &ndash; Dataset Augmentation of Quantum Circuits using Transformers</strong><br><button class=copy-to-clipboard title="KetGPT -- Dataset Augmentation of Quantum Circuits using Transformers" index=265>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-265 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: quant-ph<br>Categories: cs-AI, cs-ET, cs-LG, quant-ph, quant-ph<br>Keyword Score: 16<br>Keywords: Benchmarking, Benchmarking, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13352v3 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13352v3.pdf filename=2402.13352v3.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Quantum algorithms, represented as quantum circuits, can be used as <b>benchmarks</b> for assessing the performance of quantum systems. Existing datasets, widely utilized in the field, suffer from limitations in size and versatility, leading researchers to employ randomly generated circuits. Random circuits are, however, not representative <b>benchmarks</b> as they lack the inherent properties of real quantum algorithms for which the quantum systems are manufactured. This shortage of <code>useful' quantum &lt;b>benchmarks&lt;/b> poses a challenge to advancing the development and comparison of quantum compilers and hardware. This research aims to enhance the existing quantum circuit datasets by generating what we refer to as </code>realistic-looking&rsquo; circuits by employing the <b>Transformer</b> machine learning architecture. For this purpose, we introduce KetGPT, a tool that generates synthetic circuits in OpenQASM language, whose structure is based on quantum circuits derived from existing quantum algorithms and follows the typical patterns of human-written algorithm-based code (e.g., order of gates and qubits). Our three-fold verification process, involving manual inspection and Qiskit framework execution, <b>transformer-based</b> classification, and structural analysis, demonstrates the efficacy of KetGPT in producing large amounts of additional circuits that closely align with algorithm-based structures. Beyond <b>benchmarking,</b> we envision KetGPT contributing substantially to AI-driven quantum compilers and systems.</p></p class="citation"></blockquote><h3 id=35--266316-a-unifying-primary-framework-for-quantum-graph-neural-networks-from-quantum-graph-states-ammar-daskin-2024>(3/5 | 266/316) A unifying primary framework for quantum graph neural networks from quantum graph states (Ammar Daskin, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ammar Daskin. (2024)<br><strong>A unifying primary framework for quantum graph neural networks from quantum graph states</strong><br><button class=copy-to-clipboard title="A unifying primary framework for quantum graph neural networks from quantum graph states" index=266>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-266 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: quant-ph<br>Categories: cs-LG, quant-ph, quant-ph<br>Keyword Score: 13<br>Keywords: Graph, Graph Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13001v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13001v2.pdf filename=2402.13001v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Graph</b> <b>states</b> <b>are</b> used to represent mathematical <b>graphs</b> <b>as</b> <b>quantum</b> states on quantum computers. They can be formulated through stabilizer codes or directly quantum gates and quantum states. In this paper we show that a quantum <b>graph</b> <b>neural</b> <b>network</b> model can be understood and realized based on <b>graph</b> <b>states.</b> <b>We</b> show that they can be used either as a parameterized quantum circuits to represent neural networks or as an underlying structure to construct <b>graph</b> <b>neural</b> <b>networks</b> on quantum computers.</p></p class="citation"></blockquote><h3 id=45--267316-quantum-pseudorandomness-cannot-be-shrunk-in-a-black-box-way-samuel-bouaziz--ermann-et-al-2024>(4/5 | 267/316) Quantum Pseudorandomness Cannot Be Shrunk In a Black-Box Way (Samuel Bouaziz&ndash;Ermann et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Samuel Bouaziz&ndash;Ermann, Garazi Muguruza. (2024)<br><strong>Quantum Pseudorandomness Cannot Be Shrunk In a Black-Box Way</strong><br><button class=copy-to-clipboard title="Quantum Pseudorandomness Cannot Be Shrunk In a Black-Box Way" index=267>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-267 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: quant-ph<br>Categories: cs-CR, quant-ph, quant-ph<br>Keyword Score: 5<br>Keywords: Black Box<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13324v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13324v1.pdf filename=2402.13324v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Pseudorandom Quantum States (PRS) were introduced by Ji, Liu and Song as quantum analogous to Pseudorandom Generators. They are an ensemble of states efficiently computable but computationally indistinguishable from Haar random states. Subsequent works have shown that some cryptographic primitives can be constructed from PRSs. Moreover, recent classical and quantum oracle separations of PRS from One-Way Functions strengthen the interest in a purely quantum alternative building block for quantum cryptography, potentially weaker than OWFs. However, our lack of knowledge of extending or shrinking the number of qubits of the PRS output still makes it difficult to reproduce some of the classical proof techniques and results. Short-PRSs, that is PRSs with logarithmic size output, have been introduced in the literature along with cryptographic applications, but we still do not know how they relate to PRSs. Here we answer half of the question, by showing that it is not possible to shrink the output of a PRS from polynomial to logarithmic qubit length while still preserving the pseudorandomness property, in a relativized way. More precisely, we show that relative to Kretschmer&rsquo;s quantum oracle (TQC 2021) short-PRSs cannot exist (while PRSs exist, as shown by Kretschmer&rsquo;s work).</p></p class="citation"></blockquote><h3 id=55--268316-guarantees-on-warm-started-qaoa-single-round-approximation-ratios-for-3-regular-maxcut-and-higher-round-scaling-limits-reuben-tate-et-al-2024>(5/5 | 268/316) Guarantees on Warm-Started QAOA: Single-Round Approximation Ratios for 3-Regular MAXCUT and Higher-Round Scaling Limits (Reuben Tate et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Reuben Tate, Stephan Eidenbenz. (2024)<br><strong>Guarantees on Warm-Started QAOA: Single-Round Approximation Ratios for 3-Regular MAXCUT and Higher-Round Scaling Limits</strong><br><button class=copy-to-clipboard title="Guarantees on Warm-Started QAOA: Single-Round Approximation Ratios for 3-Regular MAXCUT and Higher-Round Scaling Limits" index=268>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-268 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: quant-ph<br>Categories: cs-DS, cs-ET, math-CO, math-OC, quant-ph, quant-ph<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12631v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12631v1.pdf filename=2402.12631v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We generalize Farhi et al.&rsquo;s 0.6924-approximation result technique of the Max-Cut Quantum Approximate Optimization Algorithm (QAOA) on 3-regular <b>graphs</b> to obtain provable lower bounds on the approximation ratio for warm-started QAOA. Given an initialization angle $\theta$, we consider warm-starts where the initial state is a product state where each qubit position is angle $\theta$ away from either the north or south pole of the Bloch sphere; of the two possible qubit positions the position of each qubit is decided by some classically obtained cut encoded as a bitstring $b$. We illustrate through plots how the properties of $b$ and the initialization angle $\theta$ influence the bound on the approximation ratios of warm-started QAOA. We consider various classical algorithms (and the cuts they produce which we use to generate the warm-start). Our results strongly suggest that there does not exist any choice of initialization angle that yields a (worst-case) approximation ratio that simultaneously beats standard QAOA and the classical algorithm used to create the warm-start. Additionally, we show that at $\theta=60^\circ$, warm-started QAOA is able to (effectively) recover the cut used to generate the warm-start, thus suggesting that in practice, this value could be a promising starting angle to explore alternate solutions in a heuristic fashion. Finally, for any combinatorial optimization problem with integer-valued objective values, we provide bounds on the required circuit depth needed for warm-started QAOA to achieve some change in approximation ratio; more specifically, we show that for small $\theta$, the bound is roughly proportional to $1/\theta$.</p></p class="citation"></blockquote><h2 id=physicssoc-ph-1>physics.soc-ph (1)</h2><h3 id=11--269316-an-evolutionary-game-with-reputation-based-imitation-mutation-dynamics-kehuan-feng-et-al-2024>(1/1 | 269/316) An evolutionary game with reputation-based imitation-mutation dynamics (Kehuan Feng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kehuan Feng, Songlin Han, Minyu Feng, Attila Szolnoki. (2024)<br><strong>An evolutionary game with reputation-based imitation-mutation dynamics</strong><br><button class=copy-to-clipboard title="An evolutionary game with reputation-based imitation-mutation dynamics" index=269>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-269 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: physics.soc-ph<br>Categories: cond-mat-stat-mech, cs-GT, nlin-AO, physics-soc-ph, physics.soc-ph<br>Keyword Score: 23<br>Keywords: Graph, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13011v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13011v1.pdf filename=2402.13011v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Reputation plays a crucial role in social interactions by affecting the fitness of individuals during an evolutionary process. Previous works have extensively studied the result of imitation dynamics without focusing on potential irrational choices in strategy updates. We now fill this gap and explore the consequence of such kind of randomness, or one may interpret it as an autonomous thinking. In particular, we study how this extended dynamics alters the evolution of cooperation when individual reputation is directly linked to collected payoff, hence providing a general fitness function. For a broadly valid conclusion, our spatial populations cover different types of interaction topologies, including lattices, small-world and scale-free <b>graphs.</b> By means of intensive <b>simulations</b> we can detect substantial increase in cooperation level that shows a reasonable stability in the presence of a notable strategy mutation.</p></p class="citation"></blockquote><h2 id=cscy-2>cs.CY (2)</h2><h3 id=12--270316-towards-fair-allocation-in-social-commerce-platforms-anjali-gupta-et-al-2024>(1/2 | 270/316) Towards Fair Allocation in Social Commerce Platforms (Anjali Gupta et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Anjali Gupta, Shreyans J. Nagori, Abhijnan Chakraborty, Rohit Vaish, Sayan Ranu, Prajit Prashant Nadkarni, Narendra Varma Dasararaju, Muthusamy Chelliah. (2024)<br><strong>Towards Fair Allocation in Social Commerce Platforms</strong><br><button class=copy-to-clipboard title="Towards Fair Allocation in Social Commerce Platforms" index=270>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-270 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CY<br>Categories: cs-CY, cs-SI, cs.CY<br>Keyword Score: 23<br>Keywords: Benchmarking, Fairness, Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12759v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12759v1.pdf filename=2402.12759v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Social commerce platforms are emerging businesses where producers sell products through re-sellers who advertise the products to other customers in their social network. Due to the increasing popularity of this business model, thousands of small producers and re-sellers are starting to depend on these platforms for their livelihood; thus, it is important to provide fair earning opportunities to them. The enormous product space in such platforms prohibits manual search, and motivates the need for <b>recommendation</b> algorithms to effectively allocate product exposure and, consequently, earning opportunities. In this work, we focus on the <b>fairness</b> of such allocations in social commerce platforms and formulate the problem of assigning products to re-sellers as a fair division problem with indivisible items under two-sided cardinality constraints, wherein each product must be given to at least a certain number of re-sellers and each re-seller must get a certain number of products. Our work systematically explores various well-studied <b>benchmarks</b> of <b>fairness</b> &ndash; including Nash social welfare, envy-freeness up to one item (EF1), and equitability up to one item (EQ1) &ndash; from both theoretical and experimental perspectives. We find that the existential and computational guarantees of these concepts known from the unconstrained setting do not extend to our constrained model. To address this limitation, we develop a mixed-integer linear program and other scalable heuristics that provide near-optimal approximation of Nash social welfare in simulated and real social commerce datasets. Overall, our work takes the first step towards achieving provable <b>fairness</b> alongside reasonable revenue guarantees on social commerce platforms.</p></p class="citation"></blockquote><h3 id=22--271316-are-large-language-models-llms-good-social-predictors-kaiqi-yang-et-al-2024>(2/2 | 271/316) Are Large Language Models (LLMs) Good Social Predictors? (Kaiqi Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kaiqi Yang, Hang Li, Hongzhi Wen, Tai-Quan Peng, Jiliang Tang, Hui Liu. (2024)<br><strong>Are Large Language Models (LLMs) Good Social Predictors?</strong><br><button class=copy-to-clipboard title="Are Large Language Models (LLMs) Good Social Predictors?" index=271>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-271 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CY<br>Categories: cs-CY, cs.CY<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12620v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12620v1.pdf filename=2402.12620v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The prediction has served as a crucial scientific method in modern social studies. With the recent advancement of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs),</b> efforts have been made to leverage <b>LLMs</b> to predict the human features in social life, such as presidential voting. These works suggest that <b>LLMs</b> are capable of generating human-like responses. However, we find that the promising performance achieved by previous studies is because of the existence of input shortcut features to the response. In fact, by removing these shortcuts, the performance is reduced dramatically. To further revisit the ability of <b>LLMs,</b> we introduce a novel social prediction task, Soc-PRF Prediction, which utilizes general features as input and simulates real-world social study settings. With the comprehensive investigations on various <b>LLMs,</b> we reveal that <b>LLMs</b> cannot work as expected on social prediction when given general input features without shortcuts. We further investigate possible reasons for this phenomenon that suggest potential ways to enhance <b>LLMs</b> for social prediction.</p></p class="citation"></blockquote><h2 id=physicsflu-dyn-1>physics.flu-dyn (1)</h2><h3 id=11--272316-a-fully-integrated-lattice-boltzmann-method-for-fluid-structure-interaction-yue-sun-et-al-2024>(1/1 | 272/316) A fully-integrated lattice Boltzmann method for fluid-structure interaction (Yue Sun et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yue Sun, Chris H. Rycroft. (2024)<br><strong>A fully-integrated lattice Boltzmann method for fluid-structure interaction</strong><br><button class=copy-to-clipboard title="A fully-integrated lattice Boltzmann method for fluid-structure interaction" index=272>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-272 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: physics.flu-dyn<br>Categories: cs-NA, math-NA, physics-flu-dyn, physics.flu-dyn<br>Keyword Score: 23<br>Keywords: Benchmarking, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12696v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12696v1.pdf filename=2402.12696v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present a fully-integrated lattice Boltzmann (LB) method for fluid&ndash;structure interaction (FSI) <b>simulations</b> that efficiently models deformable solids in complex suspensions and active systems. Our Eulerian method (LBRMT) couples finite-strain solids to the LB fluid on the same fixed computational grid with the reference map technique (RMT). An integral part of the LBRMT is a new LB boundary condition for moving deformable interfaces across different densities. With this fully Eulerian solid&ndash;fluid coupling, the LBRMT is well-suited for parallelization and simulating multi-body contact without remeshing or extra meshes. We validate its accuracy via a <b>benchmark</b> of a deformable solid in a lid-driven cavity, then showcase its versatility through examples of soft solids rotating and settling. With <b>simulations</b> of complex suspensions mixing, we highlight potentials of the LBRMT for studying collective behavior in soft matter and biofluid dynamics.</p></p class="citation"></blockquote><h2 id=cssi-3>cs.SI (3)</h2><h3 id=13--273316-ego-group-partition-a-novel-framework-for-improving-ego-experiments-in-social-networks-lu-deng-et-al-2024>(1/3 | 273/316) Ego Group Partition: A Novel Framework for Improving Ego Experiments in Social Networks (Lu Deng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lu Deng, JingJing Zhang, Yong Wang, Chuan Chen. (2024)<br><strong>Ego Group Partition: A Novel Framework for Improving Ego Experiments in Social Networks</strong><br><button class=copy-to-clipboard title="Ego Group Partition: A Novel Framework for Improving Ego Experiments in Social Networks" index=273>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-273 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SI<br>Categories: cs-SI, cs.SI, stat-AP<br>Keyword Score: 23<br>Keywords: Clustering, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12655v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12655v1.pdf filename=2402.12655v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Estimating the average treatment effect in social networks is challenging due to individuals influencing each other. One approach to address interference is ego cluster experiments, where each cluster consists of a central individual (ego) and its peers (alters). Clusters are randomized, and only the effects on egos are measured. In this work, we propose an improved framework for ego cluster experiments called ego group partition (EGP), which directly generates two groups and an ego sub-population instead of ego clusters. Under specific model assumptions, we propose two ego group partition algorithms. Compared to the original ego <b>clustering</b> algorithm, our algorithms produce more egos, yield smaller biases, and support parallel computation. The performance of our algorithms is validated through <b>simulation</b> and real-world case studies.</p></p class="citation"></blockquote><h3 id=23--274316-are-fact-checking-tools-reliable-an-evaluation-of-google-fact-check-qiangeng-yang-et-al-2024>(2/3 | 274/316) Are Fact-Checking Tools Reliable? An Evaluation of Google Fact Check (Qiangeng Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qiangeng Yang, Tess Christensen, Shlok Gilda, Juliana Fernandes, Daniela Oliveira. (2024)<br><strong>Are Fact-Checking Tools Reliable? An Evaluation of Google Fact Check</strong><br><button class=copy-to-clipboard title="Are Fact-Checking Tools Reliable? An Evaluation of Google Fact Check" index=274>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-274 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SI<br>Categories: cs-SI, cs.SI<br>Keyword Score: 10<br>Keywords: Fact Verification<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13244v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13244v1.pdf filename=2402.13244v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Fact-checking</b> <b>is</b> an important way to combat misinformation on social media, especially during significant social events such as the COVID-19 pandemic and the U.S. presidential elections. In this study, we thoroughly evaluated the performance of Google <b>Fact</b> <b>Check,</b> a search engine specifically for <b>fact-checking</b> <b>results,</b> by analyzing the results returned from Google <b>Fact</b> <b>Check</b> regarding 1,000 false claims about COVID-19. We found that Google <b>Fact</b> <b>Check</b> could not provide sufficient <b>fact-checking</b> <b>information</b> for most false claims, even though the results provided are relatively reliable and helpful. We also found that claims getting different <b>fact-checking</b> <b>verdicts</b> tend to contain different emotional tones, and different sources tend to check claims using dictionary words to different extents and at different lengths. Claims in different descriptions are likely to get different <b>fact-checking</b> <b>results.</b> We aimed to bring up the best practice of <b>fact-checking</b> <b>for</b> the general people based on our analyses.</p></p class="citation"></blockquote><h3 id=33--275316-effective-edge-ranking-via-random-walk-with-restart-renchi-yang-2024>(3/3 | 275/316) Effective Edge Ranking via Random Walk with Restart (Renchi Yang, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Renchi Yang. (2024)<br><strong>Effective Edge Ranking via Random Walk with Restart</strong><br><button class=copy-to-clipboard title="Effective Edge Ranking via Random Walk with Restart" index=275>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-275 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SI<br>Categories: cs-SI, cs.SI<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12623v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12623v1.pdf filename=2402.12623v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Given a network G, edge centrality is a metric used to evaluate the importance of edges in G, which is a key concept in analyzing networks and finds vast applications involving edge ranking. In spite of a wealth of research on devising edge centrality measures, they incur either prohibitively high computation costs or varied deficiencies that lead to sub-optimal ranking quality. To overcome their limitations, this paper proposes EdgeRAKE, a new centrality measure for edge ranking that leverages the novel notion of the edgewise random walk with restart. Based thereon, we present a linear-complexity algorithm for EdgeRAKE approximation, followed by an in-depth theoretical analysis in terms of various aspects. Extensive experiments comparing EdgeRAKE against six edge centrality metrics in <b>graph</b> analytics tasks on real networks showcase that EdgeRAKE offers superior practical effectiveness without significantly reducing computation efficiency</p></p class="citation"></blockquote><h2 id=csdb-2>cs.DB (2)</h2><h3 id=12--276316-everything-you-always-wanted-to-know-about-storage-compressibility-of-pre-trained-ml-models-but-were-afraid-to-ask-zhaoyuan-su-et-al-2024>(1/2 | 276/316) Everything You Always Wanted to Know About Storage Compressibility of Pre-Trained ML Models but Were Afraid to Ask (Zhaoyuan Su et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhaoyuan Su, Ammar Ahmed, Zirui Wang, Ali Anwar, Yue Cheng. (2024)<br><strong>Everything You Always Wanted to Know About Storage Compressibility of Pre-Trained ML Models but Were Afraid to Ask</strong><br><button class=copy-to-clipboard title="Everything You Always Wanted to Know About Storage Compressibility of Pre-Trained ML Models but Were Afraid to Ask" index=276>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-276 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DB<br>Categories: H-2-7, cs-DB, cs.DB<br>Keyword Score: 20<br>Keywords: Model Quantization, Quantization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13429v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13429v1.pdf filename=2402.13429v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>As the number of pre-trained machine learning (ML) <b>models</b> <b>is</b> growing exponentially, data reduction tools are not catching up. Existing data reduction techniques are not specifically designed for pre-trained <b>model</b> <b>(PTM)</b> dataset files. This is largely due to a lack of understanding of the patterns and characteristics of these datasets, especially those relevant to data reduction and compressibility. This paper presents the first, exhaustive analysis to date of PTM datasets on storage compressibility. Our analysis spans different types of data reduction and compression techniques, from hash-based data deduplication, data similarity detection, to dictionary-coding compression. Our analysis explores these techniques at three data granularity levels, from <b>model</b> <b>layers,</b> <b>model</b> <b>chunks,</b> to <b>model</b> <b>parameters.</b> We draw new observations that indicate that modern data reduction tools are not effective when handling PTM datasets. There is a pressing need for new compression methods that take into account PTMs&rsquo; data characteristics for effective storage reduction. Motivated by our findings, we design ELF, a simple yet effective, error-bounded, lossy floating-point compression method. ELF transforms floating-point parameters in such a way that the common exponent field of the transformed parameters can be completely eliminated to save storage space. We develop Elves, a compression framework that integrates ELF along with several other data reduction methods. Elves uses the most effective method to compress PTMs that exhibit different patterns. Evaluation shows that Elves achieves an overall compression ratio of $1.52\times$, which is $1.31\times$, $1.32\times$ and $1.29\times$ higher than a general-purpose compressor (zstd), an error-bounded lossy compressor (SZ3), and the uniform <b>model</b> <b>quantization,</b> respectively, with negligible <b>model</b> <b>accuracy</b> loss.</p></p class="citation"></blockquote><h3 id=22--277316-xling-a-learned-filter-framework-for-accelerating-high-dimensional-approximate-similarity-join-yifan-wang-et-al-2024>(2/2 | 277/316) Xling: A Learned Filter Framework for Accelerating High-Dimensional Approximate Similarity Join (Yifan Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yifan Wang, Vyom Pathak, Daisy Zhe Wang. (2024)<br><strong>Xling: A Learned Filter Framework for Accelerating High-Dimensional Approximate Similarity Join</strong><br><button class=copy-to-clipboard title="Xling: A Learned Filter Framework for Accelerating High-Dimensional Approximate Similarity Join" index=277>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-277 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DB<br>Categories: cs-AI, cs-DB, cs.DB<br>Keyword Score: 10<br>Keywords: BLOOM<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13397v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13397v1.pdf filename=2402.13397v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Similarity join finds all pairs of close points within a given distance threshold. Many similarity join methods have been proposed, but they are usually not efficient on high-dimensional space due to the curse of dimensionality and data-unawareness. We investigate the possibility of using metric space <b>Bloom</b> filter (MSBF), a family of data structures checking if a query point has neighbors in a multi-dimensional space, to speed up similarity join. However, there are several challenges when applying MSBF to similarity join, including excessive information loss, data-unawareness and hard constraint on the distance metric. In this paper, we propose Xling, a generic framework to build a learning-based metric space filter with any existing regression model, aiming at accurately predicting whether a query point has enough number of neighbors. The framework provides a suite of optimization strategies to further improve the prediction quality based on the learning model, which has demonstrated significantly higher prediction quality than existing MSBF. We also propose XJoin, one of the first filter-based similarity join methods, based on Xling. By predicting and skipping those queries without enough neighbors, XJoin can effectively reduce unnecessary neighbor searching and therefore it achieves a remarkable acceleration. Benefiting from the generalization capability of deep learning models, XJoin can be easily transferred onto new dataset (in similar distribution) without re-training. Furthermore, Xling is not limited to being applied in XJoin, instead, it acts as a flexible plugin that can be inserted to any loop-based similarity join methods for a speedup.</p></p class="citation"></blockquote><h2 id=mathoc-1>math.OC (1)</h2><h3 id=11--278316-controlling-large-electric-vehicle-charging-stations-via-user-behavior-modeling-and-stochastic-programming-alban-puech-et-al-2024>(1/1 | 278/316) Controlling Large Electric Vehicle Charging Stations via User Behavior Modeling and Stochastic Programming (Alban Puech et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Alban Puech, Tristan Rigaut, William Templier, Maud Tournoud. (2024)<br><strong>Controlling Large Electric Vehicle Charging Stations via User Behavior Modeling and Stochastic Programming</strong><br><button class=copy-to-clipboard title="Controlling Large Electric Vehicle Charging Stations via User Behavior Modeling and Stochastic Programming" index=278>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-278 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.OC<br>Categories: cs-AI, cs-CE, cs-LG, cs-SY, eess-SY, math-OC, math.OC<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13224v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13224v2.pdf filename=2402.13224v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper introduces an Electric Vehicle Charging Station (EVCS) model that incorporates real-world constraints, such as slot power limitations, contract threshold overruns penalties, or early disconnections of electric vehicles (EVs). We propose a formulation of the problem of EVCS control under uncertainty, and implement two Multi-Stage Stochastic Programming approaches that leverage user-provided information, namely, Model Predictive Control and Two-Stage Stochastic Programming. The model addresses uncertainties in charging session start and end times, as well as in energy demand. A user&rsquo;s behavior model based on a sojourn-time-dependent stochastic process enhances cost reduction while maintaining customer satisfaction. The benefits of the two proposed methods are showcased against two baselines over a 22-day <b>simulation</b> using a real-world dataset. The two-stage approach proves robust against early disconnections, considering a more significant number of uncertainty scenarios for optimization. The algorithm prioritizing user satisfaction over electricity cost achieves a 20% and 36% improvement in two user satisfaction metrics compared to an industry-standard baseline. Additionally, the algorithm striking the best balance between cost and user satisfaction exhibits a mere 3% relative cost increase compared to the theoretically optimal baseline - for which the nonanticipativity constraint is relaxed - while attaining 94% and 84% of the user satisfaction performance in the two used satisfaction metrics.</p></p class="citation"></blockquote><h2 id=csni-3>cs.NI (3)</h2><h3 id=13--279316-a-lightweight-machine-learning-approach-for-delay-aware-cell-switching-in-6g-haps-networks-görkem-berkay-koç-et-al-2024>(1/3 | 279/316) A Lightweight Machine Learning Approach for Delay-Aware Cell-Switching in 6G HAPS Networks (Görkem Berkay Koç et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Görkem Berkay Koç, Berk Çiloğlu, Metin Ozturk, Halim Yanikomeroglu. (2024)<br><strong>A Lightweight Machine Learning Approach for Delay-Aware Cell-Switching in 6G HAPS Networks</strong><br><button class=copy-to-clipboard title="A Lightweight Machine Learning Approach for Delay-Aware Cell-Switching in 6G HAPS Networks" index=279>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-279 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NI<br>Categories: cs-NI, cs.NI, eess-SP<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13096v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13096v1.pdf filename=2402.13096v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This study investigates the integration of a high altitude platform station (HAPS), a non-terrestrial network (NTN) node, into the cell-switching paradigm for energy saving. By doing so, the sustainability and ubiquitous connectivity targets can be achieved. Besides, a delay-aware approach is also adopted, where the delay profiles of users are respected in such a way that we attempt to meet the latency requirements of users with a best-effort strategy. To this end, a novel, simple, and lightweight Q-learning algorithm is designed to address the cell-switching optimization problem. During the <b>simulation</b> campaigns, different interference scenarios and delay situations between base stations are examined in terms of energy consumption and quality-of-service (QoS), and the results confirm the efficacy of the proposed Q-learning algorithm.</p></p class="citation"></blockquote><h3 id=23--280316-enhanced-physical-layer-security-for-full-duplex-symbiotic-radio-with-an-generation-and-forward-noise-suppression-chi-jin-et-al-2024>(2/3 | 280/316) Enhanced Physical Layer Security for Full-duplex Symbiotic Radio with AN Generation and Forward Noise Suppression (Chi Jin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chi Jin, Zheng Chang, Fengye Hu, Hsiao-Hwa Chen, Timo Hamalainen. (2024)<br><strong>Enhanced Physical Layer Security for Full-duplex Symbiotic Radio with AN Generation and Forward Noise Suppression</strong><br><button class=copy-to-clipboard title="Enhanced Physical Layer Security for Full-duplex Symbiotic Radio with AN Generation and Forward Noise Suppression" index=280>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-280 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NI<br>Categories: cs-NI, cs.NI<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12747v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12747v1.pdf filename=2402.12747v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Due to the constraints on power supply and limited encryption capability, data security based on physical layer security (PLS) techniques in backscatter communications has attracted a lot of attention. In this work, we propose to enhance PLS in a full-duplex symbiotic radio (FDSR) system with a proactive eavesdropper, which may overhear the information and interfere legitimate communications simultaneously by emitting attack signals. To deal with the eavesdroppers, we propose a security strategy based on pseudo-decoding and artificial noise (AN) injection to ensure the performance of legitimate communications through forward noise suppression. A novel AN signal generation scheme is proposed using a pseudo-decoding method, where AN signal is superimposed on data signal to safeguard the legitimate channel. The phase control in the forward noise suppression scheme and the power allocation between AN and data signals are optimized to maximize security throughput. The formulated problem can be solved via problem decomposition and alternate optimization algorithms. <b>Simulation</b> results demonstrate the superiority of the proposed scheme in terms of security throughput and attack mitigation performance.</p></p class="citation"></blockquote><h3 id=33--281316-federated-learning-for-iotedgefog-computing-systems-balqees-talal-hasan-et-al-2024>(3/3 | 281/316) Federated Learning for Iot/Edge/Fog Computing Systems (Balqees Talal Hasan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Balqees Talal Hasan, Ali Kadhum Idrees. (2024)<br><strong>Federated Learning for Iot/Edge/Fog Computing Systems</strong><br><button class=copy-to-clipboard title="Federated Learning for Iot/Edge/Fog Computing Systems" index=281>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-281 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NI<br>Categories: cs-NI, cs.NI<br>Keyword Score: 10<br>Keywords: Federated Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13029v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13029v1.pdf filename=2402.13029v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the help of a new architecture called Edge/Fog (E/F) computing, cloud computing services can now be extended nearer to data generator devices. E/F computing in combination with Deep Learning (DL) is a promisedtechnique that is vastly applied in numerous fields. To train their models, data producers in conventional DL architectures with E/F computing enable them to repeatedly transmit and communicate data with third-party servers, like Edge/Fog or cloud servers. Due to the extensive bandwidth needs, legal issues, and privacy risks, this architecture is frequently impractical. Through a centralized server, the models can be co-trained by FL through distributed clients, including cars, hospitals, and mobile phones, while preserving data localization. As it facilitates group learning and model optimization, FL can therefore be seen as a motivating element in the E/F computing paradigm. Although FL applications in E/F computing environments have been considered in previous studies, FL execution and hurdles in the E/F computing framework have not been thoroughly covered. In order to identify advanced solutions, this chapter will provide a review of the application of FL in E/F computing systems. We think that by doing this chapter, researchers will learn more about how E/F computing and FL enable related concepts and technologies. Some case studies about the implementation of <b>federated</b> <b>learning</b> in E/F computing are being investigated. The open issues and future research directions are introduced.</p></p class="citation"></blockquote><h2 id=cshc-3>cs.HC (3)</h2><h3 id=13--282316-solving-the-decision-making-differential-equations-from-eye-fixation-data-in-unity-software-by-using-hermite-long-short-term-memory-neural-network-kourosh-parand-et-al-2024>(1/3 | 282/316) Solving the decision-making differential equations from eye fixation data in Unity software by using Hermite Long-Short-Term Memory neural network (Kourosh Parand et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kourosh Parand, Saeed Setayeshi, Mir Mohsen Pedram, Ali Yoonesi, Aida Pakniyat. (2024)<br><strong>Solving the decision-making differential equations from eye fixation data in Unity software by using Hermite Long-Short-Term Memory neural network</strong><br><button class=copy-to-clipboard title="Solving the decision-making differential equations from eye fixation data in Unity software by using Hermite Long-Short-Term Memory neural network" index=282>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-282 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-HC, cs-NA, cs.HC, math-NA<br>Keyword Score: 20<br>Keywords: LSTM, LSTM<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13027v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13027v2.pdf filename=2402.13027v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Cognitive decision-making processes are crucial aspects of human behavior, influencing various personal and professional domains. This research delves into the application of differential equations in analyzing decision-making accuracy by leveraging eye-tracking data within a virtual industrial town setting. The study unveils a systematic approach to transforming raw data into a differential equation, essential for deciphering the relationship between eye movements during decision-making processes. Mathematical relationship extraction and variable-parameter definition pave the way for deriving a differential equation that encapsulates the growth of fixations on characters. The key factors in this equation encompass the fixation rate $(\lambda)$ and separation rate $(\mu)$, reflecting user interaction dynamics and their impact on decision-making complexities tied to user engagement with virtual characters. For a comprehensive grasp of decision dynamics, solving this differential equation requires initial fixation counts, fixation rate, and separation rate. The formulation of differential equations incorporates various considerations such as engagement duration, character-player distance, relative speed, and character attributes, enabling the representation of fixation changes, speed dynamics, distance variations, and the effects of character attributes. This comprehensive analysis not only enhances our comprehension of decision-making processes but also provides a foundational framework for predictive modeling and data-driven insights for future research and applications in cognitive science and virtual reality environments.</p></p class="citation"></blockquote><h3 id=23--283316-exploring-ai-assisted-ideation-and-prototyping-for-choreography-yimeng-liu-et-al-2024>(2/3 | 283/316) Exploring AI-assisted Ideation and Prototyping for Choreography (Yimeng Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yimeng Liu, Misha Sra. (2024)<br><strong>Exploring AI-assisted Ideation and Prototyping for Choreography</strong><br><button class=copy-to-clipboard title="Exploring AI-assisted Ideation and Prototyping for Choreography" index=283>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-283 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-HC, cs.HC<br>Keyword Score: 16<br>Keywords: Generative AI, Multi-modal, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13123v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13123v1.pdf filename=2402.13123v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Choreography creation is a <b>multimodal</b> endeavor, demanding cognitive abilities to develop creative ideas and technical expertise to convert choreographic ideas into physical dance movements. Previous endeavors have sought to reduce the complexities in the choreography creation process in both dimensions. Among them, non-AI-based systems have focused on reinforcing cognitive activities by helping analyze and understand dance movements and augmenting physical capabilities by enhancing body expressivity. On the other hand, AI-based methods have helped the creation of novel choreographic materials with <b>generative</b> <b>AI</b> algorithms. The choreography creation process is constrained by time and requires a rich set of resources to stimulate novel ideas, but the need for iterative prototyping and reduced physical dependence have not been adequately addressed by prior research. Recognizing these challenges and the research gap, we present an innovative AI-based choreography-support system. Our goal is to facilitate rapid ideation by utilizing a <b>generative</b> <b>AI</b> model that can produce diverse and novel dance sequences. The system is designed to support iterative digital dance prototyping through an interactive web-based user interface that enables the editing and modification of generated motion. We evaluated our system by inviting six choreographers to analyze its limitations and benefits and present the evaluation results along with potential directions for future work.</p></p class="citation"></blockquote><h3 id=33--284316-data-storytelling-in-data-visualisation-does-it-enhance-the-efficiency-and-effectiveness-of-information-retrieval-and-insights-comprehension-honbo-shao-et-al-2024>(3/3 | 284/316) Data Storytelling in Data Visualisation: Does it Enhance the Efficiency and Effectiveness of Information Retrieval and Insights Comprehension? (Honbo Shao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Honbo Shao, Roberto Martinez-Maldonado, Vanessa Echeverria, Lixiang Yan, Dragan Gasevic. (2024)<br><strong>Data Storytelling in Data Visualisation: Does it Enhance the Efficiency and Effectiveness of Information Retrieval and Insights Comprehension?</strong><br><button class=copy-to-clipboard title="Data Storytelling in Data Visualisation: Does it Enhance the Efficiency and Effectiveness of Information Retrieval and Insights Comprehension?" index=284>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-284 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-HC, cs.HC<br>Keyword Score: 10<br>Keywords: Information Retrieval<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12634v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12634v1.pdf filename=2402.12634v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Data storytelling (DS) is rapidly gaining attention as an approach that integrates data, visuals, and narratives to create data stories that can help a particular audience to comprehend the key messages underscored by the data with enhanced efficiency and effectiveness. It is been posited that DS can be especially advantageous for audiences with limited visualisation literacy, by presenting the data clearly and concisely. However, empirical studies confirming whether data stories indeed provide these benefits over conventional data visualisations are scarce. To bridge this gap, we conducted a study with 103 participants to determine whether DS indeed improve both efficiency and effectiveness in tasks related to <b>information</b> <b>retrieval</b> and insights comprehension. Our findings suggest that data stories do improve the efficiency of comprehension tasks, as well as the effectiveness of comprehension tasks that involve a single insight compared with conventional visualisations. Interestingly, these benefits were not associated with participants&rsquo; visualisation literacy.</p></p class="citation"></blockquote><h2 id=q-bioqm-1>q-bio.QM (1)</h2><h3 id=11--285316-integrating-deep-learning-and-synthetic-biology-a-co-design-approach-for-enhancing-gene-expression-via-n-terminal-coding-sequences-zhanglu-yan-et-al-2024>(1/1 | 285/316) Integrating Deep Learning and Synthetic Biology: A Co-Design Approach for Enhancing Gene Expression via N-terminal Coding Sequences (Zhanglu Yan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhanglu Yan, Weiran Chu, Yuhua Sheng, Kaiwen Tang, Shida Wang, Yanfeng Liu, Weng-Fai Wong. (2024)<br><strong>Integrating Deep Learning and Synthetic Biology: A Co-Design Approach for Enhancing Gene Expression via N-terminal Coding Sequences</strong><br><button class=copy-to-clipboard title="Integrating Deep Learning and Synthetic Biology: A Co-Design Approach for Enhancing Gene Expression via N-terminal Coding Sequences" index=285>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-285 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: q-bio.QM<br>Categories: cs-AI, q-bio-QM, q-bio.QM<br>Keyword Score: 20<br>Keywords: Few-shot, Word2vec<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13297v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13297v1.pdf filename=2402.13297v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>N-terminal coding sequence (NCS) influences gene expression by impacting the translation initiation rate. The NCS optimization problem is to find an NCS that maximizes gene expression. The problem is important in genetic engineering. However, current methods for NCS optimization such as rational design and statistics-guided approaches are labor-intensive yield only relatively small improvements. This paper introduces a deep learning/synthetic biology co-designed <b>few-shot</b> training workflow for NCS optimization. Our method utilizes k-nearest encoding followed by <b>word2vec</b> to encode the NCS, then performs feature extraction using attention mechanisms, before constructing a time-series network for predicting gene expression intensity, and finally a direct search algorithm identifies the optimal NCS with limited training data. We took green fluorescent protein (GFP) expressed by Bacillus subtilis as a reporting protein of NCSs, and employed the fluorescence enhancement factor as the metric of NCS optimization. Within just six iterative experiments, our model generated an NCS (MLD62) that increased average GFP expression by 5.41-fold, outperforming the state-of-the-art NCS designs. Extending our findings beyond GFP, we showed that our engineered NCS (MLD62) can effectively boost the production of N-acetylneuraminic acid by enhancing the expression of the crucial rate-limiting GNA1 gene, demonstrating its practical utility. We have open-sourced our NCS expression database and experimental procedures for public use.</p></p class="citation"></blockquote><h2 id=cssd-4>cs.SD (4)</h2><h3 id=14--286316-guiding-the-underwater-acoustic-target-recognition-with-interpretable-contrastive-learning-yuan-xie-et-al-2024>(1/4 | 286/316) Guiding the underwater acoustic target recognition with interpretable contrastive learning (Yuan Xie et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuan Xie, Jiawei Ren, Ji Xu. (2024)<br><strong>Guiding the underwater acoustic target recognition with interpretable contrastive learning</strong><br><button class=copy-to-clipboard title="Guiding the underwater acoustic target recognition with interpretable contrastive learning" index=286>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-286 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SD<br>Categories: cs-SD, cs.SD, eess-AS<br>Keyword Score: 20<br>Keywords: Contrastive Learning, In-context Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12658v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12658v1.pdf filename=2402.12658v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recognizing underwater targets from acoustic signals is a challenging task owing to the intricate ocean environments and variable underwater channels. While deep learning-based systems have become the mainstream approach for underwater acoustic target recognition, they have faced criticism for their lack of interpretability and weak generalization performance in practical applications. In this work, we apply the class activation mapping (CAM) to generate visual explanations for the predictions of a spectrogram-based recognition system. CAM can help to understand the behavior of recognition models by highlighting the regions of the input features that contribute the most to the prediction. Our explorations reveal that recognition models tend to focus on the low-frequency line spectrum and high-frequency periodic modulation information of underwater signals. Based on the observation, we propose an interpretable <b>contrastive</b> <b>learning</b> <b>(ICL)</b> strategy that employs two encoders to learn from acoustic features with different emphases (line spectrum and modulation information). By imposing constraints between encoders, the proposed strategy can enhance the generalization performance of the recognition system. Our experiments demonstrate that the proposed <b>contrastive</b> <b>learning</b> approach can improve the recognition accuracy and bring significant improvements across various underwater databases.</p></p class="citation"></blockquote><h3 id=24--287316-not-all-weights-are-created-equal-enhancing-energy-efficiency-in-on-device-streaming-speech-recognition-yang-li-et-al-2024>(2/4 | 287/316) Not All Weights Are Created Equal: Enhancing Energy Efficiency in On-Device Streaming Speech Recognition (Yang Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yang Li, Yuan Shangguan, Yuhao Wang, Liangzhen Lai, Ernie Chang, Changsheng Zhao, Yangyang Shi, Vikas Chandra. (2024)<br><strong>Not All Weights Are Created Equal: Enhancing Energy Efficiency in On-Device Streaming Speech Recognition</strong><br><button class=copy-to-clipboard title="Not All Weights Are Created Equal: Enhancing Energy Efficiency in On-Device Streaming Speech Recognition" index=287>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-287 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SD<br>Categories: cs-LG, cs-SD, cs.SD, eess-AS<br>Keyword Score: 10<br>Keywords: Automatic Speech Recognition<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13076v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13076v1.pdf filename=2402.13076v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Power consumption plays an important role in on-device streaming <b>speech</b> <b>recognition,</b> as it has a direct impact on the user experience. This study delves into how weight parameters in <b>speech</b> <b>recognition</b> models influence the overall power consumption of these models. We discovered that the impact of weight parameters on power consumption varies, influenced by factors including how often they are invoked and their placement in memory. Armed with this insight, we developed design guidelines aimed at optimizing on-device <b>speech</b> <b>recognition</b> models. These guidelines focus on minimizing power use without substantially affecting accuracy. Our method, which employs targeted compression based on the varying sensitivities of weight parameters, demonstrates superior performance compared to state-of-the-art compression methods. It achieves a reduction in energy usage of up to 47% while maintaining similar model accuracy and improving the real-time factor.</p></p class="citation"></blockquote><h3 id=34--288316-structure-informed-positional-encoding-for-music-generation-manvi-agarwal-et-al-2024>(3/4 | 288/316) Structure-informed Positional Encoding for Music Generation (Manvi Agarwal et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Manvi Agarwal, Changhong Wang, Gaël Richard. (2024)<br><strong>Structure-informed Positional Encoding for Music Generation</strong><br><button class=copy-to-clipboard title="Structure-informed Positional Encoding for Music Generation" index=288>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-288 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SD<br>Categories: cs-AI, cs-SD, cs.SD, eess-AS<br>Keyword Score: 10<br>Keywords: Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13301v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13301v1.pdf filename=2402.13301v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Music generated by deep learning methods often suffers from a lack of coherence and long-term organization. Yet, multi-scale hierarchical structure is a distinctive feature of music signals. To leverage this information, we propose a structure-informed positional encoding framework for music generation with <b>Transformers.</b> We design three variants in terms of absolute, relative and non-stationary positional information. We comprehensively test them on two symbolic music generation tasks: next-timestep prediction and accompaniment generation. As a comparison, we choose multiple baselines from the literature and demonstrate the merits of our methods using several musically-motivated evaluation metrics. In particular, our methods improve the melodic and structural consistency of the generated pieces.</p></p class="citation"></blockquote><h3 id=44--289316-singvisio-visual-analytics-of-diffusion-model-for-singing-voice-conversion-liumeng-xue-et-al-2024>(4/4 | 289/316) SingVisio: Visual Analytics of Diffusion Model for Singing Voice Conversion (Liumeng Xue et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Liumeng Xue, Chaoren Wang, Mingxuan Wang, Xueyao Zhang, Jun Han, Zhizheng Wu. (2024)<br><strong>SingVisio: Visual Analytics of Diffusion Model for Singing Voice Conversion</strong><br><button class=copy-to-clipboard title="SingVisio: Visual Analytics of Diffusion Model for Singing Voice Conversion" index=289>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-289 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SD<br>Categories: cs-HC, cs-SD, cs.SD, eess-AS<br>Keyword Score: 10<br>Keywords: Diffusion Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12660v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12660v1.pdf filename=2402.12660v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this study, we present SingVisio, an interactive visual analysis system that aims to explain the <b>diffusion</b> <b>model</b> used in singing voice conversion. SingVisio provides a visual display of the generation process in <b>diffusion</b> <b>models,</b> showcasing the step-by-step denoising of the noisy spectrum and its transformation into a clean spectrum that captures the desired singer&rsquo;s timbre. The system also facilitates side-by-side comparisons of different conditions, such as source content, melody, and target timbre, highlighting the impact of these conditions on the <b>diffusion</b> <b>generation</b> process and resulting conversions. Through comprehensive evaluations, SingVisio demonstrates its effectiveness in terms of system design, functionality, explainability, and user-friendliness. It offers users of various backgrounds valuable learning experiences and insights into the <b>diffusion</b> <b>model</b> for singing voice conversion.</p></p class="citation"></blockquote><h2 id=csne-2>cs.NE (2)</h2><h3 id=12--290316-evolutionary-reinforcement-learning-a-systematic-review-and-future-directions-yuanguo-lin-et-al-2024>(1/2 | 290/316) Evolutionary Reinforcement Learning: A Systematic Review and Future Directions (Yuanguo Lin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuanguo Lin, Fan Lin, Guorong Cai, Hong Chen, Lixin Zou, Pengcheng Wu. (2024)<br><strong>Evolutionary Reinforcement Learning: A Systematic Review and Future Directions</strong><br><button class=copy-to-clipboard title="Evolutionary Reinforcement Learning: A Systematic Review and Future Directions" index=290>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-290 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NE<br>Categories: cs-NE, cs.NE<br>Keyword Score: 20<br>Keywords: Fairness, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13296v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13296v1.pdf filename=2402.13296v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In response to the limitations of <b>reinforcement</b> <b>learning</b> and evolutionary algorithms (EAs) in complex problem-solving, Evolutionary <b>Reinforcement</b> <b>Learning</b> (EvoRL) has emerged as a synergistic solution. EvoRL integrates EAs and <b>reinforcement</b> <b>learning,</b> presenting a promising avenue for training intelligent agents. This systematic review firstly navigates through the technological background of EvoRL, examining the symbiotic relationship between EAs and <b>reinforcement</b> <b>learning</b> algorithms. We then delve into the challenges faced by both EAs and <b>reinforcement</b> <b>learning,</b> exploring their interplay and impact on the efficacy of EvoRL. Furthermore, the review underscores the need for addressing open issues related to scalability, adaptability, sample efficiency, adversarial robustness, ethic and <b>fairness</b> within the current landscape of EvoRL. Finally, we propose future directions for EvoRL, emphasizing research avenues that strive to enhance self-adaptation and self-improvement, generalization, interpretability, explainability, and so on. Serving as a comprehensive resource for researchers and practitioners, this systematic review provides insights into the current state of EvoRL and offers a guide for advancing its capabilities in the ever-evolving landscape of artificial intelligence.</p></p class="citation"></blockquote><h3 id=22--291316-sonata-self-adaptive-evolutionary-framework-for-hardware-aware-neural-architecture-search-halima-bouzidi-et-al-2024>(2/2 | 291/316) SONATA: Self-adaptive Evolutionary Framework for Hardware-aware Neural Architecture Search (Halima Bouzidi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Halima Bouzidi, Smail Niar, Hamza Ouarnoughi, El-Ghazali Talbi. (2024)<br><strong>SONATA: Self-adaptive Evolutionary Framework for Hardware-aware Neural Architecture Search</strong><br><button class=copy-to-clipboard title="SONATA: Self-adaptive Evolutionary Framework for Hardware-aware Neural Architecture Search" index=291>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-291 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NE<br>Categories: cs-LG, cs-NE, cs.NE<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13204v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13204v1.pdf filename=2402.13204v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advancements in Artificial Intelligence (AI), driven by Neural Networks (NN), demand innovative neural architecture designs, particularly within the constrained environments of Internet of Things (IoT) systems, to balance performance and efficiency. HW-aware Neural Architecture Search (HW-aware NAS) emerges as an attractive strategy to automate the design of NN using multi-objective optimization approaches, such as evolutionary algorithms. However, the intricate relationship between NN design parameters and HW-aware NAS optimization objectives remains an underexplored research area, overlooking opportunities to effectively leverage this knowledge to guide the search process accordingly. Furthermore, the large amount of evaluation data produced during the search holds untapped potential for refining the optimization strategy and improving the approximation of the Pareto front. Addressing these issues, we propose SONATA, a self-adaptive evolutionary algorithm for HW-aware NAS. Our method leverages adaptive evolutionary operators guided by the learned importance of NN design parameters. Specifically, through tree-based surrogate models and a <b>Reinforcement</b> <b>Learning</b> agent, we aspire to gather knowledge on &lsquo;How&rsquo; and &lsquo;When&rsquo; to evolve NN architectures. Comprehensive evaluations across various NAS search spaces and hardware devices on the ImageNet-1k dataset have shown the merit of SONATA with up to 0.25% improvement in accuracy and up to 2.42x gains in latency and energy. Our SONATA has seen up to sim$93.6% Pareto dominance over the native NSGA-II, further stipulating the importance of self-adaptive evolution operators in HW-aware NAS.</p></p class="citation"></blockquote><h2 id=hep-th-1>hep-th (1)</h2><h3 id=11--292316-rigor-with-machine-learning-from-field-theory-to-the-poincaré-conjecture-sergei-gukov-et-al-2024>(1/1 | 292/316) Rigor with Machine Learning from Field Theory to the Poincaré Conjecture (Sergei Gukov et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sergei Gukov, James Halverson, Fabian Ruehle. (2024)<br><strong>Rigor with Machine Learning from Field Theory to the Poincaré Conjecture</strong><br><button class=copy-to-clipboard title="Rigor with Machine Learning from Field Theory to the Poincaré Conjecture" index=292>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-292 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: hep-th<br>Categories: cs-LG, hep-th, hep-th<br>Keyword Score: 15<br>Keywords: Black Box, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13321v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13321v1.pdf filename=2402.13321v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Machine learning techniques are increasingly powerful, leading to many breakthroughs in the natural sciences, but they are often stochastic, error-prone, and blackbox. How, then, should they be utilized in fields such as theoretical physics and pure mathematics that place a premium on rigor and understanding? In this Perspective we discuss techniques for obtaining rigor in the natural sciences with machine learning. Non-rigorous methods may lead to rigorous results via conjecture generation or verification by <b>reinforcement</b> <b>learning.</b> We survey applications of these techniques-for-rigor ranging from string theory to the smooth $4$d Poincar'e conjecture in low-dimensional topology. One can also imagine building direct bridges between machine learning theory and either mathematics or theoretical physics. As examples, we describe a new approach to field theory motivated by neural network theory, and a theory of Riemannian metric flows induced by neural network gradient descent, which encompasses Perelman&rsquo;s formulation of the Ricci flow that was utilized to resolve the $3$d Poincar'e conjecture.</p></p class="citation"></blockquote><h2 id=csds-7>cs.DS (7)</h2><h3 id=17--293316-efficient-enumeration-of-large-maximal-k-plexes-qihao-cheng-et-al-2024>(1/7 | 293/316) Efficient Enumeration of Large Maximal k-Plexes (Qihao Cheng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qihao Cheng, Da Yan, Tianhao Wu, Lyuheng Yuan, Ji Cheng, Zhongyi Huang, Yang Zhou. (2024)<br><strong>Efficient Enumeration of Large Maximal k-Plexes</strong><br><button class=copy-to-clipboard title="Efficient Enumeration of Large Maximal k-Plexes" index=293>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-293 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DS<br>Categories: cs-DC, cs-DS, cs.DS<br>Keyword Score: 13<br>Keywords: Graph, Pruning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13008v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13008v1.pdf filename=2402.13008v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Finding cohesive subgraphs in a large <b>graph</b> has many important applications, such as community detection and biological network analysis. Clique is often a too strict cohesive structure since communities or biological modules rarely form as cliques for various reasons such as data noise. Therefore, $k$-plex is introduced as a popular clique relaxation, which is a <b>graph</b> where every vertex is adjacent to all but at most $k$ vertices. In this paper, we propose an efficient branch-and-bound algorithm as well as its task-based parallel version to enumerate all maximal $k$-plexes with at least $q$ vertices. Our algorithm adopts an effective search space partitioning approach that provides a good time complexity, a new pivot vertex selection method that reduces candidate vertex size, an effective upper-bounding technique to prune useless branches, and three novel <b>pruning</b> techniques by vertex pairs. Our parallel algorithm uses a timeout mechanism to eliminate straggler tasks, and maximizes cache locality while ensuring load balancing. Extensive experiments show that compared with the state-of-the-art algorithms, our sequential and parallel algorithms enumerate large maximal $k$-plexes with up to $5 \times$ and $18.9 \times$ speedup, respectively. Ablation results also demonstrate that our <b>pruning</b> techniques bring up to $7 \times$ speedup compared with our basic algorithm.</p></p class="citation"></blockquote><h3 id=27--294316-scalable-pattern-matching-in-computation-graphs-luca-mondada-et-al-2024>(2/7 | 294/316) Scalable Pattern Matching in Computation Graphs (Luca Mondada et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Luca Mondada, Pablo Andrés-Martínez. (2024)<br><strong>Scalable Pattern Matching in Computation Graphs</strong><br><button class=copy-to-clipboard title="Scalable Pattern Matching in Computation Graphs" index=294>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-294 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DS<br>Categories: cs-DS, cs.DS, math-CO, quant-ph<br>Keyword Score: 6<br>Keywords: Graph, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13065v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13065v1.pdf filename=2402.13065v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Graph</b> rewriting is a popular tool for the optimisation and modification of <b>graph</b> expressions in domains such as compilers, machine learning and quantum computing. The underlying data structures are often port <b>graphs</b> - <b>graphs</b> with labels at edge endpoints. These port labels greatly simplify pattern matching. A pre-requisite for <b>graph</b> rewriting is the ability to find subgraphs of the input that match known <b>graph</b> identities: the pattern matching problem. We propose a new solution to pattern matching in port <b>graphs.</b> Its novelty lies in the use of a pre-computed data structure that makes the pattern matching runtime complexity independent of the number of patterns. The runtime is bound by the maximum width $w$ and depth $d$ of the patterns, as well as the input <b>graph</b> size $|G|$ as $O(|G| \cdot c^w / w^{1/2} \cdot d)$ with $c = 6.75$. This offers a significant advantage over existing solutions for use cases where patterns have low width and the set of patterns is large and fixed ahead of time. In the context of quantum circuits, pattern width can be limited to qubit number. Quantum superoptimisers may use thousands of rewrite rules on circuits with less than 5 qubits, making them an ideal use case. We provide <b>benchmarks</b> showing that our algorithm offers a 20x speedup over current implementations on a dataset of 10'000 real world patterns describing quantum circuits.</p></p class="citation"></blockquote><h3 id=37--295316-online-matching-on-3-uniform-hypergraphs-sander-borst-et-al-2024>(3/7 | 295/316) Online Matching on $3$-Uniform Hypergraphs (Sander Borst et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sander Borst, Danish Kashaev, Zhuan Khye Koh. (2024)<br><strong>Online Matching on $3$-Uniform Hypergraphs</strong><br><button class=copy-to-clipboard title="Online Matching on $3$-Uniform Hypergraphs" index=295>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-295 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DS<br>Categories: cs-DS, cs.DS, math-OC<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13227v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13227v1.pdf filename=2402.13227v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The online matching problem was introduced by Karp, Vazirani and Vazirani (STOC 1990) on bipartite <b>graphs</b> with vertex arrivals. It is well-known that the optimal competitive ratio is $1-1/e$ for both integral and fractional versions of the problem. Since then, there has been considerable effort to find optimal competitive ratios for other related settings. In this work, we go beyond the <b>graph</b> case and study the online matching problem on $k$-uniform hypergraphs. For $k=3$, we provide an optimal primal-dual fractional algorithm, which achieves a competitive ratio of $(e-1)/(e+1)\approx 0.4621$. As our main technical contribution, we present a carefully constructed adversarial instance, which shows that this ratio is in fact optimal. It combines ideas from known hard instances for bipartite <b>graphs</b> under the edge-arrival and vertex-arrival models. For $k\geq 3$, we give a simple integral algorithm which performs better than greedy when the online nodes have bounded degree. As a corollary, it achieves the optimal competitive ratio of 1/2 on 3-uniform hypergraphs when every online node has degree at most 2. This is because the special case where every online node has degree 1 is equivalent to the edge-arrival model on <b>graphs,</b> for which an upper bound of 1/2 is known.</p></p class="citation"></blockquote><h3 id=47--296316-deterministic-dynamic-edge-colouring-aleksander-b-g-christiansen-2024>(4/7 | 296/316) Deterministic Dynamic Edge-Colouring (Aleksander B. G. Christiansen, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Aleksander B. G. Christiansen. (2024)<br><strong>Deterministic Dynamic Edge-Colouring</strong><br><button class=copy-to-clipboard title="Deterministic Dynamic Edge-Colouring" index=296>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-296 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DS<br>Categories: cs-DS, cs.DS<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13139v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13139v1.pdf filename=2402.13139v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Given a dynamic <b>graph</b> $G$ with $n$ vertices and $m$ edges subject to insertion an deletions of edges, we show how to maintain a $(1+\varepsilon)\Delta$-edge-colouring of $G$ without the use of randomisation. More specifically, we show a deterministic dynamic algorithm with an amortised update time of $2^{\tilde{O}_{\log \varepsilon^{-1}}(\sqrt{\log n})}$ using $(1+\varepsilon)\Delta$ colours. If $\varepsilon^{-1} \in 2^{O(\log^{0.49} n)}$, then our update time is sub-polynomial in $n$. While there exists randomised algorithms maintaining colourings with the same number of colours [Christiansen STOC'23, Duan, He, Zhang SODA'19, Bhattacarya, Costa, Panski, Solomon SODA'24] in polylogarithmic and even constant update time, this is the first deterministic algorithm to go below the greedy threshold of $2\Delta-1$ colours for all input <b>graphs.</b> On the way to our main result, we show how to dynamically maintain a shallow hierarchy of degree-splitters with both recourse and update time in $n^{o(1)}$. We believe that this algorithm might be of independent interest.</p></p class="citation"></blockquote><h3 id=57--297316-locally-rainbow-paths-till-fluschnik-et-al-2024>(5/7 | 297/316) Locally Rainbow Paths (Till Fluschnik et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Till Fluschnik, Leon Kellerhals, Malte Renken. (2024)<br><strong>Locally Rainbow Paths</strong><br><button class=copy-to-clipboard title="Locally Rainbow Paths" index=297>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-297 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DS<br>Categories: cs-DS, cs.DS<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12905v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12905v1.pdf filename=2402.12905v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce the algorithmic problem of finding a locally rainbow path of length $\ell$ connecting two distinguished vertices $s$ and $t$ in a vertex-colored directed <b>graph.</b> Herein, a path is locally rainbow if between any two visits of equally colored vertices, the path traverses consecutively at least $r$ differently colored vertices. This problem generalizes the well-known problem of finding a rainbow path. It finds natural applications whenever there are different types of resources that must be protected from overuse, such as crop sequence optimization or production process scheduling. We show that the problem is computationally intractable even if $r=2$ or if one looks for a locally rainbow among the shortest paths. On the positive side, if one looks for a path that takes only a short detour (i.e., it is slightly longer than the shortest path) and if $r$ is small, the problem can be solved efficiently. Indeed, the running time of the respective algorithm is near-optimal unless the ETH fails.</p></p class="citation"></blockquote><h3 id=67--298316-nearly-optimal-fault-tolerant-distance-oracle-dipan-dey-et-al-2024>(6/7 | 298/316) Nearly Optimal Fault Tolerant Distance Oracle (Dipan Dey et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dipan Dey, Manoj Gupta. (2024)<br><strong>Nearly Optimal Fault Tolerant Distance Oracle</strong><br><button class=copy-to-clipboard title="Nearly Optimal Fault Tolerant Distance Oracle" index=298>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-298 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DS<br>Categories: cs-DS, cs.DS<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12832v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12832v2.pdf filename=2402.12832v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present an $f$-fault tolerant distance oracle for an undirected weighted <b>graph</b> where each edge has an integral weight from $[1 \dots W]$. Given a set $F$ of $f$ edges, as well as a source node $s$ and a destination node $t$, our oracle returns the \emph{shortest path} from $s$ to $t$ avoiding $F$ in $O((cf \log (nW))^{O(f^2)})$ time, where $c > 1$ is a constant. The space complexity of our oracle is $O(f^4n^2\log^2 (nW))$. For a constant $f$, our oracle is nearly optimal both in terms of space and time (barring some logarithmic factor).</p></p class="citation"></blockquote><h3 id=77--299316-distance-recoloring-niranka-banerjee-et-al-2024>(7/7 | 299/316) Distance Recoloring (Niranka Banerjee et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Niranka Banerjee, Christian Engels, Duc A. Hoang. (2024)<br><strong>Distance Recoloring</strong><br><button class=copy-to-clipboard title="Distance Recoloring" index=299>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-299 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DS<br>Categories: cs-DM, cs-DS, cs.DS<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12705v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12705v1.pdf filename=2402.12705v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Coloring a <b>graph</b> is a well known problem and used in many different contexts. Here we want to assign $k \geq 1$ colors to each vertex of a <b>graph</b> $G$ such that each edge has two different colors at each endpoint. Such a vertex-coloring, if exists, is called a feasible coloring of $G$. \textsc{Distance Coloring} is an extension to the standard \textsc{Coloring} problem. Here we want to enforce that every pair of distinct vertices of distance less than or equal to $d$ have different colors, for integers $d \geq 1$ and $k \geq d+1$. Reconfiguration problems ask if two given configurations can be transformed into each other with certain rules. For example, the well-known \textsc{Coloring Reconfiguration} asks if there is a way to change one vertex&rsquo;s color at a time, starting from a feasible given coloring $\alpha$ of a <b>graph</b> $G$ to reach another feasible given coloring $\beta$ of $G$, such that all intermediate colorings are also feasible. In this paper, we study the reconfiguration of distance colorings on certain <b>graph</b> classes. We show that even for planar, bipartite, and $2$-degenerate <b>graphs,</b> reconfiguring distance colorings is $\mathsf{PSPACE}$-complete for $d \geq 2$ and $k = \Omega(d^2)$ via a reduction from the well-known \textsc{Sliding Tokens} problem. Additionally, we show that the problem on split <b>graphs</b> remains $\mathsf{PSPACE}$-complete when $d = 2$ and large $k$ but can be solved in polynomial time when $d \geq 3$ and $k \geq d+1$, and design a quadratic-time algorithm to solve the problem on paths for any $d \geq 2$ and $k \geq d+1$.</p></p class="citation"></blockquote><h2 id=eesssp-1>eess.SP (1)</h2><h3 id=11--300316-szcore-a-seizure-community-open-source-research-evaluation-framework-for-the-validation-of-eeg-based-automated-seizure-detection-algorithms-jonathan-dan-et-al-2024>(1/1 | 300/316) SzCORE: A Seizure Community Open-source Research Evaluation framework for the validation of EEG-based automated seizure detection algorithms (Jonathan Dan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jonathan Dan, Una Pale, Alireza Amirshahi, William Cappelletti, Thorir Mar Ingolfsson, Xiaying Wang, Andrea Cossettini, Adriano Bernini, Luca Benini, Sándor Beniczky, David Atienza, Philippe Ryvlin. (2024)<br><strong>SzCORE: A Seizure Community Open-source Research Evaluation framework for the validation of EEG-based automated seizure detection algorithms</strong><br><button class=copy-to-clipboard title="SzCORE: A Seizure Community Open-source Research Evaluation framework for the validation of EEG-based automated seizure detection algorithms" index=300>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-300 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SP<br>Categories: cs-LG, eess-SP, eess.SP<br>Keyword Score: 13<br>Keywords: Benchmarking, Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13005v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13005v2.pdf filename=2402.13005v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The need for high-quality automated seizure detection algorithms based on electroencephalography (EEG) becomes ever more pressing with the increasing use of ambulatory and long-term EEG monitoring. Heterogeneity in validation methods of these algorithms influences the reported results and makes comprehensive evaluation and comparison challenging. This heterogeneity concerns in particular the choice of datasets, evaluation methodologies, and performance metrics. In this paper, we propose a unified framework designed to establish standardization in the validation of EEG-based seizure detection algorithms. Based on existing guidelines and <b>recommendations,</b> the framework introduces a set of <b>recommendations</b> and standards related to datasets, file formats, EEG data input content, seizure annotation input and output, cross-validation strategies, and performance metrics. We also propose the 10-20 seizure detection <b>benchmark,</b> a machine-learning <b>benchmark</b> based on public datasets converted to a standardized format. This <b>benchmark</b> defines the machine-learning task as well as reporting metrics. We illustrate the use of the <b>benchmark</b> by evaluating a set of existing seizure detection algorithms. The SzCORE (Seizure Community Open-source Research Evaluation) framework and <b>benchmark</b> are made publicly available along with an open-source software library to facilitate research use, while enabling rigorous evaluation of the clinical significance of the algorithms, fostering a collective effort to more optimally detect seizures to improve the lives of people with epilepsy.</p></p class="citation"></blockquote><h2 id=econth-1>econ.TH (1)</h2><h3 id=11--301316-stable-matching-as-transportation-federico-echenique-et-al-2024>(1/1 | 301/316) Stable matching as transportation (Federico Echenique et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Federico Echenique, Joseph Root, Fedor Sandomirskiy. (2024)<br><strong>Stable matching as transportation</strong><br><button class=copy-to-clipboard title="Stable matching as transportation" index=301>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-301 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: econ.TH<br>Categories: 91B68, 90C08, cs-GT, econ-TH, econ.TH<br>Keyword Score: 10<br>Keywords: Fairness<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13378v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13378v1.pdf filename=2402.13378v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We study matching markets with aligned preferences and establish a connection between common design objectives &ndash; stability, efficiency, and <b>fairness</b> &ndash; and the theory of optimal transport. Optimal transport gives new insights into the structural properties of matchings obtained from pursuing these objectives, and into the trade-offs between different objectives. Matching markets with aligned preferences provide a tractable stylized model capturing supply-demand imbalances in a range of settings such as partnership formation, school choice, organ donor exchange, and markets with transferable utility where bargaining over transfers happens after a match is formed.</p></p class="citation"></blockquote><h2 id=csar-2>cs.AR (2)</h2><h3 id=12--302316-enabling-efficient-hybrid-systolic-computation-in-shared-l1-memory-manycore-clusters-sergio-mazzola-et-al-2024>(1/2 | 302/316) Enabling Efficient Hybrid Systolic Computation in Shared L1-Memory Manycore Clusters (Sergio Mazzola et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sergio Mazzola, Samuel Riedel, Luca Benini. (2024)<br><strong>Enabling Efficient Hybrid Systolic Computation in Shared L1-Memory Manycore Clusters</strong><br><button class=copy-to-clipboard title="Enabling Efficient Hybrid Systolic Computation in Shared L1-Memory Manycore Clusters" index=302>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-302 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AR<br>Categories: cs-AR, cs.AR<br>Keyword Score: 10<br>Keywords: Convolution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12986v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12986v1.pdf filename=2402.12986v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Systolic arrays and shared L1-memory manycore clusters are commonly used architectural paradigms that offer different trade-offs to accelerate parallel workloads. While the first excel with regular dataflow at the cost of rigid architectures and complex programming models, the second are versatile and easy to program but require explicit data flow management and synchronization. This work aims at enabling efficient systolic execution on shared L1-memory manycore clusters. We devise a flexible architecture where small and energy-efficient RISC-V cores act as the systolic array&rsquo;s processing elements (PEs) and can form diverse, reconfigurable systolic topologies through queues mapped in the cluster&rsquo;s shared memory. We introduce two low-overhead RISC-V ISA extensions for efficient systolic execution, namely Xqueue and Queue-linked registers (QLRs), which support queue management in hardware. The Xqueue extension enables single-instruction access to shared-memory-mapped queues, while QLRs allow implicit and autonomous access to them, relieving the cores of explicit communication instructions. We demonstrate Xqueue and QLRs in MemPool, an open-source manycore cluster with 256 PEs, and analyze the hybrid systolic-shared-memory architecture&rsquo;s trade-offs on matrix multiplication, <b>convolution,</b> and FFT kernels. For an area increase of just 6%, our hybrid architecture almost doubles MemPool&rsquo;s compute unit utilization to up to 95% and significantly improves energy efficiency, achieving up to 63% of power spent in the PEs. In typical conditions (TT/0.80V/25{\deg}C) in a 22nm FDX technology, our hybrid architecture runs at 600MHz with no frequency degradation and is up to 64% more energy efficient than the shared-memory baseline, achieving up to 208GOPS/W.</p></p class="citation"></blockquote><h3 id=22--303316-sat-based-exact-modulo-scheduling-mapping-for-resource-constrained-cgras-cristian-tirelli-et-al-2024>(2/2 | 303/316) SAT-based Exact Modulo Scheduling Mapping for Resource-Constrained CGRAs (Cristian Tirelli et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Cristian Tirelli, Juan Sapriza, Rubén Rodríguez Álvarez, Lorenzo Ferretti, Benoît Denkinger, Giovanni Ansaloni, José Miranda Calero, David Atienza, Laura Pozzi. (2024)<br><strong>SAT-based Exact Modulo Scheduling Mapping for Resource-Constrained CGRAs</strong><br><button class=copy-to-clipboard title="SAT-based Exact Modulo Scheduling Mapping for Resource-Constrained CGRAs" index=303>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-303 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AR<br>Categories: cs-AR, cs.AR<br>Keyword Score: 6<br>Keywords: Graph, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12834v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12834v1.pdf filename=2402.12834v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Coarse-Grain Reconfigurable Arrays (CGRAs) represent emerging low-power architectures designed to accelerate Compute-Intensive Loops (CILs). The effectiveness of CGRAs in providing acceleration relies on the quality of mapping: how efficiently the CIL is compiled onto the platform. State of the Art (SoA) compilation techniques utilize modulo scheduling to minimize the Iteration Interval (II) and use <b>graph</b> algorithms like Max-Clique Enumeration to address mapping challenges. Our work approaches the mapping problem through a satisfiability (SAT) formulation. We introduce the Kernel Mobility Schedule (KMS), an ad-hoc schedule used with the Data Flow <b>Graph</b> and CGRA architectural information to generate Boolean statements that, when satisfied, yield a valid mapping. Experimental results demonstrate SAT-MapIt outperforming SoA alternatives in almost 50% of explored <b>benchmarks.</b> Additionally, we evaluated the mapping results in a synthesizable CGRA design and emphasized the run-time metrics trends, i.e. energy efficiency and latency, across different CILs and CGRA sizes. We show that a hardware-agnostic analysis performed on compiler-level metrics can optimally prune the architectural design space, while still retaining Pareto-optimal configurations. Moreover, by exploring how implementation details impact cost and performance on real hardware, we highlight the importance of holistic software-to-hardware mapping flows, as the one presented herein.</p></p class="citation"></blockquote><h2 id=physicscomp-ph-1>physics.comp-ph (1)</h2><h3 id=11--304316-how-temporal-unrolling-supports-neural-physics-simulators-bjoern-list-et-al-2024>(1/1 | 304/316) How Temporal Unrolling Supports Neural Physics Simulators (Bjoern List et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bjoern List, Li-Wei Chen, Kartik Bali, Nils Thuerey. (2024)<br><strong>How Temporal Unrolling Supports Neural Physics Simulators</strong><br><button class=copy-to-clipboard title="How Temporal Unrolling Supports Neural Physics Simulators" index=304>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-304 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: physics.comp-ph<br>Categories: cs-LG, physics-comp-ph, physics.comp-ph<br>Keyword Score: 10<br>Keywords: Distribution Shift, Distribution Shift<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12971v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12971v1.pdf filename=2402.12971v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Unrolling training trajectories over time strongly influences the inference accuracy of neural network-augmented physics simulators. We analyze these effects by studying three variants of training neural networks on discrete ground truth trajectories. In addition to commonly used one-step setups and fully differentiable unrolling, we include a third, less widely used variant: unrolling without temporal gradients. Comparing networks trained with these three modalities makes it possible to disentangle the two dominant effects of unrolling, training <b>distribution</b> <b>shift</b> and long-term gradients. We present a detailed study across physical systems, network sizes, network architectures, training setups, and test scenarios. It provides an empirical basis for our main findings: A non-differentiable but unrolled training setup supported by a numerical solver can yield 4.5-fold improvements over a fully differentiable prediction setup that does not utilize this solver. We also quantify a difference in the accuracy of models trained in a fully differentiable setup compared to their non-differentiable counterparts. While differentiable setups perform best, the accuracy of unrolling without temporal gradients comes comparatively close. Furthermore, we empirically show that these behaviors are invariant to changes in the underlying physical system, the network architecture and size, and the numerical scheme. These results motivate integrating non-differentiable numerical simulators into training setups even if full differentiability is unavailable. We also observe that the convergence rate of common neural architectures is low compared to numerical algorithms. This encourages the use of hybrid approaches combining neural and numerical algorithms to utilize the benefits of both.</p></p class="citation"></blockquote><h2 id=csit-1>cs.IT (1)</h2><h3 id=11--305316-quantifying-privacy-via-information-density-leonhard-grosse-et-al-2024>(1/1 | 305/316) Quantifying Privacy via Information Density (Leonhard Grosse et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Leonhard Grosse, Sara Saeidian, Parastoo Sadeghi, Tobias J. Oechtering, Mikael Skoglund. (2024)<br><strong>Quantifying Privacy via Information Density</strong><br><button class=copy-to-clipboard title="Quantifying Privacy via Information Density" index=305>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-305 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: 94A17, H-1-1, cs-CR, cs-IT, cs.IT, math-IT<br>Keyword Score: 10<br>Keywords: Differential Privacy<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12967v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12967v1.pdf filename=2402.12967v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We examine the relationship between privacy metrics that utilize information density to measure information leakage between a private and a disclosed random variable. Firstly, we prove that bounding the information density from above or below in turn implies a lower or upper bound on the information density, respectively. Using this result, we establish new relationships between local information privacy, asymmetric local information privacy, pointwise maximal leakage and local <b>differential</b> <b>privacy.</b> We further provide applications of these relations to privacy mechanism design. Furthermore, we provide statements showing the equivalence between a lower bound on information density and risk-averse adversaries. More specifically, we prove an equivalence between a guessing framework and a cost-function framework that result in the desired lower bound on the information density.</p></p class="citation"></blockquote><h2 id=csdl-1>cs.DL (1)</h2><h3 id=11--306316-a-literature-review-of-literature-reviews-in-pattern-analysis-and-machine-intelligence-penghai-zhao-et-al-2024>(1/1 | 306/316) A Literature Review of Literature Reviews in Pattern Analysis and Machine Intelligence (Penghai Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Penghai Zhao, Xin Zhang, Ming-Ming Cheng, Jian Yang, Xiang Li. (2024)<br><strong>A Literature Review of Literature Reviews in Pattern Analysis and Machine Intelligence</strong><br><button class=copy-to-clipboard title="A Literature Review of Literature Reviews in Pattern Analysis and Machine Intelligence" index=306>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-306 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DL<br>Categories: cs-AI, cs-CV, cs-DL, cs.DL<br>Keyword Score: 10<br>Keywords: Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12928v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12928v2.pdf filename=2402.12928v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>By consolidating scattered knowledge, the literature review provides a comprehensive understanding of the investigated topic. However, excessive reviews, especially in the booming field of pattern analysis and machine intelligence (PAMI), raise concerns for both researchers and reviewers. In response to these concerns, this Analysis aims to provide a thorough review of reviews in the PAMI field from diverse perspectives. First, <b>large</b> <b>language</b> <b>model-empowered</b> bibliometric indicators are proposed to evaluate literature reviews automatically. To facilitate this, a meta-data database dubbed RiPAMI, and a topic dataset are constructed, which are utilized to obtain statistical characteristics of PAMI reviews. Unlike traditional bibliometric measurements, the proposed article-level indicators provide real-time and field-normalized quantified assessments of reviews without relying on user-defined keywords. Second, based on these indicators, the study presents comparative analyses of different reviews, unveiling the characteristics of publications across various fields, periods, and journals. The newly emerging AI-generated literature reviews are also appraised, and the observed differences suggest that most AI-generated reviews still lag behind human-authored reviews in several aspects. Third, we briefly provide a subjective evaluation of representative PAMI reviews and introduce a paper structure-based typology of literature reviews. This typology may improve the clarity and effectiveness for scholars in reading and writing reviews, while also serving as a guide for AI systems in generating well-organized reviews. Finally, this Analysis offers insights into the current challenges of literature reviews and envisions future directions for their development.</p></p class="citation"></blockquote><h2 id=cslo-1>cs.LO (1)</h2><h3 id=11--307316-modular-assurance-of-complex-systems-using-contract-based-design-principles-dag-mcgeorge-et-al-2024>(1/1 | 307/316) Modular Assurance of Complex Systems Using Contract-Based Design Principles (Dag McGeorge et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dag McGeorge, Jon Arne Glomsrud. (2024)<br><strong>Modular Assurance of Complex Systems Using Contract-Based Design Principles</strong><br><button class=copy-to-clipboard title="Modular Assurance of Complex Systems Using Contract-Based Design Principles" index=307>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-307 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LO<br>Categories: cs-LO, cs-SE, cs.LO<br>Keyword Score: 10<br>Keywords: Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12804v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12804v1.pdf filename=2402.12804v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A growing number of safety-critical industries agree that building confidence in complex systems can be achieved through evidence and structured argumentation framed in assurance cases. Nevertheless, assurance cases can easily become too rigorous and difficult to develop and maintain when applied to complex systems. Therefore, we propose to use contract-based development (CBD), a method to manage complexity originally developed in computer science, to simplify assurance cases by modularizing them. This paper will not only <b>summarize</b> relevant previous work such as constructing consistent modular assurance cases using CBD, but more importantly also propose a novel approach to integrate CBD with the argumentation in assurance case modules. This approach will allow interdisciplinary subject-matter and domain experts to build assurance cases together without even knowing about CBD. This helps subject matter experts outside of computer science to reap benefits from CBD and helps with interdisciplinary co-development of assurance cases that cover all the required fields. This paper motivates four rules of thumb aimed to help practitioners developing high-quality modular assurance cases. It also explains how modularization of assurance is an enabler for multi-concern assurance that accounts for the inter-dependency of different concerns such as safety, security and performance.</p></p class="citation"></blockquote><h2 id=eessiv-2>eess.IV (2)</h2><h3 id=12--308316-denoising-oct-images-using-steered-mixture-of-experts-with-multi-model-inference-aytaç-özkan-et-al-2024>(1/2 | 308/316) Denoising OCT Images Using Steered Mixture of Experts with Multi-Model Inference (Aytaç Özkan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Aytaç Özkan, Elena Stoykova, Thomas Sikora, Violeta Madjarova. (2024)<br><strong>Denoising OCT Images Using Steered Mixture of Experts with Multi-Model Inference</strong><br><button class=copy-to-clipboard title="Denoising OCT Images Using Steered Mixture of Experts with Multi-Model Inference" index=308>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-308 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 10<br>Keywords: Autoencoder<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12735v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12735v1.pdf filename=2402.12735v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In Optical Coherence Tomography (OCT), speckle noise significantly hampers image quality, affecting diagnostic accuracy. Current methods, including traditional filtering and deep learning techniques, have limitations in noise reduction and detail preservation. Addressing these challenges, this study introduces a novel denoising algorithm, Block-Matching Steered-Mixture of Experts with Multi-Model Inference and <b>Autoencoder</b> (BM-SMoE-AE). This method combines block-matched implementation of the SMoE algorithm with an enhanced <b>autoencoder</b> architecture, offering efficient speckle noise reduction while retaining critical image details. Our method stands out by providing improved edge definition and reduced processing time. Comparative analysis with existing denoising techniques demonstrates the superior performance of BM-SMoE-AE in maintaining image integrity and enhancing OCT image usability for medical diagnostics.</p></p class="citation"></blockquote><h3 id=22--309316-wmh_seg-transformer-based-u-net-for-robust-and-automatic-white-matter-hyperintensity-segmentation-across-15t-3t-and-7t-jinghang-li-et-al-2024>(2/2 | 309/316) wmh_seg: Transformer based U-Net for Robust and Automatic White Matter Hyperintensity Segmentation across 1.5T, 3T and 7T (Jinghang Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jinghang Li, Tales Santini, Yuanzhe Huang, Joseph M. Mettenburg, Tamer S. Ibrahim, Howard J. Aizenstein, Minjie Wu. (2024)<br><strong>wmh_seg: Transformer based U-Net for Robust and Automatic White Matter Hyperintensity Segmentation across 1.5T, 3T and 7T</strong><br><button class=copy-to-clipboard title="wmh_seg: Transformer based U-Net for Robust and Automatic White Matter Hyperintensity Segmentation across 1.5T, 3T and 7T" index=309>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-309 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 10<br>Keywords: Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12701v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12701v1.pdf filename=2402.12701v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>White matter hyperintensity (WMH) remains the top imaging biomarker for neurodegenerative diseases. Robust and accurate segmentation of WMH holds paramount significance for neuroimaging studies. The growing shift from 3T to 7T MRI necessitates robust tools for harmonized segmentation across field strengths and artifacts. Recent deep learning models exhibit promise in WMH segmentation but still face challenges, including diverse training data representation and limited analysis of MRI artifacts&rsquo; impact. To address these, we introduce wmh_seg, a novel deep learning model leveraging a <b>transformer-based</b> encoder from SegFormer. wmh_seg is trained on an unmatched dataset, including 1.5T, 3T, and 7T FLAIR images from various sources, alongside with artificially added MR artifacts. Our approach bridges gaps in training diversity and artifact analysis. Our model demonstrated stable performance across magnetic field strengths, scanner manufacturers, and common MR imaging artifacts. Despite the unique inhomogeneity artifacts on ultra-high field MR images, our model still offers robust and stable segmentation on 7T FLAIR images. Our model, to date, is the first that offers quality white matter lesion segmentation on 7T FLAIR images.</p></p class="citation"></blockquote><h2 id=cspl-1>cs.PL (1)</h2><h3 id=11--310316-getting-into-the-flow-towards-better-type-error-messages-for-constraint-based-type-inference-ishan-bhanuka-et-al-2024>(1/1 | 310/316) Getting into the Flow: Towards Better Type Error Messages for Constraint-Based Type Inference (Ishan Bhanuka et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ishan Bhanuka, Lionel Parreaux, David Binder, Jonathan Immanuel Brachthäuser. (2024)<br><strong>Getting into the Flow: Towards Better Type Error Messages for Constraint-Based Type Inference</strong><br><button class=copy-to-clipboard title="Getting into the Flow: Towards Better Type Error Messages for Constraint-Based Type Inference" index=310>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-310 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.PL<br>Categories: cs-HC, cs-PL, cs.PL<br>Keyword Score: 10<br>Keywords: Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12637v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12637v1.pdf filename=2402.12637v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Creating good type error messages for constraint-based type inference systems is difficult. Typical type error messages reflect implementation details of the underlying constraint-solving algorithms rather than the specific factors leading to type mismatches. We propose using subtyping constraints that capture data flow to classify and explain type errors. Our algorithm explains type errors as faulty data flows, which programmers are already used to <b>reasoning</b> about, and illustrates these data flows as sequences of relevant program locations. We show that our ideas and algorithm are not limited to languages with subtyping, as they can be readily integrated with Hindley-Milner type inference. In addition to these core contributions, we present the results of a user study to evaluate the quality of our messages compared to other implementations. While the quantitative evaluation does not show that flow-based messages improve the localization or understanding of the causes of type errors, the qualitative evaluation suggests a real need and demand for flow-based messages.</p></p class="citation"></blockquote><h2 id=cscg-3>cs.CG (3)</h2><h3 id=13--311316-greedy-monochromatic-island-partitions-steven-van-den-broek-et-al-2024>(1/3 | 311/316) Greedy Monochromatic Island Partitions (Steven van den Broek et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Steven van den Broek, Wouter Meulemans, Bettina Speckmann. (2024)<br><strong>Greedy Monochromatic Island Partitions</strong><br><button class=copy-to-clipboard title="Greedy Monochromatic Island Partitions" index=311>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-311 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CG<br>Categories: cs-CG, cs.CG<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13340v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13340v1.pdf filename=2402.13340v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Constructing partitions of colored points is a well-studied problem in discrete and computational <b>geometry.</b> We study the problem of creating a minimum-cardinality partition into monochromatic islands. Our input is a set $S$ of $n$ points in the plane where each point has one of $k \geq 2$ colors. A set of points is monochromatic if it contains points of only one color. An island $I$ is a subset of $S$ such that $\mathcal{CH}(I) \cap S = I$, where $\mathcal{CH}(I)$ denotes the convex hull of $I$. We identify an island with its convex hull; therefore, a partition into islands has the additional requirement that the convex hulls of the islands are pairwise-disjoint. We present three greedy algorithms for constructing island partitions and analyze their approximation ratios.</p></p class="citation"></blockquote><h3 id=23--312316-clustered-planarity-variants-for-level-graphs-simon-d-fink-et-al-2024>(2/3 | 312/316) Clustered Planarity Variants for Level Graphs (Simon D. Fink et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Simon D. Fink, Matthias Pfretzschner, Ignaz Rutter, Marie Diana Sieper. (2024)<br><strong>Clustered Planarity Variants for Level Graphs</strong><br><button class=copy-to-clipboard title="Clustered Planarity Variants for Level Graphs" index=312>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-312 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CG<br>Categories: cs-CG, cs-DS, cs.CG<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13153v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13153v1.pdf filename=2402.13153v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We consider variants of the clustered planarity problem for level-planar drawings. So far, only convex clusters have been studied in this setting. We introduce two new variants that both insist on a level-planar drawing of the input <b>graph</b> but relax the requirements on the shape of the clusters. In unrestricted Clustered Level Planarity (uCLP) we only require that they are bounded by simple closed curves that enclose exactly the vertices of the cluster and cross each edge of the <b>graph</b> at most once. The problem y-monotone Clustered Level Planarity (y-CLP) requires that additionally it must be possible to augment each cluster with edges that do not cross the cluster boundaries so that it becomes connected while the <b>graph</b> remains level-planar, thereby mimicking a classic characterization of clustered planarity in the level-planar setting. We give a polynomial-time algorithm for uCLP if the input <b>graph</b> is biconnected and has a single source. By contrast, we show that y-CLP is hard under the same restrictions and it remains NP-hard even if the number of levels is bounded by a constant and there is only a single non-trivial cluster.</p></p class="citation"></blockquote><h3 id=33--313316-faster-and-deterministic-subtrajectory-clustering-ivor-van-der-hoog-et-al-2024>(3/3 | 313/316) Faster and Deterministic Subtrajectory Clustering (Ivor van der Hoog et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ivor van der Hoog, Thijs van der Horst, Tim Ophelders. (2024)<br><strong>Faster and Deterministic Subtrajectory Clustering</strong><br><button class=copy-to-clipboard title="Faster and Deterministic Subtrajectory Clustering" index=313>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-313 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CG<br>Categories: F-2-2, cs-CG, cs.CG<br>Keyword Score: 3<br>Keywords: Clustering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13117v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13117v1.pdf filename=2402.13117v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Given a trajectory $T$ and a distance $\Delta$, we wish to find a set $C$ of curves of complexity at most $\ell$, such that we can cover $T$ with subcurves that each are within Fr'echet distance $\Delta$ to at least one curve in $C$. We call $C$ an $(\ell,\Delta)$-clustering and aim to find an $(\ell,\Delta)$-clustering of minimum cardinality. This problem was introduced by Akitaya $et$ $al.$ (2021) and shown to be NP-complete. The main focus has therefore been on bicriterial approximation algorithms, allowing for the <b>clustering</b> to be an $(\ell, \Theta(\Delta))$-clustering of roughly optimal size. We present algorithms that construct $(\ell,4\Delta)$-clusterings of $\mathcal{O}(k \log n)$ size, where $k$ is the size of the optimal $(\ell, \Delta)$-clustering. For the discrete Fr'echet distance, we use $\mathcal{O}(n \ell \log n)$ space and $\mathcal{O}(k n^2 \log^3 n)$ deterministic worst case time. For the continuous Fr'echet distance, we use $\mathcal{O}(n^2 \log n)$ space and $\mathcal{O}(k n^3 \log^3 n)$ time. Our algorithms significantly improve upon the <b>clustering</b> quality (improving the approximation factor in $\Delta$) and size (whenever $\ell \in \Omega(\log n)$). We offer deterministic running times comparable to known expected bounds. Additionally, in the continuous setting, we give a near-linear improvement upon the space usage. When compared only to deterministic results, we offer a near-linear speedup and a near-quadratic improvement in the space usage. When we may restrict ourselves to only considering clusters where all subtrajectories are vertex-to-vertex subcurves, we obtain even better results under the continuous Fr'echet distance. Our algorithm becomes near quadratic and uses space that is near linear in $n \ell$.</p></p class="citation"></blockquote><h2 id=mathco-1>math.CO (1)</h2><h3 id=11--314316-an-improved-lower-bound-on-the-number-of-pseudoline-arrangements-fernando-cortés-kühnast-et-al-2024>(1/1 | 314/316) An Improved Lower Bound on the Number of Pseudoline Arrangements (Fernando Cortés Kühnast et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fernando Cortés Kühnast, Stefan Felsner, Manfred Scheucher. (2024)<br><strong>An Improved Lower Bound on the Number of Pseudoline Arrangements</strong><br><button class=copy-to-clipboard title="An Improved Lower Bound on the Number of Pseudoline Arrangements" index=314>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-314 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.CO<br>Categories: G-2-1, cs-CG, cs-DM, math-CO, math.CO<br>Keyword Score: 5<br>Keywords: Geometry<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13107v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13107v1.pdf filename=2402.13107v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Arrangements of pseudolines are classic objects in discrete and computational <b>geometry.</b> They have been studied with increasing intensity since their introduction almost 100 years ago. The study of the number $B_n$ of non-isomorphic simple arrangements of $n$ pseudolines goes back to Goodman and Pollack, Knuth, and others. It is known that $B_n$ is in the order of $2^{\Theta(n^2)}$ and finding asymptotic bounds on $b_n = \frac{\log_2(B_n)}{n^2}$ remains a challenging task. In 2011, Felsner and Valtr showed that $0.1887 \leq b_n \le 0.6571$ for sufficiently large $n$. The upper bound remains untouched but in 2020 Dumitrescu and Mandal improved the lower bound constant to $0.2083$. Their approach utilizes the known values of $B_n$ for up to $n=12$. We tackle the lower bound with a dynamic programming scheme. Our new bound is $b_n \geq 0.2526$ for sufficiently large $n$. The result is based on a delicate interplay of theoretical ideas and computer assistance.</p></p class="citation"></blockquote><h2 id=mathag-1>math.AG (1)</h2><h3 id=11--315316-game-theory-of-undirected-graphical-models-irem-portakal-et-al-2024>(1/1 | 315/316) Game theory of undirected graphical models (Irem Portakal et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Irem Portakal, Javier Sendra-Arranz. (2024)<br><strong>Game theory of undirected graphical models</strong><br><button class=copy-to-clipboard title="Game theory of undirected graphical models" index=315>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-315 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.AG<br>Categories: 14Q05, 91A80, 91A06, 62R01, 14A10, cs-GT, math-AG, math.AG<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.13246v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.13246v1.pdf filename=2402.13246v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We model an $n$-player game $X$ in normal form via undirected discrete graphical models where the discrete random variables represent the players and their state spaces are the set of pure strategies. There exists an edge between the vertices of the graphical model $G$ whenever there is a dependency between the associated players. We study the Spohn conditional independence (CI) variety $\mathcal{V}<em>{X,\mathcal{C}}$, which is the intersection of the independence model $\mathcal{M}</em>{\text{global}(G)}$ with the Spohn variety of the game $X$. We prove a conjecture by the first author and Sturmfels that $\mathcal{V}<em>{X,\mathcal{C}}$ is of codimension $n$ in $\mathcal{M}</em>{\mathcal{C}}$ for a generic game $X$ with binary choices. In the case where the undirected <b>graph</b> is a disjoint union of cliques, we analyze certain algebro-geometric features of Spohn CI varieties and prove affine universality theorems.</p></p class="citation"></blockquote><h2 id=cscc-1>cs.CC (1)</h2><h3 id=11--316316-optimal-pspace-hardness-of-approximating-set-cover-reconfiguration-shuichi-hirahara-et-al-2024>(1/1 | 316/316) Optimal PSPACE-hardness of Approximating Set Cover Reconfiguration (Shuichi Hirahara et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shuichi Hirahara, Naoto Ohsaka. (2024)<br><strong>Optimal PSPACE-hardness of Approximating Set Cover Reconfiguration</strong><br><button class=copy-to-clipboard title="Optimal PSPACE-hardness of Approximating Set Cover Reconfiguration" index=316>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-316 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CC<br>Categories: cs-CC, cs-DM, cs-DS, cs.CC<br>Keyword Score: 3<br>Keywords: Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.12645v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.12645v1.pdf filename=2402.12645v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the Minmax Set Cover Reconfiguration problem, given a set system $\mathcal{F}$ over a universe and its two covers $\mathcal{C}^\mathsf{start}$ and $\mathcal{C}^\mathsf{goal}$ of size $k$, we wish to transform $\mathcal{C}^\mathsf{start}$ into $\mathcal{C}^\mathsf{goal}$ by repeatedly adding or removing a single set of $\mathcal{F}$ while covering the universe in any intermediate state. Then, the objective is to minimize the maximize size of any intermediate cover during transformation. We prove that Minmax Set Cover Reconfiguration and Minmax Dominating Set Reconfiguration are $\mathsf{PSPACE}$-hard to approximate within a factor of $2-\frac{1}{\operatorname{polyloglog} N}$, where $N$ is the size of the universe and the number of vertices in a <b>graph,</b> respectively, improving upon Ohsaka (SODA 2024) and Karthik C. S. and Manurangsi (2023). This is the first result that exhibits a sharp threshold for the approximation factor of any reconfiguration problem because both problems admit a $2$-factor approximation algorithm as per Ito, Demaine, Harvey, Papadimitriou, Sideri, Uehara, and Uno (Theor. Comput. Sci., 2011). Our proof is based on a reconfiguration analogue of the FGLSS reduction from Probabilistically Checkable Reconfiguration Proofs of Hirahara and Ohsaka (2024). We also prove that for any constant $\varepsilon \in (0,1)$, Minmax Hypergraph Vertex Cover Reconfiguration on $\operatorname{poly}(\varepsilon^{-1})$-uniform hypergraphs is $\mathsf{PSPACE}$-hard to approximate within a factor of $2-\varepsilon$.</p></p class="citation"></blockquote></div><div class="row pl-3 pr-3"><div class="col-md-6 share-buttons"></div></div><hr><div class="row next-prev-navigator"><div class="col-md-6 previous-article"><a href=/akitenkrad-blog/posts/arxiv/202402/20240221000000/ title="arXiv @ 2024.02.21" class="btn btn-outline-info"><div><i class="fas fa-chevron-circle-left"></i> Prev</div><div class=next-prev-text>arXiv @ 2024.02.21</div></a></div><div class="col-md-6 next-article"><a href=/akitenkrad-blog/posts/arxiv/202402/20240223000000/ title="arXiv @ 2024.02.23" class="btn btn-outline-info"><div>Next <i class="fas fa-chevron-circle-right"></i></div><div class=next-prev-text>arXiv @ 2024.02.23</div></a></div></div><hr></div></div></div><a id=scroll-to-top class=btn><i class="fas fa-chevron-circle-up"></i></a></section><section class=toc-section id=toc-section><div class=toc-holder><h5 class="text-center pl-3">Table of Contents</h5><hr><div class=toc><nav id=TableOfContents><ul><li><a href=#primary-categories>Primary Categories</a></li><li><a href=#keywords>Keywords</a></li><li><a href=#cscl-89>cs.CL (89)</a><ul><li><a href=#189--1316-benchmarking-retrieval-augmented-generation-for-medicine-guangzhi-xiong-et-al-2024>(1/89 | 1/316) Benchmarking Retrieval-Augmented Generation for Medicine (Guangzhi Xiong et al., 2024)</a></li><li><a href=#289--2316-can-gnn-be-good-adapter-for-llms-xuanwen-huang-et-al-2024>(2/89 | 2/316) Can GNN be Good Adapter for LLMs? (Xuanwen Huang et al., 2024)</a></li><li><a href=#389--3316-effective-and-efficient-conversation-retrieval-for-dialogue-state-tracking-with-implicit-text-summaries-seanie-lee-et-al-2024>(3/89 | 3/316) Effective and Efficient Conversation Retrieval for Dialogue State Tracking with Implicit Text Summaries (Seanie Lee et al., 2024)</a></li><li><a href=#489--4316-promptkd-distilling-student-friendly-knowledge-for-generative-language-models-via-prompt-tuning-gyeongman-kim-et-al-2024>(4/89 | 4/316) PromptKD: Distilling Student-Friendly Knowledge for Generative Language Models via Prompt Tuning (Gyeongman Kim et al., 2024)</a></li><li><a href=#589--5316-evograd-a-dynamic-take-on-the-winograd-schema-challenge-with-human-adversaries-jing-han-sun-et-al-2024>(5/89 | 5/316) EvoGrad: A Dynamic Take on the Winograd Schema Challenge with Human Adversaries (Jing Han Sun et al., 2024)</a></li><li><a href=#689--6316-me-llama-foundation-large-language-models-for-medical-applications-qianqian-xie-et-al-2024>(6/89 | 6/316) Me LLaMA: Foundation Large Language Models for Medical Applications (Qianqian Xie et al., 2024)</a></li><li><a href=#789--7316-a-survey-on-knowledge-distillation-of-large-language-models-xiaohan-xu-et-al-2024>(7/89 | 7/316) A Survey on Knowledge Distillation of Large Language Models (Xiaohan Xu et al., 2024)</a></li><li><a href=#889--8316-opdai-at-semeval-2024-task-6-small-llms-can-accelerate-hallucination-detection-with-weakly-supervised-data-chengcheng-wei-et-al-2024>(8/89 | 8/316) OPDAI at SemEval-2024 Task 6: Small LLMs can Accelerate Hallucination Detection with Weakly Supervised Data (Chengcheng Wei et al., 2024)</a></li><li><a href=#989--9316-moelora-contrastive-learning-guided-mixture-of-experts-on-parameter-efficient-fine-tuning-for-large-language-models-tongxu-luo-et-al-2024>(9/89 | 9/316) MoELoRA: Contrastive Learning Guided Mixture of Experts on Parameter-Efficient Fine-Tuning for Large Language Models (Tongxu Luo et al., 2024)</a></li><li><a href=#1089--10316-a-simple-but-effective-approach-to-improve-structured-language-model-output-for-information-extraction-yinghao-li-et-al-2024>(10/89 | 10/316) A Simple but Effective Approach to Improve Structured Language Model Output for Information Extraction (Yinghao Li et al., 2024)</a></li><li><a href=#1189--11316-few-shot-clinical-entity-recognition-in-three-languages-masked-language-models-outperform-llm-prompting-marco-naguib-et-al-2024>(11/89 | 11/316) Few shot clinical entity recognition in three languages: Masked language models outperform LLM prompting (Marco Naguib et al., 2024)</a></li><li><a href=#1289--12316-reliable-llm-based-user-simulator-for-task-oriented-dialogue-systems-ivan-sekulić-et-al-2024>(12/89 | 12/316) Reliable LLM-based User Simulator for Task-Oriented Dialogue Systems (Ivan Sekulić et al., 2024)</a></li><li><a href=#1389--13316-grafford-a-benchmark-dataset-for-testing-the-knowledge-of-object-affordances-of-language-and-vision-models-sayantan-adak-et-al-2024>(13/89 | 13/316) GRAFFORD: A Benchmark Dataset for Testing the Knowledge of Object Affordances of Language and Vision Models (Sayantan Adak et al., 2024)</a></li><li><a href=#1489--14316-the-finben-an-holistic-financial-benchmark-for-large-language-models-qianqian-xie-et-al-2024>(14/89 | 14/316) The FinBen: An Holistic Financial Benchmark for Large Language Models (Qianqian Xie et al., 2024)</a></li><li><a href=#1589--15316-the-impact-of-demonstrations-on-multilingual-in-context-learning-a-multidimensional-analysis-miaoran-zhang-et-al-2024>(15/89 | 15/316) The Impact of Demonstrations on Multilingual In-Context Learning: A Multidimensional Analysis (Miaoran Zhang et al., 2024)</a></li><li><a href=#1689--16316-question-calibration-and-multi-hop-modeling-for-temporal-question-answering-chao-xue-et-al-2024>(16/89 | 16/316) Question Calibration and Multi-Hop Modeling for Temporal Question Answering (Chao Xue et al., 2024)</a></li><li><a href=#1789--17316-humaneval-on-latest-gpt-models----2024-daniel-li-et-al-2024>(17/89 | 17/316) HumanEval on Latest GPT Models &ndash; 2024 (Daniel Li et al., 2024)</a></li><li><a href=#1889--18316-synthetic-data-almost-from-scratch-generalized-instruction-tuning-for-language-models-haoran-li-et-al-2024>(18/89 | 18/316) Synthetic Data (Almost) from Scratch: Generalized Instruction Tuning for Language Models (Haoran Li et al., 2024)</a></li><li><a href=#1989--19316-umbclu-at-semeval-2024-task-1a-and-1c-semantic-textual-relatedness-with-and-without-machine-translation-shubhashis-roy-dipta-et-al-2024>(19/89 | 19/316) UMBCLU at SemEval-2024 Task 1A and 1C: Semantic Textual Relatedness with and without machine translation (Shubhashis Roy Dipta et al., 2024)</a></li><li><a href=#2089--20316-drbenchmark-a-large-language-understanding-evaluation-benchmark-for-french-biomedical-domain-yanis-labrak-et-al-2024>(20/89 | 20/316) DrBenchmark: A Large Language Understanding Evaluation Benchmark for French Biomedical Domain (Yanis Labrak et al., 2024)</a></li><li><a href=#2189--21316-cif-bench-a-chinese-instruction-following-benchmark-for-evaluating-the-generalizability-of-large-language-models-yizhi-li-et-al-2024>(21/89 | 21/316) CIF-Bench: A Chinese Instruction-Following Benchmark for Evaluating the Generalizability of Large Language Models (Yizhi LI et al., 2024)</a></li><li><a href=#2289--22316-chatel-entity-linking-with-chatbots-yifan-ding-et-al-2024>(22/89 | 22/316) ChatEL: Entity Linking with Chatbots (Yifan Ding et al., 2024)</a></li><li><a href=#2389--23316-is-the-system-message-really-important-to-jailbreaks-in-large-language-models-xiaotian-zou-et-al-2024>(23/89 | 23/316) Is the System Message Really Important to Jailbreaks in Large Language Models? (Xiaotian Zou et al., 2024)</a></li><li><a href=#2489--24316-elad-explanation-guided-large-language-models-active-distillation-yifei-zhang-et-al-2024>(24/89 | 24/316) ELAD: Explanation-Guided Large Language Models Active Distillation (Yifei Zhang et al., 2024)</a></li><li><a href=#2589--25316-learning-to-check-unleashing-potentials-for-self-correction-in-large-language-models-che-zhang-et-al-2024>(25/89 | 25/316) Learning to Check: Unleashing Potentials for Self-Correction in Large Language Models (Che Zhang et al., 2024)</a></li><li><a href=#2689--26316-fine-tuning-prompting-in-context-learning-and-instruction-tuning-how-many-labelled-samples-do-we-need-branislav-pecher-et-al-2024>(26/89 | 26/316) Fine-Tuning, Prompting, In-Context Learning and Instruction-Tuning: How Many Labelled Samples Do We Need? (Branislav Pecher et al., 2024)</a></li><li><a href=#2789--27316-on-sensitivity-of-learning-with-limited-labelled-data-to-the-effects-of-randomness-impact-of-interactions-and-systematic-choices-branislav-pecher-et-al-2024>(27/89 | 27/316) On Sensitivity of Learning with Limited Labelled Data to the Effects of Randomness: Impact of Interactions and Systematic Choices (Branislav Pecher et al., 2024)</a></li><li><a href=#2889--28316-formulaqa-a-question-answering-dataset-for-formula-based-numerical-reasoning-xiao-li-et-al-2024>(28/89 | 28/316) FormulaQA: A Question Answering Dataset for Formula-Based Numerical Reasoning (Xiao Li et al., 2024)</a></li><li><a href=#2989--29316-chatatc-large-language-model-driven-conversational-agents-for-supporting-strategic-air-traffic-flow-management-sinan-abdulhak-et-al-2024>(29/89 | 29/316) CHATATC: Large Language Model-Driven Conversational Agents for Supporting Strategic Air Traffic Flow Management (Sinan Abdulhak et al., 2024)</a></li><li><a href=#3089--30316-identifying-semantic-induction-heads-to-understand-in-context-learning-jie-ren-et-al-2024>(30/89 | 30/316) Identifying Semantic Induction Heads to Understand In-Context Learning (Jie Ren et al., 2024)</a></li><li><a href=#3189--31316-structure-guided-prompt-instructing-large-language-model-in-multi-step-reasoning-by-exploring-graph-structure-of-the-text-kewei-cheng-et-al-2024>(31/89 | 31/316) Structure Guided Prompt: Instructing Large Language Model in Multi-Step Reasoning by Exploring Graph Structure of the Text (Kewei Cheng et al., 2024)</a></li><li><a href=#3289--32316-pirb-a-comprehensive-benchmark-of-polish-dense-and-hybrid-text-retrieval-methods-sławomir-dadas-et-al-2024>(32/89 | 32/316) PIRB: A Comprehensive Benchmark of Polish Dense and Hybrid Text Retrieval Methods (Sławomir Dadas et al., 2024)</a></li><li><a href=#3389--33316-tofueval-evaluating-hallucinations-of-llms-on-topic-focused-dialogue-summarization-liyan-tang-et-al-2024>(33/89 | 33/316) TofuEval: Evaluating Hallucinations of LLMs on Topic-Focused Dialogue Summarization (Liyan Tang et al., 2024)</a></li><li><a href=#3489--34316-agentmd-empowering-language-agents-for-risk-prediction-with-large-scale-clinical-tool-learning-qiao-jin-et-al-2024>(34/89 | 34/316) AgentMD: Empowering Language Agents for Risk Prediction with Large-Scale Clinical Tool Learning (Qiao Jin et al., 2024)</a></li><li><a href=#3589--35316-rocode-a-dataset-for-measuring-code-intelligence-from-problem-definitions-in-romanian-adrian-cosma-et-al-2024>(35/89 | 35/316) RoCode: A Dataset for Measuring Code Intelligence from Problem Definitions in Romanian (Adrian Cosma et al., 2024)</a></li><li><a href=#3689--36316-heterogeneous-graph-reasoning-for-fact-checking-over-texts-and-tables-haisong-gong-et-al-2024>(36/89 | 36/316) Heterogeneous Graph Reasoning for Fact Checking over Texts and Tables (Haisong Gong et al., 2024)</a></li><li><a href=#3789--37316-enhancing-modern-supervised-word-sense-disambiguation-models-by-semantic-lexical-resources-stefano-melacci-et-al-2024>(37/89 | 37/316) Enhancing Modern Supervised Word Sense Disambiguation Models by Semantic Lexical Resources (Stefano Melacci et al., 2024)</a></li><li><a href=#3889--38316-symba-symbolic-backward-chaining-for-multi-step-natural-language-reasoning-jinu-lee-et-al-2024>(38/89 | 38/316) SymBa: Symbolic Backward Chaining for Multi-step Natural Language Reasoning (Jinu Lee et al., 2024)</a></li><li><a href=#3989--39316-sillm-large-language-models-for-simultaneous-machine-translation-shoutao-guo-et-al-2024>(39/89 | 39/316) SiLLM: Large Language Models for Simultaneous Machine Translation (Shoutao Guo et al., 2024)</a></li><li><a href=#4089--40316-large-language-model-based-human-agent-collaboration-for-complex-task-solving-xueyang-feng-et-al-2024>(40/89 | 40/316) Large Language Model-based Human-Agent Collaboration for Complex Task Solving (Xueyang Feng et al., 2024)</a></li><li><a href=#4189--41316-exploring-the-impact-of-table-to-text-methods-on-augmenting-llm-based-question-answering-with-domain-hybrid-data-dehai-min-et-al-2024>(41/89 | 41/316) Exploring the Impact of Table-to-Text Methods on Augmenting LLM-based Question Answering with Domain Hybrid Data (Dehai Min et al., 2024)</a></li><li><a href=#4289--42316-instruction-tuned-language-models-are-better-knowledge-learners-zhengbao-jiang-et-al-2024>(42/89 | 42/316) Instruction-tuned Language Models are Better Knowledge Learners (Zhengbao Jiang et al., 2024)</a></li><li><a href=#4389--43316-owsm-ctc-an-open-encoder-only-speech-foundation-model-for-speech-recognition-translation-and-language-identification-yifan-peng-et-al-2024>(43/89 | 43/316) OWSM-CTC: An Open Encoder-Only Speech Foundation Model for Speech Recognition, Translation, and Language Identification (Yifan Peng et al., 2024)</a></li><li><a href=#4489--44316-more-discriminative-sentence-embeddings-via-semantic-graph-smoothing-chakib-fettal-et-al-2024>(44/89 | 44/316) More Discriminative Sentence Embeddings via Semantic Graph Smoothing (Chakib Fettal et al., 2024)</a></li><li><a href=#4589--45316-somelvlm-a-large-vision-language-model-for-social-media-processing-xinnong-zhang-et-al-2024>(45/89 | 45/316) SoMeLVLM: A Large Vision Language Model for Social Media Processing (Xinnong Zhang et al., 2024)</a></li><li><a href=#4689--46316-glória----a-generative-and-open-large-language-model-for-portuguese-ricardo-lopes-et-al-2024>(46/89 | 46/316) GlórIA &ndash; A Generative and Open Large Language Model for Portuguese (Ricardo Lopes et al., 2024)</a></li><li><a href=#4789--47316-arabicmmlu-assessing-massive-multitask-language-understanding-in-arabic-fajri-koto-et-al-2024>(47/89 | 47/316) ArabicMMLU: Assessing Massive Multitask Language Understanding in Arabic (Fajri Koto et al., 2024)</a></li><li><a href=#4889--48316-tree-planted-transformers-large-language-models-with-implicit-syntactic-supervision-ryo-yoshida-et-al-2024>(48/89 | 48/316) Tree-Planted Transformers: Large Language Models with Implicit Syntactic Supervision (Ryo Yoshida et al., 2024)</a></li><li><a href=#4989--49316-healthcare-copilot-eliciting-the-power-of-general-llms-for-medical-consultation-zhiyao-ren-et-al-2024>(49/89 | 49/316) Healthcare Copilot: Eliciting the Power of General LLMs for Medical Consultation (Zhiyao Ren et al., 2024)</a></li><li><a href=#5089--50316-investigating-cultural-alignment-of-large-language-models-badr-alkhamissi-et-al-2024>(50/89 | 50/316) Investigating Cultural Alignment of Large Language Models (Badr AlKhamissi et al., 2024)</a></li><li><a href=#5189--51316-smaug-fixing-failure-modes-of-preference-optimisation-with-dpo-positive-arka-pal-et-al-2024>(51/89 | 51/316) Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive (Arka Pal et al., 2024)</a></li><li><a href=#5289--52316-understanding-the-effects-of-language-specific-class-imbalance-in-multilingual-fine-tuning-vincent-jung-et-al-2024>(52/89 | 52/316) Understanding the effects of language-specific class imbalance in multilingual fine-tuning (Vincent Jung et al., 2024)</a></li><li><a href=#5389--53316-gumbelsoft-diversified-language-model-watermarking-via-the-gumbelmax-trick-jiayi-fu-et-al-2024>(53/89 | 53/316) GumbelSoft: Diversified Language Model Watermarking via the GumbelMax-trick (Jiayi Fu et al., 2024)</a></li><li><a href=#5489--54316-panda-preference-adaptation-for-enhancing-domain-specific-abilities-of-llms-an-liu-et-al-2024>(54/89 | 54/316) PANDA: Preference Adaptation for Enhancing Domain-Specific Abilities of LLMs (An Liu et al., 2024)</a></li><li><a href=#5589--55316-identifying-factual-inconsistency-in-summaries-towards-effective-utilization-of-large-language-model-liyan-xu-et-al-2024>(55/89 | 55/316) Identifying Factual Inconsistency in Summaries: Towards Effective Utilization of Large Language Model (Liyan Xu et al., 2024)</a></li><li><a href=#5689--56316-can-large-language-models-be-used-to-provide-psychological-counselling-an-analysis-of-gpt-4-generated-responses-using-role-play-dialogues-michimasa-inaba-et-al-2024>(56/89 | 56/316) Can Large Language Models be Used to Provide Psychological Counselling? An Analysis of GPT-4-Generated Responses Using Role-play Dialogues (Michimasa Inaba et al., 2024)</a></li><li><a href=#5789--57316-sql-craft-text-to-sql-through-interactive-refinement-and-enhanced-reasoning-hanchen-xia-et-al-2024>(57/89 | 57/316) SQL-CRAFT: Text-to-SQL through Interactive Refinement and Enhanced Reasoning (Hanchen Xia et al., 2024)</a></li><li><a href=#5889--58316-softqe-learned-representations-of-queries-expanded-by-llms-varad-pimpalkhute-et-al-2024>(58/89 | 58/316) SoftQE: Learned Representations of Queries Expanded by LLMs (Varad Pimpalkhute et al., 2024)</a></li><li><a href=#5989--59316-a-unified-taxonomy-guided-instruction-tuning-framework-for-entity-set-expansion-and-taxonomy-expansion-yanzhen-shen-et-al-2024>(59/89 | 59/316) A Unified Taxonomy-Guided Instruction Tuning Framework for Entity Set Expansion and Taxonomy Expansion (Yanzhen Shen et al., 2024)</a></li><li><a href=#6089--60316-bimedix-bilingual-medical-mixture-of-experts-llm-sara-pieri-et-al-2024>(60/89 | 60/316) BiMediX: Bilingual Medical Mixture of Experts LLM (Sara Pieri et al., 2024)</a></li><li><a href=#6189--61316-are-electras-sentence-embeddings-beyond-repair-the-case-of-semantic-textual-similarity-ivan-rep-et-al-2024>(61/89 | 61/316) Are ELECTRA&rsquo;s Sentence Embeddings Beyond Repair? The Case of Semantic Textual Similarity (Ivan Rep et al., 2024)</a></li><li><a href=#6289--62316-code-needs-comments-enhancing-code-llms-with-comment-augmentation-demin-song-et-al-2024>(62/89 | 62/316) Code Needs Comments: Enhancing Code LLMs with Comment Augmentation (Demin Song et al., 2024)</a></li><li><a href=#6389--63316-nl2formula-generating-spreadsheet-formulas-from-natural-language-queries-wei-zhao-et-al-2024>(63/89 | 63/316) NL2Formula: Generating Spreadsheet Formulas from Natural Language Queries (Wei Zhao et al., 2024)</a></li><li><a href=#6489--64316-enhanced-hallucination-detection-in-neural-machine-translation-through-simple-detector-aggregation-anas-himmi-et-al-2024>(64/89 | 64/316) Enhanced Hallucination Detection in Neural Machine Translation through Simple Detector Aggregation (Anas Himmi et al., 2024)</a></li><li><a href=#6589--65316-how-do-hyenas-deal-with-human-speech-speech-recognition-and-translation-with-confhyena-marco-gaido-et-al-2024>(65/89 | 65/316) How do Hyenas deal with Human Speech? Speech Recognition and Translation with ConfHyena (Marco Gaido et al., 2024)</a></li><li><a href=#6689--66316-comparing-inferential-strategies-of-humans-and-large-language-models-in-deductive-reasoning-philipp-mondorf-et-al-2024>(66/89 | 66/316) Comparing Inferential Strategies of Humans and Large Language Models in Deductive Reasoning (Philipp Mondorf et al., 2024)</a></li><li><a href=#6789--67316-advancing-large-language-models-to-capture-varied-speaking-styles-and-respond-properly-in-spoken-conversations-guan-ting-lin-et-al-2024>(67/89 | 67/316) Advancing Large Language Models to Capture Varied Speaking Styles and Respond Properly in Spoken Conversations (Guan-Ting Lin et al., 2024)</a></li><li><a href=#6889--68316-a-dual-prompting-for-interpretable-mental-health-language-models-hyolim-jeon-et-al-2024>(68/89 | 68/316) A Dual-Prompting for Interpretable Mental Health Language Models (Hyolim Jeon et al., 2024)</a></li><li><a href=#6989--69316-soft-self-consistency-improves-language-model-agents-han-wang-et-al-2024>(69/89 | 69/316) Soft Self-Consistency Improves Language Model Agents (Han Wang et al., 2024)</a></li><li><a href=#7089--70316-treeeval-benchmark-free-evaluation-of-large-language-models-through-tree-planning-xiang-li-et-al-2024>(70/89 | 70/316) TreeEval: Benchmark-Free Evaluation of Large Language Models through Tree Planning (Xiang Li et al., 2024)</a></li><li><a href=#7189--71316-event-level-knowledge-editing-hao-peng-et-al-2024>(71/89 | 71/316) Event-level Knowledge Editing (Hao Peng et al., 2024)</a></li><li><a href=#7289--72316-bias-in-language-models-beyond-trick-tests-and-toward-ruted-evaluation-kristian-lum-et-al-2024>(72/89 | 72/316) Bias in Language Models: Beyond Trick Tests and Toward RUTEd Evaluation (Kristian Lum et al., 2024)</a></li><li><a href=#7389--73316-softmax-probabilities-mostly-predict-large-language-model-correctness-on-multiple-choice-qa-benjamin-plaut-et-al-2024>(73/89 | 73/316) Softmax Probabilities (Mostly) Predict Large Language Model Correctness on Multiple-Choice Q&amp;A (Benjamin Plaut et al., 2024)</a></li><li><a href=#7489--74316-can-large-language-models-be-good-emotional-supporter-mitigating-preference-bias-on-emotional-support-conversation-dongjin-kang-et-al-2024>(74/89 | 74/316) Can Large Language Models be Good Emotional Supporter? Mitigating Preference Bias on Emotional Support Conversation (Dongjin Kang et al., 2024)</a></li><li><a href=#7589--75316-what-if-llms-have-different-world-views-simulating-alien-civilizations-with-llm-based-agents-mingyu-jin-et-al-2024>(75/89 | 75/316) What if LLMs Have Different World Views: Simulating Alien Civilizations with LLM-based Agents (Mingyu Jin et al., 2024)</a></li><li><a href=#7689--76316-when-only-time-will-tell-interpreting-how-transformers-process-local-ambiguities-through-the-lens-of-restart-incrementality-brielen-madureira-et-al-2024>(76/89 | 76/316) When Only Time Will Tell: Interpreting How Transformers Process Local Ambiguities Through the Lens of Restart-Incrementality (Brielen Madureira et al., 2024)</a></li><li><a href=#7789--77316-stable-knowledge-editing-in-large-language-models-zihao-wei-et-al-2024>(77/89 | 77/316) Stable Knowledge Editing in Large Language Models (Zihao Wei et al., 2024)</a></li><li><a href=#7889--78316-acknowledgment-of-emotional-states-generating-validating-responses-for-empathetic-dialogue-zi-haur-pang-et-al-2024>(78/89 | 78/316) Acknowledgment of Emotional States: Generating Validating Responses for Empathetic Dialogue (Zi Haur Pang et al., 2024)</a></li><li><a href=#7989--79316-an-llm-maturity-model-for-reliable-and-transparent-text-to-query-lei-yu-et-al-2024>(79/89 | 79/316) An LLM Maturity Model for Reliable and Transparent Text-to-Query (Lei Yu et al., 2024)</a></li><li><a href=#8089--80316-are-large-language-models-rational-investors-yuhang-zhou-et-al-2024>(80/89 | 80/316) Are Large Language Models Rational Investors? (Yuhang Zhou et al., 2024)</a></li><li><a href=#8189--81316-cfever-a-chinese-fact-extraction-and-verification-dataset-ying-jia-lin-et-al-2024>(81/89 | 81/316) CFEVER: A Chinese Fact Extraction and VERification Dataset (Ying-Jia Lin et al., 2024)</a></li><li><a href=#8289--82316-explaining-relationships-among-research-papers-xiangci-li-et-al-2024>(82/89 | 82/316) Explaining Relationships Among Research Papers (Xiangci Li et al., 2024)</a></li><li><a href=#8389--83316-the-hidden-space-of-transformer-language-adapters-jesujoba-o-alabi-et-al-2024>(83/89 | 83/316) The Hidden Space of Transformer Language Adapters (Jesujoba O. Alabi et al., 2024)</a></li><li><a href=#8489--84316-phonotactic-complexity-across-dialects-ryan-soh-eun-shim-et-al-2024>(84/89 | 84/316) Phonotactic Complexity across Dialects (Ryan Soh-Eun Shim et al., 2024)</a></li><li><a href=#8589--85316-autism-detection-in-speech----a-survey-nadine-probol-et-al-2024>(85/89 | 85/316) Autism Detection in Speech &ndash; A Survey (Nadine Probol et al., 2024)</a></li><li><a href=#8689--86316-backward-lens-projecting-language-model-gradients-into-the-vocabulary-space-shahar-katz-et-al-2024>(86/89 | 86/316) Backward Lens: Projecting Language Model Gradients into the Vocabulary Space (Shahar Katz et al., 2024)</a></li><li><a href=#8789--87316-handling-ambiguity-in-emotion-from-out-of-domain-detection-to-distribution-estimation-wen-wu-et-al-2024>(87/89 | 87/316) Handling Ambiguity in Emotion: From Out-of-Domain Detection to Distribution Estimation (Wen Wu et al., 2024)</a></li><li><a href=#8889--88316-simpsons-paradox-and-the-accuracy-fluency-tradeoff-in-translation-zheng-wei-lim-et-al-2024>(88/89 | 88/316) Simpson&rsquo;s Paradox and the Accuracy-Fluency Tradeoff in Translation (Zheng Wei Lim et al., 2024)</a></li><li><a href=#8989--89316-styledubber-towards-multi-scale-style-learning-for-movie-dubbing-gaoxiang-cong-et-al-2024>(89/89 | 89/316) StyleDubber: Towards Multi-Scale Style Learning for Movie Dubbing (Gaoxiang Cong et al., 2024)</a></li></ul></li><li><a href=#cscv-45>cs.CV (45)</a><ul><li><a href=#145--90316-cell-graph-transformer-for-nuclei-classification-wei-lou-et-al-2024>(1/45 | 90/316) Cell Graph Transformer for Nuclei Classification (Wei Lou et al., 2024)</a></li><li><a href=#245--91316-modality-aware-integration-with-large-language-models-for-knowledge-based-visual-question-answering-junnan-dong-et-al-2024>(2/45 | 91/316) Modality-Aware Integration with Large Language Models for Knowledge-based Visual Question Answering (Junnan Dong et al., 2024)</a></li><li><a href=#345--92316-countercurate-enhancing-physical-and-semantic-visio-linguistic-compositional-reasoning-via-counterfactual-examples-jianrui-zhang-et-al-2024>(3/45 | 92/316) CounterCurate: Enhancing Physical and Semantic Visio-Linguistic Compositional Reasoning via Counterfactual Examples (Jianrui Zhang et al., 2024)</a></li><li><a href=#445--93316-mulan-multimodal-llm-agent-for-progressive-multi-object-diffusion-sen-li-et-al-2024>(4/45 | 93/316) MuLan: Multimodal-LLM Agent for Progressive Multi-Object Diffusion (Sen Li et al., 2024)</a></li><li><a href=#545--94316-cross-domain-transfer-learning-with-corte-consistent-and-reliable-transfer-from-black-box-to-lightweight-segmentation-model-claudia-cuttano-et-al-2024>(5/45 | 94/316) Cross-Domain Transfer Learning with CoRTe: Consistent and Reliable Transfer from Black-Box to Lightweight Segmentation Model (Claudia Cuttano et al., 2024)</a></li><li><a href=#645--95316-cst-calibration-side-tuning-for-parameter-and-memory-efficient-transfer-learning-feng-chen-2024>(6/45 | 95/316) CST: Calibration Side-Tuning for Parameter and Memory Efficient Transfer Learning (Feng Chen, 2024)</a></li><li><a href=#745--96316-visual-style-prompting-with-swapping-self-attention-jaeseok-jeong-et-al-2024>(7/45 | 96/316) Visual Style Prompting with Swapping Self-Attention (Jaeseok Jeong et al., 2024)</a></li><li><a href=#845--97316-clipping-the-deception-adapting-vision-language-models-for-universal-deepfake-detection-sohail-ahmed-khan-et-al-2024>(8/45 | 97/316) CLIPping the Deception: Adapting Vision-Language Models for Universal Deepfake Detection (Sohail Ahmed Khan et al., 2024)</a></li><li><a href=#945--98316-yolo-ant-a-lightweight-detector-via-depthwise-separable-convolutional-and-large-kernel-design-for-antenna-interference-source-detection-xiaoyu-tang-et-al-2024>(9/45 | 98/316) YOLO-Ant: A Lightweight Detector via Depthwise Separable Convolutional and Large Kernel Design for Antenna Interference Source Detection (Xiaoyu Tang et al., 2024)</a></li><li><a href=#1045--99316-how-easy-is-it-to-fool-your-multimodal-llms-an-empirical-analysis-on-deceptive-prompts-yusu-qian-et-al-2024>(10/45 | 99/316) How Easy is It to Fool Your Multimodal LLMs? An Empirical Analysis on Deceptive Prompts (Yusu Qian et al., 2024)</a></li><li><a href=#1145--100316-olvit-multi-modal-state-tracking-via-attention-based-embeddings-for-video-grounded-dialog-adnen-abdessaied-et-al-2024>(11/45 | 100/316) OLViT: Multi-Modal State Tracking via Attention-Based Embeddings for Video-Grounded Dialog (Adnen Abdessaied et al., 2024)</a></li><li><a href=#1245--101316-layout-to-image-generation-with-localized-descriptions-using-controlnet-with-cross-attention-control-denis-lukovnikov-et-al-2024>(12/45 | 101/316) Layout-to-Image Generation with Localized Descriptions using ControlNet with Cross-Attention Control (Denis Lukovnikov et al., 2024)</a></li><li><a href=#1345--102316-combining-unsupervised-and-supervised-learning-in-microscopy-enables-defect-analysis-of-a-full-4h-sic-wafer-binh-duong-nguyen-et-al-2024>(13/45 | 102/316) Combining unsupervised and supervised learning in microscopy enables defect analysis of a full 4H-SiC wafer (Binh Duong Nguyen et al., 2024)</a></li><li><a href=#1445--103316-solarpanel-segmentation-self-supervised-learning-for-imperfect-datasets-sankarshanaa-sagaram-et-al-2024>(14/45 | 103/316) SolarPanel Segmentation :Self-Supervised Learning for Imperfect Datasets (Sankarshanaa Sagaram et al., 2024)</a></li><li><a href=#1545--104316-rhythmformer-extracting-rppg-signals-based-on-hierarchical-temporal-periodic-transformer-bochao-zou-et-al-2024>(15/45 | 104/316) RhythmFormer: Extracting rPPG Signals Based on Hierarchical Temporal Periodic Transformer (Bochao Zou et al., 2024)</a></li><li><a href=#1645--105316-learning-domain-invariant-temporal-dynamics-for-few-shot-action-recognition-yuke-li-et-al-2024>(16/45 | 105/316) Learning Domain-Invariant Temporal Dynamics for Few-Shot Action Recognition (Yuke Li et al., 2024)</a></li><li><a href=#1745--106316-a-touch-vision-and-language-dataset-for-multimodal-alignment-letian-fu-et-al-2024>(17/45 | 106/316) A Touch, Vision, and Language Dataset for Multimodal Alignment (Letian Fu et al., 2024)</a></li><li><a href=#1845--107316-videoprism-a-foundational-visual-encoder-for-video-understanding-long-zhao-et-al-2024>(18/45 | 107/316) VideoPrism: A Foundational Visual Encoder for Video Understanding (Long Zhao et al., 2024)</a></li><li><a href=#1945--108316-slot-vlm-slowfast-slots-for-video-language-modeling-jiaqi-xu-et-al-2024>(19/45 | 108/316) Slot-VLM: SlowFast Slots for Video-Language Modeling (Jiaqi Xu et al., 2024)</a></li><li><a href=#2045--109316-mvdiffusion-a-dense-high-resolution-multi-view-diffusion-model-for-single-or-sparse-view-3d-object-reconstruction-shitao-tang-et-al-2024>(20/45 | 109/316) MVDiffusion++: A Dense High-resolution Multi-view Diffusion Model for Single or Sparse-view 3D Object Reconstruction (Shitao Tang et al., 2024)</a></li><li><a href=#2145--110316-advancing-monocular-video-based-gait-analysis-using-motion-imitation-with-physics-based-simulation-nikolaos-smyrnakis-et-al-2024>(21/45 | 110/316) Advancing Monocular Video-Based Gait Analysis Using Motion Imitation with Physics-Based Simulation (Nikolaos Smyrnakis et al., 2024)</a></li><li><a href=#2245--111316-efficient-parameter-mining-and-freezing-for-continual-object-detection-angelo-g-menezes-et-al-2024>(22/45 | 111/316) Efficient Parameter Mining and Freezing for Continual Object Detection (Angelo G. Menezes et al., 2024)</a></li><li><a href=#2345--112316-model-composition-for-multimodal-large-language-models-chi-chen-et-al-2024>(23/45 | 112/316) Model Composition for Multimodal Large Language Models (Chi Chen et al., 2024)</a></li><li><a href=#2445--113316-visual-reasoning-in-object-centric-deep-neural-networks-a-comparative-cognition-approach-guillermo-puebla-et-al-2024>(24/45 | 113/316) Visual Reasoning in Object-Centric Deep Neural Networks: A Comparative Cognition Approach (Guillermo Puebla et al., 2024)</a></li><li><a href=#2545--114316-annotheia-a-semi-automatic-annotation-toolkit-for-audio-visual-speech-technologies-josé-m-acosta-triana-et-al-2024>(25/45 | 114/316) AnnoTheia: A Semi-Automatic Annotation Toolkit for Audio-Visual Speech Technologies (José-M. Acosta-Triana et al., 2024)</a></li><li><a href=#2645--115316-unicell-universal-cell-nucleus-classification-via-prompt-learning-junjia-huang-et-al-2024>(26/45 | 115/316) UniCell: Universal Cell Nucleus Classification via Prompt Learning (Junjia Huang et al., 2024)</a></li><li><a href=#2745--116316-diffusionnocs-managing-symmetry-and-uncertainty-in-sim2real-multi-modal-category-level-pose-estimation-takuya-ikeda-et-al-2024>(27/45 | 116/316) DiffusionNOCS: Managing Symmetry and Uncertainty in Sim2Real Multi-Modal Category-level Pose Estimation (Takuya Ikeda et al., 2024)</a></li><li><a href=#2845--117316-realcompo-dynamic-equilibrium-between-realism-and-compositionality-improves-text-to-image-diffusion-models-xinchen-zhang-et-al-2024>(28/45 | 117/316) RealCompo: Dynamic Equilibrium between Realism and Compositionality Improves Text-to-Image Diffusion Models (Xinchen Zhang et al., 2024)</a></li><li><a href=#2945--118316-mind-the-exit-pupil-gap-revisiting-the-intrinsics-of-a-standard-plenoptic-camera-tim-michels-et-al-2024>(29/45 | 118/316) Mind the Exit Pupil Gap: Revisiting the Intrinsics of a Standard Plenoptic Camera (Tim Michels et al., 2024)</a></li><li><a href=#3045--119316-occflownet-towards-self-supervised-occupancy-estimation-via-differentiable-rendering-and-occupancy-flow-simon-boeder-et-al-2024>(30/45 | 119/316) OccFlowNet: Towards Self-supervised Occupancy Estimation via Differentiable Rendering and Occupancy Flow (Simon Boeder et al., 2024)</a></li><li><a href=#3145--120316-convqg-contrastive-visual-question-generation-with-multimodal-guidance-li-mi-et-al-2024>(31/45 | 120/316) ConVQG: Contrastive Visual Question Generation with Multimodal Guidance (Li Mi et al., 2024)</a></li><li><a href=#3245--121316-aria-everyday-activities-dataset-zhaoyang-lv-et-al-2024>(32/45 | 121/316) Aria Everyday Activities Dataset (Zhaoyang Lv et al., 2024)</a></li><li><a href=#3345--122316-improving-robustness-for-joint-optimization-of-camera-poses-and-decomposed-low-rank-tensorial-radiance-fields-bo-yu-cheng-et-al-2024>(33/45 | 122/316) Improving Robustness for Joint Optimization of Camera Poses and Decomposed Low-Rank Tensorial Radiance Fields (Bo-Yu Cheng et al., 2024)</a></li><li><a href=#3445--123316-video-recap-recursive-captioning-of-hour-long-videos-md-mohaiminul-islam-et-al-2024>(34/45 | 123/316) Video ReCap: Recursive Captioning of Hour-Long Videos (Md Mohaiminul Islam et al., 2024)</a></li><li><a href=#3545--124316-uniedit-a-unified-tuning-free-framework-for-video-motion-and-appearance-editing-jianhong-bai-et-al-2024>(35/45 | 124/316) UniEdit: A Unified Tuning-Free Framework for Video Motion and Appearance Editing (Jianhong Bai et al., 2024)</a></li><li><a href=#3645--125316-toward-fairness-via-maximum-mean-discrepancy-regularization-on-logits-space-hao-wei-chung-et-al-2024>(36/45 | 125/316) Toward Fairness via Maximum Mean Discrepancy Regularization on Logits Space (Hao-Wei Chung et al., 2024)</a></li><li><a href=#3745--126316-comparison-of-conventional-hybrid-and-ctcattention-decoders-for-continuous-visual-speech-recognition-david-gimeno-gómez-et-al-2024>(37/45 | 126/316) Comparison of Conventional Hybrid and CTC/Attention Decoders for Continuous Visual Speech Recognition (David Gimeno-Gómez et al., 2024)</a></li><li><a href=#3845--127316-two-stage-rainfall-forecasting-diffusion-model-xudong-ling-et-al-2024>(38/45 | 127/316) Two-stage Rainfall-Forecasting Diffusion Model (XuDong Ling et al., 2024)</a></li><li><a href=#3945--128316-good-towards-domain-generalized-orientated-object-detection-qi-bi-et-al-2024>(39/45 | 128/316) GOOD: Towards Domain Generalized Orientated Object Detection (Qi Bi et al., 2024)</a></li><li><a href=#4045--129316-neuromorphic-synergy-for-video-binarization-shijie-lin-et-al-2024>(40/45 | 129/316) Neuromorphic Synergy for Video Binarization (Shijie Lin et al., 2024)</a></li><li><a href=#4145--130316-bronchotrack-airway-lumen-tracking-for-branch-level-bronchoscopic-localization-qingyao-tian-et-al-2024>(41/45 | 130/316) BronchoTrack: Airway Lumen Tracking for Branch-Level Bronchoscopic Localization (Qingyao Tian et al., 2024)</a></li><li><a href=#4245--131316-vadv2-end-to-end-vectorized-autonomous-driving-via-probabilistic-planning-shaoyu-chen-et-al-2024>(42/45 | 131/316) VADv2: End-to-End Vectorized Autonomous Driving via Probabilistic Planning (Shaoyu Chen et al., 2024)</a></li><li><a href=#4345--132316-maptrack-tracking-in-the-map-fei-wang-et-al-2024>(43/45 | 132/316) MapTrack: Tracking in the Map (Fei Wang et al., 2024)</a></li><li><a href=#4445--133316-pac-fno-parallel-structured-all-component-fourier-neural-operators-for-recognizing-low-quality-images-jinsung-jeon-et-al-2024>(44/45 | 133/316) PAC-FNO: Parallel-Structured All-Component Fourier Neural Operators for Recognizing Low-Quality Images (Jinsung Jeon et al., 2024)</a></li><li><a href=#4545--134316-object-level-geometric-structure-preserving-for-natural-image-stitching-wenxiao-cai-et-al-2024>(45/45 | 134/316) Object-level Geometric Structure Preserving for Natural Image Stitching (Wenxiao Cai et al., 2024)</a></li></ul></li><li><a href=#cslg-67>cs.LG (67)</a><ul><li><a href=#167--135316-reflect-rl-two-player-online-rl-fine-tuning-for-lms-runlong-zhou-et-al-2024>(1/67 | 135/316) Reflect-RL: Two-Player Online RL Fine-Tuning for LMs (Runlong Zhou et al., 2024)</a></li><li><a href=#267--136316-indiscriminate-data-poisoning-attacks-on-pre-trained-feature-extractors-yiwei-lu-et-al-2024>(2/67 | 136/316) Indiscriminate Data Poisoning Attacks on Pre-trained Feature Extractors (Yiwei Lu et al., 2024)</a></li><li><a href=#367--137316-conditional-logical-message-passing-transformer-for-complex-query-answering-chongzhi-zhang-et-al-2024>(3/67 | 137/316) Conditional Logical Message Passing Transformer for Complex Query Answering (Chongzhi Zhang et al., 2024)</a></li><li><a href=#467--138316-fgad-self-boosted-knowledge-distillation-for-an-effective-federated-graph-anomaly-detection-framework-jinyu-cai-et-al-2024>(4/67 | 138/316) FGAD: Self-boosted Knowledge Distillation for An Effective Federated Graph Anomaly Detection Framework (Jinyu Cai et al., 2024)</a></li><li><a href=#567--139316-bayesian-reward-models-for-llm-alignment-adam-x-yang-et-al-2024>(5/67 | 139/316) Bayesian Reward Models for LLM Alignment (Adam X. Yang et al., 2024)</a></li><li><a href=#667--140316-defending-jailbreak-prompts-via-in-context-adversarial-game-yujun-zhou-et-al-2024>(6/67 | 140/316) Defending Jailbreak Prompts via In-Context Adversarial Game (Yujun Zhou et al., 2024)</a></li><li><a href=#767--141316-harnessing-large-language-models-as-post-hoc-correctors-zhiqiang-zhong-et-al-2024>(7/67 | 141/316) Harnessing Large Language Models as Post-hoc Correctors (Zhiqiang Zhong et al., 2024)</a></li><li><a href=#867--142316-a-microstructure-based-graph-neural-network-for-accelerating-multiscale-simulations-j-storm-et-al-2024>(8/67 | 142/316) A Microstructure-based Graph Neural Network for Accelerating Multiscale Simulations (J. Storm et al., 2024)</a></li><li><a href=#967--143316-towards-accelerating-physical-discovery-via-non-interactive-and-interactive-multi-fidelity-bayesian-optimization-current-challenges-and-future-opportunities-arpan-biswas-et-al-2024>(9/67 | 143/316) Towards accelerating physical discovery via non-interactive and interactive multi-fidelity Bayesian Optimization: Current challenges and future opportunities (Arpan Biswas et al., 2024)</a></li><li><a href=#1067--144316-transformer-tricks-precomputing-the-first-layer-nils-graef-2024>(10/67 | 144/316) Transformer tricks: Precomputing the first layer (Nils Graef, 2024)</a></li><li><a href=#1167--145316-referee-meta-learning-for-fast-adaptation-of-locational-fairness-weiye-chen-et-al-2024>(11/67 | 145/316) Referee-Meta-Learning for Fast Adaptation of Locational Fairness (Weiye Chen et al., 2024)</a></li><li><a href=#1267--146316-chain-of-thought-empowers-transformers-to-solve-inherently-serial-problems-zhiyuan-li-et-al-2024>(12/67 | 146/316) Chain of Thought Empowers Transformers to Solve Inherently Serial Problems (Zhiyuan Li et al., 2024)</a></li><li><a href=#1367--147316-beyond-worst-case-attacks-robust-rl-with-adaptive-defense-via-non-dominated-policies-xiangyu-liu-et-al-2024>(13/67 | 147/316) Beyond Worst-case Attacks: Robust RL with Adaptive Defense via Non-dominated Policies (Xiangyu Liu et al., 2024)</a></li><li><a href=#1467--148316-trap-targeted-random-adversarial-prompt-honeypot-for-black-box-identification-martin-gubri-et-al-2024>(14/67 | 148/316) TRAP: Targeted Random Adversarial Prompt Honeypot for Black-Box Identification (Martin Gubri et al., 2024)</a></li><li><a href=#1567--149316-linksage-optimizing-job-matching-using-graph-neural-networks-ping-liu-et-al-2024>(15/67 | 149/316) LinkSAGE: Optimizing Job Matching Using Graph Neural Networks (Ping Liu et al., 2024)</a></li><li><a href=#1667--150316-buffgraph-enhancing-class-imbalanced-node-classification-via-buffer-nodes-qian-wang-et-al-2024>(16/67 | 150/316) BuffGraph: Enhancing Class-Imbalanced Node Classification via Buffer Nodes (Qian Wang et al., 2024)</a></li><li><a href=#1767--151316-align-your-intents-offline-imitation-learning-via-optimal-transport-maksim-bobrin-et-al-2024>(17/67 | 151/316) Align Your Intents: Offline Imitation Learning via Optimal Transport (Maksim Bobrin et al., 2024)</a></li><li><a href=#1867--152316-graphgini-fostering-individual-and-group-fairness-in-graph-neural-networks-anuj-kumar-sirohi-et-al-2024>(18/67 | 152/316) GRAPHGINI: Fostering Individual and Group Fairness in Graph Neural Networks (Anuj Kumar Sirohi et al., 2024)</a></li><li><a href=#1967--153316-scalable-and-reliable-deep-transfer-learning-for-intelligent-fault-detection-via-multi-scale-neural-processes-embedded-with-knowledge-zhongzhi-li-et-al-2024>(19/67 | 153/316) Scalable and reliable deep transfer learning for intelligent fault detection via multi-scale neural processes embedded with knowledge (Zhongzhi Li et al., 2024)</a></li><li><a href=#2067--154316-it-intrusion-detection-using-statistical-learning-and-testbed-measurements-xiaoxuan-wang-et-al-2024>(20/67 | 154/316) IT Intrusion Detection Using Statistical Learning and Testbed Measurements (Xiaoxuan Wang et al., 2024)</a></li><li><a href=#2167--155316-improve-cross-architecture-generalization-on-dataset-distillation-binglin-zhou-et-al-2024>(21/67 | 155/316) Improve Cross-Architecture Generalization on Dataset Distillation (Binglin Zhou et al., 2024)</a></li><li><a href=#2267--156316-investigating-the-impact-of-model-instability-on-explanations-and-uncertainty-sara-vera-marjanović-et-al-2024>(22/67 | 156/316) Investigating the Impact of Model Instability on Explanations and Uncertainty (Sara Vera Marjanović et al., 2024)</a></li><li><a href=#2367--157316-stochastic-approximation-approach-to-federated-machine-learning-srihari-p-v-et-al-2024>(23/67 | 157/316) Stochastic Approximation Approach to Federated Machine Learning (Srihari P V et al., 2024)</a></li><li><a href=#2467--158316-analysis-of-using-sigmoid-loss-for-contrastive-learning-chungpa-lee-et-al-2024>(24/67 | 158/316) Analysis of Using Sigmoid Loss for Contrastive Learning (Chungpa Lee et al., 2024)</a></li><li><a href=#2567--159316-structural-knowledge-informed-continual-multivariate-time-series-forecasting-zijie-pan-et-al-2024>(25/67 | 159/316) Structural Knowledge Informed Continual Multivariate Time Series Forecasting (Zijie Pan et al., 2024)</a></li><li><a href=#2667--160316-enhancing-real-world-complex-network-representations-with-hyperedge-augmentation-xiangyu-zhao-et-al-2024>(26/67 | 160/316) Enhancing Real-World Complex Network Representations with Hyperedge Augmentation (Xiangyu Zhao et al., 2024)</a></li><li><a href=#2767--161316-partial-search-in-a-frozen-network-is-enough-to-find-a-strong-lottery-ticket-hikari-otsuka-et-al-2024>(27/67 | 161/316) Partial Search in a Frozen Network is Enough to Find a Strong Lottery Ticket (Hikari Otsuka et al., 2024)</a></li><li><a href=#2867--162316-bayesian-neural-networks-with-domain-knowledge-priors-dylan-sam-et-al-2024>(28/67 | 162/316) Bayesian Neural Networks with Domain Knowledge Priors (Dylan Sam et al., 2024)</a></li><li><a href=#2967--163316-the-uncanny-valley-a-comprehensive-analysis-of-diffusion-models-karam-ghanem-et-al-2024>(29/67 | 163/316) The Uncanny Valley: A Comprehensive Analysis of Diffusion Models (Karam Ghanem et al., 2024)</a></li><li><a href=#3067--164316-incentivized-exploration-via-filtered-posterior-sampling-anand-kalvit-et-al-2024>(30/67 | 164/316) Incentivized Exploration via Filtered Posterior Sampling (Anand Kalvit et al., 2024)</a></li><li><a href=#3167--165316-smore-similarity-based-hyperdimensional-domain-adaptation-for-multi-sensor-time-series-classification-junyao-wang-et-al-2024>(31/67 | 165/316) SMORE: Similarity-based Hyperdimensional Domain Adaptation for Multi-Sensor Time Series Classification (Junyao Wang et al., 2024)</a></li><li><a href=#3267--166316-neural-network-diffusion-kai-wang-et-al-2024>(32/67 | 166/316) Neural Network Diffusion (Kai Wang et al., 2024)</a></li><li><a href=#3367--167316-fair-classifiers-without-fair-training-an-influence-guided-data-sampling-approach-jinlong-pang-et-al-2024>(33/67 | 167/316) Fair Classifiers Without Fair Training: An Influence-Guided Data Sampling Approach (Jinlong Pang et al., 2024)</a></li><li><a href=#3467--168316-achieving-near-optimal-regret-for-bandit-algorithms-with-uniform-last-iterate-guarantee-junyan-liu-et-al-2024>(34/67 | 168/316) Achieving Near-Optimal Regret for Bandit Algorithms with Uniform Last-Iterate Guarantee (Junyan Liu et al., 2024)</a></li><li><a href=#3567--169316-training-artificial-neural-networks-by-coordinate-search-algorithm-ehsan-rokhsatyazdi-et-al-2024>(35/67 | 169/316) Training Artificial Neural Networks by Coordinate Search Algorithm (Ehsan Rokhsatyazdi et al., 2024)</a></li><li><a href=#3667--170316-unsupervised-concept-discovery-mitigates-spurious-correlations-md-rifat-arefin-et-al-2024>(36/67 | 170/316) Unsupervised Concept Discovery Mitigates Spurious Correlations (Md Rifat Arefin et al., 2024)</a></li><li><a href=#3767--171316-towards-robust-graph-incremental-learning-on-evolving-graphs-junwei-su-et-al-2024>(37/67 | 171/316) Towards Robust Graph Incremental Learning on Evolving Graphs (Junwei Su et al., 2024)</a></li><li><a href=#3867--172316-differentiable-mapper-for-topological-optimization-of-data-representation-ziyad-oulhaj-et-al-2024>(38/67 | 172/316) Differentiable Mapper For Topological Optimization Of Data Representation (Ziyad Oulhaj et al., 2024)</a></li><li><a href=#3967--173316-statistical-curriculum-learning-an-elimination-algorithm-achieving-an-oracle-risk-omer-cohen-et-al-2024>(39/67 | 173/316) Statistical curriculum learning: An elimination algorithm achieving an oracle risk (Omer Cohen et al., 2024)</a></li><li><a href=#4067--174316-discovering-behavioral-modes-in-deep-reinforcement-learning-policies-using-trajectory-clustering-in-latent-space-sindre-benjamin-remman-et-al-2024>(40/67 | 174/316) Discovering Behavioral Modes in Deep Reinforcement Learning Policies Using Trajectory Clustering in Latent Space (Sindre Benjamin Remman et al., 2024)</a></li><li><a href=#4167--175316-federated-multi-task-learning-on-non-iid-data-silos-an-experimental-study-yuwen-yang-et-al-2024>(41/67 | 175/316) Federated Multi-Task Learning on Non-IID Data Silos: An Experimental Study (Yuwen Yang et al., 2024)</a></li><li><a href=#4267--176316-ccfc-enhancing-federated-clustering-through-feature-decorrelation-jie-yan-et-al-2024>(42/67 | 176/316) CCFC++: Enhancing Federated Clustering through Feature Decorrelation (Jie Yan et al., 2024)</a></li><li><a href=#4367--177316-tackling-byzantine-clients-in-federated-learning-youssef-allouah-et-al-2024>(43/67 | 177/316) Tackling Byzantine Clients in Federated Learning (Youssef Allouah et al., 2024)</a></li><li><a href=#4467--178316-when-and-how-learning-identifiable-latent-states-for-nonstationary-time-series-forecasting-zijian-li-et-al-2024>(44/67 | 178/316) When and How: Learning Identifiable Latent States for Nonstationary Time Series Forecasting (Zijian Li et al., 2024)</a></li><li><a href=#4567--179316-spurious-correlations-in-machine-learning-a-survey-wenqian-ye-et-al-2024>(45/67 | 179/316) Spurious Correlations in Machine Learning: A Survey (Wenqian Ye et al., 2024)</a></li><li><a href=#4667--180316-equivariant-pretrained-transformer-for-unified-geometric-learning-on-multi-domain-3d-molecules-rui-jiao-et-al-2024>(46/67 | 180/316) Equivariant Pretrained Transformer for Unified Geometric Learning on Multi-Domain 3D Molecules (Rui Jiao et al., 2024)</a></li><li><a href=#4767--181316-evolmpnn-predicting-mutational-effect-on-homologous-proteins-by-evolution-encoding-zhiqiang-zhong-et-al-2024>(47/67 | 181/316) EvolMPNN: Predicting Mutational Effect on Homologous Proteins by Evolution Encoding (Zhiqiang Zhong et al., 2024)</a></li><li><a href=#4867--182316-fairness-risks-for-group-conditionally-missing-demographics-kaiqi-jiang-et-al-2024>(48/67 | 182/316) Fairness Risks for Group-conditionally Missing Demographics (Kaiqi Jiang et al., 2024)</a></li><li><a href=#4967--183316-order-optimal-regret-in-distributed-kernel-bandits-using-uniform-sampling-with-shared-randomness-nikola-pavlovic-et-al-2024>(49/67 | 183/316) Order-Optimal Regret in Distributed Kernel Bandits using Uniform Sampling with Shared Randomness (Nikola Pavlovic et al., 2024)</a></li><li><a href=#5067--184316-how-does-selection-leak-privacy-revisiting-private-selection-and-improved-results-for-hyper-parameter-tuning-zihang-xiang-et-al-2024>(50/67 | 184/316) How Does Selection Leak Privacy: Revisiting Private Selection and Improved Results for Hyper-parameter Tuning (Zihang Xiang et al., 2024)</a></li><li><a href=#5167--185316-harmful-algal-bloom-forecasting-a-comparison-between-stream-and-batch-learning-andres-molares-ulloa-et-al-2024>(51/67 | 185/316) Harmful algal bloom forecasting. A comparison between stream and batch learning (Andres Molares-Ulloa et al., 2024)</a></li><li><a href=#5267--186316-text-guided-molecule-generation-with-diffusion-language-model-haisong-gong-et-al-2024>(52/67 | 186/316) Text-Guided Molecule Generation with Diffusion Language Model (Haisong Gong et al., 2024)</a></li><li><a href=#5367--187316-skill-or-luck-return-decomposition-via-advantage-functions-hsiao-ru-pan-et-al-2024>(53/67 | 187/316) Skill or Luck? Return Decomposition via Advantage Functions (Hsiao-Ru Pan et al., 2024)</a></li><li><a href=#5467--188316-bounding-reconstruction-attack-success-of-adversaries-without-data-priors-alexander-ziller-et-al-2024>(54/67 | 188/316) Bounding Reconstruction Attack Success of Adversaries Without Data Priors (Alexander Ziller et al., 2024)</a></li><li><a href=#5567--189316-from-movements-to-metrics-evaluating-explainable-ai-methods-in-skeleton-based-human-activity-recognition-kimji-n-pellano-et-al-2024>(55/67 | 189/316) From Movements to Metrics: Evaluating Explainable AI Methods in Skeleton-Based Human Activity Recognition (Kimji N. Pellano et al., 2024)</a></li><li><a href=#5667--190316-static-vs-dynamic-databases-for-indoor-localization-based-on-wi-fi-fingerprinting-a-discussion-from-a-data-perspective-zhe-tang-et-al-2024>(56/67 | 190/316) Static vs. Dynamic Databases for Indoor Localization based on Wi-Fi Fingerprinting: A Discussion from a Data Perspective (Zhe Tang et al., 2024)</a></li><li><a href=#5767--191316-diffusion-posterior-sampling-is-computationally-intractable-shivam-gupta-et-al-2024>(57/67 | 191/316) Diffusion Posterior Sampling is Computationally Intractable (Shivam Gupta et al., 2024)</a></li><li><a href=#5867--192316-revitalizing-multivariate-time-series-forecasting-learnable-decomposition-with-inter-series-dependencies-and-intra-series-variations-modeling-guoqi-yu-et-al-2024>(58/67 | 192/316) Revitalizing Multivariate Time Series Forecasting: Learnable Decomposition with Inter-Series Dependencies and Intra-Series Variations Modeling (Guoqi Yu et al., 2024)</a></li><li><a href=#5967--193316-learning-on-manifolds-without-manifold-learning-h-n-mhaskar-et-al-2024>(59/67 | 193/316) Learning on manifolds without manifold learning (H. N. Mhaskar et al., 2024)</a></li><li><a href=#6067--194316-a-comprehensive-review-of-machine-learning-advances-on-data-change-a-cross-field-perspective-jeng-lin-li-et-al-2024>(60/67 | 194/316) A Comprehensive Review of Machine Learning Advances on Data Change: A Cross-Field Perspective (Jeng-Lin Li et al., 2024)</a></li><li><a href=#6167--195316-multi-objective-binary-coordinate-search-for-feature-selection-sevil-zanjani-miyandoab-et-al-2024>(61/67 | 195/316) Multi-objective Binary Coordinate Search for Feature Selection (Sevil Zanjani Miyandoab et al., 2024)</a></li><li><a href=#6267--196316-chili-chemically-informed-large-scale-inorganic-nanomaterials-dataset-for-advancing-graph-machine-learning-ulrik-friis-jensen-et-al-2024>(62/67 | 196/316) CHILI: Chemically-Informed Large-scale Inorganic Nanomaterials Dataset for Advancing Graph Machine Learning (Ulrik Friis-Jensen et al., 2024)</a></li><li><a href=#6367--197316-testing-calibration-in-subquadratic-time-lunjia-hu-et-al-2024>(63/67 | 197/316) Testing Calibration in Subquadratic Time (Lunjia Hu et al., 2024)</a></li><li><a href=#6467--198316-double-machine-learning-for-causal-hybrid-modeling----applications-in-the-earth-sciences-kai-hendrik-cohrs-et-al-2024>(64/67 | 198/316) Double machine learning for causal hybrid modeling &ndash; applications in the Earth sciences (Kai-Hendrik Cohrs et al., 2024)</a></li><li><a href=#6567--199316-subiq-inverse-soft-q-learning-for-offline-imitation-with-suboptimal-demonstrations-huy-hoang-et-al-2024>(65/67 | 199/316) SubIQ: Inverse Soft-Q Learning for Offline Imitation with Suboptimal Demonstrations (Huy Hoang et al., 2024)</a></li><li><a href=#6667--200316-scalable-decentralized-algorithms-for-online-personalized-mean-estimation-franco-galante-et-al-2024>(66/67 | 200/316) Scalable Decentralized Algorithms for Online Personalized Mean Estimation (Franco Galante et al., 2024)</a></li><li><a href=#6767--201316-discriminant-distance-aware-representation-on-deterministic-uncertainty-quantification-methods-jiaxin-zhang-et-al-2024>(67/67 | 201/316) Discriminant Distance-Aware Representation on Deterministic Uncertainty Quantification Methods (Jiaxin Zhang et al., 2024)</a></li></ul></li><li><a href=#csro-6>cs.RO (6)</a><ul><li><a href=#16--202316-tiny-reinforcement-learning-for-quadruped-locomotion-using-decision-transformers-orhan-eren-akgün-et-al-2024>(1/6 | 202/316) Tiny Reinforcement Learning for Quadruped Locomotion using Decision Transformers (Orhan Eren Akgün et al., 2024)</a></li><li><a href=#26--203316-pre-trained-transformer-enabled-strategies-with-human-guided-fine-tuning-for-end-to-end-navigation-of-autonomous-vehicles-dong-hu-et-al-2024>(2/6 | 203/316) Pre-trained Transformer-Enabled Strategies with Human-Guided Fine-Tuning for End-to-end Navigation of Autonomous Vehicles (Dong Hu et al., 2024)</a></li><li><a href=#36--204316-dinobot-robot-manipulation-via-retrieval-and-alignment-with-vision-foundation-models-norman-di-palo-et-al-2024>(3/6 | 204/316) DINOBot: Robot Manipulation via Retrieval and Alignment with Vision Foundation Models (Norman Di Palo et al., 2024)</a></li><li><a href=#46--205316-a-recurrent-neural-network-enhanced-unscented-kalman-filter-for-human-motion-prediction-wansong-liu-et-al-2024>(4/6 | 205/316) A Recurrent Neural Network Enhanced Unscented Kalman Filter for Human Motion Prediction (Wansong Liu et al., 2024)</a></li><li><a href=#56--206316-n-mpc-for-deep-neural-network-based-collision-avoidance-exploiting-depth-images-martin-jacquet-et-al-2024>(5/6 | 206/316) N-MPC for Deep Neural Network-Based Collision Avoidance exploiting Depth Images (Martin Jacquet et al., 2024)</a></li><li><a href=#66--207316-autonomous-reality-modelling-for-cultural-heritage-sites-employing-cooperative-quadrupedal-robots-and-unmanned-aerial-vehicles-nikolaos-giakoumidis-et-al-2024>(6/6 | 207/316) Autonomous Reality Modelling for Cultural Heritage Sites employing cooperative quadrupedal robots and unmanned aerial vehicles (Nikolaos Giakoumidis et al., 2024)</a></li></ul></li><li><a href=#statml-3>stat.ML (3)</a><ul><li><a href=#13--208316-mode-estimation-with-partial-feedback-charles-arnal-et-al-2024>(1/3 | 208/316) Mode Estimation with Partial Feedback (Charles Arnal et al., 2024)</a></li><li><a href=#23--209316-sgd-with-clipping-is-secretly-estimating-the-median-gradient-fabian-schaipp-et-al-2024>(2/3 | 209/316) SGD with Clipping is Secretly Estimating the Median Gradient (Fabian Schaipp et al., 2024)</a></li><li><a href=#33--210316-learning-under-singularity-an-information-criterion-improving-wbic-and-sbic-lirui-liu-et-al-2024>(3/3 | 210/316) Learning under Singularity: An Information Criterion improving WBIC and sBIC (Lirui Liu et al., 2024)</a></li></ul></li><li><a href=#csai-9>cs.AI (9)</a><ul><li><a href=#19--211316-more-3smultimodal-based-offline-reinforcement-learning-with-shared-semantic-spaces-tianyu-zheng-et-al-2024>(1/9 | 211/316) MORE-3S:Multimodal-based Offline Reinforcement Learning with Shared Semantic Spaces (Tianyu Zheng et al., 2024)</a></li><li><a href=#29--212316-toward-transformers-revolutionizing-the-solution-of-mixed-integer-programs-with-transformers-joshua-f-cooper-et-al-2024>(2/9 | 212/316) Toward TransfORmers: Revolutionizing the Solution of Mixed Integer Programs with Transformers (Joshua F. Cooper et al., 2024)</a></li><li><a href=#39--213316-from-cloud-to-edge-rethinking-generative-ai-for-low-resource-design-challenges-sai-krishna-revanth-vuruma-et-al-2024>(3/9 | 213/316) From Cloud to Edge: Rethinking Generative AI for Low-Resource Design Challenges (Sai Krishna Revanth Vuruma et al., 2024)</a></li><li><a href=#49--214316-xrl-bench-a-benchmark-for-evaluating-and-comparing-explainable-reinforcement-learning-techniques-yu-xiong-et-al-2024>(4/9 | 214/316) XRL-Bench: A Benchmark for Evaluating and Comparing Explainable Reinforcement Learning Techniques (Yu Xiong et al., 2024)</a></li><li><a href=#59--215316-analyzing-operator-states-and-the-impact-of-ai-enhanced-decision-support-in-control-rooms-a-human-in-the-loop-specialized-reinforcement-learning-framework-for-intervention-strategies-ammar-n-abbas-et-al-2024>(5/9 | 215/316) Analyzing Operator States and the Impact of AI-Enhanced Decision Support in Control Rooms: A Human-in-the-Loop Specialized Reinforcement Learning Framework for Intervention Strategies (Ammar N. Abbas et al., 2024)</a></li><li><a href=#69--216316-improving-neural-based-classification-with-logical-background-knowledge-arthur-ledaguenel-et-al-2024>(6/9 | 216/316) Improving Neural-based Classification with Logical Background Knowledge (Arthur Ledaguenel et al., 2024)</a></li><li><a href=#79--217316-patient-centric-knowledge-graphs-a-survey-of-current-methods-challenges-and-applications-hassan-s-al-khatib-et-al-2024>(7/9 | 217/316) Patient-Centric Knowledge Graphs: A Survey of Current Methods, Challenges, and Applications (Hassan S. Al Khatib et al., 2024)</a></li><li><a href=#89--218316-random-graph-set-and-evidence-pattern-reasoning-model-tianxiang-zhan-et-al-2024>(8/9 | 218/316) Random Graph Set and Evidence Pattern Reasoning Model (Tianxiang Zhan et al., 2024)</a></li><li><a href=#99--219316-learning-and-sustaining-shared-normative-systems-via-bayesian-rule-induction-in-markov-games-ninell-oldenburg-et-al-2024>(9/9 | 219/316) Learning and Sustaining Shared Normative Systems via Bayesian Rule Induction in Markov Games (Ninell Oldenburg et al., 2024)</a></li></ul></li><li><a href=#eessas-3>eess.AS (3)</a><ul><li><a href=#13--220316-emo-superb-an-in-depth-look-at-speech-emotion-recognition-haibin-wu-et-al-2024>(1/3 | 220/316) EMO-SUPERB: An In-depth Look at Speech Emotion Recognition (Haibin Wu et al., 2024)</a></li><li><a href=#23--221316-plugin-speech-enhancement-a-universal-speech-enhancement-framework-inspired-by-dynamic-neural-network-yanan-chen-et-al-2024>(2/3 | 221/316) Plugin Speech Enhancement: A Universal Speech Enhancement Framework Inspired by Dynamic Neural Network (Yanan Chen et al., 2024)</a></li><li><a href=#33--222316-codec-superb-an-in-depth-analysis-of-sound-codec-models-haibin-wu-et-al-2024>(3/3 | 222/316) Codec-SUPERB: An In-Depth Analysis of Sound Codec Models (Haibin Wu et al., 2024)</a></li></ul></li><li><a href=#csir-8>cs.IR (8)</a><ul><li><a href=#18--223316-distributionally-robust-graph-based-recommendation-system-bohao-wang-et-al-2024>(1/8 | 223/316) Distributionally Robust Graph-based Recommendation System (Bohao Wang et al., 2024)</a></li><li><a href=#28--224316-understanding-and-mitigating-the-threat-of-vec2text-to-dense-retrieval-systems-shengyao-zhuang-et-al-2024>(2/8 | 224/316) Understanding and Mitigating the Threat of Vec2Text to Dense Retrieval Systems (Shengyao Zhuang et al., 2024)</a></li><li><a href=#38--225316-unlocking-the-why-of-buying-introducing-a-new-dataset-and-benchmark-for-purchase-reason-and-post-purchase-experience-tao-chen-et-al-2024>(3/8 | 225/316) Unlocking the `Why&rsquo; of Buying: Introducing a New Dataset and Benchmark for Purchase Reason and Post-Purchase Experience (Tao Chen et al., 2024)</a></li><li><a href=#48--226316-an-autonomous-large-language-model-agent-for-chemical-literature-data-mining-kexin-chen-et-al-2024>(4/8 | 226/316) An Autonomous Large Language Model Agent for Chemical Literature Data Mining (Kexin Chen et al., 2024)</a></li><li><a href=#58--227316-towards-trustworthy-reranking-a-simple-yet-effective-abstention-mechanism-hippolyte-gisserot-boukhlef-et-al-2024>(5/8 | 227/316) Towards Trustworthy Reranking: A Simple yet Effective Abstention Mechanism (Hippolyte Gisserot-Boukhlef et al., 2024)</a></li><li><a href=#68--228316-unlocking-insights-semantic-search-in-jupyter-notebooks-lan-li-et-al-2024>(6/8 | 228/316) Unlocking Insights: Semantic Search in Jupyter Notebooks (Lan Li et al., 2024)</a></li><li><a href=#78--229316-interpreting-conversational-dense-retrieval-by-rewriting-enhanced-inversion-of-session-embedding-yiruo-cheng-et-al-2024>(7/8 | 229/316) Interpreting Conversational Dense Retrieval by Rewriting-Enhanced Inversion of Session Embedding (Yiruo Cheng et al., 2024)</a></li><li><a href=#88--230316-bmlp-behavior-aware-mlp-for-heterogeneous-sequential-recommendation-weixin-li-et-al-2024>(8/8 | 230/316) BMLP: Behavior-aware MLP for Heterogeneous Sequential Recommendation (Weixin Li et al., 2024)</a></li></ul></li><li><a href=#cscr-5>cs.CR (5)</a><ul><li><a href=#15--231316-prompt-stealing-attacks-against-large-language-models-zeyang-sha-et-al-2024>(1/5 | 231/316) Prompt Stealing Attacks Against Large Language Models (Zeyang Sha et al., 2024)</a></li><li><a href=#25--232316-the-wolf-within-covert-injection-of-malice-into-mllm-societies-via-an-mllm-operative-zhen-tan-et-al-2024>(2/5 | 232/316) The Wolf Within: Covert Injection of Malice into MLLM Societies via an MLLM Operative (Zhen Tan et al., 2024)</a></li><li><a href=#35--233316-generative-ai-security-challenges-and-countermeasures-banghua-zhu-et-al-2024>(3/5 | 233/316) Generative AI Security: Challenges and Countermeasures (Banghua Zhu et al., 2024)</a></li><li><a href=#45--234316-apt-mmf-an-advanced-persistent-threat-actor-attribution-method-based-on-multimodal-and-multilevel-feature-fusion-nan-xiao-et-al-2024>(4/5 | 234/316) APT-MMF: An advanced persistent threat actor attribution method based on multimodal and multilevel feature fusion (Nan Xiao et al., 2024)</a></li><li><a href=#55--235316-robust-wide-robust-watermarking-against-instruction-driven-image-editing-runyi-hu-et-al-2024>(5/5 | 235/316) Robust-Wide: Robust Watermarking against Instruction-driven Image Editing (Runyi Hu et al., 2024)</a></li></ul></li><li><a href=#eesssy-8>eess.SY (8)</a><ul><li><a href=#18--236316-ascend-accurate-yet-efficient-end-to-end-stochastic-computing-acceleration-of-vision-transformer-tong-xie-et-al-2024>(1/8 | 236/316) ASCEND: Accurate yet Efficient End-to-End Stochastic Computing Acceleration of Vision Transformer (Tong Xie et al., 2024)</a></li><li><a href=#28--237316-electric-field-evaluation-of-reconfigurable-intelligent-surface-in-wireless-networks-zhuangzhuang-cui-et-al-2024>(2/8 | 237/316) Electric Field Evaluation of Reconfigurable Intelligent Surface in Wireless Networks (Zhuangzhuang Cui et al., 2024)</a></li><li><a href=#38--238316-autoencoder-with-ordered-variance-for-nonlinear-model-identification-midhun-t-augustine-et-al-2024>(3/8 | 238/316) Autoencoder with Ordered Variance for Nonlinear Model Identification (Midhun T. Augustine et al., 2024)</a></li><li><a href=#48--239316-gimbal-actuator-modeling-for-a-spin-stabilized-spacecraft-equipped-with-a-1dof-gimbaled-thruster-and-two-reaction-wheels-hamed-kouhi-et-al-2024>(4/8 | 239/316) Gimbal Actuator Modeling for a Spin-Stabilized Spacecraft Equipped with a 1DoF Gimbaled-Thruster and two Reaction Wheels (Hamed Kouhi et al., 2024)</a></li><li><a href=#58--240316-smart-mobility-digital-twin-based-automated-vehicle-navigation-system-a-proof-of-concept-kui-wang-et-al-2024>(5/8 | 240/316) Smart Mobility Digital Twin Based Automated Vehicle Navigation System: A Proof of Concept (Kui Wang et al., 2024)</a></li><li><a href=#68--241316-robust-model-predictive-control-for-nonlinear-discrete-time-systems-using-iterative-time-varying-constraint-tightening-daniel-d-leister-et-al-2024>(6/8 | 241/316) Robust Model Predictive Control for nonlinear discrete-time systems using iterative time-varying constraint tightening (Daniel D. Leister et al., 2024)</a></li><li><a href=#78--242316-formal-synthesis-of-controllers-for-safety-critical-autonomous-systems-developments-and-challenges-xiang-yin-et-al-2024>(7/8 | 242/316) Formal Synthesis of Controllers for Safety-Critical Autonomous Systems: Developments and Challenges (Xiang Yin et al., 2024)</a></li><li><a href=#88--243316-antifragile-perimeter-control-anticipating-and-gaining-from-disruptions-with-reinforcement-learning-linghang-sun-et-al-2024>(8/8 | 243/316) Antifragile Perimeter Control: Anticipating and Gaining from Disruptions with Reinforcement Learning (Linghang Sun et al., 2024)</a></li></ul></li><li><a href=#q-fincp-1>q-fin.CP (1)</a><ul><li><a href=#11--244316-deep-hedging-with-market-impact-andrei-neagu-et-al-2024>(1/1 | 244/316) Deep Hedging with Market Impact (Andrei Neagu et al., 2024)</a></li></ul></li><li><a href=#csgr-2>cs.GR (2)</a><ul><li><a href=#12--245316-flashtex-fast-relightable-mesh-texturing-with-lightcontrolnet-kangle-deng-et-al-2024>(1/2 | 245/316) FlashTex: Fast Relightable Mesh Texturing with LightControlNet (Kangle Deng et al., 2024)</a></li><li><a href=#22--246316-real-time-high-resolution-view-synthesis-of-complex-scenes-with-explicit-3d-visibility-reasoning-tiansong-zhou-et-al-2024>(2/2 | 246/316) Real-time High-resolution View Synthesis of Complex Scenes with Explicit 3D Visibility Reasoning (Tiansong Zhou et al., 2024)</a></li></ul></li><li><a href=#csdc-2>cs.DC (2)</a><ul><li><a href=#12--247316-energy-efficient-wireless-federated-learning-via-doubly-adaptive-quantization-xuefeng-han-et-al-2024>(1/2 | 247/316) Energy-Efficient Wireless Federated Learning via Doubly Adaptive Quantization (Xuefeng Han et al., 2024)</a></li><li><a href=#22--248316-fog-enabled-distributed-training-architecture-for-federated-learning-aditya-kumar-et-al-2024>(2/2 | 248/316) Fog enabled distributed training architecture for federated learning (Aditya Kumar et al., 2024)</a></li></ul></li><li><a href=#csse-8>cs.SE (8)</a><ul><li><a href=#18--249316-advancing-genai-assisted-programming--a-comparative-study-on-prompt-efficiency-and-code-quality-between-gpt-4-and-glm-4-angus-yang-et-al-2024>(1/8 | 249/316) Advancing GenAI Assisted Programming&ndash;A Comparative Study on Prompt Efficiency and Code Quality Between GPT-4 and GLM-4 (Angus Yang et al., 2024)</a></li><li><a href=#28--250316-go-static-contextualized-logging-statement-generation-yichen-li-et-al-2024>(2/8 | 250/316) Go Static: Contextualized Logging Statement Generation (Yichen Li et al., 2024)</a></li><li><a href=#38--251316-a-disruptive-research-playbook-for-studying-disruptive-innovations-margaret-anne-storey-et-al-2024>(3/8 | 251/316) A Disruptive Research Playbook for Studying Disruptive Innovations (Margaret-Anne Storey et al., 2024)</a></li><li><a href=#48--252316-multi-level-ml-based-burst-aware-autoscaling-for-slo-assurance-and-cost-efficiency-chunyang-meng-et-al-2024>(4/8 | 252/316) Multi-Level ML Based Burst-Aware Autoscaling for SLO Assurance and Cost Efficiency (Chunyang Meng et al., 2024)</a></li><li><a href=#58--253316-measuring-impacts-of-poisoning-on-model-parameters-and-neuron-activations-a-case-study-of-poisoning-codebert-aftab-hussain-et-al-2024>(5/8 | 253/316) Measuring Impacts of Poisoning on Model Parameters and Neuron Activations: A Case Study of Poisoning CodeBERT (Aftab Hussain et al., 2024)</a></li><li><a href=#68--254316-towards-mlops-a-devops-tools-recommender-system-for-machine-learning-system-pir-sami-ullah-shah-et-al-2024>(6/8 | 254/316) Towards MLOps: A DevOps Tools Recommender System for Machine Learning System (Pir Sami Ullah Shah et al., 2024)</a></li><li><a href=#78--255316-scaling-laws-behind-code-understanding-model-jiayi-lin-et-al-2024>(7/8 | 255/316) Scaling Laws Behind Code Understanding Model (Jiayi Lin et al., 2024)</a></li><li><a href=#88--256316-choosing-a-suitable-requirement-prioritization-method-a-survey-esraa-alhenawi-et-al-2024>(8/8 | 256/316) Choosing a Suitable Requirement Prioritization Method: A Survey (Esraa Alhenawi et al., 2024)</a></li></ul></li><li><a href=#statme-1>stat.ME (1)</a><ul><li><a href=#11--257316-integrating-active-learning-in-causal-inference-with-interference-a-novel-approach-in-online-experiments-hongtao-zhu-et-al-2024>(1/1 | 257/316) Integrating Active Learning in Causal Inference with Interference: A Novel Approach in Online Experiments (Hongtao Zhu et al., 2024)</a></li></ul></li><li><a href=#mathna-4>math.NA (4)</a><ul><li><a href=#14--258316-pdeformer-towards-a-foundation-model-for-one-dimensional-partial-differential-equations-zhanhong-ye-et-al-2024>(1/4 | 258/316) PDEformer: Towards a Foundation Model for One-Dimensional Partial Differential Equations (Zhanhong Ye et al., 2024)</a></li><li><a href=#24--259316-contractivity-of-neural-odes-an-eigenvalue-optimization-problem-nicola-guglielmi-et-al-2024>(2/4 | 259/316) Contractivity of neural ODEs: an eigenvalue optimization problem (Nicola Guglielmi et al., 2024)</a></li><li><a href=#34--260316-tensor-completion-with-bmd-factor-nuclear-norm-minimization-fan-tian-et-al-2024>(3/4 | 260/316) Tensor Completion with BMD Factor Nuclear Norm Minimization (Fan Tian et al., 2024)</a></li><li><a href=#44--261316-tree-semi-separable-matrices-a-simultaneous-generalization-of-sequentially-and-hierarchically-semi-separable-representations-nithin-govindarajan-et-al-2024>(4/4 | 261/316) Tree semi-separable matrices: a simultaneous generalization of sequentially and hierarchically semi-separable representations (Nithin Govindarajan et al., 2024)</a></li></ul></li><li><a href=#csmm-2>cs.MM (2)</a><ul><li><a href=#12--262316-a-user-friendly-framework-for-generating-model-preferred-prompts-in-text-to-image-synthesis-nailei-hei-et-al-2024>(1/2 | 262/316) A User-Friendly Framework for Generating Model-Preferred Prompts in Text-to-Image Synthesis (Nailei Hei et al., 2024)</a></li><li><a href=#22--263316-television-discourse-decoded-comprehensive-multimodal-analytics-at-scale-anmol-agarwal-et-al-2024>(2/2 | 263/316) Television Discourse Decoded: Comprehensive Multimodal Analytics at Scale (Anmol Agarwal et al., 2024)</a></li></ul></li><li><a href=#quant-ph-5>quant-ph (5)</a><ul><li><a href=#15--264316-quantum-embedding-with-transformer-for-high-dimensional-data-hao-yuan-chen-et-al-2024>(1/5 | 264/316) Quantum Embedding with Transformer for High-dimensional Data (Hao-Yuan Chen et al., 2024)</a></li><li><a href=#25--265316-ketgpt----dataset-augmentation-of-quantum-circuits-using-transformers-boran-apak-et-al-2024>(2/5 | 265/316) KetGPT &ndash; Dataset Augmentation of Quantum Circuits using Transformers (Boran Apak et al., 2024)</a></li><li><a href=#35--266316-a-unifying-primary-framework-for-quantum-graph-neural-networks-from-quantum-graph-states-ammar-daskin-2024>(3/5 | 266/316) A unifying primary framework for quantum graph neural networks from quantum graph states (Ammar Daskin, 2024)</a></li><li><a href=#45--267316-quantum-pseudorandomness-cannot-be-shrunk-in-a-black-box-way-samuel-bouaziz--ermann-et-al-2024>(4/5 | 267/316) Quantum Pseudorandomness Cannot Be Shrunk In a Black-Box Way (Samuel Bouaziz&ndash;Ermann et al., 2024)</a></li><li><a href=#55--268316-guarantees-on-warm-started-qaoa-single-round-approximation-ratios-for-3-regular-maxcut-and-higher-round-scaling-limits-reuben-tate-et-al-2024>(5/5 | 268/316) Guarantees on Warm-Started QAOA: Single-Round Approximation Ratios for 3-Regular MAXCUT and Higher-Round Scaling Limits (Reuben Tate et al., 2024)</a></li></ul></li><li><a href=#physicssoc-ph-1>physics.soc-ph (1)</a><ul><li><a href=#11--269316-an-evolutionary-game-with-reputation-based-imitation-mutation-dynamics-kehuan-feng-et-al-2024>(1/1 | 269/316) An evolutionary game with reputation-based imitation-mutation dynamics (Kehuan Feng et al., 2024)</a></li></ul></li><li><a href=#cscy-2>cs.CY (2)</a><ul><li><a href=#12--270316-towards-fair-allocation-in-social-commerce-platforms-anjali-gupta-et-al-2024>(1/2 | 270/316) Towards Fair Allocation in Social Commerce Platforms (Anjali Gupta et al., 2024)</a></li><li><a href=#22--271316-are-large-language-models-llms-good-social-predictors-kaiqi-yang-et-al-2024>(2/2 | 271/316) Are Large Language Models (LLMs) Good Social Predictors? (Kaiqi Yang et al., 2024)</a></li></ul></li><li><a href=#physicsflu-dyn-1>physics.flu-dyn (1)</a><ul><li><a href=#11--272316-a-fully-integrated-lattice-boltzmann-method-for-fluid-structure-interaction-yue-sun-et-al-2024>(1/1 | 272/316) A fully-integrated lattice Boltzmann method for fluid-structure interaction (Yue Sun et al., 2024)</a></li></ul></li><li><a href=#cssi-3>cs.SI (3)</a><ul><li><a href=#13--273316-ego-group-partition-a-novel-framework-for-improving-ego-experiments-in-social-networks-lu-deng-et-al-2024>(1/3 | 273/316) Ego Group Partition: A Novel Framework for Improving Ego Experiments in Social Networks (Lu Deng et al., 2024)</a></li><li><a href=#23--274316-are-fact-checking-tools-reliable-an-evaluation-of-google-fact-check-qiangeng-yang-et-al-2024>(2/3 | 274/316) Are Fact-Checking Tools Reliable? An Evaluation of Google Fact Check (Qiangeng Yang et al., 2024)</a></li><li><a href=#33--275316-effective-edge-ranking-via-random-walk-with-restart-renchi-yang-2024>(3/3 | 275/316) Effective Edge Ranking via Random Walk with Restart (Renchi Yang, 2024)</a></li></ul></li><li><a href=#csdb-2>cs.DB (2)</a><ul><li><a href=#12--276316-everything-you-always-wanted-to-know-about-storage-compressibility-of-pre-trained-ml-models-but-were-afraid-to-ask-zhaoyuan-su-et-al-2024>(1/2 | 276/316) Everything You Always Wanted to Know About Storage Compressibility of Pre-Trained ML Models but Were Afraid to Ask (Zhaoyuan Su et al., 2024)</a></li><li><a href=#22--277316-xling-a-learned-filter-framework-for-accelerating-high-dimensional-approximate-similarity-join-yifan-wang-et-al-2024>(2/2 | 277/316) Xling: A Learned Filter Framework for Accelerating High-Dimensional Approximate Similarity Join (Yifan Wang et al., 2024)</a></li></ul></li><li><a href=#mathoc-1>math.OC (1)</a><ul><li><a href=#11--278316-controlling-large-electric-vehicle-charging-stations-via-user-behavior-modeling-and-stochastic-programming-alban-puech-et-al-2024>(1/1 | 278/316) Controlling Large Electric Vehicle Charging Stations via User Behavior Modeling and Stochastic Programming (Alban Puech et al., 2024)</a></li></ul></li><li><a href=#csni-3>cs.NI (3)</a><ul><li><a href=#13--279316-a-lightweight-machine-learning-approach-for-delay-aware-cell-switching-in-6g-haps-networks-görkem-berkay-koç-et-al-2024>(1/3 | 279/316) A Lightweight Machine Learning Approach for Delay-Aware Cell-Switching in 6G HAPS Networks (Görkem Berkay Koç et al., 2024)</a></li><li><a href=#23--280316-enhanced-physical-layer-security-for-full-duplex-symbiotic-radio-with-an-generation-and-forward-noise-suppression-chi-jin-et-al-2024>(2/3 | 280/316) Enhanced Physical Layer Security for Full-duplex Symbiotic Radio with AN Generation and Forward Noise Suppression (Chi Jin et al., 2024)</a></li><li><a href=#33--281316-federated-learning-for-iotedgefog-computing-systems-balqees-talal-hasan-et-al-2024>(3/3 | 281/316) Federated Learning for Iot/Edge/Fog Computing Systems (Balqees Talal Hasan et al., 2024)</a></li></ul></li><li><a href=#cshc-3>cs.HC (3)</a><ul><li><a href=#13--282316-solving-the-decision-making-differential-equations-from-eye-fixation-data-in-unity-software-by-using-hermite-long-short-term-memory-neural-network-kourosh-parand-et-al-2024>(1/3 | 282/316) Solving the decision-making differential equations from eye fixation data in Unity software by using Hermite Long-Short-Term Memory neural network (Kourosh Parand et al., 2024)</a></li><li><a href=#23--283316-exploring-ai-assisted-ideation-and-prototyping-for-choreography-yimeng-liu-et-al-2024>(2/3 | 283/316) Exploring AI-assisted Ideation and Prototyping for Choreography (Yimeng Liu et al., 2024)</a></li><li><a href=#33--284316-data-storytelling-in-data-visualisation-does-it-enhance-the-efficiency-and-effectiveness-of-information-retrieval-and-insights-comprehension-honbo-shao-et-al-2024>(3/3 | 284/316) Data Storytelling in Data Visualisation: Does it Enhance the Efficiency and Effectiveness of Information Retrieval and Insights Comprehension? (Honbo Shao et al., 2024)</a></li></ul></li><li><a href=#q-bioqm-1>q-bio.QM (1)</a><ul><li><a href=#11--285316-integrating-deep-learning-and-synthetic-biology-a-co-design-approach-for-enhancing-gene-expression-via-n-terminal-coding-sequences-zhanglu-yan-et-al-2024>(1/1 | 285/316) Integrating Deep Learning and Synthetic Biology: A Co-Design Approach for Enhancing Gene Expression via N-terminal Coding Sequences (Zhanglu Yan et al., 2024)</a></li></ul></li><li><a href=#cssd-4>cs.SD (4)</a><ul><li><a href=#14--286316-guiding-the-underwater-acoustic-target-recognition-with-interpretable-contrastive-learning-yuan-xie-et-al-2024>(1/4 | 286/316) Guiding the underwater acoustic target recognition with interpretable contrastive learning (Yuan Xie et al., 2024)</a></li><li><a href=#24--287316-not-all-weights-are-created-equal-enhancing-energy-efficiency-in-on-device-streaming-speech-recognition-yang-li-et-al-2024>(2/4 | 287/316) Not All Weights Are Created Equal: Enhancing Energy Efficiency in On-Device Streaming Speech Recognition (Yang Li et al., 2024)</a></li><li><a href=#34--288316-structure-informed-positional-encoding-for-music-generation-manvi-agarwal-et-al-2024>(3/4 | 288/316) Structure-informed Positional Encoding for Music Generation (Manvi Agarwal et al., 2024)</a></li><li><a href=#44--289316-singvisio-visual-analytics-of-diffusion-model-for-singing-voice-conversion-liumeng-xue-et-al-2024>(4/4 | 289/316) SingVisio: Visual Analytics of Diffusion Model for Singing Voice Conversion (Liumeng Xue et al., 2024)</a></li></ul></li><li><a href=#csne-2>cs.NE (2)</a><ul><li><a href=#12--290316-evolutionary-reinforcement-learning-a-systematic-review-and-future-directions-yuanguo-lin-et-al-2024>(1/2 | 290/316) Evolutionary Reinforcement Learning: A Systematic Review and Future Directions (Yuanguo Lin et al., 2024)</a></li><li><a href=#22--291316-sonata-self-adaptive-evolutionary-framework-for-hardware-aware-neural-architecture-search-halima-bouzidi-et-al-2024>(2/2 | 291/316) SONATA: Self-adaptive Evolutionary Framework for Hardware-aware Neural Architecture Search (Halima Bouzidi et al., 2024)</a></li></ul></li><li><a href=#hep-th-1>hep-th (1)</a><ul><li><a href=#11--292316-rigor-with-machine-learning-from-field-theory-to-the-poincaré-conjecture-sergei-gukov-et-al-2024>(1/1 | 292/316) Rigor with Machine Learning from Field Theory to the Poincaré Conjecture (Sergei Gukov et al., 2024)</a></li></ul></li><li><a href=#csds-7>cs.DS (7)</a><ul><li><a href=#17--293316-efficient-enumeration-of-large-maximal-k-plexes-qihao-cheng-et-al-2024>(1/7 | 293/316) Efficient Enumeration of Large Maximal k-Plexes (Qihao Cheng et al., 2024)</a></li><li><a href=#27--294316-scalable-pattern-matching-in-computation-graphs-luca-mondada-et-al-2024>(2/7 | 294/316) Scalable Pattern Matching in Computation Graphs (Luca Mondada et al., 2024)</a></li><li><a href=#37--295316-online-matching-on-3-uniform-hypergraphs-sander-borst-et-al-2024>(3/7 | 295/316) Online Matching on $3$-Uniform Hypergraphs (Sander Borst et al., 2024)</a></li><li><a href=#47--296316-deterministic-dynamic-edge-colouring-aleksander-b-g-christiansen-2024>(4/7 | 296/316) Deterministic Dynamic Edge-Colouring (Aleksander B. G. Christiansen, 2024)</a></li><li><a href=#57--297316-locally-rainbow-paths-till-fluschnik-et-al-2024>(5/7 | 297/316) Locally Rainbow Paths (Till Fluschnik et al., 2024)</a></li><li><a href=#67--298316-nearly-optimal-fault-tolerant-distance-oracle-dipan-dey-et-al-2024>(6/7 | 298/316) Nearly Optimal Fault Tolerant Distance Oracle (Dipan Dey et al., 2024)</a></li><li><a href=#77--299316-distance-recoloring-niranka-banerjee-et-al-2024>(7/7 | 299/316) Distance Recoloring (Niranka Banerjee et al., 2024)</a></li></ul></li><li><a href=#eesssp-1>eess.SP (1)</a><ul><li><a href=#11--300316-szcore-a-seizure-community-open-source-research-evaluation-framework-for-the-validation-of-eeg-based-automated-seizure-detection-algorithms-jonathan-dan-et-al-2024>(1/1 | 300/316) SzCORE: A Seizure Community Open-source Research Evaluation framework for the validation of EEG-based automated seizure detection algorithms (Jonathan Dan et al., 2024)</a></li></ul></li><li><a href=#econth-1>econ.TH (1)</a><ul><li><a href=#11--301316-stable-matching-as-transportation-federico-echenique-et-al-2024>(1/1 | 301/316) Stable matching as transportation (Federico Echenique et al., 2024)</a></li></ul></li><li><a href=#csar-2>cs.AR (2)</a><ul><li><a href=#12--302316-enabling-efficient-hybrid-systolic-computation-in-shared-l1-memory-manycore-clusters-sergio-mazzola-et-al-2024>(1/2 | 302/316) Enabling Efficient Hybrid Systolic Computation in Shared L1-Memory Manycore Clusters (Sergio Mazzola et al., 2024)</a></li><li><a href=#22--303316-sat-based-exact-modulo-scheduling-mapping-for-resource-constrained-cgras-cristian-tirelli-et-al-2024>(2/2 | 303/316) SAT-based Exact Modulo Scheduling Mapping for Resource-Constrained CGRAs (Cristian Tirelli et al., 2024)</a></li></ul></li><li><a href=#physicscomp-ph-1>physics.comp-ph (1)</a><ul><li><a href=#11--304316-how-temporal-unrolling-supports-neural-physics-simulators-bjoern-list-et-al-2024>(1/1 | 304/316) How Temporal Unrolling Supports Neural Physics Simulators (Bjoern List et al., 2024)</a></li></ul></li><li><a href=#csit-1>cs.IT (1)</a><ul><li><a href=#11--305316-quantifying-privacy-via-information-density-leonhard-grosse-et-al-2024>(1/1 | 305/316) Quantifying Privacy via Information Density (Leonhard Grosse et al., 2024)</a></li></ul></li><li><a href=#csdl-1>cs.DL (1)</a><ul><li><a href=#11--306316-a-literature-review-of-literature-reviews-in-pattern-analysis-and-machine-intelligence-penghai-zhao-et-al-2024>(1/1 | 306/316) A Literature Review of Literature Reviews in Pattern Analysis and Machine Intelligence (Penghai Zhao et al., 2024)</a></li></ul></li><li><a href=#cslo-1>cs.LO (1)</a><ul><li><a href=#11--307316-modular-assurance-of-complex-systems-using-contract-based-design-principles-dag-mcgeorge-et-al-2024>(1/1 | 307/316) Modular Assurance of Complex Systems Using Contract-Based Design Principles (Dag McGeorge et al., 2024)</a></li></ul></li><li><a href=#eessiv-2>eess.IV (2)</a><ul><li><a href=#12--308316-denoising-oct-images-using-steered-mixture-of-experts-with-multi-model-inference-aytaç-özkan-et-al-2024>(1/2 | 308/316) Denoising OCT Images Using Steered Mixture of Experts with Multi-Model Inference (Aytaç Özkan et al., 2024)</a></li><li><a href=#22--309316-wmh_seg-transformer-based-u-net-for-robust-and-automatic-white-matter-hyperintensity-segmentation-across-15t-3t-and-7t-jinghang-li-et-al-2024>(2/2 | 309/316) wmh_seg: Transformer based U-Net for Robust and Automatic White Matter Hyperintensity Segmentation across 1.5T, 3T and 7T (Jinghang Li et al., 2024)</a></li></ul></li><li><a href=#cspl-1>cs.PL (1)</a><ul><li><a href=#11--310316-getting-into-the-flow-towards-better-type-error-messages-for-constraint-based-type-inference-ishan-bhanuka-et-al-2024>(1/1 | 310/316) Getting into the Flow: Towards Better Type Error Messages for Constraint-Based Type Inference (Ishan Bhanuka et al., 2024)</a></li></ul></li><li><a href=#cscg-3>cs.CG (3)</a><ul><li><a href=#13--311316-greedy-monochromatic-island-partitions-steven-van-den-broek-et-al-2024>(1/3 | 311/316) Greedy Monochromatic Island Partitions (Steven van den Broek et al., 2024)</a></li><li><a href=#23--312316-clustered-planarity-variants-for-level-graphs-simon-d-fink-et-al-2024>(2/3 | 312/316) Clustered Planarity Variants for Level Graphs (Simon D. Fink et al., 2024)</a></li><li><a href=#33--313316-faster-and-deterministic-subtrajectory-clustering-ivor-van-der-hoog-et-al-2024>(3/3 | 313/316) Faster and Deterministic Subtrajectory Clustering (Ivor van der Hoog et al., 2024)</a></li></ul></li><li><a href=#mathco-1>math.CO (1)</a><ul><li><a href=#11--314316-an-improved-lower-bound-on-the-number-of-pseudoline-arrangements-fernando-cortés-kühnast-et-al-2024>(1/1 | 314/316) An Improved Lower Bound on the Number of Pseudoline Arrangements (Fernando Cortés Kühnast et al., 2024)</a></li></ul></li><li><a href=#mathag-1>math.AG (1)</a><ul><li><a href=#11--315316-game-theory-of-undirected-graphical-models-irem-portakal-et-al-2024>(1/1 | 315/316) Game theory of undirected graphical models (Irem Portakal et al., 2024)</a></li></ul></li><li><a href=#cscc-1>cs.CC (1)</a><ul><li><a href=#11--316316-optimal-pspace-hardness-of-approximating-set-cover-reconfiguration-shuichi-hirahara-et-al-2024>(1/1 | 316/316) Optimal PSPACE-hardness of Approximating Set Cover Reconfiguration (Shuichi Hirahara et al., 2024)</a></li></ul></li></ul></nav></div></div></section></div><footer id=footer class="container-fluid text-center align-content-center footer pb-2"><div class="container pt-5"><div class="row text-left"><div class="col-md-4 col-sm-12"><h5>Navigation</h5><ul><li class=nav-item><a class=smooth-scroll href=https://akitenkrad.github.io/akitenkrad-blog/#about>About</a></li><li class=nav-item><a class=smooth-scroll href=https://akitenkrad.github.io/akitenkrad-blog/#recent-posts>Recent Posts</a></li></ul></div><div class="col-md-4 col-sm-12"><h5>Contact me:</h5><ul><li><a href=mailto:contact.to.akitenkrad@gmail.com target=_blank rel=noopener><span><i class="fas fa-envelope"></i></span> <span>contact.to.akitenkrad@gmail.com</span></a></li></ul></div></div></div><hr><div class=container><div class="row text-left"><div class=col-md-4><a id=theme href=https://github.com/hossainemruz/toha target=_blank rel=noopener><img src=/akitenkrad-blog/images/theme-logo_hu8376fd15465fef26ffe66b6bcf0ca686_13669_32x0_resize_box_3.png alt="Toha Theme Logo">
Toha</a></div><div class="col-md-4 text-center">© 2020 Akitenkrad.</div><div class="col-md-4 text-right"><a id=hugo href=https://gohugo.io/ target=_blank rel=noopener>Powered by
<img src=/akitenkrad-blog/images/hugo-logo.svg alt="Hugo Logo" height=18></a></div></div></div></footer><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/jquery-3.4.1.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/popper.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/bootstrap.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/navbar.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/plyr.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/main.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js></script><script src=https://akitenkrad.github.io/akitenkrad-blog/js/single.js></script><script>hljs.initHighlightingOnLoad()</script><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/katex/katex.min.css><script type=text/javascript defer src=https://akitenkrad.github.io/akitenkrad-blog/katex/katex.min.js></script><script type=text/javascript defer src=https://akitenkrad.github.io/akitenkrad-blog/katex/auto-render.min.js></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\[",right:"\\]",display:!0},{left:"\\(",right:"\\)",display:!1}],throwOnError:!0})})</script></body></html>