<!doctype html><html><head><title>arXiv @ 2024.02.10</title>
<meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/bootstrap.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/layouts/main.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/navigators/navbar.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/plyr.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/flag-icon.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/custom/style.css><link href=https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css rel=stylesheet integrity=sha384-9ndCyUaIbzAi2FUVXJi0CjmCapSmO7SnpJef0486qhLnuZ2cdeRhO02iuK6FUUVM crossorigin=anonymous><script async src=https://cdn.jsdelivr.net/npm/es-module-shims@1/dist/es-module-shims.min.js crossorigin=anonymous></script><script type=importmap>
{
  "imports": {
    "@popperjs/core": "https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.8/dist/esm/popper.min.js",
    "bootstrap": "https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.esm.min.js"
  }
}
</script><script src=https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js integrity=sha384-geWF76RCwLtnZ8qwWowPQNguL3RmwHVBC9FhGdlKrxdiJJigb/j/68SIy3Te4Bkz crossorigin=anonymous></script><script type=module src=https://akitenkrad.github.io/akitenkrad-blog/js/custom/custom.js></script><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Muli:wght@300;400;500;600"><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/fontawesome/css/all.min.css><link rel=icon type=image/png href=https://akitenkrad.github.io/akitenkrad-blog/images/favicons/favicon-96x96_huf1ee13f0caf27d1547f91fb46207d708_13005_42x0_resize_box_3.png><meta property="og:title" content="arXiv @ 2024.02.10"><meta property="og:description" content="Primary Categories cs.AI (16) cs.CE (2) cs.CL (38) cs.CR (3) cs.CV (36) cs.CY (3) cs.DL (1) cs.DS (1) cs.GT (2) cs.HC (6) cs.IR (2) cs.IT (4) cs.LG (65) cs.MA (2) cs.MM (1) cs.NI (1) cs.PF (1) cs.RO (15) cs.SD (1) cs.SE (9) cs.SI (1) eess.AS (2) eess.IV (6) eess.SP (2) eess.SY (7) math.NA (6) math.OC (1) physics.ao-ph (1) physics.flu-dyn (1) physics.med-ph (1) physics.optics (1) physics.soc-ph (1) q-bio.BM (1) q-bio.GN (1) q-bio."><meta property="og:type" content="article"><meta property="og:url" content="https://akitenkrad.github.io/akitenkrad-blog/posts/arxiv/202402/20240210000000/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-02-10T00:00:00+00:00"><meta property="article:modified_time" content="2024-02-10T00:00:00+00:00"><meta name=description content="arXiv @ 2024.02.10"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/atom-one-dark.min.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/layouts/single.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/navigators/sidebar.css><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/css/style.css><script type=text/javascript src=//ajax.googleapis.com/ajax/libs/jquery/3.1.1/jquery.min.js></script><script language=JavaScript>$(document).ready(function(){$("a[href^='http']:not([href*='"+location.hostname+"'])").attr("target","_blank")})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-1MYYZQG0WE"></script><script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-1MYYZQG0WE",{anonymize_ip:!1})}</script></head><body data-spy=scroll data-target=#TableOfContents data-offset=80><div class="container-fluid bg-dimmed wrapper"><nav class="navbar navbar-expand-xl top-navbar final-navbar shadow" style=position:fixed><div class=container><button class="navbar-toggler navbar-light navbar-for-sidebar" id=sidebar-toggler type=button onclick=toggleSidebar()>
<span class=navbar-toggler-icon></span>
</button>
<button class="navbar-toggler navbar-light" id=toc-toggler type=button onclick=toggleTOC()>
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse lang-selector" id=top-nav-items><ul class="navbar-nav mr-auto"><li class=nav-item><a class=navbar-brand href=/akitenkrad-blog><img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png alt=Logo>
Akitenkrad's Blog</a></li></ul><ul class="navbar-nav ml-auto"><li class=nav-item><a class=nav-link href=/akitenkrad-blog#home>Home</a></li><li class=nav-item><a class=nav-link href=/akitenkrad-blog#about>About</a></li><li class=nav-item><a class=nav-link href=/akitenkrad-blog#recent-posts>Recent Posts</a></li><li class=nav-item><a class=nav-link id=blog-link href=https://akitenkrad.github.io/akitenkrad-blog/posts>Posts</a></li><li class=nav-item><a class=nav-link id=tag-link href=https://akitenkrad.github.io/akitenkrad-blog/tags>Tags</a></li></ul></div></div><img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png class=d-none id=main-logo alt=Logo>
<img src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_42x0_resize_box_3.png class=d-none id=inverted-logo alt="Inverted Logo"></nav><section class=sidebar-section id=sidebar-section><div class=sidebar-holder><div class=sidebar id=sidebar><form class=mx-auto method=get action=/akitenkrad-blog/search><input type=text name=keyword placeholder=Search data-search id=search-box></form><div class=sidebar-tree><ul class=tree id=tree><li id=list-heading><a href=/posts data-filter=all>Posts</a></li><div class=subtree><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/>Papers</a><ul><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202205/>2022.05</a><ul><li><a href=/akitenkrad-blog/posts/papers/202205/20220518224923/ title="A Context-Aware Citation Recommendation Model with BERT and Graph Convolutional Networks">A Context-Aware Citation Recommendation Model with BERT and Graph Convolutional Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220508162318/ title="A Deep Cascade Model for Multi-Document Reading Comprehension">A Deep Cascade Model for Multi-Document Reading Comprehension</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220506021208/ title="A Primer in BERTology: What We Know About How BERT Works">A Primer in BERTology: What We Know About How BERT Works</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220529131339/ title="Attention Is All You Need">Attention Is All You Need</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220503010000/ title="DeBERTa: Decoding-Enhanced BERT with Disentangled Attention">DeBERTa: Decoding-Enhanced BERT with Disentangled Attention</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220505222900/ title="Dense Passage Retrieval for Open-Domain Question Answering">Dense Passage Retrieval for Open-Domain Question Answering</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220511010217/ title="Multi-Style Generative Reading Comprehension">Multi-Style Generative Reading Comprehension</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220530102936/ title="Neural Machine Translation of Rare Words with Subword Units">Neural Machine Translation of Rare Words with Subword Units</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220523223206/ title="RoBERTa: A Robustly Optimized BERT Pretraining Approach">RoBERTa: A Robustly Optimized BERT Pretraining Approach</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220520124748/ title="Semi-Supervised Classification with Graph Convolutional Networks">Semi-Supervised Classification with Graph Convolutional Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220509110738/ title="Survey on graph embeddings and their applications to machine learning problems on graphs">Survey on graph embeddings and their applications to machine learning problems on graphs</a></li><li><a href=/akitenkrad-blog/posts/papers/202205/20220514151839/ title="UnitedQA: A Hybrid Approach for Open Domain Question Answering">UnitedQA: A Hybrid Approach for Open Domain Question Answering</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202206/>2022.06</a><ul><li><a href=/akitenkrad-blog/posts/papers/202206/20220612105422/ title="Attributed Network Embedding for Learning in a Dynamic Environment">Attributed Network Embedding for Learning in a Dynamic Environment</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220608085622/ title="CodeBERT: A Pre-Trained Model for Programming and Natural Languages">CodeBERT: A Pre-Trained Model for Programming and Natural Languages</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220618223844/ title="High-order Proximity Preserved Embedding for Dynamic Networks">High-order Proximity Preserved Embedding for Dynamic Networks</a></li><li><a href=/akitenkrad-blog/posts/papers/202206/20220602171700/ title="S-Net: From Answer Extraction to Answer Generation for Machine Reading Comprehension">S-Net: From Answer Extraction to Answer Generation for Machine Reading Comprehension</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202207/>2022.07</a><ul><li><a href=/akitenkrad-blog/posts/papers/202207/20220727145036/ title="Dynamic Heterogeneous Graph Embedding Using Hierarchical Attentions">Dynamic Heterogeneous Graph Embedding Using Hierarchical Attentions</a></li><li><a href=/akitenkrad-blog/posts/papers/202207/20220726163444/ title="Dynamic Network Embedding Survey">Dynamic Network Embedding Survey</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202208/>2022.08</a><ul><li><a href=/akitenkrad-blog/posts/papers/202208/20220802103319/ title="Modeling Dynamic Heterogeneous Network for Link Prediction using Hierarchical Attention with Temporal RNN">Modeling Dynamic Heterogeneous Network for Link Prediction using Hierarchical Attention with Temporal RNN</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202209/>2022.09</a><ul><li><a href=/akitenkrad-blog/posts/papers/202209/20220909180827/ title="Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting">Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/papers/202305/>2023.05</a><ul><li><a href=/akitenkrad-blog/posts/papers/202305/20230513094804/ title="Improving Language Understanding by Generative Pre-Training">Improving Language Understanding by Generative Pre-Training</a></li><li><a href=/akitenkrad-blog/posts/papers/202305/20230514200241/ title="Language Models are Unsupervised Multitask Learners">Language Models are Unsupervised Multitask Learners</a></li></ul></li><li><a href=/akitenkrad-blog/posts/papers/202306/ title=2023.06>2023.06</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/akitenkrad-blog/posts/arxiv/>arXiv</a><ul class=active><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202307/>2023.07</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230701000000/ title="arXiv @ 2023.07.01">arXiv @ 2023.07.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230702000000/ title="arXiv @ 2023.07.02">arXiv @ 2023.07.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230703000000/ title="arXiv @ 2023.07.03">arXiv @ 2023.07.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230704000000/ title="arXiv @ 2023.07.04">arXiv @ 2023.07.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230705000000/ title="arXiv @ 2023.07.05">arXiv @ 2023.07.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230706000000/ title="arXiv @ 2023.07.06">arXiv @ 2023.07.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230707000000/ title="arXiv @ 2023.07.07">arXiv @ 2023.07.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230708000000/ title="arXiv @ 2023.07.08">arXiv @ 2023.07.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230709000000/ title="arXiv @ 2023.07.09">arXiv @ 2023.07.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230710000000/ title="arXiv @ 2023.07.10">arXiv @ 2023.07.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230711000000/ title="arXiv @ 2023.07.11">arXiv @ 2023.07.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230712000000/ title="arXiv @ 2023.07.12">arXiv @ 2023.07.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230713000000/ title="arXiv @ 2023.07.13">arXiv @ 2023.07.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230714000000/ title="arXiv @ 2023.07.14">arXiv @ 2023.07.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230715000000/ title="arXiv @ 2023.07.15">arXiv @ 2023.07.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230716000000/ title="arXiv @ 2023.07.16">arXiv @ 2023.07.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230717000000/ title="arXiv @ 2023.07.17">arXiv @ 2023.07.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230718000000/ title="arXiv @ 2023.07.18">arXiv @ 2023.07.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230719000000/ title="arXiv @ 2023.07.19">arXiv @ 2023.07.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230720000000/ title="arXiv @ 2023.07.20">arXiv @ 2023.07.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230721000000/ title="arXiv @ 2023.07.21">arXiv @ 2023.07.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230722000000/ title="arXiv @ 2023.07.22">arXiv @ 2023.07.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230723000000/ title="arXiv @ 2023.07.23">arXiv @ 2023.07.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230724000000/ title="arXiv @ 2023.07.24">arXiv @ 2023.07.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230725000000/ title="arXiv @ 2023.07.25">arXiv @ 2023.07.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230726000000/ title="arXiv @ 2023.07.26">arXiv @ 2023.07.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230727000000/ title="arXiv @ 2023.07.27">arXiv @ 2023.07.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230728000000/ title="arXiv @ 2023.07.28">arXiv @ 2023.07.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230729000000/ title="arXiv @ 2023.07.29">arXiv @ 2023.07.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230730000000/ title="arXiv @ 2023.07.30">arXiv @ 2023.07.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202307/20230731000000/ title="arXiv @ 2023.07.31">arXiv @ 2023.07.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202308/>2023.08</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230801000000/ title="arXiv @ 2023.08.01">arXiv @ 2023.08.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230802000000/ title="arXiv @ 2023.08.02">arXiv @ 2023.08.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230803000000/ title="arXiv @ 2023.08.03">arXiv @ 2023.08.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230804000000/ title="arXiv @ 2023.08.04">arXiv @ 2023.08.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230805000000/ title="arXiv @ 2023.08.05">arXiv @ 2023.08.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230806000000/ title="arXiv @ 2023.08.06">arXiv @ 2023.08.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230807000000/ title="arXiv @ 2023.08.07">arXiv @ 2023.08.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230808000000/ title="arXiv @ 2023.08.08">arXiv @ 2023.08.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230809000000/ title="arXiv @ 2023.08.09">arXiv @ 2023.08.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230810000000/ title="arXiv @ 2023.08.10">arXiv @ 2023.08.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230811000000/ title="arXiv @ 2023.08.11">arXiv @ 2023.08.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230812000000/ title="arXiv @ 2023.08.12">arXiv @ 2023.08.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230813000000/ title="arXiv @ 2023.08.13">arXiv @ 2023.08.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230814000000/ title="arXiv @ 2023.08.14">arXiv @ 2023.08.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230815000000/ title="arXiv @ 2023.08.15">arXiv @ 2023.08.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230816000000/ title="arXiv @ 2023.08.16">arXiv @ 2023.08.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230817000000/ title="arXiv @ 2023.08.17">arXiv @ 2023.08.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230818000000/ title="arXiv @ 2023.08.18">arXiv @ 2023.08.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230819000000/ title="arXiv @ 2023.08.19">arXiv @ 2023.08.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230820000000/ title="arXiv @ 2023.08.20">arXiv @ 2023.08.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230821000000/ title="arXiv @ 2023.08.21">arXiv @ 2023.08.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230822000000/ title="arXiv @ 2023.08.22">arXiv @ 2023.08.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230823000000/ title="arXiv @ 2023.08.23">arXiv @ 2023.08.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230824000000/ title="arXiv @ 2023.08.24">arXiv @ 2023.08.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230825000000/ title="arXiv @ 2023.08.25">arXiv @ 2023.08.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230826000000/ title="arXiv @ 2023.08.26">arXiv @ 2023.08.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230827000000/ title="arXiv @ 2023.08.27">arXiv @ 2023.08.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230828000000/ title="arXiv @ 2023.08.28">arXiv @ 2023.08.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230829000000/ title="arXiv @ 2023.08.29">arXiv @ 2023.08.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230830000000/ title="arXiv @ 2023.08.30">arXiv @ 2023.08.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202308/20230831000000/ title="arXiv @ 2023.08.31">arXiv @ 2023.08.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202309/>2023.09</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230901000000/ title="arXiv @ 2023.09.01">arXiv @ 2023.09.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230902000000/ title="arXiv @ 2023.09.02">arXiv @ 2023.09.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230903000000/ title="arXiv @ 2023.09.03">arXiv @ 2023.09.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230904000000/ title="arXiv @ 2023.09.04">arXiv @ 2023.09.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230905000000/ title="arXiv @ 2023.09.05">arXiv @ 2023.09.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230906000000/ title="arXiv @ 2023.09.06">arXiv @ 2023.09.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230907000000/ title="arXiv @ 2023.09.07">arXiv @ 2023.09.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230908000000/ title="arXiv @ 2023.09.08">arXiv @ 2023.09.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230909000000/ title="arXiv @ 2023.09.09">arXiv @ 2023.09.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230910000000/ title="arXiv @ 2023.09.10">arXiv @ 2023.09.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230911000000/ title="arXiv @ 2023.09.11">arXiv @ 2023.09.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230912000000/ title="arXiv @ 2023.09.12">arXiv @ 2023.09.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230913000000/ title="arXiv @ 2023.09.13">arXiv @ 2023.09.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230914000000/ title="arXiv @ 2023.09.14">arXiv @ 2023.09.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230915000000/ title="arXiv @ 2023.09.15">arXiv @ 2023.09.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230916000000/ title="arXiv @ 2023.09.16">arXiv @ 2023.09.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230917000000/ title="arXiv @ 2023.09.17">arXiv @ 2023.09.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230918000000/ title="arXiv @ 2023.09.18">arXiv @ 2023.09.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230919000000/ title="arXiv @ 2023.09.19">arXiv @ 2023.09.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230920000000/ title="arXiv @ 2023.09.20">arXiv @ 2023.09.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230921000000/ title="arXiv @ 2023.09.21">arXiv @ 2023.09.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230922000000/ title="arXiv @ 2023.09.22">arXiv @ 2023.09.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230923000000/ title="arXiv @ 2023.09.23">arXiv @ 2023.09.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230924000000/ title="arXiv @ 2023.09.24">arXiv @ 2023.09.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230925000000/ title="arXiv @ 2023.09.25">arXiv @ 2023.09.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230926000000/ title="arXiv @ 2023.09.26">arXiv @ 2023.09.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230927000000/ title="arXiv @ 2023.09.27">arXiv @ 2023.09.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230928000000/ title="arXiv @ 2023.09.28">arXiv @ 2023.09.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230929000000/ title="arXiv @ 2023.09.29">arXiv @ 2023.09.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202309/20230930000000/ title="arXiv @ 2023.09.30">arXiv @ 2023.09.30</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202310/>2023.1</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231001000000/ title="arXiv @ 2023.10.01">arXiv @ 2023.10.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231002000000/ title="arXiv @ 2023.10.02">arXiv @ 2023.10.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231003000000/ title="arXiv @ 2023.10.03">arXiv @ 2023.10.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231004000000/ title="arXiv @ 2023.10.04">arXiv @ 2023.10.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231005000000/ title="arXiv @ 2023.10.05">arXiv @ 2023.10.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231006000000/ title="arXiv @ 2023.10.06">arXiv @ 2023.10.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231007000000/ title="arXiv @ 2023.10.07">arXiv @ 2023.10.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231008000000/ title="arXiv @ 2023.10.08">arXiv @ 2023.10.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231009000000/ title="arXiv @ 2023.10.09">arXiv @ 2023.10.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231010000000/ title="arXiv @ 2023.10.10">arXiv @ 2023.10.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231011000000/ title="arXiv @ 2023.10.11">arXiv @ 2023.10.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231012000000/ title="arXiv @ 2023.10.12">arXiv @ 2023.10.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231013000000/ title="arXiv @ 2023.10.13">arXiv @ 2023.10.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231014000000/ title="arXiv @ 2023.10.14">arXiv @ 2023.10.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231015000000/ title="arXiv @ 2023.10.15">arXiv @ 2023.10.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231016000000/ title="arXiv @ 2023.10.16">arXiv @ 2023.10.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231017000000/ title="arXiv @ 2023.10.17">arXiv @ 2023.10.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231018000000/ title="arXiv @ 2023.10.18">arXiv @ 2023.10.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231019000000/ title="arXiv @ 2023.10.19">arXiv @ 2023.10.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231020000000/ title="arXiv @ 2023.10.20">arXiv @ 2023.10.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231021000000/ title="arXiv @ 2023.10.21">arXiv @ 2023.10.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231022000000/ title="arXiv @ 2023.10.22">arXiv @ 2023.10.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231023000000/ title="arXiv @ 2023.10.23">arXiv @ 2023.10.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231024000000/ title="arXiv @ 2023.10.24">arXiv @ 2023.10.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231025000000/ title="arXiv @ 2023.10.25">arXiv @ 2023.10.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231026000000/ title="arXiv @ 2023.10.26">arXiv @ 2023.10.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231027000000/ title="arXiv @ 2023.10.27">arXiv @ 2023.10.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231028000000/ title="arXiv @ 2023.10.28">arXiv @ 2023.10.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231029000000/ title="arXiv @ 2023.10.29">arXiv @ 2023.10.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231030000000/ title="arXiv @ 2023.10.30">arXiv @ 2023.10.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202310/20231031000000/ title="arXiv @ 2023.10.31">arXiv @ 2023.10.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202311/>2023.11</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231101000000/ title="arXiv @ 2023.11.01">arXiv @ 2023.11.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231102000000/ title="arXiv @ 2023.11.02">arXiv @ 2023.11.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231103000000/ title="arXiv @ 2023.11.03">arXiv @ 2023.11.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231104000000/ title="arXiv @ 2023.11.04">arXiv @ 2023.11.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231105000000/ title="arXiv @ 2023.11.05">arXiv @ 2023.11.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231106000000/ title="arXiv @ 2023.11.06">arXiv @ 2023.11.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231107000000/ title="arXiv @ 2023.11.07">arXiv @ 2023.11.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231108000000/ title="arXiv @ 2023.11.08">arXiv @ 2023.11.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231109000000/ title="arXiv @ 2023.11.09">arXiv @ 2023.11.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231110000000/ title="arXiv @ 2023.11.10">arXiv @ 2023.11.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231111000000/ title="arXiv @ 2023.11.11">arXiv @ 2023.11.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231112000000/ title="arXiv @ 2023.11.12">arXiv @ 2023.11.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231113000000/ title="arXiv @ 2023.11.13">arXiv @ 2023.11.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231114000000/ title="arXiv @ 2023.11.14">arXiv @ 2023.11.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231115000000/ title="arXiv @ 2023.11.15">arXiv @ 2023.11.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231116000000/ title="arXiv @ 2023.11.16">arXiv @ 2023.11.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231117000000/ title="arXiv @ 2023.11.17">arXiv @ 2023.11.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231118000000/ title="arXiv @ 2023.11.18">arXiv @ 2023.11.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231119000000/ title="arXiv @ 2023.11.19">arXiv @ 2023.11.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231120000000/ title="arXiv @ 2023.11.20">arXiv @ 2023.11.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231121000000/ title="arXiv @ 2023.11.21">arXiv @ 2023.11.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231122000000/ title="arXiv @ 2023.11.22">arXiv @ 2023.11.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231123000000/ title="arXiv @ 2023.11.23">arXiv @ 2023.11.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231124000000/ title="arXiv @ 2023.11.24">arXiv @ 2023.11.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231125000000/ title="arXiv @ 2023.11.25">arXiv @ 2023.11.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231126000000/ title="arXiv @ 2023.11.26">arXiv @ 2023.11.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231127000000/ title="arXiv @ 2023.11.27">arXiv @ 2023.11.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231128000000/ title="arXiv @ 2023.11.28">arXiv @ 2023.11.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231129000000/ title="arXiv @ 2023.11.29">arXiv @ 2023.11.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202311/20231130000000/ title="arXiv @ 2023.11.30">arXiv @ 2023.11.30</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202312/>2023.12</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231201000000/ title="arXiv @ 2023.12.01">arXiv @ 2023.12.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231202000000/ title="arXiv @ 2023.12.02">arXiv @ 2023.12.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231203000000/ title="arXiv @ 2023.12.03">arXiv @ 2023.12.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231204000000/ title="arXiv @ 2023.12.04">arXiv @ 2023.12.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231205000000/ title="arXiv @ 2023.12.05">arXiv @ 2023.12.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231206000000/ title="arXiv @ 2023.12.06">arXiv @ 2023.12.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231207000000/ title="arXiv @ 2023.12.07">arXiv @ 2023.12.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231208000000/ title="arXiv @ 2023.12.08">arXiv @ 2023.12.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231209000000/ title="arXiv @ 2023.12.09">arXiv @ 2023.12.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231210000000/ title="arXiv @ 2023.12.10">arXiv @ 2023.12.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231211000000/ title="arXiv @ 2023.12.11">arXiv @ 2023.12.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231212000000/ title="arXiv @ 2023.12.12">arXiv @ 2023.12.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231213000000/ title="arXiv @ 2023.12.13">arXiv @ 2023.12.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231214000000/ title="arXiv @ 2023.12.14">arXiv @ 2023.12.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231215000000/ title="arXiv @ 2023.12.15">arXiv @ 2023.12.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231216000000/ title="arXiv @ 2023.12.16">arXiv @ 2023.12.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231217000000/ title="arXiv @ 2023.12.17">arXiv @ 2023.12.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231218000000/ title="arXiv @ 2023.12.18">arXiv @ 2023.12.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231219000000/ title="arXiv @ 2023.12.19">arXiv @ 2023.12.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231220000000/ title="arXiv @ 2023.12.20">arXiv @ 2023.12.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231221000000/ title="arXiv @ 2023.12.21">arXiv @ 2023.12.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231222000000/ title="arXiv @ 2023.12.22">arXiv @ 2023.12.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231223000000/ title="arXiv @ 2023.12.23">arXiv @ 2023.12.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231224000000/ title="arXiv @ 2023.12.24">arXiv @ 2023.12.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231225000000/ title="arXiv @ 2023.12.25">arXiv @ 2023.12.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231226000000/ title="arXiv @ 2023.12.26">arXiv @ 2023.12.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231227000000/ title="arXiv @ 2023.12.27">arXiv @ 2023.12.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231228000000/ title="arXiv @ 2023.12.28">arXiv @ 2023.12.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231229000000/ title="arXiv @ 2023.12.29">arXiv @ 2023.12.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231230000000/ title="arXiv @ 2023.12.30">arXiv @ 2023.12.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202312/20231231000000/ title="arXiv @ 2023.12.31">arXiv @ 2023.12.31</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/arxiv/202401/>2024.01</a><ul><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240101000000/ title="arXiv @ 2024.01.01">arXiv @ 2024.01.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240102000000/ title="arXiv @ 2024.01.02">arXiv @ 2024.01.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240103000000/ title="arXiv @ 2024.01.03">arXiv @ 2024.01.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240104000000/ title="arXiv @ 2024.01.04">arXiv @ 2024.01.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240105000000/ title="arXiv @ 2024.01.05">arXiv @ 2024.01.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240106000000/ title="arXiv @ 2024.01.06">arXiv @ 2024.01.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240107000000/ title="arXiv @ 2024.01.07">arXiv @ 2024.01.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240108000000/ title="arXiv @ 2024.01.08">arXiv @ 2024.01.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240109000000/ title="arXiv @ 2024.01.09">arXiv @ 2024.01.09</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240110000000/ title="arXiv @ 2024.01.10">arXiv @ 2024.01.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240111000000/ title="arXiv @ 2024.01.11">arXiv @ 2024.01.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240112000000/ title="arXiv @ 2024.01.12">arXiv @ 2024.01.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240113000000/ title="arXiv @ 2024.01.13">arXiv @ 2024.01.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240114000000/ title="arXiv @ 2024.01.14">arXiv @ 2024.01.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240115000000/ title="arXiv @ 2024.01.15">arXiv @ 2024.01.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240116000000/ title="arXiv @ 2024.01.16">arXiv @ 2024.01.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240117000000/ title="arXiv @ 2024.01.17">arXiv @ 2024.01.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240118000000/ title="arXiv @ 2024.01.18">arXiv @ 2024.01.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240119000000/ title="arXiv @ 2024.01.19">arXiv @ 2024.01.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240120000000/ title="arXiv @ 2024.01.20">arXiv @ 2024.01.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240121000000/ title="arXiv @ 2024.01.21">arXiv @ 2024.01.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240122000000/ title="arXiv @ 2024.01.22">arXiv @ 2024.01.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240123000000/ title="arXiv @ 2024.01.23">arXiv @ 2024.01.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240124000000/ title="arXiv @ 2024.01.24">arXiv @ 2024.01.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240125000000/ title="arXiv @ 2024.01.25">arXiv @ 2024.01.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240126000000/ title="arXiv @ 2024.01.26">arXiv @ 2024.01.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240127000000/ title="arXiv @ 2024.01.27">arXiv @ 2024.01.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240128000000/ title="arXiv @ 2024.01.28">arXiv @ 2024.01.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240129000000/ title="arXiv @ 2024.01.29">arXiv @ 2024.01.29</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240130000000/ title="arXiv @ 2024.01.30">arXiv @ 2024.01.30</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202401/20240131000000/ title="arXiv @ 2024.01.31">arXiv @ 2024.01.31</a></li></ul></li><li><i class="fas fa-minus-circle"></i><a class=active href=/akitenkrad-blog/posts/arxiv/202402/>2024.02</a><ul class=active><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240201000000/ title="arXiv @ 2024.02.01">arXiv @ 2024.02.01</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240202000000/ title="arXiv @ 2024.02.02">arXiv @ 2024.02.02</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240203000000/ title="arXiv @ 2024.02.03">arXiv @ 2024.02.03</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240204000000/ title="arXiv @ 2024.02.04">arXiv @ 2024.02.04</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240205000000/ title="arXiv @ 2024.02.05">arXiv @ 2024.02.05</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240206000000/ title="arXiv @ 2024.02.06">arXiv @ 2024.02.06</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240207000000/ title="arXiv @ 2024.02.07">arXiv @ 2024.02.07</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240208000000/ title="arXiv @ 2024.02.08">arXiv @ 2024.02.08</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240209000000/ title="arXiv @ 2024.02.09">arXiv @ 2024.02.09</a></li><li><a class=active href=/akitenkrad-blog/posts/arxiv/202402/20240210000000/ title="arXiv @ 2024.02.10">arXiv @ 2024.02.10</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240211000000/ title="arXiv @ 2024.02.11">arXiv @ 2024.02.11</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240212000000/ title="arXiv @ 2024.02.12">arXiv @ 2024.02.12</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240213000000/ title="arXiv @ 2024.02.13">arXiv @ 2024.02.13</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240214000000/ title="arXiv @ 2024.02.14">arXiv @ 2024.02.14</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240215000000/ title="arXiv @ 2024.02.15">arXiv @ 2024.02.15</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240216000000/ title="arXiv @ 2024.02.16">arXiv @ 2024.02.16</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240217000000/ title="arXiv @ 2024.02.17">arXiv @ 2024.02.17</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240218000000/ title="arXiv @ 2024.02.18">arXiv @ 2024.02.18</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240219000000/ title="arXiv @ 2024.02.19">arXiv @ 2024.02.19</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240220000000/ title="arXiv @ 2024.02.20">arXiv @ 2024.02.20</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240221000000/ title="arXiv @ 2024.02.21">arXiv @ 2024.02.21</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240222000000/ title="arXiv @ 2024.02.22">arXiv @ 2024.02.22</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240223000000/ title="arXiv @ 2024.02.23">arXiv @ 2024.02.23</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240224000000/ title="arXiv @ 2024.02.24">arXiv @ 2024.02.24</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240225000000/ title="arXiv @ 2024.02.25">arXiv @ 2024.02.25</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240226000000/ title="arXiv @ 2024.02.26">arXiv @ 2024.02.26</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240227000000/ title="arXiv @ 2024.02.27">arXiv @ 2024.02.27</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240228000000/ title="arXiv @ 2024.02.28">arXiv @ 2024.02.28</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240229000000/ title="arXiv @ 2024.02.29">arXiv @ 2024.02.29</a></li></ul></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/algorithms/>Algorithms</a><ul><li><a href=/akitenkrad-blog/posts/algorithms/bandit-algorithm-basic/ title="Bandit Algorithm Basic">Bandit Algorithm Basic</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/conference/>Conference</a><ul><li><a href=/akitenkrad-blog/posts/conference/acl/ title="Annual Meeting of the Association for Computational Linguistics">Annual Meeting of the Association for Computational Linguistics</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/>Figures</a><ul><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/202208/>2022.08</a><ul><li><a href=/akitenkrad-blog/posts/figures/202208/20220822092748/ title=1ヶ月当たりの実労働時間の推移>1ヶ月当たりの実労働時間の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220813115200/ title=コンビニエンスストアの店舗数の推移>コンビニエンスストアの店舗数の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220821113322/ title=地域別1世帯当たり1ヶ月間の支出額の推移>地域別1世帯当たり1ヶ月間の支出額の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220828111836/ title="夫婦別子供の有無別生活時間 (2016)">夫婦別子供の有無別生活時間 (2016)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220827103615/ title=家族類型の変遷>家族類型の変遷</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220815131016/ title="年齢別大学院入学者数 (2021)">年齢別大学院入学者数 (2021)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220818122628/ title=情報サービス業における企業特殊的人的資本（名目）の推移>情報サービス業における企業特殊的人的資本（名目）の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220824095127/ title=有業者の社会生活における活動の変化>有業者の社会生活における活動の変化</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220814175022/ title=東京23区の乗用車保有台数>東京23区の乗用車保有台数</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220817121117/ title="業種別企業特殊的人的資本 (2018)">業種別企業特殊的人的資本 (2018)</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220825094111/ title=無業者の社会生活における活動の変化>無業者の社会生活における活動の変化</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220823104100/ title=睡眠時間の推移>睡眠時間の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220826102652/ title=総人口の推移>総人口の推移</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220820225528/ title=都道府県別国公立別学校数・学生数>都道府県別国公立別学校数・学生数</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220816120845/ title="都道府県別消費者物価指数/教育（全国平均=100）">都道府県別消費者物価指数/教育（全国平均=100）</a></li><li><a href=/akitenkrad-blog/posts/figures/202208/20220812173814/ title=金沢市の夏の月平均気温の遷移>金沢市の夏の月平均気温の遷移</a></li></ul></li><li><i class="fas fa-plus-circle"></i><a href=/akitenkrad-blog/posts/figures/202209/>2022.09</a><ul><li><a href=/akitenkrad-blog/posts/figures/202209/20220907122906/ title="ACL-2022 WordCloud">ACL-2022 WordCloud</a></li></ul></li></ul></li><li><a href=/akitenkrad-blog/posts/latex/ title="Latex Mathematics Syntax Guide">Latex Mathematics Syntax Guide</a></li><li><a href=/akitenkrad-blog/posts/markdown/ title="Markdown Sample">Markdown Sample</a></li></div></ul></div></div></div></section><section class=content-section id=content-section><div class=content><div class="container p-0 read-area"><div class="hero-area col-sm-12" id=hero-area style=background-image:url(/akitenkrad-blog/posts/arxiv/202402/20240210000000/hero.png)></div><div class=page-content><div class="author-profile ml-auto align-self-lg-center"><img class=rounded-circle src=/akitenkrad-blog/images/avatar_hu2673d53b0ac78c90b0a5a617874cdcc4_128349_120x120_fit_box_3.png alt="Author Image"><h5 class=author-name></h5><p>Saturday, Feb 10, 2024</p></div><div class=title><h1>arXiv @ 2024.02.10</h1></div><div class=taxonomy-terms><ul><li class=rounded><a href=/akitenkrad-blog/tags/arxiv class="btn, btn-sm">arXiv</a></li><li class=rounded><a href=/akitenkrad-blog/tags/published2024 class="btn, btn-sm">2024</a></li></ul></div><div class=post-content id=post-content><figure style=border:none;width:100%;display:flex;justify-content:center><iframe src=pie.html width=900 height=620 style=border:none></iframe></figure><h2 id=primary-categories>Primary Categories</h2><ul><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240210000000/#csai-16>cs.AI (16)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240210000000/#csce-2>cs.CE (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240210000000/#cscl-38>cs.CL (38)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240210000000/#cscr-3>cs.CR (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240210000000/#cscv-36>cs.CV (36)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240210000000/#cscy-3>cs.CY (3)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240210000000/#csdl-1>cs.DL (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240210000000/#csds-1>cs.DS (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240210000000/#csgt-2>cs.GT (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240210000000/#cshc-6>cs.HC (6)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240210000000/#csir-2>cs.IR (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240210000000/#csit-4>cs.IT (4)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240210000000/#cslg-65>cs.LG (65)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240210000000/#csma-2>cs.MA (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240210000000/#csmm-1>cs.MM (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240210000000/#csni-1>cs.NI (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240210000000/#cspf-1>cs.PF (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240210000000/#csro-15>cs.RO (15)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240210000000/#cssd-1>cs.SD (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240210000000/#csse-9>cs.SE (9)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240210000000/#cssi-1>cs.SI (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240210000000/#eessas-2>eess.AS (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240210000000/#eessiv-6>eess.IV (6)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240210000000/#eesssp-2>eess.SP (2)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240210000000/#eesssy-7>eess.SY (7)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240210000000/#mathna-6>math.NA (6)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240210000000/#mathoc-1>math.OC (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240210000000/#physicsao-ph-1>physics.ao-ph (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240210000000/#physicsflu-dyn-1>physics.flu-dyn (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240210000000/#physicsmed-ph-1>physics.med-ph (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240210000000/#physicsoptics-1>physics.optics (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240210000000/#physicssoc-ph-1>physics.soc-ph (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240210000000/#q-biobm-1>q-bio.BM (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240210000000/#q-biogn-1>q-bio.GN (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240210000000/#q-bioqm-1>q-bio.QM (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240210000000/#q-finst-1>q-fin.ST (1)</a></li><li><a href=/akitenkrad-blog/posts/arxiv/202402/20240210000000/#statml-5>stat.ML (5)</a></li></ul><h2 id=keywords>Keywords</h2><table border=1 class=dataframe><thead><tr style=text-align:right><th>keyword</th><th>cs.AI</th><th>cs.CL</th><th>cs.CV</th><th>cs.LG</th><th>cs.RO</th></tr></thead><tbody><tr><td>Active Learning</td><td></td><td></td><td></td><td>2</td><td>1</td></tr><tr><td>Adversarial Attack</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Adversarial Learning</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Autoencoder</td><td></td><td></td><td>1</td><td>2</td><td></td></tr><tr><td>Automatic Speech Recognition</td><td></td><td>7</td><td></td><td></td><td></td></tr><tr><td>BERT</td><td></td><td>3</td><td></td><td>1</td><td></td></tr><tr><td>Bandit Algorithm</td><td></td><td></td><td></td><td>2</td><td></td></tr><tr><td>Benchmarking</td><td>3</td><td>6</td><td>13</td><td>14</td><td></td></tr><tr><td>Botnet Detection</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>ChatGPT</td><td></td><td>2</td><td></td><td></td><td></td></tr><tr><td>Claude</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Continual Learning</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Contrastive Learning</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Convolution</td><td></td><td>1</td><td>3</td><td>2</td><td>1</td></tr><tr><td>Convolutional Neural Network</td><td></td><td></td><td>4</td><td>2</td><td>2</td></tr><tr><td>Counter-factual</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Data Augmentation</td><td></td><td>2</td><td></td><td></td><td></td></tr><tr><td>Dialogue System</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Document Classification</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Domain Adaptation</td><td></td><td></td><td>1</td><td>1</td><td></td></tr><tr><td>Edge Embedding</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Face Recognition</td><td></td><td></td><td>2</td><td></td><td></td></tr><tr><td>Fact Verification</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Fairness</td><td>1</td><td></td><td></td><td>2</td><td></td></tr><tr><td>Fake News Detection</td><td>2</td><td></td><td></td><td></td><td></td></tr><tr><td>Federated Learning</td><td></td><td></td><td></td><td>7</td><td></td></tr><tr><td>Few-shot</td><td></td><td>2</td><td></td><td></td><td></td></tr><tr><td>Fine-tuning</td><td>1</td><td>7</td><td>6</td><td>8</td><td></td></tr><tr><td>Foundation Model</td><td>2</td><td></td><td></td><td></td><td>1</td></tr><tr><td>GPT</td><td>1</td><td>11</td><td></td><td>2</td><td></td></tr><tr><td>GPT-3</td><td></td><td>2</td><td></td><td></td><td></td></tr><tr><td>GPT-3.5</td><td></td><td>2</td><td></td><td></td><td></td></tr><tr><td>GPT-4</td><td>1</td><td>7</td><td></td><td>2</td><td></td></tr><tr><td>GPT-4 turbo</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Gemini</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Generative Adversarial Network</td><td></td><td></td><td>2</td><td></td><td></td></tr><tr><td>Graph Attention Networks</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Graph Convolutional Network</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Graph Embedding</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Graph Neural Network</td><td>2</td><td></td><td></td><td>11</td><td></td></tr><tr><td>Grounding</td><td></td><td>1</td><td>2</td><td></td><td></td></tr><tr><td>Hallucination Detection</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Heuristic Approach</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>In-context Learning</td><td></td><td>6</td><td></td><td>4</td><td></td></tr><tr><td>Information Retrieval</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Instruction Tuning</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Knowledge Distillation</td><td>3</td><td></td><td>2</td><td>2</td><td></td></tr><tr><td>Knowledge Graph</td><td>5</td><td></td><td></td><td>2</td><td></td></tr><tr><td>LLaMA</td><td></td><td>1</td><td></td><td>2</td><td></td></tr><tr><td>LSTM</td><td></td><td></td><td>1</td><td>3</td><td></td></tr><tr><td>Label Smoothing</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Language Generation</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Large Language Model</td><td>14</td><td>37</td><td>11</td><td>11</td><td>4</td></tr><tr><td>Logistic Regression</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Low-Resource</td><td></td><td>5</td><td></td><td></td><td></td></tr><tr><td>Machine Unlearning</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Markov Decision Process</td><td></td><td></td><td></td><td>3</td><td></td></tr><tr><td>Markov Game</td><td></td><td></td><td></td><td>2</td><td></td></tr><tr><td>Message-Passing</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Meta Learning</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Model Compression</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Multi-modal</td><td>3</td><td>7</td><td>9</td><td>2</td><td>1</td></tr><tr><td>Mutual Information</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Named Entity Recognition</td><td></td><td>3</td><td></td><td></td><td></td></tr><tr><td>Neural Machine Translation</td><td></td><td>2</td><td></td><td></td><td></td></tr><tr><td>Node Classification</td><td>1</td><td></td><td></td><td>3</td><td></td></tr><tr><td>Object Detection</td><td></td><td></td><td>2</td><td></td><td>1</td></tr><tr><td>Offline Reinforcement Learning</td><td></td><td></td><td></td><td>2</td><td></td></tr><tr><td>Optical Character Recognition</td><td></td><td></td><td>3</td><td></td><td></td></tr><tr><td>Perplexity</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Pre-trained Language Model</td><td></td><td>2</td><td></td><td></td><td></td></tr><tr><td>Probabilistic Model</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Probabilistic Reasoning</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>Prompt</td><td>4</td><td>9</td><td>4</td><td>3</td><td></td></tr><tr><td>Pruning</td><td></td><td></td><td></td><td>2</td><td></td></tr><tr><td>Quantization</td><td></td><td></td><td></td><td>7</td><td></td></tr><tr><td>Question Answering</td><td>1</td><td>5</td><td>2</td><td>1</td><td></td></tr><tr><td>Reasoning</td><td>5</td><td>3</td><td>2</td><td>4</td><td></td></tr><tr><td>Recommendation</td><td></td><td>1</td><td></td><td>2</td><td>1</td></tr><tr><td>Recommender System</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Reinforcement Learning</td><td>3</td><td></td><td></td><td>13</td><td>3</td></tr><tr><td>Reinforcement Learning from Human Feedback</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Retrieval-Augmented Generation</td><td></td><td>3</td><td></td><td></td><td></td></tr><tr><td>RoBERTa</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Sample Size</td><td></td><td></td><td></td><td>2</td><td></td></tr><tr><td>Scaling Law</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Self-Attention</td><td></td><td></td><td>1</td><td>3</td><td></td></tr><tr><td>Self-supervised Learning</td><td></td><td></td><td>4</td><td></td><td></td></tr><tr><td>Semi-Supervised Learning</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Sentiment Analysis</td><td></td><td>2</td><td></td><td></td><td></td></tr><tr><td>Simulation</td><td></td><td>2</td><td>1</td><td>4</td><td>5</td></tr><tr><td>Simulator</td><td></td><td>2</td><td>1</td><td>4</td><td>5</td></tr><tr><td>Square Loss</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Stemming</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Stochastic Gradient Descent</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Summarization</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>SuperGLUE</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Supervised Learning</td><td></td><td>1</td><td></td><td>5</td><td></td></tr><tr><td>Text Augmentation</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>Text Embedding</td><td></td><td>1</td><td>2</td><td></td><td></td></tr><tr><td>Text-to-speech</td><td></td><td>4</td><td></td><td></td><td></td></tr><tr><td>Text2image</td><td></td><td></td><td>3</td><td></td><td></td></tr><tr><td>Transfer Learning</td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>Transformer</td><td>2</td><td>4</td><td>8</td><td>8</td><td></td></tr><tr><td>Unsupervised Learning</td><td></td><td>1</td><td></td><td>5</td><td>1</td></tr><tr><td>Variational Autoencoder</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Vision Transformer</td><td></td><td>1</td><td>4</td><td>1</td><td></td></tr><tr><td>Vision-and-Language</td><td></td><td>1</td><td>5</td><td></td><td>1</td></tr><tr><td>Visual Question Answering</td><td>1</td><td></td><td>2</td><td></td><td></td></tr><tr><td>Weakly Supervised Learning</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Word Embedding</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>Zero-shot</td><td></td><td>3</td><td>1</td><td>2</td><td>1</td></tr><tr><td>Zero-shot Learning</td><td></td><td></td><td>1</td><td></td><td></td></tr><tr><td>falcon</td><td></td><td></td><td></td><td>1</td><td></td></tr><tr><td>human-in-the-loop</td><td></td><td></td><td>2</td><td></td><td></td></tr></tbody></table><script>$(function(){$("table").addClass("keyword-table table-bordered border-success"),$("table thead").addClass("sticky-top"),$("table tbody td").css("text-align","")})</script><h2 id=cscl-38>cs.CL (38)</h2><h3 id=038--1248-in-context-principle-learning-from-mistakes-tianjun-zhang-et-al-2024>(0/38 | 1/248) In-Context Principle Learning from Mistakes (Tianjun Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tianjun Zhang, Aman Madaan, Luyu Gao, Steven Zheng, Swaroop Mishra, Yiming Yang, Niket Tandon, Uri Alon. (2024)<br><strong>In-Context Principle Learning from Mistakes</strong><br><button class=copy-to-clipboard title="In-Context Principle Learning from Mistakes" index=1>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-1 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 153<br>Keywords: Benchmarking, Few-shot, Claude, GPT, GPT-3, GPT-3.5, GPT-4, GPT-4 turbo, Question Answering, Question Answering, Reasoning, In-context Learning, In-context Learning, In-context Learning, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05403v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05403v2.pdf filename=2402.05403v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>In-context</b> <b>learning</b> <b>(ICL,</b> also known as <b>few-shot</b> <b>prompting)</b> has been the standard method of adapting <b>LLMs</b> to downstream tasks, by learning from a few input-output examples. Nonetheless, all <b>ICL-based</b> approaches only learn from correct input-output pairs. In this paper, we revisit this paradigm, by learning more from the few given input-output examples. We introduce Learning Principles (LEAP): First, we intentionally induce the model to make mistakes on these few examples; then we reflect on these mistakes, and learn explicit task-specific &ldquo;principles&rdquo; from them, which help solve similar problems and avoid common mistakes; finally, we <b>prompt</b> the model to answer unseen test <b>questions</b> <b>using</b> the original <b>few-shot</b> examples and these learned general principles. We evaluate LEAP on a wide range of <b>benchmarks,</b> including multi-hop <b>question</b> <b>answering</b> (Hotpot <b>QA),</b> textual <b>QA</b> (DROP), Big-Bench Hard <b>reasoning,</b> and math problems (GSM8K and MATH); in all these <b>benchmarks,</b> LEAP improves the strongest available <b>LLMs</b> such as <b>GPT-3.5-turbo,</b> <b>GPT-4,</b> <b>GPT-4</b> <b>turbo</b> and <b>Claude-2.1.</b> For example, LEAP improves over the standard <b>few-shot</b> <b>prompting</b> using <b>GPT-4</b> <b>by</b> 7.5% in DROP, and by 3.3% in HotpotQA. Importantly, LEAP does not require any more input or examples than the standard <b>few-shot</b> <b>prompting</b> settings.</p></p class="citation"></blockquote><h3 id=138--2248-efficient-models-for-the-detection-of-hate-abuse-and-profanity-christoph-tillmann-et-al-2024>(1/38 | 2/248) Efficient Models for the Detection of Hate, Abuse and Profanity (Christoph Tillmann et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Christoph Tillmann, Aashka Trivedi, Bishwaranjan Bhattacharjee. (2024)<br><strong>Efficient Models for the Detection of Hate, Abuse and Profanity</strong><br><button class=copy-to-clipboard title="Efficient Models for the Detection of Hate, Abuse and Profanity" index=2>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-2 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-HC, cs.CL<br>Keyword Score: 100<br>Keywords: RoBERTa, Transformer, Document Classification, Named Entity Recognition, Question Answering, Sentiment Analysis, Large Language Model, Large Language Model, Prompt, Summarization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05624v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05624v1.pdf filename=2402.05624v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> are the cornerstone for many Natural Language Processing (NLP) tasks like <b>sentiment</b> <b>analysis,</b> <b>document</b> <b>classification,</b> <b>named</b> <b>entity</b> <b>recognition,</b> <b>question</b> <b>answering,</b> <b>summarization,</b> etc. <b>LLMs</b> are often trained on data which originates from the web. This data is prone to having content with Hate, Abuse and Profanity (HAP). For a detailed definition of HAP, please refer to the Appendix. Due to the <b>LLMs</b> being exposed to HAP content during training, the models learn it and may then generate hateful or profane content. For example, when the open-source <b>RoBERTa</b> model (specifically, the <b>RoBERTA</b> base model) from the HuggingFace (HF) <b>Transformers</b> library is <b>prompted</b> to replace the mask token in <code>I do not know that Persian people are that MASK</code> it returns the word <code>stupid</code> with the highest score. This is unacceptable in civil discourse.The detection of Hate, Abuse and Profanity in text is a vital component of creating civil and unbiased <b>LLMs,</b> which is needed not only for English, but for all languages. In this article, we briefly describe the creation of HAP detectors and various ways of using them to make models civil and acceptable in the output they generate.</p></p class="citation"></blockquote><h3 id=238--3248-zero-shot-chain-of-thought-reasoning-guided-by-evolutionary-algorithms-in-large-language-models-feihu-jin-et-al-2024>(2/38 | 3/248) Zero-Shot Chain-of-Thought Reasoning Guided by Evolutionary Algorithms in Large Language Models (Feihu Jin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Feihu Jin, Yifan Liu, Ying Tan. (2024)<br><strong>Zero-Shot Chain-of-Thought Reasoning Guided by Evolutionary Algorithms in Large Language Models</strong><br><button class=copy-to-clipboard title="Zero-Shot Chain-of-Thought Reasoning Guided by Evolutionary Algorithms in Large Language Models" index=3>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-3 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 90<br>Keywords: Zero-shot, GPT, GPT-3, GPT-3.5, GPT-4, Reasoning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05376v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05376v1.pdf filename=2402.05376v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have demonstrated remarkable performance across diverse tasks and exhibited impressive <b>reasoning</b> abilities by applying <b>zero-shot</b> Chain-of-Thought (CoT) <b>prompting.</b> However, due to the evolving nature of sentence prefixes during the pre-training phase, existing <b>zero-shot</b> CoT <b>prompting</b> methods that employ identical CoT <b>prompting</b> across all task instances may not be optimal. In this paper, we introduce a novel <b>zero-shot</b> <b>prompting</b> method that leverages evolutionary algorithms to generate diverse <b>promptings</b> for <b>LLMs</b> dynamically. Our approach involves initializing two CoT <b>promptings,</b> performing evolutionary operations based on <b>LLMs</b> to create a varied set, and utilizing the <b>LLMs</b> to select a suitable CoT <b>prompting</b> for a given problem. Additionally, a rewriting operation, guided by the selected CoT <b>prompting,</b> enhances the understanding of the <b>LLMs</b> about the problem. Extensive experiments conducted across ten <b>reasoning</b> datasets demonstrate the superior performance of our proposed method compared to current <b>zero-shot</b> CoT <b>prompting</b> methods on <b>GPT-3.5-turbo</b> and <b>GPT-4.</b> Moreover, in-depth analytical experiments underscore the adaptability and effectiveness of our method in various <b>reasoning</b> tasks.</p></p class="citation"></blockquote><h3 id=338--4248-self-alignment-of-large-language-models-via-monopolylogue-based-social-scene-simulation-xianghe-pang-et-al-2024>(3/38 | 4/248) Self-Alignment of Large Language Models via Monopolylogue-based Social Scene Simulation (Xianghe Pang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xianghe Pang, Shuo Tang, Rui Ye, Yuxin Xiong, Bolun Zhang, Yanfeng Wang, Siheng Chen. (2024)<br><strong>Self-Alignment of Large Language Models via Monopolylogue-based Social Scene Simulation</strong><br><button class=copy-to-clipboard title="Self-Alignment of Large Language Models via Monopolylogue-based Social Scene Simulation" index=4>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-4 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-CY, cs.CL<br>Keyword Score: 73<br>Keywords: Benchmarking, Fine-tuning, Simulation, Simulator, GPT, GPT-4, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05699v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05699v1.pdf filename=2402.05699v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Aligning <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> with human values is imperative to mitigate potential adverse effects resulting from their misuse. Drawing from the sociological insight that acknowledging all parties&rsquo; concerns is a key factor in shaping human values, this paper proposes a novel direction to align <b>LLMs</b> by themselves: social scene <b>simulation.</b> To achieve this, we present MATRIX, a novel social scene simulator that emulates realistic scenes around a user&rsquo;s input query, enabling the <b>LLM</b> to take social consequences into account before responding. MATRIX serves as a virtual rehearsal space, akin to a Monopolylogue, where the <b>LLM</b> performs diverse roles related to the query and practice by itself. To inject this alignment, we <b>fine-tune</b> the <b>LLM</b> with MATRIX-simulated data, ensuring adherence to human values without compromising inference speed. We theoretically show that the <b>LLM</b> with MATRIX outperforms Constitutional AI under mild assumptions. Finally, extensive experiments validate that our method outperforms over 10 baselines across 4 <b>benchmarks.</b> As evidenced by 875 user ratings, our tuned 13B-size <b>LLM</b> exceeds <b>GPT-4</b> in aligning with human values. Code is available at <a href=https://github.com/pangxianghe/MATRIX>https://github.com/pangxianghe/MATRIX</a>.</p></p class="citation"></blockquote><h3 id=438--5248-unified-speech-text-pretraining-for-spoken-dialog-modeling-heeseung-kim-et-al-2024>(4/38 | 5/248) Unified Speech-Text Pretraining for Spoken Dialog Modeling (Heeseung Kim et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Heeseung Kim, Soonshin Seo, Kyeongseok Jeong, Ohsung Kwon, Jungwhan Kim, Jaehong Lee, Eunwoo Song, Myungwoo Oh, Sungroh Yoon, Kang Min Yoo. (2024)<br><strong>Unified Speech-Text Pretraining for Spoken Dialog Modeling</strong><br><button class=copy-to-clipboard title="Unified Speech-Text Pretraining for Spoken Dialog Modeling" index=5>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-5 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-SD, cs.CL, eess-AS<br>Keyword Score: 70<br>Keywords: Automatic Speech Recognition, Automatic Speech Recognition, Automatic Speech Recognition, Text-to-speech, Text-to-speech, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05706v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05706v1.pdf filename=2402.05706v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>While recent work shows promising results in expanding the capabilities of <b>large</b> <b>language</b> <b>models</b> <b>(LLM)</b> to directly understand and synthesize <b>speech,</b> <b>an</b> <b>LLM-based</b> strategy for modeling spoken dialogs remains elusive and calls for further investigation. This work proposes an extensive <b>speech-text</b> <b>LLM</b> framework, named the Unified Spoken Dialog Model (USDM), to generate coherent spoken responses with organic prosodic features relevant to the given input <b>speech</b> <b>without</b> relying on <b>automatic</b> <b>speech</b> <b>recognition</b> <b>(ASR)</b> or <b>text-to-speech</b> <b>(TTS)</b> solutions. Our approach employs a multi-step <b>speech-text</b> <b>inference</b> scheme that leverages chain-of-reasoning capabilities exhibited by the underlying <b>LLM.</b> We also propose a generalized <b>speech-text</b> <b>pretraining</b> scheme that helps with capturing cross-modal semantics. <b>Automatic</b> <b>and</b> <b>human</b> evaluations show that the proposed approach is effective in generating natural-sounding spoken responses, outperforming both prior and cascaded baselines. Detailed comparative studies reveal that, despite the cascaded approach being stronger in individual components, the joint <b>speech-text</b> <b>modeling</b> improves robustness against recognition errors and <b>speech</b> <b>quality.</b> Demo is available at <a href=https://unifiedsdm.github.io>https://unifiedsdm.github.io</a>.</p></p class="citation"></blockquote><h3 id=538--6248-a-prompt-response-to-the-demand-for-automatic-gender-neutral-translation-beatrice-savoldi-et-al-2024>(5/38 | 6/248) A Prompt Response to the Demand for Automatic Gender-Neutral Translation (Beatrice Savoldi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Beatrice Savoldi, Andrea Piergentili, Dennis Fucci, Matteo Negri, Luisa Bentivogli. (2024)<br><strong>A Prompt Response to the Demand for Automatic Gender-Neutral Translation</strong><br><button class=copy-to-clipboard title="A Prompt Response to the Demand for Automatic Gender-Neutral Translation" index=6>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-6 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 60<br>Keywords: GPT, GPT-4, Neural Machine Translation, Neural Machine Translation, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.06041v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.06041v1.pdf filename=2402.06041v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Gender-neutral translation (GNT) that avoids biased and undue binary assumptions is a pivotal challenge for the creation of more inclusive translation technologies. Advancements for this task in <b>Machine</b> <b>Translation</b> <b>(MT),</b> however, are hindered by the lack of dedicated parallel data, which are necessary to adapt <b>MT</b> systems to satisfy neutral constraints. For such a scenario, <b>large</b> <b>language</b> <b>models</b> offer hitherto unforeseen possibilities, as they come with the distinct advantage of being versatile in various (sub)tasks when provided with explicit instructions. In this paper, we explore this potential to automate GNT by comparing <b>MT</b> with the popular <b>GPT-4</b> model. Through extensive manual analyses, our study empirically reveals the inherent limitations of current <b>MT</b> systems in generating GNTs and provides valuable insights into the potential and challenges associated with <b>prompting</b> for neutrality.</p></p class="citation"></blockquote><h3 id=638--7248-emojicrypt-prompt-encryption-for-secure-communication-with-large-language-models-guo-lin-et-al-2024>(6/38 | 7/248) EmojiCrypt: Prompt Encryption for Secure Communication with Large Language Models (Guo Lin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Guo Lin, Wenyue Hua, Yongfeng Zhang. (2024)<br><strong>EmojiCrypt: Prompt Encryption for Secure Communication with Large Language Models</strong><br><button class=copy-to-clipboard title="EmojiCrypt: Prompt Encryption for Secure Communication with Large Language Models" index=7>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-7 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-CR, cs-IR, cs-LG, cs.CL<br>Keyword Score: 60<br>Keywords: Recommendation, ChatGPT, Sentiment Analysis, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05868v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05868v2.pdf filename=2402.05868v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Cloud-based <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> such as <b>ChatGPT</b> have increasingly become integral to daily operations, serving as vital tools across various applications. While these models offer substantial benefits in terms of accessibility and functionality, they also introduce significant privacy concerns: the transmission and storage of user data in cloud infrastructures pose substantial risks of data breaches and unauthorized access to sensitive information; even if the transmission and storage of data is encrypted, the <b>LLM</b> service provider itself still knows the real contents of the data, preventing individuals or entities from confidently using such <b>LLM</b> services. To address these concerns, this paper proposes a simple yet effective mechanism EmojiCrypt to protect user privacy. It uses Emoji to encrypt the user inputs before sending them to <b>LLM,</b> effectively rendering them indecipherable to human or <b>LLM&rsquo;s</b> examination while retaining the original intent of the <b>prompt,</b> thus ensuring the model&rsquo;s performance remains unaffected. We conduct experiments on three tasks, personalized <b>recommendation,</b> <b>sentiment</b> <b>analysis,</b> and tabular data analysis. Experiment results reveal that EmojiCrypt can encrypt personal information within <b>prompts</b> in such a manner that not only prevents the discernment of sensitive data by humans or <b>LLM</b> itself, but also maintains or even improves the precision without further tuning, achieving comparable or even better task accuracy than directly <b>prompting</b> the <b>LLM</b> without <b>prompt</b> encryption. These results highlight the practicality of adopting encryption measures that safeguard user privacy without compromising the functional integrity and performance of <b>LLMs.</b> Code and dataset are available at <a href=https://github.com/agiresearch/EmojiCrypt>https://github.com/agiresearch/EmojiCrypt</a>.</p></p class="citation"></blockquote><h3 id=738--8248-timearena-shaping-efficient-multitasking-language-agents-in-a-time-aware-simulation-yikai-zhang-et-al-2024>(7/38 | 8/248) TimeArena: Shaping Efficient Multitasking Language Agents in a Time-Aware Simulation (Yikai Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yikai Zhang, Siyu Yuan, Caiyu Hu, Kyle Richardson, Yanghua Xiao, Jiangjie Chen. (2024)<br><strong>TimeArena: Shaping Efficient Multitasking Language Agents in a Time-Aware Simulation</strong><br><button class=copy-to-clipboard title="TimeArena: Shaping Efficient Multitasking Language Agents in a Time-Aware Simulation" index=8>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-8 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 60<br>Keywords: Simulation, Simulator, GPT, GPT-4, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05733v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05733v1.pdf filename=2402.05733v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Despite remarkable advancements in emulating human-like behavior through <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs),</b> current textual <b>simulations</b> do not adequately address the notion of time. To this end, we introduce TimeArena, a novel textual simulated environment that incorporates complex temporal dynamics and constraints that better reflect real-life planning scenarios. In TimeArena, agents are asked to complete multiple tasks as soon as possible, allowing for parallel processing to save time. We implement the dependency between actions, the time duration for each action, and the occupancy of the agent and the objects in the environment. TimeArena grounds to 30 real-world tasks in cooking, household activities, and laboratory work. We conduct extensive experiments with various state-of-the-art <b>LLMs</b> using TimeArena. Our findings reveal that even the most powerful models, e.g., <b>GPT-4,</b> still lag behind humans in effective multitasking, underscoring the need for enhanced temporal awareness in the development of language agents.</p></p class="citation"></blockquote><h3 id=838--9248-gpt-4-generated-narratives-of-life-events-using-a-structured-narrative-prompt-a-validation-study-christopher-j-lynch-et-al-2024>(8/38 | 9/248) GPT-4 Generated Narratives of Life Events using a Structured Narrative Prompt: A Validation Study (Christopher J. Lynch et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Christopher J. Lynch, Erik Jensen, Madison H. Munro, Virginia Zamponi, Joseph Martinez, Kevin O&rsquo;Brien, Brandon Feldhaus, Katherine Smith, Ann Marie Reinhold, Ross Gore. (2024)<br><strong>GPT-4 Generated Narratives of Life Events using a Structured Narrative Prompt: A Validation Study</strong><br><button class=copy-to-clipboard title="GPT-4 Generated Narratives of Life Events using a Structured Narrative Prompt: A Validation Study" index=9>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-9 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: I-2-7; I-6-4, cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 60<br>Keywords: Zero-shot, GPT, GPT-4, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05435v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05435v1.pdf filename=2402.05435v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> play a pivotal role in generating vast arrays of narratives, facilitating a systematic exploration of their effectiveness for communicating life events in narrative form. In this study, we employ a <b>zero-shot</b> structured narrative <b>prompt</b> to generate 24,000 narratives using OpenAI&rsquo;s <b>GPT-4.</b> From this dataset, we manually classify 2,880 narratives and evaluate their validity in conveying birth, death, hiring, and firing events. Remarkably, 87.43% of the narratives sufficiently convey the intention of the structured <b>prompt.</b> To automate the identification of valid and invalid narratives, we train and validate nine Machine Learning models on the classified datasets. Leveraging these models, we extend our analysis to predict the classifications of the remaining 21,120 narratives. All the ML models excelled at classifying valid narratives as valid, but experienced challenges at simultaneously classifying invalid narratives as invalid. Our findings not only advance the study of <b>LLM</b> capabilities, limitations, and validity but also offer practical insights for narrative generation and natural language processing applications.</p></p class="citation"></blockquote><h3 id=938--10248-weblinx-real-world-website-navigation-with-multi-turn-dialogue-xing-han-lù-et-al-2024>(9/38 | 10/248) WebLINX: Real-World Website Navigation with Multi-Turn Dialogue (Xing Han Lù et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xing Han Lù, Zdeněk Kasner, Siva Reddy. (2024)<br><strong>WebLINX: Real-World Website Navigation with Multi-Turn Dialogue</strong><br><button class=copy-to-clipboard title="WebLINX: Real-World Website Navigation with Multi-Turn Dialogue" index=10>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-10 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-CV, cs-LG, cs.CL<br>Keyword Score: 59<br>Keywords: Benchmarking, Fine-tuning, Multi-modal, Multi-modal, Zero-shot, GPT, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05930v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05930v1.pdf filename=2402.05930v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We propose the problem of conversational web navigation, where a digital agent controls a web browser and follows user instructions to solve real-world tasks in a multi-turn dialogue fashion. To support this problem, we introduce WEBLINX - a <b>large-scale</b> <b>benchmark</b> <b>of</b> 100K interactions across 2300 expert demonstrations of conversational web navigation. Our <b>benchmark</b> covers a broad range of patterns on over 150 real-world websites and can be used to train and evaluate agents in diverse scenarios. Due to the magnitude of information present, <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> cannot process entire web pages in real-time. To solve this bottleneck, we design a retrieval-inspired model that efficiently prunes HTML pages by ranking relevant elements. We use the selected elements, along with screenshots and action history, to assess a variety of models for their ability to replicate human behavior when navigating the web. Our experiments span from small text-only to proprietary <b>multimodal</b> <b>LLMs.</b> We find that smaller <b>finetuned</b> decoders surpass the best <b>zero-shot</b> <b>LLMs</b> (including <b>GPT-4V),</b> but also larger <b>finetuned</b> <b>multimodal</b> models which were explicitly pretrained on screenshots. However, all <b>finetuned</b> models struggle to generalize to unseen websites. Our findings highlight the need for <b>large</b> <b>multimodal</b> <b>models</b> that can generalize to novel settings. Our code, data and models are available for research: <a href=https://mcgill-nlp.github.io/weblinx>https://mcgill-nlp.github.io/weblinx</a></p></p class="citation"></blockquote><h3 id=1038--11248-its-never-too-late-fusing-acoustic-information-into-large-language-models-for-automatic-speech-recognition-chen-chen-et-al-2024>(10/38 | 11/248) It&rsquo;s Never Too Late: Fusing Acoustic Information into Large Language Models for Automatic Speech Recognition (Chen Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chen Chen, Ruizhe Li, Yuchen Hu, Sabato Marco Siniscalchi, Pin-Yu Chen, Ensiong Chng, Chao-Han Huck Yang. (2024)<br><strong>It&rsquo;s Never Too Late: Fusing Acoustic Information into Large Language Models for Automatic Speech Recognition</strong><br><button class=copy-to-clipboard title="It's Never Too Late: Fusing Acoustic Information into Large Language Models for Automatic Speech Recognition" index=11>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-11 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-MM, cs-SD, cs.CL, eess-AS<br>Keyword Score: 56<br>Keywords: Multi-modal, Multi-modal, Automatic Speech Recognition, Automatic Speech Recognition, Automatic Speech Recognition, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05457v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05457v1.pdf filename=2402.05457v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent studies have successfully shown that <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> can be successfully used for generative error correction (GER) on top of the <b>automatic</b> <b>speech</b> <b>recognition</b> <b>(ASR)</b> output. Specifically, an <b>LLM</b> is utilized to carry out a direct mapping from the N-best hypotheses list generated by an <b>ASR</b> system to the predicted output transcription. However, despite its effectiveness, GER introduces extra data uncertainty since the <b>LLM</b> is trained without taking into account acoustic information available in the <b>speech</b> <b>signal.</b> In this work, we aim to overcome such a limitation by infusing acoustic information before generating the predicted transcription through a novel late fusion solution termed Uncertainty-Aware Dynamic Fusion (UADF). UADF is a <b>multimodal</b> fusion approach implemented into an auto-regressive decoding process and works in two stages: (i) It first analyzes and calibrates the token-level <b>LLM</b> decision, and (ii) it then dynamically assimilates the information from the acoustic modality. Experimental evidence collected from various <b>ASR</b> tasks shows that UADF surpasses existing fusion mechanisms in several ways. It yields significant improvements in word error rate (WER) while mitigating data uncertainty issues in <b>LLM</b> and addressing the poor generalization relied with sole modality during fusion. We also demonstrate that UADF seamlessly adapts to audio-visual <b>speech</b> <b>recognition.</b></p></p class="citation"></blockquote><h3 id=1138--12248-merging-facts-crafting-fallacies-evaluating-the-contradictory-nature-of-aggregated-factual-claims-in-long-form-generations-cheng-han-chiang-et-al-2024>(11/38 | 12/248) Merging Facts, Crafting Fallacies: Evaluating the Contradictory Nature of Aggregated Factual Claims in Long-Form Generations (Cheng-Han Chiang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Cheng-Han Chiang, Hung-yi Lee. (2024)<br><strong>Merging Facts, Crafting Fallacies: Evaluating the Contradictory Nature of Aggregated Factual Claims in Long-Form Generations</strong><br><button class=copy-to-clipboard title="Merging Facts, Crafting Fallacies: Evaluating the Contradictory Nature of Aggregated Factual Claims in Long-Form Generations" index=12>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-12 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: Retrieval-Augmented Generation, Retrieval-Augmented Generation, Retrieval-Augmented Generation, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05629v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05629v1.pdf filename=2402.05629v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Long-form generations from <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> contains a mix of factual and non-factual claims, making evaluating factuality difficult. To evaluate factual precision of long-form generations in a more fine-grained way, prior works propose to decompose long-form generations into multiple verifiable facts and verify those facts independently. The factuality of the generation is the proportion of verifiable facts among all the facts. Such methods assume that combining factual claims forms a factual paragraph. This paper shows that the assumption can be violated due to entity ambiguity. We show that <b>LLMs</b> can generate paragraphs that contain verifiable facts, but the facts are combined to form a non-factual paragraph due to entity ambiguity. We further reveal that existing factual precision metrics, including FActScore and citation recall, cannot properly evaluate the factuality of these non-factual paragraphs. To address this, we introduce an enhanced metric, D-FActScore, specifically designed for content with ambiguous entities. We evaluate the D-FActScores of people biographies generated with <b>retrieval-augmented</b> <b>generation</b> <b>(RAG).</b> We show that D-FActScore can better assess the factuality of paragraphs with entity ambiguity than FActScore. We also find that four widely used open-source <b>LLMs</b> tend to mix information of distinct entities to form non-factual paragraphs.</p></p class="citation"></blockquote><h3 id=1238--13248-attnlrp-attention-aware-layer-wise-relevance-propagation-for-transformers-reduan-achtibat-et-al-2024>(12/38 | 13/248) AttnLRP: Attention-Aware Layer-wise Relevance Propagation for Transformers (Reduan Achtibat et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Reduan Achtibat, Sayed Mohammad Vakilzadeh Hatefi, Maximilian Dreyer, Aakriti Jain, Thomas Wiegand, Sebastian Lapuschkin, Wojciech Samek. (2024)<br><strong>AttnLRP: Attention-Aware Layer-wise Relevance Propagation for Transformers</strong><br><button class=copy-to-clipboard title="AttnLRP: Attention-Aware Layer-wise Relevance Propagation for Transformers" index=13>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-13 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-CV, cs-LG, cs.CL<br>Keyword Score: 50<br>Keywords: LLaMA, Transformer, Reasoning, Large Language Model, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05602v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05602v1.pdf filename=2402.05602v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> are prone to biased predictions and hallucinations, underlining the paramount importance of understanding their model-internal <b>reasoning</b> process. However, achieving faithful attributions for the entirety of a black-box <b>transformer</b> model and maintaining computational efficiency is an unsolved challenge. By extending the Layer-wise Relevance Propagation attribution method to handle attention layers, we address these challenges effectively. While partial solutions exist, our method is the first to faithfully and holistically attribute not only input but also latent representations of <b>transformer</b> models with the computational efficiency similar to a singular backward pass. Through extensive evaluations against existing methods on <b>Llama</b> 2, Flan-T5 and the <b>Vision</b> <b>Transformer</b> architecture, we demonstrate that our proposed approach surpasses alternative methods in terms of faithfulness and enables the understanding of latent representations, opening up the door for concept-based explanations. We provide an open-source implementation on GitHub <a href=https://github.com/rachtibat/LRP-for-Transformers>https://github.com/rachtibat/LRP-for-Transformers</a>.</p></p class="citation"></blockquote><h3 id=1338--14248-named-entity-recognition-for-address-extraction-in-speech-to-text-transcriptions-using-synthetic-data-bibiána-lajčinová-et-al-2024>(13/38 | 14/248) Named Entity Recognition for Address Extraction in Speech-to-Text Transcriptions Using Synthetic Data (Bibiána Lajčinová et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bibiána Lajčinová, Patrik Valábek, Michal Spišiak. (2024)<br><strong>Named Entity Recognition for Address Extraction in Speech-to-Text Transcriptions Using Synthetic Data</strong><br><button class=copy-to-clipboard title="Named Entity Recognition for Address Extraction in Speech-to-Text Transcriptions Using Synthetic Data" index=14>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-14 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: I-2-7, cs-CL, cs.CL<br>Keyword Score: 50<br>Keywords: BERT, GPT, Transformer, Named Entity Recognition, Named Entity Recognition<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05545v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05545v1.pdf filename=2402.05545v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper introduces an approach for building a <b>Named</b> <b>Entity</b> <b>Recognition</b> <b>(NER)</b> model built upon a Bidirectional Encoder Representations from <b>Transformers</b> <b>(BERT)</b> architecture, specifically utilizing the SlovakBERT model. This <b>NER</b> model extracts address parts from data acquired from speech-to-text transcriptions. Due to scarcity of real data, a synthetic dataset using <b>GPT</b> API was generated. The importance of mimicking spoken language variability in this artificial data is emphasized. The performance of our <b>NER</b> model, trained solely on synthetic data, is evaluated using small real test dataset.</p></p class="citation"></blockquote><h3 id=1438--15248-benchmarking-large-language-models-on-communicative-medical-coaching-a-novel-system-and-dataset-hengguan-huang-et-al-2024>(14/38 | 15/248) Benchmarking Large Language Models on Communicative Medical Coaching: a Novel System and Dataset (Hengguan Huang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hengguan Huang, Songtao Wang, Hongfu Liu, Hao Wang, Ye Wang. (2024)<br><strong>Benchmarking Large Language Models on Communicative Medical Coaching: a Novel System and Dataset</strong><br><button class=copy-to-clipboard title="Benchmarking Large Language Models on Communicative Medical Coaching: a Novel System and Dataset" index=15>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-15 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 46<br>Keywords: Benchmarking, Benchmarking, ChatGPT, Dialogue System, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05547v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05547v1.pdf filename=2402.05547v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Traditional applications of natural language processing (NLP) in healthcare have predominantly focused on patient-centered services, enhancing patient interactions and care delivery, such as through medical <b>dialogue</b> <b>systems.</b> However, the potential of NLP to benefit inexperienced doctors, particularly in areas such as communicative medical coaching, remains largely unexplored. We introduce ``ChatCoach,&rsquo;&rsquo; an integrated human-AI cooperative framework. Within this framework, both a patient agent and a coaching agent collaboratively support medical learners in practicing their medical communication skills during consultations. Unlike traditional <b>dialogue</b> <b>systems,</b> ChatCoach provides a simulated environment where a human doctor can engage in medical <b>dialogue</b> <b>with</b> a patient agent. Simultaneously, a coaching agent provides real-time feedback to the doctor. To construct the ChatCoach system, we developed a dataset and integrated <b>Large</b> <b>Language</b> <b>Models</b> such as <b>ChatGPT</b> and Llama2, aiming to assess their effectiveness in communicative medical coaching tasks. Our comparative analysis demonstrates that instruction-tuned Llama2 significantly outperforms <b>ChatGPT&rsquo;s</b> <b>prompting-based</b> approaches.</p></p class="citation"></blockquote><h3 id=1538--16248-efficient-stagewise-pretraining-via-progressive-subnetworks-abhishek-panigrahi-et-al-2024>(15/38 | 16/248) Efficient Stagewise Pretraining via Progressive Subnetworks (Abhishek Panigrahi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Abhishek Panigrahi, Nikunj Saunshi, Kaifeng Lyu, Sobhan Miryoosefi, Sashank Reddi, Satyen Kale, Sanjiv Kumar. (2024)<br><strong>Efficient Stagewise Pretraining via Progressive Subnetworks</strong><br><button class=copy-to-clipboard title="Efficient Stagewise Pretraining via Progressive Subnetworks" index=16>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-16 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keyword Score: 40<br>Keywords: BERT, Question Answering, Large Language Model, SuperGLUE<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05913v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05913v1.pdf filename=2402.05913v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent developments in <b>large</b> <b>language</b> <b>models</b> have sparked interest in efficient pretraining methods. A recent effective paradigm is to perform stage-wise training, where the size of the model is gradually increased over the course of training (e.g. gradual stacking (Reddi et al., 2023)). While the resource and wall-time savings are appealing, it has limitations, particularly the inability to evaluate the full model during earlier stages, and degradation in model quality due to smaller model capacity in the initial stages. In this work, we propose an alternative framework, progressive subnetwork training, that maintains the full model throughout training, but only trains subnetworks within the model in each step. We focus on a simple instantiation of this framework, Random Path Training (RaPTr) that only trains a sub-path of layers in each step, progressively increasing the path lengths in stages. RaPTr achieves better pre-training loss for <b>BERT</b> and UL2 language models while requiring 20-33% fewer FLOPs compared to standard training, and is competitive or better than other efficient training methods. Furthermore, RaPTr shows better downstream performance on UL2, improving <b>QA</b> tasks and <b>SuperGLUE</b> by 1-5% compared to standard training and stacking. Finally, we provide a theoretical basis for RaPTr to justify (a) the increasing complexity of subnetworks in stages, and (b) the stability in loss across stage transitions due to residual connections and layer norm.</p></p class="citation"></blockquote><h3 id=1638--17248-is-it-possible-to-edit-large-language-models-robustly-xinbei-ma-et-al-2024>(16/38 | 17/248) Is it Possible to Edit Large Language Models Robustly? (Xinbei Ma et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xinbei Ma, Tianjie Ju, Jiyang Qiu, Zhuosheng Zhang, Hai Zhao, Lifeng Liu, Yulong Wang. (2024)<br><strong>Is it Possible to Edit Large Language Models Robustly?</strong><br><button class=copy-to-clipboard title="Is it Possible to Edit Large Language Models Robustly?" index=17>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-17 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Language Generation, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05827v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05827v1.pdf filename=2402.05827v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have played a pivotal role in building communicative AI to imitate human behaviors but face the challenge of efficient customization. To tackle this challenge, recent studies have delved into the realm of model editing, which manipulates specific memories of <b>language</b> <b>models</b> and changes the related <b>language</b> <b>generation.</b> However, the robustness of model editing remains an open question. This work seeks to understand the strengths and limitations of editing methods, thus facilitating robust, realistic applications of communicative AI. Concretely, we conduct extensive analysis to address the three key research questions. Q1: Can edited <b>LLMs</b> behave consistently resembling communicative AI in realistic situations? Q2: To what extent does the rephrasing of <b>prompts</b> lead <b>LLMs</b> to deviate from the edited knowledge memory? Q3: Which knowledge features are correlated with the performance and robustness of editing? Our experimental results uncover a substantial disparity between existing editing methods and the practical application of <b>LLMs.</b> On rephrased <b>prompts</b> that are complex and flexible but common in realistic applications, the performance of editing experiences a significant decline. Further analysis shows that more popular knowledge is memorized better, easier to recall, and more challenging to edit effectively.</p></p class="citation"></blockquote><h3 id=1738--18248-selective-forgetting-advancing-machine-unlearning-techniques-and-evaluation-in-language-models-lingzhi-wang-et-al-2024>(17/38 | 18/248) Selective Forgetting: Advancing Machine Unlearning Techniques and Evaluation in Language Models (Lingzhi Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lingzhi Wang, Xingshan Zeng, Jinsong Guo, Kam-Fai Wong, Georg Gottlob. (2024)<br><strong>Selective Forgetting: Advancing Machine Unlearning Techniques and Evaluation in Language Models</strong><br><button class=copy-to-clipboard title="Selective Forgetting: Advancing Machine Unlearning Techniques and Evaluation in Language Models" index=18>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-18 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Machine Unlearning, Information Retrieval, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05813v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05813v1.pdf filename=2402.05813v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The aim of this study is to investigate <b>Machine</b> <b>Unlearning</b> (MU), a burgeoning field focused on addressing concerns related to neural models inadvertently retaining personal or sensitive data. Here, a novel approach is introduced to achieve precise and selective forgetting within language models. Unlike previous methodologies that adopt completely opposing training objectives, this approach aims to mitigate adverse effects on language model performance, particularly in generation tasks. Furthermore, two innovative evaluation metrics are proposed: Sensitive <b>Information</b> <b>Extraction</b> Likelihood (S-EL) and Sensitive <b>Information</b> <b>Memory</b> Accuracy (S-MA), designed to gauge the effectiveness of sensitive <b>information</b> <b>elimination.</b> To reinforce the forgetting framework, an effective method for annotating sensitive scopes is presented, involving both online and offline strategies. The online selection mechanism leverages language probability scores to ensure computational efficiency, while the offline annotation entails a robust two-stage process based on <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs).</b></p></p class="citation"></blockquote><h3 id=1838--19248-autoaugment-is-what-you-need-enhancing-rule-based-augmentation-methods-in-low-resource-regimes-juhwan-choi-et-al-2024>(18/38 | 19/248) AutoAugment Is What You Need: Enhancing Rule-based Augmentation Methods in Low-resource Regimes (Juhwan Choi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Juhwan Choi, Kyohoon Jin, Junho Lee, Sangmin Song, Youngbin Kim. (2024)<br><strong>AutoAugment Is What You Need: Enhancing Rule-based Augmentation Methods in Low-resource Regimes</strong><br><button class=copy-to-clipboard title="AutoAugment Is What You Need: Enhancing Rule-based Augmentation Methods in Low-resource Regimes" index=19>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-19 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Data Augmentation, Label Smoothing, Low-Resource, Pre-trained Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05584v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05584v1.pdf filename=2402.05584v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Text <b>data</b> <b>augmentation</b> is a complex problem due to the discrete nature of sentences. Although rule-based augmentation methods are widely adopted in real-world applications because of their simplicity, they suffer from potential semantic damage. Previous researchers have suggested easy <b>data</b> <b>augmentation</b> with soft <b>labels</b> <b>(softEDA),</b> employing <b>label</b> <b>smoothing</b> to mitigate this problem. However, finding the best factor for each model and dataset is challenging; therefore, using softEDA in real-world applications is still difficult. In this paper, we propose adapting AutoAugment to solve this problem. The experimental results suggest that the proposed method can boost existing augmentation methods and that rule-based methods can enhance cutting-edge <b>pre-trained</b> <b>language</b> <b>models.</b> We offer the source code.</p></p class="citation"></blockquote><h3 id=1938--20248-noisyicl-a-little-noise-in-model-parameters-calibrates-in-context-learning-yufeng-zhao-et-al-2024>(19/38 | 20/248) NoisyICL: A Little Noise in Model Parameters Calibrates In-context Learning (Yufeng Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yufeng Zhao, Yoshihiro Sakai, Naoya Inoue. (2024)<br><strong>NoisyICL: A Little Noise in Model Parameters Calibrates In-context Learning</strong><br><button class=copy-to-clipboard title="NoisyICL: A Little Noise in Model Parameters Calibrates In-context Learning" index=20>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-20 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 40<br>Keywords: Fine-tuning, In-context Learning, In-context Learning, In-context Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05515v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05515v1.pdf filename=2402.05515v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>In-Context</b> <b>Learning</b> <b>(ICL)</b> is suffering from unsatisfactory performance and under-calibration due to high prior bias and unfaithful confidence. Some previous works <b>fine-tuned</b> language models for better <b>ICL</b> performance with enormous datasets and computing costs. In this paper, we propose NoisyICL, simply perturbing the model parameters by random noises to strive for better performance and calibration. Our experiments on 2 models and 12 downstream datasets show that NoisyICL can help <b>ICL</b> produce more accurate predictions. Our further analysis indicates that NoisyICL enables the model to provide more fair predictions, and also with less unfaithful confidence. Therefore, we believe that NoisyICL is an effective calibration of <b>ICL.</b> Our experimental code is uploaded to Github.</p></p class="citation"></blockquote><h3 id=2038--21248-spirit-lm-interleaved-spoken-and-written-language-model-tu-anh-nguyen-et-al-2024>(20/38 | 21/248) SpiRit-LM: Interleaved Spoken and Written Language Model (Tu Anh Nguyen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tu Anh Nguyen, Benjamin Muller, Bokai Yu, Marta R. Costa-jussa, Maha Elbayad, Sravya Popuri, Paul-Ambroise Duquenne, Robin Algayres, Ruslan Mavlyutov, Itai Gat, Gabriel Synnaeve, Juan Pino, Benoit Sagot, Emmanuel Dupoux. (2024)<br><strong>SpiRit-LM: Interleaved Spoken and Written Language Model</strong><br><button class=copy-to-clipboard title="SpiRit-LM: Interleaved Spoken and Written Language Model" index=21>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-21 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-SD, cs.CL, eess-AS<br>Keyword Score: 36<br>Keywords: Few-shot, Multi-modal, Multi-modal, Automatic Speech Recognition, Text-to-speech<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05755v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05755v1.pdf filename=2402.05755v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce SPIRIT-LM, a foundation <b>multimodal</b> language model that freely mixes text and speech. Our model is based on a pretrained text language model that we extend to the speech modality by continuously training it on text and speech units. Speech and text sequences are concatenated as a single set of tokens, and trained with a word-level interleaving method using a small automatically-curated speech-text parallel corpus. SPIRIT-LM comes in two versions: a BASE version that uses speech semantic units and an EXPRESSIVE version that models expressivity using pitch and style units in addition to the semantic units. For both versions, the text is encoded with subword BPE tokens. The resulting model displays both the semantic abilities of text models and the expressive abilities of speech models. Additionally, we demonstrate that SPIRIT-LM is able to learn new tasks in a <b>few-shot</b> fashion across modalities (i.e. <b>ASR,</b> <b>TTS,</b> Speech Classification).</p></p class="citation"></blockquote><h3 id=2138--22248-exploring-visual-culture-awareness-in-gpt-4v-a-comprehensive-probing-yong-cao-et-al-2024>(21/38 | 22/248) Exploring Visual Culture Awareness in GPT-4V: A Comprehensive Probing (Yong Cao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yong Cao, Wenyan Li, Jiaang Li, Yifei Yuan, Daniel Hershcovich. (2024)<br><strong>Exploring Visual Culture Awareness in GPT-4V: A Comprehensive Probing</strong><br><button class=copy-to-clipboard title="Exploring Visual Culture Awareness in GPT-4V: A Comprehensive Probing" index=22>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-22 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-CV, cs.CL<br>Keyword Score: 33<br>Keywords: Benchmarking, Low-Resource, GPT, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.06015v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.06015v1.pdf filename=2402.06015v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Pretrained large <b>Vision-Language</b> models have drawn considerable interest in recent years due to their remarkable performance. Despite considerable efforts to assess these models from diverse perspectives, the extent of visual cultural awareness in the state-of-the-art <b>GPT-4V</b> model remains unexplored. To tackle this gap, we extensively probed <b>GPT-4V</b> using the MaRVL <b>benchmark</b> dataset, aiming to investigate its capabilities and limitations in visual understanding with a focus on cultural aspects. Specifically, we introduced three visual related tasks, i.e. caption classification, pairwise captioning, and culture tag selection, to systematically delve into fine-grained visual cultural evaluation. Experimental results indicate that <b>GPT-4V</b> excels at identifying cultural concepts but still exhibits weaker performance in <b>low-resource</b> languages, such as Tamil and Swahili. Notably, through human evaluation, <b>GPT-4V</b> proves to be more culturally relevant in image captioning tasks than the original MaRVL human annotations, suggesting a promising solution for future visual cultural <b>benchmark</b> construction.</p></p class="citation"></blockquote><h3 id=2238--23248-rethinking-data-selection-for-supervised-fine-tuning-ming-shen-2024>(22/38 | 23/248) Rethinking Data Selection for Supervised Fine-Tuning (Ming Shen, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ming Shen. (2024)<br><strong>Rethinking Data Selection for Supervised Fine-Tuning</strong><br><button class=copy-to-clipboard title="Rethinking Data Selection for Supervised Fine-Tuning" index=23>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-23 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Fine-tuning, Supervised Learning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.06094v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.06094v1.pdf filename=2402.06094v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Although <b>supervised</b> <b>finetuning</b> (SFT) has emerged as an essential technique to align <b>large</b> <b>language</b> <b>models</b> with humans, it is considered superficial, with style learning being its nature. At the same time, recent works indicate the importance of data selection for SFT, showing that <b>finetuning</b> with high-quality and diverse subsets of the original dataset leads to superior downstream performance. In this work, we rethink the intuition behind data selection for SFT. Considering SFT is superficial, we propose that essential demonstrations for SFT should focus on reflecting human-like interactions instead of data quality or diversity. However, it is not straightforward to directly assess to what extent a demonstration reflects human styles. Towards an initial attempt in this direction, we find selecting instances with long responses is surprisingly more effective for SFT than utilizing full datasets or instances selected based on quality and diversity. We hypothesize that such a simple heuristic implicitly mimics a crucial aspect of human-style conversation: detailed responses are usually more helpful.</p></p class="citation"></blockquote><h3 id=2338--24248-fact-gpt-fact-checking-augmentation-via-claim-matching-with-llms-eun-cheol-choi-et-al-2024>(23/38 | 24/248) FACT-GPT: Fact-Checking Augmentation via Claim Matching with LLMs (Eun Cheol Choi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Eun Cheol Choi, Emilio Ferrara. (2024)<br><strong>FACT-GPT: Fact-Checking Augmentation via Claim Matching with LLMs</strong><br><button class=copy-to-clipboard title="FACT-GPT: Fact-Checking Augmentation via Claim Matching with LLMs" index=24>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-24 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-CY, cs-HC, cs-SI, cs.CL<br>Keyword Score: 30<br>Keywords: Fact Verification, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05904v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05904v1.pdf filename=2402.05904v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Our society is facing rampant misinformation harming public health and trust. To address the societal challenge, we introduce <b>FACT-GPT,</b> <b>a</b> system leveraging <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> to automate the claim matching stage of <b>fact-checking.</b> <b>FACT-GPT,</b> <b>trained</b> on a synthetic dataset, identifies social media content that aligns with, contradicts, or is irrelevant to previously debunked claims. Our evaluation shows that our specialized <b>LLMs</b> can match the accuracy of larger models in identifying related claims, closely mirroring human judgment. This research provides an automated solution for efficient claim matching, demonstrates the potential of <b>LLMs</b> in supporting <b>fact-checkers,</b> <b>and</b> offers valuable resources for further research in the field.</p></p class="citation"></blockquote><h3 id=2438--25248-phonetically-rich-corpus-construction-for-a-low-resourced-language-marcellus-amadeus-et-al-2024>(24/38 | 25/248) Phonetically rich corpus construction for a low-resourced language (Marcellus Amadeus et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Marcellus Amadeus, William Alberto Cruz Castañeda, Wilmer Lobato, Niasche Aquino. (2024)<br><strong>Phonetically rich corpus construction for a low-resourced language</strong><br><button class=copy-to-clipboard title="Phonetically rich corpus construction for a low-resourced language" index=25>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-25 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Low-Resource, Text-to-speech, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05794v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05794v1.pdf filename=2402.05794v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Speech technologies rely on capturing a speaker&rsquo;s voice variability while obtaining comprehensive language information. Textual <b>prompts</b> and sentence selection methods have been proposed in the literature to comprise such adequate phonetic data, referred to as a phonetically rich \textit{corpus}. However, they are still insufficient for acoustic modeling, especially critical for languages with limited resources. Hence, this paper proposes a novel approach and outlines the methodological aspects required to create a \textit{corpus} with broad phonetic coverage for a <b>low-resourced</b> language, Brazilian Portuguese. Our methodology includes text dataset collection up to a sentence selection algorithm based on triphone distribution. Furthermore, we propose a new phonemic classification according to acoustic-articulatory speech features since the absolute number of distinct triphones, or low-probability triphones, does not guarantee an adequate representation of every possible combination. Using our algorithm, we achieve a 55.8% higher percentage of distinct triphones &ndash; for samples of similar size &ndash; while the currently available phonetic-rich corpus, CETUC and <b>TTS-Portuguese,</b> 12.6% and 12.3% in comparison to a non-phonetically rich dataset.</p></p class="citation"></blockquote><h3 id=2538--26248-gpts-are-multilingual-annotators-for-sequence-generation-tasks-juhwan-choi-et-al-2024>(25/38 | 26/248) GPTs Are Multilingual Annotators for Sequence Generation Tasks (Juhwan Choi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Juhwan Choi, Eunju Lee, Kyohoon Jin, YoungBin Kim. (2024)<br><strong>GPTs Are Multilingual Annotators for Sequence Generation Tasks</strong><br><button class=copy-to-clipboard title="GPTs Are Multilingual Annotators for Sequence Generation Tasks" index=26>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-26 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: Low-Resource, GPT, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05512v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05512v1.pdf filename=2402.05512v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Data annotation is an essential step for constructing new datasets. However, the conventional approach of data annotation through crowdsourcing is both time-consuming and expensive. In addition, the complexity of this process increases when dealing with <b>low-resource</b> languages owing to the difference in the language pool of crowdworkers. To address these issues, this study proposes an autonomous annotation method by utilizing <b>large</b> <b>language</b> <b>models,</b> which have been recently demonstrated to exhibit remarkable performance. Through our experiments, we demonstrate that the proposed method is not just cost-efficient but also applicable for <b>low-resource</b> language annotation. Additionally, we constructed an image captioning dataset using our approach and are committed to open this dataset for future study. We have opened our source code for further study and reproducibility.</p></p class="citation"></blockquote><h3 id=2638--27248-large-language-models-for-psycholinguistic-plausibility-pretesting-samuel-joseph-amouyal-et-al-2024>(26/38 | 27/248) Large Language Models for Psycholinguistic Plausibility Pretesting (Samuel Joseph Amouyal et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Samuel Joseph Amouyal, Aya Meltzer-Asscher, Jonathan Berant. (2024)<br><strong>Large Language Models for Psycholinguistic Plausibility Pretesting</strong><br><button class=copy-to-clipboard title="Large Language Models for Psycholinguistic Plausibility Pretesting" index=27>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-27 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 30<br>Keywords: GPT, GPT-4, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05455v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05455v1.pdf filename=2402.05455v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In psycholinguistics, the creation of controlled materials is crucial to ensure that research outcomes are solely attributed to the intended manipulations and not influenced by extraneous factors. To achieve this, psycholinguists typically pretest linguistic materials, where a common pretest is to solicit plausibility judgments from human evaluators on specific sentences. In this work, we investigate whether Language Models (LMs) can be used to generate these plausibility judgements. We investigate a wide range of LMs across multiple linguistic structures and evaluate whether their plausibility judgements correlate with human judgements. We find that <b>GPT-4</b> plausibility judgements highly correlate with human judgements across the structures we examine, whereas other LMs correlate well with humans on commonly used syntactic structures. We then test whether this correlation implies that LMs can be used instead of humans for pretesting. We find that when coarse-grained plausibility judgements are needed, this works well, but when fine-grained judgements are necessary, even <b>GPT-4</b> does not provide satisfactory discriminative power.</p></p class="citation"></blockquote><h3 id=2738--28248-generative-echo-chamber-effects-of-llm-powered-search-systems-on-diverse-information-seeking-nikhil-sharma-et-al-2024>(27/38 | 28/248) Generative Echo Chamber? Effects of LLM-Powered Search Systems on Diverse Information Seeking (Nikhil Sharma et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nikhil Sharma, Q. Vera Liao, Ziang Xiao. (2024)<br><strong>Generative Echo Chamber? Effects of LLM-Powered Search Systems on Diverse Information Seeking</strong><br><button class=copy-to-clipboard title="Generative Echo Chamber? Effects of LLM-Powered Search Systems on Diverse Information Seeking" index=28>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-28 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-HC, cs.CL<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05880v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05880v2.pdf filename=2402.05880v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> powered conversational search systems have already been used by hundreds of millions of people, and are believed to bring many benefits over conventional search. However, while decades of research and public discourse interrogated the risk of search systems in increasing selective exposure and creating echo chambers &ndash; limiting exposure to diverse opinions and leading to opinion polarization, little is known about such a risk of <b>LLM-powered</b> conversational search. We conduct two experiments to investigate: 1) whether and how <b>LLM-powered</b> conversational search increases selective exposure compared to conventional search; 2) whether and how <b>LLMs</b> with opinion biases that either reinforce or challenge the user&rsquo;s view change the effect. Overall, we found that participants engaged in more biased information querying with <b>LLM-powered</b> conversational search, and an opinionated <b>LLM</b> reinforcing their views exacerbated this bias. These results present critical implications for the development of <b>LLMs</b> and conversational search systems, and the policy governing these technologies.</p></p class="citation"></blockquote><h3 id=2838--29248-permute-and-flip-an-optimally-robust-and-watermarkable-decoder-for-llms-xuandong-zhao-et-al-2024>(28/38 | 29/248) Permute-and-Flip: An optimally robust and watermarkable decoder for LLMs (Xuandong Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xuandong Zhao, Lei Li, Yu-Xiang Wang. (2024)<br><strong>Permute-and-Flip: An optimally robust and watermarkable decoder for LLMs</strong><br><button class=copy-to-clipboard title="Permute-and-Flip: An optimally robust and watermarkable decoder for LLMs" index=29>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-29 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-CR, cs-LG, cs.CL<br>Keyword Score: 20<br>Keywords: Large Language Model, Perplexity<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05864v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05864v1.pdf filename=2402.05864v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we propose a new decoding method called Permute-and-Flip (PF) decoder. It enjoys robustness properties similar to the standard sampling decoder, but is provably up to 2x better in its quality-robustness tradeoff than sampling and never worse than any other decoder. We also design a cryptographic watermarking scheme analogous to Aaronson&rsquo;s Gumbel watermark, but naturally tailored for PF decoder. The watermarking scheme does not change the distribution to sample, while allowing arbitrarily low false positive rate and high recall whenever the generated text has high entropy. Our experiments show that the PF decoder (and its watermarked counterpart) significantly outperform(s) naive sampling (and it&rsquo;s Gumbel watermarked counterpart) in terms of <b>perplexity,</b> while retaining the same robustness (and detectability), hence making it a promising new approach for <b>LLM</b> decoding. The code is available at <a href=https://github.com/XuandongZhao/pf-decoding>https://github.com/XuandongZhao/pf-decoding</a></p></p class="citation"></blockquote><h3 id=2938--30248-text-to-code-generation-with-modality-relative-pre-training-fenia-christopoulou-et-al-2024>(29/38 | 30/248) Text-to-Code Generation with Modality-relative Pre-training (Fenia Christopoulou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Fenia Christopoulou, Guchun Zhang, Gerasimos Lampouras. (2024)<br><strong>Text-to-Code Generation with Modality-relative Pre-training</strong><br><button class=copy-to-clipboard title="Text-to-Code Generation with Modality-relative Pre-training" index=30>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-30 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 20<br>Keywords: Transfer Learning, Pre-trained Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05783v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05783v2.pdf filename=2402.05783v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Large <b>pre-trained</b> <b>language</b> <b>models</b> have recently been expanded and applied to programming language tasks with great success, often through further pre-training of a strictly-natural language model&ndash;where training sequences typically contain both natural and (linearised) programming language. Such approaches effectively map both modalities of the sequence into the same embedding space. However, programming language keywords (e.g. &ldquo;while&rdquo;) often have very strictly defined semantics. As such, <b>transfer</b> <b>learning</b> from their natural language usage may not necessarily be beneficial to their code application and vise versa. Assuming an already <b>pre-trained</b> <b>language</b> <b>model,</b> in this work we investigate how sequence tokens can be adapted and represented differently, depending on which modality they belong to, and to the ultimate benefit of the downstream task. We experiment with separating embedding spaces between modalities during further model pre-training with modality-relative training objectives. We focus on text-to-code generation and observe consistent improvements across two backbone models and two test sets, measuring pass@$k$ and a novel incremental variation.</p></p class="citation"></blockquote><h3 id=3038--31248-multilingual-e5-text-embeddings-a-technical-report-liang-wang-et-al-2024>(30/38 | 31/248) Multilingual E5 Text Embeddings: A Technical Report (Liang Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, Furu Wei. (2024)<br><strong>Multilingual E5 Text Embeddings: A Technical Report</strong><br><button class=copy-to-clipboard title="Multilingual E5 Text Embeddings: A Technical Report" index=31>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-31 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-IR, cs.CL<br>Keyword Score: 20<br>Keywords: Fine-tuning, Text Embedding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05672v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05672v1.pdf filename=2402.05672v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This technical report presents the training methodology and evaluation results of the open-source multilingual E5 <b>text</b> <b>embedding</b> models, released in mid-2023. Three embedding models of different sizes (small / base / large) are provided, offering a balance between the inference efficiency and embedding quality. The training procedure adheres to the English E5 model recipe, involving contrastive pre-training on 1 billion multilingual <b>text</b> <b>pairs,</b> followed by <b>fine-tuning</b> on a combination of labeled datasets. Additionally, we introduce a new instruction-tuned embedding model, whose performance is on par with state-of-the-art, English-only models of similar sizes. Information regarding the model release can be found at <a href=https://github.com/microsoft/unilm/tree/master/e5>https://github.com/microsoft/unilm/tree/master/e5</a> .</p></p class="citation"></blockquote><h3 id=3138--32248-pretrained-generative-language-models-as-general-learning-frameworks-for-sequence-based-tasks-ben-fauber-2024>(31/38 | 32/248) Pretrained Generative Language Models as General Learning Frameworks for Sequence-Based Tasks (Ben Fauber, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ben Fauber. (2024)<br><strong>Pretrained Generative Language Models as General Learning Frameworks for Sequence-Based Tasks</strong><br><button class=copy-to-clipboard title="Pretrained Generative Language Models as General Learning Frameworks for Sequence-Based Tasks" index=32>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-32 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs-LG, cs.CL<br>Keyword Score: 20<br>Keywords: Fine-tuning, Fine-tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05616v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05616v1.pdf filename=2402.05616v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We propose that small pretrained foundational generative language models with millions of parameters can be utilized as a general learning framework for sequence-based tasks. Our proposal overcomes the computational resource, skill set, and timeline challenges associated with training neural networks and language models from scratch. Further, our approach focuses on creating small and highly specialized models that can accurately execute a challenging task of which the base model is incapable of performing. We demonstrate that 125M, 350M, and 1.3B parameter pretrained foundational language models can be instruction <b>fine-tuned</b> with 10,000-to-1,000,000 instruction examples to achieve near state-of-the-art results on challenging cheminformatics tasks. We also demonstrate the role of successive language model <b>fine-tuning</b> epochs on improved outcomes, as well as the importance of both data formatting and pretrained foundational language model selection for instruction <b>fine-tuning</b> success.</p></p class="citation"></blockquote><h3 id=3238--33248-establishing-degrees-of-closeness-between-audio-recordings-along-different-dimensions-using-large-scale-cross-lingual-models-maxime-fily-et-al-2024>(32/38 | 33/248) Establishing degrees of closeness between audio recordings along different dimensions using large-scale cross-lingual models (Maxime Fily et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Maxime Fily, Guillaume Wisniewski, Severine Guillaume, Gilles Adda, Alexis Michaud. (2024)<br><strong>Establishing degrees of closeness between audio recordings along different dimensions using large-scale cross-lingual models</strong><br><button class=copy-to-clipboard title="Establishing degrees of closeness between audio recordings along different dimensions using large-scale cross-lingual models" index=33>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-33 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-SD, cs.CL, eess-AS<br>Keyword Score: 20<br>Keywords: Low-Resource, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05581v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05581v1.pdf filename=2402.05581v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the highly constrained context of <b>low-resource</b> language studies, we explore vector representations of speech from a pretrained model to determine their level of abstraction with regard to the audio signal. We propose a new <b>unsupervised</b> method using ABX tests on audio recordings with carefully curated metadata to shed light on the type of information present in the representations. ABX tests determine whether the representations computed by a multilingual speech model encode a given characteristic. Three experiments are devised: one on room acoustics aspects, one on linguistic genre, and one on phonetic aspects. The results confirm that the representations extracted from recordings with different linguistic/extra-linguistic characteristics differ along the same lines. Embedding more audio signal in one vector better discriminates extra-linguistic characteristics, whereas shorter snippets are better to distinguish segmental information. The method is fully <b>unsupervised,</b> potentially opening new research avenues for comparative work on under-documented languages.</p></p class="citation"></blockquote><h3 id=3338--34248-traditional-machine-learning-models-and-bidirectional-encoder-representations-from-transformer-bert-based-automatic-classification-of-tweets-about-eating-disorders-algorithm-development-and-validation-study-josé-alberto-benítez-andrades-et-al-2024>(33/38 | 34/248) Traditional Machine Learning Models and Bidirectional Encoder Representations From Transformer (BERT)-Based Automatic Classification of Tweets About Eating Disorders: Algorithm Development and Validation Study (José Alberto Benítez-Andrades et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>José Alberto Benítez-Andrades, José-Manuel Alija-Pérez, Maria-Esther Vidal, Rafael Pastor-Vargas, María Teresa García-Ordás. (2024)<br><strong>Traditional Machine Learning Models and Bidirectional Encoder Representations From Transformer (BERT)-Based Automatic Classification of Tweets About Eating Disorders: Algorithm Development and Validation Study</strong><br><button class=copy-to-clipboard title="Traditional Machine Learning Models and Bidirectional Encoder Representations From Transformer (BERT)-Based Automatic Classification of Tweets About Eating Disorders: Algorithm Development and Validation Study" index=34>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-34 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-LG, cs.CL<br>Keyword Score: 20<br>Keywords: BERT, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05571v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05571v1.pdf filename=2402.05571v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Background: Eating disorders are increasingly prevalent, and social networks offer valuable information. Objective: Our goal was to identify efficient machine learning models for categorizing tweets related to eating disorders. Methods: Over three months, we collected tweets about eating disorders. A 2,000-tweet subset was labeled for: (1) being written by individuals with eating disorders, (2) promoting eating disorders, (3) informativeness, and (4) scientific content. Both traditional machine learning and deep learning models were employed for classification, assessing accuracy, F1 score, and computational time. Results: From 1,058,957 collected tweets, <b>transformer-based</b> bidirectional encoder representations achieved the highest F1 scores (71.1%-86.4%) across all four categories. Conclusions: <b>Transformer-based</b> models outperform traditional techniques in classifying eating disorder-related tweets, though they require more computational resources.</p></p class="citation"></blockquote><h3 id=3438--35248-improving-agent-interactions-in-virtual-environments-with-language-models-jack-zhang-2024>(34/38 | 35/248) Improving Agent Interactions in Virtual Environments with Language Models (Jack Zhang, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jack Zhang. (2024)<br><strong>Improving Agent Interactions in Virtual Environments with Language Models</strong><br><button class=copy-to-clipboard title="Improving Agent Interactions in Virtual Environments with Language Models" index=35>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-35 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 13<br>Keywords: Multi-modal, Grounding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05440v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05440v1.pdf filename=2402.05440v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Enhancing AI systems with efficient communication skills for effective human assistance necessitates proactive initiatives from the system side to discern specific circumstances and interact aptly. This research focuses on a collective building assignment in the Minecraft dataset, employing language modeling to enhance task understanding through state-of-the-art methods. These models focus on <b>grounding</b> <b>multi-modal</b> understanding and task-oriented dialogue comprehension tasks, providing insights into their interpretative and responsive capabilities. Our experimental results showcase a substantial improvement over existing methods, indicating a promising direction for future research in this domain.</p></p class="citation"></blockquote><h3 id=3538--36248-lightcam-a-fast-and-light-implementation-of-context-aware-masking-based-d-tdnn-for-speaker-verification-di-cao-et-al-2024>(35/38 | 36/248) LightCAM: A Fast and Light Implementation of Context-Aware Masking based D-TDNN for Speaker Verification (Di Cao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Di Cao, Xianchen Wang, Junfeng Zhou, Jiakai Zhang, Yanjing Lei, Wenpeng Chen. (2024)<br><strong>LightCAM: A Fast and Light Implementation of Context-Aware Masking based D-TDNN for Speaker Verification</strong><br><button class=copy-to-clipboard title="LightCAM: A Fast and Light Implementation of Context-Aware Masking based D-TDNN for Speaker Verification" index=36>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-36 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs-SD, cs.CL, eess-AS<br>Keyword Score: 10<br>Keywords: Convolution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.06073v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.06073v2.pdf filename=2402.06073v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Traditional Time Delay Neural Networks (TDNN) have achieved state-of-the-art performance at the cost of high computational complexity and slower inference speed, making them difficult to implement in an industrial environment. The Densely Connected Time Delay Neural Network (D-TDNN) with Context Aware Masking (CAM) module has proven to be an efficient structure to reduce complexity while maintaining system performance. In this paper, we propose a fast and lightweight model, LightCAM, which further adopts a depthwise separable <b>convolution</b> module (DSM) and uses multi-scale feature aggregation (MFA) for feature fusion at different levels. Extensive experiments are conducted on VoxCeleb dataset, the comparative results show that it has achieved an EER of 0.83 and MinDCF of 0.0891 in VoxCeleb1-O, which outperforms the other mainstream speaker verification methods. In addition, complexity analysis further demonstrates that the proposed architecture has lower computational cost and faster inference speed.</p></p class="citation"></blockquote><h3 id=3638--37248-faq-gen-an-automated-system-to-generate-domain-specific-faqs-to-aid-content-comprehension-sahil-kale-et-al-2024>(36/38 | 37/248) FAQ-Gen: An automated system to generate domain-specific FAQs to aid content comprehension (Sahil Kale et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sahil Kale, Gautam Khaire, Jay Patankar. (2024)<br><strong>FAQ-Gen: An automated system to generate domain-specific FAQs to aid content comprehension</strong><br><button class=copy-to-clipboard title="FAQ-Gen: An automated system to generate domain-specific FAQs to aid content comprehension" index=37>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-37 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-CL, cs.CL<br>Keyword Score: 10<br>Keywords: Question Answering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05812v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05812v1.pdf filename=2402.05812v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Frequently Asked <b>Questions</b> <b>(FAQs)</b> refer to the most common inquiries about specific content. They serve as content comprehension aids by simplifying topics and enhancing understanding through succinct presentation of information. In this paper, we address FAQ generation as a well-defined Natural Language Processing (NLP) task through the development of an end-to-end system leveraging text-to-text transformation models. We present a literature review covering traditional <b>question-answering</b> <b>systems,</b> highlighting their limitations when applied directly to the FAQ generation task. We propose our system capable of building FAQs from textual content tailored to specific domains, enhancing their accuracy and relevance. We utilise self-curated algorithms for obtaining optimal representation of information to be provided as input and also for ranking the <b>question-answer</b> <b>pairs</b> to maximise human comprehension. Qualitative human evaluation showcases the generated FAQs to be well-constructed and readable, while also utilising domain-specific constructs to highlight domain-based nuances and jargon in the original content.</p></p class="citation"></blockquote><h3 id=3738--38248-softeda-rethinking-rule-based-data-augmentation-with-soft-labels-juhwan-choi-et-al-2024>(37/38 | 38/248) SoftEDA: Rethinking Rule-Based Data Augmentation with Soft Labels (Juhwan Choi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Juhwan Choi, Kyohoon Jin, Junho Lee, Sangmin Song, Youngbin Kim. (2024)<br><strong>SoftEDA: Rethinking Rule-Based Data Augmentation with Soft Labels</strong><br><button class=copy-to-clipboard title="SoftEDA: Rethinking Rule-Based Data Augmentation with Soft Labels" index=38>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-38 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CL<br>Categories: cs-AI, cs-CL, cs.CL<br>Keyword Score: 10<br>Keywords: Data Augmentation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05591v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05591v1.pdf filename=2402.05591v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Rule-based text <b>data</b> <b>augmentation</b> is widely used for NLP tasks due to its simplicity. However, this method can potentially damage the original meaning of the text, ultimately hurting the performance of the model. To overcome this limitation, we propose a straightforward technique for applying soft labels to augmented <b>data.</b> <b>We</b> conducted experiments across seven different classification tasks and empirically demonstrated the effectiveness of our proposed approach. We have publicly opened our source code for reproducibility.</p></p class="citation"></blockquote><h2 id=csai-16>cs.AI (16)</h2><h3 id=016--39248-large-language-model-meets-graph-neural-network-in-knowledge-distillation-shengxiang-hu-et-al-2024>(0/16 | 39/248) Large Language Model Meets Graph Neural Network in Knowledge Distillation (Shengxiang Hu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shengxiang Hu, Guobing Zou, Song Yang, Yanglan Gan, Bofeng Zhang, Yixin Chen. (2024)<br><strong>Large Language Model Meets Graph Neural Network in Knowledge Distillation</strong><br><button class=copy-to-clipboard title="Large Language Model Meets Graph Neural Network in Knowledge Distillation" index=39>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-39 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: 68T30, 68R10, 68T05, cs-AI, cs-LG, cs.AI<br>Keyword Score: 113<br>Keywords: Node Classification, Graph Neural Network, Graph Neural Network, Benchmarking, Contrastive Learning, Knowledge Distillation, Knowledge Distillation, Knowledge Distillation, Instruction Tuning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05894v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05894v2.pdf filename=2402.05894v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Despite recent community revelations about the advancements and potential applications of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> in understanding Text-Attributed <b>Graph</b> <b>(TAG),</b> <b>the</b> deployment of <b>LLMs</b> for production is hindered by its high computational and storage requirements, as well as long latencies during model inference. Simultaneously, although traditional <b>Graph</b> <b>Neural</b> <b>Networks</b> <b>(GNNs)</b> are light weight and adept at learning structural features of <b>graphs,</b> <b>their</b> <b>ability</b> to grasp the complex semantics in TAG is somewhat constrained for real applications. To address these limitations, we concentrate on the downstream task of <b>node</b> <b>classification</b> in TAG and propose a novel <b>graph</b> <b>knowledge</b> <b>distillation</b> framework, termed Linguistic <b>Graph</b> <b>Knowledge</b> <b>Distillation</b> (LinguGKD), using <b>LLMs</b> as teacher models and <b>GNNs</b> as student models for <b>knowledge</b> <b>distillation.</b> It involves TAG-oriented <b>instruction</b> <b>tuning</b> of <b>LLM</b> on designed tailored <b>prompts,</b> followed by propagating <b>knowledge</b> <b>and</b> aligning the hierarchically learned <b>node</b> <b>features</b> from the teacher <b>LLM</b> to the student <b>GNN</b> in latent space, employing a layer-adaptive <b>contrastive</b> <b>learning</b> strategy. Through extensive experiments on a variety of <b>LLM</b> and <b>GNN</b> models and multiple <b>benchmark</b> datasets, the proposed LinguGKD significantly boosts the student <b>GNN&rsquo;s</b> predictive accuracy and convergence rate, without the need of extra data or model parameters. Compared to teacher <b>LLM,</b> <b>distilled</b> <b>GNN</b> achieves superior inference speed equipped with much fewer computing and storage demands, when surpassing the teacher <b>LLM&rsquo;s</b> classification accuracy on some of <b>benchmark</b> datasets.</p></p class="citation"></blockquote><h3 id=116--40248-guiding-large-language-models-with-divide-and-conquer-program-for-discerning-problem-solving-yizhou-zhang-et-al-2024>(1/16 | 40/248) Guiding Large Language Models with Divide-and-Conquer Program for Discerning Problem Solving (Yizhou Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yizhou Zhang, Lun Du, Defu Cao, Qiang Fu, Yan Liu. (2024)<br><strong>Guiding Large Language Models with Divide-and-Conquer Program for Discerning Problem Solving</strong><br><button class=copy-to-clipboard title="Guiding Large Language Models with Divide-and-Conquer Program for Discerning Problem Solving" index=40>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-40 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CL, cs-LG, cs.AI<br>Keyword Score: 80<br>Keywords: Foundation Model, Transformer, Fake News Detection, Hallucination Detection, Fake News Detection, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05359v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05359v1.pdf filename=2402.05359v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Foundation</b> <b>models,</b> such as <b>Large</b> <b>language</b> <b>Models</b> <b>(LLMs),</b> have attracted significant amount of interest due to their <b>large</b> <b>number</b> <b>of</b> applications. Existing works show that appropriate <b>prompt</b> design, such as Chain-of-Thoughts, can unlock <b>LLM&rsquo;s</b> powerful capacity in diverse areas. However, when handling tasks involving repetitive sub-tasks and/or deceptive contents, such as arithmetic calculation and article-level <b>fake</b> <b>news</b> <b>detection,</b> existing <b>prompting</b> strategies either suffers from insufficient expressive power or intermediate errors triggered by <b>hallucination.</b> <b>To</b> make <b>LLM</b> more discerning to such intermediate errors, we propose to guide <b>LLM</b> with a Divide-and-Conquer program that simultaneously ensures superior expressive power and disentangles task decomposition, sub-task resolution, and resolution assembly process. Theoretic analysis reveals that our strategy can guide <b>LLM</b> to extend the expressive power of fixed-depth <b>Transformer.</b> Experiments indicate that our proposed method can achieve better performance than typical <b>prompting</b> strategies in tasks bothered by intermediate errors and deceptive contents, such as <b>large</b> <b>integer</b> <b>multiplication,</b> <b>hallucination</b> <b>detection</b> and misinformation detection.</p></p class="citation"></blockquote><h3 id=216--41248-how-well-can-llms-negotiate-negotiationarena-platform-and-analysis-federico-bianchi-et-al-2024>(2/16 | 41/248) How Well Can LLMs Negotiate? NegotiationArena Platform and Analysis (Federico Bianchi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Federico Bianchi, Patrick John Chia, Mert Yuksekgonul, Jacopo Tagliabue, Dan Jurafsky, James Zou. (2024)<br><strong>How Well Can LLMs Negotiate? NegotiationArena Platform and Analysis</strong><br><button class=copy-to-clipboard title="How Well Can LLMs Negotiate? NegotiationArena Platform and Analysis" index=41>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-41 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CL, cs-GT, cs.AI<br>Keyword Score: 50<br>Keywords: GPT, GPT-4, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05863v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05863v1.pdf filename=2402.05863v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Negotiation is the basis of social interactions; humans negotiate everything from the price of cars to how to share common resources. With rapidly growing interest in using <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> to act as agents on behalf of human users, such <b>LLM</b> agents would also need to be able to negotiate. In this paper, we study how well <b>LLMs</b> can negotiate with each other. We develop NegotiationArena: a flexible framework for evaluating and probing the negotiation abilities of <b>LLM</b> agents. We implemented three types of scenarios in NegotiationArena to assess <b>LLM&rsquo;s</b> behaviors in allocating shared resources (ultimatum games), aggregate resources (trading games) and buy/sell goods (price negotiations). Each scenario allows for multiple turns of flexible dialogues between <b>LLM</b> agents to allow for more complex negotiations. Interestingly, <b>LLM</b> agents can significantly boost their negotiation outcomes by employing certain behavioral tactics. For example, by pretending to be desolate and desperate, <b>LLMs</b> can improve their payoffs by 20% when negotiating against the standard <b>GPT-4.</b> We also quantify irrational negotiation behaviors exhibited by the <b>LLM</b> agents, many of which also appear in humans. Together, \NegotiationArena offers a new environment to investigate <b>LLM</b> interactions, enabling new insights into <b>LLM&rsquo;s</b> theory of mind, irrationality, and <b>reasoning</b> abilities.</p></p class="citation"></blockquote><h3 id=316--42248-knowledge-graphs-meet-multi-modal-learning-a-comprehensive-survey-zhuo-chen-et-al-2024>(3/16 | 42/248) Knowledge Graphs Meet Multi-Modal Learning: A Comprehensive Survey (Zhuo Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhuo Chen, Yichi Zhang, Yin Fang, Yuxia Geng, Lingbing Guo, Xiang Chen, Qian Li, Wen Zhang, Jiaoyan Chen, Yushan Zhu, Jiaqi Li, Xiaoze Liu, Jeff Z. Pan, Ningyu Zhang, Huajun Chen. (2024)<br><strong>Knowledge Graphs Meet Multi-Modal Learning: A Comprehensive Survey</strong><br><button class=copy-to-clipboard title="Knowledge Graphs Meet Multi-Modal Learning: A Comprehensive Survey" index=42>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-42 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CV, cs-IR, cs-LG, cs.AI<br>Keyword Score: 46<br>Keywords: Benchmarking, Knowledge Graph, Knowledge Graph, Multi-modal, Question Answering, Visual Question Answering, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05391v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05391v2.pdf filename=2402.05391v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Knowledge</b> <b>Graphs</b> <b>(KGs)</b> play a pivotal role in advancing various AI applications, with the semantic web community&rsquo;s exploration into <b>multi-modal</b> dimensions unlocking new avenues for innovation. In this survey, we carefully review over 300 articles, focusing on <b>KG-aware</b> research in two principal aspects: <b>KG-driven</b> <b>Multi-Modal</b> (KG4MM) learning, where <b>KGs</b> support <b>multi-modal</b> tasks, and <b>Multi-Modal</b> <b>Knowledge</b> <b>Graph</b> (MM4KG), which extends <b>KG</b> studies into the MMKG realm. We begin by defining <b>KGs</b> and MMKGs, then explore their construction progress. Our review includes two primary task categories: <b>KG-aware</b> <b>multi-modal</b> learning tasks, such as Image Classification and <b>Visual</b> <b>Question</b> <b>Answering,</b> and intrinsic MMKG tasks like <b>Multi-modal</b> <b>Knowledge</b> <b>Graph</b> Completion and Entity Alignment, highlighting specific research trajectories. For most of these tasks, we provide definitions, evaluation <b>benchmarks,</b> and additionally outline essential insights for conducting relevant research. Finally, we discuss current challenges and identify emerging trends, such as progress in <b>Large</b> <b>Language</b> <b>Modeling</b> and <b>Multi-modal</b> Pre-training strategies. This survey aims to serve as a comprehensive reference for researchers already involved in or considering delving into <b>KG</b> and <b>multi-modal</b> learning research, offering insights into the evolving landscape of MMKG research and supporting future work.</p></p class="citation"></blockquote><h3 id=416--43248-rapid-optimization-for-jailbreaking-llms-via-subconscious-exploitation-and-echopraxia-guangyu-shen-et-al-2024>(4/16 | 43/248) Rapid Optimization for Jailbreaking LLMs via Subconscious Exploitation and Echopraxia (Guangyu Shen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Guangyu Shen, Siyuan Cheng, Kaiyuan Zhang, Guanhong Tao, Shengwei An, Lu Yan, Zhuo Zhang, Shiqing Ma, Xiangyu Zhang. (2024)<br><strong>Rapid Optimization for Jailbreaking LLMs via Subconscious Exploitation and Echopraxia</strong><br><button class=copy-to-clipboard title="Rapid Optimization for Jailbreaking LLMs via Subconscious Exploitation and Echopraxia" index=43>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-43 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CL, cs-CR, cs.AI<br>Keyword Score: 40<br>Keywords: Reasoning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05467v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05467v1.pdf filename=2402.05467v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> have become prevalent across diverse sectors, transforming human life with their extraordinary <b>reasoning</b> and comprehension abilities. As they find increased use in sensitive tasks, safety concerns have gained widespread attention. Extensive efforts have been dedicated to aligning <b>LLMs</b> with human moral principles to ensure their safe deployment. Despite their potential, recent research indicates aligned <b>LLMs</b> are prone to specialized jailbreaking <b>prompts</b> that bypass safety measures to elicit violent and harmful content. The intrinsic discrete nature and substantial scale of contemporary <b>LLMs</b> pose significant challenges in automatically generating diverse, efficient, and potent jailbreaking <b>prompts,</b> representing a continuous obstacle. In this paper, we introduce RIPPLE (Rapid Optimization via Subconscious Exploitation and Echopraxia), a novel optimization-based method inspired by two psychological concepts: subconsciousness and echopraxia, which describe the processes of the mind that occur without conscious awareness and the involuntary mimicry of actions, respectively. Evaluations across 6 open-source <b>LLMs</b> and 4 commercial <b>LLM</b> APIs show RIPPLE achieves an average Attack Success Rate of 91.5%, outperforming five current methods by up to 47.0% with an 8x reduction in overhead. Furthermore, it displays significant transferability and stealth, successfully evading established detection mechanisms. The code of our work is available at \url{https://github.com/SolidShen/RIPPLE_official/tree/official}</p></p class="citation"></blockquote><h3 id=516--44248-opentom-a-comprehensive-benchmark-for-evaluating-theory-of-mind-reasoning-capabilities-of-large-language-models-hainiu-xu-et-al-2024>(5/16 | 44/248) OpenToM: A Comprehensive Benchmark for Evaluating Theory-of-Mind Reasoning Capabilities of Large Language Models (Hainiu Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hainiu Xu, Runcong Zhao, Lixing Zhu, Jinhua Du, Yulan He. (2024)<br><strong>OpenToM: A Comprehensive Benchmark for Evaluating Theory-of-Mind Reasoning Capabilities of Large Language Models</strong><br><button class=copy-to-clipboard title="OpenToM: A Comprehensive Benchmark for Evaluating Theory-of-Mind Reasoning Capabilities of Large Language Models" index=44>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-44 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CL, cs.AI<br>Keyword Score: 33<br>Keywords: Benchmarking, Reasoning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.06044v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.06044v2.pdf filename=2402.06044v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Neural Theory-of-Mind (N-ToM), machine&rsquo;s ability to understand and keep track of the mental states of others, is pivotal in developing socially intelligent agents. However, prevalent N-ToM <b>benchmarks</b> have several shortcomings, including the presence of ambiguous and artificial narratives, absence of personality traits and preferences, a lack of questions addressing characters&rsquo; psychological mental states, and limited diversity in the questions posed. In response to these issues, we construct OpenToM, a new <b>benchmark</b> for assessing N-ToM with (1) longer and clearer narrative stories, (2) characters with explicit personality traits, (3) actions that are triggered by character intentions, and (4) questions designed to challenge <b>LLMs&rsquo;</b> capabilities of modeling characters&rsquo; mental states of both the physical and psychological world. Using OpenToM, we reveal that state-of-the-art <b>LLMs</b> thrive at modeling certain aspects of mental states in the physical world but fall short when tracking characters&rsquo; mental states in the psychological world.</p></p class="citation"></blockquote><h3 id=616--45248-training-large-language-models-for-reasoning-through-reverse-curriculum-reinforcement-learning-zhiheng-xi-et-al-2024>(6/16 | 45/248) Training Large Language Models for Reasoning through Reverse Curriculum Reinforcement Learning (Zhiheng Xi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhiheng Xi, Wenxiang Chen, Boyang Hong, Senjie Jin, Rui Zheng, Wei He, Yiwen Ding, Shichun Liu, Xin Guo, Junzhe Wang, Honglin Guo, Wei Shen, Xiaoran Fan, Yuhao Zhou, Shihan Dou, Xiao Wang, Xinbo Zhang, Peng Sun, Tao Gui, Qi Zhang, Xuanjing Huang. (2024)<br><strong>Training Large Language Models for Reasoning through Reverse Curriculum Reinforcement Learning</strong><br><button class=copy-to-clipboard title="Training Large Language Models for Reasoning through Reverse Curriculum Reinforcement Learning" index=45>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-45 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CL, cs-LG, cs.AI<br>Keyword Score: 30<br>Keywords: Reinforcement Learning, Reasoning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05808v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05808v1.pdf filename=2402.05808v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we propose R$^3$: Learning <b>Reasoning</b> through Reverse Curriculum <b>Reinforcement</b> <b>Learning</b> (RL), a novel method that employs only outcome supervision to achieve the benefits of process supervision for <b>large</b> <b>language</b> <b>models.</b> The core challenge in applying RL to complex <b>reasoning</b> is to identify a sequence of actions that result in positive rewards and provide appropriate supervision for optimization. Outcome supervision provides sparse rewards for final results without identifying error locations, whereas process supervision offers step-wise rewards but requires extensive manual annotation. R$^3$ overcomes these limitations by learning from correct demonstrations. Specifically, R$^3$ progressively slides the start state of <b>reasoning</b> from a demonstration&rsquo;s end to its beginning, facilitating easier model exploration at all stages. Thus, R$^3$ establishes a step-wise curriculum, allowing outcome supervision to offer step-level signals and precisely pinpoint errors. Using Llama2-7B, our method surpasses RL baseline on eight <b>reasoning</b> tasks by $4.1$ points on average. Notebaly, in program-based <b>reasoning</b> on GSM8K, it exceeds the baseline by $4.2$ points across three backbone models, and without any extra data, Codellama-7B + R$^3$ performs comparable to larger models or closed-source models.</p></p class="citation"></blockquote><h3 id=716--46248-integrating-llms-for-explainable-fault-diagnosis-in-complex-systems-akshay-j-dave-et-al-2024>(7/16 | 46/248) Integrating LLMs for Explainable Fault Diagnosis in Complex Systems (Akshay J. Dave et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Akshay J. Dave, Tat Nghia Nguyen, Richard B. Vilim. (2024)<br><strong>Integrating LLMs for Explainable Fault Diagnosis in Complex Systems</strong><br><button class=copy-to-clipboard title="Integrating LLMs for Explainable Fault Diagnosis in Complex Systems" index=46>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-46 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-LG, cs-SY, cs.AI, eess-SY<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.06695v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.06695v1.pdf filename=2402.06695v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper introduces an integrated system designed to enhance the explainability of fault diagnostics in complex systems, such as nuclear power plants, where operator understanding is critical for informed decision-making. By combining a physics-based diagnostic tool with a <b>Large</b> <b>Language</b> <b>Model,</b> we offer a novel solution that not only identifies faults but also provides clear, understandable explanations of their causes and implications. The system&rsquo;s efficacy is demonstrated through application to a molten salt facility, showcasing its ability to elucidate the connections between diagnosed faults and sensor data, answer operator queries, and evaluate historical sensor anomalies. Our approach underscores the importance of merging model-based diagnostics with advanced AI to improve the reliability and transparency of autonomous systems.</p></p class="citation"></blockquote><h3 id=816--47248-doing-experiments-and-revising-rules-with-natural-language-and-probabilistic-reasoning-top-piriyakulkij-et-al-2024>(8/16 | 47/248) Doing Experiments and Revising Rules with Natural Language and Probabilistic Reasoning (Top Piriyakulkij et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Top Piriyakulkij, Kevin Ellis. (2024)<br><strong>Doing Experiments and Revising Rules with Natural Language and Probabilistic Reasoning</strong><br><button class=copy-to-clipboard title="Doing Experiments and Revising Rules with Natural Language and Probabilistic Reasoning" index=47>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-47 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-CL, cs.AI<br>Keyword Score: 20<br>Keywords: Probabilistic Reasoning, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.06025v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.06025v1.pdf filename=2402.06025v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We build a computational model of how humans actively infer hidden rules by doing experiments. The basic principles behind the model is that, even if the rule is deterministic, the learner considers a broader space of fuzzy <b>probabilistic</b> <b>rules,</b> which it represents in natural language, and updates its hypotheses online after each experiment according to approximately Bayesian principles. In the same framework we also model experiment design according to information-theoretic criteria. We find that the combination of these three principles &ndash; explicit hypotheses, <b>probabilistic</b> <b>rules,</b> and online updates &ndash; can explain human performance on a Zendo-style task, and that removing any of these components leaves the model unable to account for the data.</p></p class="citation"></blockquote><h3 id=916--48248-limitations-of-agents-simulated-by-predictive-models-raymond-douglas-et-al-2024>(9/16 | 48/248) Limitations of Agents Simulated by Predictive Models (Raymond Douglas et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Raymond Douglas, Jacek Karwowski, Chan Bae, Andis Draguns, Victoria Krakovna. (2024)<br><strong>Limitations of Agents Simulated by Predictive Models</strong><br><button class=copy-to-clipboard title="Limitations of Agents Simulated by Predictive Models" index=48>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-48 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs.AI<br>Keyword Score: 20<br>Keywords: Fine-tuning, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05829v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05829v1.pdf filename=2402.05829v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>There is increasing focus on adapting predictive models into agent-like systems, most notably AI assistants based on language models. We outline two structural reasons for why these models can fail when turned into agents. First, we discuss auto-suggestive delusions. Prior work has shown theoretically that models fail to imitate agents that generated the training data if the agents relied on hidden observations: the hidden observations act as confounding variables, and the models treat actions they generate as evidence for nonexistent observations. Second, we introduce and formally study a related, novel limitation: predictor-policy incoherence. When a model generates a sequence of actions, the model&rsquo;s implicit prediction of the policy that generated those actions can serve as a confounding variable. The result is that models choose actions as if they expect future actions to be suboptimal, causing them to be overly conservative. We show that both of those failures are fixed by including a feedback loop from the environment, that is, re-training the models on their own actions. We give simple demonstrations of both limitations using Decision <b>Transformers</b> and confirm that empirical results agree with our conceptual and formal analysis. Our treatment provides a unifying view of those failure modes, and informs the question of why <b>fine-tuning</b> offline learned policies with online learning makes them more effective.</p></p class="citation"></blockquote><h3 id=1016--49248-prompting-fairness-artificial-intelligence-as-game-players-jazmia-henry-2024>(10/16 | 49/248) Prompting Fairness: Artificial Intelligence as Game Players (Jazmia Henry, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jazmia Henry. (2024)<br><strong>Prompting Fairness: Artificial Intelligence as Game Players</strong><br><button class=copy-to-clipboard title="Prompting Fairness: Artificial Intelligence as Game Players" index=49>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-49 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-GT, cs.AI<br>Keyword Score: 20<br>Keywords: Fairness, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05786v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05786v2.pdf filename=2402.05786v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Utilitarian games such as dictator games to measure <b>fairness</b> have been studied in the social sciences for decades. These games have given us insight into not only how humans view <b>fairness</b> but also in what conditions the frequency of <b>fairness,</b> altruism and greed increase or decrease. While these games have traditionally been focused on humans, the rise of AI gives us the ability to study how these models play these games. AI is becoming a constant in human interaction and examining how these models portray <b>fairness</b> in game play can give us some insight into how AI makes decisions. Over 101 rounds of the dictator game, I conclude that AI has a strong sense of <b>fairness</b> that is dependant of it it deems the person it is playing with as trustworthy, framing has a strong effect on how much AI gives a recipient when designated the trustee, and there may be evidence that AI experiences inequality aversion just as humans.</p></p class="citation"></blockquote><h3 id=1116--50248-an-interactive-agent-foundation-model-zane-durante-et-al-2024>(11/16 | 50/248) An Interactive Agent Foundation Model (Zane Durante et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zane Durante, Bidipta Sarkar, Ran Gong, Rohan Taori, Yusuke Noda, Paul Tang, Ehsan Adeli, Shrinidhi Kowshika Lakshmikanth, Kevin Schulman, Arnold Milstein, Demetri Terzopoulos, Ade Famoti, Noboru Kuno, Ashley Llorens, Hoi Vo, Katsu Ikeuchi, Li Fei-Fei, Jianfeng Gao, Naoki Wake, Qiuyuan Huang. (2024)<br><strong>An Interactive Agent Foundation Model</strong><br><button class=copy-to-clipboard title="An Interactive Agent Foundation Model" index=50>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-50 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-LG, cs-RO, cs.AI<br>Keyword Score: 16<br>Keywords: Foundation Model, Multi-modal, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05929v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05929v1.pdf filename=2402.05929v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The development of artificial intelligence systems is transitioning from creating static, task-specific models to dynamic, agent-based systems capable of performing well in a wide range of applications. We propose an Interactive Agent <b>Foundation</b> <b>Model</b> that uses a novel multi-task agent training paradigm for training AI agents across a wide range of domains, datasets, and tasks. Our training paradigm unifies diverse pre-training strategies, including visual masked auto-encoders, language modeling, and next-action prediction, enabling a versatile and adaptable AI framework. We demonstrate the performance of our framework across three separate domains &ndash; Robotics, Gaming AI, and Healthcare. Our model demonstrates its ability to generate meaningful and contextually relevant outputs in each area. The strength of our approach lies in its generality, leveraging a variety of data sources such as robotics sequences, gameplay data, large-scale video datasets, and textual information for effective <b>multimodal</b> and multi-task learning. Our approach provides a promising avenue for developing generalist, action-taking, <b>multimodal</b> systems.</p></p class="citation"></blockquote><h3 id=1216--51248-twig-towards-pre-hoc-hyperparameter-optimisation-and-cross-graph-generalisation-via-simulated-kge-models-jeffrey-sardina-et-al-2024>(12/16 | 51/248) TWIG: Towards pre-hoc Hyperparameter Optimisation and Cross-Graph Generalisation via Simulated KGE Models (Jeffrey Sardina et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jeffrey Sardina, John D. Kelleher, Declan O&rsquo;Sullivan. (2024)<br><strong>TWIG: Towards pre-hoc Hyperparameter Optimisation and Cross-Graph Generalisation via Simulated KGE Models</strong><br><button class=copy-to-clipboard title="TWIG: Towards pre-hoc Hyperparameter Optimisation and Cross-Graph Generalisation via Simulated KGE Models" index=51>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-51 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: 68R10, cs-AI, cs-LG, cs.AI<br>Keyword Score: 15<br>Keywords: Edge Embedding, Knowledge Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.06097v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.06097v1.pdf filename=2402.06097v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper we introduce TWIG (Topologically-Weighted Intelligence Generation), a novel, embedding-free paradigm for simulating the output of KGEs that uses a tiny fraction of the parameters. TWIG learns weights from inputs that consist of topological features of the graph data, with no coding for latent representations of entities or <b>edges.</b> <b>Our</b> experiments on the UMLS dataset show that a single TWIG neural network can predict the results of state-of-the-art ComplEx-N3 KGE model nearly exactly on across all hyperparameter configurations. To do this it uses a total of 2590 learnable parameters, but accurately predicts the results of 1215 different hyperparameter combinations with a combined cost of 29,322,000 parameters. Based on these results, we make two claims: 1) that KGEs do not learn latent semantics, but only latent representations of structural patterns; 2) that hyperparameter choice in KGEs is a deterministic function of the KGE model and graph structure. We further hypothesise that, as TWIG can simulate KGEs without embeddings, that node and <b>edge</b> <b>embeddings</b> are not needed to learn to accurately predict new facts in <b>KGs.</b> Finally, we formulate all of our findings under the umbrella of the <code>Structural Generalisation Hypothesis", which suggests that </code>twiggy" embedding-free / data-structure-based learning methods can allow a single neural network to simulate KGE performance, and perhaps solve the Link Prediction task, across many <b>KGs</b> from diverse domains and with different semantics.</p></p class="citation"></blockquote><h3 id=1316--52248-veni-vidi-vici-solving-the-myriad-of-challenges-before-knowledge-graph-learning-jeffrey-sardina-et-al-2024>(13/16 | 52/248) Veni, Vidi, Vici: Solving the Myriad of Challenges before Knowledge Graph Learning (Jeffrey Sardina et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jeffrey Sardina, Luca Costabello, Christophe Guéret. (2024)<br><strong>Veni, Vidi, Vici: Solving the Myriad of Challenges before Knowledge Graph Learning</strong><br><button class=copy-to-clipboard title="Veni, Vidi, Vici: Solving the Myriad of Challenges before Knowledge Graph Learning" index=52>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-52 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-LG, cs.AI<br>Keyword Score: 10<br>Keywords: Knowledge Graph, Knowledge Graph<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.06098v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.06098v1.pdf filename=2402.06098v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Knowledge</b> <b>Graphs</b> <b>(KGs)</b> have become increasingly common for representing large-scale linked data. However, their immense size has required graph learning systems to assist humans in analysis, interpretation, and pattern detection. While there have been promising results for researcher- and clinician- empowerment through a variety of <b>KG</b> learning systems, we identify four key deficiencies in state-of-the-art graph learning that simultaneously limit <b>KG</b> learning performance and diminish the ability of humans to interface optimally with these learning systems. These deficiencies are: 1) lack of expert <b>knowledge</b> <b>integration,</b> 2) instability to node degree extremity in the <b>KG,</b> 3) lack of consideration for uncertainty and relevance while learning, and 4) lack of explainability. Furthermore, we characterise state-of-the-art attempts to solve each of these problems and note that each attempt has largely been isolated from attempts to solve the other problems. Through a formalisation of these problems and a review of the literature that addresses them, we adopt the position that not only are deficiencies in these four key areas holding back human-KG empowerment, but that the divide-and-conquer approach to solving these problems as individual units rather than a whole is a significant barrier to the interface between humans and <b>KG</b> learning systems. We propose that it is only through integrated, holistic solutions to the limitations of <b>KG</b> learning systems that human and <b>KG</b> learning co-empowerment will be efficiently affected. We finally present our &ldquo;Veni, Vidi, Vici&rdquo; framework that sets a roadmap for effectively and efficiently shifting to a holistic co-empowerment model in both the <b>KG</b> learning and the broader machine learning domain.</p></p class="citation"></blockquote><h3 id=1416--53248-optimizing-delegation-in-collaborative-human-ai-hybrid-teams-andrew-fuchs-et-al-2024>(14/16 | 53/248) Optimizing Delegation in Collaborative Human-AI Hybrid Teams (Andrew Fuchs et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Andrew Fuchs, Andrea Passarella, Marco Conti. (2024)<br><strong>Optimizing Delegation in Collaborative Human-AI Hybrid Teams</strong><br><button class=copy-to-clipboard title="Optimizing Delegation in Collaborative Human-AI Hybrid Teams" index=53>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-53 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-HC, cs-LG, cs.AI<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05605v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05605v1.pdf filename=2402.05605v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>When humans and autonomous systems operate together as what we refer to as a hybrid team, we of course wish to ensure the team operates successfully and effectively. We refer to team members as agents. In our proposed framework, we address the case of hybrid teams in which, at any time, only one team member (the control agent) is authorized to act as control for the team. To determine the best selection of a control agent, we propose the addition of an AI manager (via <b>Reinforcement</b> <b>Learning)</b> which learns as an outside observer of the team. The manager learns a model of behavior linking observations of agent performance and the environment/world the team is operating in, and from these observations makes the most desirable selection of a control agent. We restrict the manager task by introducing a set of constraints. The manager constraints indicate acceptable team operation, so a violation occurs if the team enters a condition which is unacceptable and requires manager intervention. To ensure minimal added complexity or potential inefficiency for the team, the manager should attempt to minimize the number of times the team reaches a constraint violation and requires subsequent manager intervention. Therefore our manager is optimizing its selection of authorized agents to boost overall team performance while minimizing the frequency of manager intervention. We demonstrate our manager performance in a simulated driving scenario representing the case of a hybrid team of agents composed of a human driver and autonomous driving system. We perform experiments for our driving scenario with interfering vehicles, indicating the need for collision avoidance and proper speed control. Our results indicate a positive impact of our manager, with some cases resulting in increased team performance up to ~187% that of the best solo agent performance.</p></p class="citation"></blockquote><h3 id=1516--54248-kix-a-metacognitive-generalization-framework-arun-kumar-et-al-2024>(15/16 | 54/248) KIX: A Metacognitive Generalization Framework (Arun Kumar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Arun Kumar, Paul Schrater. (2024)<br><strong>KIX: A Metacognitive Generalization Framework</strong><br><button class=copy-to-clipboard title="KIX: A Metacognitive Generalization Framework" index=54>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-54 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.AI<br>Categories: cs-AI, cs-LG, cs-RO, cs.AI<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05346v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05346v1.pdf filename=2402.05346v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Humans and other animals aptly exhibit general intelligence behaviors in solving a variety of tasks with flexibility and ability to adapt to novel situations by reusing and applying high level knowledge acquired over time. But artificial agents are more of a specialist, lacking such generalist behaviors. Artificial agents will require understanding and exploiting critical structured knowledge representations. We present a metacognitive generalization framework, Knowledge-Interaction-eXecution (KIX), and argue that interactions with objects leveraging type space facilitate the learning of transferable interaction concepts and generalization. It is a natural way of integrating knowledge into <b>reinforcement</b> <b>learning</b> and promising to act as an enabler for autonomous and generalist behaviors in artificial intelligence systems.</p></p class="citation"></blockquote><h2 id=cslg-65>cs.LG (65)</h2><h3 id=065--55248-in-context-learning-can-re-learn-forbidden-tasks-sophie-xhonneux-et-al-2024>(0/65 | 55/248) In-Context Learning Can Re-learn Forbidden Tasks (Sophie Xhonneux et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sophie Xhonneux, David Dobre, Jian Tang, Gauthier Gidel, Dhanya Sridhar. (2024)<br><strong>In-Context Learning Can Re-learn Forbidden Tasks</strong><br><button class=copy-to-clipboard title="In-Context Learning Can Re-learn Forbidden Tasks" index=55>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-55 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CR, cs-LG, cs.LG<br>Keyword Score: 80<br>Keywords: Fine-tuning, Fine-tuning, In-context Learning, In-context Learning, In-context Learning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05723v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05723v1.pdf filename=2402.05723v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Despite significant investment into safety training, <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> deployed in the real world still suffer from numerous vulnerabilities. One perspective on <b>LLM</b> safety training is that it algorithmically forbids the model from answering toxic or harmful queries. To assess the effectiveness of safety training, in this work, we study forbidden tasks, i.e., tasks the model is designed to refuse to answer. Specifically, we investigate whether <b>in-context</b> <b>learning</b> <b>(ICL)</b> can be used to re-learn forbidden tasks despite the explicit <b>fine-tuning</b> of the model to refuse them. We first examine a toy example of refusing sentiment classification to demonstrate the problem. Then, we use <b>ICL</b> on a model <b>fine-tuned</b> to refuse to summarise made-up news articles. Finally, we investigate whether <b>ICL</b> can undo safety training, which could represent a major security risk. For the safety task, we look at Vicuna-7B, Starling-7B, and Llama2-7B. We show that the attack works out-of-the-box on Starling-7B and Vicuna-7B but fails on Llama2-7B. Finally, we propose an <b>ICL</b> attack that uses the chat template tokens like a <b>prompt</b> injection attack to achieve a better attack success rate on Vicuna-7B and Starling-7B. Trigger Warning: the appendix contains <b>LLM-generated</b> text with violence, suicide, and misinformation.</p></p class="citation"></blockquote><h3 id=165--56248-descriptive-kernel-convolution-network-with-improved-random-walk-kernel-meng-chieh-lee-et-al-2024>(1/65 | 56/248) Descriptive Kernel Convolution Network with Improved Random Walk Kernel (Meng-Chieh Lee et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Meng-Chieh Lee, Lingxiao Zhao, Leman Akoglu. (2024)<br><strong>Descriptive Kernel Convolution Network with Improved Random Walk Kernel</strong><br><button class=copy-to-clipboard title="Descriptive Kernel Convolution Network with Improved Random Walk Kernel" index=56>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-56 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 70<br>Keywords: Graph Convolutional Network, Graph Neural Network, Convolution, Convolutional Neural Network, Supervised Learning, Unsupervised Learning, Botnet Detection<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.06087v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.06087v1.pdf filename=2402.06087v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Graph kernels used to be the dominant approach to feature engineering for structured data, which are superseded by modern <b>GNNs</b> as the former lacks learnability. Recently, a suite of Kernel <b>Convolution</b> <b>Networks</b> (KCNs) successfully revitalized graph kernels by introducing learnability, which convolves input with learnable hidden graphs using a certain graph kernel. The random walk kernel (RWK) has been used as the default kernel in many KCNs, gaining increasing attention. In this paper, we first revisit the RWK and its current usage in KCNs, revealing several shortcomings of the existing designs, and propose an improved graph kernel RWK+, by introducing color-matching random walks and deriving its efficient computation. We then propose RWK+CN, a KCN that uses RWK+ as the core kernel to learn descriptive graph features with an <b>unsupervised</b> objective, which can not be achieved by <b>GNNs.</b> Further, by unrolling RWK+, we discover its connection with a regular <b>GCN</b> layer, and propose a novel <b>GNN</b> layer RWK+Conv. In the first part of experiments, we demonstrate the descriptive learning ability of RWK+CN with the improved random walk kernel RWK+ on <b>unsupervised</b> pattern mining tasks; in the second part, we show the effectiveness of RWK+ for a variety of KCN architectures and <b>supervised</b> graph learning tasks, and demonstrate the expressiveness of RWK+Conv layer, especially on the graph-level tasks. RWK+ and RWK+Conv adapt to various real-world applications, including web applications such as <b>bot</b> <b>detection</b> in a web-scale Twitter social network, and community classification in Reddit social interaction networks.</p></p class="citation"></blockquote><h3 id=265--57248-classifying-nodes-in-graphs-without-gnns-daniel-winter-et-al-2024>(2/65 | 57/248) Classifying Nodes in Graphs without GNNs (Daniel Winter et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Daniel Winter, Niv Cohen, Yedid Hoshen. (2024)<br><strong>Classifying Nodes in Graphs without GNNs</strong><br><button class=copy-to-clipboard title="Classifying Nodes in Graphs without GNNs" index=57>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-57 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs-SI, cs.LG<br>Keyword Score: 63<br>Keywords: Message-Passing, Node Classification, Graph Neural Network, Graph Neural Network, Benchmarking, Knowledge Distillation, Stemming<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05934v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05934v1.pdf filename=2402.05934v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Graph</b> <b>neural</b> <b>networks</b> <b>(GNNs)</b> are the dominant paradigm for classifying <b>nodes</b> <b>in</b> a <b>graph,</b> <b>but</b> <b>they</b> have several undesirable attributes <b>stemming</b> from their message passing architecture. Recently, <b>distillation</b> methods succeeded in eliminating the use of <b>GNNs</b> at test time but they still require them during training. We perform a careful analysis of the role that <b>GNNs</b> play in <b>distillation</b> methods. This analysis leads us to propose a fully <b>GNN-free</b> approach for <b>node</b> <b>classification,</b> not requiring them at train or test time. Our method consists of three key components: smoothness constraints, pseudo-labeling iterations and neighborhood-label histograms. Our final approach can match the state-of-the-art accuracy on standard popular <b>benchmarks</b> such as citation and co-purchase networks, without training a <b>GNN.</b></p></p class="citation"></blockquote><h3 id=365--58248-limits-of-transformer-language-models-on-learning-algorithmic-compositions-jonathan-thomm-et-al-2024>(3/65 | 58/248) Limits of Transformer Language Models on Learning Algorithmic Compositions (Jonathan Thomm et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jonathan Thomm, Aleksandar Terzic, Geethan Karunaratne, Giacomo Camposampiero, Bernhard Schölkopf, Abbas Rahimi. (2024)<br><strong>Limits of Transformer Language Models on Learning Algorithmic Compositions</strong><br><button class=copy-to-clipboard title="Limits of Transformer Language Models on Learning Algorithmic Compositions" index=58>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-58 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CL, cs-LG, cs.LG<br>Keyword Score: 60<br>Keywords: GPT, GPT-4, Gemini, LLaMA, Transformer, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05785v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05785v2.pdf filename=2402.05785v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We analyze the capabilities of <b>Transformer</b> language models on learning discrete algorithms. To this end, we introduce two new tasks demanding the composition of several discrete sub-tasks. On both training <b>LLaMA</b> models from scratch and <b>prompting</b> on <b>GPT-4</b> and <b>Gemini</b> we measure learning compositions of learned primitives. We observe that the compositional capabilities of state-of-the-art <b>Transformer</b> language models are very limited and sample-wise scale worse than relearning all sub-tasks for a new algorithmic composition. We also present a theorem in complexity theory, showing that gradient descent on memorizing feedforward models can be exponentially data inefficient.</p></p class="citation"></blockquote><h3 id=465--59248-game-theoretic-counterfactual-explanation-for-graph-neural-networks-chirag-chhablani-et-al-2024>(4/65 | 59/248) Game-theoretic Counterfactual Explanation for Graph Neural Networks (Chirag Chhablani et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chirag Chhablani, Sarthak Jain, Akshay Channesh, Ian A. Kash, Sourav Medya. (2024)<br><strong>Game-theoretic Counterfactual Explanation for Graph Neural Networks</strong><br><button class=copy-to-clipboard title="Game-theoretic Counterfactual Explanation for Graph Neural Networks" index=59>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-59 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 50<br>Keywords: Node Classification, Graph Neural Network, Graph Neural Network, Counter-factual, Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.06030v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.06030v1.pdf filename=2402.06030v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Graph</b> <b>Neural</b> <b>Networks</b> <b>(GNNs)</b> have been a powerful tool for <b>node</b> <b>classification</b> tasks in complex networks. However, their decision-making processes remain a black-box to users, making it challenging to understand the <b>reasoning</b> behind their predictions. <b>Counterfactual</b> explanations (CFE) have shown promise in enhancing the interpretability of machine learning models. Prior approaches to compute CFE for <b>GNNS</b> often are learning-based approaches that require training additional <b>graphs.</b> <b>In</b> <b>this</b> paper, we propose a semivalue-based, non-learning approach to generate CFE for <b>node</b> <b>classification</b> tasks, eliminating the need for any additional training. Our results reveals that computing Banzhaf values requires lower sample complexity in identifying the <b>counterfactual</b> explanations compared to other popular methods such as computing Shapley values. Our empirical evidence indicates computing Banzhaf values can achieve up to a fourfold speed up compared to Shapley values. We also design a thresholding method for computing Banzhaf values and show theoretical and empirical results on its robustness in noisy environments, making it superior to Shapley values. Furthermore, the thresholded Banzhaf values are shown to enhance efficiency without compromising the quality (i.e., fidelity) in the explanations in three popular graph datasets.</p></p class="citation"></blockquote><h3 id=565--60248-on-the-convergence-of-zeroth-order-federated-tuning-in-large-language-models-zhenqing-ling-et-al-2024>(5/65 | 60/248) On the Convergence of Zeroth-Order Federated Tuning in Large Language Models (Zhenqing Ling et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhenqing Ling, Daoyuan Chen, Liuyi Yao, Yaliang Li, Ying Shen. (2024)<br><strong>On the Convergence of Zeroth-Order Federated Tuning in Large Language Models</strong><br><button class=copy-to-clipboard title="On the Convergence of Zeroth-Order Federated Tuning in Large Language Models" index=60>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-60 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CL, cs-LG, cs.LG<br>Keyword Score: 50<br>Keywords: Federated Learning, Fine-tuning, Stochastic Gradient Descent, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05926v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05926v1.pdf filename=2402.05926v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The confluence of <b>Federated</b> <b>Learning</b> (FL) and <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> is ushering in a new era in privacy-preserving natural language processing. However, the intensive memory requirements for <b>fine-tuning</b> <b>LLMs</b> pose significant challenges, especially when deploying on edge devices with limited computational resources. To circumvent this, we explore the novel integration of Memory-efficient Zeroth-Order Optimization within a <b>federated</b> <b>setting,</b> a synergy we denote as FedMeZO. Our study is the first to examine the theoretical underpinnings of FedMeZO in the context of <b>LLMs,</b> tackling key questions regarding the influence of <b>large</b> <b>parameter</b> <b>spaces</b> on optimization behavior, the establishment of convergence properties, and the identification of critical parameters for convergence to inform personalized <b>federated</b> <b>strategies.</b> Our extensive empirical evidence supports the theory, showing that FedMeZO not only converges faster than traditional first-order methods such as <b>SGD</b> but also significantly reduces GPU memory usage during training to levels comparable to those during inference. Moreover, the proposed personalized FL strategy that is built upon the theoretical insights to customize the client-wise learning rate can effectively accelerate loss reduction. We hope our work can help to bridge theoretical and practical aspects of <b>federated</b> <b>fine-tuning</b> for <b>LLMs</b> and facilitate further development and research.</p></p class="citation"></blockquote><h3 id=665--61248-offline-actor-critic-reinforcement-learning-scales-to-large-models-jost-tobias-springenberg-et-al-2024>(6/65 | 61/248) Offline Actor-Critic Reinforcement Learning Scales to Large Models (Jost Tobias Springenberg et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jost Tobias Springenberg, Abbas Abdolmaleki, Jingwei Zhang, Oliver Groth, Michael Bloesch, Thomas Lampe, Philemon Brakel, Sarah Bechtle, Steven Kapturowski, Roland Hafner, Nicolas Heess, Martin Riedmiller. (2024)<br><strong>Offline Actor-Critic Reinforcement Learning Scales to Large Models</strong><br><button class=copy-to-clipboard title="Offline Actor-Critic Reinforcement Learning Scales to Large Models" index=61>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-61 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs-RO, cs.LG<br>Keyword Score: 50<br>Keywords: Reinforcement Learning, Supervised Learning, Supervised Learning, Transformer, Scaling Law<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05546v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05546v1.pdf filename=2402.05546v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We show that offline actor-critic <b>reinforcement</b> <b>learning</b> can scale to large models - such as <b>transformers</b> - and follows similar <b>scaling</b> <b>laws</b> as <b>supervised</b> <b>learning.</b> We find that offline actor-critic algorithms can outperform strong, <b>supervised,</b> <b>behavioral</b> cloning baselines for multi-task training on a large dataset containing both sub-optimal and expert behavior on 132 continuous control tasks. We introduce a Perceiver-based actor-critic model and elucidate the key model features needed to make offline RL work with self- and cross-attention modules. Overall, we find that: i) simple offline actor critic algorithms are a natural choice for gradually moving away from the currently predominant paradigm of behavioral cloning, and ii) via offline RL it is possible to learn multi-task policies that master many domains simultaneously, including real robotics tasks, from sub-optimal demonstrations or self-generated data.</p></p class="citation"></blockquote><h3 id=765--62248-accurate-lora-finetuning-quantization-of-llms-via-information-retention-haotong-qin-et-al-2024>(7/65 | 62/248) Accurate LoRA-Finetuning Quantization of LLMs via Information Retention (Haotong Qin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haotong Qin, Xudong Ma, Xingyu Zheng, Xiaoyang Li, Yang Zhang, Shouda Liu, Jie Luo, Xianglong Liu, Michele Magno. (2024)<br><strong>Accurate LoRA-Finetuning Quantization of LLMs via Information Retention</strong><br><button class=copy-to-clipboard title="Accurate LoRA-Finetuning Quantization of LLMs via Information Retention" index=62>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-62 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CL, cs-LG, cs.LG<br>Keyword Score: 50<br>Keywords: Fine-tuning, Quantization, Quantization, LLaMA, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05445v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05445v1.pdf filename=2402.05445v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The LoRA-finetuning <b>quantization</b> of <b>LLMs</b> has been extensively studied to obtain accurate yet compact <b>LLMs</b> for deployment on resource-constrained hardware. However, existing methods cause the <b>quantized</b> <b>LLM</b> to severely degrade and even fail to benefit from the <b>finetuning</b> of LoRA. This paper proposes a novel IR-QLoRA for pushing <b>quantized</b> <b>LLMs</b> with LoRA to be highly accurate through information retention. The proposed IR-QLoRA mainly relies on two technologies derived from the perspective of unified information: (1) statistics-based Information Calibration <b>Quantization</b> allows the <b>quantized</b> parameters of <b>LLM</b> to retain original information accurately; (2) <b>finetuning-based</b> Information Elastic Connection makes LoRA utilizes elastic representation transformation with diverse information. Comprehensive experiments show that IR-QLoRA can significantly improve accuracy across <b>LLaMA</b> and LLaMA2 families under 2-4 bit-widths, e.g., 4- bit <b>LLaMA-7B</b> achieves 1.4% improvement on MMLU compared with the state-of-the-art methods. The significant performance gain requires only a tiny 0.31% additional time consumption, revealing the satisfactory efficiency of our IRQLoRA. We highlight that IR-QLoRA enjoys excellent versatility, compatible with various frameworks (e.g., NormalFloat and Integer <b>quantization)</b> and brings general accuracy gains. The code is available at <a href=https://github.com/htqin/ir-qlora>https://github.com/htqin/ir-qlora</a>.</p></p class="citation"></blockquote><h3 id=865--63248-let-your-graph-do-the-talking-encoding-structured-data-for-llms-bryan-perozzi-et-al-2024>(8/65 | 63/248) Let Your Graph Do the Talking: Encoding Structured Data for LLMs (Bryan Perozzi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bryan Perozzi, Bahare Fatemi, Dustin Zelle, Anton Tsitsulin, Mehran Kazemi, Rami Al-Rfou, Jonathan Halcrow. (2024)<br><strong>Let Your Graph Do the Talking: Encoding Structured Data for LLMs</strong><br><button class=copy-to-clipboard title="Let Your Graph Do the Talking: Encoding Structured Data for LLMs" index=63>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-63 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: I-5-1; I-2-6; I-2-7, cs-AI, cs-LG, cs-SI, cs.LG, stat-ML<br>Keyword Score: 48<br>Keywords: Benchmarking, Knowledge Graph, Reasoning, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05862v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05862v1.pdf filename=2402.05862v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>How can we best encode structured data into sequential form for use in <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)?</b> In this work, we introduce a parameter-efficient method to explicitly represent structured data for <b>LLMs.</b> Our method, GraphToken, learns an encoding function to extend <b>prompts</b> with explicit structured information. Unlike other work which focuses on limited domains (e.g. <b>knowledge</b> <b>graph</b> representation), our work is the first effort focused on the general encoding of structured data to be used for various <b>reasoning</b> tasks. We show that explicitly representing the graph structure allows significant improvements to graph <b>reasoning</b> tasks. Specifically, we see across the board improvements - up to 73% points - on node, edge and, graph-level tasks from the GraphQA <b>benchmark.</b></p></p class="citation"></blockquote><h3 id=965--64248-empowering-machine-learning-models-with-contextual-knowledge-for-enhancing-the-detection-of-eating-disorders-in-social-media-posts-josé-alberto-benítez-andrades-et-al-2024>(9/65 | 64/248) Empowering machine learning models with contextual knowledge for enhancing the detection of eating disorders in social media posts (José Alberto Benítez-Andrades et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>José Alberto Benítez-Andrades, María Teresa García-Ordás, Mayra Russo, Ahmad Sakor, Luis Daniel Fernandes Rotger, Maria-Esther Vidal. (2024)<br><strong>Empowering machine learning models with contextual knowledge for enhancing the detection of eating disorders in social media posts</strong><br><button class=copy-to-clipboard title="Empowering machine learning models with contextual knowledge for enhancing the detection of eating disorders in social media posts" index=64>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-64 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CL, cs-LG, cs.LG<br>Keyword Score: 45<br>Keywords: Graph Embedding, Knowledge Graph, BERT, falcon, Word Embedding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05536v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05536v1.pdf filename=2402.05536v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Social networks are vital for information sharing, especially in the health sector for discussing diseases and treatments. These platforms, however, often feature posts as brief texts, posing challenges for Artificial Intelligence (AI) in understanding context. We introduce a novel hybrid approach combining community-maintained <b>knowledge</b> <b>graphs</b> <b>(like</b> Wikidata) with deep learning to enhance the categorization of social media posts. This method uses advanced entity recognizers and linkers (like <b>Falcon</b> 2.0) to connect short post entities to <b>knowledge</b> <b>graphs.</b> <b>Knowledge</b> <b>graph</b> <b>embeddings</b> (KGEs) and contextualized <b>word</b> <b>embeddings</b> (like <b>BERT)</b> are then employed to create rich, context-based representations of these posts. Our focus is on the health domain, particularly in identifying posts related to eating disorders (e.g., anorexia, bulimia) to aid healthcare providers in early diagnosis. We tested our approach on a dataset of 2,000 tweets about eating disorders, finding that merging <b>word</b> <b>embeddings</b> with <b>knowledge</b> <b>graph</b> <b>information</b> enhances the predictive models&rsquo; reliability. This methodology aims to assist health experts in spotting patterns indicative of mental disorders, thereby improving early detection and accurate diagnosis for personalized medicine.</p></p class="citation"></blockquote><h3 id=1065--65248-scaling-artificial-intelligence-for-digital-wargaming-in-support-of-decision-making-scotty-black-et-al-2024>(10/65 | 65/248) Scaling Artificial Intelligence for Digital Wargaming in Support of Decision-Making (Scotty Black et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Scotty Black, Christian Darken. (2024)<br><strong>Scaling Artificial Intelligence for Digital Wargaming in Support of Decision-Making</strong><br><button class=copy-to-clipboard title="Scaling Artificial Intelligence for Digital Wargaming in Support of Decision-Making" index=65>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-65 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 40<br>Keywords: Recommendation, Reinforcement Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.06075v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.06075v1.pdf filename=2402.06075v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this unprecedented era of technology-driven transformation, it becomes more critical than ever that we aggressively invest in developing robust artificial intelligence (AI) for wargaming in support of decision-making. By advancing AI-enabled systems and pairing these with human judgment, we will be able to enhance all-domain awareness, improve the speed and quality of our decision cycles, offer <b>recommendations</b> for novel courses of action, and more rapidly counter our adversary&rsquo;s actions. It therefore becomes imperative that we accelerate the development of AI to help us better address the complexity of modern challenges and dilemmas that currently requires human intelligence and, if possible, attempt to surpass human intelligence&ndash;not to replace humans, but to augment and better inform human decision-making at machine speed. Although deep <b>reinforcement</b> <b>learning</b> continues to show promising results in intelligent agent behavior development for the long-horizon, complex tasks typically found in combat modeling and <b>simulation,</b> further research is needed to enable the scaling of AI to deal with these intricate and expansive state-spaces characteristic of wargaming for either concept development, education, or analysis. To help address this challenge, in our research, we are developing and implementing a hierarchical <b>reinforcement</b> <b>learning</b> framework that includes a multi-model approach and dimension-invariant observation abstractions.</p></p class="citation"></blockquote><h3 id=1165--66248-federated-offline-reinforcement-learning-collaborative-single-policy-coverage-suffices-jiin-woo-et-al-2024>(11/65 | 66/248) Federated Offline Reinforcement Learning: Collaborative Single-Policy Coverage Suffices (Jiin Woo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiin Woo, Laixi Shi, Gauri Joshi, Yuejie Chi. (2024)<br><strong>Federated Offline Reinforcement Learning: Collaborative Single-Policy Coverage Suffices</strong><br><button class=copy-to-clipboard title="Federated Offline Reinforcement Learning: Collaborative Single-Policy Coverage Suffices" index=66>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-66 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs-MA, cs.LG, stat-ML<br>Keyword Score: 40<br>Keywords: Federated Learning, Markov Decision Process, Offline Reinforcement Learning, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05876v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05876v1.pdf filename=2402.05876v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Offline</b> <b>reinforcement</b> <b>learning</b> (RL), which seeks to learn an optimal policy using <b>offline</b> <b>data,</b> <b>has</b> garnered significant interest due to its potential in critical applications where online data collection is infeasible or expensive. This work explores the benefit of <b>federated</b> <b>learning</b> for <b>offline</b> <b>RL,</b> <b>aiming</b> at collaboratively leveraging <b>offline</b> <b>datasets</b> <b>at</b> multiple agents. Focusing on finite-horizon episodic tabular Markov decision processes <b>(MDPs),</b> we design FedLCB-Q, a variant of the popular model-free Q-learning algorithm tailored for <b>federated</b> <b>offline</b> <b>RL.</b> <b>FedLCB-Q</b> updates local Q-functions at agents with novel learning rate schedules and aggregates them at a central server using importance averaging and a carefully designed pessimistic penalty term. Our sample complexity analysis reveals that, with appropriately chosen parameters and synchronization schedules, FedLCB-Q achieves linear speedup in terms of the number of agents without requiring high-quality datasets at individual agents, as long as the local datasets collectively cover the state-action space visited by the optimal policy, highlighting the power of collaboration in the <b>federated</b> <b>setting.</b> In fact, the sample complexity almost matches that of the single-agent counterpart, as if all the data are stored at a central location, up to polynomial factors of the horizon length. Furthermore, FedLCB-Q is communication-efficient, where the number of communication rounds is only linear with respect to the horizon length up to logarithmic factors.</p></p class="citation"></blockquote><h3 id=1265--67248-mesoscale-traffic-forecasting-for-real-time-bottleneck-and-shockwave-prediction-raphael-chekroun-et-al-2024>(12/65 | 67/248) Mesoscale Traffic Forecasting for Real-Time Bottleneck and Shockwave Prediction (Raphael Chekroun et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Raphael Chekroun, Han Wang, Jonathan Lee, Marin Toromanoff, Sascha Hornauer, Fabien Moutarde, Maria Laura Delle Monache. (2024)<br><strong>Mesoscale Traffic Forecasting for Real-Time Bottleneck and Shockwave Prediction</strong><br><button class=copy-to-clipboard title="Mesoscale Traffic Forecasting for Real-Time Bottleneck and Shockwave Prediction" index=67>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-67 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs-RO, cs.LG<br>Keyword Score: 40<br>Keywords: LSTM, LSTM, LSTM, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05663v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05663v1.pdf filename=2402.05663v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Accurate real-time traffic state forecasting plays a pivotal role in traffic control research. In particular, the CIRCLES consortium project necessitates predictive techniques to mitigate the impact of data source delays. After the success of the MegaVanderTest experiment, this paper aims at overcoming the current system limitations and develop a more suited approach to improve the real-time traffic state estimation for the next iterations of the experiment. In this paper, we introduce the SA-LSTM, a deep forecasting method integrating <b>Self-Attention</b> (SA) on the spatial dimension with <b>Long</b> <b>Short-Term</b> <b>Memory</b> <b>(LSTM)</b> yielding state-of-the-art results in real-time mesoscale traffic forecasting. We extend this approach to multi-step forecasting with the n-step SA-LSTM, which outperforms traditional multi-step forecasting methods in the trade-off between short-term and <b>long-term</b> <b>predictions,</b> <b>all</b> <b>while</b> operating in real-time.</p></p class="citation"></blockquote><h3 id=1365--68248-rethinking-propagation-for-unsupervised-graph-domain-adaptation-meihan-liu-et-al-2024>(13/65 | 68/248) Rethinking Propagation for Unsupervised Graph Domain Adaptation (Meihan Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Meihan Liu, Zeyu Fang, Zhen Zhang, Ming Gu, Sheng Zhou, Xin Wang, Jiajun Bu. (2024)<br><strong>Rethinking Propagation for Unsupervised Graph Domain Adaptation</strong><br><button class=copy-to-clipboard title="Rethinking Propagation for Unsupervised Graph Domain Adaptation" index=68>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-68 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 40<br>Keywords: Graph Neural Network, Graph Neural Network, Unsupervised Learning, Domain Adaptation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05660v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05660v1.pdf filename=2402.05660v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Unsupervised</b> <b>Graph</b> <b>Domain</b> <b>Adaptation</b> (UGDA) aims to transfer knowledge from a labelled source <b>graph</b> <b>to</b> <b>an</b> unlabelled target <b>graph</b> <b>in</b> <b>order</b> to address the distribution shifts between <b>graph</b> <b>domains.</b> <b>Previous</b> works have primarily focused on aligning data from the source and target <b>graph</b> <b>in</b> <b>the</b> representation space learned by <b>graph</b> <b>neural</b> <b>networks</b> <b>(GNNs).</b> However, the inherent generalization capability of <b>GNNs</b> has been largely overlooked. Motivated by our empirical analysis, we reevaluate the role of <b>GNNs</b> in <b>graph</b> <b>domain</b> <b>adaptation</b> and uncover the pivotal role of the propagation process in <b>GNNs</b> for adapting to different <b>graph</b> <b>domains.</b> <b>We</b> provide a comprehensive theoretical analysis of UGDA and derive a generalization bound for multi-layer <b>GNNs.</b> By formulating <b>GNN</b> Lipschitz for k-layer <b>GNNs,</b> we show that the target risk bound can be tighter by removing propagation layers in source <b>graph</b> <b>and</b> <b>stacking</b> multiple propagation layers in target <b>graph.</b> <b>Based</b> <b>on</b> the empirical and theoretical analysis mentioned above, we propose a simple yet effective approach called A2GNN for <b>graph</b> <b>domain</b> <b>adaptation.</b> Through extensive experiments on real-world datasets, we demonstrate the effectiveness of our proposed A2GNN framework.</p></p class="citation"></blockquote><h3 id=1465--69248-reinforcement-learning-as-a-catalyst-for-robust-and-fair-federated-learning-deciphering-the-dynamics-of-client-contributions-jialuo-he-et-al-2024>(14/65 | 69/248) Reinforcement Learning as a Catalyst for Robust and Fair Federated Learning: Deciphering the Dynamics of Client Contributions (Jialuo He et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jialuo He, Wei Chen, Xiaojin Zhang. (2024)<br><strong>Reinforcement Learning as a Catalyst for Robust and Fair Federated Learning: Deciphering the Dynamics of Client Contributions</strong><br><button class=copy-to-clipboard title="Reinforcement Learning as a Catalyst for Robust and Fair Federated Learning: Deciphering the Dynamics of Client Contributions" index=69>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-69 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-DC, cs-LG, cs.LG<br>Keyword Score: 40<br>Keywords: Fairness, Federated Learning, Reinforcement Learning, Adversarial Attack<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05541v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05541v1.pdf filename=2402.05541v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advancements in <b>federated</b> <b>learning</b> (FL) have produced models that retain user privacy by training across multiple decentralized devices or systems holding local data samples. However, these strategies often neglect the inherent challenges of statistical heterogeneity and vulnerability to <b>adversarial</b> <b>attacks,</b> which can degrade model robustness and <b>fairness.</b> Personalized FL strategies offer some respite by adjusting models to fit individual client profiles, yet they tend to neglect server-side aggregation vulnerabilities. To address these issues, we propose <b>Reinforcement</b> <b>Federated</b> <b>Learning</b> (RFL), a novel framework that leverages deep <b>reinforcement</b> <b>learning</b> to adaptively optimize client contribution during aggregation, thereby enhancing both model robustness against malicious clients and <b>fairness</b> across participants under non-identically distributed settings. To achieve this goal, we propose a meticulous approach involving a Deep Deterministic Policy Gradient-based algorithm for continuous control of aggregation weights, an innovative client selection method based on model parameter distances, and a reward mechanism guided by validation set performance. Empirically, extensive experiments demonstrate that, in terms of robustness, RFL outperforms the state-of-the-art methods, while maintaining comparable levels of <b>fairness,</b> offering a promising solution to build resilient and fair <b>federated</b> <b>systems.</b></p></p class="citation"></blockquote><h3 id=1565--70248-learning-to-route-among-specialized-experts-for-zero-shot-generalization-mohammed-muqeeth-et-al-2024>(15/65 | 70/248) Learning to Route Among Specialized Experts for Zero-Shot Generalization (Mohammed Muqeeth et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mohammed Muqeeth, Haokun Liu, Yufan Liu, Colin Raffel. (2024)<br><strong>Learning to Route Among Specialized Experts for Zero-Shot Generalization</strong><br><button class=copy-to-clipboard title="Learning to Route Among Specialized Experts for Zero-Shot Generalization" index=70>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-70 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 33<br>Keywords: Graph Attention Networks, Benchmarking, Fine-tuning, Zero-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05859v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05859v1.pdf filename=2402.05859v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recently, there has been a widespread proliferation of &ldquo;expert&rdquo; language models that are specialized to a specific task or domain through parameter-efficient <b>fine-tuning.</b> How can we recycle large collections of expert language models to improve <b>zero-shot</b> generalization to unseen tasks? In this work, we propose Post-Hoc Adaptive Tokenwise <b>Gating</b> Over an Ocean of Specialized Experts (PHATGOOSE), which learns to route among specialized modules that were produced through parameter-efficient <b>fine-tuning.</b> Unlike past methods that learn to route among specialized models, PHATGOOSE explores the possibility that <b>zero-shot</b> generalization will be improved if different experts can be adaptively chosen for each token and at each layer in the model. Crucially, our method is post-hoc - it does not require simultaneous access to the datasets used to create the specialized models and only requires a modest amount of additional compute after each expert model is trained. In experiments covering a range of specialized model collections and <b>zero-shot</b> generalization <b>benchmarks,</b> we find that PHATGOOSE outperforms past methods for post-hoc routing and, in some cases, outperforms explicit multitask training (which requires simultaneous data access). To better understand the routing strategy learned by PHATGOOSE, we perform qualitative experiments to validate that PHATGOOSE&rsquo;s performance stems from its ability to make adaptive per-token and per-module expert choices. We release all of our code to support future work on improving <b>zero-shot</b> generalization by recycling specialized experts.</p></p class="citation"></blockquote><h3 id=1665--71248-sparse-vq-transformer-an-ffn-free-framework-with-vector-quantization-for-enhanced-time-series-forecasting-yanjun-zhao-et-al-2024>(16/65 | 71/248) Sparse-VQ Transformer: An FFN-Free Framework with Vector Quantization for Enhanced Time Series Forecasting (Yanjun Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yanjun Zhao, Tian Zhou, Chao Chen, Liang Sun, Yi Qian, Rong Jin. (2024)<br><strong>Sparse-VQ Transformer: An FFN-Free Framework with Vector Quantization for Enhanced Time Series Forecasting</strong><br><button class=copy-to-clipboard title="Sparse-VQ Transformer: An FFN-Free Framework with Vector Quantization for Enhanced Time Series Forecasting" index=71>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-71 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 33<br>Keywords: Benchmarking, Quantization, Quantization, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05830v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05830v1.pdf filename=2402.05830v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Time series analysis is vital for numerous applications, and <b>transformers</b> have become increasingly prominent in this domain. Leading methods customize the <b>transformer</b> architecture from NLP and CV, utilizing a patching technique to convert continuous signals into segments. Yet, time series data are uniquely challenging due to significant distribution shifts and intrinsic noise levels. To address these two challenges,we introduce the Sparse Vector <b>Quantized</b> FFN-Free <b>Transformer</b> (Sparse-VQ). Our methodology capitalizes on a sparse vector <b>quantization</b> technique coupled with Reverse Instance Normalization (RevIN) to reduce noise impact and capture sufficient statistics for forecasting, serving as an alternative to the Feed-Forward layer (FFN) in the <b>transformer</b> architecture. Our FFN-free approach trims the parameter count, enhancing computational efficiency and reducing overfitting. Through evaluations across ten <b>benchmark</b> datasets, including the newly introduced CAISO dataset, Sparse-VQ surpasses leading models with a 7.84% and 4.17% decrease in MAE for univariate and multivariate time series forecasting, respectively. Moreover, it can be seamlessly integrated with existing <b>transformer-based</b> models to elevate their performance.</p></p class="citation"></blockquote><h3 id=1765--72248-repquant-towards-accurate-post-training-quantization-of-large-transformer-models-via-scale-reparameterization-zhikai-li-et-al-2024>(17/65 | 72/248) RepQuant: Towards Accurate Post-Training Quantization of Large Transformer Models via Scale Reparameterization (Zhikai Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhikai Li, Xuewen Liu, Jing Zhang, Qingyi Gu. (2024)<br><strong>RepQuant: Towards Accurate Post-Training Quantization of Large Transformer Models via Scale Reparameterization</strong><br><button class=copy-to-clipboard title="RepQuant: Towards Accurate Post-Training Quantization of Large Transformer Models via Scale Reparameterization" index=72>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-72 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 33<br>Keywords: Multi-modal, Quantization, Quantization, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05628v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05628v1.pdf filename=2402.05628v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Large <b>transformer</b> models have demonstrated remarkable success. Post-training <b>quantization</b> (PTQ), which requires only a small dataset for calibration and avoids end-to-end retraining, is a promising solution for compressing these large models. Regrettably, existing PTQ methods typically exhibit non-trivial performance loss. We find that the performance bottleneck stems from over-consideration of hardware compatibility in the <b>quantization</b> process, compelling them to reluctantly employ simple quantizers, albeit at the expense of accuracy. With the above insights, we propose RepQuant, a novel PTQ framework with <b>quantization-inference</b> decoupling paradigm to address the above issues. RepQuant employs complex quantizers in the <b>quantization</b> process and simplified quantizers in the inference process, and performs mathematically equivalent transformations between the two through <b>quantization</b> scale reparameterization, thus ensuring both accurate <b>quantization</b> and efficient inference. More specifically, we focus on two components with extreme distributions: LayerNorm activations and Softmax activations. Initially, we apply channel-wise <b>quantization</b> and log$\sqrt{2}$ <b>quantization,</b> respectively, which are tailored to their distributions. In particular, for the former, we introduce a learnable per-channel dual clipping scheme, which is designed to efficiently identify outliers in the unbalanced activations with fine granularity. Then, we reparameterize the scales to hardware-friendly layer-wise <b>quantization</b> and log2 <b>quantization</b> for inference. Moreover, <b>quantized</b> weight reconstruction is seamlessly integrated into the above procedure to further push the performance limits. Extensive experiments are performed on different large-scale <b>transformer</b> variants on multiple tasks, including vision, language, and <b>multi-modal</b> <b>transformers,</b> and RepQuant encouragingly demonstrates significant performance advantages.</p></p class="citation"></blockquote><h3 id=1865--73248-hypergraph-node-classification-with-graph-neural-networks-bohan-tang-et-al-2024>(18/65 | 73/248) Hypergraph Node Classification With Graph Neural Networks (Bohan Tang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bohan Tang, Zexi Liu, Keyue Jiang, Siheng Chen, Xiaowen Dong. (2024)<br><strong>Hypergraph Node Classification With Graph Neural Networks</strong><br><button class=copy-to-clipboard title="Hypergraph Node Classification With Graph Neural Networks" index=73>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-73 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG, eess-SP, stat-ML<br>Keyword Score: 33<br>Keywords: Node Classification, Graph Neural Network, Graph Neural Network, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05569v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05569v1.pdf filename=2402.05569v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Hypergraphs, with hyperedges connecting more than two <b>nodes,</b> <b>are</b> key for modelling higher-order interactions in real-world data. The success of <b>graph</b> <b>neural</b> <b>networks</b> <b>(GNNs)</b> reveals the capability of neural networks to process data with pairwise interactions. This inspires the usage of neural networks for data with higher-order interactions, thereby leading to the development of hypergraph neural networks (HyperGNNs). <b>GNNs</b> and HyperGNNs are typically considered distinct since they are designed for data on different geometric topologies. However, in this paper, we theoretically demonstrate that, in the context of <b>node</b> <b>classification,</b> most HyperGNNs can be approximated using a <b>GNN</b> with a weighted clique expansion of the hypergraph. This leads to WCE-GNN, a simple and efficient framework comprising a <b>GNN</b> and a weighted clique expansion (WCE), for hypergraph <b>node</b> <b>classification.</b> Experiments on nine real-world hypergraph <b>node</b> <b>classification</b> <b>benchmarks</b> showcase that WCE-GNN demonstrates not only higher classification accuracy compared to state-of-the-art HyperGNNs, but also superior memory and runtime efficiency.</p></p class="citation"></blockquote><h3 id=1965--74248-subgen-token-generation-in-sublinear-time-and-memory-amir-zandieh-et-al-2024>(19/65 | 74/248) SubGen: Token Generation in Sublinear Time and Memory (Amir Zandieh et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Amir Zandieh, Insu Han, Vahab Mirrokni, Amin Karbasi. (2024)<br><strong>SubGen: Token Generation in Sublinear Time and Memory</strong><br><button class=copy-to-clipboard title="SubGen: Token Generation in Sublinear Time and Memory" index=74>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-74 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-DS, cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Question Answering, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.06082v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.06082v1.pdf filename=2402.06082v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Despite the significant success of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs),</b> their extensive memory requirements pose challenges for deploying them in long-context token generation. The substantial memory footprint of <b>LLM</b> decoders arises from the necessity to store all previous tokens in the attention module, a requirement imposed by key-value (KV) caching. In this work, our focus is on developing an efficient compression technique for the KV cache. Empirical evidence indicates a significant clustering tendency within key embeddings in the attention module. Building on this key insight, we have devised a novel caching method with sublinear complexity, employing online clustering on key tokens and online $\ell_2$ sampling on values. The result is a provably accurate and efficient attention decoding algorithm, termed SubGen. Not only does this algorithm ensure a sublinear memory footprint and sublinear time complexity, but we also establish a tight error bound for our approach. Empirical evaluations on long-context <b>question-answering</b> <b>tasks</b> demonstrate that SubGen significantly outperforms existing and state-of-the-art KV cache compression methods in terms of performance and efficiency.</p></p class="citation"></blockquote><h3 id=2065--75248-scaling-intelligent-agents-in-combat-simulations-for-wargaming-scotty-black-et-al-2024>(20/65 | 75/248) Scaling Intelligent Agents in Combat Simulations for Wargaming (Scotty Black et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Scotty Black, Christian Darken. (2024)<br><strong>Scaling Intelligent Agents in Combat Simulations for Wargaming</strong><br><button class=copy-to-clipboard title="Scaling Intelligent Agents in Combat Simulations for Wargaming" index=75>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-75 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Reinforcement Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.06694v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.06694v1.pdf filename=2402.06694v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Remaining competitive in future conflicts with technologically-advanced competitors requires us to accelerate our research and development in artificial intelligence (AI) for wargaming. More importantly, leveraging machine learning for intelligent combat behavior development will be key to one day achieving superhuman performance in this domain&ndash;elevating the quality and accelerating the speed of our decisions in future wars. Although deep <b>reinforcement</b> <b>learning</b> (RL) continues to show promising results in intelligent agent behavior development in games, it has yet to perform at or above the human level in the long-horizon, complex tasks typically found in combat modeling and <b>simulation.</b> Capitalizing on the proven potential of RL and recent successes of hierarchical <b>reinforcement</b> <b>learning</b> (HRL), our research is investigating and extending the use of HRL to create intelligent agents capable of performing effectively in these large and complex <b>simulation</b> environments. Our ultimate goal is to develop an agent capable of superhuman performance that could then serve as an AI advisor to military planners and decision-makers. This papers covers our ongoing approach and the first three of our five research areas aimed at managing the exponential growth of computations that have thus far limited the use of AI in combat <b>simulations:</b> (1) developing an HRL training framework and agent architecture for combat units; (2) developing a multi-model framework for agent decision-making; (3) developing dimension-invariant observation abstractions of the state space to manage the exponential growth of computations; (4) developing an intrinsic rewards engine to enable long-term planning; and (5) implementing this framework into a higher-fidelity combat <b>simulation.</b></p></p class="citation"></blockquote><h3 id=2165--76248-implicit-bias-and-fast-convergence-rates-for-self-attention-bhavya-vasudeva-et-al-2024>(21/65 | 76/248) Implicit Bias and Fast Convergence Rates for Self-attention (Bhavya Vasudeva et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Bhavya Vasudeva, Puneesh Deora, Christos Thrampoulidis. (2024)<br><strong>Implicit Bias and Fast Convergence Rates for Self-attention</strong><br><button class=copy-to-clipboard title="Implicit Bias and Fast Convergence Rates for Self-attention" index=76>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-76 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, math-OC, stat-ML<br>Keyword Score: 30<br>Keywords: Logistic Regression, Transformer, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05738v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05738v1.pdf filename=2402.05738v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Self-attention,</b> the core mechanism of <b>transformers,</b> distinguishes them from traditional neural networks and drives their outstanding performance. Towards developing the fundamental optimization principles of <b>self-attention,</b> we investigate the implicit bias of gradient descent (GD) in training a <b>self-attention</b> layer with fixed linear decoder in binary classification. Drawing inspiration from the study of GD in linear <b>logistic</b> <b>regression</b> over separable data, recent work demonstrates that as the number of iterations $t$ approaches infinity, the key-query matrix $W_t$ converges locally (with respect to the initialization direction) to a hard-margin SVM solution $W_{mm}$. Our work enhances this result in four aspects. Firstly, we identify non-trivial data settings for which convergence is provably global, thus shedding light on the optimization landscape. Secondly, we provide the first finite-time convergence rate for $W_t$ to $W_{mm}$, along with quantifying the rate of sparsification in the attention map. Thirdly, through an analysis of normalized GD and Polyak step-size, we demonstrate analytically that adaptive step-size rules can accelerate the convergence of <b>self-attention.</b> Additionally, we remove the restriction of prior work on a fixed linear decoder. Our results reinforce the implicit-bias perspective of <b>self-attention</b> and strengthen its connections to implicit-bias in linear <b>logistic</b> <b>regression,</b> despite the intricate non-convex nature of the former.</p></p class="citation"></blockquote><h3 id=2265--77248-model-based-rl-for-mean-field-games-is-not-statistically-harder-than-single-agent-rl-jiawei-huang-et-al-2024>(22/65 | 77/248) Model-Based RL for Mean-Field Games is not Statistically Harder than Single-Agent RL (Jiawei Huang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiawei Huang, Niao He, Andreas Krause. (2024)<br><strong>Model-Based RL for Mean-Field Games is not Statistically Harder than Single-Agent RL</strong><br><button class=copy-to-clipboard title="Model-Based RL for Mean-Field Games is not Statistically Harder than Single-Agent RL" index=77>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-77 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-GT, cs-LG, cs.LG, stat-ML<br>Keyword Score: 30<br>Keywords: Heuristic Approach, Markov Game, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05724v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05724v1.pdf filename=2402.05724v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We study the sample complexity of <b>reinforcement</b> <b>learning</b> (RL) in Mean-Field Games (MFGs) with model-based function approximation that requires strategic exploration to find a Nash Equilibrium policy. We introduce the Partial Model-Based Eluder Dimension (P-MBED), a more effective notion to characterize the model class complexity. Notably, P-MBED measures the complexity of the single-agent model class converted from the given mean-field model class, and potentially, can be exponentially lower than the MBED proposed by \citet{huang2023statistical}. We contribute a model elimination algorithm featuring a novel exploration strategy and establish sample complexity results polynomial w.r.t.~P-MBED. Crucially, our results reveal that, under the basic realizability and Lipschitz continuity assumptions, \emph{learning Nash Equilibrium in MFGs is no more statistically challenging than solving a logarithmic number of single-agent RL problems}. We further extend our results to Multi-Type MFGs, generalizing from conventional MFGs and involving multiple types of agents. This extension implies statistical tractability of a broader class of <b>Markov</b> <b>Games</b> through the efficacy of mean-field approximation. Finally, inspired by our theoretical algorithm, we present a <b>heuristic</b> <b>approach</b> with improved computational efficiency and empirically demonstrate its effectiveness.</p></p class="citation"></blockquote><h3 id=2365--78248-unichain-and-aperiodicity-are-sufficient-for-asymptotic-optimality-of-average-reward-restless-bandits-yige-hong-et-al-2024>(23/65 | 78/248) Unichain and Aperiodicity are Sufficient for Asymptotic Optimality of Average-Reward Restless Bandits (Yige Hong et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yige Hong, Qiaomin Xie, Yudong Chen, Weina Wang. (2024)<br><strong>Unichain and Aperiodicity are Sufficient for Asymptotic Optimality of Average-Reward Restless Bandits</strong><br><button class=copy-to-clipboard title="Unichain and Aperiodicity are Sufficient for Asymptotic Optimality of Average-Reward Restless Bandits" index=78>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-78 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: 90C40, G-3; I-6, cs-LG, cs.LG, math-OC, math-PR<br>Keyword Score: 30<br>Keywords: Bandit Algorithm, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05689v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05689v1.pdf filename=2402.05689v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We consider the infinite-horizon, average-reward restless <b>bandit</b> problem in discrete time. We propose a new class of policies that are designed to drive a progressively larger subset of arms toward the optimal distribution. We show that our policies are asymptotically optimal with an $O(1/\sqrt{N})$ optimality gap for an $N$-armed problem, provided that the single-armed relaxed problem is unichain and aperiodic. Our approach departs from most existing work that focuses on index or priority policies, which rely on the Uniform Global Attractor Property (UGAP) to guarantee convergence to the optimum, or a recently developed <b>simulation-based</b> policy, which requires a Synchronization Assumption (SA).</p></p class="citation"></blockquote><h3 id=2465--79248-binding-dynamics-in-rotating-features-sindy-löwe-et-al-2024>(24/65 | 79/248) Binding Dynamics in Rotating Features (Sindy Löwe et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sindy Löwe, Francesco Locatello, Max Welling. (2024)<br><strong>Binding Dynamics in Rotating Features</strong><br><button class=copy-to-clipboard title="Binding Dynamics in Rotating Features" index=79>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-79 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CV, cs-LG, cs.LG, q-bio-NC<br>Keyword Score: 30<br>Keywords: Unsupervised Learning, Reasoning, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05627v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05627v1.pdf filename=2402.05627v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In human cognition, the binding problem describes the open question of how the brain flexibly integrates diverse information into cohesive object representations. Analogously, in machine learning, there is a pursuit for models capable of strong generalization and <b>reasoning</b> by learning object-centric representations in an <b>unsupervised</b> manner. Drawing from neuroscientific theories, Rotating Features learn such representations by introducing vector-valued features that encapsulate object characteristics in their magnitudes and object affiliation in their orientations. The &ldquo;$\chi$-binding&rdquo; mechanism, embedded in every layer of the architecture, has been shown to be crucial, but remains poorly understood. In this paper, we propose an alternative &ldquo;cosine binding&rdquo; mechanism, which explicitly computes the alignment between features and adjusts weights accordingly, and we show that it achieves equivalent performance. This allows us to draw direct connections to <b>self-attention</b> and biological neural processes, and to shed light on the fundamental dynamics for object-centric representations to emerge in Rotating Features.</p></p class="citation"></blockquote><h3 id=2565--80248-asynchronous-diffusion-learning-with-agent-subsampling-and-local-updates-elsa-rizk-et-al-2024>(25/65 | 80/248) Asynchronous Diffusion Learning with Agent Subsampling and Local Updates (Elsa Rizk et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Elsa Rizk, Kun Yuan, Ali H. Sayed. (2024)<br><strong>Asynchronous Diffusion Learning with Agent Subsampling and Local Updates</strong><br><button class=copy-to-clipboard title="Asynchronous Diffusion Learning with Agent Subsampling and Local Updates" index=80>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-80 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs-MA, cs.LG<br>Keyword Score: 30<br>Keywords: Federated Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05529v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05529v1.pdf filename=2402.05529v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this work, we examine a network of agents operating asynchronously, aiming to discover an ideal global model that suits individual local datasets. Our assumption is that each agent independently chooses when to participate throughout the algorithm and the specific subset of its neighbourhood with which it will cooperate at any given moment. When an agent chooses to take part, it undergoes multiple local updates before conveying its outcomes to the sub-sampled neighbourhood. Under this setup, we prove that the resulting asynchronous diffusion strategy is stable in the mean-square error sense and provide performance guarantees specifically for the <b>federated</b> <b>learning</b> setting. We illustrate the findings with numerical <b>simulations.</b></p></p class="citation"></blockquote><h3 id=2665--81248-neural-circuit-diagrams-robust-diagrams-for-the-communication-implementation-and-analysis-of-deep-learning-architectures-vincent-abbott-2024>(26/65 | 81/248) Neural Circuit Diagrams: Robust Diagrams for the Communication, Implementation, and Analysis of Deep Learning Architectures (Vincent Abbott, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Vincent Abbott. (2024)<br><strong>Neural Circuit Diagrams: Robust Diagrams for the Communication, Implementation, and Analysis of Deep Learning Architectures</strong><br><button class=copy-to-clipboard title="Neural Circuit Diagrams: Robust Diagrams for the Communication, Implementation, and Analysis of Deep Learning Architectures" index=81>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-81 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Convolution, Transformer, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05424v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05424v1.pdf filename=2402.05424v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Diagrams matter. Unfortunately, the deep learning community has no standard method for diagramming architectures. The current combination of linear algebra notation and ad-hoc diagrams fails to offer the necessary precision to understand architectures in all their detail. However, this detail is critical for faithful implementation, mathematical analysis, further innovation, and ethical assurances. I present neural circuit diagrams, a graphical language tailored to the needs of communicating deep learning architectures. Neural circuit diagrams naturally keep track of the changing arrangement of data, precisely show how operations are broadcast over axes, and display the critical parallel behavior of linear operations. A lingering issue with existing diagramming methods is the inability to simultaneously express the detail of axes and the free arrangement of data, which neural circuit diagrams solve. Their compositional structure is analogous to code, creating a close correspondence between diagrams and implementation. In this work, I introduce neural circuit diagrams for an audience of machine learning researchers. After introducing neural circuit diagrams, I cover a host of architectures to show their utility and breed familiarity. This includes the <b>transformer</b> architecture, <b>convolution</b> (and its difficult-to-explain extensions), residual networks, the U-Net, and the <b>vision</b> <b>transformer.</b> I include a Jupyter notebook that provides evidence for the close correspondence between diagrams and code. Finally, I examine backpropagation using neural circuit diagrams. I show their utility in providing mathematical insight and analyzing algorithms&rsquo; time and space complexities.</p></p class="citation"></blockquote><h3 id=2765--82248-taser-temporal-adaptive-sampling-for-fast-and-accurate-dynamic-graph-representation-learning-gangda-deng-et-al-2024>(27/65 | 82/248) TASER: Temporal Adaptive Sampling for Fast and Accurate Dynamic Graph Representation Learning (Gangda Deng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Gangda Deng, Hongkuan Zhou, Hanqing Zeng, Yinglong Xia, Christopher Leung, Jianbo Li, Rajgopal Kannan, Viktor Prasanna. (2024)<br><strong>TASER: Temporal Adaptive Sampling for Fast and Accurate Dynamic Graph Representation Learning</strong><br><button class=copy-to-clipboard title="TASER: Temporal Adaptive Sampling for Fast and Accurate Dynamic Graph Representation Learning" index=82>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-82 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Graph Neural Network, Recommendation, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05396v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05396v1.pdf filename=2402.05396v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recently, Temporal <b>Graph</b> <b>Neural</b> <b>Networks</b> (TGNNs) have demonstrated state-of-the-art performance in various high-impact applications, including fraud detection and content <b>recommendation.</b> Despite the success of TGNNs, they are prone to the prevalent noise found in real-world dynamic <b>graphs</b> <b>like</b> <b>time-deprecated</b> links and skewed interaction distribution. The noise causes two critical issues that significantly compromise the accuracy of TGNNs: (1) models are <b>supervised</b> by inferior interactions, and (2) noisy input induces high variance in the aggregated messages. However, current TGNN denoising techniques do not consider the diverse and dynamic noise pattern of each node. In addition, they also suffer from the excessive mini-batch generation overheads caused by traversing more neighbors. We believe the remedy for fast and accurate TGNNs lies in temporal adaptive sampling. In this work, we propose TASER, the first adaptive sampling method for TGNNs optimized for accuracy, efficiency, and scalability. TASER adapts its mini-batch selection based on training dynamics and temporal neighbor selection based on the contextual, structural, and temporal properties of past interactions. To alleviate the bottleneck in mini-batch generation, TASER implements a pure GPU-based temporal neighbor finder and a dedicated GPU feature cache. We evaluate the performance of TASER using two state-of-the-art backbone TGNNs. On five popular datasets, TASER outperforms the corresponding baselines by an average of 2.3% in Mean Reciprocal Rank (MRR) while achieving an average of 5.1x speedup in training time.</p></p class="citation"></blockquote><h3 id=2865--83248-noise-contrastive-alignment-of-language-models-with-explicit-rewards-huayu-chen-et-al-2024>(28/65 | 83/248) Noise Contrastive Alignment of Language Models with Explicit Rewards (Huayu Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Huayu Chen, Guande He, Hang Su, Jun Zhu. (2024)<br><strong>Noise Contrastive Alignment of Language Models with Explicit Rewards</strong><br><button class=copy-to-clipboard title="Noise Contrastive Alignment of Language Models with Explicit Rewards" index=83>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-83 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CL, cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Fine-tuning, GPT, GPT-4<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05369v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05369v1.pdf filename=2402.05369v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>User intentions are typically formalized as evaluation rewards to be maximized when <b>fine-tuning</b> language models (LMs). Existing alignment methods, such as Direct Preference Optimization (DPO), are mainly tailored for pairwise preference data where rewards are implicitly defined rather than explicitly given. In this paper, we introduce a general framework for LM alignment, leveraging Noise Contrastive Estimation (NCE) to bridge the gap in handling reward datasets explicitly annotated with scalar evaluations. Our framework comprises two parallel algorithms, NCA and InfoNCA, both enabling the direct extraction of an LM policy from reward data as well as preference data. Notably, we show that the DPO loss is a special case of our proposed InfoNCA objective under pairwise preference settings, thereby integrating and extending current alignment theories. By contrasting NCA and InfoNCA, we show that InfoNCA and DPO adjust relative likelihood across different responses to a single instruction, while NCA optimizes absolute likelihood for each response. We apply our methods to align a 7B language model with a <b>GPT-4</b> annotated reward dataset. Experimental results suggest that InfoNCA surpasses the DPO baseline in <b>GPT-4</b> evaluations, while NCA enjoys better training stability with competitive performance.</p></p class="citation"></blockquote><h3 id=2965--84248-exploring-learning-complexity-for-downstream-data-pruning-wenyu-jiang-et-al-2024>(29/65 | 84/248) Exploring Learning Complexity for Downstream Data Pruning (Wenyu Jiang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wenyu Jiang, Zhenlong Liu, Zejian Xie, Songxin Zhang, Bingyi Jing, Hongxin Wei. (2024)<br><strong>Exploring Learning Complexity for Downstream Data Pruning</strong><br><button class=copy-to-clipboard title="Exploring Learning Complexity for Downstream Data Pruning" index=84>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-84 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 30<br>Keywords: Fine-tuning, Pruning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05356v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05356v1.pdf filename=2402.05356v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The over-parameterized pre-trained models pose a great challenge to <b>fine-tuning</b> with limited computation resources. An intuitive solution is to prune the less informative samples from the <b>fine-tuning</b> dataset. A series of training-based scoring functions are proposed to quantify the informativeness of the data subset but the <b>pruning</b> cost becomes non-negligible due to the heavy parameter updating. For efficient <b>pruning,</b> it is viable to adapt the similarity scoring function of geometric-based methods from training-based to training-free. However, we empirically show that such adaption distorts the original <b>pruning</b> and results in inferior performance on the downstream tasks. In this paper, we propose to treat the learning complexity (LC) as the scoring function for classification and regression tasks. Specifically, the learning complexity is defined as the average predicted confidence of subnets with different capacities, which encapsulates data processing within a converged model. Then we preserve the diverse and easy samples for <b>fine-tuning.</b> Extensive experiments with vision datasets demonstrate the effectiveness and efficiency of the proposed scoring function for classification tasks. For the instruction <b>fine-tuning</b> of <b>large</b> <b>language</b> <b>models,</b> our method achieves state-of-the-art performance with stable convergence, outperforming the full training with only 10% of the instruction dataset.</p></p class="citation"></blockquote><h3 id=3065--85248-eugene-explainable-unsupervised-approximation-of-graph-edit-distance-aditya-bommakanti-et-al-2024>(30/65 | 85/248) EUGENE: Explainable Unsupervised Approximation of Graph Edit Distance (Aditya Bommakanti et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Aditya Bommakanti, Harshith Reddy Vonteri, Sayan Ranu, Panagiotis Karras. (2024)<br><strong>EUGENE: Explainable Unsupervised Approximation of Graph Edit Distance</strong><br><button class=copy-to-clipboard title="EUGENE: Explainable Unsupervised Approximation of Graph Edit Distance" index=85>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-85 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 23<br>Keywords: Benchmarking, Recommender System, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05885v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05885v1.pdf filename=2402.05885v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The need to identify graphs having small structural distance from a query arises in biology, chemistry, <b>recommender</b> <b>systems,</b> and social network analysis. Among several methods to measure inter graph distance, Graph Edit Distance (GED) is preferred for its comprehensibility, yet hindered by the NP-hardness of its computation. State-of-the-art GED approximations predominantly employ neural methods, which, however, (i) lack an explanatory edit path corresponding to the approximated GED; (ii) require the NP-hard generation of ground-truth GEDs for training; and (iii) necessitate separate training on each dataset. In this paper, we propose an efficient algebraic unsuper vised method, EUGENE, that approximates GED and yields edit paths corresponding to the approx imated cost, while eliminating the need for ground truth generation and data-specific training. Extensive experimental evaluation demonstrates that the aforementioned benefits of EUGENE do not come at the cost of efficacy. Specifically, EUGENE consistently ranks among the most accurate methods across all of the <b>benchmark</b> datasets and outperforms majority of the neural approaches.</p></p class="citation"></blockquote><h3 id=3165--86248-fusionsf-fuse-heterogeneous-modalities-in-a-vector-quantized-framework-for-robust-solar-power-forecasting-ziqing-ma-et-al-2024>(31/65 | 86/248) FusionSF: Fuse Heterogeneous Modalities in a Vector Quantized Framework for Robust Solar Power Forecasting (Ziqing Ma et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ziqing Ma, Wenwei Wang, Tian Zhou, Chao Chen, Bingqing Peng, Liang Sun, Rong Jin. (2024)<br><strong>FusionSF: Fuse Heterogeneous Modalities in a Vector Quantized Framework for Robust Solar Power Forecasting</strong><br><button class=copy-to-clipboard title="FusionSF: Fuse Heterogeneous Modalities in a Vector Quantized Framework for Robust Solar Power Forecasting" index=86>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-86 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CV, cs-LG, cs.LG<br>Keyword Score: 23<br>Keywords: Multi-modal, Quantization, Zero-shot<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05823v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05823v1.pdf filename=2402.05823v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Accurate solar power forecasting is crucial to integrate photovoltaic plants into the electric grid, schedule and secure the power grid safety. This problem becomes more demanding for those newly installed solar plants which lack sufficient data. Current research predominantly relies on historical solar power data or numerical weather prediction in a single-modality format, ignoring the complementary information provided in different modalities. In this paper, we propose a multi-modality fusion framework to integrate historical power data, numerical weather prediction, and satellite images, significantly improving forecast performance. We introduce a vector <b>quantized</b> framework that aligns modalities with varying information densities, striking a balance between integrating sufficient information and averting model overfitting. Our framework demonstrates strong <b>zero-shot</b> forecasting capability, which is especially useful for those newly installed plants. Moreover, we collect and release a <b>multi-modal</b> solar power (MMSP) dataset from real-world plants to further promote the research of <b>multi-modal</b> solar forecasting algorithms. Our extensive experiments show that our model not only operates with robustness but also boosts accuracy in both <b>zero-shot</b> forecasting and scenarios rich with training data, surpassing leading models. We have incorporated it into our eForecaster platform and deployed it for more than 300 solar plants with a capacity of over 15GW.</p></p class="citation"></blockquote><h3 id=3265--87248-improving-token-based-world-models-with-parallel-observation-prediction-lior-cohen-et-al-2024>(32/65 | 87/248) Improving Token-Based World Models with Parallel Observation Prediction (Lior Cohen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lior Cohen, Kaixin Wang, Bingyi Kang, Shie Mannor. (2024)<br><strong>Improving Token-Based World Models with Parallel Observation Prediction</strong><br><button class=copy-to-clipboard title="Improving Token-Based World Models with Parallel Observation Prediction" index=87>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-87 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 23<br>Keywords: Benchmarking, Reinforcement Learning, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05643v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05643v2.pdf filename=2402.05643v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Motivated by the success of <b>Transformers</b> when applied to sequences of discrete symbols, token-based world models (TBWMs) were recently proposed as sample-efficient methods. In TBWMs, the world model consumes agent experience as a language-like sequence of tokens, where each observation constitutes a sub-sequence. However, during imagination, the sequential token-by-token generation of next observations results in a severe bottleneck, leading to long training times, poor GPU utilization, and limited representations. To resolve this bottleneck, we devise a novel Parallel Observation Prediction (POP) mechanism. POP augments a Retentive Network (RetNet) with a novel forward mode tailored to our <b>reinforcement</b> <b>learning</b> setting. We incorporate POP in a novel TBWM agent named REM (Retentive Environment Model), showcasing a 15.4x faster imagination compared to prior TBWMs. REM attains superhuman performance on 12 out of 26 games of the Atari 100K <b>benchmark,</b> while training in less than 12 hours. Our code is available at \url{https://github.com/leor-c/REM}.</p></p class="citation"></blockquote><h3 id=3365--88248-flashback-understanding-and-mitigating-forgetting-in-federated-learning-mohammed-aljahdali-et-al-2024>(33/65 | 88/248) Flashback: Understanding and Mitigating Forgetting in Federated Learning (Mohammed Aljahdali et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mohammed Aljahdali, Ahmed M. Abdelmoniem, Marco Canini, Samuel Horváth. (2024)<br><strong>Flashback: Understanding and Mitigating Forgetting in Federated Learning</strong><br><button class=copy-to-clipboard title="Flashback: Understanding and Mitigating Forgetting in Federated Learning" index=88>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-88 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CV, cs-DC, cs-LG, cs.LG<br>Keyword Score: 23<br>Keywords: Benchmarking, Federated Learning, Knowledge Distillation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05558v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05558v1.pdf filename=2402.05558v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In <b>Federated</b> <b>Learning</b> (FL), forgetting, or the loss of knowledge across rounds, hampers algorithm convergence, particularly in the presence of severe data heterogeneity among clients. This study explores the nuances of this issue, emphasizing the critical role of forgetting in FL&rsquo;s inefficient learning within heterogeneous data contexts. Knowledge loss occurs in both client-local updates and server-side aggregation steps; addressing one without the other fails to mitigate forgetting. We introduce a metric to measure forgetting granularly, ensuring distinct recognition amid new knowledge acquisition. Leveraging these insights, we propose Flashback, an FL algorithm with a dynamic <b>distillation</b> approach that is used to regularize the local models, and effectively aggregate their knowledge. Across different <b>benchmarks,</b> Flashback outperforms other methods, mitigates forgetting, and achieves faster round-to-target-accuracy, by converging in 6 to 16 rounds.</p></p class="citation"></blockquote><h3 id=3465--89248-activedp-bridging-active-learning-and-data-programming-naiqing-guan-et-al-2024>(34/65 | 89/248) ActiveDP: Bridging Active Learning and Data Programming (Naiqing Guan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Naiqing Guan, Nick Koudas. (2024)<br><strong>ActiveDP: Bridging Active Learning and Data Programming</strong><br><button class=copy-to-clipboard title="ActiveDP: Bridging Active Learning and Data Programming" index=89>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-89 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-DB, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Active Learning, Weakly Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.06056v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.06056v1.pdf filename=2402.06056v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Modern machine learning models require large labelled datasets to achieve good performance, but manually labelling large datasets is expensive and time-consuming. The data programming paradigm enables users to label large datasets efficiently but produces noisy labels, which deteriorates the downstream model&rsquo;s performance. The <b>active</b> <b>learning</b> paradigm, on the other hand, can acquire accurate labels but only for a small fraction of instances. In this paper, we propose ActiveDP, an interactive framework bridging <b>active</b> <b>learning</b> and data programming together to generate labels with both high accuracy and coverage, combining the strengths of both paradigms. Experiments show that ActiveDP outperforms previous <b>weak</b> <b>supervision</b> and <b>active</b> <b>learning</b> approaches and consistently performs well under different labelling budgets.</p></p class="citation"></blockquote><h3 id=3565--90248-risk-sensitive-multi-agent-reinforcement-learning-in-network-aggregative-markov-games-hafez-ghaemi-et-al-2024>(35/65 | 90/248) Risk-Sensitive Multi-Agent Reinforcement Learning in Network Aggregative Markov Games (Hafez Ghaemi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hafez Ghaemi, Hamed Kebriaei, Alireza Ramezani Moghaddam, Majid Nili Ahamdabadi. (2024)<br><strong>Risk-Sensitive Multi-Agent Reinforcement Learning in Network Aggregative Markov Games</strong><br><button class=copy-to-clipboard title="Risk-Sensitive Multi-Agent Reinforcement Learning in Network Aggregative Markov Games" index=90>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-90 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: I-2-6; I-2-11, cs-AI, cs-LG, cs-MA, cs.LG<br>Keyword Score: 20<br>Keywords: Markov Game, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05906v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05906v1.pdf filename=2402.05906v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Classical multi-agent <b>reinforcement</b> <b>learning</b> (MARL) assumes risk neutrality and complete objectivity for agents. However, in settings where agents need to consider or model human economic or social preferences, a notion of risk must be incorporated into the RL optimization problem. This will be of greater importance in MARL where other human or non-human agents are involved, possibly with their own risk-sensitive policies. In this work, we consider risk-sensitive and non-cooperative MARL with cumulative prospect theory (CPT), a non-convex risk measure and a generalization of coherent measures of risk. CPT is capable of explaining loss aversion in humans and their tendency to overestimate/underestimate small/large probabilities. We propose a distributed sampling-based actor-critic (AC) algorithm with CPT risk for network aggregative <b>Markov</b> <b>games</b> (NAMGs), which we call Distributed Nested CPT-AC. Under a set of assumptions, we prove the convergence of the algorithm to a subjective notion of <b>Markov</b> <b>perfect</b> Nash equilibrium in NAMGs. The experimental results show that subjective CPT policies obtained by our algorithm can be different from the risk-neutral ones, and agents with a higher loss aversion are more inclined to socially isolate themselves in an NAMG.</p></p class="citation"></blockquote><h3 id=3665--91248-discovering-temporally-aware-reinforcement-learning-algorithms-matthew-thomas-jackson-et-al-2024>(36/65 | 91/248) Discovering Temporally-Aware Reinforcement Learning Algorithms (Matthew Thomas Jackson et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Matthew Thomas Jackson, Chris Lu, Louis Kirsch, Robert Tjarko Lange, Shimon Whiteson, Jakob Nicolaus Foerster. (2024)<br><strong>Discovering Temporally-Aware Reinforcement Learning Algorithms</strong><br><button class=copy-to-clipboard title="Discovering Temporally-Aware Reinforcement Learning Algorithms" index=91>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-91 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Meta Learning, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05828v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05828v1.pdf filename=2402.05828v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advancements in <b>meta-learning</b> <b>have</b> enabled the automatic discovery of novel <b>reinforcement</b> <b>learning</b> algorithms parameterized by surrogate objective functions. To improve upon manually designed algorithms, the parameterization of this learned objective function must be expressive enough to represent novel principles of learning (instead of merely recovering already established ones) while still generalizing to a wide range of settings outside of its <b>meta-training</b> <b>distribution.</b> However, existing methods focus on discovering objective functions that, like many widely used objective functions in <b>reinforcement</b> <b>learning,</b> do not take into account the total number of steps allowed for training, or &ldquo;training horizon&rdquo;. In contrast, humans use a plethora of different learning objectives across the course of acquiring a new ability. For instance, students may alter their studying techniques based on the proximity to exam deadlines and their self-assessed capabilities. This paper contends that ignoring the optimization time horizon significantly restricts the expressive potential of discovered learning algorithms. We propose a simple augmentation to two existing objective discovery approaches that allows the discovered algorithm to dynamically update its objective function throughout the agent&rsquo;s training procedure, resulting in expressive schedules and increased generalization across different training horizons. In the process, we find that commonly used <b>meta-gradient</b> <b>approaches</b> fail to discover such adaptive objective functions while evolution strategies discover highly dynamic learning rules. We demonstrate the effectiveness of our approach on a wide range of tasks and analyze the resulting learned algorithms, which we find effectively balance exploration and exploitation by modifying the structure of their learning rules throughout the agent&rsquo;s lifetime.</p></p class="citation"></blockquote><h3 id=3765--92248-latent-variable-model-for-high-dimensional-point-process-with-structured-missingness-maksim-sinelnikov-et-al-2024>(37/65 | 92/248) Latent variable model for high-dimensional point process with structured missingness (Maksim Sinelnikov et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Maksim Sinelnikov, Manuel Haussmann, Harri Lähdesmäki. (2024)<br><strong>Latent variable model for high-dimensional point process with structured missingness</strong><br><button class=copy-to-clipboard title="Latent variable model for high-dimensional point process with structured missingness" index=92>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-92 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-ML<br>Keyword Score: 20<br>Keywords: Autoencoder, Variational Autoencoder<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05758v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05758v1.pdf filename=2402.05758v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Longitudinal data are important in numerous fields, such as healthcare, sociology and seismology, but real-world datasets present notable challenges for practitioners because they can be high-dimensional, contain structured missingness patterns, and measurement time points can be governed by an unknown stochastic process. While various solutions have been suggested, the majority of them have been designed to account for only one of these challenges. In this work, we propose a flexible and efficient latent-variable model that is capable of addressing all these limitations. Our approach utilizes Gaussian processes to capture temporal correlations between samples and their associated missingness masks as well as to model the underlying point process. We construct our model as a <b>variational</b> <b>autoencoder</b> together with deep neural network parameterised encoder and decoder models, and develop a scalable amortised <b>variational</b> <b>inference</b> approach for efficient model training. We demonstrate competitive performance using both simulated and real datasets.</p></p class="citation"></blockquote><h3 id=3865--93248-generalized-preference-optimization-a-unified-approach-to-offline-alignment-yunhao-tang-et-al-2024>(38/65 | 93/248) Generalized Preference Optimization: A Unified Approach to Offline Alignment (Yunhao Tang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yunhao Tang, Zhaohan Daniel Guo, Zeyu Zheng, Daniele Calandriello, Rémi Munos, Mark Rowland, Pierre Harvey Richemond, Michal Valko, Bernardo Ávila Pires, Bilal Piot. (2024)<br><strong>Generalized Preference Optimization: A Unified Approach to Offline Alignment</strong><br><button class=copy-to-clipboard title="Generalized Preference Optimization: A Unified Approach to Offline Alignment" index=93>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-93 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Fine-tuning, Reinforcement Learning from Human Feedback<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05749v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05749v1.pdf filename=2402.05749v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Offline preference optimization allows <b>fine-tuning</b> large models directly from offline data, and has proved effective in recent alignment practices. We propose generalized preference optimization (GPO), a family of offline losses parameterized by a general class of convex functions. GPO enables a unified view over preference optimization, encompassing existing algorithms such as DPO, IPO and SLiC as special cases, while naturally introducing new variants. The GPO framework also sheds light on how offline algorithms enforce regularization, through the design of the convex function that defines the loss. Our analysis and experiments reveal the connections and subtle differences between the offline regularization and the KL divergence regularization intended by the canonical <b>RLHF</b> formulation. In all, our results present new algorithmic toolkits and empirical insights to alignment practitioners.</p></p class="citation"></blockquote><h3 id=3965--94248-simultaneously-achieving-group-exposure-fairness-and-within-group-meritocracy-in-stochastic-bandits-subham-pokhriyal-et-al-2024>(39/65 | 94/248) Simultaneously Achieving Group Exposure Fairness and Within-Group Meritocracy in Stochastic Bandits (Subham Pokhriyal et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Subham Pokhriyal, Shweta Jain, Ganesh Ghalme, Swapnil Dhamal, Sujit Gujar. (2024)<br><strong>Simultaneously Achieving Group Exposure Fairness and Within-Group Meritocracy in Stochastic Bandits</strong><br><button class=copy-to-clipboard title="Simultaneously Achieving Group Exposure Fairness and Within-Group Meritocracy in Stochastic Bandits" index=94>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-94 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CY, cs-LG, cs-MA, cs.LG<br>Keyword Score: 20<br>Keywords: Bandit Algorithm, Fairness<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05575v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05575v1.pdf filename=2402.05575v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Existing approaches to <b>fairness</b> in stochastic multi-armed <b>bandits</b> (MAB) primarily focus on exposure guarantee to individual arms. When arms are naturally grouped by certain attribute(s), we propose Bi-Level <b>Fairness,</b> which considers two levels of <b>fairness.</b> At the first level, Bi-Level <b>Fairness</b> guarantees a certain minimum exposure to each group. To address the unbalanced allocation of pulls to individual arms within a group, we consider meritocratic <b>fairness</b> at the second level, which ensures that each arm is pulled according to its merit within the group. Our work shows that we can adapt a UCB-based algorithm to achieve a Bi-Level <b>Fairness</b> by providing (i) anytime Group Exposure <b>Fairness</b> guarantees and (ii) ensuring individual-level Meritocratic <b>Fairness</b> within each group. We first show that one can decompose regret bounds into two components: (a) regret due to anytime group exposure <b>fairness</b> and (b) regret due to meritocratic <b>fairness</b> within each group. Our proposed algorithm BF-UCB balances these two regrets optimally to achieve the upper bound of $O(\sqrt{T})$ on regret; $T$ being the stopping time. With the help of simulated experiments, we further show that BF-UCB achieves sub-linear regret; provides better group and individual exposure guarantees compared to existing algorithms; and does not result in a significant drop in reward with respect to UCB algorithm, which does not impose any <b>fairness</b> constraint.</p></p class="citation"></blockquote><h3 id=4065--95248-differentially-private-model-based-offline-reinforcement-learning-alexandre-rio-et-al-2024>(40/65 | 95/248) Differentially Private Model-Based Offline Reinforcement Learning (Alexandre Rio et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Alexandre Rio, Merwan Barlier, Igor Colin, Albert Thomas. (2024)<br><strong>Differentially Private Model-Based Offline Reinforcement Learning</strong><br><button class=copy-to-clipboard title="Differentially Private Model-Based Offline Reinforcement Learning" index=95>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-95 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CR, cs-LG, cs.LG, stat-ML<br>Keyword Score: 20<br>Keywords: Offline Reinforcement Learning, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05525v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05525v1.pdf filename=2402.05525v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We address <b>offline</b> <b>reinforcement</b> <b>learning</b> with privacy guarantees, where the goal is to train a policy that is differentially private with respect to individual trajectories in the dataset. To achieve this, we introduce DP-MORL, an MBRL algorithm coming with differential privacy guarantees. A private model of the environment is first learned from <b>offline</b> <b>data</b> <b>using</b> DP-FedAvg, a training method for neural networks that provides differential privacy guarantees at the trajectory level. Then, we use model-based policy optimization to derive a policy from the (penalized) private model, without any further interaction with the system or access to the input data. We empirically show that DP-MORL enables the training of private RL agents from <b>offline</b> <b>data</b> <b>and</b> we furthermore outline the price of privacy in this setting.</p></p class="citation"></blockquote><h3 id=4165--96248-multi-timescale-ensemble-q-learning-for-markov-decision-process-policy-optimization-talha-bozkus-et-al-2024>(41/65 | 96/248) Multi-Timescale Ensemble Q-learning for Markov Decision Process Policy Optimization (Talha Bozkus et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Talha Bozkus, Urbashi Mitra. (2024)<br><strong>Multi-Timescale Ensemble Q-learning for Markov Decision Process Policy Optimization</strong><br><button class=copy-to-clipboard title="Multi-Timescale Ensemble Q-learning for Markov Decision Process Policy Optimization" index=96>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-96 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, eess-SP<br>Keyword Score: 20<br>Keywords: Markov Decision Process, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05476v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05476v1.pdf filename=2402.05476v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Reinforcement</b> <b>learning</b> (RL) is a classical tool to solve network control or policy optimization problems in unknown environments. The original Q-learning suffers from performance and complexity challenges across very large networks. Herein, a novel model-free ensemble <b>reinforcement</b> <b>learning</b> algorithm which adapts the classical Q-learning is proposed to handle these challenges for networks which admit <b>Markov</b> <b>decision</b> <b>process</b> (MDP) models. Multiple Q-learning algorithms are run on multiple, distinct, synthetically created and structurally related Markovian environments in parallel; the outputs are fused using an adaptive weighting mechanism based on the Jensen-Shannon divergence (JSD) to obtain an approximately optimal policy with low complexity. The theoretical justification of the algorithm, including the convergence of key statistics and Q-functions are provided. Numerical results across several network models show that the proposed algorithm can achieve up to 55% less average policy error with up to 50% less runtime complexity than the state-of-the-art Q-learning algorithms. Numerical results validate assumptions made in the theoretical analysis.</p></p class="citation"></blockquote><h3 id=4265--97248-everybody-prune-now-structured-pruning-of-llms-with-only-forward-passes-lucio-dery-et-al-2024>(42/65 | 97/248) Everybody Prune Now: Structured Pruning of LLMs with only Forward Passes (Lucio Dery et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lucio Dery, Steven Kolawole, Jean-François Kagy, Virginia Smith, Graham Neubig, Ameet Talwalkar. (2024)<br><strong>Everybody Prune Now: Structured Pruning of LLMs with only Forward Passes</strong><br><button class=copy-to-clipboard title="Everybody Prune Now: Structured Pruning of LLMs with only Forward Passes" index=97>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-97 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-CL, cs-LG, cs.LG<br>Keyword Score: 20<br>Keywords: Pruning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05406v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05406v2.pdf filename=2402.05406v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Given the generational gap in available hardware between lay practitioners and the most endowed institutions, <b>LLMs</b> are becoming increasingly inaccessible as they grow in size. Whilst many approaches have been proposed to compress <b>LLMs</b> to make their resource consumption manageable, these methods themselves tend to be resource intensive, putting them out of the reach of the very user groups they target. In this work, we explore the problem of structured <b>pruning</b> of <b>LLMs</b> using only forward passes. We seek to empower practitioners to prune models so large that their available hardware has just enough memory to run inference. We develop Bonsai, a gradient-free, perturbative <b>pruning</b> method capable of delivering small, fast, and accurate pruned models. We observe that Bonsai outputs pruned models that (i) outperform those generated by more expensive gradient-based structured <b>pruning</b> methods, and (ii) are twice as fast (with comparable accuracy) as those generated by semi-structured <b>pruning</b> methods requiring comparable resources as Bonsai. We also leverage Bonsai to produce a new sub-2B model using a single A6000 that yields state-of-the-art performance on 4/6 tasks on the Huggingface Open <b>LLM</b> leaderboard.</p></p class="citation"></blockquote><h3 id=4365--98248-direct-acquisition-optimization-for-low-budget-active-learning-zhuokai-zhao-et-al-2024>(43/65 | 98/248) Direct Acquisition Optimization for Low-Budget Active Learning (Zhuokai Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhuokai Zhao, Yibo Jiang, Yuxin Chen. (2024)<br><strong>Direct Acquisition Optimization for Low-Budget Active Learning</strong><br><button class=copy-to-clipboard title="Direct Acquisition Optimization for Low-Budget Active Learning" index=98>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-98 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 13<br>Keywords: Active Learning, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.06045v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.06045v1.pdf filename=2402.06045v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Active</b> <b>Learning</b> (AL) has gained prominence in integrating data-intensive machine learning (ML) models into domains with limited labeled data. However, its effectiveness diminishes significantly when the labeling budget is low. In this paper, we first empirically observe the performance degradation of existing AL algorithms in the low-budget settings, and then introduce Direct Acquisition Optimization (DAO), a novel AL algorithm that optimizes sample selections based on expected true loss reduction. Specifically, DAO utilizes influence functions to update model parameters and incorporates an additional acquisition strategy to mitigate bias in loss estimation. This approach facilitates a more accurate estimation of the overall error reduction, without extensive computations or reliance on labeled data. Experiments demonstrate DAO&rsquo;s effectiveness in low budget settings, outperforming state-of-the-arts approaches across seven <b>benchmarks.</b></p></p class="citation"></blockquote><h3 id=4465--99248-optimizing-predictive-ai-in-physical-design-flows-with-mini-pixel-batch-gradient-descent-haoyu-yang-et-al-2024>(44/65 | 99/248) Optimizing Predictive AI in Physical Design Flows with Mini Pixel Batch Gradient Descent (Haoyu Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haoyu Yang, Anthony Agnesina, Haoxing Ren. (2024)<br><strong>Optimizing Predictive AI in Physical Design Flows with Mini Pixel Batch Gradient Descent</strong><br><button class=copy-to-clipboard title="Optimizing Predictive AI in Physical Design Flows with Mini Pixel Batch Gradient Descent" index=99>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-99 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 13<br>Keywords: Benchmarking, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.06034v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.06034v1.pdf filename=2402.06034v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Exploding predictive AI has enabled fast yet effective evaluation and decision-making in modern chip physical design flows. State-of-the-art frameworks typically include the objective of minimizing the mean square error (MSE) between the prediction and the ground truth. We argue the averaging effect of MSE induces limitations in both model training and deployment, and good MSE behavior does not guarantee the capability of these models to assist physical design flows which are likely sabotaged due to a small portion of prediction error. To address this, we propose mini-pixel batch gradient descent (MPGD), a plug-and-play optimization algorithm that takes the most informative entries into consideration, offering probably faster and better convergence. Experiments on representative <b>benchmark</b> suits show the significant benefits of MPGD on various physical design prediction tasks using <b>CNN</b> or Graph-based models.</p></p class="citation"></blockquote><h3 id=4565--100248-decision-theory-guided-deep-reinforcement-learning-for-fast-learning-zelin-wan-et-al-2024>(45/65 | 100/248) Decision Theory-Guided Deep Reinforcement Learning for Fast Learning (Zelin Wan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zelin Wan, Jin-Hee Cho, Mu Zhu, Ahmed H. Anwar, Charles Kamhoua, Munindar P. Singh. (2024)<br><strong>Decision Theory-Guided Deep Reinforcement Learning for Fast Learning</strong><br><button class=copy-to-clipboard title="Decision Theory-Guided Deep Reinforcement Learning for Fast Learning" index=100>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-100 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-GT, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.06023v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.06023v1.pdf filename=2402.06023v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper introduces a novel approach, Decision Theory-guided Deep <b>Reinforcement</b> <b>Learning</b> (DT-guided DRL), to address the inherent cold start problem in DRL. By integrating decision theory principles, DT-guided DRL enhances agents&rsquo; initial performance and robustness in complex environments, enabling more efficient and reliable convergence during learning. Our investigation encompasses two primary problem contexts: the cart pole and maze navigation challenges. Experimental results demonstrate that the integration of decision theory not only facilitates effective initial guidance for DRL agents but also promotes a more structured and informed exploration strategy, particularly in environments characterized by large and intricate state spaces. The results of experiment demonstrate that DT-guided DRL can provide significantly higher rewards compared to regular DRL. Specifically, during the initial phase of training, the DT-guided DRL yields up to an 184% increase in accumulated reward. Moreover, even after reaching convergence, it maintains a superior performance, ending with up to 53% more reward than standard DRL in large maze problems. DT-guided DRL represents an advancement in mitigating a fundamental challenge of DRL by leveraging functions informed by human (designer) knowledge, setting a foundation for further research in this promising interdisciplinary domain.</p></p class="citation"></blockquote><h3 id=4665--101248-guided-evolution-with-binary-discriminators-for-ml-program-search-john-d-co-reyes-et-al-2024>(46/65 | 101/248) Guided Evolution with Binary Discriminators for ML Program Search (John D. Co-Reyes et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>John D. Co-Reyes, Yingjie Miao, George Tucker, Aleksandra Faust, Esteban Real. (2024)<br><strong>Guided Evolution with Binary Discriminators for ML Program Search</strong><br><button class=copy-to-clipboard title="Guided Evolution with Binary Discriminators for ML Program Search" index=101>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-101 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs-NE, cs.LG<br>Keyword Score: 10<br>Keywords: Graph Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05821v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05821v1.pdf filename=2402.05821v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>How to automatically design better machine learning programs is an open problem within AutoML. While evolution has been a popular tool to search for better ML programs, using learning itself to guide the search has been less successful and less understood on harder problems but has the promise to dramatically increase the speed and final performance of the optimization process. We propose guiding evolution with a binary discriminator, trained online to distinguish which program is better given a pair of programs. The discriminator selects better programs without having to perform a costly evaluation and thus speed up the convergence of evolution. Our method can encode a wide variety of ML components including symbolic optimizers, neural architectures, RL loss functions, and symbolic regression equations with the same directed acyclic graph representation. By combining this representation with modern <b>GNNs</b> and an adaptive mutation strategy, we demonstrate our method can speed up evolution across a set of diverse problems including a 3.7x speedup on the symbolic search for ML optimizers and a 4x speedup for RL loss functions.</p></p class="citation"></blockquote><h3 id=4765--102248-on-calibration-and-conformal-prediction-of-deep-classifiers-lahav-dabah-et-al-2024>(47/65 | 102/248) On Calibration and Conformal Prediction of Deep Classifiers (Lahav Dabah et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lahav Dabah, Tom Tirer. (2024)<br><strong>On Calibration and Conformal Prediction of Deep Classifiers</strong><br><button class=copy-to-clipboard title="On Calibration and Conformal Prediction of Deep Classifiers" index=102>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-102 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-ML<br>Keyword Score: 10<br>Keywords: Reasoning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05806v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05806v1.pdf filename=2402.05806v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In many classification applications, the prediction of a deep neural network (DNN) based classifier needs to be accompanied with some confidence indication. Two popular post-processing approaches for that aim are: 1) calibration: modifying the classifier&rsquo;s softmax values such that their maximum (associated with the prediction) better estimates the correctness probability; and 2) conformal prediction (CP): devising a score (based on the softmax values) from which a set of predictions with theoretically guaranteed marginal coverage of the correct class is produced. While in practice both types of indications can be desired, so far the interplay between them has not been investigated. Toward filling this gap, in this paper we study the effect of temperature scaling, arguably the most common calibration technique, on prominent CP methods. We start with an extensive empirical study that among other insights shows that, surprisingly, calibration has a detrimental effect on popular adaptive CP methods: it frequently leads to larger prediction sets. Then, we turn to theoretically analyze this behavior. We reveal several mathematical properties of the procedure, according to which we provide a <b>reasoning</b> for the phenomenon. Our study suggests that it may be worthwhile to utilize adaptive CP methods, chosen for their enhanced conditional coverage, based on softmax values prior to (or after canceling) temperature scaling calibration.</p></p class="citation"></blockquote><h3 id=4865--103248-unsupervised-discovery-of-clinical-disease-signatures-using-probabilistic-independence-thomas-a-lasko-et-al-2024>(48/65 | 103/248) Unsupervised Discovery of Clinical Disease Signatures Using Probabilistic Independence (Thomas A. Lasko et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Thomas A. Lasko, John M. Still, Thomas Z. Li, Marco Barbero Mota, William W. Stead, Eric V. Strobl, Bennett A. Landman, Fabien Maldonado. (2024)<br><strong>Unsupervised Discovery of Clinical Disease Signatures Using Probabilistic Independence</strong><br><button class=copy-to-clipboard title="Unsupervised Discovery of Clinical Disease Signatures Using Probabilistic Independence" index=103>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-103 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: I-2-6; I-2-1; J-3, cs-LG, cs.LG, stat-AP, stat-ML<br>Keyword Score: 10<br>Keywords: Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05802v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05802v1.pdf filename=2402.05802v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Insufficiently precise diagnosis of clinical disease is likely responsible for many treatment failures, even for common conditions and treatments. With a large enough dataset, it may be possible to use <b>unsupervised</b> machine learning to define clinical disease patterns more precisely. We present an approach to learning these patterns by using probabilistic independence to disentangle the imprint on the medical record of causal latent sources of disease. We inferred a broad set of 2000 clinical signatures of latent sources from 9195 variables in 269,099 Electronic Health Records. The learned signatures produced better discrimination than the original variables in a lung cancer prediction task unknown to the inference algorithm, predicting 3-year malignancy in patients with no history of cancer before a solitary lung nodule was discovered. More importantly, the signatures&rsquo; greater explanatory power identified pre-nodule signatures of apparently undiagnosed cancer in many of those patients.</p></p class="citation"></blockquote><h3 id=4965--104248-comparison-of-machine-learning-and-statistical-approaches-for-digital-elevation-model-dem-correction-interim-results-chukwuma-okolie-et-al-2024>(49/65 | 104/248) Comparison of machine learning and statistical approaches for digital elevation model (DEM) correction: interim results (Chukwuma Okolie et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chukwuma Okolie, Adedayo Adeleke, Julian Smit, Jon Mills, Iyke Maduako, Caleb Ogbeta. (2024)<br><strong>Comparison of machine learning and statistical approaches for digital elevation model (DEM) correction: interim results</strong><br><button class=copy-to-clipboard title="Comparison of machine learning and statistical approaches for digital elevation model (DEM) correction: interim results" index=104>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-104 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.06688v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.06688v1.pdf filename=2402.06688v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Several methods have been proposed for correcting the elevation bias in digital elevation models (DEMs) for example, linear regression. Nowadays, <b>supervised</b> machine learning enables the modelling of complex relationships between variables, and has been deployed by researchers in a variety of fields. In the existing literature, several studies have adopted either machine learning or statistical approaches in the task of DEM correction. However, to our knowledge, none of these studies have compared the performance of both approaches, especially with regard to open-access global DEMs. Our previous work has already shown the potential of machine learning approaches, specifically gradient boosted decision trees (GBDTs) for DEM correction. In this study, we share some results from the comparison of three recent implementations of gradient boosted decision trees (XGBoost, LightGBM and CatBoost), versus multiple linear regression (MLR) for enhancing the vertical accuracy of 30 m Copernicus and AW3D global DEMs in Cape Town, South Africa.</p></p class="citation"></blockquote><h3 id=5065--105248-analysing-the-sample-complexity-of-opponent-shaping-kitty-fung-et-al-2024>(50/65 | 105/248) Analysing the Sample Complexity of Opponent Shaping (Kitty Fung et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kitty Fung, Qizhen Zhang, Chris Lu, Jia Wan, Timon Willi, Jakob Foerster. (2024)<br><strong>Analysing the Sample Complexity of Opponent Shaping</strong><br><button class=copy-to-clipboard title="Analysing the Sample Complexity of Opponent Shaping" index=105>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-105 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-GT, cs-LG, cs-MA, cs.LG<br>Keyword Score: 10<br>Keywords: Markov Decision Process<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05782v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05782v1.pdf filename=2402.05782v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Learning in general-sum games often yields collectively sub-optimal results. Addressing this, opponent shaping (OS) methods actively guide the learning processes of other agents, empirically leading to improved individual and group performances in many settings. Early OS methods use higher-order derivatives to shape the learning of co-players, making them unsuitable for shaping multiple learning steps. Follow-up work, Model-free Opponent Shaping (M-FOS), addresses these by reframing the OS problem as a meta-game. In contrast to early OS methods, there is little theoretical understanding of the M-FOS framework. Providing theoretical guarantees for M-FOS is hard because A) there is little literature on theoretical sample complexity bounds for meta-reinforcement learning B) M-FOS operates in continuous state and action spaces, so theoretical analysis is challenging. In this work, we present R-FOS, a tabular version of M-FOS that is more suitable for theoretical analysis. R-FOS discretises the continuous meta-game MDP into a tabular MDP. Within this discretised MDP, we adapt the $R_{max}$ algorithm, most prominently used to derive PAC-bounds for <b>MDPs,</b> as the meta-learner in the R-FOS algorithm. We derive a sample complexity bound that is exponential in the cardinality of the inner state and action space and the number of agents. Our bound guarantees that, with high probability, the final policy learned by an R-FOS agent is close to the optimal policy, apart from a constant factor. Finally, we investigate how R-FOS&rsquo;s sample complexity scales in the size of state-action space. Our theoretical results on scaling are supported empirically in the Matching Pennies environment.</p></p class="citation"></blockquote><h3 id=5165--106248-stable-autonomous-flow-matching-christopher-iliffe-sprague-et-al-2024>(51/65 | 106/248) Stable Autonomous Flow Matching (Christopher Iliffe Sprague et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Christopher Iliffe Sprague, Arne Elofsson, Hossein Azizpour. (2024)<br><strong>Stable Autonomous Flow Matching</strong><br><button class=copy-to-clipboard title="Stable Autonomous Flow Matching" index=106>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-106 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs-SY, cs.LG, eess-SY<br>Keyword Score: 10<br>Keywords: In-context Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05774v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05774v1.pdf filename=2402.05774v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In contexts where data samples represent a physically stable state, it is often assumed that the data points represent the local minima of an energy landscape. In control theory, it is well-known that energy can serve as an effective Lyapunov function. Despite this, connections between control theory and generative models in the literature are sparse, even though there are several machine learning applications with physically stable data points. In this paper, we focus on such data and a recent class of deep generative models called flow matching. We apply tools of stochastic stability for time-independent systems to flow matching models. In doing so, we characterize the space of flow matching models that are amenable to this treatment, as well as draw connections to other control theory principles. We demonstrate our theoretical results on two examples.</p></p class="citation"></blockquote><h3 id=5265--107248-is-adversarial-training-with-compressed-datasets-effective-tong-chen-et-al-2024>(52/65 | 107/248) Is Adversarial Training with Compressed Datasets Effective? (Tong Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tong Chen, Raghavendra Selvan. (2024)<br><strong>Is Adversarial Training with Compressed Datasets Effective?</strong><br><button class=copy-to-clipboard title="Is Adversarial Training with Compressed Datasets Effective?" index=107>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-107 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Adversarial Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05675v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05675v1.pdf filename=2402.05675v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Dataset Condensation (DC) refers to the recent class of dataset compression methods that generate a smaller, synthetic, dataset from a larger dataset. This synthetic dataset retains the essential information of the original dataset, enabling models trained on it to achieve performance levels comparable to those trained on the full dataset. Most current DC methods have mainly concerned with achieving high test performance with limited data budget, and have not directly addressed the question of <b>adversarial</b> <b>robustness.</b> In this work, we investigate the impact of <b>adversarial</b> <b>robustness</b> on models trained with compressed datasets. We show that the compressed datasets obtained from DC methods are not effective in transferring <b>adversarial</b> <b>robustness</b> to models. As a solution to improve dataset compression efficiency and <b>adversarial</b> <b>robustness</b> simultaneously, we propose a novel robustness-aware dataset compression method based on finding the Minimal Finite Covering (MFC) of the dataset. The proposed method is (1) obtained by one-time computation and is applicable for any model, (2) more effective than DC methods when applying <b>adversarial</b> <b>training</b> over MFC, (3) provably robust by minimizing the generalized <b>adversarial</b> <b>loss.</b> Additionally, empirical evaluation on three datasets shows that the proposed method is able to achieve better robustness and performance trade-off compared to DC methods such as distribution matching.</p></p class="citation"></blockquote><h3 id=5365--108248-sωi-score-based-o-information-estimation-mustapha-bounoua-et-al-2024>(53/65 | 108/248) S$Ω$I: Score-based O-INFORMATION Estimation (Mustapha Bounoua et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mustapha Bounoua, Giulio Franzese, Pietro Michiardi. (2024)<br><strong>S$Ω$I: Score-based O-INFORMATION Estimation</strong><br><button class=copy-to-clipboard title="S$Ω$I: Score-based O-INFORMATION Estimation" index=108>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-108 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-IT, cs-LG, cs.LG, math-IT<br>Keyword Score: 10<br>Keywords: Mutual Information<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05667v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05667v1.pdf filename=2402.05667v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The analysis of scientific data and complex multivariate systems requires information quantities that capture relationships among multiple random variables. Recently, new information-theoretic measures have been developed to overcome the shortcomings of classical ones, such as <b>mutual</b> <b>information,</b> that are restricted to considering pairwise interactions. Among them, the concept of information synergy and redundancy is crucial for understanding the high-order dependencies between variables. One of the most prominent and versatile measures based on this concept is O-information, which provides a clear and scalable way to quantify the synergy-redundancy balance in multivariate systems. However, its practical application is limited to simplified cases. In this work, we introduce S$\Omega$I, which allows for the first time to compute O-information without restrictive assumptions about the system. Our experiments validate our approach on synthetic data, and demonstrate the effectiveness of S$\Omega$I in the context of a real-world use case.</p></p class="citation"></blockquote><h3 id=5465--109248-determining-the-severity-of-parkinsons-disease-in-patients-using-a-multi-task-neural-network-maría-teresa-garcía-ordás-et-al-2024>(54/65 | 109/248) Determining the severity of Parkinson&rsquo;s disease in patients using a multi task neural network (María Teresa García-Ordás et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>María Teresa García-Ordás, José Alberto Benítez-Andrades, Jose Aveleira-Mata, José-Manuel Alija-Pérez, Carmen Benavides. (2024)<br><strong>Determining the severity of Parkinson&rsquo;s disease in patients using a multi task neural network</strong><br><button class=copy-to-clipboard title="Determining the severity of Parkinson's disease in patients using a multi task neural network" index=109>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-109 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs-SD, cs.LG, eess-AS<br>Keyword Score: 10<br>Keywords: Autoencoder<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05491v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05491v1.pdf filename=2402.05491v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Parkinson&rsquo;s disease is easy to diagnose when it is advanced, but it is very difficult to diagnose in its early stages. Early diagnosis is essential to be able to treat the symptoms. It impacts on daily activities and reduces the quality of life of both the patients and their families and it is also the second most prevalent neurodegenerative disorder after Alzheimer in people over the age of 60. Most current studies on the prediction of Parkinson&rsquo;s severity are carried out in advanced stages of the disease. In this work, the study analyzes a set of variables that can be easily extracted from voice analysis, making it a very non-intrusive technique. In this paper, a method based on different deep learning techniques is proposed with two purposes. On the one hand, to find out if a person has severe or non-severe Parkinson&rsquo;s disease, and on the other hand, to determine by means of regression techniques the degree of evolution of the disease in a given patient. The UPDRS (Unified Parkinson&rsquo;s Disease Rating Scale) has been used by taking into account both the motor and total labels, and the best results have been obtained using a mixed multi-layer perceptron (MLP) that classifies and regresses at the same time and the most important features of the data obtained are taken as input, using an <b>autoencoder.</b> A success rate of 99.15% has been achieved in the problem of predicting whether a person suffers from severe Parkinson&rsquo;s disease or non-severe Parkinson&rsquo;s disease. In the degree of disease involvement prediction problem case, a MSE (Mean Squared Error) of 0.15 has been obtained. Using a full deep learning pipeline for data preprocessing and classification has proven to be very promising in the field Parkinson&rsquo;s outperforming the state-of-the-art proposals.</p></p class="citation"></blockquote><h3 id=5565--110248-learning-uncertainty-aware-temporally-extended-actions-joongkyu-lee-et-al-2024>(55/65 | 110/248) Learning Uncertainty-Aware Temporally-Extended Actions (Joongkyu Lee et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Joongkyu Lee, Seung Joon Park, Yunhao Tang, Min-hwan Oh. (2024)<br><strong>Learning Uncertainty-Aware Temporally-Extended Actions</strong><br><button class=copy-to-clipboard title="Learning Uncertainty-Aware Temporally-Extended Actions" index=110>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-110 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-ML<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05439v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05439v1.pdf filename=2402.05439v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In <b>reinforcement</b> <b>learning,</b> temporal abstraction in the action space, exemplified by action repetition, is a technique to facilitate policy learning through extended actions. However, a primary limitation in previous studies of action repetition is its potential to degrade performance, particularly when sub-optimal actions are repeated. This issue often negates the advantages of action repetition. To address this, we propose a novel algorithm named Uncertainty-aware Temporal Extension (UTE). UTE employs ensemble methods to accurately measure uncertainty during action extension. This feature allows policies to strategically choose between emphasizing exploration or adopting an uncertainty-averse approach, tailored to their specific needs. We demonstrate the effectiveness of UTE through experiments in Gridworld and Atari 2600 environments. Our findings show that UTE outperforms existing action repetition algorithms, effectively mitigating their inherent limitations and significantly enhancing policy learning efficiency.</p></p class="citation"></blockquote><h3 id=5665--111248-version-age-based-client-scheduling-policy-for-federated-learning-xinyi-hu-et-al-2024>(56/65 | 111/248) Version age-based client scheduling policy for federated learning (Xinyi Hu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xinyi Hu, Nikolaos Pappas, Howard H. Yang. (2024)<br><strong>Version age-based client scheduling policy for federated learning</strong><br><button class=copy-to-clipboard title="Version age-based client scheduling policy for federated learning" index=111>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-111 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-DC, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Federated Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05407v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05407v1.pdf filename=2402.05407v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Federated</b> <b>Learning</b> (FL) has emerged as a privacy-preserving machine learning paradigm facilitating collaborative training across multiple clients without sharing local data. Despite advancements in edge device capabilities, communication bottlenecks present challenges in aggregating a large number of clients; only a portion of the clients can update their parameters upon each global aggregation. This phenomenon introduces the critical challenge of stragglers in FL and the profound impact of client scheduling policies on global model convergence and stability. Existing scheduling strategies address staleness but predominantly focus on either timeliness or content. Motivated by this, we introduce the novel concept of Version Age of Information (VAoI) to FL. Unlike traditional Age of Information metrics, VAoI considers both timeliness and content staleness. Each client&rsquo;s version age is updated discretely, indicating the freshness of information. VAoI is incorporated into the client scheduling policy to minimize the average VAoI, mitigating the impact of outdated local updates and enhancing the stability of FL systems.</p></p class="citation"></blockquote><h3 id=5765--112248-attention-as-robust-representation-for-time-series-forecasting-peisong-niu-et-al-2024>(57/65 | 112/248) Attention as Robust Representation for Time Series Forecasting (PeiSong Niu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>PeiSong Niu, Tian Zhou, Xue Wang, Liang Sun, Rong Jin. (2024)<br><strong>Attention as Robust Representation for Time Series Forecasting</strong><br><button class=copy-to-clipboard title="Attention as Robust Representation for Time Series Forecasting" index=112>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-112 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05370v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05370v1.pdf filename=2402.05370v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Time series forecasting is essential for many practical applications, with the adoption of <b>transformer-based</b> models on the rise due to their impressive performance in NLP and CV. <b>Transformers&rsquo;</b> key feature, the attention mechanism, dynamically fusing embeddings to enhance data representation, often relegating attention weights to a byproduct role. Yet, time series data, characterized by noise and non-stationarity, poses significant forecasting challenges. Our approach elevates attention weights as the primary representation for time series, capitalizing on the temporal relationships among data points to improve forecasting accuracy. Our study shows that an attention map, structured using global landmarks and local windows, acts as a robust kernel representation for data points, withstanding noise and shifts in distribution. Our method outperforms state-of-the-art models, reducing mean squared error (MSE) in multivariate time series forecasting by a notable 3.6% without altering the core neural network architecture. It serves as a versatile component that can readily replace recent patching based embedding schemes in <b>transformer-based</b> models, boosting their performance.</p></p class="citation"></blockquote><h3 id=5865--113248-revisiting-early-learning-regularization-when-federated-learning-meets-noisy-labels-taehyeon-kim-et-al-2024>(58/65 | 113/248) Revisiting Early-Learning Regularization When Federated Learning Meets Noisy Labels (Taehyeon Kim et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Taehyeon Kim, Donggyu Kim, Se-Young Yun. (2024)<br><strong>Revisiting Early-Learning Regularization When Federated Learning Meets Noisy Labels</strong><br><button class=copy-to-clipboard title="Revisiting Early-Learning Regularization When Federated Learning Meets Noisy Labels" index=113>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-113 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-DC, cs-LG, cs.LG<br>Keyword Score: 10<br>Keywords: Federated Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05353v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05353v1.pdf filename=2402.05353v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the evolving landscape of <b>federated</b> <b>learning</b> (FL), addressing label noise presents unique challenges due to the decentralized and diverse nature of data collection across clients. Traditional centralized learning approaches to mitigate label noise are constrained in FL by privacy concerns and the heterogeneity of client data. This paper revisits early-learning regularization, introducing an innovative strategy, <b>Federated</b> <b>Label-mixture</b> Regularization (FLR). FLR adeptly adapts to FL&rsquo;s complexities by generating new pseudo labels, blending local and global model predictions. This method not only enhances the accuracy of the global model in both i.i.d. and non-i.i.d. settings but also effectively counters the memorization of noisy labels. Demonstrating compatibility with existing label noise and FL techniques, FLR paves the way for improved generalization in FL environments fraught with label inaccuracies.</p></p class="citation"></blockquote><h3 id=5965--114248-sharp-rates-in-dependent-learning-theory-avoiding-sample-size-deflation-for-the-square-loss-ingvar-ziemann-et-al-2024>(59/65 | 114/248) Sharp Rates in Dependent Learning Theory: Avoiding Sample Size Deflation for the Square Loss (Ingvar Ziemann et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ingvar Ziemann, Stephen Tu, George J. Pappas, Nikolai Matni. (2024)<br><strong>Sharp Rates in Dependent Learning Theory: Avoiding Sample Size Deflation for the Square Loss</strong><br><button class=copy-to-clipboard title="Sharp Rates in Dependent Learning Theory: Avoiding Sample Size Deflation for the Square Loss" index=114>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-114 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG, stat-ML<br>Keyword Score: 8<br>Keywords: Sample Size, Square Loss<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05928v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05928v1.pdf filename=2402.05928v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this work, we study statistical learning with dependent ($\beta$-mixing) data and <b>square</b> <b>loss</b> in a hypothesis class $\mathscr{F}\subset L_{\Psi_p}$ where $\Psi_p$ is the norm $|f|<em>{\Psi_p} \triangleq \sup</em>{m\geq 1} m^{-1/p} |f|<em>{L^m} $ for some $p\in [2,\infty]$. Our inquiry is motivated by the search for a sharp noise interaction term, or variance proxy, in learning with dependent data. Absent any realizability assumption, typical non-asymptotic results exhibit variance proxies that are deflated \emph{multiplicatively} by the mixing time of the underlying covariates process. We show that whenever the topologies of $L^2$ and $\Psi_p$ are comparable on our hypothesis class $\mathscr{F}$ &ndash; that is, $\mathscr{F}$ is a weakly sub-Gaussian class: $|f|</em>{\Psi_p} \lesssim |f|_{L^2}^\eta$ for some $\eta\in (0,1]$ &ndash; the empirical risk minimizer achieves a rate that only depends on the complexity of the class and second order statistics in its leading term. Our result holds whether the problem is realizable or not and we refer to this as a \emph{near mixing-free rate}, since direct dependence on mixing is relegated to an additive higher order term. We arrive at our result by combining the above notion of a weakly sub-Gaussian class with mixed tail generic chaining. This combination allows us to compute sharp, instance-optimal rates for a wide range of problems. %Our approach, reliant on mixed tail generic chaining, allows us to obtain sharp, instance-optimal rates. Examples that satisfy our framework include sub-Gaussian linear regression, more general smoothly parameterized function classes, finite hypothesis classes, and bounded smoothness classes.</p></p class="citation"></blockquote><h3 id=6065--115248-function-aligned-regression-a-method-explicitly-learns-functional-derivatives-from-data-dixian-zhu-et-al-2024>(60/65 | 115/248) Function Aligned Regression: A Method Explicitly Learns Functional Derivatives from Data (Dixian Zhu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dixian Zhu, Livnat Jerby-Arnon. (2024)<br><strong>Function Aligned Regression: A Method Explicitly Learns Functional Derivatives from Data</strong><br><button class=copy-to-clipboard title="Function Aligned Regression: A Method Explicitly Learns Functional Derivatives from Data" index=115>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-115 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs.LG<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.06104v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.06104v1.pdf filename=2402.06104v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Regression is a fundamental task in machine learning that has garnered extensive attention over the past decades. The conventional approach for regression involves employing loss functions that primarily concentrate on aligning model prediction with the ground truth for each individual data sample, which, as we show, can result in sub-optimal prediction of the relationships between the different samples. Recent research endeavors have introduced novel perspectives by incorporating label similarity information to regression. However, a notable gap persists in these approaches when it comes to fully capturing the intricacies of the underlying ground truth function. In this work, we propose FAR (Function Aligned Regression) as a arguably better and more efficient solution to fit the underlying function of ground truth by capturing functional derivatives. We demonstrate the effectiveness of the proposed method practically on 2 synthetic datasets and on 8 extensive real-world tasks from 6 <b>benchmark</b> datasets with other 8 competitive baselines. The code is open-sourced at \url{https://github.com/DixianZhu/FAR}.</p></p class="citation"></blockquote><h3 id=6165--116248-contrastive-approach-to-prior-free-positive-unlabeled-learning-anish-acharya-et-al-2024>(61/65 | 116/248) Contrastive Approach to Prior Free Positive Unlabeled Learning (Anish Acharya et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Anish Acharya, Sujay Sanghavi. (2024)<br><strong>Contrastive Approach to Prior Free Positive Unlabeled Learning</strong><br><button class=copy-to-clipboard title="Contrastive Approach to Prior Free Positive Unlabeled Learning" index=116>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-116 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-CV, cs-LG, cs.LG<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.06038v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.06038v1.pdf filename=2402.06038v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Positive Unlabeled (PU) learning refers to the task of learning a binary classifier given a few labeled positive samples, and a set of unlabeled samples (which could be positive or negative). In this paper, we propose a novel PU learning framework, that starts by learning a feature space through pretext-invariant representation learning and then applies pseudo-labeling to the unlabeled examples, leveraging the concentration property of the embeddings. Overall, our proposed approach handily outperforms state-of-the-art PU learning methods across several standard PU <b>benchmark</b> datasets, while not requiring a-priori knowledge or estimate of class prior. Remarkably, our method remains effective even when labeled data is scant, where most PU learning algorithms falter. We also provide simple theoretical analysis motivating our proposed algorithms and establish generalization guarantee for our approach.</p></p class="citation"></blockquote><h3 id=6265--117248-off-policy-distributional-qλ-distributional-rl-without-importance-sampling-yunhao-tang-et-al-2024>(62/65 | 117/248) Off-policy Distributional Q($λ$): Distributional RL without Importance Sampling (Yunhao Tang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yunhao Tang, Mark Rowland, Rémi Munos, Bernardo Ávila Pires, Will Dabney. (2024)<br><strong>Off-policy Distributional Q($λ$): Distributional RL without Importance Sampling</strong><br><button class=copy-to-clipboard title="Off-policy Distributional Q($λ$): Distributional RL without Importance Sampling" index=117>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-117 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05766v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05766v1.pdf filename=2402.05766v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce off-policy distributional Q($\lambda$), a new addition to the family of off-policy distributional evaluation algorithms. Off-policy distributional Q($\lambda$) does not apply importance sampling for off-policy learning, which introduces intriguing interactions with signed measures. Such unique properties distributional Q($\lambda$) from other existing alternatives such as distributional Retrace. We characterize the algorithmic properties of distributional Q($\lambda$) and validate theoretical insights with tabular experiments. We show how distributional Q($\lambda$)-C51, a combination of Q($\lambda$) with the C51 agent, exhibits promising results on deep RL <b>benchmarks.</b></p></p class="citation"></blockquote><h3 id=6365--118248-digital-computers-break-the-curse-of-dimensionality-adaptive-bounds-via-finite-geometry-anastasis-kratsios-et-al-2024>(63/65 | 118/248) Digital Computers Break the Curse of Dimensionality: Adaptive Bounds via Finite Geometry (Anastasis Kratsios et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Anastasis Kratsios, A. Martina Neuman, Gudmund Pammer. (2024)<br><strong>Digital Computers Break the Curse of Dimensionality: Adaptive Bounds via Finite Geometry</strong><br><button class=copy-to-clipboard title="Digital Computers Break the Curse of Dimensionality: Adaptive Bounds via Finite Geometry" index=118>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-118 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-LG, cs.LG<br>Keyword Score: 3<br>Keywords: Sample Size<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05576v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05576v1.pdf filename=2402.05576v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Many of the foundations of machine learning rely on the idealized premise that all input and output spaces are infinite, e.g.~$\mathbb{R}^d$. This core assumption is systematically violated in practice due to digital computing limitations from finite machine precision, rounding, and limited RAM. In short, digital computers operate on finite grids in $\mathbb{R}^d$. By exploiting these discrete structures, we show the curse of dimensionality in statistical learning is systematically broken when models are implemented on real computers. Consequentially, we obtain new generalization bounds with dimension-free rates for kernel and deep ReLU MLP regressors, which are implemented on real-world machines. Our results are derived using a new non-asymptotic concentration of measure result between a probability measure over any finite metric space and its empirical version associated with $N$ i.i.d. <b>samples</b> <b>when</b> measured in the $1$-Wasserstein distance. Unlike standard concentration of measure results, the concentration rates in our bounds do not hold uniformly for all <b>sample</b> <b>sizes</b> $N$; instead, our rates can adapt to any given $N$. This yields significantly tighter bounds for realistic <b>sample</b> <b>sizes</b> while achieving the optimal worst-case rate of $\mathcal{O}(1/N^{1/2})$ for massive. Our results are built on new techniques combining metric embedding theory with optimal transport</p></p class="citation"></blockquote><h3 id=6465--119248-difftop-differentiable-trajectory-optimization-for-deep-reinforcement-and-imitation-learning-weikang-wan-et-al-2024>(64/65 | 119/248) DiffTOP: Differentiable Trajectory Optimization for Deep Reinforcement and Imitation Learning (Weikang Wan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Weikang Wan, Yufei Wang, Zackory Erickson, David Held. (2024)<br><strong>DiffTOP: Differentiable Trajectory Optimization for Deep Reinforcement and Imitation Learning</strong><br><button class=copy-to-clipboard title="DiffTOP: Differentiable Trajectory Optimization for Deep Reinforcement and Imitation Learning" index=119>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-119 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.LG<br>Categories: cs-AI, cs-LG, cs-RO, cs.LG<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05421v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05421v1.pdf filename=2402.05421v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper introduces DiffTOP, which utilizes Differentiable Trajectory OPtimization as the policy representation to generate actions for deep reinforcement and imitation learning. Trajectory optimization is a powerful and widely used algorithm in control, parameterized by a cost and a dynamics function. The key to our approach is to leverage the recent progress in differentiable trajectory optimization, which enables computing the gradients of the loss with respect to the parameters of trajectory optimization. As a result, the cost and dynamics functions of trajectory optimization can be learned end-to-end. DiffTOP addresses the ``objective mismatch&rsquo;&rsquo; issue of prior model-based RL algorithms, as the dynamics model in DiffTOP is learned to directly maximize task performance by differentiating the policy gradient loss through the trajectory optimization process. We further <b>benchmark</b> DiffTOP for imitation learning on standard robotic manipulation task suites with high-dimensional sensory observations and compare our method to feed-forward policy classes as well as Energy-Based Models (EBM) and Diffusion. Across 15 model-based RL tasks and 13 imitation learning tasks with high-dimensional image and point cloud inputs, DiffTOP outperforms prior state-of-the-art methods in both domains.</p></p class="citation"></blockquote><h2 id=cscv-36>cs.CV (36)</h2><h3 id=036--120248-question-aware-vision-transformer-for-multimodal-reasoning-roy-ganz-et-al-2024>(0/36 | 120/248) Question Aware Vision Transformer for Multimodal Reasoning (Roy Ganz et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Roy Ganz, Yair Kittenplon, Aviad Aberdam, Elad Ben Avraham, Oren Nuriel, Shai Mazor, Ron Litman. (2024)<br><strong>Question Aware Vision Transformer for Multimodal Reasoning</strong><br><button class=copy-to-clipboard title="Question Aware Vision Transformer for Multimodal Reasoning" index=120>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-120 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 76<br>Keywords: Multi-modal, Multi-modal, Transformer, Question Answering, Reasoning, Large Language Model, Large Language Model, Vision Transformer, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05472v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05472v1.pdf filename=2402.05472v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Vision-Language</b> <b>(VL)</b> models have gained significant research focus, enabling remarkable advances in <b>multimodal</b> <b>reasoning.</b> These architectures typically comprise a <b>vision</b> <b>encoder,</b> a <b>Large</b> <b>Language</b> <b>Model</b> <b>(LLM),</b> and a projection module that aligns visual features with the <b>LLM&rsquo;s</b> representation space. Despite their success, a critical limitation persists: the <b>vision</b> <b>encoding</b> process remains decoupled from user queries, often in the form of image-related questions. Consequently, the resulting visual features may not be optimally attuned to the query-specific elements of the image. To address this, we introduce <b>QA-ViT,</b> a Question Aware <b>Vision</b> <b>Transformer</b> approach for <b>multimodal</b> <b>reasoning,</b> which embeds question awareness directly within the <b>vision</b> <b>encoder.</b> This integration results in dynamic visual features focusing on relevant image aspects to the posed question. <b>QA-ViT</b> is model-agnostic and can be incorporated efficiently into any VL architecture. Extensive experiments demonstrate the effectiveness of applying our method to various <b>multimodal</b> architectures, leading to consistent improvement across diverse tasks and showcasing its potential for enhancing visual and scene-text understanding.</p></p class="citation"></blockquote><h3 id=136--121248-cic-a-framework-for-culturally-aware-image-captioning-youngsik-yun-et-al-2024>(1/36 | 121/248) CIC: A framework for Culturally-aware Image Captioning (Youngsik Yun et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Youngsik Yun, Jihie Kim. (2024)<br><strong>CIC: A framework for Culturally-aware Image Captioning</strong><br><button class=copy-to-clipboard title="CIC: A framework for Culturally-aware Image Captioning" index=121>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-121 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CL, cs-CV, cs.CV<br>Keyword Score: 70<br>Keywords: Question Answering, Visual Question Answering, Visual Question Answering, Large Language Model, Large Language Model, Prompt, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05374v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05374v1.pdf filename=2402.05374v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Image Captioning generates descriptive sentences from images using <b>Vision-Language</b> Pre-trained models (VLPs) such as BLIP, which has improved greatly. However, current methods lack the generation of detailed descriptive captions for the cultural elements depicted in the images, such as the traditional clothing worn by people from Asian cultural groups. In this paper, we propose a new framework, \textbf{Culturally-aware Image Captioning (CIC)}, that generates captions and describes cultural elements extracted from cultural <b>visual</b> <b>elements</b> <b>in</b> images representing cultures. Inspired by methods combining <b>visual</b> <b>modality</b> <b>and</b> <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> through appropriate <b>prompts,</b> our framework (1) generates <b>questions</b> <b>based</b> on cultural categories from images, (2) extracts cultural <b>visual</b> <b>elements</b> <b>from</b> <b>Visual</b> <b>Question</b> <b>Answering</b> <b>(VQA)</b> using generated <b>questions,</b> <b>and</b> (3) generates culturally-aware captions using <b>LLMs</b> with the <b>prompts.</b> Our human evaluation conducted on 45 participants from 4 different cultural groups with a high understanding of the corresponding culture shows that our proposed framework generates more culturally descriptive captions when compared to the image captioning baseline based on VLPs. Our code and dataset will be made publicly available upon acceptance.</p></p class="citation"></blockquote><h3 id=236--122248-sphinx-x-scaling-data-and-parameters-for-a-family-of-multi-modal-large-language-models-peng-gao-et-al-2024>(2/36 | 122/248) SPHINX-X: Scaling Data and Parameters for a Family of Multi-modal Large Language Models (Peng Gao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Peng Gao, Renrui Zhang, Chris Liu, Longtian Qiu, Siyuan Huang, Weifeng Lin, Shitian Zhao, Shijie Geng, Ziyi Lin, Peng Jin, Kaipeng Zhang, Wenqi Shao, Chao Xu, Conghui He, Junjun He, Hao Shao, Pan Lu, Hongsheng Li, Yu Qiao. (2024)<br><strong>SPHINX-X: Scaling Data and Parameters for a Family of Multi-modal Large Language Models</strong><br><button class=copy-to-clipboard title="SPHINX-X: Scaling Data and Parameters for a Family of Multi-modal Large Language Models" index=122>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-122 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CL, cs-CV, cs-LG, cs.CV<br>Keyword Score: 52<br>Keywords: Optical Character Recognition, Benchmarking, Benchmarking, Multi-modal, Multi-modal, Large Language Model, Large Language Model, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05935v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05935v1.pdf filename=2402.05935v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We propose SPHINX-X, an extensive Multimodality <b>Large</b> <b>Language</b> <b>Model</b> (MLLM) series developed upon SPHINX. To improve the architecture and training efficiency, we modify the SPHINX framework by removing redundant visual encoders, bypassing fully-padded sub-images with skip tokens, and simplifying multi-stage training into a one-stage all-in-one paradigm. To fully unleash the potential of MLLMs, we assemble a comprehensive multi-domain and <b>multimodal</b> dataset covering publicly available resources in language, vision, and <b>vision-language</b> tasks. We further enrich this collection with our curated <b>OCR</b> intensive and Set-of-Mark datasets, extending the diversity and generality. By training over different base <b>LLMs</b> including TinyLlama1.1B, InternLM2-7B, LLaMA2-13B, and Mixtral8x7B, we obtain a spectrum of MLLMs that vary in parameter size and multilingual capabilities. Comprehensive <b>benchmarking</b> reveals a strong correlation between the <b>multi-modal</b> performance with the data and parameter scales. Code and models are released at <a href=https://github.com/Alpha-VLLM/LLaMA2-Accessory>https://github.com/Alpha-VLLM/LLaMA2-Accessory</a></p></p class="citation"></blockquote><h3 id=336--123248-animated-stickers-bringing-stickers-to-life-with-video-diffusion-david-yan-et-al-2024>(3/36 | 123/248) Animated Stickers: Bringing Stickers to Life with Video Diffusion (David Yan et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>David Yan, Winnie Zhang, Luxin Zhang, Anmol Kalia, Dingkang Wang, Ankit Ramchandani, Miao Liu, Albert Pumarola, Edgar Schoenfeld, Elliot Blanchard, Krishna Narni, Yaqiao Luo, Lawrence Chen, Guan Pang, Ali Thabet, Peter Vajda, Amy Bearman, Licheng Yu. (2024)<br><strong>Animated Stickers: Bringing Stickers to Life with Video Diffusion</strong><br><button class=copy-to-clipboard title="Animated Stickers: Bringing Stickers to Life with Video Diffusion" index=123>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-123 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 50<br>Keywords: Fine-tuning, Knowledge Distillation, human-in-the-loop, Text2image, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.06088v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.06088v1.pdf filename=2402.06088v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce animated stickers, a video diffusion model which generates an animation conditioned on a text <b>prompt</b> and static sticker image. Our model is built on top of the state-of-the-art Emu <b>text-to-image</b> model, with the addition of temporal layers to model motion. Due to the domain gap, i.e. differences in visual and motion style, a model which performed well on generating natural videos can no longer generate vivid videos when applied to stickers. To bridge this gap, we employ a two-stage <b>finetuning</b> pipeline: first with weakly in-domain data, followed by <b>human-in-the-loop</b> (HITL) strategy which we term ensemble-of-teachers. It <b>distills</b> the best qualities of multiple teachers into a smaller student model. We show that this strategy allows us to specifically target improvements to motion quality while maintaining the style from the static image. With inference optimizations, our model is able to generate an eight-frame video with high-quality, interesting, and relevant motion in under one second.</p></p class="citation"></blockquote><h3 id=436--124248-editable-scene-simulation-for-autonomous-driving-via-collaborative-llm-agents-yuxi-wei-et-al-2024>(4/36 | 124/248) Editable Scene Simulation for Autonomous Driving via Collaborative LLM-Agents (Yuxi Wei et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuxi Wei, Zi Wang, Yifan Lu, Chenxin Xu, Changxing Liu, Hao Zhao, Siheng Chen, Yanfeng Wang. (2024)<br><strong>Editable Scene Simulation for Autonomous Driving via Collaborative LLM-Agents</strong><br><button class=copy-to-clipboard title="Editable Scene Simulation for Autonomous Driving via Collaborative LLM-Agents" index=124>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-124 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Simulation, Simulator, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05746v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05746v1.pdf filename=2402.05746v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Scene <b>simulation</b> in autonomous driving has gained significant attention because of its huge potential for generating customized data. However, existing editable scene <b>simulation</b> approaches face limitations in terms of user interaction efficiency, multi-camera photo-realistic rendering and external digital assets integration. To address these challenges, this paper introduces ChatSim, the first system that enables editable photo-realistic 3D driving scene <b>simulations</b> via natural language commands with external digital assets. To enable editing with high command flexibility,~ChatSim leverages a <b>large</b> <b>language</b> <b>model</b> <b>(LLM)</b> agent collaboration framework. To generate photo-realistic outcomes, ChatSim employs a novel multi-camera neural radiance field method. Furthermore, to unleash the potential of extensive high-quality digital assets, ChatSim employs a novel multi-camera lighting estimation method to achieve scene-consistent assets&rsquo; rendering. Our experiments on Waymo Open Dataset demonstrate that ChatSim can handle complex language commands and generate corresponding photo-realistic scene videos.</p></p class="citation"></blockquote><h3 id=536--125248-on-convolutional-vision-transformers-for-yield-prediction-alvin-inderka-et-al-2024>(5/36 | 125/248) On Convolutional Vision Transformers for Yield Prediction (Alvin Inderka et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Alvin Inderka, Florian Huber, Volker Steinhage. (2024)<br><strong>On Convolutional Vision Transformers for Yield Prediction</strong><br><button class=copy-to-clipboard title="On Convolutional Vision Transformers for Yield Prediction" index=125>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-125 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Convolution, Convolutional Neural Network, Transformer, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05557v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05557v1.pdf filename=2402.05557v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>While a variety of methods offer good yield prediction on histogrammed remote sensing data, <b>vision</b> <b>Transformers</b> are only sparsely represented in the literature. The <b>Convolution</b> <b>vision</b> <b>Transformer</b> (CvT) is being tested to evaluate <b>vision</b> <b>Transformers</b> that are currently achieving state-of-the-art results in many other <b>vision</b> <b>tasks.</b> CvT combines some of the advantages of <b>convolution</b> with the advantages of dynamic attention and global context fusion of <b>Transformers.</b> It performs worse than widely tested methods such as XGBoost and <b>CNNs,</b> but shows that <b>Transformers</b> have potential to improve yield prediction.</p></p class="citation"></blockquote><h3 id=636--126248-enhancing-zero-shot-counting-via-language-guided-exemplar-learning-mingjie-wang-et-al-2024>(6/36 | 126/248) Enhancing Zero-shot Counting via Language-guided Exemplar Learning (Mingjie Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mingjie Wang, Jun Zhou, Yong Dai, Eric Buys, Minglun Gong. (2024)<br><strong>Enhancing Zero-shot Counting via Language-guided Exemplar Learning</strong><br><button class=copy-to-clipboard title="Enhancing Zero-shot Counting via Language-guided Exemplar Learning" index=126>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-126 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 40<br>Keywords: Zero-shot, Large Language Model, Large Language Model, Zero-shot Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05394v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05394v1.pdf filename=2402.05394v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recently, Class-Agnostic Counting (CAC) problem has garnered increasing attention owing to its intriguing generality and superior efficiency compared to Category-Specific Counting (CSC). This paper proposes a novel ExpressCount to enhance <b>zero-shot</b> <b>object</b> counting by delving deeply into language-guided exemplar learning. Specifically, the ExpressCount is comprised of an innovative Language-oriented Exemplar Perceptron and a downstream visual <b>Zero-shot</b> <b>Counting</b> pipeline. Thereinto, the perceptron hammers at exploiting accurate exemplar cues from collaborative language-vision signals by inheriting rich semantic priors from the prevailing pre-trained <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs),</b> whereas the counting pipeline excels in mining fine-grained features through dual-branch and cross-attention schemes, contributing to the high-quality similarity learning. Apart from building a bridge between the <b>LLM</b> in vogue and the visual counting tasks, expression-guided exemplar estimation significantly advances <b>zero-shot</b> <b>learning</b> capabilities for counting instances with arbitrary classes. Moreover, devising a FSC-147-Express with annotations of meticulous linguistic expressions pioneers a new venue for developing and validating language-based counting models. Extensive experiments demonstrate the state-of-the-art performance of our ExpressCount, even showcasing the accuracy on par with partial CSC models.</p></p class="citation"></blockquote><h3 id=736--127248-crema-multimodal-compositional-video-reasoning-via-efficient-modular-adaptation-and-fusion-shoubin-yu-et-al-2024>(7/36 | 127/248) CREMA: Multimodal Compositional Video Reasoning via Efficient Modular Adaptation and Fusion (Shoubin Yu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shoubin Yu, Jaehong Yoon, Mohit Bansal. (2024)<br><strong>CREMA: Multimodal Compositional Video Reasoning via Efficient Modular Adaptation and Fusion</strong><br><button class=copy-to-clipboard title="CREMA: Multimodal Compositional Video Reasoning via Efficient Modular Adaptation and Fusion" index=127>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-127 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CL, cs-CV, cs.CV<br>Keyword Score: 36<br>Keywords: Multi-modal, Multi-modal, Transformer, Reasoning, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05889v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05889v1.pdf filename=2402.05889v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Despite impressive advancements in <b>multimodal</b> compositional <b>reasoning</b> approaches, they are still limited in their flexibility and efficiency by processing fixed modality inputs while updating a lot of model parameters. This paper tackles these critical challenges and proposes CREMA, an efficient and modular modality-fusion framework for injecting any new modality into video <b>reasoning.</b> We first augment multiple informative modalities (such as optical flow, 3D point cloud, audio) from given videos without extra human annotation by leveraging existing pre-trained models. Next, we introduce a query <b>transformer</b> with multiple parameter-efficient modules associated with each accessible modality. It projects diverse modality features to the <b>LLM</b> token embedding space, allowing the model to integrate different data types for response generation. Furthermore, we propose a fusion module designed to compress <b>multimodal</b> queries, maintaining computational efficiency in the <b>LLM</b> while combining additional modalities. We validate our method on video-3D, video-audio, and video-language <b>reasoning</b> tasks and achieve better/equivalent performance against strong <b>multimodal</b> <b>LLMs,</b> including BLIP-2, 3D-LLM, and SeViLA while using 96% fewer trainable parameters. We provide extensive analyses of CREMA, including the impact of each modality on <b>reasoning</b> domains, the design of the fusion module, and example visualizations.</p></p class="citation"></blockquote><h3 id=836--128248-memory-efficient-vision-transformers-an-activation-aware-mixed-rank-compression-strategy-seyedarmin-azizi-et-al-2024>(8/36 | 128/248) Memory-Efficient Vision Transformers: An Activation-Aware Mixed-Rank Compression Strategy (Seyedarmin Azizi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Seyedarmin Azizi, Mahdi Nazemi, Massoud Pedram. (2024)<br><strong>Memory-Efficient Vision Transformers: An Activation-Aware Mixed-Rank Compression Strategy</strong><br><button class=copy-to-clipboard title="Memory-Efficient Vision Transformers: An Activation-Aware Mixed-Rank Compression Strategy" index=128>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-128 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV, stat-ML<br>Keyword Score: 33<br>Keywords: Benchmarking, Model Compression, Transformer, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.06004v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.06004v1.pdf filename=2402.06004v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>As <b>Vision</b> <b>Transformers</b> (ViTs) increasingly set new <b>benchmarks</b> in computer <b>vision,</b> <b>their</b> practical deployment on inference engines is often hindered by their significant memory bandwidth and (on-chip) memory footprint requirements. This paper addresses this memory limitation by introducing an activation-aware <b>model</b> <b>compression</b> methodology that uses selective low-rank weight tensor approximations of different layers to reduce the parameter count of ViTs. The key idea is to decompose the weight tensors into a sum of two parameter-efficient tensors while minimizing the error between the product of the input activations with the original weight tensor and the product of the input activations with the approximate tensor sum. This approximation is further refined by adopting an efficient layer-wise error compensation technique that uses the gradient of the layer&rsquo;s output loss. The combination of these techniques achieves excellent results while it avoids being trapped in a shallow local minimum early in the optimization process and strikes a good balance between the <b>model</b> <b>compression</b> and output accuracy. Notably, the presented method significantly reduces the parameter count of DeiT-B by 60% with less than 1% accuracy drop on the ImageNet dataset, overcoming the usual accuracy degradation seen in low-rank approximations. In addition to this, the presented compression technique can compress large DeiT/ViT <b>models</b> <b>to</b> have about the same <b>model</b> <b>size</b> as smaller DeiT/ViT variants while yielding up to 1.8% accuracy gain. These results highlight the efficacy of our approach, presenting a viable solution for embedding ViTs in memory-constrained environments without compromising their performance.</p></p class="citation"></blockquote><h3 id=936--129248-mamba-nd-selective-state-space-modeling-for-multi-dimensional-data-shufan-li-et-al-2024>(9/36 | 129/248) Mamba-ND: Selective State Space Modeling for Multi-Dimensional Data (Shufan Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shufan Li, Harkanwar Singh, Aditya Grover. (2024)<br><strong>Mamba-ND: Selective State Space Modeling for Multi-Dimensional Data</strong><br><button class=copy-to-clipboard title="Mamba-ND: Selective State Space Modeling for Multi-Dimensional Data" index=129>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-129 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 33<br>Keywords: Benchmarking, LSTM, Transformer, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05892v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05892v1.pdf filename=2402.05892v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In recent years, <b>Transformers</b> have become the de-facto architecture for sequence modeling on text and a variety of multi-dimensional data, such as images and video. However, the use of <b>self-attention</b> layers in a <b>Transformer</b> incurs prohibitive compute and memory complexity that scales quadratically w.r.t. the sequence length. A recent architecture, Mamba, based on state space models has been shown to achieve comparable performance for modeling text sequences, while scaling linearly with the sequence length. In this work, we present Mamba-ND, a generalized design extending the Mamba architecture to arbitrary multi-dimensional data. Our design alternatively unravels the input data across different dimensions following row-major orderings. We provide a systematic comparison of Mamba-ND with several other alternatives, based on prior multi-dimensional extensions such as Bi-directional <b>LSTMs</b> and S4ND. Empirically, we show that Mamba-ND demonstrates performance competitive with the state-of-the-art on a variety of multi-dimensional <b>benchmarks,</b> including ImageNet-1K classification, HMDB-51 action recognition, and ERA5 weather forecasting.</p></p class="citation"></blockquote><h3 id=1036--130248-instagen-enhancing-object-detection-by-training-on-synthetic-dataset-chengjian-feng-et-al-2024>(10/36 | 130/248) InstaGen: Enhancing Object Detection by Training on Synthetic Dataset (Chengjian Feng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chengjian Feng, Yujie Zhong, Zequn Jie, Weidi Xie, Lin Ma. (2024)<br><strong>InstaGen: Enhancing Object Detection by Training on Synthetic Dataset</strong><br><button class=copy-to-clipboard title="InstaGen: Enhancing Object Detection by Training on Synthetic Dataset" index=130>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-130 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Object Detection, Grounding, Text Embedding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05937v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05937v1.pdf filename=2402.05937v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we introduce a novel paradigm to enhance the ability of <b>object</b> <b>detector,</b> e.g., expanding categories or improving detection performance, by training on synthetic dataset generated from diffusion models. Specifically, we integrate an instance-level <b>grounding</b> head into a pre-trained, generative diffusion model, to augment it with the ability of localising arbitrary instances in the generated images. The <b>grounding</b> head is trained to align the <b>text</b> <b>embedding</b> of category names with the regional visual feature of the diffusion model, using supervision from an off-the-shelf <b>object</b> <b>detector,</b> and a novel self-training scheme on (novel) categories not covered by the detector. This enhanced version of diffusion model, termed as InstaGen, can serve as a data synthesizer for <b>object</b> <b>detection.</b> We conduct thorough experiments to show that, <b>object</b> <b>detector</b> can be enhanced while training on the synthetic dataset from InstaGen, demonstrating superior performance over existing state-of-the-art methods in open-vocabulary (+4.5 AP) and data-sparse (+1.2 to 5.2 AP) scenarios.</p></p class="citation"></blockquote><h3 id=1136--131248-clicksam-fine-tuning-segment-anything-model-using-click-prompts-for-ultrasound-image-segmentation-aimee-guo-et-al-2024>(11/36 | 131/248) ClickSAM: Fine-tuning Segment Anything Model using click prompts for ultrasound image segmentation (Aimee Guo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Aimee Guo, Gace Fei, Hemanth Pasupuletic, Jing Wang. (2024)<br><strong>ClickSAM: Fine-tuning Segment Anything Model using click prompts for ultrasound image segmentation</strong><br><button class=copy-to-clipboard title="ClickSAM: Fine-tuning Segment Anything Model using click prompts for ultrasound image segmentation" index=131>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-131 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV, physics-med-ph<br>Keyword Score: 30<br>Keywords: Fine-tuning, Fine-tuning, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05902v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05902v2.pdf filename=2402.05902v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The newly released Segment Anything Model (SAM) is a popular tool used in image processing due to its superior segmentation accuracy, variety of input <b>prompts,</b> training capabilities, and efficient model design. However, its current model is trained on a diverse dataset not tailored to medical images, particularly ultrasound images. Ultrasound images tend to have a lot of noise, making it difficult to segment out important structures. In this project, we developed ClickSAM, which <b>fine-tunes</b> the Segment Anything Model using click <b>prompts</b> for ultrasound images. ClickSAM has two stages of training: the first stage is trained on single-click <b>prompts</b> centered in the ground-truth contours, and the second stage focuses on improving the model performance through additional positive and negative click <b>prompts.</b> By comparing the first stage predictions to the ground-truth masks, true positive, false positive, and false negative segments are calculated. Positive clicks are generated using the true positive and false negative segments, and negative clicks are generated using the false positive segments. The Centroidal Voronoi Tessellation algorithm is then employed to collect positive and negative click <b>prompts</b> in each segment that are used to enhance the model performance during the second stage of training. With click-train methods, ClickSAM exhibits superior performance compared to other existing models for ultrasound image segmentation.</p></p class="citation"></blockquote><h3 id=1236--132248-memory-consolidation-enables-long-context-video-understanding-ivana-balažević-et-al-2024>(12/36 | 132/248) Memory Consolidation Enables Long-Context Video Understanding (Ivana Balažević et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ivana Balažević, Yuge Shi, Pinelopi Papalampidi, Rahma Chaabouni, Skanda Koppula, Olivier J. Hénaff. (2024)<br><strong>Memory Consolidation Enables Long-Context Video Understanding</strong><br><button class=copy-to-clipboard title="Memory Consolidation Enables Long-Context Video Understanding" index=132>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-132 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Fine-tuning, Transformer, Vision Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05861v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05861v1.pdf filename=2402.05861v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Most <b>transformer-based</b> video encoders are limited to short temporal contexts due to their quadratic complexity. While various attempts have been made to extend this context, this has often come at the cost of both conceptual and computational complexity. We propose to instead re-purpose existing pre-trained video <b>transformers</b> by simply <b>fine-tuning</b> them to attend to memories derived non-parametrically from past activations. By leveraging redundancy reduction, our memory-consolidated <b>vision</b> <b>transformer</b> (MC-ViT) effortlessly extends its context far into the past and exhibits excellent scaling behavior when learning from longer videos. In doing so, MC-ViT sets a new state-of-the-art in long-context video understanding on EgoSchema, Perception Test, and Diving48, outperforming methods that benefit from orders of magnitude more parameters.</p></p class="citation"></blockquote><h3 id=1336--133248-spiking-neural-network-enhanced-hand-gesture-recognition-using-low-cost-single-photon-avalanche-diode-array-zhenya-zang-et-al-2024>(13/36 | 133/248) Spiking Neural Network Enhanced Hand Gesture Recognition Using Low-Cost Single-photon Avalanche Diode Array (Zhenya Zang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhenya Zang, Xingda Li, David Day Uei Li. (2024)<br><strong>Spiking Neural Network Enhanced Hand Gesture Recognition Using Low-Cost Single-photon Avalanche Diode Array</strong><br><button class=copy-to-clipboard title="Spiking Neural Network Enhanced Hand Gesture Recognition Using Low-Cost Single-photon Avalanche Diode Array" index=133>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-133 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV, eess-IV<br>Keyword Score: 30<br>Keywords: Convolution, Convolutional Neural Network, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05441v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05441v1.pdf filename=2402.05441v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present a compact spiking <b>convolutional</b> <b>neural</b> <b>network</b> (SCNN) and spiking multilayer perceptron (SMLP) to recognize ten different gestures in dark and bright light environments, using a $9.6 single-photon avalanche diode (SPAD) array. In our hand gesture recognition (HGR) system, photon intensity data was leveraged to train and test the network. A vanilla <b>convolutional</b> <b>neural</b> <b>network</b> <b>(CNN)</b> was also implemented to compare the performance of SCNN with the same network topologies and training strategies. Our SCNN was trained from scratch instead of being converted from the <b>CNN.</b> We tested the three models in dark and ambient light (AL)-corrupted environments. The results indicate that SCNN achieves comparable accuracy (90.8%) to <b>CNN</b> (92.9%) and exhibits lower floating operations with only 8 timesteps. SMLP also presents a trade-off between computational workload and accuracy. The code and collected datasets of this work are available at <a href=https://github.com/zzy666666zzy/TinyLiDAR_NET_SNN>https://github.com/zzy666666zzy/TinyLiDAR_NET_SNN</a>.</p></p class="citation"></blockquote><h3 id=1436--134248-task-customized-masked-autoencoder-via-mixture-of-cluster-conditional-experts-zhili-liu-et-al-2024>(14/36 | 134/248) Task-customized Masked AutoEncoder via Mixture of Cluster-conditional Experts (Zhili Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhili Liu, Kai Chen, Jianhua Han, Lanqing Hong, Hang Xu, Zhenguo Li, James T. Kwok. (2024)<br><strong>Task-customized Masked AutoEncoder via Mixture of Cluster-conditional Experts</strong><br><button class=copy-to-clipboard title="Task-customized Masked AutoEncoder via Mixture of Cluster-conditional Experts" index=134>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-134 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-LG, cs.CV<br>Keyword Score: 30<br>Keywords: Autoencoder, Self-supervised Learning, Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05382v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05382v1.pdf filename=2402.05382v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Masked Autoencoder~(MAE) is a prevailing <b>self-supervised</b> <b>learning</b> method that achieves promising results in model pre-training. However, when the various downstream tasks have data distributions different from the pre-training data, the semantically irrelevant pre-training information might result in negative transfer, impeding MAE&rsquo;s scalability. To address this issue, we propose a novel MAE-based pre-training paradigm, Mixture of Cluster-conditional Experts (MoCE), which can be trained once but provides customized pre-training models for diverse downstream tasks. Different from the mixture of experts (MoE), our MoCE trains each expert only with semantically relevant images by using cluster-conditional gates. Thus, each downstream task can be allocated to its customized model pre-trained with data most similar to the downstream data. Experiments on a collection of 11 downstream tasks show that MoCE outperforms the vanilla MAE by 2.45% on average. It also obtains new state-of-the-art <b>self-supervised</b> <b>learning</b> results on detection and segmentation.</p></p class="citation"></blockquote><h3 id=1536--135248-get-what-you-want-not-what-you-dont-image-content-suppression-for-text-to-image-diffusion-models-senmao-li-et-al-2024>(15/36 | 135/248) Get What You Want, Not What You Don&rsquo;t: Image Content Suppression for Text-to-Image Diffusion Models (Senmao Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Senmao Li, Joost van de Weijer, Taihang Hu, Fahad Shahbaz Khan, Qibin Hou, Yaxing Wang, Jian Yang. (2024)<br><strong>Get What You Want, Not What You Don&rsquo;t: Image Content Suppression for Text-to-Image Diffusion Models</strong><br><button class=copy-to-clipboard title="Get What You Want, Not What You Don't: Image Content Suppression for Text-to-Image Diffusion Models" index=135>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-135 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 30<br>Keywords: Text2image, Prompt, Text Embedding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05375v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05375v1.pdf filename=2402.05375v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The success of recent <b>text-to-image</b> <b>diffusion</b> models is largely due to their capacity to be guided by a complex <b>text</b> <b>prompt,</b> which enables users to precisely describe the desired content. However, these models struggle to effectively suppress the generation of undesired content, which is explicitly requested to be omitted from the generated image in the <b>prompt.</b> In this paper, we analyze how to manipulate the <b>text</b> <b>embeddings</b> and remove unwanted content from them. We introduce two contributions, which we refer to as $\textit{soft-weighted regularization}$ and $\textit{inference-time <b>text</b> <b>embedding</b> optimization}$. The first regularizes the <b>text</b> <b>embedding</b> matrix and effectively suppresses the undesired content. The second method aims to further suppress the unwanted content generation of the <b>prompt,</b> and encourages the generation of desired content. We evaluate our method quantitatively and qualitatively on extensive experiments, validating its effectiveness. Furthermore, our method is generalizability to both the pixel-space diffusion models (i.e. DeepFloyd-IF) and the latent-space diffusion models (i.e. Stable Diffusion).</p></p class="citation"></blockquote><h3 id=1636--136248-early-fusion-of-features-for-semantic-segmentation-anupam-gupta-et-al-2024>(16/36 | 136/248) Early Fusion of Features for Semantic Segmentation (Anupam Gupta et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Anupam Gupta, Ashok Krishnamurthy, Lisa Singh. (2024)<br><strong>Early Fusion of Features for Semantic Segmentation</strong><br><button class=copy-to-clipboard title="Early Fusion of Features for Semantic Segmentation" index=136>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-136 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 23<br>Keywords: Benchmarking, Convolution, Fine-tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.06091v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.06091v1.pdf filename=2402.06091v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper introduces a novel segmentation framework that integrates a classifier network with a reverse HRNet architecture for efficient image segmentation. Our approach utilizes a ResNet-50 backbone, pretrained in a semi-supervised manner, to generate feature maps at various scales. These maps are then processed by a reverse HRNet, which is adapted to handle varying channel dimensions through 1x1 <b>convolutions,</b> to produce the final segmentation output. We strategically avoid <b>fine-tuning</b> the backbone network to minimize memory consumption during training. Our methodology is rigorously tested across several <b>benchmark</b> datasets including Mapillary Vistas, Cityscapes, CamVid, COCO, and PASCAL-VOC2012, employing metrics such as pixel accuracy and mean Intersection over Union (mIoU) to evaluate segmentation performance. The results demonstrate the effectiveness of our proposed model in achieving high segmentation accuracy, indicating its potential for various applications in image analysis. By leveraging the strengths of both the ResNet-50 and reverse HRNet within a unified framework, we present a robust solution to the challenges of image segmentation.</p></p class="citation"></blockquote><h3 id=1736--137248-point-vos-pointing-up-video-object-segmentation-idil-esen-zulfikar-et-al-2024>(17/36 | 137/248) Point-VOS: Pointing Up Video Object Segmentation (Idil Esen Zulfikar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Idil Esen Zulfikar, Sabarinath Mahadevan, Paul Voigtlaender, Bastian Leibe. (2024)<br><strong>Point-VOS: Pointing Up Video Object Segmentation</strong><br><button class=copy-to-clipboard title="Point-VOS: Pointing Up Video Object Segmentation" index=137>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-137 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 23<br>Keywords: Benchmarking, Grounding, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05917v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05917v1.pdf filename=2402.05917v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Current state-of-the-art Video Object Segmentation (VOS) methods rely on dense per-object mask annotations both during training and testing. This requires time-consuming and costly video annotation mechanisms. We propose a novel Point-VOS task with a spatio-temporally sparse point-wise annotation scheme that substantially reduces the annotation effort. We apply our annotation scheme to two large-scale video datasets with text descriptions and annotate over 19M points across 133K objects in 32K videos. Based on our annotations, we propose a new Point-VOS <b>benchmark,</b> and a corresponding point-based training mechanism, which we use to establish strong baseline results. We show that existing VOS methods can easily be adapted to leverage our point annotations during training, and can achieve results close to the fully-supervised performance when trained on pseudo-masks generated from these points. In addition, we show that our data can be used to improve models that connect vision and language, by evaluating it on the Video Narrative <b>Grounding</b> (VNG) task. We will make our code and annotations available at <a href=https://pointvos.github.io>https://pointvos.github.io</a>.</p></p class="citation"></blockquote><h3 id=1836--138248-avatarmmc-3d-head-avatar-generation-and-editing-with-multi-modal-conditioning-wamiq-reyaz-para-et-al-2024>(18/36 | 138/248) AvatarMMC: 3D Head Avatar Generation and Editing with Multi-Modal Conditioning (Wamiq Reyaz Para et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wamiq Reyaz Para, Abdelrahman Eldesokey, Zhenyu Li, Pradyumna Reddy, Jiankang Deng, Peter Wonka. (2024)<br><strong>AvatarMMC: 3D Head Avatar Generation and Editing with Multi-Modal Conditioning</strong><br><button class=copy-to-clipboard title="AvatarMMC: 3D Head Avatar Generation and Editing with Multi-Modal Conditioning" index=138>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-138 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-GR, cs.CV<br>Keyword Score: 23<br>Keywords: Generative Adversarial Network, Generative Adversarial Network, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05803v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05803v1.pdf filename=2402.05803v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce an approach for 3D head avatar generation and editing with <b>multi-modal</b> conditioning based on a 3D <b>Generative</b> <b>Adversarial</b> <b>Network</b> <b>(GAN)</b> and a Latent Diffusion Model (LDM). 3D <b>GANs</b> can generate high-quality head avatars given a single or no condition. However, it is challenging to generate samples that adhere to multiple conditions of different modalities. On the other hand, LDMs excel at learning complex conditional distributions. To this end, we propose to exploit the conditioning capabilities of LDMs to enable <b>multi-modal</b> control over the latent space of a pre-trained 3D <b>GAN.</b> Our method can generate and edit 3D head avatars given a mixture of control signals such as RGB input, segmentation masks, and global attributes. This provides better control over the generation and editing of synthetic avatars both globally and locally. Experiments show that our proposed approach outperforms a solely <b>GAN-based</b> approach both qualitatively and quantitatively on generation and editing tasks. To the best of our knowledge, our approach is the first to introduce <b>multi-modal</b> conditioning to 3D avatar generation and editing. \href{avatarmmc-sig24.github.io}{Project Page}</p></p class="citation"></blockquote><h3 id=1936--139248-scalable-diffusion-models-with-state-space-backbone-zhengcong-fei-et-al-2024>(19/36 | 139/248) Scalable Diffusion Models with State Space Backbone (Zhengcong Fei et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhengcong Fei, Mingyuan Fan, Changqian Yu, Junshi Huang. (2024)<br><strong>Scalable Diffusion Models with State Space Backbone</strong><br><button class=copy-to-clipboard title="Scalable Diffusion Models with State Space Backbone" index=139>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-139 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-MM, cs.CV<br>Keyword Score: 23<br>Keywords: Benchmarking, Convolutional Neural Network, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05608v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05608v1.pdf filename=2402.05608v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents a new exploration into a category of diffusion models built upon state space architecture. We endeavor to train diffusion models for image data, wherein the traditional U-Net backbone is supplanted by a state space backbone, functioning on raw patches or latent space. Given its notable efficacy in accommodating long-range dependencies, Diffusion State Space Models (DiS) are distinguished by treating all inputs including time, condition, and noisy image patches as tokens. Our assessment of DiS encompasses both unconditional and class-conditional image generation scenarios, revealing that DiS exhibits comparable, if not superior, performance to <b>CNN-based</b> or <b>Transformer-based</b> U-Net architectures of commensurate size. Furthermore, we analyze the scalability of DiS, gauged by the forward pass complexity quantified in Gflops. DiS models with higher Gflops, achieved through augmentation of depth/width or augmentation of input tokens, consistently demonstrate lower FID. In addition to demonstrating commendable scalability characteristics, DiS-H/2 models in latent space achieve performance levels akin to prior diffusion models on class-conditional ImageNet <b>benchmarks</b> at the resolution of 256$\times$256 and 512$\times$512, while significantly reducing the computational burden. The code and models are available at: <a href=https://github.com/feizc/DiS>https://github.com/feizc/DiS</a>.</p></p class="citation"></blockquote><h3 id=2036--140248-privacy-preserving-synthetic-continual-semantic-segmentation-for-robotic-surgery-mengya-xu-et-al-2024>(20/36 | 140/248) Privacy-Preserving Synthetic Continual Semantic Segmentation for Robotic Surgery (Mengya Xu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mengya Xu, Mobarakol Islam, Long Bai, Hongliang Ren. (2024)<br><strong>Privacy-Preserving Synthetic Continual Semantic Segmentation for Robotic Surgery</strong><br><button class=copy-to-clipboard title="Privacy-Preserving Synthetic Continual Semantic Segmentation for Robotic Surgery" index=140>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-140 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Continual Learning, Knowledge Distillation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05860v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05860v1.pdf filename=2402.05860v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Deep Neural Networks (DNNs) based semantic segmentation of the robotic instruments and tissues can enhance the precision of surgical activities in robot-assisted surgery. However, in biological learning, DNNs cannot learn incremental tasks over time and exhibit catastrophic forgetting, which refers to the sharp decline in performance on previously learned tasks after learning a new one. Specifically, when data scarcity is the issue, the model shows a rapid drop in performance on previously learned instruments after learning new data with new instruments. The problem becomes worse when it limits releasing the dataset of the old instruments for the old model due to privacy concerns and the unavailability of the data for the new or updated version of the instruments for the <b>continual</b> <b>learning</b> model. For this purpose, we develop a privacy-preserving synthetic <b>continual</b> <b>semantic</b> segmentation framework by blending and harmonizing (i) open-source old instruments foreground to the synthesized background without revealing real patient data in public and (ii) new instruments foreground to extensively augmented real background. To boost the balanced logit <b>distillation</b> from the old model to the <b>continual</b> <b>learning</b> model, we design overlapping class-aware temperature normalization (CAT) by controlling model learning utility. We also introduce multi-scale shifted-feature <b>distillation</b> (SD) to maintain long and short-range spatial relationships among the semantic objects where conventional short-range spatial features with limited information reduce the power of feature <b>distillation.</b> We demonstrate the effectiveness of our framework on the EndoVis 2017 and 2018 instrument segmentation dataset with a generalized <b>continual</b> <b>learning</b> setting. Code is available at~\url{https://github.com/XuMengyaAmy/Synthetic_CAT_SD}.</p></p class="citation"></blockquote><h3 id=2136--141248-real-time-holistic-robot-pose-estimation-with-unknown-states-shikun-ban-et-al-2024>(21/36 | 141/248) Real-time Holistic Robot Pose Estimation with Unknown States (Shikun Ban et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shikun Ban, Juling Fan, Wentao Zhu, Xiaoxuan Ma, Yu Qiao, Yizhou Wang. (2024)<br><strong>Real-time Holistic Robot Pose Estimation with Unknown States</strong><br><button class=copy-to-clipboard title="Real-time Holistic Robot Pose Estimation with Unknown States" index=141>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-141 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-RO, cs.CV<br>Keyword Score: 20<br>Keywords: Self-supervised Learning, Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05655v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05655v1.pdf filename=2402.05655v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Estimating robot pose from RGB images is a crucial problem in computer vision and robotics. While previous methods have achieved promising performance, most of them presume full knowledge of robot internal states, e.g. ground-truth robot joint angles, which are not always available in real-world scenarios. On the other hand, existing approaches that estimate robot pose without joint state priors suffer from heavy computation burdens and thus cannot support real-time applications. This work addresses the urgent need for efficient robot pose estimation with unknown states. We propose an end-to-end pipeline for real-time, holistic robot pose estimation from a single RGB image, even in the absence of known robot states. Our method decomposes the problem into estimating camera-to-robot rotation, robot state parameters, keypoint locations, and root depth. We further design a corresponding neural network module for each task. This approach allows for learning multi-facet representations and facilitates sim-to-real transfer through <b>self-supervised</b> <b>learning.</b> Notably, our method achieves inference with a single feedforward, eliminating the need for costly test-time iterative optimization. As a result, it delivers a 12-time speed boost with state-of-the-art accuracy, enabling real-time holistic robot pose estimation for the first time. Code is available at <a href=https://oliverbansk.github.io/Holistic-Robot-Pose/>https://oliverbansk.github.io/Holistic-Robot-Pose/</a>.</p></p class="citation"></blockquote><h3 id=2236--142248-resmatch-referring-expression-segmentation-in-a-semi-supervised-manner-ying-zang-et-al-2024>(22/36 | 142/248) RESMatch: Referring Expression Segmentation in a Semi-Supervised Manner (Ying Zang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ying Zang, Chenglong Fu, Runlong Cao, Didi Zhu, Min Zhang, Wenjun Hu, Lanyun Zhu, Tianrun Chen. (2024)<br><strong>RESMatch: Referring Expression Segmentation in a Semi-Supervised Manner</strong><br><button class=copy-to-clipboard title="RESMatch: Referring Expression Segmentation in a Semi-Supervised Manner" index=142>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-142 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 20<br>Keywords: Semi-Supervised Learning, Text Augmentation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05589v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05589v2.pdf filename=2402.05589v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Referring expression segmentation (RES), a task that involves localizing specific instance-level objects based on free-form linguistic descriptions, has emerged as a crucial frontier in human-AI interaction. It demands an intricate understanding of both visual and textual contexts and often requires extensive training data. This paper introduces RESMatch, the first <b>semi-supervised</b> <b>learning</b> (SSL) approach for RES, aimed at reducing reliance on exhaustive data annotation. Extensive validation on multiple RES datasets demonstrates that RESMatch significantly outperforms baseline approaches, establishing a new state-of-the-art. Although existing SSL techniques are effective in image segmentation, we find that they fall short in RES. Facing the challenges including the comprehension of free-form linguistic descriptions and the variability in object attributes, RESMatch introduces a trifecta of adaptations: revised strong perturbation, <b>text</b> <b>augmentation,</b> and adjustments for pseudo-label quality and strong-weak supervision. This pioneering work lays the groundwork for future research in <b>semi-supervised</b> <b>learning</b> for referring expression segmentation.</p></p class="citation"></blockquote><h3 id=2336--143248-segmentation-free-connectionist-temporal-classification-loss-based-ocr-model-for-text-captcha-classification-vaibhav-khatavkar-et-al-2024>(23/36 | 143/248) Segmentation-free Connectionist Temporal Classification loss based OCR Model for Text Captcha Classification (Vaibhav Khatavkar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Vaibhav Khatavkar, Makarand Velankar, Sneha Petkar. (2024)<br><strong>Segmentation-free Connectionist Temporal Classification loss based OCR Model for Text Captcha Classification</strong><br><button class=copy-to-clipboard title="Segmentation-free Connectionist Temporal Classification loss based OCR Model for Text Captcha Classification" index=143>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-143 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CR, cs-CV, cs-LG, cs.CV<br>Keyword Score: 20<br>Keywords: Optical Character Recognition, Optical Character Recognition<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05417v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05417v1.pdf filename=2402.05417v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Captcha are widely used to secure systems from automatic responses by distinguishing computer responses from human responses. Text, audio, video, picture picture-based <b>Optical</b> <b>Character</b> <b>Recognition</b> <b>(OCR)</b> are used for creating captcha. Text-based <b>OCR</b> captcha are the most often used captcha which faces issues namely, complex and distorted contents. There are attempts to build captcha detection and classification-based systems using machine learning and neural networks, which need to be tuned for accuracy. The existing systems face challenges in the recognition of distorted characters, handling variable-length captcha and finding sequential dependencies in captcha. In this work, we propose a segmentation-free <b>OCR</b> model for text captcha classification based on the connectionist temporal classification loss technique. The proposed model is trained and tested on a publicly available captcha dataset. The proposed model gives 99.80% character level accuracy, while 95% word level accuracy. The accuracy of the proposed model is compared with the state-of-the-art models and proves to be effective. The variable length complex captcha can be thus processed with the segmentation-free connectionist temporal classification loss technique with dependencies which will be massively used in securing the software systems.</p></p class="citation"></blockquote><h3 id=2436--144248-efficient-expression-neutrality-estimation-with-application-to-face-recognition-utility-prediction-marcel-grimmer-et-al-2024>(24/36 | 144/248) Efficient Expression Neutrality Estimation with Application to Face Recognition Utility Prediction (Marcel Grimmer et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Marcel Grimmer, Raymond N. J. Veldhuis, Christoph Busch. (2024)<br><strong>Efficient Expression Neutrality Estimation with Application to Face Recognition Utility Prediction</strong><br><button class=copy-to-clipboard title="Efficient Expression Neutrality Estimation with Application to Face Recognition Utility Prediction" index=144>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-144 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-HC, cs.CV<br>Keyword Score: 16<br>Keywords: Face Recognition, Benchmarking, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05548v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05548v1.pdf filename=2402.05548v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The recognition performance of biometric systems strongly depends on the quality of the compared biometric samples. Motivated by the goal of establishing a common understanding of <b>face</b> <b>image</b> quality and enabling system interoperability, the committee draft of ISO/IEC 29794-5 introduces expression neutrality as one of many component quality elements affecting recognition performance. In this study, we train classifiers to assess facial expression neutrality using seven datasets. We conduct extensive performance <b>benchmarking</b> to evaluate their classification and <b>face</b> <b>recognition</b> utility prediction abilities. Our experiments reveal significant differences in how each classifier distinguishes &ldquo;neutral&rdquo; from &ldquo;non-neutral&rdquo; expressions. While Random Forests and AdaBoost classifiers are most suitable for distinguishing neutral from non-neutral facial expressions with high accuracy, they underperform compared to Support Vector Machines in predicting <b>face</b> <b>recognition</b> utility.</p></p class="citation"></blockquote><h3 id=2536--145248-clip-loc-multi-modal-landmark-association-for-global-localization-in-object-based-maps-shigemichi-matsuzaki-et-al-2024>(25/36 | 145/248) CLIP-Loc: Multi-modal Landmark Association for Global Localization in Object-based Maps (Shigemichi Matsuzaki et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shigemichi Matsuzaki, Takuma Sugino, Kazuhito Tanaka, Zijun Sha, Shintaro Nakaoka, Shintaro Yoshizawa, Kazuhiro Shintani. (2024)<br><strong>CLIP-Loc: Multi-modal Landmark Association for Global Localization in Object-based Maps</strong><br><button class=copy-to-clipboard title="CLIP-Loc: Multi-modal Landmark Association for Global Localization in Object-based Maps" index=145>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-145 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs-RO, cs.CV<br>Keyword Score: 13<br>Keywords: Multi-modal, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.06092v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.06092v1.pdf filename=2402.06092v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper describes a <b>multi-modal</b> data association method for global localization using object-based maps and camera images. In global localization, or relocalization, using object-based maps, existing methods typically resort to matching all possible combinations of detected objects and landmarks with the same object category, followed by inlier extraction using RANSAC or brute-force search. This approach becomes infeasible as the number of landmarks increases due to the exponential growth of correspondence candidates. In this paper, we propose labeling landmarks with natural language descriptions and extracting correspondences based on conceptual similarity with image observations using a Vision Language Model (VLM). By leveraging detailed text information, our approach efficiently extracts correspondences compared to methods using only object categories. Through experiments, we demonstrate that the proposed method enables more accurate global localization with fewer iterations compared to baseline methods, exhibiting its efficiency.</p></p class="citation"></blockquote><h3 id=2636--146248-diffspeaker-speech-driven-3d-facial-animation-with-diffusion-transformer-zhiyuan-ma-et-al-2024>(26/36 | 146/248) DiffSpeaker: Speech-Driven 3D Facial Animation with Diffusion Transformer (Zhiyuan Ma et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Zhiyuan Ma, Xiangyu Zhu, Guojun Qi, Chen Qian, Zhaoxiang Zhang, Zhen Lei. (2024)<br><strong>DiffSpeaker: Speech-Driven 3D Facial Animation with Diffusion Transformer</strong><br><button class=copy-to-clipboard title="DiffSpeaker: Speech-Driven 3D Facial Animation with Diffusion Transformer" index=146>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-146 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 13<br>Keywords: Benchmarking, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05712v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05712v1.pdf filename=2402.05712v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Speech-driven 3D facial animation is important for many multimedia applications. Recent work has shown promise in using either Diffusion models or <b>Transformer</b> architectures for this task. However, their mere aggregation does not lead to improved performance. We suspect this is due to a shortage of paired audio-4D data, which is crucial for the <b>Transformer</b> to effectively perform as a denoiser within the Diffusion framework. To tackle this issue, we present DiffSpeaker, a <b>Transformer-based</b> network equipped with novel biased conditional attention modules. These modules serve as substitutes for the traditional self/cross-attention in standard <b>Transformers,</b> incorporating thoughtfully designed biases that steer the attention mechanisms to concentrate on both the relevant task-specific and diffusion-related conditions. We also explore the trade-off between accurate lip synchronization and non-verbal facial expressions within the Diffusion paradigm. Experiments show our model not only achieves state-of-the-art performance on existing <b>benchmarks,</b> but also fast inference speed owing to its ability to generate facial motions in parallel.</p></p class="citation"></blockquote><h3 id=2736--147248-daplankton-benchmark-dataset-for-multi-instrument-plankton-recognition-via-fine-grained-domain-adaptation-daniel-batrakhanov-et-al-2024>(27/36 | 147/248) DAPlankton: Benchmark Dataset for Multi-instrument Plankton Recognition via Fine-grained Domain Adaptation (Daniel Batrakhanov et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Daniel Batrakhanov, Tuomas Eerola, Kaisa Kraft, Lumi Haraguchi, Lasse Lensu, Sanna Suikkanen, María Teresa Camarena-Gómez, Jukka Seppälä, Heikki Kälviäinen. (2024)<br><strong>DAPlankton: Benchmark Dataset for Multi-instrument Plankton Recognition via Fine-grained Domain Adaptation</strong><br><button class=copy-to-clipboard title="DAPlankton: Benchmark Dataset for Multi-instrument Plankton Recognition via Fine-grained Domain Adaptation" index=147>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-147 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 13<br>Keywords: Benchmarking, Domain Adaptation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05615v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05615v1.pdf filename=2402.05615v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Plankton recognition provides novel possibilities to study various environmental aspects and an interesting real-world context to develop <b>domain</b> <b>adaptation</b> (DA) methods. Different imaging instruments cause <b>domain</b> <b>shift</b> between datasets hampering the development of general plankton recognition methods. A promising remedy for this is DA allowing to adapt a model trained on one instrument to other instruments. In this paper, we present a new DA dataset called DAPlankton which consists of phytoplankton images obtained with different instruments. Phytoplankton provides a challenging DA problem due to the fine-grained nature of the task and high class imbalance in real-world datasets. DAPlankton consists of two subsets. DAPlankton_LAB contains images of cultured phytoplankton providing a balanced dataset with minimal label uncertainty. DAPlankton_SEA consists of images collected from the Baltic Sea providing challenging real-world data with large intra-class variance and class imbalance. We further present a <b>benchmark</b> comparison of three widely used DA methods.</p></p class="citation"></blockquote><h3 id=2836--148248-migc-multi-instance-generation-controller-for-text-to-image-synthesis-dewei-zhou-et-al-2024>(28/36 | 148/248) MIGC: Multi-Instance Generation Controller for Text-to-Image Synthesis (Dewei Zhou et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dewei Zhou, You Li, Fan Ma, Zongxin Yang, Yi Yang. (2024)<br><strong>MIGC: Multi-Instance Generation Controller for Text-to-Image Synthesis</strong><br><button class=copy-to-clipboard title="MIGC: Multi-Instance Generation Controller for Text-to-Image Synthesis" index=148>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-148 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 13<br>Keywords: Benchmarking, Text2image<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05408v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05408v1.pdf filename=2402.05408v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present a Multi-Instance Generation (MIG) task, simultaneously generating multiple instances with diverse controls in one image. Given a set of predefined coordinates and their corresponding descriptions, the task is to ensure that generated instances are accurately at the designated locations and that all instances&rsquo; attributes adhere to their corresponding description. This broadens the scope of current research on Single-instance generation, elevating it to a more versatile and practical dimension. Inspired by the idea of divide and conquer, we introduce an innovative approach named Multi-Instance Generation Controller (MIGC) to address the challenges of the MIG task. Initially, we break down the MIG task into several subtasks, each involving the shading of a single instance. To ensure precise shading for each instance, we introduce an instance enhancement attention mechanism. Lastly, we aggregate all the shaded instances to provide the necessary information for accurately generating multiple instances in stable diffusion (SD). To evaluate how well generation models perform on the MIG task, we provide a COCO-MIG <b>benchmark</b> along with an evaluation pipeline. Extensive experiments were conducted on the proposed COCO-MIG <b>benchmark,</b> as well as on various commonly used <b>benchmarks.</b> The evaluation results illustrate the exceptional control capabilities of our model in terms of quantity, position, attribute, and interaction.</p></p class="citation"></blockquote><h3 id=2936--149248-clr-face-conditional-latent-refinement-for-blind-face-restoration-using-score-based-diffusion-models-maitreya-suin-et-al-2024>(29/36 | 149/248) CLR-Face: Conditional Latent Refinement for Blind Face Restoration Using Score-Based Diffusion Models (Maitreya Suin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Maitreya Suin, Rama Chellappa. (2024)<br><strong>CLR-Face: Conditional Latent Refinement for Blind Face Restoration Using Score-Based Diffusion Models</strong><br><button class=copy-to-clipboard title="CLR-Face: Conditional Latent Refinement for Blind Face Restoration Using Score-Based Diffusion Models" index=149>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-149 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Face Recognition<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.06106v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.06106v1.pdf filename=2402.06106v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent generative-prior-based methods have shown promising blind <b>face</b> <b>restoration</b> performance. They usually project the degraded images to the latent space and then decode high-quality <b>faces</b> <b>either</b> by single-stage latent optimization or directly from the encoding. Generating fine-grained facial details faithful to inputs remains a challenging problem. Most existing methods produce either overly smooth outputs or alter the identity as they attempt to balance between generation and reconstruction. This may be attributed to the typical trade-off between quality and resolution in the latent space. If the latent space is highly compressed, the decoded output is more robust to degradations but shows worse fidelity. On the other hand, a more flexible latent space can capture intricate facial details better, but is extremely difficult to optimize for highly degraded <b>faces</b> <b>using</b> existing techniques. To address these issues, we introduce a diffusion-based-prior inside a VQGAN architecture that focuses on learning the distribution over uncorrupted latent embeddings. With such knowledge, we iteratively recover the clean embedding conditioning on the degraded counterpart. Furthermore, to ensure the reverse diffusion trajectory does not deviate from the underlying identity, we train a separate Identity Recovery Network and use its output to constrain the reverse diffusion process. Specifically, using a learnable latent mask, we add gradients from a <b>face-recognition</b> <b>network</b> to a subset of latent features that correlates with the finer identity-related details in the pixel space, leaving the other features untouched. Disentanglement between perception and fidelity in the latent space allows us to achieve the best of both worlds. We perform extensive evaluations on multiple real and synthetic datasets to validate the superiority of our approach.</p></p class="citation"></blockquote><h3 id=3036--150248-collaborative-control-for-geometry-conditioned-pbr-image-generation-shimon-vainer-et-al-2024>(30/36 | 150/248) Collaborative Control for Geometry-Conditioned PBR Image Generation (Shimon Vainer et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Shimon Vainer, Mark Boss, Mathias Parger, Konstantin Kutsy, Dante De Nigris, Ciara Rowles, Nicolas Perony, Simon Donné. (2024)<br><strong>Collaborative Control for Geometry-Conditioned PBR Image Generation</strong><br><button class=copy-to-clipboard title="Collaborative Control for Geometry-Conditioned PBR Image Generation" index=150>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-150 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: I-4-0, cs-CV, cs-GR, cs.CV<br>Keyword Score: 10<br>Keywords: Fine-tuning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05919v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05919v1.pdf filename=2402.05919v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Current 3D content generation builds on generative models that output RGB images. Modern graphics pipelines, however, require physically-based rendering (PBR) material properties. We propose to model the PBR image distribution directly to avoid photometric inaccuracies in RGB generation and the inherent ambiguity in extracting PBR from RGB. Existing paradigms for cross-modal <b>finetuning</b> are not suited for PBR generation due to a lack of data and the high dimensionality of the output modalities: we overcome both challenges by retaining a frozen RGB model and tightly linking a newly trained PBR model using a novel cross-network communication paradigm. As the base RGB model is fully frozen, the proposed method does not risk catastrophic forgetting during <b>finetuning</b> and remains compatible with techniques such as IPAdapter pretrained for the base RGB model. We validate our design choices, robustness to data sparsity, and compare against existing paradigms with an extensive experimental section.</p></p class="citation"></blockquote><h3 id=3136--151248-jacquard-v2-refining-datasets-using-the-human-in-the-loop-data-correction-method-qiuhao-li-et-al-2024>(31/36 | 151/248) Jacquard V2: Refining Datasets using the Human In the Loop Data Correction Method (Qiuhao Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Qiuhao Li, Shenghai Yuan. (2024)<br><strong>Jacquard V2: Refining Datasets using the Human In the Loop Data Correction Method</strong><br><button class=copy-to-clipboard title="Jacquard V2: Refining Datasets using the Human In the Loop Data Correction Method" index=151>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-151 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-AI, cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: human-in-the-loop<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05747v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05747v1.pdf filename=2402.05747v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the context of rapid advancements in industrial automation, vision-based robotic grasping plays an increasingly crucial role. In order to enhance visual recognition accuracy, the utilization of large-scale datasets is imperative for training models to acquire implicit knowledge related to the handling of various objects. Creating datasets from scratch is a time and labor-intensive process. Moreover, existing datasets often contain errors due to automated annotations aimed at expediency, making the improvement of these datasets a substantial research challenge. Consequently, several issues have been identified in the annotation of grasp bounding boxes within the popular Jacquard Grasp. We propose utilizing a Human-In-The-Loop(HIL) method to enhance dataset quality. This approach relies on backbone deep learning networks to predict object positions and orientations for robotic grasping. Predictions with Intersection over Union (IOU) values below 0.2 undergo an assessment by human operators. After their evaluation, the data is categorized into False Negatives(FN) and True Negatives(TN). FN are then subcategorized into either missing annotations or catastrophic labeling errors. Images lacking labels are augmented with valid grasp bounding box information, whereas images afflicted by catastrophic labeling errors are completely removed. The open-source tool Labelbee was employed for 53,026 iterations of HIL dataset enhancement, leading to the removal of 2,884 images and the incorporation of ground truth information for 30,292 images. The enhanced dataset, named the Jacquard V2 Grasping Dataset, served as the training data for a range of neural networks.</p></p class="citation"></blockquote><h3 id=3236--152248-descanning-from-scanned-to-the-original-images-with-a-color-correction-diffusion-model-junghun-cha-et-al-2024>(32/36 | 152/248) Descanning: From Scanned to the Original Images with a Color Correction Diffusion Model (Junghun Cha et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Junghun Cha, Ali Haider, Seoyun Yang, Hoeyeong Jin, Subin Yang, A. F. M. Shahab Uddin, Jaehyoung Kim, Soo Ye Kim, Sung-Ho Bae. (2024)<br><strong>Descanning: From Scanned to the Original Images with a Color Correction Diffusion Model</strong><br><button class=copy-to-clipboard title="Descanning: From Scanned to the Original Images with a Color Correction Diffusion Model" index=152>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-152 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV, eess-IV<br>Keyword Score: 10<br>Keywords: Probabilistic Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05350v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05350v1.pdf filename=2402.05350v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>A significant volume of analog information, i.e., documents and images, have been digitized in the form of scanned copies for storing, sharing, and/or analyzing in the digital world. However, the quality of such contents is severely degraded by various distortions caused by printing, storing, and scanning processes in the physical world. Although restoring high-quality content from scanned copies has become an indispensable task for many products, it has not been systematically explored, and to the best of our knowledge, no public datasets are available. In this paper, we define this problem as Descanning and introduce a new high-quality and large-scale dataset named DESCAN-18K. It contains 18K pairs of original and scanned images collected in the wild containing multiple complex degradations. In order to eliminate such complex degradations, we propose a new image restoration model called DescanDiffusion consisting of a color encoder that corrects the global color degradation and a conditional denoising diffusion <b>probabilistic</b> <b>model</b> (DDPM) that removes local degradations. To further improve the generalization ability of DescanDiffusion, we also design a synthetic data generation scheme by reproducing prominent degradations in scanned images. We demonstrate that our DescanDiffusion outperforms other baselines including commercial restoration products, objectively and subjectively, via comprehensive experiments and analyses.</p></p class="citation"></blockquote><h3 id=3336--153248-scrapping-the-web-for-early-wildfire-detection-mateo-lostanlen-et-al-2024>(33/36 | 153/248) Scrapping The Web For Early Wildfire Detection (Mateo Lostanlen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mateo Lostanlen, Felix Veith, Cristian Buc, Valentin Barriere. (2024)<br><strong>Scrapping The Web For Early Wildfire Detection</strong><br><button class=copy-to-clipboard title="Scrapping The Web For Early Wildfire Detection" index=153>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-153 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 10<br>Keywords: Object Detection<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05349v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05349v1.pdf filename=2402.05349v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Early wildfire detection is of the utmost importance to enable rapid response efforts, and thus minimize the negative impacts of wildfire spreads. To this end, we present \Pyro, a web-scraping-based dataset composed of videos of wildfires from a network of cameras that were enhanced with manual bounding-box-level annotations. Our dataset was filtered based on a strategy to improve the quality and diversity of the data, reducing the final data to a set of 10,000 images. We ran experiments using a state-of-the-art <b>object</b> <b>detection</b> model and found out that the proposed dataset is challenging and its use in concordance with other public dataset helps to reach higher results overall. We will make our code and data publicly available.</p></p class="citation"></blockquote><h3 id=3436--154248-uav-rain1k-a-benchmark-for-raindrop-removal-from-uav-aerial-imagery-wenhui-chang-et-al-2024>(34/36 | 154/248) UAV-Rain1k: A Benchmark for Raindrop Removal from UAV Aerial Imagery (Wenhui Chang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wenhui Chang, Hongming Chen, Xin He, Xiang Chen, Liangduo Shen. (2024)<br><strong>UAV-Rain1k: A Benchmark for Raindrop Removal from UAV Aerial Imagery</strong><br><button class=copy-to-clipboard title="UAV-Rain1k: A Benchmark for Raindrop Removal from UAV Aerial Imagery" index=154>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-154 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05773v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05773v1.pdf filename=2402.05773v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Raindrops adhering to the lens of UAVs can obstruct visibility of the background scene and degrade image quality. Despite recent progress in image deraining methods and datasets, there is a lack of focus on raindrop removal from UAV aerial imagery due to the unique challenges posed by varying angles and rapid movement during drone flight. To fill the gap in this research, we first construct a new <b>benchmark</b> dataset for removing raindrops from UAV images, called UAV-Rain1k. In this letter, we provide a dataset generation pipeline, which includes modeling raindrop shapes using Blender, collecting background images from various UAV angles, random sampling of rain masks and etc. Based on the proposed <b>benchmark,</b> we further present a comprehensive evaluation of existing representative image deraining algorithms, and reveal future research opportunities worth exploring. The proposed dataset will be publicly available at <a href=https://github.com/cschenxiang/UAV-Rain1k>https://github.com/cschenxiang/UAV-Rain1k</a>.</p></p class="citation"></blockquote><h3 id=3536--155248-mtsa-snn-a-multi-modal-time-series-analysis-model-based-on-spiking-neural-network-chengzhi-liu-et-al-2024>(35/36 | 155/248) MTSA-SNN: A Multi-modal Time Series Analysis Model Based on Spiking Neural Network (Chengzhi Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chengzhi Liu, Chong Zhong, Mingyu Jin, Zheng Tao, Zihong Luo, Chenghao Liu, Shuliang Zhao. (2024)<br><strong>MTSA-SNN: A Multi-modal Time Series Analysis Model Based on Spiking Neural Network</strong><br><button class=copy-to-clipboard title="MTSA-SNN: A Multi-modal Time Series Analysis Model Based on Spiking Neural Network" index=155>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-155 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CV<br>Categories: cs-CV, cs.CV<br>Keyword Score: 3<br>Keywords: Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05423v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05423v1.pdf filename=2402.05423v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Time series analysis and modelling constitute a crucial research area. Traditional artificial neural networks struggle with complex, non-stationary time series data due to high computational complexity, limited ability to capture temporal information, and difficulty in handling event-driven data. To address these challenges, we propose a <b>Multi-modal</b> Time Series Analysis Model Based on Spiking Neural Network (MTSA-SNN). The Pulse Encoder unifies the encoding of temporal images and sequential information in a common pulse-based representation. The Joint Learning Module employs a joint learning function and weight allocation mechanism to fuse information from <b>multi-modal</b> pulse signals complementary. Additionally, we incorporate wavelet transform operations to enhance the model&rsquo;s ability to analyze and evaluate temporal information. Experimental results demonstrate that our method achieved superior performance on three complex time-series tasks. This work provides an effective event-driven approach to overcome the challenges associated with analyzing intricate temporal information. Access to the source code is available at <a href=https://github.com/Chenngzz/MTSA-SNN%7D%7Bhttps://github.com/Chenngzz/MTSA-SNN>https://github.com/Chenngzz/MTSA-SNN}{https://github.com/Chenngzz/MTSA-SNN</a></p></p class="citation"></blockquote><h2 id=cshc-6>cs.HC (6)</h2><h3 id=06--156248-llms-among-us-generative-ai-participating-in-digital-discourse-kristina-radivojevic-et-al-2024>(0/6 | 156/248) LLMs Among Us: Generative AI Participating in Digital Discourse (Kristina Radivojevic et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kristina Radivojevic, Nicholas Clark, Paul Brenner. (2024)<br><strong>LLMs Among Us: Generative AI Participating in Digital Discourse</strong><br><button class=copy-to-clipboard title="LLMs Among Us: Generative AI Participating in Digital Discourse" index=156>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-156 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-AI, cs-CY, cs-HC, cs-SI, cs.HC<br>Keyword Score: 70<br>Keywords: Generative AI, Claude, GPT, GPT-4, LLaMA, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07940v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07940v1.pdf filename=2402.07940v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The emergence of <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> has great potential to reshape the landscape of many social media platforms. While this can bring promising opportunities, it also raises many threats, such as biases and privacy concerns, and may contribute to the spread of propaganda by malicious actors. We developed the <b>&ldquo;LLMs</b> Among Us&rdquo; experimental framework on top of the Mastodon social media platform for bot and human participants to communicate without knowing the ratio or nature of bot and human participants. We built 10 personas with three different <b>LLMs,</b> <b>GPT-4,</b> <b>LLama</b> 2 Chat, and <b>Claude.</b> We conducted three rounds of the experiment and surveyed participants after each round to measure the ability of <b>LLMs</b> to pose as human participants without human detection. We found that participants correctly identified the nature of other users in the experiment only 42% of the time despite knowing the presence of both bots and humans. We also found that the choice of persona had substantially more impact on human perception than the choice of mainstream <b>LLMs.</b></p></p class="citation"></blockquote><h3 id=16--157248-keyframer-empowering-animation-design-using-large-language-models-tiffany-tseng-et-al-2024>(1/6 | 157/248) Keyframer: Empowering Animation Design using Large Language Models (Tiffany Tseng et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tiffany Tseng, Ruijia Cheng, Jeffrey Nichols. (2024)<br><strong>Keyframer: Empowering Animation Design using Large Language Models</strong><br><button class=copy-to-clipboard title="Keyframer: Empowering Animation Design using Large Language Models" index=157>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-157 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-HC, cs.HC<br>Keyword Score: 30<br>Keywords: Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.06071v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.06071v1.pdf filename=2402.06071v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> have the potential to impact a wide range of creative domains, but the application of <b>LLMs</b> to animation is underexplored and presents novel challenges such as how users might effectively describe motion in natural language. In this paper, we present Keyframer, a design tool for animating static images (SVGs) with natural language. Informed by interviews with professional animation designers and engineers, Keyframer supports exploration and refinement of animations through the combination of <b>prompting</b> and direct editing of generated output. The system also enables users to request design variants, supporting comparison and ideation. Through a user study with 13 participants, we contribute a characterization of user <b>prompting</b> strategies, including a taxonomy of semantic <b>prompt</b> types for describing motion and a &lsquo;decomposed&rsquo; <b>prompting</b> style where users continually adapt their goals in response to generated output.We share how direct editing along with <b>prompting</b> enables iteration beyond one-shot <b>prompting</b> interfaces common in generative tools today. Through this work, we propose how <b>LLMs</b> might empower a range of audiences to engage with animation creation.</p></p class="citation"></blockquote><h3 id=26--158248-randomness-is-all-you-need-semantic-traversal-of-problem-solution-spaces-with-large-language-models-thomas-sandholm-et-al-2024>(2/6 | 158/248) Randomness Is All You Need: Semantic Traversal of Problem-Solution Spaces with Large Language Models (Thomas Sandholm et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Thomas Sandholm, Sayandev Mukherjee, Bernardo A. Huberman. (2024)<br><strong>Randomness Is All You Need: Semantic Traversal of Problem-Solution Spaces with Large Language Models</strong><br><button class=copy-to-clipboard title="Randomness Is All You Need: Semantic Traversal of Problem-Solution Spaces with Large Language Models" index=158>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-158 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-AI, cs-CY, cs-HC, cs.HC<br>Keyword Score: 30<br>Keywords: Fine-tuning, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.06053v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.06053v1.pdf filename=2402.06053v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present a novel approach to exploring innovation problem and solution domains using <b>LLM</b> <b>fine-tuning</b> with a custom idea database. By semantically traversing the bi-directional problem and solution tree at different temperature levels we achieve high diversity in solution edit distance while still remaining close to the original problem statement semantically. In addition to finding a variety of solutions to a given problem, this method can also be used to refine and clarify the original problem statement. As further validation of the approach, we implemented a proof-of-concept Slack bot to serve as an innovation assistant.</p></p class="citation"></blockquote><h3 id=36--159248-ufo-a-ui-focused-agent-for-windows-os-interaction-chaoyun-zhang-et-al-2024>(3/6 | 159/248) UFO: A UI-Focused Agent for Windows OS Interaction (Chaoyun Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Chaoyun Zhang, Liqun Li, Shilin He, Xu Zhang, Bo Qiao, Si Qin, Minghua Ma, Yu Kang, Qingwei Lin, Saravan Rajmohan, Dongmei Zhang, Qi Zhang. (2024)<br><strong>UFO: A UI-Focused Agent for Windows OS Interaction</strong><br><button class=copy-to-clipboard title="UFO: A UI-Focused Agent for Windows OS Interaction" index=159>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-159 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-AI, cs-CL, cs-HC, cs.HC<br>Keyword Score: 30<br>Keywords: Human Intervention, GPT, Grounding<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.07939v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.07939v1.pdf filename=2402.07939v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We introduce UFO, an innovative UI-Focused agent to fulfill user requests tailored to applications on Windows OS, harnessing the capabilities of <b>GPT-Vision.</b> UFO employs a dual-agent framework to meticulously observe and analyze the graphical user interface (GUI) and control information of Windows applications. This enables the agent to seamlessly navigate and operate within individual applications and across them to fulfill user requests, even when spanning multiple applications. The framework incorporates a control interaction module, facilitating action <b>grounding</b> without <b>human</b> <b>intervention</b> and enabling fully automated execution. Consequently, UFO transforms arduous and time-consuming processes into simple tasks achievable solely through natural language commands. We conducted testing of UFO across 9 popular Windows applications, encompassing a variety of scenarios reflective of users&rsquo; daily usage. The results, derived from both quantitative metrics and real-case studies, underscore the superior effectiveness of UFO in fulfilling user requests. To the best of our knowledge, UFO stands as the first UI agent specifically tailored for task completion within the Windows OS environment. The open-source code for UFO is available on <a href=https://github.com/microsoft/UFO>https://github.com/microsoft/UFO</a>.</p></p class="citation"></blockquote><h3 id=46--160248-personalizing-driver-safety-interfaces-via-driver-cognitive-factors-inference-emily-s-sumner-et-al-2024>(4/6 | 160/248) Personalizing Driver Safety Interfaces via Driver Cognitive Factors Inference (Emily S Sumner et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Emily S Sumner, Jonathan DeCastro, Jean Costa, Deepak E Gopinath, Everlyne Kimani, Shabnam Hakimi, Allison Morgan, Andrew Best, Hieu Nguyen, Daniel J Brooks, Bassam ul Haq, Andrew Patrikalakis, Hiroshi Yasuda, Kate Sieck, Avinash Balachandran, Tiffany Chen, Guy Rosman. (2024)<br><strong>Personalizing Driver Safety Interfaces via Driver Cognitive Factors Inference</strong><br><button class=copy-to-clipboard title="Personalizing Driver Safety Interfaces via Driver Cognitive Factors Inference" index=160>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-160 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-HC, cs.HC<br>Keyword Score: 10<br>Keywords: Recurrent Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05893v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05893v1.pdf filename=2402.05893v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advances in AI and intelligent vehicle technology hold promise to revolutionize mobility and transportation, in the form of advanced driving assistance (ADAS) interfaces. Although it is widely recognized that certain cognitive factors, such as impulsivity and inhibitory control, are related to risky driving behavior, play a significant role in on-road risk-taking, existing systems fail to leverage such factors. Varying levels of these cognitive factors could influence the effectiveness and acceptance of driver safety interfaces. We demonstrate an approach for personalizing driver interaction via driver safety interfaces that are triggered based on a learned <b>recurrent</b> <b>neural</b> <b>network.</b> The network is trained from a population of human drivers to infer impulsivity and inhibitory control from recent driving behavior. Using a high-fidelity vehicle motion simulator, we demonstrate the ability to deduce these factors from driver behavior. We then use these inferred factors to make instantaneous determinations on whether or not to engage a driver safety interface. This interface aims to decrease a driver&rsquo;s speed during yellow lights and reduce their inclination to run through them.</p></p class="citation"></blockquote><h3 id=56--161248-form-from-a-design-space-of-social-media-systems-amy-x-zhang-et-al-2024>(5/6 | 161/248) Form-From: A Design Space of Social Media Systems (Amy X. Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Amy X. Zhang, Michael S. Bernstein, David R. Karger, Mark S. Ackerman. (2024)<br><strong>Form-From: A Design Space of Social Media Systems</strong><br><button class=copy-to-clipboard title="Form-From: A Design Space of Social Media Systems" index=161>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-161 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.HC<br>Categories: cs-HC, cs-SI, cs.HC<br>Keyword Score: 10<br>Keywords: Knowledge Distillation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05388v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05388v1.pdf filename=2402.05388v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Social media systems are as varied as they are pervasive. They have been almost universally adopted for a broad range of purposes including work, entertainment, activism, and decision making. As a result, they have also diversified, with many distinct designs differing in content type, organization, delivery mechanism, access control, and many other dimensions. In this work, we aim to characterize and then <b>distill</b> a concise design space of social media systems that can help us understand similarities and differences, recognize potential consequences of design choice, and identify spaces for innovation. Our model, which we call Form-From, characterizes social media based on (1) the form of the content, either threaded or flat, and (2) from where or from whom one might receive content, ranging from spaces to networks to the commons. We derive Form-From inductively from a larger set of 62 dimensions organized into 10 categories. To demonstrate the utility of our model, we trace the history of social media systems as they traverse the Form-From space over time, and we identify common design patterns within cells of the model.</p></p class="citation"></blockquote><h2 id=cscr-3>cs.CR (3)</h2><h3 id=03--162248-comprehensive-assessment-of-jailbreak-attacks-against-llms-junjie-chu-et-al-2024>(0/3 | 162/248) Comprehensive Assessment of Jailbreak Attacks Against LLMs (Junjie Chu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Junjie Chu, Yugeng Liu, Ziqing Yang, Xinyue Shen, Michael Backes, Yang Zhang. (2024)<br><strong>Comprehensive Assessment of Jailbreak Attacks Against LLMs</strong><br><button class=copy-to-clipboard title="Comprehensive Assessment of Jailbreak Attacks Against LLMs" index=162>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-162 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-AI, cs-CL, cs-CR, cs-LG, cs.CR<br>Keyword Score: 63<br>Keywords: Benchmarking, GPT, GPT-3, GPT-3.5, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05668v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05668v1.pdf filename=2402.05668v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Misuse of the <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> has raised widespread concern. To address this issue, safeguards have been taken to ensure that <b>LLMs</b> align with social ethics. However, recent findings have revealed an unsettling vulnerability bypassing the safeguards of <b>LLMs,</b> known as jailbreak attacks. By applying techniques, such as employing role-playing scenarios, adversarial examples, or subtle subversion of safety objectives as a <b>prompt,</b> <b>LLMs</b> can produce an inappropriate or even harmful response. While researchers have studied several categories of jailbreak attacks, they have done so in isolation. To fill this gap, we present the first <b>large-scale</b> <b>measurement</b> <b>of</b> various jailbreak attack methods. We concentrate on 13 cutting-edge jailbreak methods from four categories, 160 questions from 16 violation categories, and six popular <b>LLMs.</b> Our extensive experimental results demonstrate that the optimized jailbreak <b>prompts</b> consistently achieve the highest attack success rates, as well as exhibit robustness across different <b>LLMs.</b> Some jailbreak <b>prompt</b> datasets, available from the Internet, can also achieve high attack success rates on many <b>LLMs,</b> such as ChatGLM3, <b>GPT-3.5,</b> and PaLM2. Despite the claims from many organizations regarding the coverage of violation categories in their policies, the attack success rates from these categories remain high, indicating the challenges of effectively aligning <b>LLM</b> policies and the ability to counter jailbreak attacks. We also discuss the trade-off between the attack performance and efficiency, as well as show that the transferability of the jailbreak <b>prompts</b> is still viable, becoming an option for black-box models. Overall, our research highlights the necessity of evaluating different jailbreak methods. We hope our study can provide insights for future research on jailbreak attacks and serve as a <b>benchmark</b> tool for evaluating them for practitioners.</p></p class="citation"></blockquote><h3 id=13--163248-domain-agnostic-hardware-fingerprinting-based-device-identifier-for-zero-trust-iot-security-abdurrahman-elmaghbub-et-al-2024>(1/3 | 163/248) Domain-Agnostic Hardware Fingerprinting-Based Device Identifier for Zero-Trust IoT Security (Abdurrahman Elmaghbub et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Abdurrahman Elmaghbub, Bechir Hamdaoui. (2024)<br><strong>Domain-Agnostic Hardware Fingerprinting-Based Device Identifier for Zero-Trust IoT Security</strong><br><button class=copy-to-clipboard title="Domain-Agnostic Hardware Fingerprinting-Based Device Identifier for Zero-Trust IoT Security" index=163>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-163 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs-NI, cs.CR, eess-SP<br>Keyword Score: 40<br>Keywords: Convolution, Convolutional Neural Network, Convolutional Neural Network, Zero Trust<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05332v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05332v1.pdf filename=2402.05332v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Next-generation networks aim for comprehensive connectivity, interconnecting humans, machines, devices, and systems seamlessly. This interconnectivity raises concerns about privacy and security, given the potential network-wide impact of a single compromise. To address this challenge, the <b>Zero</b> <b>Trust</b> (ZT) paradigm emerges as a key method for safeguarding network integrity and data confidentiality. This work introduces EPS-CNN, a novel deep-learning-based wireless device identification framework designed to serve as the device authentication layer within the ZT architecture, with a focus on resource-constrained IoT devices. At the core of EPS-CNN, a <b>Convolutional</b> <b>Neural</b> <b>Network</b> <b>(CNN)</b> is utilized to generate the device identity from a unique RF signal representation, known as the Double-Sided Envelope Power Spectrum (EPS), which effectively captures the device-specific hardware characteristics while ignoring device-unrelated information. Experimental evaluations show that the proposed framework achieves over 99%, 93%, and 95% testing accuracy when tested in same-domain (day, location, and channel), cross-day, and cross-location scenarios, respectively. Our findings demonstrate the superiority of the proposed framework in enhancing the accuracy, robustness, and adaptability of deep learning-based methods, thus offering a pioneering solution for enabling ZT IoT device identification.</p></p class="citation"></blockquote><h3 id=23--164248-buffer-overflow-in-mixture-of-experts-jamie-hayes-et-al-2024>(2/3 | 164/248) Buffer Overflow in Mixture of Experts (Jamie Hayes et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jamie Hayes, Ilia Shumailov, Itay Yona. (2024)<br><strong>Buffer Overflow in Mixture of Experts</strong><br><button class=copy-to-clipboard title="Buffer Overflow in Mixture of Experts" index=164>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-164 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CR<br>Categories: cs-CR, cs-LG, cs.CR<br>Keyword Score: 10<br>Keywords: Foundation Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05526v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05526v1.pdf filename=2402.05526v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Mixture of Experts (MoE) has become a key ingredient for scaling large <b>foundation</b> <b>models</b> while keeping inference costs steady. We show that expert routing strategies that have cross-batch dependencies are vulnerable to attacks. Malicious queries can be sent to a model and can affect a model&rsquo;s output on other benign queries if they are grouped in the same batch. We demonstrate this via a proof-of-concept attack in a toy experimental setting.</p></p class="citation"></blockquote><h2 id=q-finst-1>q-fin.ST (1)</h2><h3 id=01--165248-a-study-on-stock-forecasting-using-deep-learning-and-statistical-models-himanshu-gupta-et-al-2024>(0/1 | 165/248) A Study on Stock Forecasting Using Deep Learning and Statistical Models (Himanshu Gupta et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Himanshu Gupta, Aditya Jaiswal. (2024)<br><strong>A Study on Stock Forecasting Using Deep Learning and Statistical Models</strong><br><button class=copy-to-clipboard title="A Study on Stock Forecasting Using Deep Learning and Statistical Models" index=165>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-165 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: q-fin.ST<br>Categories: cs-LG, q-fin-ST, q-fin.ST<br>Keyword Score: 60<br>Keywords: Convolution, Convolutional Neural Network, Convolutional Neural Network, LSTM, Recurrent Neural Network, Recurrent Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.06689v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.06689v1.pdf filename=2402.06689v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Predicting a fast and accurate model for stock price forecasting is been a challenging task and this is an active area of research where it is yet to be found which is the best way to forecast the stock price. Machine learning, deep learning and statistical analysis techniques are used here to get the accurate result so the investors can see the future trend and maximize the return of investment in stock trading. This paper will review many deep learning algorithms for stock price forecasting. We use a record of s&amp;p 500 index data for training and testing. The survey motive is to check various deep learning and statistical model techniques for stock price forecasting that are Moving Averages, ARIMA which are statistical techniques and <b>LSTM,</b> <b>RNN,</b> <b>CNN,</b> and FULL <b>CNN</b> which are deep learning models. It will discuss various models, including the Auto regression integration moving average model, the <b>Recurrent</b> <b>neural</b> <b>network</b> model, the long short-term model which is the type of <b>RNN</b> used for long dependency for data, the <b>convolutional</b> <b>neural</b> <b>network</b> model, and the full <b>convolutional</b> <b>neural</b> <b>network</b> model, in terms of error calculation or percentage of accuracy that how much it is accurate which measures by the function like Root mean square error, mean absolute error, mean squared error. The model can be used to predict the stock price by checking the low MAE value as lower the MAE value the difference between the predicting and the actual value will be less and this model will predict the price more accurately than other models.</p></p class="citation"></blockquote><h2 id=eesssp-2>eess.SP (2)</h2><h3 id=02--166248-graph-neural-networks-for-physical-layer-security-in-multi-user-flexible-duplex-networks-tharaka-perera-et-al-2024>(0/2 | 166/248) Graph Neural Networks for Physical-Layer Security in Multi-User Flexible-Duplex Networks (Tharaka Perera et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tharaka Perera, Saman Atapattu, Yuting Fang, Jamie Evans. (2024)<br><strong>Graph Neural Networks for Physical-Layer Security in Multi-User Flexible-Duplex Networks</strong><br><button class=copy-to-clipboard title="Graph Neural Networks for Physical-Layer Security in Multi-User Flexible-Duplex Networks" index=166>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-166 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SP<br>Categories: cs-AI, cs-CR, cs-LG, eess-SP, eess.SP<br>Keyword Score: 60<br>Keywords: Graph Neural Network, Graph Neural Network, Simulation, Simulator, Unsupervised Learning, Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05378v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05378v1.pdf filename=2402.05378v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper explores Physical-Layer Security (PLS) in Flexible Duplex (FlexD) networks, considering scenarios involving eavesdroppers. Our investigation revolves around the intricacies of the sum secrecy rate maximization problem, particularly when faced with coordinated and distributed eavesdroppers employing a Minimum Mean Square Error (MMSE) receiver. Our contributions include an iterative classical optimization solution and an <b>unsupervised</b> <b>learning</b> strategy based on <b>Graph</b> <b>Neural</b> <b>Networks</b> <b>(GNNs).</b> To the best of our knowledge, this work marks the initial exploration of <b>GNNs</b> for PLS applications. Additionally, we extend the <b>GNN</b> approach to address the absence of eavesdroppers&rsquo; channel knowledge. Extensive numerical <b>simulations</b> highlight FlexD&rsquo;s superiority over Half-Duplex (HD) communications and the <b>GNN</b> approach&rsquo;s superiority over the classical method in both performance and time complexity.</p></p class="citation"></blockquote><h3 id=12--167248-a-non-intrusive-neural-quality-assessment-model-for-surface-electromyography-signals-cho-yuan-lee-et-al-2024>(1/2 | 167/248) A Non-Intrusive Neural Quality Assessment Model for Surface Electromyography Signals (Cho-Yuan Lee et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Cho-Yuan Lee, Kuan-Chen Wang, Kai-Chun Liu, Xugang Lu, Ping-Chen Yeh, Yu Tsao. (2024)<br><strong>A Non-Intrusive Neural Quality Assessment Model for Surface Electromyography Signals</strong><br><button class=copy-to-clipboard title="A Non-Intrusive Neural Quality Assessment Model for Surface Electromyography Signals" index=167>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-167 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SP<br>Categories: cs-LG, eess-SP, eess.SP<br>Keyword Score: 10<br>Keywords: Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05482v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05482v1.pdf filename=2402.05482v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In practical scenarios involving the measurement of surface electromyography (sEMG) in muscles, particularly those areas near the heart, one of the primary sources of contamination is the presence of electrocardiogram (ECG) signals. To assess the quality of real-world sEMG data more effectively, this study proposes QASE-net, a new non-intrusive model that predicts the SNR of sEMG signals. QASE-net combines <b>CNN-BLSTM</b> with attention mechanisms and follows an end-to-end training strategy. Our experimental framework utilizes real-world sEMG and ECG data from two open-access databases, the Non-Invasive Adaptive Prosthetics Database and the MIT-BIH Normal Sinus Rhythm Database, respectively. The experimental results demonstrate the superiority of QASE-net over the previous assessment model, exhibiting significantly reduced prediction errors and notably higher linear correlations with the ground truth. These findings show the potential of QASE-net to substantially enhance the reliability and precision of sEMG quality assessment in practical applications.</p></p class="citation"></blockquote><h2 id=physicsflu-dyn-1>physics.flu-dyn (1)</h2><h3 id=01--168248-reduced-order-modeling-of-unsteady-fluid-flow-using-neural-network-ensembles-rakesh-halder-et-al-2024>(0/1 | 168/248) Reduced-order modeling of unsteady fluid flow using neural network ensembles (Rakesh Halder et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Rakesh Halder, Mohammadmehdi Ataei, Hesam Salehipour, Krzysztof Fidkowski, Kevin Maki. (2024)<br><strong>Reduced-order modeling of unsteady fluid flow using neural network ensembles</strong><br><button class=copy-to-clipboard title="Reduced-order modeling of unsteady fluid flow using neural network ensembles" index=168>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-168 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: physics.flu-dyn<br>Categories: cs-LG, physics-flu-dyn, physics.flu-dyn<br>Keyword Score: 60<br>Keywords: Autoencoder, Convolution, LSTM, LSTM, LSTM, Recurrent Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05372v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05372v1.pdf filename=2402.05372v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The use of deep learning has become increasingly popular in reduced-order models (ROMs) to obtain low-dimensional representations of full-order models. <b>Convolutional</b> <b>autoencoders</b> (CAEs) are often used to this end as they are adept at handling data that are spatially distributed, including solutions to partial differential equations. When applied to unsteady physics problems, ROMs also require a model for time-series prediction of the low-dimensional latent variables. <b>Long</b> <b>short-term</b> <b>memory</b> <b>(LSTM)</b> networks, a type of <b>recurrent</b> <b>neural</b> <b>network</b> useful for modeling sequential data, are frequently employed in data-driven ROMs for autoregressive time-series prediction. When making predictions at unseen design points over <b>long</b> <b>time</b> <b>horizons,</b> <b>error</b> propagation is a frequently encountered issue, where errors made early on can compound over time and lead to large inaccuracies. In this work, we propose using bagging, a commonly used ensemble learning technique, to develop a fully data-driven ROM framework referred to as the CAE-eLSTM ROM that uses CAEs for spatial reconstruction of the full-order model and <b>LSTM</b> ensembles for time-series prediction. When applied to two unsteady fluid dynamics problems, our results show that the presented framework effectively reduces error propagation and leads to more accurate time-series prediction of latent variables at unseen points.</p></p class="citation"></blockquote><h2 id=csse-9>cs.SE (9)</h2><h3 id=09--169248-how-to-refactor-this-code-an-exploratory-study-on-developer-chatgpt-refactoring-conversations-eman-abdullah-alomar-et-al-2024>(0/9 | 169/248) How to Refactor this Code? An Exploratory Study on Developer-ChatGPT Refactoring Conversations (Eman Abdullah AlOmar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Eman Abdullah AlOmar, Anushkrishna Venkatakrishnan, Mohamed Wiem Mkaouer, Christian D. Newman, Ali Ouni. (2024)<br><strong>How to Refactor this Code? An Exploratory Study on Developer-ChatGPT Refactoring Conversations</strong><br><button class=copy-to-clipboard title="How to Refactor this Code? An Exploratory Study on Developer-ChatGPT Refactoring Conversations" index=169>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-169 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-SE, cs.SE<br>Keyword Score: 50<br>Keywords: ChatGPT, Text Mining, Large Language Model, Large Language Model, Prompt<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.06013v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.06013v1.pdf filename=2402.06013v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs),</b> like <b>ChatGPT,</b> have gained widespread popularity and usage in various software engineering tasks, including refactoring, testing, code review, and program comprehension. Despite recent studies delving into refactoring documentation in commit messages, issues, and code review, little is known about how developers articulate their refactoring needs when interacting with <b>ChatGPT.</b> In this paper, our goal is to explore conversations between developers and <b>ChatGPT</b> related to refactoring to better understand how developers identify areas for improvement in code and how <b>ChatGPT</b> addresses developers&rsquo; needs. Our approach relies on <b>text</b> <b>mining</b> refactoring-related conversations from 17,913 <b>ChatGPT</b> <b>prompts</b> and responses, and investigating developers&rsquo; explicit refactoring intention. Our results reveal that (1) developer-ChatGPT conversations commonly involve generic and specific terms/phrases; (2) developers often make generic refactoring requests, while <b>ChatGPT</b> typically includes the refactoring intention; and (3) various learning settings when <b>prompting</b> <b>ChatGPT</b> in the context of refactoring. We envision that our findings contribute to a broader understanding of the collaboration between developers and AI models, in the context of code refactoring, with implications for model improvement, tool development, and best practices in software engineering.</p></p class="citation"></blockquote><h3 id=19--170248-neural-models-for-source-code-synthesis-and-completion-mitodru-niyogi-2024>(1/9 | 170/248) Neural Models for Source Code Synthesis and Completion (Mitodru Niyogi, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mitodru Niyogi. (2024)<br><strong>Neural Models for Source Code Synthesis and Completion</strong><br><button class=copy-to-clipboard title="Neural Models for Source Code Synthesis and Completion" index=170>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-170 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-CL, cs-LG, cs-PL, cs-SE, cs.SE<br>Keyword Score: 50<br>Keywords: Data Augmentation, RoBERTa, Semantic Parsing, BLEU, Masked Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.06690v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.06690v1.pdf filename=2402.06690v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Natural language (NL) to code suggestion systems assist developers in Integrated Development Environments (IDEs) by translating NL utterances into compilable code snippet. The current approaches mainly involve hard-coded, rule-based systems based on <b>semantic</b> <b>parsing.</b> These systems make heavy use of hand-crafted rules that map patterns in NL or elements in its syntax parse tree to various query constructs and can only work on a limited subset of NL with a restricted NL syntax. These systems are unable to extract <b>semantic</b> <b>information</b> from the coding intents of the developer, and often fail to infer types, names, and the context of the source code to get accurate system-level code suggestions. In this master thesis, we present sequence-to-sequence deep learning models and training paradigms to map NL to general-purpose programming languages that can assist users with suggestions of source code snippets, given a NL intent, and also extend auto-completion functionality of the source code to users while they are writing source code. The developed architecture incorporates contextual awareness into neural models which generate source code tokens directly instead of generating parse trees/abstract meaning representations from the source code and converting them back to source code. The proposed pretraining strategy and the <b>data</b> <b>augmentation</b> techniques improve the performance of the proposed architecture. The proposed architecture has been found to exceed the performance of a neural <b>semantic</b> <b>parser,</b> TranX, based on the <b>BLEU-4</b> metric by 10.82%. Thereafter, a finer analysis for the parsable code translations from the NL intent for CoNaLA challenge was introduced. The proposed system is bidirectional as it can be also used to generate NL code documentation given source code. Lastly, a <b>RoBERTa</b> <b>masked</b> <b>language</b> <b>model</b> for Python was proposed to extend the developed system for code completion.</p></p class="citation"></blockquote><h3 id=29--171248-rocks-coding-not-development--a-human-centric-experimental-evaluation-of-llm-supported-se-tasks-wei-wang-et-al-2024>(2/9 | 171/248) Rocks Coding, Not Development&ndash;A Human-Centric, Experimental Evaluation of LLM-Supported SE Tasks (Wei Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Wei Wang, Huilong Ning, Gaowei Zhang, Libo Liu, Yi Wang. (2024)<br><strong>Rocks Coding, Not Development&ndash;A Human-Centric, Experimental Evaluation of LLM-Supported SE Tasks</strong><br><button class=copy-to-clipboard title="Rocks Coding, Not Development--A Human-Centric, Experimental Evaluation of LLM-Supported SE Tasks" index=171>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-171 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-AI, cs-SE, cs.SE<br>Keyword Score: 40<br>Keywords: Generative AI, ChatGPT, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05650v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05650v2.pdf filename=2402.05650v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recently, <b>large</b> <b>language</b> <b>models</b> <b>(LLM)</b> based <b>generative</b> <b>AI</b> has been gaining momentum for their impressive high-quality performances in multiple domains, particularly after the release of the <b>ChatGPT.</b> Many believe that they have the potential to perform general-purpose problem-solving in software development and replace human software developers. Nevertheless, there are in a lack of serious investigation into the capability of these <b>LLM</b> techniques in fulfilling software development tasks. In a controlled 2 $\times$ 2 between-subject experiment with 109 participants, we examined whether and to what degree working with <b>ChatGPT</b> was helpful in the coding task and typical software development task and how people work with <b>ChatGPT.</b> We found that while <b>ChatGPT</b> performed well in solving simple coding problems, its performance in supporting typical software development tasks was not that good. We also observed the interactions between participants and <b>ChatGPT</b> and found the relations between the interactions and the outcomes. Our study thus provides first-hand insights into using <b>ChatGPT</b> to fulfill software engineering tasks with real-world developers and motivates the need for novel interaction mechanisms that help developers effectively work with <b>large</b> <b>language</b> <b>models</b> to achieve desired outcomes.</p></p class="citation"></blockquote><h3 id=39--172248-do-large-code-models-understand-programming-concepts-a-black-box-approach-ashish-hooda-et-al-2024>(3/9 | 172/248) Do Large Code Models Understand Programming Concepts? A Black-box Approach (Ashish Hooda et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ashish Hooda, Mihai Christodorescu, Miltos Allamanis, Aaron Wilson, Kassem Fawaz, Somesh Jha. (2024)<br><strong>Do Large Code Models Understand Programming Concepts? A Black-box Approach</strong><br><button class=copy-to-clipboard title="Do Large Code Models Understand Programming Concepts? A Black-box Approach" index=172>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-172 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-AI, cs-LG, cs-PL, cs-SE, cs.SE<br>Keyword Score: 40<br>Keywords: Counter-factual, Code Generation, Text Generation, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05980v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05980v1.pdf filename=2402.05980v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Large</b> <b>Language</b> <b>Models&rsquo;</b> success on <b>text</b> <b>generation</b> has also made them better at <b>code</b> <b>generation</b> and coding tasks. While a lot of work has demonstrated their remarkable performance on tasks such as <b>code</b> <b>completion</b> and editing, it is still unclear as to why. We help bridge this gap by exploring to what degree auto-regressive models understand the logical constructs of the underlying programs. We propose <b>Counterfactual</b> Analysis for Programming Concept Predicates (CACP) as a <b>counterfactual</b> testing framework to evaluate whether <b>Large</b> <b>Code</b> <b>Models</b> understand programming concepts. With only black-box access to the model, we use CACP to evaluate ten popular <b>Large</b> <b>Code</b> <b>Models</b> for four different programming concepts. Our findings suggest that current models lack understanding of concepts such as data flow and control flow.</p></p class="citation"></blockquote><h3 id=49--173248-the-impact-of-ai-tool-on-engineering-at-anz-bank-an-emperical-study-on-github-copilot-within-coporate-environment-sayan-chatterjee-et-al-2024>(4/9 | 173/248) The Impact of AI Tool on Engineering at ANZ Bank An Emperical Study on GitHub Copilot within Coporate Environment (Sayan Chatterjee et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Sayan Chatterjee, Ching Louis Liu, Gareth Rowland, Tim Hogarth. (2024)<br><strong>The Impact of AI Tool on Engineering at ANZ Bank An Emperical Study on GitHub Copilot within Coporate Environment</strong><br><button class=copy-to-clipboard title="The Impact of AI Tool on Engineering at ANZ Bank An Emperical Study on GitHub Copilot within Coporate Environment" index=173>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-173 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-AI, cs-SE, cs.SE<br>Keyword Score: 20<br>Keywords: Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05636v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05636v1.pdf filename=2402.05636v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The increasing popularity of AI, particularly <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs),</b> has significantly impacted various domains, including Software Engineering. This study explores the integration of AI tools in software engineering practices within a <b>large</b> <b>organization.</b> <b>We</b> focus on ANZ Bank, which employs over 5000 engineers covering all aspects of the software development life cycle. This paper details an experiment conducted using GitHub Copilot, a notable AI tool, within a controlled environment to evaluate its effectiveness in real-world engineering tasks. Additionally, this paper shares initial findings on the productivity improvements observed after GitHub Copilot was adopted on a <b>large</b> <b>scale,</b> <b>with</b> about 1000 engineers using it. ANZ Bank&rsquo;s six-week experiment with GitHub Copilot included two weeks of preparation and four weeks of active testing. The study evaluated participant sentiment and the tool&rsquo;s impact on productivity, code quality, and security. Initially, participants used GitHub Copilot for proposed use-cases, with their feedback gathered through regular surveys. In the second phase, they were divided into Control and Copilot groups, each tackling the same Python challenges, and their experiences were again surveyed. Results showed a notable boost in productivity and code quality with GitHub Copilot, though its impact on code security remained inconclusive. Participant responses were overall positive, confirming GitHub Copilot&rsquo;s effectiveness in <b>large-scale</b> <b>software</b> <b>engineering</b> environments. Early data from 1000 engineers also indicated a significant increase in productivity and job satisfaction.</p></p class="citation"></blockquote><h3 id=59--174248-anticopypaster-20-whitebox-just-in-time-code-duplicates-extraction-eman-abdullah-alomar-et-al-2024>(5/9 | 174/248) AntiCopyPaster 2.0: Whitebox just-in-time code duplicates extraction (Eman Abdullah AlOmar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Eman Abdullah AlOmar, Benjamin Knobloch, Thomas Kain, Christopher Kalish, Mohamed Wiem Mkaouer, Ali Ouni. (2024)<br><strong>AntiCopyPaster 2.0: Whitebox just-in-time code duplicates extraction</strong><br><button class=copy-to-clipboard title="AntiCopyPaster 2.0: Whitebox just-in-time code duplicates extraction" index=174>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-174 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-SE, cs.SE<br>Keyword Score: 10<br>Keywords: Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.06035v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.06035v1.pdf filename=2402.06035v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>AntiCopyPaster is an IntelliJ IDEA plugin, implemented to detect and refactor duplicate code interactively as soon as a duplicate is introduced. The plugin only recommends the extraction of a duplicate when it is worth it. In contrast to current Extract Method refactoring approaches, our tool seamlessly integrates with the developer&rsquo;s workflow and actively provides <b>recommendations</b> for refactorings. This work extends our tool to allow developers to customize the detection rules, i.e., metrics, based on their needs and preferences. The plugin and its source code are publicly available on GitHub at <a href=https://github.com/refactorings/anti-copy-paster>https://github.com/refactorings/anti-copy-paster</a>. The demonstration video can be found on YouTube: <a href=https://youtu.be/>https://youtu.be/</a> Y1sbfpds2Ms.</p></p class="citation"></blockquote><h3 id=69--175248-using-changeset-descriptions-as-a-data-source-to-assist-feature-location-muslim-chochlov-et-al-2024>(6/9 | 175/248) Using Changeset Descriptions as a Data Source to Assist Feature Location (Muslim Chochlov et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Muslim Chochlov, Michael English, Jim Buckley. (2024)<br><strong>Using Changeset Descriptions as a Data Source to Assist Feature Location</strong><br><button class=copy-to-clipboard title="Using Changeset Descriptions as a Data Source to Assist Feature Location" index=175>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-175 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-SE, cs.SE<br>Keyword Score: 10<br>Keywords: Information Retrieval<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05711v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05711v1.pdf filename=2402.05711v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Feature location attempts to assist developers in discovering functionality in source code. Many textual feature location techniques utilize <b>information</b> <b>retrieval</b> and rely on comments and identifiers of source code to describe software entities. An interesting alternative would be to employ the changeset descriptions of the code altered in that changeset as a data source to describe such software entities. To investigate this we implement a technique utilizing changeset descriptions and conduct an empirical study to observe this technique&rsquo;s overall performance. Moreover, we study how the granularity (i.e. file or method level of software entities) and changeset range inclusion (i.e. most recent or all historical changesets) affect such an approach. The results of a preliminary study with Rhino and Mylyn.Tasks systems suggest that the approach could lead to a potentially efficient feature location technique. They also suggest that it is advantageous in terms of the effort to configure the technique at method level granularity and that older changesets from older systems may reduce the effectiveness of the technique.</p></p class="citation"></blockquote><h3 id=79--176248-investigating-reproducibility-in-deep-learning-based-software-fault-prediction-adil-mukhtar-et-al-2024>(7/9 | 176/248) Investigating Reproducibility in Deep Learning-Based Software Fault Prediction (Adil Mukhtar et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Adil Mukhtar, Dietmar Jannach, Franz Wotawa. (2024)<br><strong>Investigating Reproducibility in Deep Learning-Based Software Fault Prediction</strong><br><button class=copy-to-clipboard title="Investigating Reproducibility in Deep Learning-Based Software Fault Prediction" index=176>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-176 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-LG, cs-SE, cs.SE<br>Keyword Score: 10<br>Keywords: Software Fault Prediction<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05645v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05645v1.pdf filename=2402.05645v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Over the past few years, deep learning methods have been applied for a wide range of <b>Software</b> <b>Engineering</b> <b>(SE)</b> tasks, including in particular for the important task of automatically predicting and localizing faults in <b>software.</b> <b>With</b> <b>the</b> rapid adoption of increasingly complex machine learning models, it however becomes more and more difficult for scholars to reproduce the results that are reported in the literature. This is in particular the case when the applied deep learning models and the evaluation methodology are not properly documented and when code and data are not shared. Given some recent &ndash; and very worrying &ndash; findings regarding reproducibility and progress in other areas of applied machine learning, the goal of this work is to analyze to what extent the field of <b>software</b> <b>engineering,</b> <b>in</b> particular in the area of <b>software</b> <b>fault</b> <b>prediction,</b> is plagued by similar problems. We have therefore conducted a systematic review of the current literature and examined the level of reproducibility of 56 research articles that were published between 2019 and 2022 in top-tier <b>software</b> <b>engineering</b> <b>conferences.</b> Our analysis revealed that scholars are apparently largely aware of the reproducibility problem, and about two thirds of the papers provide code for their proposed deep learning models. However, it turned out that in the vast majority of cases, crucial elements for reproducibility are missing, such as the code of the compared baselines, code for data pre-processing or code for hyperparameter tuning. In these cases, it therefore remains challenging to exactly reproduce the results in the current research literature. Overall, our meta-analysis therefore calls for improved research practices to ensure the reproducibility of machine-learning based research.</p></p class="citation"></blockquote><h3 id=89--177248-polaris-a-framework-to-guide-the-development-of-trustworthy-ai-systems-maria-teresa-baldassarre-et-al-2024>(8/9 | 177/248) POLARIS: A framework to guide the development of Trustworthy AI systems (Maria Teresa Baldassarre et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Maria Teresa Baldassarre, Domenico Gigante, Marcos Kalinowski, Azzurra Ragone. (2024)<br><strong>POLARIS: A framework to guide the development of Trustworthy AI systems</strong><br><button class=copy-to-clipboard title="POLARIS: A framework to guide the development of Trustworthy AI systems" index=177>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-177 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SE<br>Categories: cs-SE, cs.SE<br>Keyword Score: 10<br>Keywords: Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05340v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05340v1.pdf filename=2402.05340v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In the ever-expanding landscape of Artificial Intelligence (AI), where innovation thrives and new products and services are continuously being delivered, ensuring that AI systems are designed and developed responsibly throughout their entire lifecycle is crucial. To this end, several AI ethics principles and guidelines have been issued to which AI systems should conform. Nevertheless, relying solely on high-level AI ethics principles is far from sufficient to ensure the responsible engineering of AI systems. In this field, AI professionals often navigate by sight. Indeed, while <b>recommendations</b> promoting Trustworthy AI (TAI) exist, these are often high-level statements that are difficult to translate into concrete implementation strategies. There is a significant gap between high-level AI ethics principles and low-level concrete practices for AI professionals. To address this challenge, our work presents an experience report where we develop a novel holistic framework for Trustworthy AI - designed to bridge the gap between theory and practice - and report insights from its application in an industrial case study. The framework is built on the result of a systematic review of the state of the practice, a survey, and think-aloud interviews with 34 AI practitioners. The framework, unlike most of those already in the literature, is designed to provide actionable guidelines and tools to support different types of stakeholders throughout the entire Software Development Life Cycle (SDLC). Our goal is to empower AI professionals to confidently navigate the ethical dimensions of TAI through practical insights, ensuring that the vast potential of AI is exploited responsibly for the benefit of society as a whole.</p></p class="citation"></blockquote><h2 id=cscy-3>cs.CY (3)</h2><h3 id=03--178248-examining-gender-and-racial-bias-in-large-vision-language-models-using-a-novel-dataset-of-parallel-images-kathleen-c-fraser-et-al-2024>(0/3 | 178/248) Examining Gender and Racial Bias in Large Vision-Language Models Using a Novel Dataset of Parallel Images (Kathleen C. Fraser et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kathleen C. Fraser, Svetlana Kiritchenko. (2024)<br><strong>Examining Gender and Racial Bias in Large Vision-Language Models Using a Novel Dataset of Parallel Images</strong><br><button class=copy-to-clipboard title="Examining Gender and Racial Bias in Large Vision-Language Models Using a Novel Dataset of Parallel Images" index=178>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-178 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CY<br>Categories: cs-CL, cs-CV, cs-CY, cs.CY<br>Keyword Score: 50<br>Keywords: Question Answering, Visual Question Answering, Large Language Model, Large Language Model, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05779v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05779v1.pdf filename=2402.05779v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Following on recent advances in <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> and subsequent chat models, a new wave of <b>large</b> <b>vision-language</b> <b>models</b> (LVLMs) has emerged. Such models can incorporate images as input in addition to text, and perform tasks such as <b>visual</b> <b>question</b> <b>answering,</b> image captioning, story generation, etc. Here, we examine potential gender and racial biases in such systems, based on the perceived characteristics of the people in the input images. To accomplish this, we present a new dataset PAIRS (PArallel Images for eveRyday Scenarios). The PAIRS dataset contains sets of AI-generated images of people, such that the images are highly similar in terms of background and <b>visual</b> <b>content,</b> <b>but</b> differ along the dimensions of gender (man, woman) and race (Black, white). By querying the LVLMs with such images, we observe significant differences in the responses according to the perceived gender or race of the person depicted.</p></p class="citation"></blockquote><h3 id=13--179248-a-framework-for-assessing-proportionate-intervention-with-face-recognition-systems-in-real-life-scenarios-pablo-negri-et-al-2024>(1/3 | 179/248) A Framework for Assessing Proportionate Intervention with Face Recognition Systems in Real-Life Scenarios (Pablo Negri et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Pablo Negri, Isabelle Hupont, Emilia Gomez. (2024)<br><strong>A Framework for Assessing Proportionate Intervention with Face Recognition Systems in Real-Life Scenarios</strong><br><button class=copy-to-clipboard title="A Framework for Assessing Proportionate Intervention with Face Recognition Systems in Real-Life Scenarios" index=179>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-179 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CY<br>Categories: cs-CY, cs.CY<br>Keyword Score: 10<br>Keywords: Face Recognition<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05731v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05731v1.pdf filename=2402.05731v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Face</b> <b>recognition</b> (FR) has reached a high technical maturity. However, its use needs to be carefully assessed from an ethical perspective, especially in sensitive scenarios. This is precisely the focus of this paper: the use of FR for the identification of specific subjects in moderately to densely crowded spaces (e.g. public spaces, sports stadiums, train stations) and law enforcement scenarios. In particular, there is a need to consider the trade-off between the need to protect privacy and fundamental rights of citizens as well as their safety. Recent Artificial Intelligence (AI) policies, notably the European AI Act, propose that such FR interventions should be proportionate and deployed only when strictly necessary. Nevertheless, concrete guidelines on how to address the concept of proportional FR intervention are lacking to date. This paper proposes a framework to contribute to assessing whether an FR intervention is proportionate or not for a given context of use in the above mentioned scenarios. It also identifies the main quantitative and qualitative variables relevant to the FR intervention decision (e.g. number of people in the scene, level of harm that the person(s) in search could perpetrate, consequences to individual rights and freedoms) and propose a 2D graphical model making it possible to balance these variables in terms of ethical cost vs security gain. Finally, different FR scenarios inspired by real-world deployments validate the proposed model. The framework is conceived as a simple support tool for decision makers when confronted with the deployment of an FR system.</p></p class="citation"></blockquote><h3 id=23--180248-a-survey-on-safe-multi-modal-learning-system-tianyi-zhao-et-al-2024>(2/3 | 180/248) A Survey on Safe Multi-Modal Learning System (Tianyi Zhao et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Tianyi Zhao, Liangliang Zhang, Yao Ma, Lu Cheng. (2024)<br><strong>A Survey on Safe Multi-Modal Learning System</strong><br><button class=copy-to-clipboard title="A Survey on Safe Multi-Modal Learning System" index=180>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-180 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CY<br>Categories: cs-AI, cs-CY, cs.CY<br>Keyword Score: 6<br>Keywords: Multi-modal, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05355v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05355v1.pdf filename=2402.05355v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>With the wide deployment of <b>multimodal</b> learning systems (MMLS) in real-world scenarios, safety concerns have become increasingly prominent. The absence of systematic research into their safety is a significant barrier to progress in this field. To bridge the gap, we present the first taxonomy for MMLS safety, identifying four essential pillars of these concerns. Leveraging this taxonomy, we conduct in-depth reviews for each pillar, highlighting key limitations based on the current state of development. Finally, we pinpoint unique challenges in MMLS safety and provide potential directions for future research.</p></p class="citation"></blockquote><h2 id=csir-2>cs.IR (2)</h2><h3 id=02--181248-counterclr-counterfactual-contrastive-learning-with-non-random-missing-data-in-recommendation-jun-wang-et-al-2024>(0/2 | 181/248) CounterCLR: Counterfactual Contrastive Learning with Non-random Missing Data in Recommendation (Jun Wang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jun Wang, Haoxuan Li, Chi Zhang, Dongxu Liang, Enyun Yu, Wenwu Ou, Wenjia Wang. (2024)<br><strong>CounterCLR: Counterfactual Contrastive Learning with Non-random Missing Data in Recommendation</strong><br><button class=copy-to-clipboard title="CounterCLR: Counterfactual Contrastive Learning with Non-random Missing Data in Recommendation" index=181>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-181 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 50<br>Keywords: Contrastive Learning, Counter-factual, Recommendation, Recommender System, Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05740v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05740v1.pdf filename=2402.05740v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Recommender</b> <b>systems</b> are designed to learn user preferences from observed feedback and comprise many fundamental tasks, such as rating prediction and post-click conversion rate (pCVR) prediction. However, the observed feedback usually suffer from two issues: selection bias and data sparsity, where biased and insufficient feedback seriously degrade the performance of <b>recommender</b> <b>systems</b> in terms of accuracy and ranking. Existing solutions for handling the issues, such as data imputation and inverse propensity score, are highly susceptible to additional trained imputation or propensity models. In this work, we propose a novel <b>counterfactual</b> <b>contrastive</b> <b>learning</b> framework for <b>recommendation,</b> named CounterCLR, to tackle the problem of non-random missing data by exploiting the advances in contrast learning. Specifically, the proposed CounterCLR employs a deep representation network, called CauNet, to infer non-random missing data in <b>recommendations</b> and perform user preference modeling by further introducing a <b>self-supervised</b> <b>contrastive</b> <b>learning</b> task. Our CounterCLR mitigates the selection bias problem without the need for additional models or estimators, while also enhancing the generalization ability in cases of sparse data. Experiments on real-world datasets demonstrate the effectiveness and superiority of our method.</p></p class="citation"></blockquote><h3 id=12--182248-natural-language-user-profiles-for-transparent-and-scrutable-recommendations-jerome-ramos-et-al-2024>(1/2 | 182/248) Natural Language User Profiles for Transparent and Scrutable Recommendations (Jerome Ramos et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jerome Ramos, Hossen A. Rahmani, Xi Wang, Xiao Fu, Aldo Lipani. (2024)<br><strong>Natural Language User Profiles for Transparent and Scrutable Recommendations</strong><br><button class=copy-to-clipboard title="Natural Language User Profiles for Transparent and Scrutable Recommendations" index=182>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-182 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IR<br>Categories: cs-IR, cs.IR<br>Keyword Score: 20<br>Keywords: Recommendation, Recommender System<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05810v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05810v1.pdf filename=2402.05810v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Current state-of-the-art <b>recommender</b> <b>systems</b> predominantly rely on either implicit or explicit feedback from users to suggest new items. While effective in recommending novel options, these conventional systems often use uninterpretable embeddings. This lack of transparency not only limits user understanding of why certain items are suggested but also reduces the user&rsquo;s ability to easily scrutinize and edit their preferences. For example, if a user has a change in interests, they would need to make significant changes to their interaction history to adjust the model&rsquo;s <b>recommendations.</b> To address these limitations, we introduce a novel method that utilizes user reviews to craft personalized, natural language profiles describing users&rsquo; preferences. Through these descriptive profiles, our system provides transparent <b>recommendations</b> in natural language. Our evaluations show that this novel approach maintains a performance level on par with established <b>recommender</b> <b>systems,</b> but with the added benefits of transparency and user control. By enabling users to scrutinize why certain items are recommended, they can more easily verify, adjust, and have greater autonomy over their <b>recommendations.</b></p></p class="citation"></blockquote><h2 id=csro-15>cs.RO (15)</h2><h3 id=015--183248-real-world-robot-applications-of-foundation-models-a-review-kento-kawaharazuka-et-al-2024>(0/15 | 183/248) Real-World Robot Applications of Foundation Models: A Review (Kento Kawaharazuka et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kento Kawaharazuka, Tatsuya Matsushima, Andrew Gambardella, Jiaxian Guo, Chris Paxton, Andy Zeng. (2024)<br><strong>Real-World Robot Applications of Foundation Models: A Review</strong><br><button class=copy-to-clipboard title="Real-World Robot Applications of Foundation Models: A Review" index=183>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-183 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-CV, cs-LG, cs-RO, cs.RO<br>Keyword Score: 40<br>Keywords: Foundation Model, Large Language Model, Large Language Model, Vision-and-Language<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05741v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05741v1.pdf filename=2402.05741v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent developments in <b>foundation</b> <b>models,</b> like <b>Large</b> <b>Language</b> <b>Models</b> <b>(LLMs)</b> and <b>Vision-Language</b> Models (VLMs), trained on extensive data, facilitate flexible application across different tasks and modalities. Their impact spans various fields, including healthcare, education, and robotics. This paper provides an overview of the practical application of <b>foundation</b> <b>models</b> in real-world robotics, with a primary emphasis on the replacement of specific components within existing robot systems. The summary encompasses the perspective of input-output relationships in <b>foundation</b> <b>models,</b> as well as their role in perception, motion planning, and control within the field of robotics. This paper concludes with a discussion of future challenges and implications for practical robot applications.</p></p class="citation"></blockquote><h3 id=115--184248-driving-everywhere-with-large-language-model-policy-adaptation-boyi-li-et-al-2024>(1/15 | 184/248) Driving Everywhere with Large Language Model Policy Adaptation (Boyi Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Boyi Li, Yue Wang, Jiageng Mao, Boris Ivanovic, Sushant Veer, Karen Leung, Marco Pavone. (2024)<br><strong>Driving Everywhere with Large Language Model Policy Adaptation</strong><br><button class=copy-to-clipboard title="Driving Everywhere with Large Language Model Policy Adaptation" index=184>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-184 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-CL, cs-RO, cs.RO<br>Keyword Score: 30<br>Keywords: Zero-shot, Large Language Model, Large Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05932v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05932v1.pdf filename=2402.05932v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Adapting driving behavior to new environments, customs, and laws is a long-standing problem in autonomous driving, precluding the widespread deployment of autonomous vehicles (AVs). In this paper, we present LLaDA, a simple yet powerful tool that enables human drivers and autonomous vehicles alike to drive everywhere by adapting their tasks and motion plans to traffic rules in new locations. LLaDA achieves this by leveraging the impressive <b>zero-shot</b> generalizability of <b>large</b> <b>language</b> <b>models</b> <b>(LLMs)</b> in interpreting the traffic rules in the local driver handbook. Through an extensive user study, we show that LLaDA&rsquo;s instructions are useful in disambiguating in-the-wild unexpected situations. We also demonstrate LLaDA&rsquo;s ability to adapt AV motion planning policies in real-world datasets; LLaDA outperforms baseline planning approaches on all our metrics. Please check our website for more details: <a href=https://boyiliee.github.io/llada>https://boyiliee.github.io/llada</a>.</p></p class="citation"></blockquote><h3 id=215--185248-uplam-robust-panoptic-localization-and-mapping-leveraging-perception-uncertainties-kshitij-sirohi-et-al-2024>(2/15 | 185/248) uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception Uncertainties (Kshitij Sirohi et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kshitij Sirohi, Daniel Büscher, Wolfram Burgard. (2024)<br><strong>uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception Uncertainties</strong><br><button class=copy-to-clipboard title="uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception Uncertainties" index=185>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-185 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 30<br>Keywords: Convolution, Convolutional Neural Network, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05840v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05840v1.pdf filename=2402.05840v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The availability of a reliable map and a robust localization system is critical for the operation of an autonomous vehicle. In a modern system, both mapping and localization solutions generally employ <b>convolutional</b> <b>neural</b> <b>network</b> <b>(CNN)</b> &ndash;based perception. Hence, any algorithm should consider potential errors in perception for safe and robust functioning. In this work, we present uncertainty-aware panoptic Localization and Mapping (uPLAM), which employs perception uncertainty as a bridge to fuse the perception information with classical localization and mapping approaches. We introduce an uncertainty-based map aggregation technique to create a long-term panoptic bird&rsquo;s eye view map and provide an associated mapping uncertainty. Our map consists of surface semantics and landmarks with unique IDs. Moreover, we present panoptic uncertainty-aware particle filter-based localization. To this end, we propose an uncertainty-based particle importance weight calculation for the adaptive incorporation of perception information into localization. We also present a new dataset for evaluating long-term panoptic mapping and map-based localization. Extensive evaluations showcase that our proposed uncertainty incorporation leads to better mapping with reliable uncertainty estimates and accurate localization. We make our dataset and code available at: \url{http://uplam.cs.uni-freiburg.de}</p></p class="citation"></blockquote><h3 id=315--186248-learning-to-control-emulated-muscles-in-real-robots-towards-exploiting-bio-inspired-actuator-morphology-pierre-schumacher-et-al-2024>(3/15 | 186/248) Learning to Control Emulated Muscles in Real Robots: Towards Exploiting Bio-Inspired Actuator Morphology (Pierre Schumacher et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Pierre Schumacher, Lorenz Krause, Jan Schneider, Dieter Büchler, Georg Martius, Daniel Haeufle. (2024)<br><strong>Learning to Control Emulated Muscles in Real Robots: Towards Exploiting Bio-Inspired Actuator Morphology</strong><br><button class=copy-to-clipboard title="Learning to Control Emulated Muscles in Real Robots: Towards Exploiting Bio-Inspired Actuator Morphology" index=186>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-186 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 30<br>Keywords: Reinforcement Learning, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05371v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05371v1.pdf filename=2402.05371v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent studies have demonstrated the immense potential of exploiting muscle actuator morphology for natural and robust movement &ndash; in <b>simulation.</b> A validation on real robotic hardware is yet missing. In this study, we emulate muscle actuator properties on hardware in real-time, taking advantage of modern and affordable electric motors. We demonstrate that our setup can emulate a simplified muscle model on a real robot while being controlled by a learned policy. We improve upon an existing muscle model by deriving a damping rule that ensures that the model is not only performant and stable but also tuneable for the real hardware. Our policies are trained by <b>reinforcement</b> <b>learning</b> entirely in <b>simulation,</b> where we show that previously reported benefits of muscles extend to the case of quadruped locomotion and hopping: the learned policies are more robust and exhibit more regular gaits. Finally, we confirm that the learned policies can be executed on real hardware and show that sim-to-real transfer with real-time emulated muscles on a quadruped robot is possible. These results show that artificial muscles can be highly beneficial actuators for future generations of robust legged robots.</p></p class="citation"></blockquote><h3 id=415--187248-gliding-in-extreme-waters-dynamic-modeling-and-nonlinear-control-of-an-agile-underwater-glider-hanzhi-yang-et-al-2024>(4/15 | 187/248) Gliding in extreme waters: Dynamic Modeling and Nonlinear Control of an Agile Underwater Glider (Hanzhi Yang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hanzhi Yang, Nina Mahmoudian. (2024)<br><strong>Gliding in extreme waters: Dynamic Modeling and Nonlinear Control of an Agile Underwater Glider</strong><br><button class=copy-to-clipboard title="Gliding in extreme waters: Dynamic Modeling and Nonlinear Control of an Agile Underwater Glider" index=187>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-187 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs-SY, cs.RO, eess-SY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.06055v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.06055v1.pdf filename=2402.06055v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper describes the modeling of a custom-made underwater glider capable of flexible maneuvers in constrained areas and proposes a control system. Due to the lack of external actuators, underwater gliders can be greatly influenced by environmental disturbance. In addition, the nonlinearity of the system affects the motions during the transition between each flight segment. Here, a data-driven parameter estimation experimental methodology is proposed to identify the nonlinear dynamics model for our underwater glider using an underwater motion capture system. Then, a nonlinear system controller is designed based on Lyapunov function to overcome environmental disturbance, potential modeling errors, and nonlinearity during flight state transitions. The capability of lowering the impact of environmental disturbance is validated in <b>simulations.</b> A hybrid control system applying PID controller to maintain steady state flights and the proposed controller to switch between states is also demonstrated by performing complex maneuvers in <b>simulation.</b> The proposed control system can be applied to gliders for reliable navigation in dynamic water areas such as fjords where the sea conditions may vary from calm to rough seasonally.</p></p class="citation"></blockquote><h3 id=515--188248-on-experimental-emulation-of-printability-and-fleet-aware-generic-mesh-decomposition-for-enabling-aerial-3d-printing-marios-nektarios-stamatopoulos-et-al-2024>(5/15 | 188/248) On Experimental Emulation of Printability and Fleet Aware Generic Mesh Decomposition for Enabling Aerial 3D Printing (Marios-Nektarios Stamatopoulos et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Marios-Nektarios Stamatopoulos, Avijit Banerjee, George Nikolakopoulos. (2024)<br><strong>On Experimental Emulation of Printability and Fleet Aware Generic Mesh Decomposition for Enabling Aerial 3D Printing</strong><br><button class=copy-to-clipboard title="On Experimental Emulation of Printability and Fleet Aware Generic Mesh Decomposition for Enabling Aerial 3D Printing" index=188>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-188 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05853v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05853v1.pdf filename=2402.05853v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This article introduces an experimental emulation of a novel chunk-based flexible multi-DoF aerial 3D printing framework. The experimental demonstration of the overall autonomy focuses on precise motion planning and task allocation for a UAV, traversing through a series of planned space-filling paths involved in the aerial 3D printing process without physically depositing the overlaying material. The flexible multi-DoF aerial 3D printing is a newly developed framework and has the potential to strategically distribute the envisioned 3D model to be printed into small, manageable chunks suitable for distributed 3D printing. Moreover, by harnessing the dexterous flexibility due to the 6 DoF motion of UAV, the framework enables the provision of integrating the overall autonomy stack, potentially opening up an entirely new frontier in additive manufacturing. However, it&rsquo;s essential to note that the feasibility of this pioneering concept is still in its very early stage of development, which yet needs to be experimentally verified. Towards this direction, experimental emulation serves as the crucial stepping stone, providing a pseudo mockup scenario by virtual material deposition, helping to identify technological gaps from <b>simulation</b> to reality. Experimental emulation results, supported by critical analysis and discussion, lay the foundation for addressing the technological and research challenges to significantly push the boundaries of the state-of-the-art 3D printing mechanism.</p></p class="citation"></blockquote><h3 id=615--189248-an-optimal-control-formulation-of-tool-affordance-applied-to-impact-tasks-boyang-ti-et-al-2024>(6/15 | 189/248) An Optimal Control Formulation of Tool Affordance Applied to Impact Tasks (Boyang Ti et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Boyang Ti, Yongsheng Gao, Jie Zhao, Sylvain Calinon. (2024)<br><strong>An Optimal Control Formulation of Tool Affordance Applied to Impact Tasks</strong><br><button class=copy-to-clipboard title="An Optimal Control Formulation of Tool Affordance Applied to Impact Tasks" index=189>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-189 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05502v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05502v2.pdf filename=2402.05502v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Humans use tools to complete impact-aware tasks such as hammering a nail or playing tennis. The postures adopted to use these tools can significantly influence the performance of these tasks, where the force or velocity of the hand holding a tool plays a crucial role. The underlying motion planning challenge consists of grabbing the tool in preparation for the use of this tool with an optimal body posture. Directional manipulability describes the dexterity of force and velocity in a joint configuration along a specific direction. In order to take directional manipulability and tool affordances into account, we apply an optimal control method combining iterative linear quadratic regulator(iLQR) with the alternating direction method of multipliers(ADMM). Our approach considers the notion of tool affordances to solve motion planning problems, by introducing a cost based on directional velocity manipulability. The proposed approach is applied to impact tasks in <b>simulation</b> and on a real 7-axis robot, specifically in a nail-hammering task with the assistance of a pilot hole. Our comparison study demonstrates the importance of maximizing directional manipulability in impact-aware tasks.</p></p class="citation"></blockquote><h3 id=715--190248-cure-simulation-augmented-auto-tuning-in-robotics-md-abir-hossen-et-al-2024>(7/15 | 190/248) CURE: Simulation-Augmented Auto-Tuning in Robotics (Md Abir Hossen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Md Abir Hossen, Sonam Kharade, Jason M. O&rsquo;Kane, Bradley Schmerl, David Garlan, Pooyan Jamshidi. (2024)<br><strong>CURE: Simulation-Augmented Auto-Tuning in Robotics</strong><br><button class=copy-to-clipboard title="CURE: Simulation-Augmented Auto-Tuning in Robotics" index=190>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-190 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-RO, cs.RO<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05399v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05399v1.pdf filename=2402.05399v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Robotic systems are typically composed of various subsystems, such as localization and navigation, each encompassing numerous configurable components (e.g., selecting different planning algorithms). Once an algorithm has been selected for a component, its associated configuration options must be set to the appropriate values. Configuration options across the system stack interact non-trivially. Finding optimal configurations for highly configurable robots to achieve desired performance poses a significant challenge due to the interactions between configuration options across software and hardware that result in an exponentially large and complex configuration space. These challenges are further compounded by the need for transferability between different environments and robotic platforms. Data efficient optimization algorithms (e.g., Bayesian optimization) have been increasingly employed to automate the tuning of configurable parameters in cyber-physical systems. However, such optimization algorithms converge at later stages, often after exhausting the allocated budget (e.g., optimization steps, allotted time) and lacking transferability. This paper proposes CURE &ndash; a method that identifies causally relevant configuration options, enabling the optimization process to operate in a reduced search space, thereby enabling faster optimization of robot performance. CURE abstracts the causal relationships between various configuration options and robot performance objectives by learning a causal model in the source (a low-cost environment such as the Gazebo simulator) and applying the learned knowledge to perform optimization in the target (e.g., Turtlebot 3 physical robot). We demonstrate the effectiveness and transferability of CURE by conducting experiments that involve varying degrees of deployment changes in both physical robots and <b>simulation.</b></p></p class="citation"></blockquote><h3 id=815--191248-real-world-fluid-directed-rigid-body-control-via-deep-reinforcement-learning-mohak-bhardwaj-et-al-2024>(8/15 | 191/248) Real-World Fluid Directed Rigid Body Control via Deep Reinforcement Learning (Mohak Bhardwaj et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mohak Bhardwaj, Thomas Lampe, Michael Neunert, Francesco Romano, Abbas Abdolmaleki, Arunkumar Byravan, Markus Wulfmeier, Martin Riedmiller, Jonas Buchli. (2024)<br><strong>Real-World Fluid Directed Rigid Body Control via Deep Reinforcement Learning</strong><br><button class=copy-to-clipboard title="Real-World Fluid Directed Rigid Body Control via Deep Reinforcement Learning" index=191>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-191 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-LG, cs-RO, cs.RO<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.06102v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.06102v1.pdf filename=2402.06102v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advances in real-world applications of <b>reinforcement</b> <b>learning</b> (RL) have relied on the ability to accurately simulate systems at scale. However, domains such as fluid dynamical systems exhibit complex dynamic phenomena that are hard to simulate at high integration rates, limiting the direct application of modern deep RL algorithms to often expensive or safety critical hardware. In this work, we introduce &ldquo;Box o Flows&rdquo;, a novel benchtop experimental control system for systematically evaluating RL algorithms in dynamic real-world scenarios. We describe the key components of the Box o Flows, and through a series of experiments demonstrate how state-of-the-art model-free RL algorithms can synthesize a variety of complex behaviors via simple reward specifications. Furthermore, we explore the role of offline RL in data-efficient hypothesis testing by reusing past experiences. We believe that the insights gained from this preliminary study and the availability of systems like the Box o Flows support the way forward for developing systematic RL algorithms that can be generally applied to complex, dynamical systems. Supplementary material and videos of experiments are available at <a href=https://sites.google.com/view/box-o-flows/home>https://sites.google.com/view/box-o-flows/home</a>.</p></p class="citation"></blockquote><h3 id=915--192248-body-schema-acquisition-through-active-learning-ruben-martinez-cantin-et-al-2024>(9/15 | 192/248) Body Schema Acquisition through Active Learning (Ruben Martinez-Cantin et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ruben Martinez-Cantin, Manuel Lopes, Luis Montesano. (2024)<br><strong>Body Schema Acquisition through Active Learning</strong><br><button class=copy-to-clipboard title="Body Schema Acquisition through Active Learning" index=192>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-192 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 10<br>Keywords: Active Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.06067v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.06067v1.pdf filename=2402.06067v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present an <b>active</b> <b>learning</b> algorithm for the problem of body schema learning, i.e. estimating a kinematic model of a serial robot. The learning process is done online using Recursive Least Squares (RLS) estimation, which outperforms gradient methods usually applied in the literature. In addiction, the method provides the required information to apply an <b>active</b> <b>learning</b> algorithm to find the optimal set of robot configurations and observations to improve the learning process. By selecting the most informative observations, the proposed method minimizes the required amount of data. We have developed an efficient version of the <b>active</b> <b>learning</b> algorithm to select the points in real-time. The algorithms have been tested and compared using both simulated environments and a real humanoid robot.</p></p class="citation"></blockquote><h3 id=1015--193248-intelligent-mode-switching-framework-for-teleoperation-burak-kizilkaya-et-al-2024>(10/15 | 193/248) Intelligent Mode-switching Framework for Teleoperation (Burak Kizilkaya et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Burak Kizilkaya, Changyang She, Guodong Zhao, Muhammad Ali Imran. (2024)<br><strong>Intelligent Mode-switching Framework for Teleoperation</strong><br><button class=copy-to-clipboard title="Intelligent Mode-switching Framework for Teleoperation" index=193>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-193 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-LG, cs-NI, cs-RO, cs.RO<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.06047v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.06047v1.pdf filename=2402.06047v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Teleoperation can be very difficult due to limited perception, high communication latency, and limited degrees of freedom (DoFs) at the operator side. Autonomous teleoperation is proposed to overcome this difficulty by predicting user intentions and performing some parts of the task autonomously to decrease the demand on the operator and increase the task completion rate. However, decision-making for mode-switching is generally assumed to be done by the operator, which brings an extra DoF to be controlled by the operator and introduces extra mental demand. On the other hand, the communication perspective is not investigated in the current literature, although communication imperfections and resource limitations are the main bottlenecks for teleoperation. In this study, we propose an intelligent mode-switching framework by jointly considering mode-switching and communication systems. User intention recognition is done at the operator side. Based on user intention recognition, a deep <b>reinforcement</b> <b>learning</b> (DRL) agent is trained and deployed at the operator side to seamlessly switch between autonomous and teleoperation modes. A real-world data set is collected from our teleoperation testbed to train both user intention recognition and DRL algorithms. Our results show that the proposed framework can achieve up to 50% communication load reduction with improved task completion probability.</p></p class="citation"></blockquote><h3 id=1115--194248-anatomy-of-a-robotaxi-crash-lessons-from-the-cruise-pedestrian-dragging-mishap-philip-koopman-2024>(11/15 | 194/248) Anatomy of a Robotaxi Crash: Lessons from the Cruise Pedestrian Dragging Mishap (Philip Koopman, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Philip Koopman. (2024)<br><strong>Anatomy of a Robotaxi Crash: Lessons from the Cruise Pedestrian Dragging Mishap</strong><br><button class=copy-to-clipboard title="Anatomy of a Robotaxi Crash: Lessons from the Cruise Pedestrian Dragging Mishap" index=194>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-194 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-AI, cs-RO, cs.RO<br>Keyword Score: 10<br>Keywords: Recommendation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.06046v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.06046v1.pdf filename=2402.06046v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>An October 2023 crash between a GM Cruise robotaxi and a pedestrian in San Francisco resulted not only in a severe injury, but also dramatic upheaval at that company that will likely have lasting effects throughout the industry. The issues stem not just from the crash facts themselves, but also how Cruise mishandled dealing with their robotaxi dragging a pedestrian under the vehicle after the initial post-crash stop. A pair of external investigation reports provide raw material describing the incident and critique the company response from a regulatory interaction point of view, but did not include potential safety <b>recommendations</b> in scope. We use that report material to highlight specific facts and relationships between events by tying together different pieces of the report material. We then explore safety lessons that might be learned with regard to technology, operational safety practices, and organizational reaction to incidents.</p></p class="citation"></blockquote><h3 id=1215--195248-a-versatile-robotic-hand-with-3d-perception-force-sensing-for-autonomous-manipulation-nikolaus-correll-et-al-2024>(12/15 | 195/248) A versatile robotic hand with 3D perception, force sensing for autonomous manipulation (Nikolaus Correll et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nikolaus Correll, Dylan Kriegman, Stephen Otto, James Watson. (2024)<br><strong>A versatile robotic hand with 3D perception, force sensing for autonomous manipulation</strong><br><button class=copy-to-clipboard title="A versatile robotic hand with 3D perception, force sensing for autonomous manipulation" index=195>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-195 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 10<br>Keywords: Object Detection<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.06018v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.06018v1.pdf filename=2402.06018v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We describe a force-controlled robotic gripper with built-in tactile and 3D perception. We also describe a complete autonomous manipulation pipeline consisting of <b>object</b> <b>detection,</b> segmentation, point cloud processing, force-controlled manipulation, and symbolic (re)-planning. The design emphasizes versatility in terms of applications, manufacturability, use of commercial off-the-shelf parts, and open-source software. We validate the design by characterizing force control (achieving up to 32N, controllable in steps of 0.08N), force measurement, and two manipulation demonstrations: assembly of the Siemens gear assembly problem, and a sensor-based stacking task requiring replanning. These demonstrate robust execution of long sequences of sensor-based manipulation tasks, which makes the resulting platform a solid foundation for researchers in task-and-motion planning, educators, and quick prototyping of household, industrial and warehouse automation tasks.</p></p class="citation"></blockquote><h3 id=1315--196248-funcgrasp-learning-object-centric-neural-grasp-functions-from-single-annotated-example-object-hanzhi-chen-et-al-2024>(13/15 | 196/248) FuncGrasp: Learning Object-Centric Neural Grasp Functions from Single Annotated Example Object (Hanzhi Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hanzhi Chen, Binbin Xu, Stefan Leutenegger. (2024)<br><strong>FuncGrasp: Learning Object-Centric Neural Grasp Functions from Single Annotated Example Object</strong><br><button class=copy-to-clipboard title="FuncGrasp: Learning Object-Centric Neural Grasp Functions from Single Annotated Example Object" index=196>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-196 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-CV, cs-RO, cs.RO<br>Keyword Score: 10<br>Keywords: Unsupervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05644v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05644v1.pdf filename=2402.05644v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We present FuncGrasp, a framework that can infer dense yet reliable grasp configurations for unseen objects using one annotated object and single-view RGB-D observation via categorical priors. Unlike previous works that only transfer a set of grasp poses, FuncGrasp aims to transfer infinite configurations parameterized by an object-centric continuous grasp function across varying instances. To ease the transfer process, we propose Neural Surface Grasping Fields (NSGF), an effective neural representation defined on the surface to densely encode grasp configurations. Further, we exploit function-to-function transfer using sphere primitives to establish semantically meaningful categorical correspondences, which are learned in an <b>unsupervised</b> fashion without any expert knowledge. We showcase the effectiveness through extensive experiments in both simulators and the real world. Remarkably, our framework significantly outperforms several strong baseline methods in terms of density and reliability for generated grasps.</p></p class="citation"></blockquote><h3 id=1415--197248-youve-got-to-feel-it-to-believe-it-multi-modal-bayesian-inference-for-semantic-and-property-prediction-parker-ewen-et-al-2024>(14/15 | 197/248) You&rsquo;ve Got to Feel It To Believe It: Multi-Modal Bayesian Inference for Semantic and Property Prediction (Parker Ewen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Parker Ewen, Hao Chen, Yuzhen Chen, Anran Li, Anup Bagali, Gitesh Gunjal, Ram Vasudevan. (2024)<br><strong>You&rsquo;ve Got to Feel It To Believe It: Multi-Modal Bayesian Inference for Semantic and Property Prediction</strong><br><button class=copy-to-clipboard title="You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for Semantic and Property Prediction" index=197>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-197 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.RO<br>Categories: cs-RO, cs.RO<br>Keyword Score: 3<br>Keywords: Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05872v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05872v2.pdf filename=2402.05872v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Robots must be able to understand their surroundings to perform complex tasks in challenging environments and many of these complex tasks require estimates of physical properties such as friction or weight. Estimating such properties using learning is challenging due to the large amounts of labelled data required for training and the difficulty of updating these learned models online at run time. To overcome these challenges, this paper introduces a novel, <b>multi-modal</b> approach for representing semantic predictions and physical property estimates jointly in a probabilistic manner. By using conjugate pairs, the proposed method enables closed-form Bayesian updates given visual and tactile measurements without requiring additional training data. The efficacy of the proposed algorithm is demonstrated through several hardware experiments. In particular, this paper illustrates that by conditioning semantic classifications on physical properties, the proposed method quantitatively outperforms state-of-the-art semantic classification methods that rely on vision alone. To further illustrate its utility, the proposed method is used in several applications including to represent affordance-based properties probabilistically and a challenging terrain traversal task using a legged robot. In the latter task, the proposed method represents the coefficient of friction of the terrain probabilistically, which enables the use of an on-line risk-aware planner that switches the legged robot from a dynamic gait to a static, stable gait when the expected value of the coefficient of friction falls below a given threshold. Videos of these case studies are presented in the multimedia attachment. The proposed framework includes an open-source C++ and ROS interface.</p></p class="citation"></blockquote><h2 id=csce-2>cs.CE (2)</h2><h3 id=02--198248-i-fenn-with-temporal-convolutional-networks-expediting-the-load-history-analysis-of-non-local-gradient-damage-propagation-panos-pantidis-et-al-2024>(0/2 | 198/248) I-FENN with Temporal Convolutional Networks: expediting the load-history analysis of non-local gradient damage propagation (Panos Pantidis et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Panos Pantidis, Habiba Eldababy, Diab Abueidda, Mostafa E. Mobasher. (2024)<br><strong>I-FENN with Temporal Convolutional Networks: expediting the load-history analysis of non-local gradient damage propagation</strong><br><button class=copy-to-clipboard title="I-FENN with Temporal Convolutional Networks: expediting the load-history analysis of non-local gradient damage propagation" index=198>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-198 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CE<br>Categories: cs-CE, cs.CE<br>Keyword Score: 40<br>Keywords: Convolution, Convolutional Neural Network, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05460v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05460v1.pdf filename=2402.05460v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this paper, we demonstrate for the first time how the Integrated Finite Element Neural Network (I-FENN) framework, previously proposed by the authors, can efficiently simulate the entire loading history of non-local gradient damage propagation. To achieve this goal, we first adopt a Temporal <b>Convolutional</b> <b>Network</b> (TCN) as the neural network of choice to capture the history-dependent evolution of the non-local strain in a coarsely meshed domain. The quality of the network predictions governs the computational performance of I-FENN, and therefore we perform an extended investigation aimed at enhancing them. We explore a data-driven vs. physics-informed TCN setup to arrive at an optimum network training, evaluating the network based on a coherent set of relevant performance metrics. We address the crucial issue of training a physics-informed network with input data that span vastly different length scales by proposing a systematic way of input normalization and output un-normalization. We then integrate the trained TCN within the nonlinear iterative FEM solver and apply I-FENN to simulate the damage propagation analysis. I-FENN is always applied in mesh idealizations different from the one used for the TCN training, showcasing the framework&rsquo;s ability to be used at progressively refined mesh resolutions. We illustrate several cases that I-FENN completes the <b>simulation</b> using either a modified or a full Newton-Raphson scheme, and we showcase its computational savings compared to both the classical monolithic and staggered FEM solvers. We underline that we satisfy very strict convergence criteria for every increment across the entire <b>simulation,</b> providing clear evidence of the robustness and accuracy of I-FENN. All the code and data used in this work will be made publicly available upon publication of the article.</p></p class="citation"></blockquote><h3 id=12--199248-shape-optimization-of-eigenfrequencies-in-mems-gyroscopes-daniel-schiwietz-et-al-2024>(1/2 | 199/248) Shape Optimization of Eigenfrequencies in MEMS Gyroscopes (Daniel Schiwietz et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Daniel Schiwietz, Marian Hörsting, Eva Maria Weig, Peter Degenfeld-Schonburg, Matthias Wenzel. (2024)<br><strong>Shape Optimization of Eigenfrequencies in MEMS Gyroscopes</strong><br><button class=copy-to-clipboard title="Shape Optimization of Eigenfrequencies in MEMS Gyroscopes" index=199>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-199 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.CE<br>Categories: cs-CE, cs.CE<br>Keyword Score: 10<br>Keywords: Human Intervention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05837v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05837v1.pdf filename=2402.05837v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Microelectromechanical systems (MEMS) gyroscopes are widely used in consumer and automotive applications. They have to fulfill a vast number of product requirements which lead to complex mechanical designs of the resonating structure. Arriving at a final design is a cumbersome process that relies heavily on <b>human</b> <b>experience</b> in conjunction with design optimization methods. In this work, we apply node-based shape optimization to the design of a MEMS gyroscope. For that purpose, we parametrize the coordinates of the nodes of the finite element method (FEM) mesh that discretize the shapes of the springs. We then implement the gradients of the mechanical eigenfrequencies and typical MEMS manufacturability constraints, with respect to the design parameters, in a FEM code. Using gradient-based optimization we tune the gyroscope&rsquo;s frequency split and shift spurious modes away from the first three multiples of the gyroscope&rsquo;s drive frequency while manufacturability constraints are fulfilled. The resulting optimized design exhibits novel geometrical shapes which defy any <b>human</b> <b>intuition.</b> Overall, we demonstrate that shape optimization can not only solve optimization problems in MEMS design without required <b>human</b> <b>intervention,</b> but also explores geometry solutions which can otherwise not be addressed. In this way, node-based shape optimization opens up a much larger space of possible design solutions, which is crucial for facing the ever increasing product requirements. Our approach is generic and applicable to many other types of MEMS resonators.</p></p class="citation"></blockquote><h2 id=q-bioqm-1>q-bio.QM (1)</h2><h3 id=01--200248-anfinsen-goes-neural-a-graphical-model-for-conditional-antibody-design-nayoung-kim-et-al-2024>(0/1 | 200/248) Anfinsen Goes Neural: a Graphical Model for Conditional Antibody Design (Nayoung Kim et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nayoung Kim, Minsu Kim, Jinkyoo Park. (2024)<br><strong>Anfinsen Goes Neural: a Graphical Model for Conditional Antibody Design</strong><br><button class=copy-to-clipboard title="Anfinsen Goes Neural: a Graphical Model for Conditional Antibody Design" index=200>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-200 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: q-bio.QM<br>Categories: cs-LG, q-bio-QM, q-bio.QM<br>Keyword Score: 33<br>Keywords: Graph Neural Network, Graph Neural Network, Benchmarking, Pre-trained Language Model<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05982v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05982v1.pdf filename=2402.05982v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Antibody design plays a pivotal role in advancing therapeutics. Although deep learning has made rapid progress in this field, existing methods make limited use of general protein knowledge and assume a graphical model (GM) that violates empirical findings on proteins. To address these limitations, we present Anfinsen Goes Neural (AGN), a graphical model that uses a pre-trained protein language model <b>(pLM)</b> and encodes a seminal finding on proteins called Anfinsen&rsquo;s dogma. Our framework follows a two-step process of sequence generation with <b>pLM</b> and structure prediction with <b>graph</b> <b>neural</b> <b>network</b> <b>(GNN).</b> Experiments show that our approach outperforms state-of-the-art results on <b>benchmark</b> experiments. We also address a critical limitation of non-autoregressive models &ndash; namely, that they tend to generate unrealistic sequences with overly repeating tokens. To resolve this, we introduce a composition-based regularization term to the cross-entropy objective that allows an efficient trade-off between high performance and low token repetition. We demonstrate that our approach establishes a Pareto frontier over the current state-of-the-art. Our code is available at <a href=https://github.com/lkny123/AGN>https://github.com/lkny123/AGN</a>.</p></p class="citation"></blockquote><h2 id=csma-2>cs.MA (2)</h2><h3 id=02--201248-offline-risk-sensitive-rl-with-partial-observability-to-enhance-performance-in-human-robot-teaming-giorgio-angelotti-et-al-2024>(0/2 | 201/248) Offline Risk-sensitive RL with Partial Observability to Enhance Performance in Human-Robot Teaming (Giorgio Angelotti et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Giorgio Angelotti, Caroline P. C. Chanel, Adam H. M. Pinto, Christophe Lounis, Corentin Chauffaut, Nicolas Drougard. (2024)<br><strong>Offline Risk-sensitive RL with Partial Observability to Enhance Performance in Human-Robot Teaming</strong><br><button class=copy-to-clipboard title="Offline Risk-sensitive RL with Partial Observability to Enhance Performance in Human-Robot Teaming" index=201>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-201 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.MA<br>Categories: cs-AI, cs-HC, cs-LG, cs-MA, cs-RO, cs.MA<br>Keyword Score: 30<br>Keywords: Markov Decision Process, Offline Reinforcement Learning, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05703v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05703v1.pdf filename=2402.05703v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The integration of physiological computing into mixed-initiative human-robot interaction systems offers valuable advantages in autonomous task allocation by incorporating real-time features as human state observations into the decision-making system. This approach may alleviate the cognitive load on human operators by intelligently allocating mission tasks between agents. Nevertheless, accommodating a diverse pool of human participants with varying physiological and behavioral measurements presents a substantial challenge. To address this, resorting to a probabilistic framework becomes necessary, given the inherent uncertainty and partial observability on the human&rsquo;s state. Recent research suggests to learn a Partially Observable <b>Markov</b> <b>Decision</b> <b>Process</b> (POMDP) model from a data set of previously collected experiences that can be solved using <b>Offline</b> <b>Reinforcement</b> <b>Learning</b> (ORL) methods. In the present work, we not only highlight the potential of partially observable representations and physiological measurements to improve human operator state estimation and performance, but also enhance the overall mission effectiveness of a human-robot team. Importantly, as the fixed data set may not contain enough information to fully represent complex stochastic processes, we propose a method to incorporate model uncertainty, thus enabling risk-sensitive sequential decision-making. Experiments were conducted with a group of twenty-six human participants within a simulated robot teleoperation environment, yielding empirical evidence of the method&rsquo;s efficacy. The obtained adaptive task allocation policy led to statistically significant higher scores than the one that was used to collect the data set, allowing for generalization across diverse participants also taking into account risk-sensitive metrics.</p></p class="citation"></blockquote><h3 id=12--202248-linking-vision-and-multi-agent-communication-through-visible-light-communication-using-event-cameras-haruyuki-nakagawa-et-al-2024>(1/2 | 202/248) Linking Vision and Multi-Agent Communication through Visible Light Communication using Event Cameras (Haruyuki Nakagawa et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Haruyuki Nakagawa, Yoshitaka Miyatani, Asako Kanezaki. (2024)<br><strong>Linking Vision and Multi-Agent Communication through Visible Light Communication using Event Cameras</strong><br><button class=copy-to-clipboard title="Linking Vision and Multi-Agent Communication through Visible Light Communication using Event Cameras" index=202>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-202 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.MA<br>Categories: cs-MA, cs-RO, cs.MA<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05619v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05619v1.pdf filename=2402.05619v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Various robots, rovers, drones, and other agents of mass-produced products are expected to encounter scenes where they intersect and collaborate in the near future. In such multi-agent systems, individual identification and communication play crucial roles. In this paper, we explore camera-based visible light communication using event cameras to tackle this problem. An event camera captures the events occurring in regions with changes in brightness and can be utilized as a receiver for visible light communication, leveraging its high temporal resolution. Generally, agents with identical appearances in mass-produced products are visually indistinguishable when using conventional CMOS cameras. Therefore, linking visual information with information acquired through conventional radio communication is challenging. We empirically demonstrate the advantages of a visible light communication system employing event cameras and LEDs for visual individual identification over conventional CMOS cameras with ArUco marker recognition. In the <b>simulation,</b> we also verified scenarios where our event camera-based visible light communication outperforms conventional radio communication in situations with visually indistinguishable multi-agents. Finally, our newly implemented multi-agent system verifies its functionality through physical robot experiments.</p></p class="citation"></blockquote><h2 id=csit-4>cs.IT (4)</h2><h3 id=04--203248-coded-many-user-multiple-access-via-approximate-message-passing-xiaoqi-liu-et-al-2024>(0/4 | 203/248) Coded Many-User Multiple Access via Approximate Message Passing (Xiaoqi Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xiaoqi Liu, Kuan Hsieh, Ramji Venkataramanan. (2024)<br><strong>Coded Many-User Multiple Access via Approximate Message Passing</strong><br><button class=copy-to-clipboard title="Coded Many-User Multiple Access via Approximate Message Passing" index=203>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-203 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, eess-SP, math-IT<br>Keyword Score: 30<br>Keywords: Message-Passing, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05625v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05625v1.pdf filename=2402.05625v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We consider communication over the Gaussian multiple-access channel in the regime where the number of users grows linearly with the codelength. We investigate coded CDMA schemes where each user&rsquo;s information is encoded via a linear code before being modulated with a signature sequence. We propose an efficient approximate message passing (AMP) decoder that can be tailored to the structure of the linear code, and provide an exact asymptotic characterization of its performance. Based on this result, we consider a decoder that integrates AMP and belief propagation and characterize the tradeoff between spectral efficiency and signal-to-noise ratio, for a given target error rate. <b>Simulation</b> results are provided to demonstrate the benefits of the concatenated scheme at finite lengths.</p></p class="citation"></blockquote><h3 id=14--204248-boosting-dynamic-tdd-in-small-cell-networks-by-the-multiplicative-weight-update-method-jiaqi-zhu-et-al-2024>(1/4 | 204/248) Boosting Dynamic TDD in Small Cell Networks by the Multiplicative Weight Update Method (Jiaqi Zhu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jiaqi Zhu, Nikolaos Pappas, Howard H. Yang. (2024)<br><strong>Boosting Dynamic TDD in Small Cell Networks by the Multiplicative Weight Update Method</strong><br><button class=copy-to-clipboard title="Boosting Dynamic TDD in Small Cell Networks by the Multiplicative Weight Update Method" index=204>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-204 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, math-IT<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05641v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05641v1.pdf filename=2402.05641v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We leverage the Multiplicative Weight Update (MWU) method to develop a decentralized algorithm that significantly improves the performance of dynamic time division duplexing (D-TDD) in small cell networks. The proposed algorithm adaptively adjusts the time portion allocated to uplink (UL) and downlink (DL) transmissions at every node during each scheduled time slot, aligning the packet transmissions toward the most appropriate link directions according to the feedback of signal-to-interference ratio information. Our <b>simulation</b> results reveal that compared to the (conventional) fixed configuration of UL/DL transmission probabilities in D-TDD, incorporating MWU into D-TDD brings about a two-fold improvement of mean packet throughput in the DL and a three-fold improvement of the same performance metric in the UL, resulting in the D-TDD even outperforming Static-TDD in the UL. It also shows that the proposed scheme maintains a consistent performance gain in the presence of an ascending traffic load, validating its effectiveness in boosting the network performance. This work also demonstrates an approach that accounts for algorithmic considerations at the forefront when solving stochastic problems.</p></p class="citation"></blockquote><h3 id=24--205248-can-channels-be-fully-inferred-between-two-antenna-panels-y-qiu-et-al-2024>(2/4 | 205/248) Can Channels be Fully Inferred Between Two Antenna Panels? (Y. Qiu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Y. Qiu, D. W, Y. Zeng. (2024)<br><strong>Can Channels be Fully Inferred Between Two Antenna Panels?</strong><br><button class=copy-to-clipboard title="Can Channels be Fully Inferred Between Two Antenna Panels?" index=205>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-205 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, eess-SP, math-IT<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05387v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05387v1.pdf filename=2402.05387v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This letter considers a two-panel massive multiple-input multiple-output (MIMO) communication system, where the base station (BS) is equipped with two antenna panels that may use different frequency bands for communication. By exploiting the geometric relationships between antenna panels, efficient channel inference methods across antenna panels are proposed to reduce the overhead of real-time channel estimation. Four scenarios are considered, namely far-field free-space, near-field free-space, multi-path sharing far-field scatterers, and multi-path sharing near-field scatterers. For both far-field and near-field free-space scenarios, we show that the channel of one panel can be fully inferred from that of the other panel, as long as the multi-path components (MPCs) composing the channel can be resolved. On the other hand, for the multi-path scenarios sharing far-field or near-field scatterers, only the angles or range of angles of the MPCs can be inferred, respectively. <b>Simulation</b> results based on commercial 3D ray-tracing software are presented to validate our developed channel inference techniques.</p></p class="citation"></blockquote><h3 id=34--206248-localized-and-distributed-beyond-diagonal-reconfigurable-intelligent-surfaces-with-lossy-interconnections-modeling-and-optimization-matteo-nerini-et-al-2024>(3/4 | 206/248) Localized and Distributed Beyond Diagonal Reconfigurable Intelligent Surfaces with Lossy Interconnections: Modeling and Optimization (Matteo Nerini et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Matteo Nerini, Golsa Ghiaasi, Bruno Clerckx. (2024)<br><strong>Localized and Distributed Beyond Diagonal Reconfigurable Intelligent Surfaces with Lossy Interconnections: Modeling and Optimization</strong><br><button class=copy-to-clipboard title="Localized and Distributed Beyond Diagonal Reconfigurable Intelligent Surfaces with Lossy Interconnections: Modeling and Optimization" index=206>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-206 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.IT<br>Categories: cs-IT, cs.IT, eess-SP, math-IT<br>Keyword Score: 10<br>Keywords: Scaling Law<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05881v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05881v1.pdf filename=2402.05881v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Reconfigurable intelligent surface (RIS) is a key technology to control the communication environment in future wireless networks. Recently, beyond diagonal RIS (BD-RIS) emerged as a generalization of RIS achieving larger coverage through additional tunable impedance components interconnecting the RIS elements. However, conventional RIS and BD-RIS can effectively serve only users in their proximity, resulting in limited coverage. To overcome this limitation, in this paper, we investigate distributed RIS, whose elements are distributed over a wide region, in opposition to localized RIS commonly considered in the literature. The <b>scaling</b> <b>laws</b> of distributed BD-RIS reveal that it offers significant gains over distributed conventional RIS and localized BD-RIS, enabled by its interconnections allowing signal propagation within the BD-RIS. To assess the practical performance of distributed BD-RIS, we model and optimize BD-RIS with lossy interconnections through transmission line theory. Our model accounts for phase changes and losses over the BD-RIS interconnections arising when the interconnection lengths are not much smaller than the wavelength. Numerical results show that the performance of localized BD-RIS is only slightly impacted by losses, given the short interconnection lengths. Besides, distributed BD-RIS can achieve orders of magnitude of gains over conventional RIS, even in the presence of low losses.</p></p class="citation"></blockquote><h2 id=eessas-2>eess.AS (2)</h2><h3 id=02--207248-sound-source-separation-using-latent-variational-block-wise-disentanglement-karim-helwani-et-al-2024>(0/2 | 207/248) Sound Source Separation Using Latent Variational Block-Wise Disentanglement (Karim Helwani et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Karim Helwani, Masahito Togami, Paris Smaragdis, Michael M. Goodwin. (2024)<br><strong>Sound Source Separation Using Latent Variational Block-Wise Disentanglement</strong><br><button class=copy-to-clipboard title="Sound Source Separation Using Latent Variational Block-Wise Disentanglement" index=207>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-207 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.AS<br>Categories: cs-LG, cs-SD, eess-AS, eess.AS<br>Keyword Score: 30<br>Keywords: Autoencoder, Out-of-distribution, Variational Autoencoder<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.06683v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.06683v1.pdf filename=2402.06683v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>While neural network approaches have made significant strides in resolving classical signal processing problems, it is often the case that hybrid approaches that draw insight from both signal processing and neural networks produce more complete solutions. In this paper, we present a hybrid classical digital signal processing/deep neural network (DSP/DNN) approach to source separation (SS) highlighting the theoretical link between <b>variational</b> <b>autoencoder</b> and classical approaches to SS. We propose a system that transforms the single channel under-determined SS task to an equivalent multichannel over-determined SS problem in a properly designed latent space. The separation task in the latent space is treated as finding a <b>variational</b> <b>block-wise</b> disentangled representation of the mixture. We show empirically, that the design choices and the <b>variational</b> <b>formulation</b> of the task at hand motivated by the classical signal processing theoretical results lead to robustness to unseen <b>out-of-distribution</b> data and reduction of the overfitting risk. To address the resulting permutation issue we explicitly incorporate a novel differentiable permutation loss function and augment the model with a memory mechanism to keep track of the statistics of the individual sources.</p></p class="citation"></blockquote><h3 id=12--208248-integrating-self-supervised-speech-model-with-pseudo-word-level-targets-from-visually-grounded-speech-model-hung-chieh-fang-et-al-2024>(1/2 | 208/248) Integrating Self-supervised Speech Model with Pseudo Word-level Targets from Visually-grounded Speech Model (Hung-Chieh Fang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hung-Chieh Fang, Nai-Xuan Ye, Yi-Jen Shih, Puyuan Peng, Hsuan-Fu Wang, Layne Berry, Hung-yi Lee, David Harwath. (2024)<br><strong>Integrating Self-supervised Speech Model with Pseudo Word-level Targets from Visually-grounded Speech Model</strong><br><button class=copy-to-clipboard title="Integrating Self-supervised Speech Model with Pseudo Word-level Targets from Visually-grounded Speech Model" index=208>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-208 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.AS<br>Categories: cs-CL, cs-LG, eess-AS, eess.AS<br>Keyword Score: 13<br>Keywords: Benchmarking, Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05819v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05819v1.pdf filename=2402.05819v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Recent advances in <b>self-supervised</b> speech models have shown significant improvement in many downstream tasks. However, these models predominantly centered on frame-level training objectives, which can fall short in spoken language understanding tasks that require semantic comprehension. Existing works often rely on additional speech-text data as intermediate targets, which is costly in the real-world setting. To address this challenge, we propose Pseudo-Word HuBERT (PW-HuBERT), a framework that integrates pseudo word-level targets into the training process, where the targets are derived from a visually-ground speech model, notably eliminating the need for speech-text paired data. Our experimental results on four spoken language understanding (SLU) <b>benchmarks</b> suggest the superiority of our model in capturing semantic information.</p></p class="citation"></blockquote><h2 id=eessiv-6>eess.IV (6)</h2><h3 id=06--209248-memory-efficient-deep-end-to-end-posterior-network-deepen-for-inverse-problems-jyothi-rikhab-chand-et-al-2024>(0/6 | 209/248) Memory-efficient deep end-to-end posterior network (DEEPEN) for inverse problems (Jyothi Rikhab Chand et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jyothi Rikhab Chand, Mathews Jacob. (2024)<br><strong>Memory-efficient deep end-to-end posterior network (DEEPEN) for inverse problems</strong><br><button class=copy-to-clipboard title="Memory-efficient deep end-to-end posterior network (DEEPEN) for inverse problems" index=209>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-209 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 30<br>Keywords: Convolution, Convolutional Neural Network, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05422v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05422v1.pdf filename=2402.05422v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>End-to-End (E2E) unrolled optimization frameworks show promise for Magnetic Resonance (MR) image recovery, but suffer from high memory usage during training. In addition, these deterministic approaches do not offer opportunities for sampling from the posterior distribution. In this paper, we introduce a memory-efficient approach for E2E learning of the posterior distribution. We represent this distribution as the combination of a data-consistency-induced likelihood term and an energy model for the prior, parameterized by a <b>Convolutional</b> <b>Neural</b> <b>Network</b> <b>(CNN).</b> The <b>CNN</b> weights are learned from training data in an E2E fashion using maximum likelihood optimization. The learned model enables the recovery of images from undersampled measurements using the Maximum A Posteriori (MAP) optimization. In addition, the posterior model can be sampled to derive uncertainty maps about the reconstruction. Experiments on parallel MR image reconstruction show that our approach performs comparable to the memory-intensive E2E unrolled algorithm, performs better than its memory-efficient counterpart, and can provide uncertainty maps. Our framework paves the way towards MR image reconstruction in 3D and higher dimensions</p></p class="citation"></blockquote><h3 id=16--210248-histohdr-net-histogram-equalization-for-single-ldr-to-hdr-image-translation-hrishav-bakul-barua-et-al-2024>(1/6 | 210/248) HistoHDR-Net: Histogram Equalization for Single LDR to HDR Image Translation (Hrishav Bakul Barua et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hrishav Bakul Barua, Ganesh Krishnasamy, KokSheik Wong, Abhinav Dhall, Kalin Stefanov. (2024)<br><strong>HistoHDR-Net: Histogram Equalization for Single LDR to HDR Image Translation</strong><br><button class=copy-to-clipboard title="HistoHDR-Net: Histogram Equalization for Single LDR to HDR Image Translation" index=210>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-210 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: Artificial intelligence, Computer vision, Machine learning, Deep
learning, I-3-3; I-4-5, cs-AI, cs-CV, cs-GR, cs-LG, cs-MM, eess-IV, eess.IV<br>Keyword Score: 20<br>Keywords: Image2text, Self-Attention<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.06692v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.06692v1.pdf filename=2402.06692v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>High Dynamic Range (HDR) imaging aims to replicate the high visual quality and clarity of real-world scenes. Due to the high costs associated with HDR imaging, the literature offers various data-driven methods for HDR <b>image</b> <b>reconstruction</b> from Low Dynamic Range (LDR) counterparts. A common limitation of these approaches is missing details in regions of the reconstructed HDR <b>images,</b> <b>which</b> are over- or under-exposed in the input LDR <b>images.</b> <b>To</b> this end, we propose a simple and effective method, HistoHDR-Net, to recover the fine details (e.g., color, contrast, saturation, and brightness) of HDR <b>images</b> <b>via</b> a fusion-based approach utilizing histogram-equalized LDR <b>images</b> <b>along</b> with <b>self-attention</b> guidance. Our experiments demonstrate the efficacy of the proposed approach over the state-of-art methods.</p></p class="citation"></blockquote><h3 id=26--211248-capability-enhancement-of-the-x-ray-micro-tomography-system-via-ml-assisted-approaches-dhruvi-shah-et-al-2024>(2/6 | 211/248) Capability enhancement of the X-ray micro-tomography system via ML-assisted approaches (Dhruvi Shah et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Dhruvi Shah, Shruti Mehta, Ashish Agrawal, Shishir Purohit, Bhaskar Chaudhury. (2024)<br><strong>Capability enhancement of the X-ray micro-tomography system via ML-assisted approaches</strong><br><button class=copy-to-clipboard title="Capability enhancement of the X-ray micro-tomography system via ML-assisted approaches" index=211>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-211 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-LG, eess-IV, eess.IV, physics-app-ph, physics-ins-det<br>Keyword Score: 20<br>Keywords: Convolution, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05983v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05983v1.pdf filename=2402.05983v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Ring artifacts in X-ray micro-CT images are one of the primary causes of concern in their accurate visual interpretation and quantitative analysis. The geometry of X-ray micro-CT scanners is similar to the medical CT machines, except the sample is rotated with a stationary source and detector. The ring artifacts are caused by a defect or non-linear responses in detector pixels during the MicroCT data acquisition. Artifacts in MicroCT images can often be so severe that the images are no longer useful for further analysis. Therefore, it is essential to comprehend the causes of artifacts and potential solutions to maximize image quality. This article presents a <b>convolution</b> neural network <b>(CNN)-based</b> Deep Learning (DL) model inspired by UNet with a series of encoder and decoder units with skip connections for removal of ring artifacts. The proposed architecture has been evaluated using the Structural Similarity Index Measure (SSIM) and Mean Squared Error (MSE). Additionally, the results are compared with conventional filter-based non-ML techniques and are found to be better than the latter.</p></p class="citation"></blockquote><h3 id=36--212248-unleashing-the-infinity-power-of-geometry-a-novel-geometry-aware-transformer-goat-for-whole-slide-histopathology-image-analysis-mingxin-liu-et-al-2024>(3/6 | 212/248) Unleashing the Infinity Power of Geometry: A Novel Geometry-Aware Transformer (GOAT) for Whole Slide Histopathology Image Analysis (Mingxin Liu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mingxin Liu, Yunzan Liu, Pengbo Xu, Jiquan Ma. (2024)<br><strong>Unleashing the Infinity Power of Geometry: A Novel Geometry-Aware Transformer (GOAT) for Whole Slide Histopathology Image Analysis</strong><br><button class=copy-to-clipboard title="Unleashing the Infinity Power of Geometry: A Novel Geometry-Aware Transformer (GOAT) for Whole Slide Histopathology Image Analysis" index=212>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-212 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, eess-IV, eess.IV<br>Keyword Score: 20<br>Keywords: Weakly-supervised Learning, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05373v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05373v1.pdf filename=2402.05373v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The histopathology analysis is of great significance for the diagnosis and prognosis of cancers, however, it has great challenges due to the enormous heterogeneity of gigapixel whole slide images (WSIs) and the intricate representation of pathological features. However, recent methods have not adequately exploited geometrical representation in WSIs which is significant in disease diagnosis. Therefore, we proposed a novel <b>weakly-supervised</b> framework, Geometry-Aware <b>Transformer</b> (GOAT), in which we urge the model to pay attention to the geometric characteristics within the tumor microenvironment which often serve as potent indicators. In addition, a context-aware attention mechanism is designed to extract and enhance the morphological features within WSIs.</p></p class="citation"></blockquote><h3 id=46--213248-using-yolo-v7-to-detect-kidney-in-magnetic-resonance-imaging-pouria-yazdian-anari-et-al-2024>(4/6 | 213/248) Using YOLO v7 to Detect Kidney in Magnetic Resonance Imaging (Pouria Yazdian Anari et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Pouria Yazdian Anari, Fiona Obiezu, Nathan Lay, Fatemeh Dehghani Firouzabadi, Aditi Chaurasia, Mahshid Golagha, Shiva Singh, Fatemeh Homayounieh, Aryan Zahergivar, Stephanie Harmon, Evrim Turkbey, Rabindra Gautam, Kevin Ma, Maria Merino, Elizabeth C. Jones, Mark W. Ball, W. Marston Linehan, Baris Turkbey, Ashkan A. Malayeri. (2024)<br><strong>Using YOLO v7 to Detect Kidney in Magnetic Resonance Imaging</strong><br><button class=copy-to-clipboard title="Using YOLO v7 to Detect Kidney in Magnetic Resonance Imaging" index=213>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-213 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, cs-LG, eess-IV, eess.IV<br>Keyword Score: 13<br>Keywords: Object Detection, Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05817v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05817v2.pdf filename=2402.05817v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Introduction This study explores the use of the latest You Only Look Once (YOLO V7) <b>object</b> <b>detection</b> method to enhance kidney detection in medical imaging by training and testing a modified YOLO V7 on medical image formats. Methods Study includes 878 patients with various subtypes of renal cell carcinoma (RCC) and 206 patients with normal kidneys. A total of 5657 MRI scans for 1084 patients were retrieved. 326 patients with 1034 tumors recruited from a retrospective maintained database, and bounding boxes were drawn around their tumors. A primary model was trained on 80% of annotated cases, with 20% saved for testing (primary test set). The best primary model was then used to identify tumors in the remaining 861 patients and bounding box coordinates were generated on their scans using the model. Ten <b>benchmark</b> training sets were created with generated coordinates on not-segmented patients. The final model used to predict the kidney in the primary test set. We reported the positive predictive value (PPV), sensitivity, and mean average precision (mAP). Results The primary training set showed an average PPV of 0.94 +/- 0.01, sensitivity of 0.87 +/- 0.04, and mAP of 0.91 +/- 0.02. The best primary model yielded a PPV of 0.97, sensitivity of 0.92, and mAP of 0.95. The final model demonstrated an average PPV of 0.95 +/- 0.03, sensitivity of 0.98 +/- 0.004, and mAP of 0.95 +/- 0.01. Conclusion Using a semi-supervised approach with a medical image library, we developed a high-performing model for kidney detection. Further external validation is required to assess the model&rsquo;s generalizability.</p></p class="citation"></blockquote><h3 id=56--214248-joint-end-to-end-image-compression-and-denoising-leveraging-contrastive-learning-and-multi-scale-self-onns-yuxin-xie-et-al-2024>(5/6 | 214/248) Joint End-to-End Image Compression and Denoising: Leveraging Contrastive Learning and Multi-Scale Self-ONNs (Yuxin Xie et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yuxin Xie, Li Yu, Farhad Pakdaman, Moncef Gabbouj. (2024)<br><strong>Joint End-to-End Image Compression and Denoising: Leveraging Contrastive Learning and Multi-Scale Self-ONNs</strong><br><button class=copy-to-clipboard title="Joint End-to-End Image Compression and Denoising: Leveraging Contrastive Learning and Multi-Scale Self-ONNs" index=214>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-214 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.IV<br>Categories: cs-CV, cs-MM, eess-IV, eess.IV<br>Keyword Score: 10<br>Keywords: Contrastive Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05582v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05582v1.pdf filename=2402.05582v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Noisy images are a challenge to image compression algorithms due to the inherent difficulty of compressing noise. As noise cannot easily be discerned from image details, such as high-frequency signals, its presence leads to extra bits needed for compression. Since the emerging learned image compression paradigm enables end-to-end optimization of codecs, recent efforts were made to integrate denoising into the compression model, relying on clean image features to guide denoising. However, these methods exhibit suboptimal performance under high noise levels, lacking the capability to generalize across diverse noise types. In this paper, we propose a novel method integrating a multi-scale denoiser comprising of Self Organizing Operational Neural Networks, for joint image compression and denoising. We employ <b>contrastive</b> <b>learning</b> to boost the network ability to differentiate noise from high frequency signal components, by emphasizing the correlation between noisy and clean counterparts. Experimental results demonstrate the effectiveness of the proposed method both in rate-distortion performance, and codec speed, outperforming the current state-of-the-art.</p></p class="citation"></blockquote><h2 id=cssi-1>cs.SI (1)</h2><h3 id=01--215248-get-tok-a-genai-enriched-multimodal-tiktok-dataset-documenting-the-2022-attempted-coup-in-peru-gabriela-pinto-et-al-2024>(0/1 | 215/248) GET-Tok: A GenAI-Enriched Multimodal TikTok Dataset Documenting the 2022 Attempted Coup in Peru (Gabriela Pinto et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Gabriela Pinto, Keith Burghardt, Kristina Lerman, Emilio Ferrara. (2024)<br><strong>GET-Tok: A GenAI-Enriched Multimodal TikTok Dataset Documenting the 2022 Attempted Coup in Peru</strong><br><button class=copy-to-clipboard title="GET-Tok: A GenAI-Enriched Multimodal TikTok Dataset Documenting the 2022 Attempted Coup in Peru" index=215>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-215 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SI<br>Categories: cs-CY, cs-HC, cs-SI, cs.SI<br>Keyword Score: 26<br>Keywords: Optical Character Recognition, Generative AI, Multi-modal, Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05882v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05882v1.pdf filename=2402.05882v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>TikTok is one of the largest and fastest-growing social media sites in the world. TikTok features, however, such as voice transcripts, are often missing and other important features, such as <b>OCR</b> or video descriptions, do not exist. We introduce the <b>Generative</b> <b>AI</b> Enriched TikTok (GET-Tok) data, a pipeline for collecting TikTok videos and enriched data by augmenting the TikTok Research API with <b>generative</b> <b>AI</b> models. As a case study, we collect videos about the attempted coup in Peru initiated by its former President, Pedro Castillo, and its accompanying protests. The data includes information on 43,697 videos published from November 20, 2022 to March 1, 2023 (102 days). <b>Generative</b> <b>AI</b> augments the collected data via transcripts of TikTok videos, text descriptions of what is shown in the videos, what text is displayed within the video, and the stances expressed in the video. Overall, this pipeline will contribute to a better understanding of online discussion in a <b>multimodal</b> setting with applications of <b>Generative</b> <b>AI,</b> especially outlining the utility of this pipeline in non-English-language social media. Our code used to produce the pipeline is in a public Github repository: <a href=https://github.com/gabbypinto/GET-Tok-Peru>https://github.com/gabbypinto/GET-Tok-Peru</a>.</p></p class="citation"></blockquote><h2 id=cspf-1>cs.PF (1)</h2><h3 id=01--216248-reconsidering-the-performance-of-devs-modeling-and-simulation-environments-using-the-devstone-benchmark-josé-l-risco-martín-et-al-2024>(0/1 | 216/248) Reconsidering the performance of DEVS modeling and simulation environments using the DEVStone benchmark (José L. Risco-Martín et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>José L. Risco-Martín, Saurabh Mittal, Juan Carlos Fabero, Marina Zapater, Román Hermida. (2024)<br><strong>Reconsidering the performance of DEVS modeling and simulation environments using the DEVStone benchmark</strong><br><button class=copy-to-clipboard title="Reconsidering the performance of DEVS modeling and simulation environments using the DEVStone benchmark" index=216>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-216 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.PF<br>Categories: cs-PF, cs.PF<br>Keyword Score: 23<br>Keywords: Benchmarking, Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05483v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05483v1.pdf filename=2402.05483v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The Discrete Event System Specification formalism (DEVS), which supports hierarchical and modular model composition, has been widely used to understand, analyze and develop a variety of systems. DEVS has been implemented in various languages and platforms over the years. The DEVStone <b>benchmark</b> was conceived to generate a set of models with varied structure and behavior, and to automate the evaluation of the performance of DEVS-based simulators. However, DEVStone is still in a preliminar phase and more model analysis is required. In this paper, we revisit DEVStone introducing new equations to compute the number of events triggered. We also introduce a new <b>benchmark,</b> called HOmem, designed as an alternative version of HOmod, with similar CPU and memory requirements, but with an easier implementation and analytically more manageable. Finally, we compare both the performance and memory footprint of five different DEVS simulators in two different hardware platforms.</p></p class="citation"></blockquote><h2 id=eesssy-7>eess.SY (7)</h2><h3 id=07--217248-multi-network-constrained-operational-optimization-in-community-integrated-energy-systems-a-safe-reinforcement-learning-approach-ze-hu-et-al-2024>(0/7 | 217/248) Multi-Network Constrained Operational Optimization in Community Integrated Energy Systems: A Safe Reinforcement Learning Approach (Ze Hu et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ze Hu, Ka Wing Chan, Ziqing Zhu, Xiang Wei, Siqi Bu. (2024)<br><strong>Multi-Network Constrained Operational Optimization in Community Integrated Energy Systems: A Safe Reinforcement Learning Approach</strong><br><button class=copy-to-clipboard title="Multi-Network Constrained Operational Optimization in Community Integrated Energy Systems: A Safe Reinforcement Learning Approach" index=217>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-217 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 23<br>Keywords: Benchmarking, Markov Decision Process, Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05412v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05412v1.pdf filename=2402.05412v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The integrated community energy system (ICES) has emerged as a promising solution for enhancing the efficiency of the distribution system by effectively coordinating multiple energy sources. However, the operational optimization of ICES is hindered by the physical constraints of heterogeneous networks including electricity, natural gas, and heat. These challenges are difficult to address due to the non-linearity of network constraints and the high complexity of multi-network coordination. This paper, therefore, proposes a novel Safe <b>Reinforcement</b> <b>Learning</b> (SRL) algorithm to optimize the multi-network constrained operation problem of ICES. Firstly, a comprehensive ICES model is established considering integrated demand response (IDR), multiple energy devices, and network constraints. The multi-network operational optimization problem of ICES is then presented and reformulated as a constrained <b>Markov</b> <b>Decision</b> <b>Process</b> (C-MDP) accounting for violating physical network constraints. The proposed novel SRL algorithm, named Primal-Dual Twin Delayed Deep Deterministic Policy Gradient (PD-TD3), solves the C-MDP by employing a Lagrangian multiplier to penalize the multi-network constraint violation, ensuring that violations are within a tolerated range and avoid over-conservative strategy with a low reward at the same time. The proposed algorithm accurately estimates the cumulative reward and cost of the training process, thus achieving a fair balance between improving profits and reducing constraint violations in a privacy-protected environment with only partial information. A case study comparing the proposed algorithm with <b>benchmark</b> RL algorithms demonstrates the computational performance in increasing total profits and alleviating the network constraint violations.</p></p class="citation"></blockquote><h3 id=17--218248-consensus-driven-deviated-pursuit-for-guaranteed-simultaneous-interception-of-moving-targets-abhinav-sinha-et-al-2024>(1/7 | 218/248) Consensus-driven Deviated Pursuit for Guaranteed Simultaneous Interception of Moving Targets (Abhinav Sinha et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Abhinav Sinha, Dwaipayan Mukherjee, Shashi Ranjan Kumar. (2024)<br><strong>Consensus-driven Deviated Pursuit for Guaranteed Simultaneous Interception of Moving Targets</strong><br><button class=copy-to-clipboard title="Consensus-driven Deviated Pursuit for Guaranteed Simultaneous Interception of Moving Targets" index=218>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-218 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-MA, cs-SY, eess-SY, eess.SY, math-DS, math-OC, nlin-AO<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05918v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05918v1.pdf filename=2402.05918v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This work proposes a cooperative strategy that employs deviated pursuit guidance to simultaneously intercept a moving (but not manoeuvring) target. As opposed to many existing cooperative guidance strategies which use estimates of time-to-go, based on proportional-navigation guidance, the proposed strategy uses an exact expression for time-to-go to ensure simultaneous interception. The guidance design considers nonlinear engagement kinematics, allowing the proposed strategy to remain effective over a large operating regime. Unlike existing strategies on simultaneous interception that achieve interception at the average value of their initial time-to-go estimates, this work provides flexibility in the choice of impact time. By judiciously choosing the edge weights of the communication network, a weighted consensus in time-to-go can be achieved. It has been shown that by allowing an edge weight to be negative, consensus in time-to-go can even be achieved for an impact time that lies outside the convex hull of the set of initial time-to-go values of the individual interceptors. The bounds on such negative weights have been analysed for some special graphs, using Nyquist criterion. <b>Simulations</b> are provided to vindicate the efficacy of the proposed strategy.</p></p class="citation"></blockquote><h3 id=27--219248-underwater-mems-gyrocompassing-a-virtual-testing-ground-daniel-engelsman-et-al-2024>(2/7 | 219/248) Underwater MEMS Gyrocompassing: A Virtual Testing Ground (Daniel Engelsman et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Daniel Engelsman, Itzik Klein. (2024)<br><strong>Underwater MEMS Gyrocompassing: A Virtual Testing Ground</strong><br><button class=copy-to-clipboard title="Underwater MEMS Gyrocompassing: A Virtual Testing Ground" index=219>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-219 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05790v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05790v2.pdf filename=2402.05790v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In underwater navigation, accurate heading information is crucial for accurately and continuously tracking trajectories, especially during extended missions beneath the waves. In order to determine the initial heading, a gyrocompassing procedure must be employed. As unmanned underwater vehicles (UUV) are susceptible to ocean currents and other disturbances, the model-based gyrocompassing procedure may experience degraded performance. To cope with such situations, this paper introduces a dedicated learning framework aimed at mitigating environmental effects and offering precise underwater gyrocompassing. Through the analysis of the dynamic UUV signature obtained from inertial measurements, our proposed framework learns to refine disturbed signals, enabling a focused examination of the earth&rsquo;s rotation rate vector. Leveraging recent machine learning advancements, empirical <b>simulations</b> assess the framework&rsquo;s adaptability to challenging underwater conditions. Ultimately, its contribution lies in providing a resilient gyrocompassing solution for UUVs.</p></p class="citation"></blockquote><h3 id=37--220248-stochastic-colregs-evaluation-for-safe-navigation-under-uncertainty-peter-nicholas-hansen-et-al-2024>(3/7 | 220/248) Stochastic COLREGs Evaluation for Safe Navigation under Uncertainty (Peter Nicholas Hansen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Peter Nicholas Hansen, Dimitrios Papageorgiou, Roberto Galeazzi, Mogens Blanke. (2024)<br><strong>Stochastic COLREGs Evaluation for Safe Navigation under Uncertainty</strong><br><button class=copy-to-clipboard title="Stochastic COLREGs Evaluation for Safe Navigation under Uncertainty" index=220>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-220 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-RO, cs-SY, eess-SY, eess.SY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05662v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05662v1.pdf filename=2402.05662v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The encounter situation between marine vessels determines how they should navigate to obey COLREGs, but time-varying and stochastic uncertainty in estimation of angles of encounter, and of closest point of approach, easily give rise to different assessment of situation at two approaching vessels. This may lead to high-risk conditions and could cause collision. This article considers decision making under uncertainty and suggests a novel method for probabilistic interpretation of vessel encounters that is explainable and provides a measure of uncertainty in the evaluation. The method is equally useful for decision support on a manned bridge as on Marine Autonomous Surface Ships (MASS) where it provides input for automated navigation. The method makes formal safety assessment and validation feasible. We obtain a resilient algorithm for machine interpretation of COLREGs under uncertainty and show its efficacy by <b>simulations.</b></p></p class="citation"></blockquote><h3 id=47--221248-triangular-phase-shift-detector-for-drone-precise-vertical-landing-rf-systems-víctor-araña-pulido-et-al-2024>(4/7 | 221/248) Triangular phase-shift detector for drone precise vertical landing RF systems (Víctor Araña-Pulido et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Víctor Araña-Pulido, Eugenio Jiménez-Yguácel, Francisco Cabrera-Almeida, Pedro Quintana-Morales. (2024)<br><strong>Triangular phase-shift detector for drone precise vertical landing RF systems</strong><br><button class=copy-to-clipboard title="Triangular phase-shift detector for drone precise vertical landing RF systems" index=221>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-221 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 10<br>Keywords: Phase-shift Detector<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05914v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05914v1.pdf filename=2402.05914v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents a circuit for precise vertical landing of drones based on a three <b>phase-shifts</b> <b>detection</b> of a single frequency transmitted from the landing point. The circuit can be considered as a new navigation sensor that assists in guidance corrections for landing at a specific point. The circuit has three inputs to which the signal transmitted from an oscillator located at the landing point arrives with different delays. The input signals are combined in pairs in each of the three analog phase detectors, after having passed through 3 dB@90 o hybrid couplers that guarantee a theoretical non-ambiguous <b>phase-shift</b> <b>range</b> of +-90 degree. Each output has a voltage that is proportional to the <b>phase-shift</b> <b>between</b> each of the input signals, which in turn depend on the position relative to the landing point. A simple landing algorithm based on <b>phase-shift</b> <b>values</b> is proposed, which could be integrated into the same flight control platform, thus avoiding the need to add additional processing components. To demonstrate the feasibility of the proposed design, a triangular <b>phase-shift</b> <b>detector</b> prototype has been implemented using commercial devices. Calibration and measurements at 2.46 GHz show a dynamic range of 30 dB and a non-ambiguous detection range of +-80 degree in the worst cases. Those specs let us to track the drone during the landing maneuver in an inverted cone formed by a surface with a +-4.19 m radius at 10m high and the landing point.</p></p class="citation"></blockquote><h3 id=57--222248-design-and-prototyping-of-transmissive-ris-aided-wireless-communication-jianan-zhang-et-al-2024>(5/7 | 222/248) Design and Prototyping of Transmissive RIS-Aided Wireless Communication (Jianan Zhang et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jianan Zhang, Rujing Xiong, Junshuo Liu, Tiebin Mi, Robert Caiming Qiu. (2024)<br><strong>Design and Prototyping of Transmissive RIS-Aided Wireless Communication</strong><br><button class=copy-to-clipboard title="Design and Prototyping of Transmissive RIS-Aided Wireless Communication" index=222>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-222 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 10<br>Keywords: Quantization<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05570v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05570v1.pdf filename=2402.05570v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Reconfigurable Intelligent Surfaces (RISs) exhibit promising enhancements in coverage and data rates for wireless communication systems, particularly in the context of 5G and beyond. This paper introduces a novel approach by focusing on the design and prototyping of a transmissive RIS, contrasting with existing research predominantly centered on reflective RIS. The achievement of 1-bit transmissive RIS through the antisymmetry configuration of the two PIN diodes, nearly uniform transmission magnitudes but inversed phase states in a wide band can be obtained. A transmissive RIS prototype consisting of 16 $\times$ 16 elements is meticulously designed, fabricated, and subjected to measurement to validate the proposed design. The results demonstrate that the proposed RIS unit cell achieves effective 1-bit phase tuning with minimal insertion loss and a transmission bandwidth of 3 dB exceeding $20%$ at 5.8GHz. By dynamically modulating the <b>quantized</b> code distributions on the RIS, it becomes possible to construct scanning beams. The experimental outcomes of the RIS-assisted communication system validate that, in comparison to scenarios without RIS, the signal receiving power experiences an increase of approximately 7dB when RIS is deployed to overcome obstacles. This underscores the potential applicability of mobile RIS in practical communication.</p></p class="citation"></blockquote><h3 id=67--223248-internal-model-control-design-for-systems-learned-by-control-affine-neural-nonlinear-autoregressive-exogenous-models-jing-xie-et-al-2024>(6/7 | 223/248) Internal Model Control design for systems learned by Control Affine Neural Nonlinear Autoregressive Exogenous Models (Jing Xie et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jing Xie, Fabio Bonassi, Riccardo Scattolini. (2024)<br><strong>Internal Model Control design for systems learned by Control Affine Neural Nonlinear Autoregressive Exogenous Models</strong><br><button class=copy-to-clipboard title="Internal Model Control design for systems learned by Control Affine Neural Nonlinear Autoregressive Exogenous Models" index=223>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-223 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: eess.SY<br>Categories: cs-SY, eess-SY, eess.SY<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05607v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05607v1.pdf filename=2402.05607v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper explores the use of Control Affine Neural Nonlinear AutoRegressive eXogenous (CA-NNARX) models for nonlinear system identification and model-based control design. The idea behind this architecture is to match the known control-affine structure of the system to achieve improved performance. Coherently with recent literature of neural networks for data-driven control, we first analyze the stability properties of CA-NNARX models, devising sufficient conditions for their incremental Input-to-State Stability ($\delta$ISS) that can be enforced at the model training stage. The model&rsquo;s stability property is then leveraged to design a stable Internal Model Control (IMC) architecture. The proposed control scheme is tested on a simulated Quadruple Tank <b>benchmark</b> system to address the output reference tracking problem. The results achieved show that (i) the modeling accuracy of CA-NNARX is superior to the one of a standard NNARX model for given weight size and training epochs, and (ii) the proposed IMC law provides performance comparable to the ones of a standard Model Predictive Controller (MPC) at a significantly lower computational burden.</p></p class="citation"></blockquote><h2 id=csgt-2>cs.GT (2)</h2><h3 id=02--224248-combining-voting-and-abstract-argumentation-to-understand-online-discussions-michael-bernreiter-et-al-2024>(0/2 | 224/248) Combining Voting and Abstract Argumentation to Understand Online Discussions (Michael Bernreiter et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Michael Bernreiter, Jan Maly, Oliviero Nardi, Stefan Woltran. (2024)<br><strong>Combining Voting and Abstract Argumentation to Understand Online Discussions</strong><br><button class=copy-to-clipboard title="Combining Voting and Abstract Argumentation to Understand Online Discussions" index=224>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-224 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.GT<br>Categories: cs-GT, cs.GT<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05895v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05895v1.pdf filename=2402.05895v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Online discussion platforms are a vital part of the public discourse in a deliberative democracy. However, how to interpret the outcomes of the discussions on these platforms is often unclear. In this paper, we propose a novel and explainable method for selecting a set of most representative, consistent points of view by combining methods from computational social choice and abstract argumentation. Specifically, we model online discussions as abstract argumentation frameworks combined with information regarding which arguments voters approve of. Based on ideas from approval-based multiwinner voting, we introduce several voting rules for selecting a set of preferred extensions that represents voters&rsquo; points of view. We compare the proposed methods across several dimensions, theoretically and in numerical <b>simulations,</b> and give clear suggestions on which methods to use depending on the specific situation.</p></p class="citation"></blockquote><h3 id=12--225248-when-is-mean-field-reinforcement-learning-tractable-and-relevant-batuhan-yardim-et-al-2024>(1/2 | 225/248) When is Mean-Field Reinforcement Learning Tractable and Relevant? (Batuhan Yardim et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Batuhan Yardim, Artur Goldman, Niao He. (2024)<br><strong>When is Mean-Field Reinforcement Learning Tractable and Relevant?</strong><br><button class=copy-to-clipboard title="When is Mean-Field Reinforcement Learning Tractable and Relevant?" index=225>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-225 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.GT<br>Categories: cs-GT, cs-MA, cs.GT, math-OC<br>Keyword Score: 10<br>Keywords: Reinforcement Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05757v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05757v1.pdf filename=2402.05757v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Mean-field <b>reinforcement</b> <b>learning</b> has become a popular theoretical framework for efficiently approximating large-scale multi-agent <b>reinforcement</b> <b>learning</b> (MARL) problems exhibiting symmetry. However, questions remain regarding the applicability of mean-field approximations: in particular, their approximation accuracy of real-world systems and conditions under which they become computationally tractable. We establish explicit finite-agent bounds for how well the MFG solution approximates the true $N$-player game for two popular mean-field solution concepts. Furthermore, for the first time, we establish explicit lower bounds indicating that MFGs are poor or uninformative at approximating $N$-player games assuming only Lipschitz dynamics and rewards. Finally, we analyze the computational complexity of solving MFGs with only Lipschitz properties and prove that they are in the class of \textsc{PPAD}-complete problems conjectured to be intractable, similar to general sum $N$ player games. Our theoretical results underscore the limitations of MFGs and complement and justify existing work by proving difficulty in the absence of common theoretical assumptions.</p></p class="citation"></blockquote><h2 id=statml-5>stat.ML (5)</h2><h3 id=05--226248-prior-dependent-allocations-for-bayesian-fixed-budget-best-arm-identification-in-structured-bandits-nicolas-nguyen-et-al-2024>(0/5 | 226/248) Prior-Dependent Allocations for Bayesian Fixed-Budget Best-Arm Identification in Structured Bandits (Nicolas Nguyen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nicolas Nguyen, Imad Aouali, András György, Claire Vernade. (2024)<br><strong>Prior-Dependent Allocations for Bayesian Fixed-Budget Best-Arm Identification in Structured Bandits</strong><br><button class=copy-to-clipboard title="Prior-Dependent Allocations for Bayesian Fixed-Budget Best-Arm Identification in Structured Bandits" index=226>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-226 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-LG, stat-ML, stat.ML<br>Keyword Score: 20<br>Keywords: Bandit Algorithm, Structured Bandit<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05878v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05878v1.pdf filename=2402.05878v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We study the problem of Bayesian fixed-budget best-arm identification (BAI) in <b>structured</b> <b>bandits.</b> We propose an algorithm that uses fixed allocations based on the prior information and the structure of the environment. We provide theoretical bounds on its performance across diverse models, including the first prior-dependent upper bounds for linear and hierarchical BAI. Our key contribution is introducing new proof methods that result in tighter bounds for multi-armed BAI compared to existing methods. We extensively compare our approach to other fixed-budget BAI methods, demonstrating its consistent and robust performance in various settings. Our work improves our understanding of Bayesian fixed-budget BAI in <b>structured</b> <b>bandits</b> and highlights the effectiveness of our approach in practical scenarios.</p></p class="citation"></blockquote><h3 id=15--227248-how-do-transformers-perform-in-context-autoregressive-learning-michael-e-sander-et-al-2024>(1/5 | 227/248) How do Transformers perform In-Context Autoregressive Learning? (Michael E. Sander et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Michael E. Sander, Raja Giryes, Taiji Suzuki, Mathieu Blondel, Gabriel Peyré. (2024)<br><strong>How do Transformers perform In-Context Autoregressive Learning?</strong><br><button class=copy-to-clipboard title="How do Transformers perform In-Context Autoregressive Learning?" index=227>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-227 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-LG, stat-ML, stat.ML<br>Keyword Score: 20<br>Keywords: Transformer, In-context Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05787v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05787v1.pdf filename=2402.05787v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br><b>Transformers</b> have achieved state-of-the-art performance in language modeling tasks. However, the reasons behind their tremendous success are still unclear. In this paper, towards a better understanding, we train a <b>Transformer</b> model on a simple next token prediction task, where sequences are generated as a first-order autoregressive process $s_{t+1} = W s_t$. We show how a trained <b>Transformer</b> predicts the next token by first learning $W$ <b>in-context,</b> then applying a prediction mapping. We call the resulting procedure <b>in-context</b> autoregressive learning. More precisely, focusing on commuting orthogonal matrices $W$, we first show that a trained one-layer linear <b>Transformer</b> implements one step of gradient descent for the minimization of an inner objective function, when considering augmented tokens. When the tokens are not augmented, we characterize the global minima of a one-layer diagonal linear multi-head <b>Transformer.</b> Importantly, we exhibit orthogonality between heads and show that positional encoding captures trigonometric relations in the data. On the experimental side, we consider the general case of non-commuting orthogonal matrices and generalize our theoretical findings.</p></p class="citation"></blockquote><h3 id=25--228248-remedi-corrective-transformations-for-improved-neural-entropy-estimation-viktor-nilsson-et-al-2024>(2/5 | 228/248) REMEDI: Corrective Transformations for Improved Neural Entropy Estimation (Viktor Nilsson et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Viktor Nilsson, Anirban Samaddar, Sandeep Madireddy, Pierre Nyquist. (2024)<br><strong>REMEDI: Corrective Transformations for Improved Neural Entropy Estimation</strong><br><button class=copy-to-clipboard title="REMEDI: Corrective Transformations for Improved Neural Entropy Estimation" index=228>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-228 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: 94A17 (Primary) 68T01, 94A08 (Secondary), cs-LG, stat-ML, stat.ML<br>Keyword Score: 20<br>Keywords: Supervised Learning, Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05718v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05718v1.pdf filename=2402.05718v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Information theoretic quantities play a central role in machine learning. The recent surge in the complexity of data and models has increased the demand for accurate estimation of these quantities. However, as the dimension grows the estimation presents significant challenges, with existing methods struggling already in relatively low dimensions. To address this issue, in this work, we introduce $\texttt{REMEDI}$ for efficient and accurate estimation of differential entropy, a fundamental information theoretic quantity. The approach combines the minimization of the cross-entropy for simple, adaptive base models and the estimation of their deviation, in terms of the relative entropy, from the data density. Our approach demonstrates improvement across a broad spectrum of estimation tasks, encompassing entropy estimation on both synthetic and natural data. Further, we extend important theoretical consistency results to a more generalized setting required by our approach. We illustrate how the framework can be naturally extended to information theoretic <b>supervised</b> <b>learning</b> models, with a specific focus on the Information Bottleneck approach. It is demonstrated that the method delivers better accuracy compared to the existing methods in Information Bottleneck. In addition, we explore a natural connection between $\texttt{REMEDI}$ and generative modeling using rejection sampling and Langevin dynamics.</p></p class="citation"></blockquote><h3 id=35--229248-a-high-dimensional-model-for-adversarial-training-geometry-and-trade-offs-kasimir-tanner-et-al-2024>(3/5 | 229/248) A High Dimensional Model for Adversarial Training: Geometry and Trade-Offs (Kasimir Tanner et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Kasimir Tanner, Matteo Vilucchio, Bruno Loureiro, Florent Krzakala. (2024)<br><strong>A High Dimensional Model for Adversarial Training: Geometry and Trade-Offs</strong><br><button class=copy-to-clipboard title="A High Dimensional Model for Adversarial Training: Geometry and Trade-Offs" index=229>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-229 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cond-mat-dis-nn, cs-LG, stat-ML, stat.ML<br>Keyword Score: 10<br>Keywords: Adversarial Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05674v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05674v1.pdf filename=2402.05674v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This work investigates <b>adversarial</b> <b>training</b> in the context of margin-based linear classifiers in the high-dimensional regime where the dimension $d$ and the number of data points $n$ diverge with a fixed ratio $\alpha = n / d$. We introduce a tractable mathematical model where the interplay between the data and <b>adversarial</b> <b>attacker</b> geometries can be studied, while capturing the core phenomenology observed in the <b>adversarial</b> <b>robustness</b> literature. Our main theoretical contribution is an exact asymptotic description of the sufficient statistics for the <b>adversarial</b> <b>empirical</b> risk minimiser, under generic convex and non-increasing losses. Our result allow us to precisely characterise which directions in the data are associated with a higher generalisation/robustness trade-off, as defined by a robustness and a usefulness metric. In particular, we unveil the existence of directions which can be defended without penalising accuracy. Finally, we show the advantage of defending non-robust features during training, identifying a uniform protection as an inherently effective defence mechanism.</p></p class="citation"></blockquote><h3 id=45--230248-classification-under-nuisance-parameters-and-generalized-label-shift-in-likelihood-free-inference-luca-masserano-et-al-2024>(4/5 | 230/248) Classification under Nuisance Parameters and Generalized Label Shift in Likelihood-Free Inference (Luca Masserano et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Luca Masserano, Alex Shen, Michele Doro, Tommaso Dorigo, Rafael Izbicki, Ann B. Lee. (2024)<br><strong>Classification under Nuisance Parameters and Generalized Label Shift in Likelihood-Free Inference</strong><br><button class=copy-to-clipboard title="Classification under Nuisance Parameters and Generalized Label Shift in Likelihood-Free Inference" index=230>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-230 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: stat.ML<br>Categories: cs-LG, stat-ML, stat.ML<br>Keyword Score: 10<br>Keywords: Domain Adaptation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05330v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05330v1.pdf filename=2402.05330v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>An open scientific challenge is how to classify events with reliable measures of uncertainty, when we have a mechanistic model of the data-generating process but the distribution over both labels and latent nuisance parameters is different between train and target data. We refer to this type of distributional shift as generalized label shift (GLS). Direct classification using observed data $\mathbf{X}$ as covariates leads to biased predictions and invalid uncertainty estimates of labels $Y$. We overcome these biases by proposing a new method for robust uncertainty quantification that casts classification as a hypothesis testing problem under nuisance parameters. The key idea is to estimate the classifier&rsquo;s receiver operating characteristic (ROC) across the entire nuisance parameter space, which allows us to devise cutoffs that are invariant under GLS. Our method effectively endows a pre-trained classifier with <b>domain</b> <b>adaptation</b> capabilities and returns valid prediction sets while maintaining high power. We demonstrate its performance on two challenging scientific problems in biology and astroparticle physics with data from realistic mechanistic models.</p></p class="citation"></blockquote><h2 id=csmm-1>cs.MM (1)</h2><h3 id=01--231248-performance-evaluation-of-associative-watermarking-using-statistical-neurodynamics-ryoto-kanegae-et-al-2024>(0/1 | 231/248) Performance Evaluation of Associative Watermarking Using Statistical Neurodynamics (Ryoto Kanegae et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Ryoto Kanegae, Masaki Kawamura. (2024)<br><strong>Performance Evaluation of Associative Watermarking Using Statistical Neurodynamics</strong><br><button class=copy-to-clipboard title="Performance Evaluation of Associative Watermarking Using Statistical Neurodynamics" index=231>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-231 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.MM<br>Categories: cond-mat-stat-mech, cs-MM, cs.MM<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05508v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05508v1.pdf filename=2402.05508v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We theoretically evaluated the performance of our proposed associative watermarking method in which the watermark is not embedded directly into the image. We previously proposed a watermarking method that extends the zero-watermarking model by applying associative memory models. In this model, the hetero-associative memory model is introduced to the mapping process between image features and watermarks, and the auto-associative memory model is applied to correct watermark errors. We herein show that the associative watermarking model outperforms the zero-watermarking model through computer <b>simulations</b> using actual images. In this paper, we describe how we derive the macroscopic state equation for the associative watermarking model using the Okada theory. The theoretical results obtained by the fourth-order theory were in good agreement with those obtained by computer <b>simulations.</b> Furthermore, the performance of the associative watermarking model was evaluated using the bit error rate of the watermark, both theoretically and using computer <b>simulations.</b></p></p class="citation"></blockquote><h2 id=cssd-1>cs.SD (1)</h2><h3 id=01--232248-multispecies-bird-sound-recognition-using-a-fully-convolutional-neural-network-maría-teresa-garcía-ordás-et-al-2024>(0/1 | 232/248) Multispecies bird sound recognition using a fully convolutional neural network (María Teresa García-Ordás et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>María Teresa García-Ordás, Sergio Rubio-Martín, José Alberto Benítez-Andrades, Hector Alaiz-Moretón, Isaías García-Rodríguez. (2024)<br><strong>Multispecies bird sound recognition using a fully convolutional neural network</strong><br><button class=copy-to-clipboard title="Multispecies bird sound recognition using a fully convolutional neural network" index=232>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-232 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.SD<br>Categories: cs-SD, cs.SD, eess-AS<br>Keyword Score: 20<br>Keywords: Convolution, Convolutional Neural Network<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05489v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05489v1.pdf filename=2402.05489v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This study proposes a method based on fully <b>convolutional</b> <b>neural</b> <b>networks</b> (FCNs) to identify migratory birds from their songs, with the objective of recognizing which birds pass through certain areas and at what time. To determine the best FCN architecture, extensive experimentation was conducted through a grid search, exploring the optimal depth, width, and activation function of the network. The results showed that the optimal number of filters is 400 in the widest layer, with 4 <b>convolutional</b> <b>blocks</b> <b>with</b> maxpooling and an adaptive activation function. The proposed FCN offers a significant advantage over other techniques, as it can recognize the sound of a bird in audio of any length with an accuracy greater than 85%. Furthermore, due to its architecture, the network can detect more than one species from audio and can carry out near-real-time sound recognition. Additionally, the proposed method is lightweight, making it ideal for deployment and use in IoT devices. The study also presents a comparative analysis of the proposed method against other techniques, demonstrating an improvement of over 67% in the best-case scenario. These findings contribute to advancing the field of bird sound recognition and provide valuable insights into the practical application of FCNs in real-world scenarios.</p></p class="citation"></blockquote><h2 id=physicssoc-ph-1>physics.soc-ph (1)</h2><h3 id=01--233248-social-physics-informed-diffusion-model-for-crowd-simulation-hongyi-chen-et-al-2024>(0/1 | 233/248) Social Physics Informed Diffusion Model for Crowd Simulation (Hongyi Chen et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hongyi Chen, Jingtao Ding, Yong Li, Yue Wang, Xiao-Ping Zhang. (2024)<br><strong>Social Physics Informed Diffusion Model for Crowd Simulation</strong><br><button class=copy-to-clipboard title="Social Physics Informed Diffusion Model for Crowd Simulation" index=233>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-233 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: physics.soc-ph<br>Categories: cs-AI, cs-LG, physics-soc-ph, physics.soc-ph<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.06680v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.06680v1.pdf filename=2402.06680v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Crowd <b>simulation</b> holds crucial applications in various domains, such as urban planning, architectural design, and traffic arrangement. In recent years, physics-informed machine learning methods have achieved state-of-the-art performance in crowd <b>simulation</b> but fail to model the heterogeneity and multi-modality of human movement comprehensively. In this paper, we propose a social physics-informed diffusion model named SPDiff to mitigate the above gap. SPDiff takes both the interactive and historical information of crowds in the current timeframe to reverse the diffusion process, thereby generating the distribution of pedestrian movement in the subsequent timeframe. Inspired by the well-known social physics model, i.e., Social Force, regarding crowd dynamics, we design a crowd interaction module to guide the denoising process and further enhance this module with the equivariant properties of crowd interactions. To mitigate error accumulation in long-term <b>simulations,</b> we propose a multi-frame rollout training algorithm for diffusion modeling. Experiments conducted on two real-world datasets demonstrate the superior performance of SPDiff in terms of macroscopic and microscopic evaluation metrics. Code and appendix are available at <a href=https://github.com/tsinghua-fib-lab/SPDiff>https://github.com/tsinghua-fib-lab/SPDiff</a>.</p></p class="citation"></blockquote><h2 id=csni-1>cs.NI (1)</h2><h3 id=01--234248-a-state-of-the-art-survey-on-full-duplex-network-design-yonghwi-kim-et-al-2024>(0/1 | 234/248) A State-of-the-art Survey on Full-duplex Network Design (Yonghwi Kim et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Yonghwi Kim, Hyung-Joo Moon, Hanju Yoo, Byoungnam, Kim, Kai-Kit Wong, Chan-Byoung Chae. (2024)<br><strong>A State-of-the-art Survey on Full-duplex Network Design</strong><br><button class=copy-to-clipboard title="A State-of-the-art Survey on Full-duplex Network Design" index=234>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-234 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.NI<br>Categories: cs-NI, cs-SY, cs.NI, eess-SP, eess-SY<br>Keyword Score: 20<br>Keywords: Simulation, Simulator<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05402v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05402v1.pdf filename=2402.05402v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Full-duplex (FD) technology is gaining popularity for integration into a wide range of wireless networks due to its demonstrated potential in recent studies. In contrast to half-duplex (HD) technology, the implementation of FD in networks necessitates considering inter-node interference (INI) from various network perspectives. When deploying FD technology in networks, several critical factors must be taken into account. These include self-interference (SI) and the requisite SI cancellation (SIC) processes, as well as the selection of multiple user equipment (UE) per time slot. Additionally, inter-node interference (INI), including cross-link interference (CLI) and inter-cell interference (ICI), become crucial issues during concurrent uplink (UL) and downlink (DL) transmission and reception, similar to SI. Since most INI is challenging to eliminate, a comprehensive investigation that covers radio resource control (RRC), medium access control (MAC), and the physical layer (PHY) is essential in the context of FD network design, rather than focusing on individual network layers and types. This paper covers state-of-the-art studies, including protocols and documents from 3GPP for FD, MAC protocol, user scheduling, and CLI handling. The methods are also compared through a network-level system <b>simulation</b> based on 3D ray-tracing.</p></p class="citation"></blockquote><h2 id=physicsao-ph-1>physics.ao-ph (1)</h2><h3 id=01--235248-ai4fapar-how-artificial-intelligence-can-help-to-forecast-the-seasonal-earth-observation-signal-filip-sabo-et-al-2024>(0/1 | 235/248) Ai4Fapar: How artificial intelligence can help to forecast the seasonal earth observation signal (Filip Sabo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Filip Sabo, Martin Claverie, Michele Meroni, Arthur Hrast Essenfelder. (2024)<br><strong>Ai4Fapar: How artificial intelligence can help to forecast the seasonal earth observation signal</strong><br><button class=copy-to-clipboard title="Ai4Fapar: How artificial intelligence can help to forecast the seasonal earth observation signal" index=235>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-235 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: physics.ao-ph<br>Categories: cs-LG, physics-ao-ph, physics.ao-ph<br>Keyword Score: 13<br>Keywords: Benchmarking, Transformer<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.06684v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.06684v1.pdf filename=2402.06684v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper investigated the potential of a multivariate <b>Transformer</b> model to forecast the temporal trajectory of the Fraction of Absorbed Photosynthetically Active Radiation (FAPAR) for short (1 month) and long horizon (more than 1 month) periods at the regional level in Europe and North Africa. The input data covers the period from 2002 to 2022 and includes remote sensing and weather data for modelling FAPAR predictions. The model was evaluated using a leave one year out cross-validation and compared with the climatological <b>benchmark.</b> Results show that the <b>transformer</b> model outperforms the <b>benchmark</b> model for one month forecasting horizon, after which the climatological <b>benchmark</b> is better. The RMSE values of the <b>transformer</b> model ranged from 0.02 to 0.04 FAPAR units for the first 2 months of predictions. Overall, the tested <b>Transformer</b> model is a valid method for FAPAR forecasting, especially when combined with weather data and used for short-term predictions.</p></p class="citation"></blockquote><h2 id=physicsoptics-1>physics.optics (1)</h2><h3 id=01--236248-3d-2d-neural-nets-for-phase-retrieval-in-noisy-interferometric-imaging-andrew-h-proppe-et-al-2024>(0/1 | 236/248) 3D-2D Neural Nets for Phase Retrieval in Noisy Interferometric Imaging (Andrew H. Proppe et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Andrew H. Proppe, Guillaume Thekkadath, Duncan England, Philip J. Bustard, Frédéric Bouchard, Jeff S. Lundeen, Benjamin J. Sussman. (2024)<br><strong>3D-2D Neural Nets for Phase Retrieval in Noisy Interferometric Imaging</strong><br><button class=copy-to-clipboard title="3D-2D Neural Nets for Phase Retrieval in Noisy Interferometric Imaging" index=236>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-236 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: physics.optics<br>Categories: cs-CV, cs-LG, eess-IV, physics-optics, physics.optics<br>Keyword Score: 10<br>Keywords: Convolution<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.06063v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.06063v1.pdf filename=2402.06063v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In recent years, neural networks have been used to solve phase retrieval problems in imaging with superior accuracy and speed than traditional techniques, especially in the presence of noise. However, in the context of interferometric imaging, phase noise has been largely unaddressed by existing neural network architectures. Such noise arises naturally in an interferometer due to mechanical instabilities or atmospheric turbulence, limiting measurement acquisition times and posing a challenge in scenarios with limited light intensity, such as remote sensing. Here, we introduce a 3D-2D Phase Retrieval U-Net (PRUNe) that takes noisy and randomly phase-shifted interferograms as inputs, and outputs a single 2D phase image. A 3D downsampling <b>convolutional</b> encoder captures correlations within and between frames to produce a 2D latent space, which is upsampled by a 2D decoder into a phase image. We test our model against a state-of-the-art singular value decomposition algorithm and find PRUNe reconstructions consistently show more accurate and smooth reconstructions, with a x2.5 - 4 lower mean squared error at multiple signal-to-noise ratios for interferograms with low (&lt; 1 photon/pixel) and high (~100 photons/pixel) signal intensity. Our model presents a faster and more accurate approach to perform phase retrieval in extremely low light intensity interferometry in presence of phase noise, and will find application in other multi-frame noisy imaging techniques.</p></p class="citation"></blockquote><h2 id=q-biobm-1>q-bio.BM (1)</h2><h3 id=01--237248-dirichlet-flow-matching-with-applications-to-dna-sequence-design-hannes-stark-et-al-2024>(0/1 | 237/248) Dirichlet Flow Matching with Applications to DNA Sequence Design (Hannes Stark et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Hannes Stark, Bowen Jing, Chenyu Wang, Gabriele Corso, Bonnie Berger, Regina Barzilay, Tommi Jaakkola. (2024)<br><strong>Dirichlet Flow Matching with Applications to DNA Sequence Design</strong><br><button class=copy-to-clipboard title="Dirichlet Flow Matching with Applications to DNA Sequence Design" index=237>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-237 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: q-bio.BM<br>Categories: cs-LG, q-bio-BM, q-bio.BM<br>Keyword Score: 10<br>Keywords: Knowledge Distillation<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05841v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05841v1.pdf filename=2402.05841v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Discrete diffusion or flow models could enable faster and more controllable sequence generation than autoregressive models. We show that na"ive linear flow matching on the simplex is insufficient toward this goal since it suffers from discontinuities in the training target and further pathologies. To overcome this, we develop Dirichlet flow matching on the simplex based on mixtures of Dirichlet distributions as probability paths. In this framework, we derive a connection between the mixtures&rsquo; scores and the flow&rsquo;s vector field that allows for classifier and classifier-free guidance. Further, we provide <b>distilled</b> Dirichlet flow matching, which enables one-step sequence generation with minimal performance hits, resulting in $O(L)$ speedups compared to autoregressive models. On complex DNA sequence generation tasks, we demonstrate superior performance compared to all baselines in distributional metrics and in achieving desired design targets for generated sequences. Finally, we show that our classifier-free guidance approach improves unconditional generation and is effective for generating DNA that satisfies design targets. Code is available at <a href=https://github.com/HannesStark/dirichlet-flow-matching>https://github.com/HannesStark/dirichlet-flow-matching</a>.</p></p class="citation"></blockquote><h2 id=mathna-6>math.NA (6)</h2><h3 id=06--238248-strassens-algorithm-is-not-optimally-accurate-jean-guillaume-dumas-et-al-2024>(0/6 | 238/248) Strassen&rsquo;s algorithm is not optimally accurate (Jean-Guillaume Dumas et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Jean-Guillaume Dumas, Clément Pernet, Alexandre Sedoglavic. (2024)<br><strong>Strassen&rsquo;s algorithm is not optimally accurate</strong><br><button class=copy-to-clipboard title="Strassen's algorithm is not optimally accurate" index=238>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-238 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: cs-NA, cs-SC, math-NA, math.NA<br>Keyword Score: 10<br>Keywords: Tensor Decomposition<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05630v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05630v1.pdf filename=2402.05630v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We propose a non-commutative algorithm for multiplying 2x2 matrices using 7 coefficient products. This algorithm reaches simultaneously a better accuracy in practice compared to previously known such fast algorithms, and a time complexity bound with the best currently known leading term (obtained via alternate basis sparsification). To build this algorithm, we consider matrix and <b>tensor</b> <b>norms</b> bounds governing the stability and accuracy of numerical matrix multiplication. First, we reduce those bounds by minimizing a growth factor along the unique orbit of Strassen&rsquo;s 2x2-matrix multiplication <b>tensor</b> <b>decomposition.</b> Second, we develop heuristics for minimizing the number of operations required to realize a given bilinear formula, while further improving its accuracy. Third, we perform an alternate basis sparsification that improves on the time complexity constant and mostly preserves the overall accuracy.</p></p class="citation"></blockquote><h3 id=16--239248-neural-functional-a-posteriori-error-estimates-vladimir-fanaskov-et-al-2024>(1/6 | 239/248) Neural functional a posteriori error estimates (Vladimir Fanaskov et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Vladimir Fanaskov, Alexander Rudikov, Ivan Oseledets. (2024)<br><strong>Neural functional a posteriori error estimates</strong><br><button class=copy-to-clipboard title="Neural functional a posteriori error estimates" index=239>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-239 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: cs-NA, math-NA, math.NA<br>Keyword Score: 10<br>Keywords: Supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05585v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05585v1.pdf filename=2402.05585v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We propose a new loss function for <b>supervised</b> and physics-informed training of neural networks and operators that incorporates a posteriori error estimate. More specifically, during the training stage, the neural network learns additional physical fields that lead to rigorous error majorants after a computationally cheap postprocessing stage. Theoretical results are based upon the theory of functional a posteriori error estimates, which allows for the systematic construction of such loss functions for a diverse class of practically relevant partial differential equations. From the numerical side, we demonstrate on a series of elliptic problems that for a variety of architectures and approaches (physics-informed neural networks, physics-informed neural operators, neural operators, and classical architectures in the regression and physics-informed settings), we can reach better or comparable accuracy and in addition to that cheaply recover high-quality upper bounds on the error after training.</p></p class="citation"></blockquote><h3 id=26--240248-neural-multigrid-architectures-vladimir-fanaskov-2024>(2/6 | 240/248) Neural Multigrid Architectures (Vladimir Fanaskov, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Vladimir Fanaskov. (2024)<br><strong>Neural Multigrid Architectures</strong><br><button class=copy-to-clipboard title="Neural Multigrid Architectures" index=240>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-240 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: cs-AI, cs-NA, math-NA, math.NA<br>Keyword Score: 10<br>Keywords: Parameter Sharing<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05563v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05563v1.pdf filename=2402.05563v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>We propose a convenient matrix-free neural architecture for the multigrid method. The architecture is simple enough to be implemented in less than fifty lines of code, yet it encompasses a large number of distinct multigrid solvers. We argue that a fixed neural network without dense layers can not realize an efficient iterative method. Because of that, standard training protocols do not lead to competitive solvers. To overcome this difficulty, we use <b>parameter</b> <b>sharing</b> and serialization of layers. The resulting network can be trained on linear problems with thousands of unknowns and retains its efficiency on problems with millions of unknowns. From the point of view of numerical linear algebra network&rsquo;s training corresponds to finding optimal smoothers for the geometric multigrid method. We demonstrate our approach on a few second-order elliptic equations. For tested linear systems, we obtain from two to five times smaller spectral radius of the error propagation matrix compare to a basic linear multigrid with Jacobi smoother.</p></p class="citation"></blockquote><h3 id=36--241248-how-to-split-a-tera-polynomial-françois-vigneron-et-al-2024>(3/6 | 241/248) How to split a tera-polynomial (François Vigneron et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>François Vigneron, Nicolae Mihalache. (2024)<br><strong>How to split a tera-polynomial</strong><br><button class=copy-to-clipboard title="How to split a tera-polynomial" index=241>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-241 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: cs-NA, math-DS, math-NA, math.NA<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.06083v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.06083v1.pdf filename=2402.06083v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This article presents a new algorithm to compute all the roots of two families of polynomials that are of interest for the Mandelbrot set $\mathcal{M}$ : the roots of those polynomials are respectively the parameters $c\in\mathcal{M}$ associated with periodic critical dynamics for $f_c(z)=z^2+c$ (hyperbolic centers) or with pre-periodic dynamics (Misiurewicz-Thurston parameters). The algorithm is based on the computation of discrete level lines that provide excellent starting points for the Newton method. In practice, we observe that these polynomials can be split in linear time of the degree. This article is paired with a code library \citelib{MLib} that implements this algorithm. Using this library and about 723 000 core-hours on the HPC center \textit{Rom'eo} (Reims), we have successfully found all hyperbolic centers of period $\leq 41$ and all Misiurewicz-Thurston parameters whose period and pre-period sum to $\leq 35$. Concretely, this task involves splitting a tera-polynomial, i.e. a polynomial of degree $\sim10^{12}$, which is orders of magnitude ahead of the previous state of the art. It also involves dealing with the certifiability of our numerical results, which is an issue that we address in detail, both mathematically and along the production chain. The certified database is available to the scientific community. For the smaller periods that can be represented using only hardware arithmetic (floating points FP80), the implementation of our algorithm can split the corresponding polynomials of degree $\sim10^{9}$ in less than one day-core. We complement these <b>benchmarks</b> with a statistical analysis of the separation of the roots, which confirms that no other polynomial in these families can be split without using higher precision arithmetic.</p></p class="citation"></blockquote><h3 id=46--242248-sear-pc-sensitivity-enhanced-arbitrary-polynomial-chaos-nick-pepper-et-al-2024>(4/6 | 242/248) SeAr PC: Sensitivity Enhanced Arbitrary Polynomial Chaos (Nick Pepper et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Nick Pepper, Francesco Montomoli, Kyriakos Kantarakias. (2024)<br><strong>SeAr PC: Sensitivity Enhanced Arbitrary Polynomial Chaos</strong><br><button class=copy-to-clipboard title="SeAr PC: Sensitivity Enhanced Arbitrary Polynomial Chaos" index=242>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-242 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: cs-NA, math-NA, math.NA<br>Keyword Score: 3<br>Keywords: Multi-modal<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05507v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05507v1.pdf filename=2402.05507v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>This paper presents a method for performing Uncertainty Quantification in high-dimensional uncertain spaces by combining arbitrary polynomial chaos with a recently proposed scheme for sensitivity enhancement (1). Including available sensitivity information offers a way to mitigate the curse of dimensionality in Polynomial Chaos Expansions (PCEs). Coupling the sensitivity enhancement to arbitrary Polynomial Chaos allows the formulation to be extended to a wide range of stochastic processes, including <b>multi-modal,</b> fat-tailed, and truncated probability distributions. In so doing, this work addresses two of the barriers to widespread industrial application of PCEs. The method is demonstrated for a number of synthetic test cases, including an uncertainty analysis of a Finite Element structure, determined using Topology Optimisation, with 306 uncertain inputs. We demonstrate that by exploiting sensitivity information, PCEs can feasibly be applied to such problems and through the Sobol sensitivity indices, can allow a designer to easily visualise the spatial distribution of the contributions to uncertainty in the structure.</p></p class="citation"></blockquote><h3 id=56--243248-robust-implicit-adaptive-low-rank-time-stepping-methods-for-matrix-differential-equations-daniel-appelö-et-al-2024>(5/6 | 243/248) Robust Implicit Adaptive Low Rank Time-Stepping Methods for Matrix Differential Equations (Daniel Appelö et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Daniel Appelö, Yingda Cheng. (2024)<br><strong>Robust Implicit Adaptive Low Rank Time-Stepping Methods for Matrix Differential Equations</strong><br><button class=copy-to-clipboard title="Robust Implicit Adaptive Low Rank Time-Stepping Methods for Matrix Differential Equations" index=243>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-243 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.NA<br>Categories: 65, cs-NA, math-NA, math.NA<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05347v2 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05347v2.pdf filename=2402.05347v2.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this work, we develop implicit rank-adaptive schemes for time-dependent matrix differential equations. The dynamic low rank approximation (DLRA) is a well-known technique to capture the dynamic low rank structure based on Dirac-Frenkel time-dependent variational principle. In recent years, it has attracted a lot of attention due to its wide applicability. Our schemes are inspired by the three-step procedure used in the rank adaptive version of the unconventional robust integrator (the so called BUG integrator) for DLRA. First, a prediction (basis update) step is made computing the approximate column and row spaces at the next time level. Second, a Galerkin evolution step is invoked using a base implicit solve for the small core matrix. Finally, a truncation is made according to a prescribed error threshold. Since the DLRA is evolving the differential equation projected on to the tangent space of the low rank manifold, the error estimate of the BUG integrator contains the tangent projection (modeling) error which cannot be easily controlled by mesh refinement. This can cause convergence issue for equations with cross terms. To address this issue, we propose a simple modification, consisting of merging the row and column spaces from the explicit step truncation method together with the BUG spaces in the prediction step. In addition, we propose an adaptive strategy where the BUG spaces are only computed if the residual for the solution obtained from the prediction space by explicit step truncation method, is too large. We prove stability and estimate the local truncation error of the schemes under assumptions. We <b>benchmark</b> the schemes in several tests, such as anisotropic diffusion, solid body rotation and the combination of the two, to show robust convergence properties.</p></p class="citation"></blockquote><h2 id=physicsmed-ph-1>physics.med-ph (1)</h2><h3 id=01--244248-neural-graphics-primitives-based-deformable-image-registration-for-on-the-fly-motion-extraction-xia-li-et-al-2024>(0/1 | 244/248) Neural Graphics Primitives-based Deformable Image Registration for On-the-fly Motion Extraction (Xia Li et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Xia Li, Fabian Zhang, Muheng Li, Damien Weber, Antony Lomax, Joachim Buhmann, Ye Zhang. (2024)<br><strong>Neural Graphics Primitives-based Deformable Image Registration for On-the-fly Motion Extraction</strong><br><button class=copy-to-clipboard title="Neural Graphics Primitives-based Deformable Image Registration for On-the-fly Motion Extraction" index=244>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-244 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: physics.med-ph<br>Categories: cs-CV, physics-med-ph, physics.med-ph<br>Keyword Score: 10<br>Keywords: Self-supervised Learning<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05568v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05568v1.pdf filename=2402.05568v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Intra-fraction motion in radiotherapy is commonly modeled using deformable image registration (DIR). However, existing methods often struggle to balance speed and accuracy, limiting their applicability in clinical scenarios. This study introduces a novel approach that harnesses Neural Graphics Primitives (NGP) to optimize the displacement vector field (DVF). Our method leverages learned primitives, processed as splats, and interpolates within space using a shallow neural network. Uniquely, it enables <b>self-supervised</b> optimization at an ultra-fast speed, negating the need for pre-training on extensive datasets and allowing seamless adaptation to new cases. We validated this approach on the 4D-CT lung dataset DIR-lab, achieving a target registration error (TRE) of 1.15\pm1.15 mm within a remarkable time of 1.77 seconds. Notably, our method also addresses the sliding boundary problem, a common challenge in conventional DIR methods.</p></p class="citation"></blockquote><h2 id=csds-1>cs.DS (1)</h2><h3 id=01--245248-tight-approximation-bounds-on-a-simple-algorithm-for-minimum-average-search-time-in-trees-svein-høgemo-2024>(0/1 | 245/248) Tight Approximation Bounds on a Simple Algorithm for Minimum Average Search Time in Trees (Svein Høgemo, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Svein Høgemo. (2024)<br><strong>Tight Approximation Bounds on a Simple Algorithm for Minimum Average Search Time in Trees</strong><br><button class=copy-to-clipboard title="Tight Approximation Bounds on a Simple Algorithm for Minimum Average Search Time in Trees" index=245>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-245 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DS<br>Categories: G-2-2, cs-DS, cs.DS<br>Keyword Score: 10<br>Keywords: Hierarchical Clustering<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05560v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05560v1.pdf filename=2402.05560v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>The graph invariant EPT-sum has cropped up in several unrelated fields in later years: As an objective function for <b>hierarchical</b> <b>clustering,</b> as a more fine-grained version of the classical edge ranking problem, and, specifically when the input is a vertex-weighted tree, as a measure of average/expected search length in a partially ordered set. The EPT-sum of a graph $G$ is defined as the minimum sum of the depth of every leaf in an edge partition tree (EPT), a rooted tree where leaves correspond to vertices in $G$ and internal nodes correspond to edges in $G$. A simple algorithm that approximates EPT-sum on trees is given by recursively choosing the most balanced edge in the input tree $G$ to build an EPT of $G$. Due to its fast runtime, this balanced cut algorithm is used in practice. In this paper, we show that the balanced cut algorithm gives a 1.5-approximation of EPT-sum on trees, which amounts to a tight analysis and answers a question posed by Cicalese et al. in 2014.</p></p class="citation"></blockquote><h2 id=q-biogn-1>q-bio.GN (1)</h2><h3 id=01--246248-machine-learning-applied-to-omics-data-aida-calviño-et-al-2024>(0/1 | 246/248) Machine learning applied to omics data (Aida Calviño et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Aida Calviño, Almudena Moreno-Ribera, Silvia Pineda. (2024)<br><strong>Machine learning applied to omics data</strong><br><button class=copy-to-clipboard title="Machine learning applied to omics data" index=246>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-246 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: q-bio.GN<br>Categories: cs-LG, q-bio-GN, q-bio.GN, stat-AP<br>Keyword Score: 10<br>Keywords: Logistic Regression<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05543v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05543v1.pdf filename=2402.05543v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>In this chapter we illustrate the use of some Machine Learning techniques in the context of omics data. More precisely, we review and evaluate the use of Random Forest and Penalized Multinomial <b>Logistic</b> <b>Regression</b> for integrative analysis of genomics and immunomics in pancreatic cancer. Furthermore, we propose the use of association rules with predictive purposes to overcome the low predictive power of the previously mentioned models. Finally, we apply the reviewed methods to a real data set from TCGA made of 107 tumoral pancreatic samples and 117,486 germline SNPs, showing the good performance of the proposed methods to predict the immunological infiltration in pancreatic cancer.</p></p class="citation"></blockquote><h2 id=csdl-1>cs.DL (1)</h2><h3 id=01--247248-can-chatgpt-evaluate-research-quality-mike-thelwall-2024>(0/1 | 247/248) Can ChatGPT evaluate research quality? (Mike Thelwall, 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Mike Thelwall. (2024)<br><strong>Can ChatGPT evaluate research quality?</strong><br><button class=copy-to-clipboard title="Can ChatGPT evaluate research quality?" index=247>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-247 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: cs.DL<br>Categories: cs-AI, cs-DL, cs.DL<br>Keyword Score: 10<br>Keywords: ChatGPT<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05519v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05519v1.pdf filename=2402.05519v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Purpose: Assess whether <b>ChatGPT</b> 4.0 is accurate enough to perform research evaluations on journal articles to automate this time-consuming task. Design/methodology/approach: Test the extent to which <b>ChatGPT-4</b> can assess the quality of journal articles using a case study of the published scoring guidelines of the UK Research Excellence Framework (REF) 2021 to create a research evaluation <b>ChatGPT.</b> This was applied to 51 of my own articles and compared against my own quality judgements. Findings: <b>ChatGPT-4</b> can produce plausible document summaries and quality evaluation rationales that match the REF criteria. Its overall scores have weak correlations with my self-evaluation scores of the same documents (averaging r=0.281 over 15 iterations, with 8 being statistically significantly different from 0). In contrast, the average scores from the 15 iterations produced a statistically significant positive correlation of 0.509. Thus, averaging scores from multiple <b>ChatGPT-4</b> rounds seems more effective than individual scores. The positive correlation may be due to <b>ChatGPT</b> being able to extract the author&rsquo;s significance, rigour, and originality claims from inside each paper. If my weakest articles are removed, then the correlation with average scores (r=0.200) falls below statistical significance, suggesting that <b>ChatGPT</b> struggles to make fine-grained evaluations. Research limitations: The data is self-evaluations of a convenience sample of articles from one academic in one field. Practical implications: Overall, <b>ChatGPT</b> does not yet seem to be accurate enough to be trusted for any formal or informal research quality evaluation tasks. Research evaluators, including journal editors, should therefore take steps to control its use. Originality/value: This is the first published attempt at post-publication expert review accuracy testing for <b>ChatGPT.</b></p></p class="citation"></blockquote><h2 id=mathoc-1>math.OC (1)</h2><h3 id=01--248248-machine-learning-augmented-branch-and-bound-for-mixed-integer-linear-programming-lara-scavuzzo-et-al-2024>(0/1 | 248/248) Machine Learning Augmented Branch and Bound for Mixed Integer Linear Programming (Lara Scavuzzo et al., 2024)</h3><style>blockquote.citation{margin-top:1.5rem;margin-bottom:1.5rem;position:relative;padding:7px 16px;box-sizing:border-box;font-style:italic;color:#585858;border:solid 3px #585858}blockquote.citation:before{padding-top:6px;display:inline-block;position:absolute;top:-20px;left:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10d";background:#585858;color:#fff;font-size:28px}blockquote.citation:after{padding-top:6px;display:inline-block;position:absolute;bottom:-20px;right:-20px;width:40px;height:40px;line-height:40px;border-radius:50%;text-align:center;font:var(--fa-font-solid);content:"\f10e";background:#585858;color:#fff;font-size:28px;font-weight:900}blockquote.citation p.citation{padding:0;margin:10px 0;margin-left:1.5rem;line-height:1.7}blockquote.citation cite{display:block;text-align:right;color:#888;font-size:.9em}</style><blockquote class=citation><p class=citation><p>Lara Scavuzzo, Karen Aardal, Andrea Lodi, Neil Yorke-Smith. (2024)<br><strong>Machine Learning Augmented Branch and Bound for Mixed Integer Linear Programming</strong><br><button class=copy-to-clipboard title="Machine Learning Augmented Branch and Bound for Mixed Integer Linear Programming" index=248>
<span class=copy-to-clipboard-item>Copy Title<span></button></p><div class="toast toast-copied toast-index-248 align-items-center text-bg-secondary border-0 position-absolute top-0 end-0" role=alert aria-live=assertive aria-atomic=true><div class=d-flex><div class=toast-body>Copied!</div></div></div><hr><p>Primary Category: math.OC<br>Categories: cs-LG, math-OC, math.OC<br>Keyword Score: 3<br>Keywords: Benchmarking<br><a type=button class="btn btn-outline-primary" href=http://arxiv.org/abs/2402.05501v1 target=_blank>Paper Link</a>
<button type=button class="btn btn-outline-primary download-pdf" url=https://arxiv.org/pdf/2402.05501v1.pdf filename=2402.05501v1.pdf>Download PDF</button></p><hr><p><strong>ABSTRACT</strong><br>Mixed Integer Linear Programming (MILP) is a pillar of mathematical optimization that offers a powerful modeling language for a wide range of applications. During the past decades, enormous algorithmic progress has been made in solving MILPs, and many commercial and academic software packages exist. Nevertheless, the availability of data, both from problem instances and from solvers, and the desire to solve new problems and larger (real-life) instances, trigger the need for continuing algorithmic development. MILP solvers use branch and bound as their main component. In recent years, there has been an explosive development in the use of machine learning algorithms for enhancing all main tasks involved in the branch-and-bound algorithm, such as primal heuristics, branching, cutting planes, node selection and solver configuration decisions. This paper presents a survey of such approaches, addressing the vision of integration of machine learning and mathematical optimization as complementary technologies, and how this integration can benefit MILP solving. In particular, we give detailed attention to machine learning algorithms that automatically optimize some metric of branch-and-bound efficiency. We also address how to represent MILPs in the context of applying learning algorithms, MILP <b>benchmarks</b> and software.</p></p class="citation"></blockquote></div><div class="row pl-3 pr-3"><div class="col-md-6 share-buttons"></div></div><hr><div class="row next-prev-navigator"><div class="col-md-6 previous-article"><a href=/akitenkrad-blog/posts/arxiv/202402/20240209000000/ title="arXiv @ 2024.02.09" class="btn btn-outline-info"><div><i class="fas fa-chevron-circle-left"></i> Prev</div><div class=next-prev-text>arXiv @ 2024.02.09</div></a></div><div class="col-md-6 next-article"><a href=/akitenkrad-blog/posts/arxiv/202402/20240211000000/ title="arXiv @ 2024.02.11" class="btn btn-outline-info"><div>Next <i class="fas fa-chevron-circle-right"></i></div><div class=next-prev-text>arXiv @ 2024.02.11</div></a></div></div><hr></div></div></div><a id=scroll-to-top class=btn><i class="fas fa-chevron-circle-up"></i></a></section><section class=toc-section id=toc-section><div class=toc-holder><h5 class="text-center pl-3">Table of Contents</h5><hr><div class=toc><nav id=TableOfContents><ul><li><a href=#primary-categories>Primary Categories</a></li><li><a href=#keywords>Keywords</a></li><li><a href=#cscl-38>cs.CL (38)</a><ul><li><a href=#038--1248-in-context-principle-learning-from-mistakes-tianjun-zhang-et-al-2024>(0/38 | 1/248) In-Context Principle Learning from Mistakes (Tianjun Zhang et al., 2024)</a></li><li><a href=#138--2248-efficient-models-for-the-detection-of-hate-abuse-and-profanity-christoph-tillmann-et-al-2024>(1/38 | 2/248) Efficient Models for the Detection of Hate, Abuse and Profanity (Christoph Tillmann et al., 2024)</a></li><li><a href=#238--3248-zero-shot-chain-of-thought-reasoning-guided-by-evolutionary-algorithms-in-large-language-models-feihu-jin-et-al-2024>(2/38 | 3/248) Zero-Shot Chain-of-Thought Reasoning Guided by Evolutionary Algorithms in Large Language Models (Feihu Jin et al., 2024)</a></li><li><a href=#338--4248-self-alignment-of-large-language-models-via-monopolylogue-based-social-scene-simulation-xianghe-pang-et-al-2024>(3/38 | 4/248) Self-Alignment of Large Language Models via Monopolylogue-based Social Scene Simulation (Xianghe Pang et al., 2024)</a></li><li><a href=#438--5248-unified-speech-text-pretraining-for-spoken-dialog-modeling-heeseung-kim-et-al-2024>(4/38 | 5/248) Unified Speech-Text Pretraining for Spoken Dialog Modeling (Heeseung Kim et al., 2024)</a></li><li><a href=#538--6248-a-prompt-response-to-the-demand-for-automatic-gender-neutral-translation-beatrice-savoldi-et-al-2024>(5/38 | 6/248) A Prompt Response to the Demand for Automatic Gender-Neutral Translation (Beatrice Savoldi et al., 2024)</a></li><li><a href=#638--7248-emojicrypt-prompt-encryption-for-secure-communication-with-large-language-models-guo-lin-et-al-2024>(6/38 | 7/248) EmojiCrypt: Prompt Encryption for Secure Communication with Large Language Models (Guo Lin et al., 2024)</a></li><li><a href=#738--8248-timearena-shaping-efficient-multitasking-language-agents-in-a-time-aware-simulation-yikai-zhang-et-al-2024>(7/38 | 8/248) TimeArena: Shaping Efficient Multitasking Language Agents in a Time-Aware Simulation (Yikai Zhang et al., 2024)</a></li><li><a href=#838--9248-gpt-4-generated-narratives-of-life-events-using-a-structured-narrative-prompt-a-validation-study-christopher-j-lynch-et-al-2024>(8/38 | 9/248) GPT-4 Generated Narratives of Life Events using a Structured Narrative Prompt: A Validation Study (Christopher J. Lynch et al., 2024)</a></li><li><a href=#938--10248-weblinx-real-world-website-navigation-with-multi-turn-dialogue-xing-han-lù-et-al-2024>(9/38 | 10/248) WebLINX: Real-World Website Navigation with Multi-Turn Dialogue (Xing Han Lù et al., 2024)</a></li><li><a href=#1038--11248-its-never-too-late-fusing-acoustic-information-into-large-language-models-for-automatic-speech-recognition-chen-chen-et-al-2024>(10/38 | 11/248) It&rsquo;s Never Too Late: Fusing Acoustic Information into Large Language Models for Automatic Speech Recognition (Chen Chen et al., 2024)</a></li><li><a href=#1138--12248-merging-facts-crafting-fallacies-evaluating-the-contradictory-nature-of-aggregated-factual-claims-in-long-form-generations-cheng-han-chiang-et-al-2024>(11/38 | 12/248) Merging Facts, Crafting Fallacies: Evaluating the Contradictory Nature of Aggregated Factual Claims in Long-Form Generations (Cheng-Han Chiang et al., 2024)</a></li><li><a href=#1238--13248-attnlrp-attention-aware-layer-wise-relevance-propagation-for-transformers-reduan-achtibat-et-al-2024>(12/38 | 13/248) AttnLRP: Attention-Aware Layer-wise Relevance Propagation for Transformers (Reduan Achtibat et al., 2024)</a></li><li><a href=#1338--14248-named-entity-recognition-for-address-extraction-in-speech-to-text-transcriptions-using-synthetic-data-bibiána-lajčinová-et-al-2024>(13/38 | 14/248) Named Entity Recognition for Address Extraction in Speech-to-Text Transcriptions Using Synthetic Data (Bibiána Lajčinová et al., 2024)</a></li><li><a href=#1438--15248-benchmarking-large-language-models-on-communicative-medical-coaching-a-novel-system-and-dataset-hengguan-huang-et-al-2024>(14/38 | 15/248) Benchmarking Large Language Models on Communicative Medical Coaching: a Novel System and Dataset (Hengguan Huang et al., 2024)</a></li><li><a href=#1538--16248-efficient-stagewise-pretraining-via-progressive-subnetworks-abhishek-panigrahi-et-al-2024>(15/38 | 16/248) Efficient Stagewise Pretraining via Progressive Subnetworks (Abhishek Panigrahi et al., 2024)</a></li><li><a href=#1638--17248-is-it-possible-to-edit-large-language-models-robustly-xinbei-ma-et-al-2024>(16/38 | 17/248) Is it Possible to Edit Large Language Models Robustly? (Xinbei Ma et al., 2024)</a></li><li><a href=#1738--18248-selective-forgetting-advancing-machine-unlearning-techniques-and-evaluation-in-language-models-lingzhi-wang-et-al-2024>(17/38 | 18/248) Selective Forgetting: Advancing Machine Unlearning Techniques and Evaluation in Language Models (Lingzhi Wang et al., 2024)</a></li><li><a href=#1838--19248-autoaugment-is-what-you-need-enhancing-rule-based-augmentation-methods-in-low-resource-regimes-juhwan-choi-et-al-2024>(18/38 | 19/248) AutoAugment Is What You Need: Enhancing Rule-based Augmentation Methods in Low-resource Regimes (Juhwan Choi et al., 2024)</a></li><li><a href=#1938--20248-noisyicl-a-little-noise-in-model-parameters-calibrates-in-context-learning-yufeng-zhao-et-al-2024>(19/38 | 20/248) NoisyICL: A Little Noise in Model Parameters Calibrates In-context Learning (Yufeng Zhao et al., 2024)</a></li><li><a href=#2038--21248-spirit-lm-interleaved-spoken-and-written-language-model-tu-anh-nguyen-et-al-2024>(20/38 | 21/248) SpiRit-LM: Interleaved Spoken and Written Language Model (Tu Anh Nguyen et al., 2024)</a></li><li><a href=#2138--22248-exploring-visual-culture-awareness-in-gpt-4v-a-comprehensive-probing-yong-cao-et-al-2024>(21/38 | 22/248) Exploring Visual Culture Awareness in GPT-4V: A Comprehensive Probing (Yong Cao et al., 2024)</a></li><li><a href=#2238--23248-rethinking-data-selection-for-supervised-fine-tuning-ming-shen-2024>(22/38 | 23/248) Rethinking Data Selection for Supervised Fine-Tuning (Ming Shen, 2024)</a></li><li><a href=#2338--24248-fact-gpt-fact-checking-augmentation-via-claim-matching-with-llms-eun-cheol-choi-et-al-2024>(23/38 | 24/248) FACT-GPT: Fact-Checking Augmentation via Claim Matching with LLMs (Eun Cheol Choi et al., 2024)</a></li><li><a href=#2438--25248-phonetically-rich-corpus-construction-for-a-low-resourced-language-marcellus-amadeus-et-al-2024>(24/38 | 25/248) Phonetically rich corpus construction for a low-resourced language (Marcellus Amadeus et al., 2024)</a></li><li><a href=#2538--26248-gpts-are-multilingual-annotators-for-sequence-generation-tasks-juhwan-choi-et-al-2024>(25/38 | 26/248) GPTs Are Multilingual Annotators for Sequence Generation Tasks (Juhwan Choi et al., 2024)</a></li><li><a href=#2638--27248-large-language-models-for-psycholinguistic-plausibility-pretesting-samuel-joseph-amouyal-et-al-2024>(26/38 | 27/248) Large Language Models for Psycholinguistic Plausibility Pretesting (Samuel Joseph Amouyal et al., 2024)</a></li><li><a href=#2738--28248-generative-echo-chamber-effects-of-llm-powered-search-systems-on-diverse-information-seeking-nikhil-sharma-et-al-2024>(27/38 | 28/248) Generative Echo Chamber? Effects of LLM-Powered Search Systems on Diverse Information Seeking (Nikhil Sharma et al., 2024)</a></li><li><a href=#2838--29248-permute-and-flip-an-optimally-robust-and-watermarkable-decoder-for-llms-xuandong-zhao-et-al-2024>(28/38 | 29/248) Permute-and-Flip: An optimally robust and watermarkable decoder for LLMs (Xuandong Zhao et al., 2024)</a></li><li><a href=#2938--30248-text-to-code-generation-with-modality-relative-pre-training-fenia-christopoulou-et-al-2024>(29/38 | 30/248) Text-to-Code Generation with Modality-relative Pre-training (Fenia Christopoulou et al., 2024)</a></li><li><a href=#3038--31248-multilingual-e5-text-embeddings-a-technical-report-liang-wang-et-al-2024>(30/38 | 31/248) Multilingual E5 Text Embeddings: A Technical Report (Liang Wang et al., 2024)</a></li><li><a href=#3138--32248-pretrained-generative-language-models-as-general-learning-frameworks-for-sequence-based-tasks-ben-fauber-2024>(31/38 | 32/248) Pretrained Generative Language Models as General Learning Frameworks for Sequence-Based Tasks (Ben Fauber, 2024)</a></li><li><a href=#3238--33248-establishing-degrees-of-closeness-between-audio-recordings-along-different-dimensions-using-large-scale-cross-lingual-models-maxime-fily-et-al-2024>(32/38 | 33/248) Establishing degrees of closeness between audio recordings along different dimensions using large-scale cross-lingual models (Maxime Fily et al., 2024)</a></li><li><a href=#3338--34248-traditional-machine-learning-models-and-bidirectional-encoder-representations-from-transformer-bert-based-automatic-classification-of-tweets-about-eating-disorders-algorithm-development-and-validation-study-josé-alberto-benítez-andrades-et-al-2024>(33/38 | 34/248) Traditional Machine Learning Models and Bidirectional Encoder Representations From Transformer (BERT)-Based Automatic Classification of Tweets About Eating Disorders: Algorithm Development and Validation Study (José Alberto Benítez-Andrades et al., 2024)</a></li><li><a href=#3438--35248-improving-agent-interactions-in-virtual-environments-with-language-models-jack-zhang-2024>(34/38 | 35/248) Improving Agent Interactions in Virtual Environments with Language Models (Jack Zhang, 2024)</a></li><li><a href=#3538--36248-lightcam-a-fast-and-light-implementation-of-context-aware-masking-based-d-tdnn-for-speaker-verification-di-cao-et-al-2024>(35/38 | 36/248) LightCAM: A Fast and Light Implementation of Context-Aware Masking based D-TDNN for Speaker Verification (Di Cao et al., 2024)</a></li><li><a href=#3638--37248-faq-gen-an-automated-system-to-generate-domain-specific-faqs-to-aid-content-comprehension-sahil-kale-et-al-2024>(36/38 | 37/248) FAQ-Gen: An automated system to generate domain-specific FAQs to aid content comprehension (Sahil Kale et al., 2024)</a></li><li><a href=#3738--38248-softeda-rethinking-rule-based-data-augmentation-with-soft-labels-juhwan-choi-et-al-2024>(37/38 | 38/248) SoftEDA: Rethinking Rule-Based Data Augmentation with Soft Labels (Juhwan Choi et al., 2024)</a></li></ul></li><li><a href=#csai-16>cs.AI (16)</a><ul><li><a href=#016--39248-large-language-model-meets-graph-neural-network-in-knowledge-distillation-shengxiang-hu-et-al-2024>(0/16 | 39/248) Large Language Model Meets Graph Neural Network in Knowledge Distillation (Shengxiang Hu et al., 2024)</a></li><li><a href=#116--40248-guiding-large-language-models-with-divide-and-conquer-program-for-discerning-problem-solving-yizhou-zhang-et-al-2024>(1/16 | 40/248) Guiding Large Language Models with Divide-and-Conquer Program for Discerning Problem Solving (Yizhou Zhang et al., 2024)</a></li><li><a href=#216--41248-how-well-can-llms-negotiate-negotiationarena-platform-and-analysis-federico-bianchi-et-al-2024>(2/16 | 41/248) How Well Can LLMs Negotiate? NegotiationArena Platform and Analysis (Federico Bianchi et al., 2024)</a></li><li><a href=#316--42248-knowledge-graphs-meet-multi-modal-learning-a-comprehensive-survey-zhuo-chen-et-al-2024>(3/16 | 42/248) Knowledge Graphs Meet Multi-Modal Learning: A Comprehensive Survey (Zhuo Chen et al., 2024)</a></li><li><a href=#416--43248-rapid-optimization-for-jailbreaking-llms-via-subconscious-exploitation-and-echopraxia-guangyu-shen-et-al-2024>(4/16 | 43/248) Rapid Optimization for Jailbreaking LLMs via Subconscious Exploitation and Echopraxia (Guangyu Shen et al., 2024)</a></li><li><a href=#516--44248-opentom-a-comprehensive-benchmark-for-evaluating-theory-of-mind-reasoning-capabilities-of-large-language-models-hainiu-xu-et-al-2024>(5/16 | 44/248) OpenToM: A Comprehensive Benchmark for Evaluating Theory-of-Mind Reasoning Capabilities of Large Language Models (Hainiu Xu et al., 2024)</a></li><li><a href=#616--45248-training-large-language-models-for-reasoning-through-reverse-curriculum-reinforcement-learning-zhiheng-xi-et-al-2024>(6/16 | 45/248) Training Large Language Models for Reasoning through Reverse Curriculum Reinforcement Learning (Zhiheng Xi et al., 2024)</a></li><li><a href=#716--46248-integrating-llms-for-explainable-fault-diagnosis-in-complex-systems-akshay-j-dave-et-al-2024>(7/16 | 46/248) Integrating LLMs for Explainable Fault Diagnosis in Complex Systems (Akshay J. Dave et al., 2024)</a></li><li><a href=#816--47248-doing-experiments-and-revising-rules-with-natural-language-and-probabilistic-reasoning-top-piriyakulkij-et-al-2024>(8/16 | 47/248) Doing Experiments and Revising Rules with Natural Language and Probabilistic Reasoning (Top Piriyakulkij et al., 2024)</a></li><li><a href=#916--48248-limitations-of-agents-simulated-by-predictive-models-raymond-douglas-et-al-2024>(9/16 | 48/248) Limitations of Agents Simulated by Predictive Models (Raymond Douglas et al., 2024)</a></li><li><a href=#1016--49248-prompting-fairness-artificial-intelligence-as-game-players-jazmia-henry-2024>(10/16 | 49/248) Prompting Fairness: Artificial Intelligence as Game Players (Jazmia Henry, 2024)</a></li><li><a href=#1116--50248-an-interactive-agent-foundation-model-zane-durante-et-al-2024>(11/16 | 50/248) An Interactive Agent Foundation Model (Zane Durante et al., 2024)</a></li><li><a href=#1216--51248-twig-towards-pre-hoc-hyperparameter-optimisation-and-cross-graph-generalisation-via-simulated-kge-models-jeffrey-sardina-et-al-2024>(12/16 | 51/248) TWIG: Towards pre-hoc Hyperparameter Optimisation and Cross-Graph Generalisation via Simulated KGE Models (Jeffrey Sardina et al., 2024)</a></li><li><a href=#1316--52248-veni-vidi-vici-solving-the-myriad-of-challenges-before-knowledge-graph-learning-jeffrey-sardina-et-al-2024>(13/16 | 52/248) Veni, Vidi, Vici: Solving the Myriad of Challenges before Knowledge Graph Learning (Jeffrey Sardina et al., 2024)</a></li><li><a href=#1416--53248-optimizing-delegation-in-collaborative-human-ai-hybrid-teams-andrew-fuchs-et-al-2024>(14/16 | 53/248) Optimizing Delegation in Collaborative Human-AI Hybrid Teams (Andrew Fuchs et al., 2024)</a></li><li><a href=#1516--54248-kix-a-metacognitive-generalization-framework-arun-kumar-et-al-2024>(15/16 | 54/248) KIX: A Metacognitive Generalization Framework (Arun Kumar et al., 2024)</a></li></ul></li><li><a href=#cslg-65>cs.LG (65)</a><ul><li><a href=#065--55248-in-context-learning-can-re-learn-forbidden-tasks-sophie-xhonneux-et-al-2024>(0/65 | 55/248) In-Context Learning Can Re-learn Forbidden Tasks (Sophie Xhonneux et al., 2024)</a></li><li><a href=#165--56248-descriptive-kernel-convolution-network-with-improved-random-walk-kernel-meng-chieh-lee-et-al-2024>(1/65 | 56/248) Descriptive Kernel Convolution Network with Improved Random Walk Kernel (Meng-Chieh Lee et al., 2024)</a></li><li><a href=#265--57248-classifying-nodes-in-graphs-without-gnns-daniel-winter-et-al-2024>(2/65 | 57/248) Classifying Nodes in Graphs without GNNs (Daniel Winter et al., 2024)</a></li><li><a href=#365--58248-limits-of-transformer-language-models-on-learning-algorithmic-compositions-jonathan-thomm-et-al-2024>(3/65 | 58/248) Limits of Transformer Language Models on Learning Algorithmic Compositions (Jonathan Thomm et al., 2024)</a></li><li><a href=#465--59248-game-theoretic-counterfactual-explanation-for-graph-neural-networks-chirag-chhablani-et-al-2024>(4/65 | 59/248) Game-theoretic Counterfactual Explanation for Graph Neural Networks (Chirag Chhablani et al., 2024)</a></li><li><a href=#565--60248-on-the-convergence-of-zeroth-order-federated-tuning-in-large-language-models-zhenqing-ling-et-al-2024>(5/65 | 60/248) On the Convergence of Zeroth-Order Federated Tuning in Large Language Models (Zhenqing Ling et al., 2024)</a></li><li><a href=#665--61248-offline-actor-critic-reinforcement-learning-scales-to-large-models-jost-tobias-springenberg-et-al-2024>(6/65 | 61/248) Offline Actor-Critic Reinforcement Learning Scales to Large Models (Jost Tobias Springenberg et al., 2024)</a></li><li><a href=#765--62248-accurate-lora-finetuning-quantization-of-llms-via-information-retention-haotong-qin-et-al-2024>(7/65 | 62/248) Accurate LoRA-Finetuning Quantization of LLMs via Information Retention (Haotong Qin et al., 2024)</a></li><li><a href=#865--63248-let-your-graph-do-the-talking-encoding-structured-data-for-llms-bryan-perozzi-et-al-2024>(8/65 | 63/248) Let Your Graph Do the Talking: Encoding Structured Data for LLMs (Bryan Perozzi et al., 2024)</a></li><li><a href=#965--64248-empowering-machine-learning-models-with-contextual-knowledge-for-enhancing-the-detection-of-eating-disorders-in-social-media-posts-josé-alberto-benítez-andrades-et-al-2024>(9/65 | 64/248) Empowering machine learning models with contextual knowledge for enhancing the detection of eating disorders in social media posts (José Alberto Benítez-Andrades et al., 2024)</a></li><li><a href=#1065--65248-scaling-artificial-intelligence-for-digital-wargaming-in-support-of-decision-making-scotty-black-et-al-2024>(10/65 | 65/248) Scaling Artificial Intelligence for Digital Wargaming in Support of Decision-Making (Scotty Black et al., 2024)</a></li><li><a href=#1165--66248-federated-offline-reinforcement-learning-collaborative-single-policy-coverage-suffices-jiin-woo-et-al-2024>(11/65 | 66/248) Federated Offline Reinforcement Learning: Collaborative Single-Policy Coverage Suffices (Jiin Woo et al., 2024)</a></li><li><a href=#1265--67248-mesoscale-traffic-forecasting-for-real-time-bottleneck-and-shockwave-prediction-raphael-chekroun-et-al-2024>(12/65 | 67/248) Mesoscale Traffic Forecasting for Real-Time Bottleneck and Shockwave Prediction (Raphael Chekroun et al., 2024)</a></li><li><a href=#1365--68248-rethinking-propagation-for-unsupervised-graph-domain-adaptation-meihan-liu-et-al-2024>(13/65 | 68/248) Rethinking Propagation for Unsupervised Graph Domain Adaptation (Meihan Liu et al., 2024)</a></li><li><a href=#1465--69248-reinforcement-learning-as-a-catalyst-for-robust-and-fair-federated-learning-deciphering-the-dynamics-of-client-contributions-jialuo-he-et-al-2024>(14/65 | 69/248) Reinforcement Learning as a Catalyst for Robust and Fair Federated Learning: Deciphering the Dynamics of Client Contributions (Jialuo He et al., 2024)</a></li><li><a href=#1565--70248-learning-to-route-among-specialized-experts-for-zero-shot-generalization-mohammed-muqeeth-et-al-2024>(15/65 | 70/248) Learning to Route Among Specialized Experts for Zero-Shot Generalization (Mohammed Muqeeth et al., 2024)</a></li><li><a href=#1665--71248-sparse-vq-transformer-an-ffn-free-framework-with-vector-quantization-for-enhanced-time-series-forecasting-yanjun-zhao-et-al-2024>(16/65 | 71/248) Sparse-VQ Transformer: An FFN-Free Framework with Vector Quantization for Enhanced Time Series Forecasting (Yanjun Zhao et al., 2024)</a></li><li><a href=#1765--72248-repquant-towards-accurate-post-training-quantization-of-large-transformer-models-via-scale-reparameterization-zhikai-li-et-al-2024>(17/65 | 72/248) RepQuant: Towards Accurate Post-Training Quantization of Large Transformer Models via Scale Reparameterization (Zhikai Li et al., 2024)</a></li><li><a href=#1865--73248-hypergraph-node-classification-with-graph-neural-networks-bohan-tang-et-al-2024>(18/65 | 73/248) Hypergraph Node Classification With Graph Neural Networks (Bohan Tang et al., 2024)</a></li><li><a href=#1965--74248-subgen-token-generation-in-sublinear-time-and-memory-amir-zandieh-et-al-2024>(19/65 | 74/248) SubGen: Token Generation in Sublinear Time and Memory (Amir Zandieh et al., 2024)</a></li><li><a href=#2065--75248-scaling-intelligent-agents-in-combat-simulations-for-wargaming-scotty-black-et-al-2024>(20/65 | 75/248) Scaling Intelligent Agents in Combat Simulations for Wargaming (Scotty Black et al., 2024)</a></li><li><a href=#2165--76248-implicit-bias-and-fast-convergence-rates-for-self-attention-bhavya-vasudeva-et-al-2024>(21/65 | 76/248) Implicit Bias and Fast Convergence Rates for Self-attention (Bhavya Vasudeva et al., 2024)</a></li><li><a href=#2265--77248-model-based-rl-for-mean-field-games-is-not-statistically-harder-than-single-agent-rl-jiawei-huang-et-al-2024>(22/65 | 77/248) Model-Based RL for Mean-Field Games is not Statistically Harder than Single-Agent RL (Jiawei Huang et al., 2024)</a></li><li><a href=#2365--78248-unichain-and-aperiodicity-are-sufficient-for-asymptotic-optimality-of-average-reward-restless-bandits-yige-hong-et-al-2024>(23/65 | 78/248) Unichain and Aperiodicity are Sufficient for Asymptotic Optimality of Average-Reward Restless Bandits (Yige Hong et al., 2024)</a></li><li><a href=#2465--79248-binding-dynamics-in-rotating-features-sindy-löwe-et-al-2024>(24/65 | 79/248) Binding Dynamics in Rotating Features (Sindy Löwe et al., 2024)</a></li><li><a href=#2565--80248-asynchronous-diffusion-learning-with-agent-subsampling-and-local-updates-elsa-rizk-et-al-2024>(25/65 | 80/248) Asynchronous Diffusion Learning with Agent Subsampling and Local Updates (Elsa Rizk et al., 2024)</a></li><li><a href=#2665--81248-neural-circuit-diagrams-robust-diagrams-for-the-communication-implementation-and-analysis-of-deep-learning-architectures-vincent-abbott-2024>(26/65 | 81/248) Neural Circuit Diagrams: Robust Diagrams for the Communication, Implementation, and Analysis of Deep Learning Architectures (Vincent Abbott, 2024)</a></li><li><a href=#2765--82248-taser-temporal-adaptive-sampling-for-fast-and-accurate-dynamic-graph-representation-learning-gangda-deng-et-al-2024>(27/65 | 82/248) TASER: Temporal Adaptive Sampling for Fast and Accurate Dynamic Graph Representation Learning (Gangda Deng et al., 2024)</a></li><li><a href=#2865--83248-noise-contrastive-alignment-of-language-models-with-explicit-rewards-huayu-chen-et-al-2024>(28/65 | 83/248) Noise Contrastive Alignment of Language Models with Explicit Rewards (Huayu Chen et al., 2024)</a></li><li><a href=#2965--84248-exploring-learning-complexity-for-downstream-data-pruning-wenyu-jiang-et-al-2024>(29/65 | 84/248) Exploring Learning Complexity for Downstream Data Pruning (Wenyu Jiang et al., 2024)</a></li><li><a href=#3065--85248-eugene-explainable-unsupervised-approximation-of-graph-edit-distance-aditya-bommakanti-et-al-2024>(30/65 | 85/248) EUGENE: Explainable Unsupervised Approximation of Graph Edit Distance (Aditya Bommakanti et al., 2024)</a></li><li><a href=#3165--86248-fusionsf-fuse-heterogeneous-modalities-in-a-vector-quantized-framework-for-robust-solar-power-forecasting-ziqing-ma-et-al-2024>(31/65 | 86/248) FusionSF: Fuse Heterogeneous Modalities in a Vector Quantized Framework for Robust Solar Power Forecasting (Ziqing Ma et al., 2024)</a></li><li><a href=#3265--87248-improving-token-based-world-models-with-parallel-observation-prediction-lior-cohen-et-al-2024>(32/65 | 87/248) Improving Token-Based World Models with Parallel Observation Prediction (Lior Cohen et al., 2024)</a></li><li><a href=#3365--88248-flashback-understanding-and-mitigating-forgetting-in-federated-learning-mohammed-aljahdali-et-al-2024>(33/65 | 88/248) Flashback: Understanding and Mitigating Forgetting in Federated Learning (Mohammed Aljahdali et al., 2024)</a></li><li><a href=#3465--89248-activedp-bridging-active-learning-and-data-programming-naiqing-guan-et-al-2024>(34/65 | 89/248) ActiveDP: Bridging Active Learning and Data Programming (Naiqing Guan et al., 2024)</a></li><li><a href=#3565--90248-risk-sensitive-multi-agent-reinforcement-learning-in-network-aggregative-markov-games-hafez-ghaemi-et-al-2024>(35/65 | 90/248) Risk-Sensitive Multi-Agent Reinforcement Learning in Network Aggregative Markov Games (Hafez Ghaemi et al., 2024)</a></li><li><a href=#3665--91248-discovering-temporally-aware-reinforcement-learning-algorithms-matthew-thomas-jackson-et-al-2024>(36/65 | 91/248) Discovering Temporally-Aware Reinforcement Learning Algorithms (Matthew Thomas Jackson et al., 2024)</a></li><li><a href=#3765--92248-latent-variable-model-for-high-dimensional-point-process-with-structured-missingness-maksim-sinelnikov-et-al-2024>(37/65 | 92/248) Latent variable model for high-dimensional point process with structured missingness (Maksim Sinelnikov et al., 2024)</a></li><li><a href=#3865--93248-generalized-preference-optimization-a-unified-approach-to-offline-alignment-yunhao-tang-et-al-2024>(38/65 | 93/248) Generalized Preference Optimization: A Unified Approach to Offline Alignment (Yunhao Tang et al., 2024)</a></li><li><a href=#3965--94248-simultaneously-achieving-group-exposure-fairness-and-within-group-meritocracy-in-stochastic-bandits-subham-pokhriyal-et-al-2024>(39/65 | 94/248) Simultaneously Achieving Group Exposure Fairness and Within-Group Meritocracy in Stochastic Bandits (Subham Pokhriyal et al., 2024)</a></li><li><a href=#4065--95248-differentially-private-model-based-offline-reinforcement-learning-alexandre-rio-et-al-2024>(40/65 | 95/248) Differentially Private Model-Based Offline Reinforcement Learning (Alexandre Rio et al., 2024)</a></li><li><a href=#4165--96248-multi-timescale-ensemble-q-learning-for-markov-decision-process-policy-optimization-talha-bozkus-et-al-2024>(41/65 | 96/248) Multi-Timescale Ensemble Q-learning for Markov Decision Process Policy Optimization (Talha Bozkus et al., 2024)</a></li><li><a href=#4265--97248-everybody-prune-now-structured-pruning-of-llms-with-only-forward-passes-lucio-dery-et-al-2024>(42/65 | 97/248) Everybody Prune Now: Structured Pruning of LLMs with only Forward Passes (Lucio Dery et al., 2024)</a></li><li><a href=#4365--98248-direct-acquisition-optimization-for-low-budget-active-learning-zhuokai-zhao-et-al-2024>(43/65 | 98/248) Direct Acquisition Optimization for Low-Budget Active Learning (Zhuokai Zhao et al., 2024)</a></li><li><a href=#4465--99248-optimizing-predictive-ai-in-physical-design-flows-with-mini-pixel-batch-gradient-descent-haoyu-yang-et-al-2024>(44/65 | 99/248) Optimizing Predictive AI in Physical Design Flows with Mini Pixel Batch Gradient Descent (Haoyu Yang et al., 2024)</a></li><li><a href=#4565--100248-decision-theory-guided-deep-reinforcement-learning-for-fast-learning-zelin-wan-et-al-2024>(45/65 | 100/248) Decision Theory-Guided Deep Reinforcement Learning for Fast Learning (Zelin Wan et al., 2024)</a></li><li><a href=#4665--101248-guided-evolution-with-binary-discriminators-for-ml-program-search-john-d-co-reyes-et-al-2024>(46/65 | 101/248) Guided Evolution with Binary Discriminators for ML Program Search (John D. Co-Reyes et al., 2024)</a></li><li><a href=#4765--102248-on-calibration-and-conformal-prediction-of-deep-classifiers-lahav-dabah-et-al-2024>(47/65 | 102/248) On Calibration and Conformal Prediction of Deep Classifiers (Lahav Dabah et al., 2024)</a></li><li><a href=#4865--103248-unsupervised-discovery-of-clinical-disease-signatures-using-probabilistic-independence-thomas-a-lasko-et-al-2024>(48/65 | 103/248) Unsupervised Discovery of Clinical Disease Signatures Using Probabilistic Independence (Thomas A. Lasko et al., 2024)</a></li><li><a href=#4965--104248-comparison-of-machine-learning-and-statistical-approaches-for-digital-elevation-model-dem-correction-interim-results-chukwuma-okolie-et-al-2024>(49/65 | 104/248) Comparison of machine learning and statistical approaches for digital elevation model (DEM) correction: interim results (Chukwuma Okolie et al., 2024)</a></li><li><a href=#5065--105248-analysing-the-sample-complexity-of-opponent-shaping-kitty-fung-et-al-2024>(50/65 | 105/248) Analysing the Sample Complexity of Opponent Shaping (Kitty Fung et al., 2024)</a></li><li><a href=#5165--106248-stable-autonomous-flow-matching-christopher-iliffe-sprague-et-al-2024>(51/65 | 106/248) Stable Autonomous Flow Matching (Christopher Iliffe Sprague et al., 2024)</a></li><li><a href=#5265--107248-is-adversarial-training-with-compressed-datasets-effective-tong-chen-et-al-2024>(52/65 | 107/248) Is Adversarial Training with Compressed Datasets Effective? (Tong Chen et al., 2024)</a></li><li><a href=#5365--108248-sωi-score-based-o-information-estimation-mustapha-bounoua-et-al-2024>(53/65 | 108/248) S$Ω$I: Score-based O-INFORMATION Estimation (Mustapha Bounoua et al., 2024)</a></li><li><a href=#5465--109248-determining-the-severity-of-parkinsons-disease-in-patients-using-a-multi-task-neural-network-maría-teresa-garcía-ordás-et-al-2024>(54/65 | 109/248) Determining the severity of Parkinson&rsquo;s disease in patients using a multi task neural network (María Teresa García-Ordás et al., 2024)</a></li><li><a href=#5565--110248-learning-uncertainty-aware-temporally-extended-actions-joongkyu-lee-et-al-2024>(55/65 | 110/248) Learning Uncertainty-Aware Temporally-Extended Actions (Joongkyu Lee et al., 2024)</a></li><li><a href=#5665--111248-version-age-based-client-scheduling-policy-for-federated-learning-xinyi-hu-et-al-2024>(56/65 | 111/248) Version age-based client scheduling policy for federated learning (Xinyi Hu et al., 2024)</a></li><li><a href=#5765--112248-attention-as-robust-representation-for-time-series-forecasting-peisong-niu-et-al-2024>(57/65 | 112/248) Attention as Robust Representation for Time Series Forecasting (PeiSong Niu et al., 2024)</a></li><li><a href=#5865--113248-revisiting-early-learning-regularization-when-federated-learning-meets-noisy-labels-taehyeon-kim-et-al-2024>(58/65 | 113/248) Revisiting Early-Learning Regularization When Federated Learning Meets Noisy Labels (Taehyeon Kim et al., 2024)</a></li><li><a href=#5965--114248-sharp-rates-in-dependent-learning-theory-avoiding-sample-size-deflation-for-the-square-loss-ingvar-ziemann-et-al-2024>(59/65 | 114/248) Sharp Rates in Dependent Learning Theory: Avoiding Sample Size Deflation for the Square Loss (Ingvar Ziemann et al., 2024)</a></li><li><a href=#6065--115248-function-aligned-regression-a-method-explicitly-learns-functional-derivatives-from-data-dixian-zhu-et-al-2024>(60/65 | 115/248) Function Aligned Regression: A Method Explicitly Learns Functional Derivatives from Data (Dixian Zhu et al., 2024)</a></li><li><a href=#6165--116248-contrastive-approach-to-prior-free-positive-unlabeled-learning-anish-acharya-et-al-2024>(61/65 | 116/248) Contrastive Approach to Prior Free Positive Unlabeled Learning (Anish Acharya et al., 2024)</a></li><li><a href=#6265--117248-off-policy-distributional-qλ-distributional-rl-without-importance-sampling-yunhao-tang-et-al-2024>(62/65 | 117/248) Off-policy Distributional Q($λ$): Distributional RL without Importance Sampling (Yunhao Tang et al., 2024)</a></li><li><a href=#6365--118248-digital-computers-break-the-curse-of-dimensionality-adaptive-bounds-via-finite-geometry-anastasis-kratsios-et-al-2024>(63/65 | 118/248) Digital Computers Break the Curse of Dimensionality: Adaptive Bounds via Finite Geometry (Anastasis Kratsios et al., 2024)</a></li><li><a href=#6465--119248-difftop-differentiable-trajectory-optimization-for-deep-reinforcement-and-imitation-learning-weikang-wan-et-al-2024>(64/65 | 119/248) DiffTOP: Differentiable Trajectory Optimization for Deep Reinforcement and Imitation Learning (Weikang Wan et al., 2024)</a></li></ul></li><li><a href=#cscv-36>cs.CV (36)</a><ul><li><a href=#036--120248-question-aware-vision-transformer-for-multimodal-reasoning-roy-ganz-et-al-2024>(0/36 | 120/248) Question Aware Vision Transformer for Multimodal Reasoning (Roy Ganz et al., 2024)</a></li><li><a href=#136--121248-cic-a-framework-for-culturally-aware-image-captioning-youngsik-yun-et-al-2024>(1/36 | 121/248) CIC: A framework for Culturally-aware Image Captioning (Youngsik Yun et al., 2024)</a></li><li><a href=#236--122248-sphinx-x-scaling-data-and-parameters-for-a-family-of-multi-modal-large-language-models-peng-gao-et-al-2024>(2/36 | 122/248) SPHINX-X: Scaling Data and Parameters for a Family of Multi-modal Large Language Models (Peng Gao et al., 2024)</a></li><li><a href=#336--123248-animated-stickers-bringing-stickers-to-life-with-video-diffusion-david-yan-et-al-2024>(3/36 | 123/248) Animated Stickers: Bringing Stickers to Life with Video Diffusion (David Yan et al., 2024)</a></li><li><a href=#436--124248-editable-scene-simulation-for-autonomous-driving-via-collaborative-llm-agents-yuxi-wei-et-al-2024>(4/36 | 124/248) Editable Scene Simulation for Autonomous Driving via Collaborative LLM-Agents (Yuxi Wei et al., 2024)</a></li><li><a href=#536--125248-on-convolutional-vision-transformers-for-yield-prediction-alvin-inderka-et-al-2024>(5/36 | 125/248) On Convolutional Vision Transformers for Yield Prediction (Alvin Inderka et al., 2024)</a></li><li><a href=#636--126248-enhancing-zero-shot-counting-via-language-guided-exemplar-learning-mingjie-wang-et-al-2024>(6/36 | 126/248) Enhancing Zero-shot Counting via Language-guided Exemplar Learning (Mingjie Wang et al., 2024)</a></li><li><a href=#736--127248-crema-multimodal-compositional-video-reasoning-via-efficient-modular-adaptation-and-fusion-shoubin-yu-et-al-2024>(7/36 | 127/248) CREMA: Multimodal Compositional Video Reasoning via Efficient Modular Adaptation and Fusion (Shoubin Yu et al., 2024)</a></li><li><a href=#836--128248-memory-efficient-vision-transformers-an-activation-aware-mixed-rank-compression-strategy-seyedarmin-azizi-et-al-2024>(8/36 | 128/248) Memory-Efficient Vision Transformers: An Activation-Aware Mixed-Rank Compression Strategy (Seyedarmin Azizi et al., 2024)</a></li><li><a href=#936--129248-mamba-nd-selective-state-space-modeling-for-multi-dimensional-data-shufan-li-et-al-2024>(9/36 | 129/248) Mamba-ND: Selective State Space Modeling for Multi-Dimensional Data (Shufan Li et al., 2024)</a></li><li><a href=#1036--130248-instagen-enhancing-object-detection-by-training-on-synthetic-dataset-chengjian-feng-et-al-2024>(10/36 | 130/248) InstaGen: Enhancing Object Detection by Training on Synthetic Dataset (Chengjian Feng et al., 2024)</a></li><li><a href=#1136--131248-clicksam-fine-tuning-segment-anything-model-using-click-prompts-for-ultrasound-image-segmentation-aimee-guo-et-al-2024>(11/36 | 131/248) ClickSAM: Fine-tuning Segment Anything Model using click prompts for ultrasound image segmentation (Aimee Guo et al., 2024)</a></li><li><a href=#1236--132248-memory-consolidation-enables-long-context-video-understanding-ivana-balažević-et-al-2024>(12/36 | 132/248) Memory Consolidation Enables Long-Context Video Understanding (Ivana Balažević et al., 2024)</a></li><li><a href=#1336--133248-spiking-neural-network-enhanced-hand-gesture-recognition-using-low-cost-single-photon-avalanche-diode-array-zhenya-zang-et-al-2024>(13/36 | 133/248) Spiking Neural Network Enhanced Hand Gesture Recognition Using Low-Cost Single-photon Avalanche Diode Array (Zhenya Zang et al., 2024)</a></li><li><a href=#1436--134248-task-customized-masked-autoencoder-via-mixture-of-cluster-conditional-experts-zhili-liu-et-al-2024>(14/36 | 134/248) Task-customized Masked AutoEncoder via Mixture of Cluster-conditional Experts (Zhili Liu et al., 2024)</a></li><li><a href=#1536--135248-get-what-you-want-not-what-you-dont-image-content-suppression-for-text-to-image-diffusion-models-senmao-li-et-al-2024>(15/36 | 135/248) Get What You Want, Not What You Don&rsquo;t: Image Content Suppression for Text-to-Image Diffusion Models (Senmao Li et al., 2024)</a></li><li><a href=#1636--136248-early-fusion-of-features-for-semantic-segmentation-anupam-gupta-et-al-2024>(16/36 | 136/248) Early Fusion of Features for Semantic Segmentation (Anupam Gupta et al., 2024)</a></li><li><a href=#1736--137248-point-vos-pointing-up-video-object-segmentation-idil-esen-zulfikar-et-al-2024>(17/36 | 137/248) Point-VOS: Pointing Up Video Object Segmentation (Idil Esen Zulfikar et al., 2024)</a></li><li><a href=#1836--138248-avatarmmc-3d-head-avatar-generation-and-editing-with-multi-modal-conditioning-wamiq-reyaz-para-et-al-2024>(18/36 | 138/248) AvatarMMC: 3D Head Avatar Generation and Editing with Multi-Modal Conditioning (Wamiq Reyaz Para et al., 2024)</a></li><li><a href=#1936--139248-scalable-diffusion-models-with-state-space-backbone-zhengcong-fei-et-al-2024>(19/36 | 139/248) Scalable Diffusion Models with State Space Backbone (Zhengcong Fei et al., 2024)</a></li><li><a href=#2036--140248-privacy-preserving-synthetic-continual-semantic-segmentation-for-robotic-surgery-mengya-xu-et-al-2024>(20/36 | 140/248) Privacy-Preserving Synthetic Continual Semantic Segmentation for Robotic Surgery (Mengya Xu et al., 2024)</a></li><li><a href=#2136--141248-real-time-holistic-robot-pose-estimation-with-unknown-states-shikun-ban-et-al-2024>(21/36 | 141/248) Real-time Holistic Robot Pose Estimation with Unknown States (Shikun Ban et al., 2024)</a></li><li><a href=#2236--142248-resmatch-referring-expression-segmentation-in-a-semi-supervised-manner-ying-zang-et-al-2024>(22/36 | 142/248) RESMatch: Referring Expression Segmentation in a Semi-Supervised Manner (Ying Zang et al., 2024)</a></li><li><a href=#2336--143248-segmentation-free-connectionist-temporal-classification-loss-based-ocr-model-for-text-captcha-classification-vaibhav-khatavkar-et-al-2024>(23/36 | 143/248) Segmentation-free Connectionist Temporal Classification loss based OCR Model for Text Captcha Classification (Vaibhav Khatavkar et al., 2024)</a></li><li><a href=#2436--144248-efficient-expression-neutrality-estimation-with-application-to-face-recognition-utility-prediction-marcel-grimmer-et-al-2024>(24/36 | 144/248) Efficient Expression Neutrality Estimation with Application to Face Recognition Utility Prediction (Marcel Grimmer et al., 2024)</a></li><li><a href=#2536--145248-clip-loc-multi-modal-landmark-association-for-global-localization-in-object-based-maps-shigemichi-matsuzaki-et-al-2024>(25/36 | 145/248) CLIP-Loc: Multi-modal Landmark Association for Global Localization in Object-based Maps (Shigemichi Matsuzaki et al., 2024)</a></li><li><a href=#2636--146248-diffspeaker-speech-driven-3d-facial-animation-with-diffusion-transformer-zhiyuan-ma-et-al-2024>(26/36 | 146/248) DiffSpeaker: Speech-Driven 3D Facial Animation with Diffusion Transformer (Zhiyuan Ma et al., 2024)</a></li><li><a href=#2736--147248-daplankton-benchmark-dataset-for-multi-instrument-plankton-recognition-via-fine-grained-domain-adaptation-daniel-batrakhanov-et-al-2024>(27/36 | 147/248) DAPlankton: Benchmark Dataset for Multi-instrument Plankton Recognition via Fine-grained Domain Adaptation (Daniel Batrakhanov et al., 2024)</a></li><li><a href=#2836--148248-migc-multi-instance-generation-controller-for-text-to-image-synthesis-dewei-zhou-et-al-2024>(28/36 | 148/248) MIGC: Multi-Instance Generation Controller for Text-to-Image Synthesis (Dewei Zhou et al., 2024)</a></li><li><a href=#2936--149248-clr-face-conditional-latent-refinement-for-blind-face-restoration-using-score-based-diffusion-models-maitreya-suin-et-al-2024>(29/36 | 149/248) CLR-Face: Conditional Latent Refinement for Blind Face Restoration Using Score-Based Diffusion Models (Maitreya Suin et al., 2024)</a></li><li><a href=#3036--150248-collaborative-control-for-geometry-conditioned-pbr-image-generation-shimon-vainer-et-al-2024>(30/36 | 150/248) Collaborative Control for Geometry-Conditioned PBR Image Generation (Shimon Vainer et al., 2024)</a></li><li><a href=#3136--151248-jacquard-v2-refining-datasets-using-the-human-in-the-loop-data-correction-method-qiuhao-li-et-al-2024>(31/36 | 151/248) Jacquard V2: Refining Datasets using the Human In the Loop Data Correction Method (Qiuhao Li et al., 2024)</a></li><li><a href=#3236--152248-descanning-from-scanned-to-the-original-images-with-a-color-correction-diffusion-model-junghun-cha-et-al-2024>(32/36 | 152/248) Descanning: From Scanned to the Original Images with a Color Correction Diffusion Model (Junghun Cha et al., 2024)</a></li><li><a href=#3336--153248-scrapping-the-web-for-early-wildfire-detection-mateo-lostanlen-et-al-2024>(33/36 | 153/248) Scrapping The Web For Early Wildfire Detection (Mateo Lostanlen et al., 2024)</a></li><li><a href=#3436--154248-uav-rain1k-a-benchmark-for-raindrop-removal-from-uav-aerial-imagery-wenhui-chang-et-al-2024>(34/36 | 154/248) UAV-Rain1k: A Benchmark for Raindrop Removal from UAV Aerial Imagery (Wenhui Chang et al., 2024)</a></li><li><a href=#3536--155248-mtsa-snn-a-multi-modal-time-series-analysis-model-based-on-spiking-neural-network-chengzhi-liu-et-al-2024>(35/36 | 155/248) MTSA-SNN: A Multi-modal Time Series Analysis Model Based on Spiking Neural Network (Chengzhi Liu et al., 2024)</a></li></ul></li><li><a href=#cshc-6>cs.HC (6)</a><ul><li><a href=#06--156248-llms-among-us-generative-ai-participating-in-digital-discourse-kristina-radivojevic-et-al-2024>(0/6 | 156/248) LLMs Among Us: Generative AI Participating in Digital Discourse (Kristina Radivojevic et al., 2024)</a></li><li><a href=#16--157248-keyframer-empowering-animation-design-using-large-language-models-tiffany-tseng-et-al-2024>(1/6 | 157/248) Keyframer: Empowering Animation Design using Large Language Models (Tiffany Tseng et al., 2024)</a></li><li><a href=#26--158248-randomness-is-all-you-need-semantic-traversal-of-problem-solution-spaces-with-large-language-models-thomas-sandholm-et-al-2024>(2/6 | 158/248) Randomness Is All You Need: Semantic Traversal of Problem-Solution Spaces with Large Language Models (Thomas Sandholm et al., 2024)</a></li><li><a href=#36--159248-ufo-a-ui-focused-agent-for-windows-os-interaction-chaoyun-zhang-et-al-2024>(3/6 | 159/248) UFO: A UI-Focused Agent for Windows OS Interaction (Chaoyun Zhang et al., 2024)</a></li><li><a href=#46--160248-personalizing-driver-safety-interfaces-via-driver-cognitive-factors-inference-emily-s-sumner-et-al-2024>(4/6 | 160/248) Personalizing Driver Safety Interfaces via Driver Cognitive Factors Inference (Emily S Sumner et al., 2024)</a></li><li><a href=#56--161248-form-from-a-design-space-of-social-media-systems-amy-x-zhang-et-al-2024>(5/6 | 161/248) Form-From: A Design Space of Social Media Systems (Amy X. Zhang et al., 2024)</a></li></ul></li><li><a href=#cscr-3>cs.CR (3)</a><ul><li><a href=#03--162248-comprehensive-assessment-of-jailbreak-attacks-against-llms-junjie-chu-et-al-2024>(0/3 | 162/248) Comprehensive Assessment of Jailbreak Attacks Against LLMs (Junjie Chu et al., 2024)</a></li><li><a href=#13--163248-domain-agnostic-hardware-fingerprinting-based-device-identifier-for-zero-trust-iot-security-abdurrahman-elmaghbub-et-al-2024>(1/3 | 163/248) Domain-Agnostic Hardware Fingerprinting-Based Device Identifier for Zero-Trust IoT Security (Abdurrahman Elmaghbub et al., 2024)</a></li><li><a href=#23--164248-buffer-overflow-in-mixture-of-experts-jamie-hayes-et-al-2024>(2/3 | 164/248) Buffer Overflow in Mixture of Experts (Jamie Hayes et al., 2024)</a></li></ul></li><li><a href=#q-finst-1>q-fin.ST (1)</a><ul><li><a href=#01--165248-a-study-on-stock-forecasting-using-deep-learning-and-statistical-models-himanshu-gupta-et-al-2024>(0/1 | 165/248) A Study on Stock Forecasting Using Deep Learning and Statistical Models (Himanshu Gupta et al., 2024)</a></li></ul></li><li><a href=#eesssp-2>eess.SP (2)</a><ul><li><a href=#02--166248-graph-neural-networks-for-physical-layer-security-in-multi-user-flexible-duplex-networks-tharaka-perera-et-al-2024>(0/2 | 166/248) Graph Neural Networks for Physical-Layer Security in Multi-User Flexible-Duplex Networks (Tharaka Perera et al., 2024)</a></li><li><a href=#12--167248-a-non-intrusive-neural-quality-assessment-model-for-surface-electromyography-signals-cho-yuan-lee-et-al-2024>(1/2 | 167/248) A Non-Intrusive Neural Quality Assessment Model for Surface Electromyography Signals (Cho-Yuan Lee et al., 2024)</a></li></ul></li><li><a href=#physicsflu-dyn-1>physics.flu-dyn (1)</a><ul><li><a href=#01--168248-reduced-order-modeling-of-unsteady-fluid-flow-using-neural-network-ensembles-rakesh-halder-et-al-2024>(0/1 | 168/248) Reduced-order modeling of unsteady fluid flow using neural network ensembles (Rakesh Halder et al., 2024)</a></li></ul></li><li><a href=#csse-9>cs.SE (9)</a><ul><li><a href=#09--169248-how-to-refactor-this-code-an-exploratory-study-on-developer-chatgpt-refactoring-conversations-eman-abdullah-alomar-et-al-2024>(0/9 | 169/248) How to Refactor this Code? An Exploratory Study on Developer-ChatGPT Refactoring Conversations (Eman Abdullah AlOmar et al., 2024)</a></li><li><a href=#19--170248-neural-models-for-source-code-synthesis-and-completion-mitodru-niyogi-2024>(1/9 | 170/248) Neural Models for Source Code Synthesis and Completion (Mitodru Niyogi, 2024)</a></li><li><a href=#29--171248-rocks-coding-not-development--a-human-centric-experimental-evaluation-of-llm-supported-se-tasks-wei-wang-et-al-2024>(2/9 | 171/248) Rocks Coding, Not Development&ndash;A Human-Centric, Experimental Evaluation of LLM-Supported SE Tasks (Wei Wang et al., 2024)</a></li><li><a href=#39--172248-do-large-code-models-understand-programming-concepts-a-black-box-approach-ashish-hooda-et-al-2024>(3/9 | 172/248) Do Large Code Models Understand Programming Concepts? A Black-box Approach (Ashish Hooda et al., 2024)</a></li><li><a href=#49--173248-the-impact-of-ai-tool-on-engineering-at-anz-bank-an-emperical-study-on-github-copilot-within-coporate-environment-sayan-chatterjee-et-al-2024>(4/9 | 173/248) The Impact of AI Tool on Engineering at ANZ Bank An Emperical Study on GitHub Copilot within Coporate Environment (Sayan Chatterjee et al., 2024)</a></li><li><a href=#59--174248-anticopypaster-20-whitebox-just-in-time-code-duplicates-extraction-eman-abdullah-alomar-et-al-2024>(5/9 | 174/248) AntiCopyPaster 2.0: Whitebox just-in-time code duplicates extraction (Eman Abdullah AlOmar et al., 2024)</a></li><li><a href=#69--175248-using-changeset-descriptions-as-a-data-source-to-assist-feature-location-muslim-chochlov-et-al-2024>(6/9 | 175/248) Using Changeset Descriptions as a Data Source to Assist Feature Location (Muslim Chochlov et al., 2024)</a></li><li><a href=#79--176248-investigating-reproducibility-in-deep-learning-based-software-fault-prediction-adil-mukhtar-et-al-2024>(7/9 | 176/248) Investigating Reproducibility in Deep Learning-Based Software Fault Prediction (Adil Mukhtar et al., 2024)</a></li><li><a href=#89--177248-polaris-a-framework-to-guide-the-development-of-trustworthy-ai-systems-maria-teresa-baldassarre-et-al-2024>(8/9 | 177/248) POLARIS: A framework to guide the development of Trustworthy AI systems (Maria Teresa Baldassarre et al., 2024)</a></li></ul></li><li><a href=#cscy-3>cs.CY (3)</a><ul><li><a href=#03--178248-examining-gender-and-racial-bias-in-large-vision-language-models-using-a-novel-dataset-of-parallel-images-kathleen-c-fraser-et-al-2024>(0/3 | 178/248) Examining Gender and Racial Bias in Large Vision-Language Models Using a Novel Dataset of Parallel Images (Kathleen C. Fraser et al., 2024)</a></li><li><a href=#13--179248-a-framework-for-assessing-proportionate-intervention-with-face-recognition-systems-in-real-life-scenarios-pablo-negri-et-al-2024>(1/3 | 179/248) A Framework for Assessing Proportionate Intervention with Face Recognition Systems in Real-Life Scenarios (Pablo Negri et al., 2024)</a></li><li><a href=#23--180248-a-survey-on-safe-multi-modal-learning-system-tianyi-zhao-et-al-2024>(2/3 | 180/248) A Survey on Safe Multi-Modal Learning System (Tianyi Zhao et al., 2024)</a></li></ul></li><li><a href=#csir-2>cs.IR (2)</a><ul><li><a href=#02--181248-counterclr-counterfactual-contrastive-learning-with-non-random-missing-data-in-recommendation-jun-wang-et-al-2024>(0/2 | 181/248) CounterCLR: Counterfactual Contrastive Learning with Non-random Missing Data in Recommendation (Jun Wang et al., 2024)</a></li><li><a href=#12--182248-natural-language-user-profiles-for-transparent-and-scrutable-recommendations-jerome-ramos-et-al-2024>(1/2 | 182/248) Natural Language User Profiles for Transparent and Scrutable Recommendations (Jerome Ramos et al., 2024)</a></li></ul></li><li><a href=#csro-15>cs.RO (15)</a><ul><li><a href=#015--183248-real-world-robot-applications-of-foundation-models-a-review-kento-kawaharazuka-et-al-2024>(0/15 | 183/248) Real-World Robot Applications of Foundation Models: A Review (Kento Kawaharazuka et al., 2024)</a></li><li><a href=#115--184248-driving-everywhere-with-large-language-model-policy-adaptation-boyi-li-et-al-2024>(1/15 | 184/248) Driving Everywhere with Large Language Model Policy Adaptation (Boyi Li et al., 2024)</a></li><li><a href=#215--185248-uplam-robust-panoptic-localization-and-mapping-leveraging-perception-uncertainties-kshitij-sirohi-et-al-2024>(2/15 | 185/248) uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception Uncertainties (Kshitij Sirohi et al., 2024)</a></li><li><a href=#315--186248-learning-to-control-emulated-muscles-in-real-robots-towards-exploiting-bio-inspired-actuator-morphology-pierre-schumacher-et-al-2024>(3/15 | 186/248) Learning to Control Emulated Muscles in Real Robots: Towards Exploiting Bio-Inspired Actuator Morphology (Pierre Schumacher et al., 2024)</a></li><li><a href=#415--187248-gliding-in-extreme-waters-dynamic-modeling-and-nonlinear-control-of-an-agile-underwater-glider-hanzhi-yang-et-al-2024>(4/15 | 187/248) Gliding in extreme waters: Dynamic Modeling and Nonlinear Control of an Agile Underwater Glider (Hanzhi Yang et al., 2024)</a></li><li><a href=#515--188248-on-experimental-emulation-of-printability-and-fleet-aware-generic-mesh-decomposition-for-enabling-aerial-3d-printing-marios-nektarios-stamatopoulos-et-al-2024>(5/15 | 188/248) On Experimental Emulation of Printability and Fleet Aware Generic Mesh Decomposition for Enabling Aerial 3D Printing (Marios-Nektarios Stamatopoulos et al., 2024)</a></li><li><a href=#615--189248-an-optimal-control-formulation-of-tool-affordance-applied-to-impact-tasks-boyang-ti-et-al-2024>(6/15 | 189/248) An Optimal Control Formulation of Tool Affordance Applied to Impact Tasks (Boyang Ti et al., 2024)</a></li><li><a href=#715--190248-cure-simulation-augmented-auto-tuning-in-robotics-md-abir-hossen-et-al-2024>(7/15 | 190/248) CURE: Simulation-Augmented Auto-Tuning in Robotics (Md Abir Hossen et al., 2024)</a></li><li><a href=#815--191248-real-world-fluid-directed-rigid-body-control-via-deep-reinforcement-learning-mohak-bhardwaj-et-al-2024>(8/15 | 191/248) Real-World Fluid Directed Rigid Body Control via Deep Reinforcement Learning (Mohak Bhardwaj et al., 2024)</a></li><li><a href=#915--192248-body-schema-acquisition-through-active-learning-ruben-martinez-cantin-et-al-2024>(9/15 | 192/248) Body Schema Acquisition through Active Learning (Ruben Martinez-Cantin et al., 2024)</a></li><li><a href=#1015--193248-intelligent-mode-switching-framework-for-teleoperation-burak-kizilkaya-et-al-2024>(10/15 | 193/248) Intelligent Mode-switching Framework for Teleoperation (Burak Kizilkaya et al., 2024)</a></li><li><a href=#1115--194248-anatomy-of-a-robotaxi-crash-lessons-from-the-cruise-pedestrian-dragging-mishap-philip-koopman-2024>(11/15 | 194/248) Anatomy of a Robotaxi Crash: Lessons from the Cruise Pedestrian Dragging Mishap (Philip Koopman, 2024)</a></li><li><a href=#1215--195248-a-versatile-robotic-hand-with-3d-perception-force-sensing-for-autonomous-manipulation-nikolaus-correll-et-al-2024>(12/15 | 195/248) A versatile robotic hand with 3D perception, force sensing for autonomous manipulation (Nikolaus Correll et al., 2024)</a></li><li><a href=#1315--196248-funcgrasp-learning-object-centric-neural-grasp-functions-from-single-annotated-example-object-hanzhi-chen-et-al-2024>(13/15 | 196/248) FuncGrasp: Learning Object-Centric Neural Grasp Functions from Single Annotated Example Object (Hanzhi Chen et al., 2024)</a></li><li><a href=#1415--197248-youve-got-to-feel-it-to-believe-it-multi-modal-bayesian-inference-for-semantic-and-property-prediction-parker-ewen-et-al-2024>(14/15 | 197/248) You&rsquo;ve Got to Feel It To Believe It: Multi-Modal Bayesian Inference for Semantic and Property Prediction (Parker Ewen et al., 2024)</a></li></ul></li><li><a href=#csce-2>cs.CE (2)</a><ul><li><a href=#02--198248-i-fenn-with-temporal-convolutional-networks-expediting-the-load-history-analysis-of-non-local-gradient-damage-propagation-panos-pantidis-et-al-2024>(0/2 | 198/248) I-FENN with Temporal Convolutional Networks: expediting the load-history analysis of non-local gradient damage propagation (Panos Pantidis et al., 2024)</a></li><li><a href=#12--199248-shape-optimization-of-eigenfrequencies-in-mems-gyroscopes-daniel-schiwietz-et-al-2024>(1/2 | 199/248) Shape Optimization of Eigenfrequencies in MEMS Gyroscopes (Daniel Schiwietz et al., 2024)</a></li></ul></li><li><a href=#q-bioqm-1>q-bio.QM (1)</a><ul><li><a href=#01--200248-anfinsen-goes-neural-a-graphical-model-for-conditional-antibody-design-nayoung-kim-et-al-2024>(0/1 | 200/248) Anfinsen Goes Neural: a Graphical Model for Conditional Antibody Design (Nayoung Kim et al., 2024)</a></li></ul></li><li><a href=#csma-2>cs.MA (2)</a><ul><li><a href=#02--201248-offline-risk-sensitive-rl-with-partial-observability-to-enhance-performance-in-human-robot-teaming-giorgio-angelotti-et-al-2024>(0/2 | 201/248) Offline Risk-sensitive RL with Partial Observability to Enhance Performance in Human-Robot Teaming (Giorgio Angelotti et al., 2024)</a></li><li><a href=#12--202248-linking-vision-and-multi-agent-communication-through-visible-light-communication-using-event-cameras-haruyuki-nakagawa-et-al-2024>(1/2 | 202/248) Linking Vision and Multi-Agent Communication through Visible Light Communication using Event Cameras (Haruyuki Nakagawa et al., 2024)</a></li></ul></li><li><a href=#csit-4>cs.IT (4)</a><ul><li><a href=#04--203248-coded-many-user-multiple-access-via-approximate-message-passing-xiaoqi-liu-et-al-2024>(0/4 | 203/248) Coded Many-User Multiple Access via Approximate Message Passing (Xiaoqi Liu et al., 2024)</a></li><li><a href=#14--204248-boosting-dynamic-tdd-in-small-cell-networks-by-the-multiplicative-weight-update-method-jiaqi-zhu-et-al-2024>(1/4 | 204/248) Boosting Dynamic TDD in Small Cell Networks by the Multiplicative Weight Update Method (Jiaqi Zhu et al., 2024)</a></li><li><a href=#24--205248-can-channels-be-fully-inferred-between-two-antenna-panels-y-qiu-et-al-2024>(2/4 | 205/248) Can Channels be Fully Inferred Between Two Antenna Panels? (Y. Qiu et al., 2024)</a></li><li><a href=#34--206248-localized-and-distributed-beyond-diagonal-reconfigurable-intelligent-surfaces-with-lossy-interconnections-modeling-and-optimization-matteo-nerini-et-al-2024>(3/4 | 206/248) Localized and Distributed Beyond Diagonal Reconfigurable Intelligent Surfaces with Lossy Interconnections: Modeling and Optimization (Matteo Nerini et al., 2024)</a></li></ul></li><li><a href=#eessas-2>eess.AS (2)</a><ul><li><a href=#02--207248-sound-source-separation-using-latent-variational-block-wise-disentanglement-karim-helwani-et-al-2024>(0/2 | 207/248) Sound Source Separation Using Latent Variational Block-Wise Disentanglement (Karim Helwani et al., 2024)</a></li><li><a href=#12--208248-integrating-self-supervised-speech-model-with-pseudo-word-level-targets-from-visually-grounded-speech-model-hung-chieh-fang-et-al-2024>(1/2 | 208/248) Integrating Self-supervised Speech Model with Pseudo Word-level Targets from Visually-grounded Speech Model (Hung-Chieh Fang et al., 2024)</a></li></ul></li><li><a href=#eessiv-6>eess.IV (6)</a><ul><li><a href=#06--209248-memory-efficient-deep-end-to-end-posterior-network-deepen-for-inverse-problems-jyothi-rikhab-chand-et-al-2024>(0/6 | 209/248) Memory-efficient deep end-to-end posterior network (DEEPEN) for inverse problems (Jyothi Rikhab Chand et al., 2024)</a></li><li><a href=#16--210248-histohdr-net-histogram-equalization-for-single-ldr-to-hdr-image-translation-hrishav-bakul-barua-et-al-2024>(1/6 | 210/248) HistoHDR-Net: Histogram Equalization for Single LDR to HDR Image Translation (Hrishav Bakul Barua et al., 2024)</a></li><li><a href=#26--211248-capability-enhancement-of-the-x-ray-micro-tomography-system-via-ml-assisted-approaches-dhruvi-shah-et-al-2024>(2/6 | 211/248) Capability enhancement of the X-ray micro-tomography system via ML-assisted approaches (Dhruvi Shah et al., 2024)</a></li><li><a href=#36--212248-unleashing-the-infinity-power-of-geometry-a-novel-geometry-aware-transformer-goat-for-whole-slide-histopathology-image-analysis-mingxin-liu-et-al-2024>(3/6 | 212/248) Unleashing the Infinity Power of Geometry: A Novel Geometry-Aware Transformer (GOAT) for Whole Slide Histopathology Image Analysis (Mingxin Liu et al., 2024)</a></li><li><a href=#46--213248-using-yolo-v7-to-detect-kidney-in-magnetic-resonance-imaging-pouria-yazdian-anari-et-al-2024>(4/6 | 213/248) Using YOLO v7 to Detect Kidney in Magnetic Resonance Imaging (Pouria Yazdian Anari et al., 2024)</a></li><li><a href=#56--214248-joint-end-to-end-image-compression-and-denoising-leveraging-contrastive-learning-and-multi-scale-self-onns-yuxin-xie-et-al-2024>(5/6 | 214/248) Joint End-to-End Image Compression and Denoising: Leveraging Contrastive Learning and Multi-Scale Self-ONNs (Yuxin Xie et al., 2024)</a></li></ul></li><li><a href=#cssi-1>cs.SI (1)</a><ul><li><a href=#01--215248-get-tok-a-genai-enriched-multimodal-tiktok-dataset-documenting-the-2022-attempted-coup-in-peru-gabriela-pinto-et-al-2024>(0/1 | 215/248) GET-Tok: A GenAI-Enriched Multimodal TikTok Dataset Documenting the 2022 Attempted Coup in Peru (Gabriela Pinto et al., 2024)</a></li></ul></li><li><a href=#cspf-1>cs.PF (1)</a><ul><li><a href=#01--216248-reconsidering-the-performance-of-devs-modeling-and-simulation-environments-using-the-devstone-benchmark-josé-l-risco-martín-et-al-2024>(0/1 | 216/248) Reconsidering the performance of DEVS modeling and simulation environments using the DEVStone benchmark (José L. Risco-Martín et al., 2024)</a></li></ul></li><li><a href=#eesssy-7>eess.SY (7)</a><ul><li><a href=#07--217248-multi-network-constrained-operational-optimization-in-community-integrated-energy-systems-a-safe-reinforcement-learning-approach-ze-hu-et-al-2024>(0/7 | 217/248) Multi-Network Constrained Operational Optimization in Community Integrated Energy Systems: A Safe Reinforcement Learning Approach (Ze Hu et al., 2024)</a></li><li><a href=#17--218248-consensus-driven-deviated-pursuit-for-guaranteed-simultaneous-interception-of-moving-targets-abhinav-sinha-et-al-2024>(1/7 | 218/248) Consensus-driven Deviated Pursuit for Guaranteed Simultaneous Interception of Moving Targets (Abhinav Sinha et al., 2024)</a></li><li><a href=#27--219248-underwater-mems-gyrocompassing-a-virtual-testing-ground-daniel-engelsman-et-al-2024>(2/7 | 219/248) Underwater MEMS Gyrocompassing: A Virtual Testing Ground (Daniel Engelsman et al., 2024)</a></li><li><a href=#37--220248-stochastic-colregs-evaluation-for-safe-navigation-under-uncertainty-peter-nicholas-hansen-et-al-2024>(3/7 | 220/248) Stochastic COLREGs Evaluation for Safe Navigation under Uncertainty (Peter Nicholas Hansen et al., 2024)</a></li><li><a href=#47--221248-triangular-phase-shift-detector-for-drone-precise-vertical-landing-rf-systems-víctor-araña-pulido-et-al-2024>(4/7 | 221/248) Triangular phase-shift detector for drone precise vertical landing RF systems (Víctor Araña-Pulido et al., 2024)</a></li><li><a href=#57--222248-design-and-prototyping-of-transmissive-ris-aided-wireless-communication-jianan-zhang-et-al-2024>(5/7 | 222/248) Design and Prototyping of Transmissive RIS-Aided Wireless Communication (Jianan Zhang et al., 2024)</a></li><li><a href=#67--223248-internal-model-control-design-for-systems-learned-by-control-affine-neural-nonlinear-autoregressive-exogenous-models-jing-xie-et-al-2024>(6/7 | 223/248) Internal Model Control design for systems learned by Control Affine Neural Nonlinear Autoregressive Exogenous Models (Jing Xie et al., 2024)</a></li></ul></li><li><a href=#csgt-2>cs.GT (2)</a><ul><li><a href=#02--224248-combining-voting-and-abstract-argumentation-to-understand-online-discussions-michael-bernreiter-et-al-2024>(0/2 | 224/248) Combining Voting and Abstract Argumentation to Understand Online Discussions (Michael Bernreiter et al., 2024)</a></li><li><a href=#12--225248-when-is-mean-field-reinforcement-learning-tractable-and-relevant-batuhan-yardim-et-al-2024>(1/2 | 225/248) When is Mean-Field Reinforcement Learning Tractable and Relevant? (Batuhan Yardim et al., 2024)</a></li></ul></li><li><a href=#statml-5>stat.ML (5)</a><ul><li><a href=#05--226248-prior-dependent-allocations-for-bayesian-fixed-budget-best-arm-identification-in-structured-bandits-nicolas-nguyen-et-al-2024>(0/5 | 226/248) Prior-Dependent Allocations for Bayesian Fixed-Budget Best-Arm Identification in Structured Bandits (Nicolas Nguyen et al., 2024)</a></li><li><a href=#15--227248-how-do-transformers-perform-in-context-autoregressive-learning-michael-e-sander-et-al-2024>(1/5 | 227/248) How do Transformers perform In-Context Autoregressive Learning? (Michael E. Sander et al., 2024)</a></li><li><a href=#25--228248-remedi-corrective-transformations-for-improved-neural-entropy-estimation-viktor-nilsson-et-al-2024>(2/5 | 228/248) REMEDI: Corrective Transformations for Improved Neural Entropy Estimation (Viktor Nilsson et al., 2024)</a></li><li><a href=#35--229248-a-high-dimensional-model-for-adversarial-training-geometry-and-trade-offs-kasimir-tanner-et-al-2024>(3/5 | 229/248) A High Dimensional Model for Adversarial Training: Geometry and Trade-Offs (Kasimir Tanner et al., 2024)</a></li><li><a href=#45--230248-classification-under-nuisance-parameters-and-generalized-label-shift-in-likelihood-free-inference-luca-masserano-et-al-2024>(4/5 | 230/248) Classification under Nuisance Parameters and Generalized Label Shift in Likelihood-Free Inference (Luca Masserano et al., 2024)</a></li></ul></li><li><a href=#csmm-1>cs.MM (1)</a><ul><li><a href=#01--231248-performance-evaluation-of-associative-watermarking-using-statistical-neurodynamics-ryoto-kanegae-et-al-2024>(0/1 | 231/248) Performance Evaluation of Associative Watermarking Using Statistical Neurodynamics (Ryoto Kanegae et al., 2024)</a></li></ul></li><li><a href=#cssd-1>cs.SD (1)</a><ul><li><a href=#01--232248-multispecies-bird-sound-recognition-using-a-fully-convolutional-neural-network-maría-teresa-garcía-ordás-et-al-2024>(0/1 | 232/248) Multispecies bird sound recognition using a fully convolutional neural network (María Teresa García-Ordás et al., 2024)</a></li></ul></li><li><a href=#physicssoc-ph-1>physics.soc-ph (1)</a><ul><li><a href=#01--233248-social-physics-informed-diffusion-model-for-crowd-simulation-hongyi-chen-et-al-2024>(0/1 | 233/248) Social Physics Informed Diffusion Model for Crowd Simulation (Hongyi Chen et al., 2024)</a></li></ul></li><li><a href=#csni-1>cs.NI (1)</a><ul><li><a href=#01--234248-a-state-of-the-art-survey-on-full-duplex-network-design-yonghwi-kim-et-al-2024>(0/1 | 234/248) A State-of-the-art Survey on Full-duplex Network Design (Yonghwi Kim et al., 2024)</a></li></ul></li><li><a href=#physicsao-ph-1>physics.ao-ph (1)</a><ul><li><a href=#01--235248-ai4fapar-how-artificial-intelligence-can-help-to-forecast-the-seasonal-earth-observation-signal-filip-sabo-et-al-2024>(0/1 | 235/248) Ai4Fapar: How artificial intelligence can help to forecast the seasonal earth observation signal (Filip Sabo et al., 2024)</a></li></ul></li><li><a href=#physicsoptics-1>physics.optics (1)</a><ul><li><a href=#01--236248-3d-2d-neural-nets-for-phase-retrieval-in-noisy-interferometric-imaging-andrew-h-proppe-et-al-2024>(0/1 | 236/248) 3D-2D Neural Nets for Phase Retrieval in Noisy Interferometric Imaging (Andrew H. Proppe et al., 2024)</a></li></ul></li><li><a href=#q-biobm-1>q-bio.BM (1)</a><ul><li><a href=#01--237248-dirichlet-flow-matching-with-applications-to-dna-sequence-design-hannes-stark-et-al-2024>(0/1 | 237/248) Dirichlet Flow Matching with Applications to DNA Sequence Design (Hannes Stark et al., 2024)</a></li></ul></li><li><a href=#mathna-6>math.NA (6)</a><ul><li><a href=#06--238248-strassens-algorithm-is-not-optimally-accurate-jean-guillaume-dumas-et-al-2024>(0/6 | 238/248) Strassen&rsquo;s algorithm is not optimally accurate (Jean-Guillaume Dumas et al., 2024)</a></li><li><a href=#16--239248-neural-functional-a-posteriori-error-estimates-vladimir-fanaskov-et-al-2024>(1/6 | 239/248) Neural functional a posteriori error estimates (Vladimir Fanaskov et al., 2024)</a></li><li><a href=#26--240248-neural-multigrid-architectures-vladimir-fanaskov-2024>(2/6 | 240/248) Neural Multigrid Architectures (Vladimir Fanaskov, 2024)</a></li><li><a href=#36--241248-how-to-split-a-tera-polynomial-françois-vigneron-et-al-2024>(3/6 | 241/248) How to split a tera-polynomial (François Vigneron et al., 2024)</a></li><li><a href=#46--242248-sear-pc-sensitivity-enhanced-arbitrary-polynomial-chaos-nick-pepper-et-al-2024>(4/6 | 242/248) SeAr PC: Sensitivity Enhanced Arbitrary Polynomial Chaos (Nick Pepper et al., 2024)</a></li><li><a href=#56--243248-robust-implicit-adaptive-low-rank-time-stepping-methods-for-matrix-differential-equations-daniel-appelö-et-al-2024>(5/6 | 243/248) Robust Implicit Adaptive Low Rank Time-Stepping Methods for Matrix Differential Equations (Daniel Appelö et al., 2024)</a></li></ul></li><li><a href=#physicsmed-ph-1>physics.med-ph (1)</a><ul><li><a href=#01--244248-neural-graphics-primitives-based-deformable-image-registration-for-on-the-fly-motion-extraction-xia-li-et-al-2024>(0/1 | 244/248) Neural Graphics Primitives-based Deformable Image Registration for On-the-fly Motion Extraction (Xia Li et al., 2024)</a></li></ul></li><li><a href=#csds-1>cs.DS (1)</a><ul><li><a href=#01--245248-tight-approximation-bounds-on-a-simple-algorithm-for-minimum-average-search-time-in-trees-svein-høgemo-2024>(0/1 | 245/248) Tight Approximation Bounds on a Simple Algorithm for Minimum Average Search Time in Trees (Svein Høgemo, 2024)</a></li></ul></li><li><a href=#q-biogn-1>q-bio.GN (1)</a><ul><li><a href=#01--246248-machine-learning-applied-to-omics-data-aida-calviño-et-al-2024>(0/1 | 246/248) Machine learning applied to omics data (Aida Calviño et al., 2024)</a></li></ul></li><li><a href=#csdl-1>cs.DL (1)</a><ul><li><a href=#01--247248-can-chatgpt-evaluate-research-quality-mike-thelwall-2024>(0/1 | 247/248) Can ChatGPT evaluate research quality? (Mike Thelwall, 2024)</a></li></ul></li><li><a href=#mathoc-1>math.OC (1)</a><ul><li><a href=#01--248248-machine-learning-augmented-branch-and-bound-for-mixed-integer-linear-programming-lara-scavuzzo-et-al-2024>(0/1 | 248/248) Machine Learning Augmented Branch and Bound for Mixed Integer Linear Programming (Lara Scavuzzo et al., 2024)</a></li></ul></li></ul></nav></div></div></section></div><footer id=footer class="container-fluid text-center align-content-center footer pb-2"><div class="container pt-5"><div class="row text-left"><div class="col-md-4 col-sm-12"><h5>Navigation</h5><ul><li class=nav-item><a class=smooth-scroll href=https://akitenkrad.github.io/akitenkrad-blog/#about>About</a></li><li class=nav-item><a class=smooth-scroll href=https://akitenkrad.github.io/akitenkrad-blog/#recent-posts>Recent Posts</a></li></ul></div><div class="col-md-4 col-sm-12"><h5>Contact me:</h5><ul><li><a href=mailto:contact.to.akitenkrad@gmail.com target=_blank rel=noopener><span><i class="fas fa-envelope"></i></span> <span>contact.to.akitenkrad@gmail.com</span></a></li></ul></div></div></div><hr><div class=container><div class="row text-left"><div class=col-md-4><a id=theme href=https://github.com/hossainemruz/toha target=_blank rel=noopener><img src=/akitenkrad-blog/images/theme-logo_hu8376fd15465fef26ffe66b6bcf0ca686_13669_32x0_resize_box_3.png alt="Toha Theme Logo">
Toha</a></div><div class="col-md-4 text-center">© 2020 Akitenkrad.</div><div class="col-md-4 text-right"><a id=hugo href=https://gohugo.io/ target=_blank rel=noopener>Powered by
<img src=/akitenkrad-blog/images/hugo-logo.svg alt="Hugo Logo" height=18></a></div></div></div></footer><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/jquery-3.4.1.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/popper.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/bootstrap.min.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/navbar.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/plyr.js></script><script type=text/javascript src=https://akitenkrad.github.io/akitenkrad-blog/js/main.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js></script><script src=https://akitenkrad.github.io/akitenkrad-blog/js/single.js></script><script>hljs.initHighlightingOnLoad()</script><link rel=stylesheet href=https://akitenkrad.github.io/akitenkrad-blog/katex/katex.min.css><script type=text/javascript defer src=https://akitenkrad.github.io/akitenkrad-blog/katex/katex.min.js></script><script type=text/javascript defer src=https://akitenkrad.github.io/akitenkrad-blog/katex/auto-render.min.js></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\[",right:"\\]",display:!0},{left:"\\(",right:"\\)",display:!1}],throwOnError:!0})})</script></body></html>